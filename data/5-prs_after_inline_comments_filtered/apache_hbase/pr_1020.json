{"pr_number": 1020, "pr_title": "HBASE-23653 Expose content of meta table in web ui", "pr_createdAt": "2020-01-10T22:50:09Z", "pr_url": "https://github.com/apache/hbase/pull/1020", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3OTY2Ng==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365479666", "bodyText": "this should be HConstants.STATE_QUALIFIER", "author": "ndimiduk", "createdAt": "2020-01-11T00:24:16Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final ExecutorService pool;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.pool = buildThreadPool();\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  @Override\n+  public Iterator<RegionReplicaInfo> iterator() {\n+    return limitIterator();\n+  }\n+\n+  public LimitIterator<RegionReplicaInfo> limitIterator() {\n+    logger.debug(\"initiating meta scan, {}\", this);\n+\n+    final AsyncTable<ScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME, pool);\n+    // TODO: buffering the entire result set seems unnecessary.\n+    final List<RegionReplicaInfo> results = asyncTable.scanAll(buildScan()).join()\n+      .stream()\n+      .map(RegionReplicaInfo::from)\n+      .flatMap(Collection::stream)\n+      .collect(Collectors.toList());\n+    return new LimitIterator<>(\n+      results.iterator(), Optional.ofNullable(scanLimit).orElse(SCAN_LIMIT_DEFAULT));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static ExecutorService buildThreadPool() {\n+    return Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setNameFormat(\"MetaBrowser-%d\")\n+      .setDaemon(true)\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) -> logger.info(\"Error in worker thread, {}\", throwable.getMessage()))\n+      .build());\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(NAME_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .orElse(null);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final Optional<String> requestValueStr = Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_LIMIT_PARAM))\n+      .filter(StringUtils::isNotBlank);\n+    if (!requestValueStr.isPresent()) { return null; }\n+\n+    final Integer requestValue = requestValueStr\n+      .flatMap(MetaBrowser::tryParseInt)\n+      .orElse(null);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanLimitMalformedErrorMessage(requestValueStr.get()));\n+      return null;\n+    }\n+    if (requestValue <= 0) {\n+      errorMessages.add(buildScanLimitLTEZero(requestValue));\n+      return SCAN_LIMIT_DEFAULT;\n+    }\n+\n+    final int truncatedValue = Math.min(requestValue, SCAN_LIMIT_MAX);\n+    if (requestValue != truncatedValue) {\n+      errorMessages.add(buildScanLimitExceededErrorMessage(requestValue));\n+    }\n+    return truncatedValue;\n+  }\n+\n+  private RegionState.State resolveScanRegionState(final HttpServletRequest request) {\n+    final Optional<String> requestValueStr = Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_REGION_STATE_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode);\n+    if (!requestValueStr.isPresent()) { return null; }\n+\n+    final RegionState.State requestValue = requestValueStr\n+      .flatMap(val -> tryValueOf(RegionState.State.class, val))\n+      .orElse(null);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanRegionStateMalformedErrorMessage(requestValueStr.get()));\n+      return null;\n+    }\n+    return requestValue;\n+  }\n+\n+  private static byte[] resolveScanStart(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_START_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .map(Bytes::toBytesBinary)\n+      .orElse(null);\n+  }\n+\n+  private static TableName resolveScanTable(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_TABLE_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .map(TableName::valueOf)\n+      .orElse(null);\n+  }\n+\n+  private static Filter buildTableFilter(final TableName tableName) {\n+    return new PrefixFilter(tableName.toBytes());\n+  }\n+\n+  private static Filter buildScanRegionStateFilter(final RegionState.State state) {\n+    return new SingleColumnValueFilter(\n+      HConstants.CATALOG_FAMILY,\n+      HConstants.TABLE_STATE_QUALIFIER,", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ2ODQ1MQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365468451", "bodyText": "Any reason not to use Iterators.limit() from guava?", "author": "bharathv", "createdAt": "2020-01-10T23:21:39Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/LimitIterator.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.util.Iterator;\n+import java.util.NoSuchElementException;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n+\n+/**\n+ * An {@link Iterator} over {@code delegate} that limits results to the first {@code limit}", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjAxOTQwOA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366019408", "bodyText": "Please read the rest of the comment :)", "author": "ndimiduk", "createdAt": "2020-01-13T20:46:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ2ODQ1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3NTgxMw==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365475813", "bodyText": "Doesn't ResultScanner support an iterator()? Wrap it with a limit iterator instead of scan all? This way, I guess 10 such requests with max limit can be heavy for the master.", "author": "bharathv", "createdAt": "2020-01-11T00:00:30Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final ExecutorService pool;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.pool = buildThreadPool();\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  @Override\n+  public Iterator<RegionReplicaInfo> iterator() {\n+    return limitIterator();\n+  }\n+\n+  public LimitIterator<RegionReplicaInfo> limitIterator() {\n+    logger.debug(\"initiating meta scan, {}\", this);\n+\n+    final AsyncTable<ScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME, pool);\n+    // TODO: buffering the entire result set seems unnecessary.", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA0MDAxMw==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366040013", "bodyText": "So this gets a bit messy. A ResultScanner needs to be closed when the underlying iterator is exhausted. Right now there's no interface for closing that loop. I haven't found a great way to both (1) keep the interface in table.jsp limited to an Iterator<RegionReplicaInfo> and (2) ensure the scanner resources are cleaned up. The alternative I've thought of is to have MetaBrowser return a ResultScanner and leave table.jsp to do the work of transforming the results and limiting iteration. It seems like too much work left up to the caller.\nSuggestions?", "author": "ndimiduk", "createdAt": "2020-01-13T21:33:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3NTgxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA5MzA5NQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366093095", "bodyText": "Okay, I believe I have a solution to this. One sec.", "author": "ndimiduk", "createdAt": "2020-01-14T00:03:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3NTgxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3NjE5OQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365476199", "bodyText": "Curious why it runs in its own thread. Add a clarifying comment?", "author": "bharathv", "createdAt": "2020-01-11T00:02:28Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final ExecutorService pool;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.pool = buildThreadPool();\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  @Override\n+  public Iterator<RegionReplicaInfo> iterator() {\n+    return limitIterator();\n+  }\n+\n+  public LimitIterator<RegionReplicaInfo> limitIterator() {\n+    logger.debug(\"initiating meta scan, {}\", this);\n+\n+    final AsyncTable<ScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME, pool);\n+    // TODO: buffering the entire result set seems unnecessary.\n+    final List<RegionReplicaInfo> results = asyncTable.scanAll(buildScan()).join()\n+      .stream()\n+      .map(RegionReplicaInfo::from)\n+      .flatMap(Collection::stream)\n+      .collect(Collectors.toList());\n+    return new LimitIterator<>(\n+      results.iterator(), Optional.ofNullable(scanLimit).orElse(SCAN_LIMIT_DEFAULT));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static ExecutorService buildThreadPool() {", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjAyMjUwOQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366022509", "bodyText": "The docs say it's dangerous to not provide an executor service for callback handlers that do work. Without it, they'll execute on an RPC thread, which leads to starvation. On the other hand, it looks like AsyncTableImpl doesn't use provided ExecutorService when executing getScanner. Bleh, confusing. Will remove.", "author": "ndimiduk", "createdAt": "2020-01-13T20:54:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3NjE5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3NzA1MQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365477051", "bodyText": "haha , functional style is not as readable :D (my personal opinion)", "author": "bharathv", "createdAt": "2020-01-11T00:07:10Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final ExecutorService pool;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.pool = buildThreadPool();\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  @Override\n+  public Iterator<RegionReplicaInfo> iterator() {\n+    return limitIterator();\n+  }\n+\n+  public LimitIterator<RegionReplicaInfo> limitIterator() {\n+    logger.debug(\"initiating meta scan, {}\", this);\n+\n+    final AsyncTable<ScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME, pool);\n+    // TODO: buffering the entire result set seems unnecessary.\n+    final List<RegionReplicaInfo> results = asyncTable.scanAll(buildScan()).join()\n+      .stream()\n+      .map(RegionReplicaInfo::from)\n+      .flatMap(Collection::stream)\n+      .collect(Collectors.toList());\n+    return new LimitIterator<>(\n+      results.iterator(), Optional.ofNullable(scanLimit).orElse(SCAN_LIMIT_DEFAULT));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static ExecutorService buildThreadPool() {\n+    return Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setNameFormat(\"MetaBrowser-%d\")\n+      .setDaemon(true)\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) -> logger.info(\"Error in worker thread, {}\", throwable.getMessage()))\n+      .build());\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(NAME_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .orElse(null);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final Optional<String> requestValueStr = Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_LIMIT_PARAM))\n+      .filter(StringUtils::isNotBlank);\n+    if (!requestValueStr.isPresent()) { return null; }\n+\n+    final Integer requestValue = requestValueStr\n+      .flatMap(MetaBrowser::tryParseInt)\n+      .orElse(null);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanLimitMalformedErrorMessage(requestValueStr.get()));\n+      return null;\n+    }\n+    if (requestValue <= 0) {\n+      errorMessages.add(buildScanLimitLTEZero(requestValue));\n+      return SCAN_LIMIT_DEFAULT;\n+    }\n+\n+    final int truncatedValue = Math.min(requestValue, SCAN_LIMIT_MAX);\n+    if (requestValue != truncatedValue) {\n+      errorMessages.add(buildScanLimitExceededErrorMessage(requestValue));\n+    }\n+    return truncatedValue;\n+  }\n+\n+  private RegionState.State resolveScanRegionState(final HttpServletRequest request) {\n+    final Optional<String> requestValueStr = Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_REGION_STATE_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode);\n+    if (!requestValueStr.isPresent()) { return null; }\n+\n+    final RegionState.State requestValue = requestValueStr\n+      .flatMap(val -> tryValueOf(RegionState.State.class, val))\n+      .orElse(null);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanRegionStateMalformedErrorMessage(requestValueStr.get()));\n+      return null;\n+    }\n+    return requestValue;\n+  }\n+\n+  private static byte[] resolveScanStart(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_START_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .map(Bytes::toBytesBinary)\n+      .orElse(null);\n+  }\n+\n+  private static TableName resolveScanTable(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_TABLE_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .map(TableName::valueOf)\n+      .orElse(null);\n+  }\n+\n+  private static Filter buildTableFilter(final TableName tableName) {\n+    return new PrefixFilter(tableName.toBytes());\n+  }\n+\n+  private static Filter buildScanRegionStateFilter(final RegionState.State state) {\n+    return new SingleColumnValueFilter(\n+      HConstants.CATALOG_FAMILY,\n+      HConstants.TABLE_STATE_QUALIFIER,\n+      CompareOperator.EQUAL,\n+      // use the same serialization strategy as found in MetaTableAccessor#addRegionStateToPut\n+      Bytes.toBytes(state.name()));\n+  }\n+\n+  private Optional<Filter> buildScanFilter() {\n+    if (scanTable == null && scanRegionState == null) {\n+      return Optional.empty();\n+    }\n+\n+    final List<Filter> filters = new ArrayList<>(2);\n+    Optional.ofNullable(scanTable)\n+      .map(MetaBrowser::buildTableFilter)\n+      .ifPresent(filters::add);\n+    Optional.ofNullable(scanRegionState)", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjAyNzE1NA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366027154", "bodyText": "Yeah, it's probably a bit overboard here. I started a back port to branch-1 and found there's only a couple places where Optional makes things easier.", "author": "ndimiduk", "createdAt": "2020-01-13T21:05:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3NzA1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3NzE3MA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365477170", "bodyText": "nit: I think this could use a javadoc.", "author": "bharathv", "createdAt": "2020-01-11T00:07:55Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final ExecutorService pool;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.pool = buildThreadPool();\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  @Override\n+  public Iterator<RegionReplicaInfo> iterator() {\n+    return limitIterator();\n+  }\n+\n+  public LimitIterator<RegionReplicaInfo> limitIterator() {\n+    logger.debug(\"initiating meta scan, {}\", this);\n+\n+    final AsyncTable<ScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME, pool);\n+    // TODO: buffering the entire result set seems unnecessary.\n+    final List<RegionReplicaInfo> results = asyncTable.scanAll(buildScan()).join()\n+      .stream()\n+      .map(RegionReplicaInfo::from)\n+      .flatMap(Collection::stream)\n+      .collect(Collectors.toList());\n+    return new LimitIterator<>(\n+      results.iterator(), Optional.ofNullable(scanLimit).orElse(SCAN_LIMIT_DEFAULT));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static ExecutorService buildThreadPool() {\n+    return Executors.newSingleThreadExecutor(new ThreadFactoryBuilder()\n+      .setNameFormat(\"MetaBrowser-%d\")\n+      .setDaemon(true)\n+      .setUncaughtExceptionHandler(\n+        (thread, throwable) -> logger.info(\"Error in worker thread, {}\", throwable.getMessage()))\n+      .build());\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(NAME_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .orElse(null);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final Optional<String> requestValueStr = Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_LIMIT_PARAM))\n+      .filter(StringUtils::isNotBlank);\n+    if (!requestValueStr.isPresent()) { return null; }\n+\n+    final Integer requestValue = requestValueStr\n+      .flatMap(MetaBrowser::tryParseInt)\n+      .orElse(null);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanLimitMalformedErrorMessage(requestValueStr.get()));\n+      return null;\n+    }\n+    if (requestValue <= 0) {\n+      errorMessages.add(buildScanLimitLTEZero(requestValue));\n+      return SCAN_LIMIT_DEFAULT;\n+    }\n+\n+    final int truncatedValue = Math.min(requestValue, SCAN_LIMIT_MAX);\n+    if (requestValue != truncatedValue) {\n+      errorMessages.add(buildScanLimitExceededErrorMessage(requestValue));\n+    }\n+    return truncatedValue;\n+  }\n+\n+  private RegionState.State resolveScanRegionState(final HttpServletRequest request) {\n+    final Optional<String> requestValueStr = Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_REGION_STATE_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode);\n+    if (!requestValueStr.isPresent()) { return null; }\n+\n+    final RegionState.State requestValue = requestValueStr\n+      .flatMap(val -> tryValueOf(RegionState.State.class, val))\n+      .orElse(null);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanRegionStateMalformedErrorMessage(requestValueStr.get()));\n+      return null;\n+    }\n+    return requestValue;\n+  }\n+\n+  private static byte[] resolveScanStart(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_START_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .map(Bytes::toBytesBinary)\n+      .orElse(null);\n+  }\n+\n+  private static TableName resolveScanTable(final HttpServletRequest request) {\n+    return Optional.ofNullable(request)\n+      .map(req -> req.getParameter(SCAN_TABLE_PARAM))\n+      .filter(StringUtils::isNotBlank)\n+      .map(MetaBrowser::urlDecode)\n+      .map(TableName::valueOf)\n+      .orElse(null);\n+  }\n+\n+  private static Filter buildTableFilter(final TableName tableName) {\n+    return new PrefixFilter(tableName.toBytes());\n+  }\n+\n+  private static Filter buildScanRegionStateFilter(final RegionState.State state) {\n+    return new SingleColumnValueFilter(\n+      HConstants.CATALOG_FAMILY,\n+      HConstants.TABLE_STATE_QUALIFIER,\n+      CompareOperator.EQUAL,\n+      // use the same serialization strategy as found in MetaTableAccessor#addRegionStateToPut\n+      Bytes.toBytes(state.name()));\n+  }\n+\n+  private Optional<Filter> buildScanFilter() {\n+    if (scanTable == null && scanRegionState == null) {\n+      return Optional.empty();\n+    }\n+\n+    final List<Filter> filters = new ArrayList<>(2);\n+    Optional.ofNullable(scanTable)\n+      .map(MetaBrowser::buildTableFilter)\n+      .ifPresent(filters::add);\n+    Optional.ofNullable(scanRegionState)\n+      .map(MetaBrowser::buildScanRegionStateFilter)\n+      .ifPresent(filters::add);\n+\n+    if (filters.size() == 1) {\n+      return Optional.of(filters.get(0));\n+    }\n+\n+    return Optional.of(new FilterList(FilterList.Operator.MUST_PASS_ALL, filters));\n+  }\n+\n+  private Scan buildScan() {\n+    final Scan metaScan = new Scan()\n+      .addFamily(HConstants.CATALOG_FAMILY)\n+      .readVersions(1)\n+      .setLimit(Optional.ofNullable(scanLimit).orElse(SCAN_LIMIT_DEFAULT) + 1);\n+    Optional.ofNullable(scanStart)\n+      .ifPresent(startRow -> metaScan.withStartRow(startRow, false));\n+    buildScanFilter().ifPresent(metaScan::setFilter);\n+    return metaScan;\n+  }\n+\n+  private <T> void maybeAddParam(final QueryStringEncoder encoder, final String paramName,", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3ODcwMg==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365478702", "bodyText": "nit: the accessors methods could work directly on regioninfo object ? (less code).", "author": "bharathv", "createdAt": "2020-01-11T00:17:59Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.commons.lang3.builder.EqualsBuilder;\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.HRegionLocation;\n+import org.apache.hadoop.hbase.MetaTableAccessor;\n+import org.apache.hadoop.hbase.RegionLocations;\n+import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.master.assignment.RegionStateStore;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A POJO that consolidates the information about a single region replica that's stored in meta.\n+ */\n+@InterfaceAudience.Private\n+public final class RegionReplicaInfo {\n+  private final byte[] row;\n+  private final RegionInfo regionInfo;\n+  private final byte[] regionName;\n+  private final byte[] startKey;\n+  private final byte[] endKey;\n+  private final Integer replicaId;\n+  private final RegionState.State regionState;\n+  private final ServerName serverName;\n+\n+  private RegionReplicaInfo(final Result result, final HRegionLocation location) {\n+    final Optional<Result> maybeResult = Optional.ofNullable(result);\n+    final Optional<HRegionLocation> maybeLocation = Optional.ofNullable(location);\n+    final Optional<RegionInfo> maybeRegionInfo = maybeLocation.map(HRegionLocation::getRegion);\n+\n+    this.row = maybeResult.map(Result::getRow).orElse(null);", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA0NTc5NQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366045795", "bodyText": "Pushes the null-checking out to access instead of at object creation. I generally prefer checking to happen at object creation, since the values don't change. You think the one approach will be easier to read/maintain than the other?", "author": "ndimiduk", "createdAt": "2020-01-13T21:46:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ3ODcwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ4MDMwNg==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365480306", "bodyText": "nice.. for some reason, I thought we already had this somewhere, looked around but didn't find one.", "author": "bharathv", "createdAt": "2020-01-11T00:28:23Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.StringJoiner;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.hbase.client.AsyncAdmin;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.junit.Rule;\n+import org.junit.rules.ExternalResource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link Rule} that clears all user tables and namespaces before the test executes.\n+ */\n+public class ClearUserNamespacesAndTablesRule extends ExternalResource {", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ4MDk1Mg==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365480952", "bodyText": "May be add a comment that this class doesn't close the connection by the end of it and needs to chained with TestConnectionRule which does it.", "author": "bharathv", "createdAt": "2020-01-11T00:31:57Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.StringJoiner;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.hbase.client.AsyncAdmin;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.junit.Rule;\n+import org.junit.rules.ExternalResource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link Rule} that clears all user tables and namespaces before the test executes.\n+ */\n+public class ClearUserNamespacesAndTablesRule extends ExternalResource {\n+  private static final Logger logger =\n+    LoggerFactory.getLogger(ClearUserNamespacesAndTablesRule.class);\n+\n+  private final Supplier<AsyncConnection> connectionSupplier;", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ4MTA1OQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r365481059", "bodyText": "nit: You'll probably run into checkstyle issues with inline if blocks.", "author": "bharathv", "createdAt": "2020-01-11T00:32:42Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/TestClusterRule.java", "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import java.io.IOException;\n+import java.util.concurrent.CompletableFuture;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.ConnectionFactory;\n+import org.junit.Rule;\n+import org.junit.rules.ExternalResource;\n+\n+/**\n+ * A {@link Rule} that manages an instance of the {@link MiniHBaseCluster}.\n+ */\n+public class TestClusterRule extends ExternalResource {\n+  private final HBaseTestingUtility testingUtility;\n+  private final StartMiniClusterOption miniClusterOptions;\n+\n+  private MiniHBaseCluster miniCluster;\n+\n+  public TestClusterRule() {\n+    this(StartMiniClusterOption.builder().build());\n+  }\n+\n+  public TestClusterRule(final StartMiniClusterOption miniClusterOptions) {\n+    this.testingUtility = new HBaseTestingUtility();\n+    this.miniClusterOptions = miniClusterOptions;\n+  }\n+\n+  public CompletableFuture<AsyncConnection> createConnection() {\n+    if (miniCluster == null) { throw new IllegalStateException(\"test cluster not initialized\"); }", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA0NzQ3MA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366047470", "bodyText": "Yeah but I like them better... reminds me I want to see if check style supports this style and propose the change on dev@.", "author": "ndimiduk", "createdAt": "2020-01-13T21:50:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ4MTA1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA2NDg2Mg==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366064862", "bodyText": "Oops. All this logging should be at TRACE level.", "author": "ndimiduk", "createdAt": "2020-01-13T22:33:17Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.StringJoiner;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.hbase.client.AsyncAdmin;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.junit.Rule;\n+import org.junit.rules.ExternalResource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link Rule} that clears all user tables and namespaces before the test executes.\n+ */\n+public class ClearUserNamespacesAndTablesRule extends ExternalResource {\n+  private static final Logger logger =\n+    LoggerFactory.getLogger(ClearUserNamespacesAndTablesRule.class);\n+\n+  private final Supplier<AsyncConnection> connectionSupplier;\n+  private AsyncAdmin admin;\n+\n+  public ClearUserNamespacesAndTablesRule(final Supplier<AsyncConnection> connectionSupplier) {\n+    this.connectionSupplier = connectionSupplier;\n+  }\n+\n+  @Override\n+  protected void before() throws Throwable {\n+    final AsyncConnection connection = Objects.requireNonNull(connectionSupplier.get());\n+    admin = connection.getAdmin();\n+\n+    clearTablesAndNamespaces().join();\n+  }\n+\n+  private CompletableFuture<Void> clearTablesAndNamespaces() {\n+    return deleteUserTables().thenCompose(_void -> deleteUserNamespaces());\n+  }\n+\n+  private CompletableFuture<Void> deleteUserTables() {\n+    return listTableNames()\n+      .thenApply(tableNames -> tableNames.stream()\n+        .map(tableName -> disableIfEnabled(tableName).thenCompose(_void -> deleteTable(tableName)))\n+        .toArray(CompletableFuture[]::new))\n+      .thenCompose(CompletableFuture::allOf);\n+  }\n+\n+  private CompletableFuture<List<TableName>> listTableNames() {\n+    return CompletableFuture.runAsync(() -> logger.info(\"listing tables\"))", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA3NDQ0Mg==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366074442", "bodyText": "Good", "author": "saintstack", "createdAt": "2020-01-13T22:59:42Z", "path": "hbase-client/src/main/java/org/apache/hadoop/hbase/RegionLocations.java", "diffHunk": "@@ -31,7 +34,7 @@\n  * (assuming small number of locations)\n  */\n @InterfaceAudience.Private\n-public class RegionLocations {\n+public class RegionLocations implements Iterable<HRegionLocation> {", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA3NDk1MA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366074950", "bodyText": "hbase-common? Especially given this is generic.", "author": "saintstack", "createdAt": "2020-01-13T23:01:12Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/LimitIterator.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.util.Iterator;\n+import java.util.NoSuchElementException;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n+\n+/**\n+ * An {@link Iterator} over {@code delegate} that limits results to the first {@code limit}\n+ * entries.\n+ * <p>Could just use {@link Iterators#limit(Iterator, int)} except that our consumer needs an API\n+ * to check if the underlying iterator is not yet exhausted.\n+ */\n+@InterfaceAudience.Private\n+public class LimitIterator<T> implements Iterator<T> {\n+\n+  private final Iterator<T> delegate;\n+  private final int limit;\n+  private int count;\n+\n+  LimitIterator(final Iterator<T> delegate, final int limit) {\n+    this.delegate = delegate;\n+    this.limit = limit;\n+    this.count = 0;\n+  }\n+\n+  /**\n+   * @return {@code true} when {@code delegate} has more entries, {@code false} otherwise.\n+   */\n+  public boolean delegateHasMore() {\n+    return delegate.hasNext();\n+  }\n+\n+  @Override\n+  public boolean hasNext() {\n+    if (count < limit) {\n+      return delegate.hasNext();\n+    }\n+    return false;\n+  }\n+\n+  @Override\n+  public T next() {\n+    if (!hasNext()) { throw new NoSuchElementException(); }\n+    count++;\n+    return delegate.next();\n+  }\n+}", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA5MTczNw==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366091737", "bodyText": "Actually, now that I've refactored off the delegateHasMore method, I can go back to using the implementation provided by Guava.", "author": "ndimiduk", "createdAt": "2020-01-13T23:57:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA3NDk1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MDE2NA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366080164", "bodyText": "Can I page through a 100k hbase:meta ?", "author": "saintstack", "createdAt": "2020-01-13T23:17:33Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA5Mjg1NA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366092854", "bodyText": "Yes, but not in a single page. A user can view up to SCAN_LIMIT_MAX rows in a single request. If there are more results, the next page link is rendered. When clicked, the next page worth of results are rendered. This pagination continues until all results are exhausted.\nThe scanner is not held across button clicks, so the result set is not stable. If new entries are inserted before the \"lastRow\" pagination token, then the user will no see them. There's probably also a bug around region replicas, because I don't track how many replicas have been rendered by the page, only the last row rendered. Thus it's possible to have up to all non-primary replicas for one region slip through the cracks.", "author": "ndimiduk", "createdAt": "2020-01-14T00:02:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MDE2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjEwNjQzMg==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366106432", "bodyText": "I think some form of this could be captured in the javadoc class comment. Otherwise, one needs to understand how this class paginates the result by setting the start row intelligently..", "author": "bharathv", "createdAt": "2020-01-14T00:56:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MDE2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUxMzYzNg==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366513636", "bodyText": "Big fat javadoc applied.", "author": "ndimiduk", "createdAt": "2020-01-14T18:54:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MDE2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MDM0NA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366080344", "bodyText": "Above all have to public?", "author": "saintstack", "createdAt": "2020-01-13T23:18:15Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final ExecutorService pool;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.pool = buildThreadPool();\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA5Mjk4Ng==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366092986", "bodyText": "To be accessed from table.jsp, yes, I believe so.", "author": "ndimiduk", "createdAt": "2020-01-14T00:02:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MDM0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MDk4MQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366080981", "bodyText": "Scan all each time we page?", "author": "saintstack", "createdAt": "2020-01-13T23:20:24Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.Collectors;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.ScanResultConsumer;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser implements Iterable<RegionReplicaInfo> {\n+  private static final Logger logger = LoggerFactory.getLogger(MetaBrowser.class);\n+\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final ExecutorService pool;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.pool = buildThreadPool();\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  @Override\n+  public Iterator<RegionReplicaInfo> iterator() {\n+    return limitIterator();\n+  }\n+\n+  public LimitIterator<RegionReplicaInfo> limitIterator() {\n+    logger.debug(\"initiating meta scan, {}\", this);\n+\n+    final AsyncTable<ScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME, pool);\n+    // TODO: buffering the entire result set seems unnecessary.\n+    final List<RegionReplicaInfo> results = asyncTable.scanAll(buildScan()).join()", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MTgwNw==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366081807", "bodyText": "MetaTable Visitor no good to you in here?", "author": "saintstack", "createdAt": "2020-01-13T23:22:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MDk4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA5NDAxOQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366094019", "bodyText": "Scan all each time we page?\n\nScan all of meta? No. Scanning starts at the row represented by SCAN_START_PARAM and is limited by SCAN_LIMIT_PARAM and the other filter parameters. See the TestMetaBrowser#paginate* tests.\n\nMetaTable Visitor no good to you in here?\n\nIs that a class? I figured out how to do away with scanAll; moving to the old fashioned ResultScanner.", "author": "ndimiduk", "createdAt": "2020-01-14T00:06:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MDk4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MjIwNA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366082204", "bodyText": "How's this differ from a RegionInfo?", "author": "saintstack", "createdAt": "2020-01-13T23:24:17Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.commons.lang3.builder.EqualsBuilder;\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.HRegionLocation;\n+import org.apache.hadoop.hbase.MetaTableAccessor;\n+import org.apache.hadoop.hbase.RegionLocations;\n+import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.master.assignment.RegionStateStore;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A POJO that consolidates the information about a single region replica that's stored in meta.", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MjQxNA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366082414", "bodyText": "It has state and serverName? A location has RI and SN? Should this subclass one of them then?", "author": "saintstack", "createdAt": "2020-01-13T23:25:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MjIwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA5NjcyMA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366096720", "bodyText": "How's this differ from a RegionInfo?\n\nI guess it's an HRegionLocation + meta table's rowkey + RegionState with best-effort accessors. I don't think subclassing buys us much.", "author": "ndimiduk", "createdAt": "2020-01-14T00:16:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MjIwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MjY0NQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366082645", "bodyText": "These have to be public?", "author": "saintstack", "createdAt": "2020-01-13T23:25:50Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.commons.lang3.builder.EqualsBuilder;\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.HRegionLocation;\n+import org.apache.hadoop.hbase.MetaTableAccessor;\n+import org.apache.hadoop.hbase.RegionLocations;\n+import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.master.assignment.RegionStateStore;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A POJO that consolidates the information about a single region replica that's stored in meta.\n+ */\n+@InterfaceAudience.Private\n+public final class RegionReplicaInfo {\n+  private final byte[] row;\n+  private final RegionInfo regionInfo;\n+  private final byte[] regionName;\n+  private final byte[] startKey;\n+  private final byte[] endKey;\n+  private final Integer replicaId;\n+  private final RegionState.State regionState;\n+  private final ServerName serverName;\n+\n+  private RegionReplicaInfo(final Result result, final HRegionLocation location) {\n+    final Optional<Result> maybeResult = Optional.ofNullable(result);\n+    final Optional<HRegionLocation> maybeLocation = Optional.ofNullable(location);\n+    final Optional<RegionInfo> maybeRegionInfo = maybeLocation.map(HRegionLocation::getRegion);\n+\n+    this.row = maybeResult.map(Result::getRow).orElse(null);\n+    this.regionInfo = maybeRegionInfo.orElse(null);\n+    this.regionName = maybeRegionInfo.map(RegionInfo::getRegionName).orElse(null);\n+    this.startKey = maybeRegionInfo.map(RegionInfo::getStartKey).orElse(null);\n+    this.endKey = maybeRegionInfo.map(RegionInfo::getEndKey).orElse(null);\n+    this.replicaId = maybeRegionInfo.map(RegionInfo::getReplicaId).orElse(null);\n+    this.regionState = result != null && maybeRegionInfo.isPresent()\n+      ? RegionStateStore.getRegionState(result, maybeRegionInfo.get())\n+      : null;\n+    this.serverName = maybeLocation.map(HRegionLocation::getServerName).orElse(null);\n+  }\n+\n+  public static List<RegionReplicaInfo> from(final Result result) {\n+    if (result == null) {\n+      return Collections.singletonList(null);\n+    }\n+\n+    final RegionLocations locations = MetaTableAccessor.getRegionLocations(result);\n+    if (locations == null) {\n+      return Collections.singletonList(null);\n+    }\n+\n+    return StreamSupport.stream(locations.spliterator(), false)\n+      .map(location -> new RegionReplicaInfo(result, location))\n+      .collect(Collectors.toList());\n+  }\n+\n+  public byte[] getRow() {\n+    return row;\n+  }\n+\n+  public RegionInfo getRegionInfo() {\n+    return regionInfo;\n+  }\n+\n+  public byte[] getRegionName() {\n+    return regionName;\n+  }\n+\n+  public byte[] getStartKey() {\n+    return startKey;\n+  }\n+\n+  public byte[] getEndKey() {\n+    return endKey;\n+  }\n+\n+  public Integer getReplicaId() {\n+    return replicaId;\n+  }\n+\n+  public RegionState.State getRegionState() {\n+    return regionState;\n+  }\n+\n+  public ServerName getServerName() {", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA5Njk2NA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366096964", "bodyText": "For the same reason as the accessors in MetaBrowser: they need to be reachable by table.jsp.", "author": "ndimiduk", "createdAt": "2020-01-14T00:17:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MjY0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4Mzc1MA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366083750", "bodyText": "One thought, why cleanup after a test? What if it takes a bunch of work cleaning up, more than just start fresh?", "author": "saintstack", "createdAt": "2020-01-13T23:29:17Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/ClearUserNamespacesAndTablesRule.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.StringJoiner;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.hbase.client.AsyncAdmin;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.junit.Rule;\n+import org.junit.rules.ExternalResource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link Rule} that clears all user tables and namespaces before the test executes.\n+ */\n+public class ClearUserNamespacesAndTablesRule extends ExternalResource {\n+  private static final Logger logger =\n+    LoggerFactory.getLogger(ClearUserNamespacesAndTablesRule.class);\n+\n+  private final Supplier<AsyncConnection> connectionSupplier;\n+  private AsyncAdmin admin;\n+\n+  public ClearUserNamespacesAndTablesRule(final Supplier<AsyncConnection> connectionSupplier) {\n+    this.connectionSupplier = connectionSupplier;\n+  }\n+\n+  @Override\n+  protected void before() throws Throwable {\n+    final AsyncConnection connection = Objects.requireNonNull(connectionSupplier.get());\n+    admin = connection.getAdmin();\n+\n+    clearTablesAndNamespaces().join();\n+  }\n+\n+  private CompletableFuture<Void> clearTablesAndNamespaces() {\n+    return deleteUserTables().thenCompose(_void -> deleteUserNamespaces());\n+  }\n+\n+  private CompletableFuture<Void> deleteUserTables() {\n+    return listTableNames()\n+      .thenApply(tableNames -> tableNames.stream()\n+        .map(tableName -> disableIfEnabled(tableName).thenCompose(_void -> deleteTable(tableName)))\n+        .toArray(CompletableFuture[]::new))\n+      .thenCompose(CompletableFuture::allOf);\n+  }\n+\n+  private CompletableFuture<List<TableName>> listTableNames() {\n+    return CompletableFuture.runAsync(() -> logger.info(\"listing tables\"))\n+      .thenCompose(_void -> admin.listTableNames(false))\n+      .thenApply(tableNames -> {\n+        final StringJoiner joiner = new StringJoiner(\", \", \"[\", \"]\");\n+        tableNames.stream().map(TableName::getNameAsString).forEach(joiner::add);\n+        logger.info(\"found existing tables {}\", joiner.toString());\n+        return tableNames;\n+      });\n+  }\n+\n+  private CompletableFuture<Boolean> isTableEnabled(final TableName tableName) {\n+    return admin.isTableEnabled(tableName)\n+      .thenApply(isEnabled -> {\n+        logger.info(\"table {} is enabled.\", tableName);\n+        return isEnabled;\n+      });\n+  }\n+\n+  private CompletableFuture<Void> disableIfEnabled(final TableName tableName) {\n+    return isTableEnabled(tableName)\n+      .thenCompose(isEnabled -> {\n+        if (isEnabled) { return disableTable(tableName); }\n+        return CompletableFuture.completedFuture(null);\n+      });\n+  }\n+\n+  private CompletableFuture<Void> disableTable(final TableName tableName) {\n+    return CompletableFuture.runAsync(() -> logger.info(\"disabling enabled table {}\", tableName))\n+      .thenCompose(_void -> admin.disableTable(tableName));\n+  }\n+\n+  private CompletableFuture<Void> deleteTable(final TableName tableName) {\n+    return CompletableFuture.runAsync(() -> logger.info(\"deleting disabled table {}\", tableName))\n+      .thenCompose(_void -> admin.deleteTable(tableName));\n+  }\n+\n+  private CompletableFuture<List<String>> listUserNamespaces() {\n+    return CompletableFuture.runAsync(() -> logger.info(\"listing namespaces\"))\n+      .thenCompose(_void -> admin.listNamespaceDescriptors())\n+      .thenApply(namespaceDescriptors -> {\n+        final StringJoiner joiner = new StringJoiner(\", \", \"[\", \"]\");\n+        final List<String> names = namespaceDescriptors.stream()\n+          .map(NamespaceDescriptor::getName)\n+          .peek(joiner::add)\n+          .collect(Collectors.toList());\n+        logger.info(\"found existing namespaces {}\", joiner.toString());\n+        return names;\n+      })\n+      .thenApply(namespaces -> namespaces.stream()\n+        .filter(namespace -> !Objects.equals(\n+          namespace, NamespaceDescriptor.SYSTEM_NAMESPACE.getName()))\n+        .filter(namespace -> !Objects.equals(\n+          namespace, NamespaceDescriptor.DEFAULT_NAMESPACE.getName()))\n+        .collect(Collectors.toList()));\n+  }\n+\n+  private CompletableFuture<Void> deleteNamespace(final String namespace) {\n+    return CompletableFuture.runAsync(() -> logger.info(\"deleting namespace {}\", namespace))\n+      .thenCompose(_void -> admin.deleteNamespace(namespace));\n+  }\n+\n+  private CompletableFuture<Void> deleteUserNamespaces() {\n+    return listUserNamespaces()\n+      .thenCompose(namespaces -> CompletableFuture.allOf(\n+        namespaces.stream()\n+          .map(this::deleteNamespace)\n+          .toArray(CompletableFuture[]::new)));\n+  }\n+}", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA5ODYwOA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366098608", "bodyText": "I'm writing tests over the content of meta. So I can\n\nclean up after each test method; or\ntear down and spin up a new mini cluster for each test method; or\ncreate a fixed set of tables @BeforeClass and write all test methods around those tables.\n\nI chose the first option because (2) would make for a really slow test run and (3) is very inflexible in terms of how each test method is written, and renders all other test methods brittle to tweaks of any one of them.", "author": "ndimiduk", "createdAt": "2020-01-14T00:23:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4Mzc1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4MzgwOA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366083808", "bodyText": "Fun", "author": "saintstack", "createdAt": "2020-01-13T23:29:33Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/ConnectionRule.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import java.io.IOException;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.Supplier;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.junit.Rule;\n+import org.junit.rules.ExternalResource;\n+\n+/**\n+ * A {@link Rule} that manages an instance of {@link AsyncConnection}.\n+ */\n+public class ConnectionRule extends ExternalResource {\n+\n+  private final Supplier<CompletableFuture<AsyncConnection>> connectionSupplier;\n+  private AsyncConnection connection;\n+\n+  public ConnectionRule(final Supplier<CompletableFuture<AsyncConnection>> connectionSupplier) {\n+    this.connectionSupplier = connectionSupplier;\n+  }\n+\n+  public AsyncConnection getConnection() {\n+    return connection;\n+  }\n+\n+  @Override\n+  protected void before() throws Throwable {\n+    this.connection = connectionSupplier.get().join();\n+  }\n+\n+  @Override\n+  protected void after() {\n+    if (this.connection != null) {\n+      try {\n+        connection.close();\n+      } catch (IOException e) {\n+        throw new RuntimeException(e);\n+      }\n+    }\n+  }\n+}", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4NDA4Mw==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366084083", "bodyText": "This only reason for import?", "author": "saintstack", "createdAt": "2020-01-13T23:30:28Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/client/hamcrest/BytesMatchers.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.client.hamcrest;\n+\n+import static org.hamcrest.core.Is.is;", "originalCommit": "fbbfa58a3186bbc3c1460adcd445cf9fd089ff8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA5OTIwOQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366099209", "bodyText": "I don't follow the question. This class comes out of hamcrest-core.", "author": "ndimiduk", "createdAt": "2020-01-14T00:26:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4NDA4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjE1NzQzMQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366157431", "bodyText": "Is this the only class we use from hamcrest? Are we adding a new dependency to modules just for one class? Its a good one I presume.", "author": "saintstack", "createdAt": "2020-01-14T05:25:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4NDA4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ2MzY3Nw==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366463677", "bodyText": "is is provided by the existing dependency, hamcrest-core. I've added the dependency on hamcrest-library for the the collection- and bean-related matchers that it provides. This patch makes use of contains and hasProperty, but I look forward to writing more tests that make use of others. It's quite unusual to depend only on hamcrest-core; usually -core and -library come as a pair in user applications. I believe -core is stand-alone so that 3rd party extension libraries (such as Spotify's java-hamcrest can pull in the minimal set if they so choose.", "author": "ndimiduk", "createdAt": "2020-01-14T17:09:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4NDA4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzIyMzg2NQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r367223865", "bodyText": "ok", "author": "saintstack", "createdAt": "2020-01-16T04:17:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA4NDA4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjEwODE2Mg==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366108162", "bodyText": "nit: If toString() is the only needed call, no need to use generics / templatize it? Just pass an object?", "author": "bharathv", "createdAt": "2020-01-14T01:03:05Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,378 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.stream.StreamSupport;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AdvancedScanResultConsumer;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser {\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  public Results getResults() {\n+    final AsyncTable<AdvancedScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME);\n+    return new Results(asyncTable.getScanner(buildScan()));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return resolveRequestParameter(request, NAME_PARAM);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_LIMIT_PARAM);\n+    if (StringUtils.isBlank(requestValueStr)) {\n+      return null;\n+    }\n+\n+    final Integer requestValue = tryParseInt(requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanLimitMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    if (requestValue <= 0) {\n+      errorMessages.add(buildScanLimitLTEZero(requestValue));\n+      return SCAN_LIMIT_DEFAULT;\n+    }\n+\n+    final int truncatedValue = Math.min(requestValue, SCAN_LIMIT_MAX);\n+    if (requestValue != truncatedValue) {\n+      errorMessages.add(buildScanLimitExceededErrorMessage(requestValue));\n+    }\n+    return truncatedValue;\n+  }\n+\n+  private RegionState.State resolveScanRegionState(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_REGION_STATE_PARAM);\n+    if (requestValueStr == null) {\n+      return null;\n+    }\n+    final RegionState.State requestValue = tryValueOf(RegionState.State.class, requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanRegionStateMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    return requestValue;\n+  }\n+\n+  private static byte[] resolveScanStart(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_START_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return Bytes.toBytesBinary(requestValue);\n+  }\n+\n+  private static TableName resolveScanTable(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_TABLE_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return TableName.valueOf(requestValue);\n+  }\n+\n+  private static String resolveRequestParameter(final HttpServletRequest request,\n+    final String param) {\n+    if (request == null) {\n+      return null;\n+    }\n+    final String requestValueStrEnc = request.getParameter(param);\n+    if (StringUtils.isBlank(requestValueStrEnc)) {\n+      return null;\n+    }\n+    return urlDecode(requestValueStrEnc);\n+  }\n+\n+  private static Filter buildTableFilter(final TableName tableName) {\n+    return new PrefixFilter(tableName.toBytes());\n+  }\n+\n+  private static Filter buildScanRegionStateFilter(final RegionState.State state) {\n+    return new SingleColumnValueFilter(\n+      HConstants.CATALOG_FAMILY,\n+      HConstants.STATE_QUALIFIER,\n+      CompareOperator.EQUAL,\n+      // use the same serialization strategy as found in MetaTableAccessor#addRegionStateToPut\n+      Bytes.toBytes(state.name()));\n+  }\n+\n+  private Filter buildScanFilter() {\n+    if (scanTable == null && scanRegionState == null) {\n+      return null;\n+    }\n+\n+    final List<Filter> filters = new ArrayList<>(2);\n+    if (scanTable != null) {\n+      filters.add(buildTableFilter(scanTable));\n+    }\n+    if (scanRegionState != null) {\n+      filters.add(buildScanRegionStateFilter(scanRegionState));\n+    }\n+    if (filters.size() == 1) {\n+      return filters.get(0);\n+    }\n+    return new FilterList(FilterList.Operator.MUST_PASS_ALL, filters);\n+  }\n+\n+  private Scan buildScan() {\n+    final Scan metaScan = new Scan()\n+      .addFamily(HConstants.CATALOG_FAMILY)\n+      .readVersions(1)\n+      .setLimit((scanLimit != null ? scanLimit : SCAN_LIMIT_DEFAULT) + 1);\n+    if (scanStart != null) {\n+      metaScan.withStartRow(scanStart, false);\n+    }\n+    final Filter filter = buildScanFilter();\n+    if (filter != null) {\n+      metaScan.setFilter(filter);\n+    }\n+    return metaScan;\n+  }\n+\n+  /**\n+   * Adds {@code value} to {@code encoder} under {@code paramName} when {@code value} is non-null.\n+   */\n+  private <T> void addParam(final QueryStringEncoder encoder, final String paramName,", "originalCommit": "fe6b1a02902d2ed3706b0438d79352ad47a01ea9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ2ODU0OA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366468548", "bodyText": "Meh, I suppose.", "author": "ndimiduk", "createdAt": "2020-01-14T17:19:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjEwODE2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjEwOTE4MQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366109181", "bodyText": "nit: return SCAN_LIMIT_DEFAULT here and simplify callers?", "author": "bharathv", "createdAt": "2020-01-14T01:07:07Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,378 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.stream.StreamSupport;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AdvancedScanResultConsumer;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser {\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  public Results getResults() {\n+    final AsyncTable<AdvancedScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME);\n+    return new Results(asyncTable.getScanner(buildScan()));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return resolveRequestParameter(request, NAME_PARAM);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_LIMIT_PARAM);\n+    if (StringUtils.isBlank(requestValueStr)) {\n+      return null;", "originalCommit": "fe6b1a02902d2ed3706b0438d79352ad47a01ea9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ2ODM5Ng==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366468396", "bodyText": "I think you're right. Now that I've implemented a protocol for error handling, I should be able to clean up this case. Let me review all the accessors under the same lens.", "author": "ndimiduk", "createdAt": "2020-01-14T17:18:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjEwOTE4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUwNTMxMg==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366505312", "bodyText": "Oh, no we cannot. We need to differentiate between a value specified or not.", "author": "ndimiduk", "createdAt": "2020-01-14T18:37:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjEwOTE4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjExMDY2MQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366110661", "bodyText": "I was about to suggest something like this.. wrap the output in some auto-closeable implementation that can be used in callers.. neat...", "author": "bharathv", "createdAt": "2020-01-14T01:13:53Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,378 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.stream.StreamSupport;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AdvancedScanResultConsumer;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser {\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  public Results getResults() {\n+    final AsyncTable<AdvancedScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME);\n+    return new Results(asyncTable.getScanner(buildScan()));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return resolveRequestParameter(request, NAME_PARAM);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_LIMIT_PARAM);\n+    if (StringUtils.isBlank(requestValueStr)) {\n+      return null;\n+    }\n+\n+    final Integer requestValue = tryParseInt(requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanLimitMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    if (requestValue <= 0) {\n+      errorMessages.add(buildScanLimitLTEZero(requestValue));\n+      return SCAN_LIMIT_DEFAULT;\n+    }\n+\n+    final int truncatedValue = Math.min(requestValue, SCAN_LIMIT_MAX);\n+    if (requestValue != truncatedValue) {\n+      errorMessages.add(buildScanLimitExceededErrorMessage(requestValue));\n+    }\n+    return truncatedValue;\n+  }\n+\n+  private RegionState.State resolveScanRegionState(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_REGION_STATE_PARAM);\n+    if (requestValueStr == null) {\n+      return null;\n+    }\n+    final RegionState.State requestValue = tryValueOf(RegionState.State.class, requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanRegionStateMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    return requestValue;\n+  }\n+\n+  private static byte[] resolveScanStart(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_START_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return Bytes.toBytesBinary(requestValue);\n+  }\n+\n+  private static TableName resolveScanTable(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_TABLE_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return TableName.valueOf(requestValue);\n+  }\n+\n+  private static String resolveRequestParameter(final HttpServletRequest request,\n+    final String param) {\n+    if (request == null) {\n+      return null;\n+    }\n+    final String requestValueStrEnc = request.getParameter(param);\n+    if (StringUtils.isBlank(requestValueStrEnc)) {\n+      return null;\n+    }\n+    return urlDecode(requestValueStrEnc);\n+  }\n+\n+  private static Filter buildTableFilter(final TableName tableName) {\n+    return new PrefixFilter(tableName.toBytes());\n+  }\n+\n+  private static Filter buildScanRegionStateFilter(final RegionState.State state) {\n+    return new SingleColumnValueFilter(\n+      HConstants.CATALOG_FAMILY,\n+      HConstants.STATE_QUALIFIER,\n+      CompareOperator.EQUAL,\n+      // use the same serialization strategy as found in MetaTableAccessor#addRegionStateToPut\n+      Bytes.toBytes(state.name()));\n+  }\n+\n+  private Filter buildScanFilter() {\n+    if (scanTable == null && scanRegionState == null) {\n+      return null;\n+    }\n+\n+    final List<Filter> filters = new ArrayList<>(2);\n+    if (scanTable != null) {\n+      filters.add(buildTableFilter(scanTable));\n+    }\n+    if (scanRegionState != null) {\n+      filters.add(buildScanRegionStateFilter(scanRegionState));\n+    }\n+    if (filters.size() == 1) {\n+      return filters.get(0);\n+    }\n+    return new FilterList(FilterList.Operator.MUST_PASS_ALL, filters);\n+  }\n+\n+  private Scan buildScan() {\n+    final Scan metaScan = new Scan()\n+      .addFamily(HConstants.CATALOG_FAMILY)\n+      .readVersions(1)\n+      .setLimit((scanLimit != null ? scanLimit : SCAN_LIMIT_DEFAULT) + 1);\n+    if (scanStart != null) {\n+      metaScan.withStartRow(scanStart, false);\n+    }\n+    final Filter filter = buildScanFilter();\n+    if (filter != null) {\n+      metaScan.setFilter(filter);\n+    }\n+    return metaScan;\n+  }\n+\n+  /**\n+   * Adds {@code value} to {@code encoder} under {@code paramName} when {@code value} is non-null.\n+   */\n+  private <T> void addParam(final QueryStringEncoder encoder, final String paramName,\n+    final T value) {\n+    if (value != null) {\n+      encoder.addParam(paramName, value.toString());\n+    }\n+  }\n+\n+  private QueryStringEncoder buildFirstPageEncoder() {\n+    final QueryStringEncoder encoder =\n+      new QueryStringEncoder(request.getRequestURI());\n+    addParam(encoder, NAME_PARAM, name);\n+    addParam(encoder, SCAN_LIMIT_PARAM, scanLimit);\n+    addParam(encoder, SCAN_REGION_STATE_PARAM, scanRegionState);\n+    addParam(encoder, SCAN_TABLE_PARAM, scanTable);\n+    return encoder;\n+  }\n+\n+  public String buildFirstPageUrl() {\n+    return buildFirstPageEncoder().toString();\n+  }\n+\n+  public static String buildStartParamFrom(final byte[] lastRow) {\n+    if (lastRow == null) {\n+      return null;\n+    }\n+    return urlEncode(Bytes.toStringBinary(lastRow));\n+  }\n+\n+  public String buildNextPageUrl(final byte[] lastRow) {\n+    final QueryStringEncoder encoder = buildFirstPageEncoder();\n+    final String startRow = buildStartParamFrom(lastRow);\n+    addParam(encoder, SCAN_START_PARAM, startRow);\n+    return encoder.toString();\n+  }\n+\n+  private static String urlEncode(final String val) {\n+    if (StringUtils.isEmpty(val)) {\n+      return null;\n+    }\n+    try {\n+      return URLEncoder.encode(val, StandardCharsets.UTF_8.toString());\n+    } catch (UnsupportedEncodingException e) {\n+      return null;\n+    }\n+  }\n+\n+  private static String urlDecode(final String val) {\n+    if (StringUtils.isEmpty(val)) {\n+      return null;\n+    }\n+    try {\n+      return URLDecoder.decode(val, StandardCharsets.UTF_8.toString());\n+    } catch (UnsupportedEncodingException e) {\n+      return null;\n+    }\n+  }\n+\n+  private static Integer tryParseInt(final String val) {\n+    if (StringUtils.isEmpty(val)) {\n+      return null;\n+    }\n+    try {\n+      return Integer.parseInt(val);\n+    } catch (NumberFormatException e) {\n+      return null;\n+    }\n+  }\n+\n+  private static <T extends Enum<T>> T tryValueOf(final Class<T> clazz,\n+    final String value) {\n+    if (clazz == null || value == null) {\n+      return null;\n+    }\n+    try {\n+      return T.valueOf(clazz, value);\n+    } catch (IllegalArgumentException e) {\n+      return null;\n+    }\n+  }\n+\n+  private static String buildScanLimitExceededErrorMessage(final int requestValue) {\n+    return String.format(\n+      \"Requested SCAN_LIMIT value %d exceeds maximum value %d.\", requestValue, SCAN_LIMIT_MAX);\n+  }\n+\n+  private static String buildScanLimitMalformedErrorMessage(final String requestValue) {\n+    return String.format(\n+      \"Requested SCAN_LIMIT value '%s' cannot be parsed as an integer.\", requestValue);\n+  }\n+\n+  private static String buildScanLimitLTEZero(final int requestValue) {\n+    return String.format(\"Requested SCAN_LIMIT value %d is <= 0.\", requestValue);\n+  }\n+\n+  private static String buildScanRegionStateMalformedErrorMessage(final String requestValue) {\n+    return String.format(\n+      \"Requested SCAN_REGION_STATE value '%s' cannot be parsed as a RegionState.\", requestValue);\n+  }\n+\n+  /**\n+   * Encapsulates the results produced by this {@link MetaBrowser} instance.\n+   */\n+  public final class Results implements AutoCloseable, Iterable<RegionReplicaInfo> {", "originalCommit": "fe6b1a02902d2ed3706b0438d79352ad47a01ea9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjExMTA0MQ==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366111041", "bodyText": "Thanks for the clean up.. this is what I had in mind.", "author": "bharathv", "createdAt": "2020-01-14T01:15:46Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/RegionReplicaInfo.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.commons.lang3.builder.EqualsBuilder;\n+import org.apache.commons.lang3.builder.HashCodeBuilder;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.HRegionLocation;\n+import org.apache.hadoop.hbase.MetaTableAccessor;\n+import org.apache.hadoop.hbase.RegionLocations;\n+import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.master.assignment.RegionStateStore;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+\n+/**\n+ * A POJO that consolidates the information about a single region replica that's stored in meta.\n+ */\n+@InterfaceAudience.Private\n+public final class RegionReplicaInfo {\n+  private final byte[] row;\n+  private final RegionInfo regionInfo;\n+  private final RegionState.State regionState;\n+  private final ServerName serverName;\n+\n+  private RegionReplicaInfo(final Result result, final HRegionLocation location) {\n+    this.row = result != null ? result.getRow() : null;\n+    this.regionInfo = location != null ? location.getRegion() : null;\n+    this.regionState = (result != null && regionInfo != null)\n+      ? RegionStateStore.getRegionState(result, regionInfo)\n+      : null;\n+    this.serverName = location != null ? location.getServerName() : null;\n+  }\n+\n+  public static List<RegionReplicaInfo> from(final Result result) {\n+    if (result == null) {\n+      return Collections.singletonList(null);\n+    }\n+\n+    final RegionLocations locations = MetaTableAccessor.getRegionLocations(result);\n+    if (locations == null) {\n+      return Collections.singletonList(null);\n+    }\n+\n+    return StreamSupport.stream(locations.spliterator(), false)\n+      .map(location -> new RegionReplicaInfo(result, location))\n+      .collect(Collectors.toList());\n+  }\n+\n+  public byte[] getRow() {\n+    return row;\n+  }\n+\n+  public RegionInfo getRegionInfo() {\n+    return regionInfo;\n+  }\n+\n+  public byte[] getRegionName() {", "originalCommit": "fe6b1a02902d2ed3706b0438d79352ad47a01ea9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjExMjU2Mg==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366112562", "bodyText": "I think, the +1 part is pretty subtle.. add a comment that it is needed for pagination hasNext() support?", "author": "bharathv", "createdAt": "2020-01-14T01:22:28Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,378 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.stream.StreamSupport;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AdvancedScanResultConsumer;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser {\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  public Results getResults() {\n+    final AsyncTable<AdvancedScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME);\n+    return new Results(asyncTable.getScanner(buildScan()));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return resolveRequestParameter(request, NAME_PARAM);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_LIMIT_PARAM);\n+    if (StringUtils.isBlank(requestValueStr)) {\n+      return null;\n+    }\n+\n+    final Integer requestValue = tryParseInt(requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanLimitMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    if (requestValue <= 0) {\n+      errorMessages.add(buildScanLimitLTEZero(requestValue));\n+      return SCAN_LIMIT_DEFAULT;\n+    }\n+\n+    final int truncatedValue = Math.min(requestValue, SCAN_LIMIT_MAX);\n+    if (requestValue != truncatedValue) {\n+      errorMessages.add(buildScanLimitExceededErrorMessage(requestValue));\n+    }\n+    return truncatedValue;\n+  }\n+\n+  private RegionState.State resolveScanRegionState(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_REGION_STATE_PARAM);\n+    if (requestValueStr == null) {\n+      return null;\n+    }\n+    final RegionState.State requestValue = tryValueOf(RegionState.State.class, requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanRegionStateMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    return requestValue;\n+  }\n+\n+  private static byte[] resolveScanStart(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_START_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return Bytes.toBytesBinary(requestValue);\n+  }\n+\n+  private static TableName resolveScanTable(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_TABLE_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return TableName.valueOf(requestValue);\n+  }\n+\n+  private static String resolveRequestParameter(final HttpServletRequest request,\n+    final String param) {\n+    if (request == null) {\n+      return null;\n+    }\n+    final String requestValueStrEnc = request.getParameter(param);\n+    if (StringUtils.isBlank(requestValueStrEnc)) {\n+      return null;\n+    }\n+    return urlDecode(requestValueStrEnc);\n+  }\n+\n+  private static Filter buildTableFilter(final TableName tableName) {\n+    return new PrefixFilter(tableName.toBytes());\n+  }\n+\n+  private static Filter buildScanRegionStateFilter(final RegionState.State state) {\n+    return new SingleColumnValueFilter(\n+      HConstants.CATALOG_FAMILY,\n+      HConstants.STATE_QUALIFIER,\n+      CompareOperator.EQUAL,\n+      // use the same serialization strategy as found in MetaTableAccessor#addRegionStateToPut\n+      Bytes.toBytes(state.name()));\n+  }\n+\n+  private Filter buildScanFilter() {\n+    if (scanTable == null && scanRegionState == null) {\n+      return null;\n+    }\n+\n+    final List<Filter> filters = new ArrayList<>(2);\n+    if (scanTable != null) {\n+      filters.add(buildTableFilter(scanTable));\n+    }\n+    if (scanRegionState != null) {\n+      filters.add(buildScanRegionStateFilter(scanRegionState));\n+    }\n+    if (filters.size() == 1) {\n+      return filters.get(0);\n+    }\n+    return new FilterList(FilterList.Operator.MUST_PASS_ALL, filters);\n+  }\n+\n+  private Scan buildScan() {\n+    final Scan metaScan = new Scan()\n+      .addFamily(HConstants.CATALOG_FAMILY)\n+      .readVersions(1)\n+      .setLimit((scanLimit != null ? scanLimit : SCAN_LIMIT_DEFAULT) + 1);", "originalCommit": "fe6b1a02902d2ed3706b0438d79352ad47a01ea9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ2Nzk4OA==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366467988", "bodyText": "Rgr.", "author": "ndimiduk", "createdAt": "2020-01-14T17:18:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjExMjU2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjQ4MzgzMg==", "url": "https://github.com/apache/hbase/pull/1020#discussion_r366483832", "bodyText": "this can be package-protected as it's only used internally and within the unit test.", "author": "ndimiduk", "createdAt": "2020-01-14T17:51:08Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/webapp/MetaBrowser.java", "diffHunk": "@@ -0,0 +1,378 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.webapp;\n+\n+import java.io.UnsupportedEncodingException;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.stream.StreamSupport;\n+import javax.servlet.http.HttpServletRequest;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.commons.lang3.builder.ToStringBuilder;\n+import org.apache.commons.lang3.builder.ToStringStyle;\n+import org.apache.hadoop.hbase.CompareOperator;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.AdvancedScanResultConsumer;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n+import org.apache.hadoop.hbase.client.AsyncTable;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.PrefixFilter;\n+import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n+import org.apache.hadoop.hbase.master.RegionState;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.hbase.thirdparty.com.google.common.collect.Iterators;\n+import org.apache.hbase.thirdparty.io.netty.handler.codec.http.QueryStringEncoder;\n+\n+/**\n+ * A support class for the \"Meta Entries\" section in\n+ * {@code resources/hbase-webapps/master/table.jsp}.\n+ */\n+@InterfaceAudience.Private\n+public class MetaBrowser {\n+  public static final String NAME_PARAM = \"name\";\n+  public static final String SCAN_LIMIT_PARAM = \"scan_limit\";\n+  public static final String SCAN_REGION_STATE_PARAM = \"scan_region_state\";\n+  public static final String SCAN_START_PARAM = \"scan_start\";\n+  public static final String SCAN_TABLE_PARAM = \"scan_table\";\n+\n+  public static final int SCAN_LIMIT_DEFAULT = 10;\n+  public static final int SCAN_LIMIT_MAX = 10_000;\n+\n+  private final AsyncConnection connection;\n+  private final HttpServletRequest request;\n+  private final List<String> errorMessages;\n+  private final String name;\n+  private final Integer scanLimit;\n+  private final RegionState.State scanRegionState;\n+  private final byte[] scanStart;\n+  private final TableName scanTable;\n+\n+  public MetaBrowser(final AsyncConnection connection, final HttpServletRequest request) {\n+    this.connection = connection;\n+    this.request = request;\n+    this.errorMessages = new LinkedList<>();\n+    this.name = resolveName(request);\n+    this.scanLimit = resolveScanLimit(request);\n+    this.scanRegionState = resolveScanRegionState(request);\n+    this.scanStart = resolveScanStart(request);\n+    this.scanTable = resolveScanTable(request);\n+  }\n+\n+  public List<String> getErrorMessages() {\n+    return errorMessages;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Integer getScanLimit() {\n+    return scanLimit;\n+  }\n+\n+  public byte[] getScanStart() {\n+    return scanStart;\n+  }\n+\n+  public RegionState.State getScanRegionState() {\n+    return scanRegionState;\n+  }\n+\n+  public TableName getScanTable() {\n+    return scanTable;\n+  }\n+\n+  public Results getResults() {\n+    final AsyncTable<AdvancedScanResultConsumer> asyncTable =\n+      connection.getTable(TableName.META_TABLE_NAME);\n+    return new Results(asyncTable.getScanner(buildScan()));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+      .append(\"scanStart\", scanStart)\n+      .append(\"scanLimit\", scanLimit)\n+      .append(\"scanTable\", scanTable)\n+      .append(\"scanRegionState\", scanRegionState)\n+      .toString();\n+  }\n+\n+  private static String resolveName(final HttpServletRequest request) {\n+    return resolveRequestParameter(request, NAME_PARAM);\n+  }\n+\n+  private Integer resolveScanLimit(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_LIMIT_PARAM);\n+    if (StringUtils.isBlank(requestValueStr)) {\n+      return null;\n+    }\n+\n+    final Integer requestValue = tryParseInt(requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanLimitMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    if (requestValue <= 0) {\n+      errorMessages.add(buildScanLimitLTEZero(requestValue));\n+      return SCAN_LIMIT_DEFAULT;\n+    }\n+\n+    final int truncatedValue = Math.min(requestValue, SCAN_LIMIT_MAX);\n+    if (requestValue != truncatedValue) {\n+      errorMessages.add(buildScanLimitExceededErrorMessage(requestValue));\n+    }\n+    return truncatedValue;\n+  }\n+\n+  private RegionState.State resolveScanRegionState(final HttpServletRequest request) {\n+    final String requestValueStr = resolveRequestParameter(request, SCAN_REGION_STATE_PARAM);\n+    if (requestValueStr == null) {\n+      return null;\n+    }\n+    final RegionState.State requestValue = tryValueOf(RegionState.State.class, requestValueStr);\n+    if (requestValue == null) {\n+      errorMessages.add(buildScanRegionStateMalformedErrorMessage(requestValueStr));\n+      return null;\n+    }\n+    return requestValue;\n+  }\n+\n+  private static byte[] resolveScanStart(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_START_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return Bytes.toBytesBinary(requestValue);\n+  }\n+\n+  private static TableName resolveScanTable(final HttpServletRequest request) {\n+    final String requestValue = resolveRequestParameter(request, SCAN_TABLE_PARAM);\n+    if (requestValue == null) {\n+      return null;\n+    }\n+    return TableName.valueOf(requestValue);\n+  }\n+\n+  private static String resolveRequestParameter(final HttpServletRequest request,\n+    final String param) {\n+    if (request == null) {\n+      return null;\n+    }\n+    final String requestValueStrEnc = request.getParameter(param);\n+    if (StringUtils.isBlank(requestValueStrEnc)) {\n+      return null;\n+    }\n+    return urlDecode(requestValueStrEnc);\n+  }\n+\n+  private static Filter buildTableFilter(final TableName tableName) {\n+    return new PrefixFilter(tableName.toBytes());\n+  }\n+\n+  private static Filter buildScanRegionStateFilter(final RegionState.State state) {\n+    return new SingleColumnValueFilter(\n+      HConstants.CATALOG_FAMILY,\n+      HConstants.STATE_QUALIFIER,\n+      CompareOperator.EQUAL,\n+      // use the same serialization strategy as found in MetaTableAccessor#addRegionStateToPut\n+      Bytes.toBytes(state.name()));\n+  }\n+\n+  private Filter buildScanFilter() {\n+    if (scanTable == null && scanRegionState == null) {\n+      return null;\n+    }\n+\n+    final List<Filter> filters = new ArrayList<>(2);\n+    if (scanTable != null) {\n+      filters.add(buildTableFilter(scanTable));\n+    }\n+    if (scanRegionState != null) {\n+      filters.add(buildScanRegionStateFilter(scanRegionState));\n+    }\n+    if (filters.size() == 1) {\n+      return filters.get(0);\n+    }\n+    return new FilterList(FilterList.Operator.MUST_PASS_ALL, filters);\n+  }\n+\n+  private Scan buildScan() {\n+    final Scan metaScan = new Scan()\n+      .addFamily(HConstants.CATALOG_FAMILY)\n+      .readVersions(1)\n+      .setLimit((scanLimit != null ? scanLimit : SCAN_LIMIT_DEFAULT) + 1);\n+    if (scanStart != null) {\n+      metaScan.withStartRow(scanStart, false);\n+    }\n+    final Filter filter = buildScanFilter();\n+    if (filter != null) {\n+      metaScan.setFilter(filter);\n+    }\n+    return metaScan;\n+  }\n+\n+  /**\n+   * Adds {@code value} to {@code encoder} under {@code paramName} when {@code value} is non-null.\n+   */\n+  private <T> void addParam(final QueryStringEncoder encoder, final String paramName,\n+    final T value) {\n+    if (value != null) {\n+      encoder.addParam(paramName, value.toString());\n+    }\n+  }\n+\n+  private QueryStringEncoder buildFirstPageEncoder() {\n+    final QueryStringEncoder encoder =\n+      new QueryStringEncoder(request.getRequestURI());\n+    addParam(encoder, NAME_PARAM, name);\n+    addParam(encoder, SCAN_LIMIT_PARAM, scanLimit);\n+    addParam(encoder, SCAN_REGION_STATE_PARAM, scanRegionState);\n+    addParam(encoder, SCAN_TABLE_PARAM, scanTable);\n+    return encoder;\n+  }\n+\n+  public String buildFirstPageUrl() {\n+    return buildFirstPageEncoder().toString();\n+  }\n+\n+  public static String buildStartParamFrom(final byte[] lastRow) {", "originalCommit": "fe6b1a02902d2ed3706b0438d79352ad47a01ea9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "18d86a4d85b8a70a36131dd02f9e06753b05cad7", "url": "https://github.com/apache/hbase/commit/18d86a4d85b8a70a36131dd02f9e06753b05cad7", "message": "PR feedback", "committedDate": "2020-01-14T18:53:18Z", "type": "forcePushed"}, {"oid": "f2d794831fa53849c437c27143844a20e338ab92", "url": "https://github.com/apache/hbase/commit/f2d794831fa53849c437c27143844a20e338ab92", "message": "HBASE-23653 Expose content of meta table in web ui\n\nAdds a display of the content of 'hbase:meta' to the Master's\ntable.jsp, when that table is selected. Supports basic pagination,\nfiltering, &c.", "committedDate": "2020-01-15T17:37:11Z", "type": "forcePushed"}, {"oid": "a62c8f51c4f84a4eacd1246244ae9e544ed94526", "url": "https://github.com/apache/hbase/commit/a62c8f51c4f84a4eacd1246244ae9e544ed94526", "message": "HBASE-23653 Expose content of meta table in web ui\n\nAdds a display of the content of 'hbase:meta' to the Master's\ntable.jsp, when that table is selected. Supports basic pagination,\nfiltering, &c.", "committedDate": "2020-01-15T17:47:49Z", "type": "commit"}, {"oid": "a62c8f51c4f84a4eacd1246244ae9e544ed94526", "url": "https://github.com/apache/hbase/commit/a62c8f51c4f84a4eacd1246244ae9e544ed94526", "message": "HBASE-23653 Expose content of meta table in web ui\n\nAdds a display of the content of 'hbase:meta' to the Master's\ntable.jsp, when that table is selected. Supports basic pagination,\nfiltering, &c.", "committedDate": "2020-01-15T17:47:49Z", "type": "forcePushed"}]}