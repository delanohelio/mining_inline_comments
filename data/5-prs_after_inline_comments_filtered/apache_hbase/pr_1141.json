{"pr_number": 1141, "pr_title": "HBASE-23808 [Flakey Test] TestMasterShutdown#testMasterShutdownBefore\u2026", "pr_createdAt": "2020-02-07T01:44:19Z", "pr_url": "https://github.com/apache/hbase/pull/1141", "timeline": [{"oid": "c184bf695e50f2260b7e59c17eb4930646ffde5f", "url": "https://github.com/apache/hbase/commit/c184bf695e50f2260b7e59c17eb4930646ffde5f", "message": "HBASE-23808 [Flakey Test] TestMasterShutdown#testMasterShutdownBeforeStartingAnyRegionServer\n\nBe a bit more dogmatic about terminating the minicluster between test\nmethods. I doubt this resolves the root issue, but we'll see.", "committedDate": "2020-02-07T01:45:37Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5NjYwOA==", "url": "https://github.com/apache/hbase/pull/1141#discussion_r376196608", "bodyText": "nit: Can probably be simplified to masterThreads != null && masterThreads.size() >=3...\nbecause the second check automatically means isNotEmpty().", "author": "bharathv", "createdAt": "2020-02-07T03:16:41Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -66,43 +78,45 @@ public void testMasterShutdown() throws Exception {\n     Configuration conf = HBaseConfiguration.create();\n \n     // Start the cluster\n-    HBaseTestingUtility htu = new HBaseTestingUtility(conf);\n-    StartMiniClusterOption option = StartMiniClusterOption.builder()\n-        .numMasters(NUM_MASTERS).numRegionServers(NUM_RS).numDataNodes(NUM_RS).build();\n-    htu.startMiniCluster(option);\n-    MiniHBaseCluster cluster = htu.getHBaseCluster();\n-\n-    // get all the master threads\n-    List<MasterThread> masterThreads = cluster.getMasterThreads();\n-\n-    // wait for each to come online\n-    for (MasterThread mt : masterThreads) {\n-      assertTrue(mt.isAlive());\n-    }\n-\n-    // find the active master\n-    HMaster active = null;\n-    for (int i = 0; i < masterThreads.size(); i++) {\n-      if (masterThreads.get(i).getMaster().isActiveMaster()) {\n-        active = masterThreads.get(i).getMaster();\n-        break;\n+    try {\n+      htu = new HBaseTestingUtility(conf);\n+      StartMiniClusterOption option = StartMiniClusterOption.builder()\n+        .numMasters(NUM_MASTERS)\n+        .numRegionServers(NUM_RS)\n+        .numDataNodes(NUM_RS)\n+        .build();\n+      final MiniHBaseCluster cluster = htu.startMiniCluster(option);\n+\n+      // wait for all master thread to spawn and start their run loop.\n+      final long thirtySeconds = TimeUnit.SECONDS.toMillis(30);\n+      final long oneSecond = TimeUnit.SECONDS.toMillis(1);\n+      assertNotEquals(-1, htu.waitFor(thirtySeconds, oneSecond, () -> {\n+        final List<MasterThread> masterThreads = cluster.getMasterThreads();\n+        return CollectionUtils.isNotEmpty(masterThreads)", "originalCommit": "c184bf695e50f2260b7e59c17eb4930646ffde5f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5OTI5OA==", "url": "https://github.com/apache/hbase/pull/1141#discussion_r376199298", "bodyText": "nit: You could just use shutdownMiniCluster(). It appears null safe on underlying minicluster", "author": "bharathv", "createdAt": "2020-02-07T03:29:17Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -128,41 +142,50 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n     conf.setInt(ServerManager.WAIT_ON_REGIONSERVERS_MINTOSTART, 1);\n \n     // Start the cluster\n-    final HBaseTestingUtility util = new HBaseTestingUtility(conf);\n-    util.startMiniDFSCluster(3);\n-    util.startMiniZKCluster();\n-    util.createRootDir();\n-    final LocalHBaseCluster cluster =\n-        new LocalHBaseCluster(conf, NUM_MASTERS, NUM_RS, HMaster.class,\n-            MiniHBaseCluster.MiniHBaseClusterRegionServer.class);\n-    final int MASTER_INDEX = 0;\n-    final MasterThread master = cluster.getMasters().get(MASTER_INDEX);\n-    master.start();\n-    LOG.info(\"Called master start on \" + master.getName());\n-    Thread shutdownThread = new Thread(\"Shutdown-Thread\") {\n-      @Override\n-      public void run() {\n-        LOG.info(\"Before call to shutdown master\");\n-        try (Connection connection = createConnection(util); Admin admin = connection.getAdmin()) {\n-          admin.shutdown();\n-        } catch (Exception e) {\n-          LOG.info(\"Error while calling Admin.shutdown, which is expected: \" + e.getMessage());\n+    LocalHBaseCluster cluster = null;\n+    try {\n+      htu = new HBaseTestingUtility(conf);\n+      htu.startMiniDFSCluster(3);\n+      htu.startMiniZKCluster();\n+      htu.createRootDir();\n+      cluster = new LocalHBaseCluster(conf, NUM_MASTERS, NUM_RS, HMaster.class,\n+        MiniHBaseCluster.MiniHBaseClusterRegionServer.class);\n+      final int MASTER_INDEX = 0;\n+      final MasterThread master = cluster.getMasters().get(MASTER_INDEX);\n+      master.start();\n+      LOG.info(\"Called master start on \" + master.getName());\n+      final LocalHBaseCluster finalCluster = cluster;\n+      Thread shutdownThread = new Thread(\"Shutdown-Thread\") {\n+        @Override\n+        public void run() {\n+          LOG.info(\"Before call to shutdown master\");\n+          try (Connection connection = createConnection(htu); Admin admin = connection.getAdmin()) {\n+            admin.shutdown();\n+          } catch (Exception e) {\n+            LOG.info(\"Error while calling Admin.shutdown, which is expected: \" + e.getMessage());\n+          }\n+          LOG.info(\"After call to shutdown master\");\n+          finalCluster.waitOnMaster(MASTER_INDEX);\n         }\n-        LOG.info(\"After call to shutdown master\");\n-        cluster.waitOnMaster(MASTER_INDEX);\n+      };\n+      shutdownThread.start();\n+      LOG.info(\"Called master join on \" + master.getName());\n+      master.join();\n+      shutdownThread.join();\n+\n+      List<MasterThread> masterThreads = cluster.getMasters();\n+      // make sure all the masters properly shutdown\n+      assertEquals(0, masterThreads.size());\n+    } finally {\n+      if (cluster != null) {\n+        cluster.shutdown();\n+      }\n+      if (htu != null) {\n+        htu.shutdownMiniZKCluster();", "originalCommit": "c184bf695e50f2260b7e59c17eb4930646ffde5f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc3ODE2OQ==", "url": "https://github.com/apache/hbase/pull/1141#discussion_r377778169", "bodyText": "nod", "author": "ndimiduk", "createdAt": "2020-02-11T17:15:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5OTI5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5OTQzOA==", "url": "https://github.com/apache/hbase/pull/1141#discussion_r376199438", "bodyText": "This is the place that the master registry exposed a race ..(shutdown goes missing..). Rebase will not be clean now :'(", "author": "bharathv", "createdAt": "2020-02-07T03:30:07Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -128,41 +142,50 @@ public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n     conf.setInt(ServerManager.WAIT_ON_REGIONSERVERS_MINTOSTART, 1);\n \n     // Start the cluster\n-    final HBaseTestingUtility util = new HBaseTestingUtility(conf);\n-    util.startMiniDFSCluster(3);\n-    util.startMiniZKCluster();\n-    util.createRootDir();\n-    final LocalHBaseCluster cluster =\n-        new LocalHBaseCluster(conf, NUM_MASTERS, NUM_RS, HMaster.class,\n-            MiniHBaseCluster.MiniHBaseClusterRegionServer.class);\n-    final int MASTER_INDEX = 0;\n-    final MasterThread master = cluster.getMasters().get(MASTER_INDEX);\n-    master.start();\n-    LOG.info(\"Called master start on \" + master.getName());\n-    Thread shutdownThread = new Thread(\"Shutdown-Thread\") {\n-      @Override\n-      public void run() {\n-        LOG.info(\"Before call to shutdown master\");\n-        try (Connection connection = createConnection(util); Admin admin = connection.getAdmin()) {\n-          admin.shutdown();\n-        } catch (Exception e) {\n-          LOG.info(\"Error while calling Admin.shutdown, which is expected: \" + e.getMessage());\n+    LocalHBaseCluster cluster = null;\n+    try {\n+      htu = new HBaseTestingUtility(conf);\n+      htu.startMiniDFSCluster(3);\n+      htu.startMiniZKCluster();\n+      htu.createRootDir();\n+      cluster = new LocalHBaseCluster(conf, NUM_MASTERS, NUM_RS, HMaster.class,\n+        MiniHBaseCluster.MiniHBaseClusterRegionServer.class);\n+      final int MASTER_INDEX = 0;\n+      final MasterThread master = cluster.getMasters().get(MASTER_INDEX);\n+      master.start();\n+      LOG.info(\"Called master start on \" + master.getName());\n+      final LocalHBaseCluster finalCluster = cluster;\n+      Thread shutdownThread = new Thread(\"Shutdown-Thread\") {\n+        @Override\n+        public void run() {\n+          LOG.info(\"Before call to shutdown master\");\n+          try (Connection connection = createConnection(htu); Admin admin = connection.getAdmin()) {\n+            admin.shutdown();", "originalCommit": "c184bf695e50f2260b7e59c17eb4930646ffde5f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ4MTkxMg==", "url": "https://github.com/apache/hbase/pull/1141#discussion_r376481912", "bodyText": "I think this is the source of the instability I've seen in this test, but I cannot reproduce it reliably.\nYou see this test failing reliably on your branch?", "author": "ndimiduk", "createdAt": "2020-02-07T16:23:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5OTQzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUzNjkwOA==", "url": "https://github.com/apache/hbase/pull/1141#discussion_r376536908", "bodyText": "No, not after adding this check \n  \n    \n      hbase/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java\n    \n    \n         Line 141\n      in\n      d110c08\n    \n    \n    \n    \n\n        \n          \n           // Switching to master registry exposed a race in the master bootstrap that can result in a \n        \n    \n  \n\n\nThis check basically works around the actual problem without fixing it. Ideally you could do the same because that check is anyway applied in the master after the branch merge.\nFwiw, I think this race is exposed after committing HBASE-23764, because that speeds up the connections.", "author": "bharathv", "createdAt": "2020-02-07T18:22:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5OTQzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM5NDU0NA==", "url": "https://github.com/apache/hbase/pull/1141#discussion_r377394544", "bodyText": "I re-read your changes on the branch. Let me try to merge those with the general cleanup I'm attempting here. Hopefully when it's done, you can just drop the change to this test from the feature branch.", "author": "ndimiduk", "createdAt": "2020-02-11T00:23:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE5OTQzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjYyNTY4NA==", "url": "https://github.com/apache/hbase/pull/1141#discussion_r376625684", "bodyText": "For sure? Maybe medium when you run it standalone but under load, maybe it goes over the 50s upper-bound (only saying because did a pass on these recently).", "author": "saintstack", "createdAt": "2020-02-07T21:54:40Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -33,23 +34,34 @@\n import org.apache.hadoop.hbase.client.Admin;\n import org.apache.hadoop.hbase.client.Connection;\n import org.apache.hadoop.hbase.client.ConnectionFactory;\n-import org.apache.hadoop.hbase.testclassification.LargeTests;\n import org.apache.hadoop.hbase.testclassification.MasterTests;\n+import org.apache.hadoop.hbase.testclassification.MediumTests;\n import org.apache.hadoop.hbase.util.JVMClusterUtil.MasterThread;\n+import org.junit.Before;\n import org.junit.ClassRule;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;\n \n-@Category({MasterTests.class, LargeTests.class})\n+@Category({MasterTests.class, MediumTests.class})", "originalCommit": "c184bf695e50f2260b7e59c17eb4930646ffde5f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM5MTc2OA==", "url": "https://github.com/apache/hbase/pull/1141#discussion_r377391768", "bodyText": "Seems to be snappy for me... What's the idea behind these annotations? They're supposed to indicate the local dev experience, or the overworked Jenkins experience?", "author": "ndimiduk", "createdAt": "2020-02-11T00:13:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjYyNTY4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIyMjQzOA==", "url": "https://github.com/apache/hbase/pull/1141#discussion_r377222438", "bodyText": "You don't want to get a new conf to go w/ the new htu?", "author": "saintstack", "createdAt": "2020-02-10T17:56:34Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -66,43 +78,45 @@ public void testMasterShutdown() throws Exception {\n     Configuration conf = HBaseConfiguration.create();\n \n     // Start the cluster\n-    HBaseTestingUtility htu = new HBaseTestingUtility(conf);\n-    StartMiniClusterOption option = StartMiniClusterOption.builder()\n-        .numMasters(NUM_MASTERS).numRegionServers(NUM_RS).numDataNodes(NUM_RS).build();\n-    htu.startMiniCluster(option);\n-    MiniHBaseCluster cluster = htu.getHBaseCluster();\n-\n-    // get all the master threads\n-    List<MasterThread> masterThreads = cluster.getMasterThreads();\n-\n-    // wait for each to come online\n-    for (MasterThread mt : masterThreads) {\n-      assertTrue(mt.isAlive());\n-    }\n-\n-    // find the active master\n-    HMaster active = null;\n-    for (int i = 0; i < masterThreads.size(); i++) {\n-      if (masterThreads.get(i).getMaster().isActiveMaster()) {\n-        active = masterThreads.get(i).getMaster();\n-        break;\n+    try {\n+      htu = new HBaseTestingUtility(conf);", "originalCommit": "c184bf695e50f2260b7e59c17eb4930646ffde5f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM5MTkwNg==", "url": "https://github.com/apache/hbase/pull/1141#discussion_r377391906", "bodyText": "This is a new conf, created above... Let me move them in here after htu is created but before cluster start. I'm not clear on why hbase.ipc.client.failed.servers.expiry is set so aggressively.", "author": "ndimiduk", "createdAt": "2020-02-11T00:13:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIyMjQzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAzNjI3OA==", "url": "https://github.com/apache/hbase/pull/1141#discussion_r378036278", "bodyText": "On hbase.ipc.client.failed.servers.expiry being aggressive... remove the config?", "author": "saintstack", "createdAt": "2020-02-12T04:32:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIyMjQzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODUwMzE2Mg==", "url": "https://github.com/apache/hbase/pull/1141#discussion_r378503162", "bodyText": "yeah, removed.", "author": "ndimiduk", "createdAt": "2020-02-12T20:46:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIyMjQzOA=="}], "type": "inlineReview"}, {"oid": "c699e2a09ec5b0c345cb892a55236276ae607275", "url": "https://github.com/apache/hbase/commit/c699e2a09ec5b0c345cb892a55236276ae607275", "message": "PR Feedback and further test development", "committedDate": "2020-02-11T17:14:19Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzk4ODMxNA==", "url": "https://github.com/apache/hbase/pull/1141#discussion_r377988314", "bodyText": "Bah. need to delete this commented code.", "author": "ndimiduk", "createdAt": "2020-02-12T00:59:14Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -66,103 +82,172 @@ public void testMasterShutdown() throws Exception {\n     Configuration conf = HBaseConfiguration.create();\n \n     // Start the cluster\n-    HBaseTestingUtility htu = new HBaseTestingUtility(conf);\n-    StartMiniClusterOption option = StartMiniClusterOption.builder()\n-        .numMasters(NUM_MASTERS).numRegionServers(NUM_RS).numDataNodes(NUM_RS).build();\n-    htu.startMiniCluster(option);\n-    MiniHBaseCluster cluster = htu.getHBaseCluster();\n-\n-    // get all the master threads\n-    List<MasterThread> masterThreads = cluster.getMasterThreads();\n-\n-    // wait for each to come online\n-    for (MasterThread mt : masterThreads) {\n-      assertTrue(mt.isAlive());\n-    }\n-\n-    // find the active master\n-    HMaster active = null;\n-    for (int i = 0; i < masterThreads.size(); i++) {\n-      if (masterThreads.get(i).getMaster().isActiveMaster()) {\n-        active = masterThreads.get(i).getMaster();\n-        break;\n+    try {\n+      htu = new HBaseTestingUtility(conf);\n+      StartMiniClusterOption option = StartMiniClusterOption.builder()\n+        .numMasters(NUM_MASTERS)\n+        .numRegionServers(NUM_RS)\n+        .numDataNodes(NUM_RS)\n+        .build();\n+      final MiniHBaseCluster cluster = htu.startMiniCluster(option);\n+\n+      // wait for all master thread to spawn and start their run loop.\n+      final long thirtySeconds = TimeUnit.SECONDS.toMillis(30);\n+      final long oneSecond = TimeUnit.SECONDS.toMillis(1);\n+      assertNotEquals(-1, htu.waitFor(thirtySeconds, oneSecond, () -> {\n+        final List<MasterThread> masterThreads = cluster.getMasterThreads();\n+        return masterThreads != null\n+          && masterThreads.size() >= 3\n+          && masterThreads.stream().allMatch(Thread::isAlive);\n+      }));\n+\n+      // find the active master\n+      final HMaster active = cluster.getMaster();\n+      assertNotNull(active);\n+\n+      // make sure the other two are backup masters\n+      ClusterMetrics status = active.getClusterMetrics();\n+      assertEquals(2, status.getBackupMasterNames().size());\n+\n+      // tell the active master to shutdown the cluster\n+      active.shutdown();\n+      assertNotEquals(-1, htu.waitFor(thirtySeconds, oneSecond,\n+        () -> CollectionUtils.isEmpty(cluster.getLiveMasterThreads())));\n+      assertNotEquals(-1, htu.waitFor(thirtySeconds, oneSecond,\n+        () -> CollectionUtils.isEmpty(cluster.getLiveRegionServerThreads())));\n+    } finally {\n+      if (htu != null) {\n+        htu.shutdownMiniCluster();\n+        htu = null;\n       }\n     }\n-    assertNotNull(active);\n-    // make sure the other two are backup masters\n-    ClusterMetrics status = active.getClusterMetrics();\n-    assertEquals(2, status.getBackupMasterNames().size());\n-\n-    // tell the active master to shutdown the cluster\n-    active.shutdown();\n-\n-    for (int i = NUM_MASTERS - 1; i >= 0 ;--i) {\n-      cluster.waitOnMaster(i);\n-    }\n-    // make sure all the masters properly shutdown\n-    assertEquals(0, masterThreads.size());\n-\n-    htu.shutdownMiniCluster();\n   }\n \n-  private Connection createConnection(HBaseTestingUtility util) throws InterruptedException {\n-    // the cluster may have not been initialized yet which means we can not get the cluster id thus\n-    // an exception will be thrown. So here we need to retry.\n-    for (;;) {\n-      try {\n-        return ConnectionFactory.createConnection(util.getConfiguration());\n-      } catch (Exception e) {\n-        Thread.sleep(10);\n+  /**\n+   * This test appears to be an intentional race between a thread that issues a shutdown RPC to the\n+   * master, while the master is concurrently realizing it cannot initialize because there are no\n+   * region servers available to it. The expected behavior is that master initialization is\n+   * interruptable via said shutdown RPC.\n+   */\n+  @Test\n+  public void testMasterShutdownBeforeStartingAnyRegionServer() throws Exception {\n+    LocalHBaseCluster hbaseCluster = null;\n+    try {\n+      htu =  new HBaseTestingUtility(\n+        createMasterShutdownBeforeStartingAnyRegionServerConfiguration());\n+\n+      // configure a cluster with\n+      final StartMiniClusterOption options = StartMiniClusterOption.builder()\n+        .numDataNodes(1)\n+        .numMasters(1)\n+        .numRegionServers(0)\n+        .masterClass(HMaster.class)\n+        .rsClass(MiniHBaseCluster.MiniHBaseClusterRegionServer.class)\n+        .createRootDir(true)\n+        .build();\n+\n+      // Can't simply `htu.startMiniCluster(options)` because that method waits for the master to\n+      // start completely. However, this test's premise is that a partially started master should\n+      // still respond to a shutdown RPC. So instead, we manage each component lifecycle\n+      // independently.\n+      // I think it's not worth refactoring HTU's helper methods just for this class.\n+      htu.startMiniDFSCluster(options.getNumDataNodes());\n+      htu.startMiniZKCluster(options.getNumZkServers());\n+      htu.createRootDir();\n+      hbaseCluster = new LocalHBaseCluster(htu.getConfiguration(), options.getNumMasters(),\n+        options.getNumRegionServers(), options.getMasterClass(), options.getRsClass());\n+      final MasterThread masterThread = hbaseCluster.getMasters().get(0);\n+\n+      final CompletableFuture<Void> shutdownFuture = CompletableFuture.runAsync(() -> {\n+        // Switching to master registry exacerbated a race in the master bootstrap that can result\n+        // in a lost shutdown command (HBASE-8422). The race is essentially because the server\n+        // manager in HMaster is not initialized by the time shutdown() RPC (below) is made to the\n+        // master. The suspected reason as to why it was uncommon before HBASE-18095 is because the\n+        // connection creation with ZK registry is so slow that by then the server manager is\n+        // usually init'ed in time for the RPC to be made. For now, adding an explicit wait() in\n+        // the test, waiting for the server manager to become available.\n+        final long timeout = TimeUnit.MINUTES.toMillis(10);\n+//        assertNotEquals(\"timeout waiting for server manager to become available.\",", "originalCommit": "11e1cd3ab1c6916ae0878f7309a670ef306fe2a2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzk5MzYxMA==", "url": "https://github.com/apache/hbase/pull/1141#discussion_r377993610", "bodyText": "I think another reasonable alternative here is to reject the RPC back to the client with some form of DoNotRetryIOException and a message about cannot shutdown from an inactive master.\nWhat do you think?", "author": "ndimiduk", "createdAt": "2020-02-12T01:20:20Z", "path": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java", "diffHunk": "@@ -2804,9 +2804,15 @@ public MemoryBoundedLogMessageBuffer getRegionServerFatalLogBuffer() {\n    * Master runs a coordinated stop of all RegionServers and then itself.\n    */\n   public void shutdown() throws IOException {\n+    if (!isInitialized()) {\n+      LOG.info(\"Shutdown requested but we're not the active master. Proceeding as a stop.\");", "originalCommit": "11e1cd3ab1c6916ae0878f7309a670ef306fe2a2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAxODQzNQ==", "url": "https://github.com/apache/hbase/pull/1141#discussion_r378018435", "bodyText": "Well, I'm not entirely sure if this is the intended way to stop stand-by masters. Reading the code.. it looks like the active master coordinates a proper cluster shutdown. In that process, it removes the /hbase/running znode that the standb-bys keep a watch on. See the following code in ActiveMasterManager..\n@Override\n  public void nodeDeleted(String path) {\n\n    // We need to keep track of the cluster's shutdown status while\n    // we wait on the current master. We consider that, if the cluster\n    // was already in a \"shutdown\" state when we started, that this master\n    // is part of a new cluster that was started shortly after the old cluster\n    // shut down, so that state is now irrelevant. This means that the shutdown\n    // state must be set while we wait on the active master in order\n    // to shutdown this master. See HBASE-8519.\n    if(path.equals(watcher.getZNodePaths().clusterStateZNode) && !master.isStopped()) {\n      clusterShutDown.set(true);\n    }\n\n\nIdeally they should shut themselves down if the ZK event notifications happen as expected. Is that not the case?", "author": "bharathv", "createdAt": "2020-02-12T03:04:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzk5MzYxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAzNTEyOQ==", "url": "https://github.com/apache/hbase/pull/1141#discussion_r378035129", "bodyText": "Yeah, this stuff is tricky and there be dragons if you disturb the current order.\nThe standbys don't go down currently?", "author": "saintstack", "createdAt": "2020-02-12T04:26:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzk5MzYxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODUxMDE2Mw==", "url": "https://github.com/apache/hbase/pull/1141#discussion_r378510163", "bodyText": "@bharathv has the gist of it. At the point of this race condition -- all four of these fields are null at the time the rpc is received -- the master will simply do nothing. However, any master (active or backup) can currently receive the rpc and if it's clusterStatusTracker is non-null, it will delete this ZK node. From there, in the case of a backup master, the ActiveMasterManager will notice and stop itself.\nRelated, looks like there's an early-out in ServerManager#shutdown that can result in a master stopping without properly shutting down its procedure store.\n    if (onlineServers.isEmpty()) {\n      // we do not synchronize here so this may cause a double stop, but not a big deal\n      master.stop(\"OnlineServer=0 right after cluster shutdown set\");\n    }", "author": "ndimiduk", "createdAt": "2020-02-12T21:01:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzk5MzYxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAzNTQyMg==", "url": "https://github.com/apache/hbase/pull/1141#discussion_r378035422", "bodyText": "We can skip an @after ? i.e. should the shutdown of cluster be in @after if not there already.", "author": "saintstack", "createdAt": "2020-02-12T04:27:48Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -30,26 +34,38 @@\n import org.apache.hadoop.hbase.LocalHBaseCluster;\n import org.apache.hadoop.hbase.MiniHBaseCluster;\n import org.apache.hadoop.hbase.StartMiniClusterOption;\n-import org.apache.hadoop.hbase.client.Admin;\n-import org.apache.hadoop.hbase.client.Connection;\n+import org.apache.hadoop.hbase.Waiter;\n+import org.apache.hadoop.hbase.client.AsyncConnection;\n import org.apache.hadoop.hbase.client.ConnectionFactory;\n import org.apache.hadoop.hbase.testclassification.LargeTests;\n import org.apache.hadoop.hbase.testclassification.MasterTests;\n import org.apache.hadoop.hbase.util.JVMClusterUtil.MasterThread;\n+import org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient;\n+import org.junit.Before;\n import org.junit.ClassRule;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;\n \n @Category({MasterTests.class, LargeTests.class})\n public class TestMasterShutdown {\n+  private static final Logger LOG = LoggerFactory.getLogger(TestMasterShutdown.class);\n \n   @ClassRule\n   public static final HBaseClassTestRule CLASS_RULE =\n       HBaseClassTestRule.forClass(TestMasterShutdown.class);\n \n-  private static final Logger LOG = LoggerFactory.getLogger(TestMasterShutdown.class);\n+  private HBaseTestingUtility htu;\n+\n+  @Before\n+  public void shutdownCluster() throws IOException {\n+    if (htu != null) {", "originalCommit": "11e1cd3ab1c6916ae0878f7309a670ef306fe2a2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODUwMDgwMg==", "url": "https://github.com/apache/hbase/pull/1141#discussion_r378500802", "bodyText": "I'm not exactly sure what the junit contract is on executing finally clauses and after method in the face of timeouts, as implemented with our classifier rule. I was being extra-special redundant with this cleanup. Probably one or the other is fine. Let me see if i can understand what junit does.", "author": "ndimiduk", "createdAt": "2020-02-12T20:41:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAzNTQyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTA0OTY4OA==", "url": "https://github.com/apache/hbase/pull/1141#discussion_r379049688", "bodyText": "I tried writing a test for HBaseClassTestRule  and the Timeout class it uses to assert the behavior of a miniCluster in the face of the interrupt, but that took me round in circles. Going to pass on that for the time being.", "author": "ndimiduk", "createdAt": "2020-02-13T18:46:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAzNTQyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAzNjQ5Ng==", "url": "https://github.com/apache/hbase/pull/1141#discussion_r378036496", "bodyText": "Somehow we are skipping this? Odd. Move to an @after?", "author": "saintstack", "createdAt": "2020-02-12T04:33:51Z", "path": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java", "diffHunk": "@@ -66,103 +82,172 @@ public void testMasterShutdown() throws Exception {\n     Configuration conf = HBaseConfiguration.create();\n \n     // Start the cluster\n-    HBaseTestingUtility htu = new HBaseTestingUtility(conf);\n-    StartMiniClusterOption option = StartMiniClusterOption.builder()\n-        .numMasters(NUM_MASTERS).numRegionServers(NUM_RS).numDataNodes(NUM_RS).build();\n-    htu.startMiniCluster(option);\n-    MiniHBaseCluster cluster = htu.getHBaseCluster();\n-\n-    // get all the master threads\n-    List<MasterThread> masterThreads = cluster.getMasterThreads();\n-\n-    // wait for each to come online\n-    for (MasterThread mt : masterThreads) {\n-      assertTrue(mt.isAlive());\n-    }\n-\n-    // find the active master\n-    HMaster active = null;\n-    for (int i = 0; i < masterThreads.size(); i++) {\n-      if (masterThreads.get(i).getMaster().isActiveMaster()) {\n-        active = masterThreads.get(i).getMaster();\n-        break;\n+    try {\n+      htu = new HBaseTestingUtility(conf);\n+      StartMiniClusterOption option = StartMiniClusterOption.builder()\n+        .numMasters(NUM_MASTERS)\n+        .numRegionServers(NUM_RS)\n+        .numDataNodes(NUM_RS)\n+        .build();\n+      final MiniHBaseCluster cluster = htu.startMiniCluster(option);\n+\n+      // wait for all master thread to spawn and start their run loop.\n+      final long thirtySeconds = TimeUnit.SECONDS.toMillis(30);\n+      final long oneSecond = TimeUnit.SECONDS.toMillis(1);\n+      assertNotEquals(-1, htu.waitFor(thirtySeconds, oneSecond, () -> {\n+        final List<MasterThread> masterThreads = cluster.getMasterThreads();\n+        return masterThreads != null\n+          && masterThreads.size() >= 3\n+          && masterThreads.stream().allMatch(Thread::isAlive);\n+      }));\n+\n+      // find the active master\n+      final HMaster active = cluster.getMaster();\n+      assertNotNull(active);\n+\n+      // make sure the other two are backup masters\n+      ClusterMetrics status = active.getClusterMetrics();\n+      assertEquals(2, status.getBackupMasterNames().size());\n+\n+      // tell the active master to shutdown the cluster\n+      active.shutdown();\n+      assertNotEquals(-1, htu.waitFor(thirtySeconds, oneSecond,\n+        () -> CollectionUtils.isEmpty(cluster.getLiveMasterThreads())));\n+      assertNotEquals(-1, htu.waitFor(thirtySeconds, oneSecond,\n+        () -> CollectionUtils.isEmpty(cluster.getLiveRegionServerThreads())));\n+    } finally {\n+      if (htu != null) {", "originalCommit": "11e1cd3ab1c6916ae0878f7309a670ef306fe2a2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "4ac802f2be3075c92454375b3e2e1da1d90e2de6", "url": "https://github.com/apache/hbase/commit/4ac802f2be3075c92454375b3e2e1da1d90e2de6", "message": "HBASE-23808 [Flakey Test] TestMasterShutdown#testMasterShutdownBeforeStartingAnyRegionServer\n\nBe a bit more dogmatic about terminating the minicluster between test\nmethods. I doubt this resolves the root issue, but we'll see.", "committedDate": "2020-02-13T16:18:54Z", "type": "commit"}, {"oid": "f1e248ac2dba6773c89bd3cec269e41a630a92d5", "url": "https://github.com/apache/hbase/commit/f1e248ac2dba6773c89bd3cec269e41a630a92d5", "message": "PR Feedback and further test development", "committedDate": "2020-02-13T16:18:54Z", "type": "commit"}, {"oid": "9845e5e468ccb32f5b39195ec966ab14ed02186b", "url": "https://github.com/apache/hbase/commit/9845e5e468ccb32f5b39195ec966ab14ed02186b", "message": "When it's not the active master, accept shutdown RPC as a stop instead", "committedDate": "2020-02-13T16:18:54Z", "type": "commit"}, {"oid": "0758e04bbe16083c5bab185ecf52c4947d6dc7e0", "url": "https://github.com/apache/hbase/commit/0758e04bbe16083c5bab185ecf52c4947d6dc7e0", "message": "PR Feedback", "committedDate": "2020-02-13T18:43:55Z", "type": "commit"}, {"oid": "0758e04bbe16083c5bab185ecf52c4947d6dc7e0", "url": "https://github.com/apache/hbase/commit/0758e04bbe16083c5bab185ecf52c4947d6dc7e0", "message": "PR Feedback", "committedDate": "2020-02-13T18:43:55Z", "type": "forcePushed"}, {"oid": "269588ba341b3603a2ed7ee6a98103f419179010", "url": "https://github.com/apache/hbase/commit/269588ba341b3603a2ed7ee6a98103f419179010", "message": "Actually waitFor master", "committedDate": "2020-02-13T19:01:58Z", "type": "commit"}]}