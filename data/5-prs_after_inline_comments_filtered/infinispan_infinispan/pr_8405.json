{"pr_number": 8405, "pr_title": "ISPN-11930 Rocks db converted to new SPI", "pr_createdAt": "2020-05-29T05:22:23Z", "pr_url": "https://github.com/infinispan/infinispan/pull/8405", "timeline": [{"oid": "be24dc2231fa8baed421fe4a622b5347481a67a9", "url": "https://github.com/infinispan/infinispan/commit/be24dc2231fa8baed421fe4a622b5347481a67a9", "message": "ISPN-11931 Add blockhound to rocksdb module", "committedDate": "2020-05-29T18:17:00Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDYxNDYxMw==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r434614613", "bodyText": "ContextBuilder as PersistenceMockUtil is not exclusive to InvocationContext?", "author": "ryanemerson", "createdAt": "2020-06-03T14:33:34Z", "path": "core/src/test/java/org/infinispan/util/PersistenceMockUtil.java", "diffHunk": "@@ -41,6 +47,51 @@\n  */\n public class PersistenceMockUtil {\n \n+   public static class Builder {", "originalCommit": "be24dc2231fa8baed421fe4a622b5347481a67a9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg0MTE3MQ==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r434841171", "bodyText": "Sorry, I am not sure what you mean here. Are you proposing I move the Builder class somewhere else?", "author": "wburns", "createdAt": "2020-06-03T20:42:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDYxNDYxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTEwNTI3MA==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r435105270", "bodyText": "Sorry, my message isn't clear in hindsight. I meant shall we rename to class ContextBuilder as the PersistenceMockUtil class is not exclusively used for creating/modifying InvocationContext instances. The name Builder is fine as an inner class that creates an instance of the parent class, e.g. DataFormat.Builder, however that is not the case with  PersistenceMockUtil.Builder. So my thoughts were that renaming the class to PersistenceMockUtil.ContextBuilder or even PersistenceMockUtil.InvocationContextBuilder makes it's purpose more explicit.", "author": "ryanemerson", "createdAt": "2020-06-04T09:07:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDYxNDYxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI1MTE2Ng==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r435251166", "bodyText": "Sounds good. I guess because the class only creates an InvocationContext currently I hadn't thought about the name, but I agree.", "author": "wburns", "createdAt": "2020-06-04T13:26:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDYxNDYxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDYxNTczMw==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r434615733", "bodyText": "impl package?", "author": "ryanemerson", "createdAt": "2020-06-03T14:34:57Z", "path": "persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/internal/RocksDBBlockHoundIntegration.java", "diffHunk": "@@ -0,0 +1,20 @@\n+package org.infinispan.persistence.rocksdb.internal;", "originalCommit": "be24dc2231fa8baed421fe4a622b5347481a67a9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg0MjAxMQ==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r434842011", "bodyText": "I personally find it being in a package like internal a bit better as it really isn't an implementation class of the public interfaces. I have just repurposed impl in some modules since it is private. WDYT?", "author": "wburns", "createdAt": "2020-06-03T20:43:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDYxNTczMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTEwMjU4OA==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r435102588", "bodyText": "That's a good point. I hadn't realised that we already used .internal. packages in some places, so let's stick with that.", "author": "ryanemerson", "createdAt": "2020-06-04T09:03:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDYxNTczMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDYyOTc1Ng==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r434629756", "bodyText": "Missing @ConfiguredBy(RocksDBStoreConfiguration.class)", "author": "ryanemerson", "createdAt": "2020-06-03T14:54:02Z", "path": "persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java", "diffHunk": "@@ -67,1091 +67,933 @@\n import org.rocksdb.WriteOptions;\n \n import io.reactivex.rxjava3.core.Flowable;\n-\n-@Store\n-@ConfiguredBy(RocksDBStoreConfiguration.class)\n-public class RocksDBStore<K,V> implements SegmentedAdvancedLoadWriteStore<K,V> {\n-    private static final Log log = LogFactory.getLog(RocksDBStore.class, Log.class);\n-    static final String DATABASE_PROPERTY_NAME_WITH_SUFFIX = \"database.\";\n-    static final String COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX = \"data.\";\n-\n-    protected RocksDBStoreConfiguration configuration;\n-    private RocksDB db;\n-    private RocksDB expiredDb;\n-    private InitializationContext ctx;\n-    private TimeService timeService;\n-    private Semaphore semaphore;\n-    private WriteOptions dataWriteOptions;\n-    private RocksDBHandler handler;\n-    private Properties databaseProperties;\n-    private Properties columnFamilyProperties;\n-    private Marshaller marshaller;\n-    private MarshallableEntryFactory<K, V> entryFactory;\n-    private volatile boolean stopped = true;\n-\n-    @Override\n-    public void init(InitializationContext ctx) {\n-        this.configuration = ctx.getConfiguration();\n-        this.ctx = ctx;\n-        this.timeService = ctx.getTimeService();\n-        this.marshaller = ctx.getPersistenceMarshaller();\n-        this.semaphore = new Semaphore(Integer.MAX_VALUE, true);\n-        this.entryFactory = ctx.getMarshallableEntryFactory();\n-        ctx.getPersistenceMarshaller().register(new PersistenceContextInitializerImpl());\n-    }\n-\n-    @Override\n-    public void start() {\n-\n-        AdvancedCache cache = ctx.getCache().getAdvancedCache();\n-        KeyPartitioner keyPartitioner = cache.getComponentRegistry().getComponent(KeyPartitioner.class);\n-        if (configuration.segmented()) {\n-            handler = new SegmentedRocksDBHandler(cache.getCacheConfiguration().clustering().hash().numSegments(),\n-                  keyPartitioner);\n-        } else {\n-            handler = new NonSegmentedRocksDBHandler(keyPartitioner);\n-        }\n-\n-        // Has to be done before we open the database, so we can pass the properties\n-        Properties allProperties = configuration.properties();\n-        for (Map.Entry<Object, Object> entry : allProperties.entrySet()) {\n-            String key = entry.getKey().toString();\n-            if (key.startsWith(DATABASE_PROPERTY_NAME_WITH_SUFFIX)) {\n-                if (databaseProperties == null) {\n-                    databaseProperties = new Properties();\n-                }\n-                databaseProperties.setProperty(key.substring(DATABASE_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n-            } else if (key.startsWith(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX)) {\n-                if (columnFamilyProperties == null) {\n-                    columnFamilyProperties = new Properties();\n-                }\n-                columnFamilyProperties.setProperty(key.substring(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n-            }\n-        }\n-\n-        try {\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+\n+public class RocksDBStore<K, V> implements NonBlockingStore<K, V> {", "originalCommit": "be24dc2231fa8baed421fe4a622b5347481a67a9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY0OTU4OQ==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r434649589", "bodyText": "Never called.", "author": "ryanemerson", "createdAt": "2020-06-03T15:20:41Z", "path": "persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java", "diffHunk": "@@ -67,1091 +67,933 @@\n import org.rocksdb.WriteOptions;\n \n import io.reactivex.rxjava3.core.Flowable;\n-\n-@Store\n-@ConfiguredBy(RocksDBStoreConfiguration.class)\n-public class RocksDBStore<K,V> implements SegmentedAdvancedLoadWriteStore<K,V> {\n-    private static final Log log = LogFactory.getLog(RocksDBStore.class, Log.class);\n-    static final String DATABASE_PROPERTY_NAME_WITH_SUFFIX = \"database.\";\n-    static final String COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX = \"data.\";\n-\n-    protected RocksDBStoreConfiguration configuration;\n-    private RocksDB db;\n-    private RocksDB expiredDb;\n-    private InitializationContext ctx;\n-    private TimeService timeService;\n-    private Semaphore semaphore;\n-    private WriteOptions dataWriteOptions;\n-    private RocksDBHandler handler;\n-    private Properties databaseProperties;\n-    private Properties columnFamilyProperties;\n-    private Marshaller marshaller;\n-    private MarshallableEntryFactory<K, V> entryFactory;\n-    private volatile boolean stopped = true;\n-\n-    @Override\n-    public void init(InitializationContext ctx) {\n-        this.configuration = ctx.getConfiguration();\n-        this.ctx = ctx;\n-        this.timeService = ctx.getTimeService();\n-        this.marshaller = ctx.getPersistenceMarshaller();\n-        this.semaphore = new Semaphore(Integer.MAX_VALUE, true);\n-        this.entryFactory = ctx.getMarshallableEntryFactory();\n-        ctx.getPersistenceMarshaller().register(new PersistenceContextInitializerImpl());\n-    }\n-\n-    @Override\n-    public void start() {\n-\n-        AdvancedCache cache = ctx.getCache().getAdvancedCache();\n-        KeyPartitioner keyPartitioner = cache.getComponentRegistry().getComponent(KeyPartitioner.class);\n-        if (configuration.segmented()) {\n-            handler = new SegmentedRocksDBHandler(cache.getCacheConfiguration().clustering().hash().numSegments(),\n-                  keyPartitioner);\n-        } else {\n-            handler = new NonSegmentedRocksDBHandler(keyPartitioner);\n-        }\n-\n-        // Has to be done before we open the database, so we can pass the properties\n-        Properties allProperties = configuration.properties();\n-        for (Map.Entry<Object, Object> entry : allProperties.entrySet()) {\n-            String key = entry.getKey().toString();\n-            if (key.startsWith(DATABASE_PROPERTY_NAME_WITH_SUFFIX)) {\n-                if (databaseProperties == null) {\n-                    databaseProperties = new Properties();\n-                }\n-                databaseProperties.setProperty(key.substring(DATABASE_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n-            } else if (key.startsWith(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX)) {\n-                if (columnFamilyProperties == null) {\n-                    columnFamilyProperties = new Properties();\n-                }\n-                columnFamilyProperties.setProperty(key.substring(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n-            }\n-        }\n-\n-        try {\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+\n+public class RocksDBStore<K, V> implements NonBlockingStore<K, V> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass(), Log.class);\n+   private static final boolean trace = log.isTraceEnabled();\n+\n+   static final String DATABASE_PROPERTY_NAME_WITH_SUFFIX = \"database.\";\n+   static final String COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX = \"data.\";\n+\n+   protected RocksDBStoreConfiguration configuration;\n+   private RocksDB db;\n+   private RocksDB expiredDb;\n+   private InitializationContext ctx;\n+   private TimeService timeService;\n+   private WriteOptions dataWriteOptions;\n+   private RocksDBHandler handler;\n+   private Properties databaseProperties;\n+   private Properties columnFamilyProperties;\n+   private Marshaller marshaller;\n+   private KeyPartitioner keyPartitioner;\n+   private MarshallableEntryFactory<K, V> entryFactory;\n+   private BlockingManager blockingManager;\n+\n+   @Override\n+   public CompletionStage<Void> start(InitializationContext ctx) {\n+      this.configuration = ctx.getConfiguration();\n+      this.ctx = ctx;\n+      this.timeService = ctx.getTimeService();\n+      this.marshaller = ctx.getPersistenceMarshaller();\n+      this.entryFactory = ctx.getMarshallableEntryFactory();\n+      this.blockingManager = ctx.getBlockingManager();\n+      this.keyPartitioner = ctx.getKeyPartitioner();\n+\n+      ctx.getPersistenceMarshaller().register(new PersistenceContextInitializerImpl());\n+\n+      AdvancedCache cache = ctx.getCache().getAdvancedCache();\n+      if (configuration.segmented()) {\n+         handler = new SegmentedRocksDBHandler(cache.getCacheConfiguration().clustering().hash().numSegments());\n+      } else {\n+         handler = new NonSegmentedRocksDBHandler(keyPartitioner);\n+      }\n+\n+      // Has to be done before we open the database, so we can pass the properties\n+      Properties allProperties = configuration.properties();\n+      for (Map.Entry<Object, Object> entry : allProperties.entrySet()) {\n+         String key = entry.getKey().toString();\n+         if (key.startsWith(DATABASE_PROPERTY_NAME_WITH_SUFFIX)) {\n+            if (databaseProperties == null) {\n+               databaseProperties = new Properties();\n+            }\n+            databaseProperties.setProperty(key.substring(DATABASE_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n+         } else if (key.startsWith(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX)) {\n+            if (columnFamilyProperties == null) {\n+               columnFamilyProperties = new Properties();\n+            }\n+            columnFamilyProperties.setProperty(key.substring(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n+         }\n+      }\n+\n+      return blockingManager.runBlocking(() -> {\n+         try {\n             db = handler.open(getLocation(), dataDbOptions());\n             expiredDb = openDatabase(getExpirationLocation(), expiredDbOptions());\n-            stopped = false;\n-        } catch (Exception e) {\n+         } catch (Exception e) {\n             throw new CacheConfigurationException(\"Unable to open database\", e);\n-        }\n-    }\n-\n-    private Path getLocation() {\n-        return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.location(), ctx.getCache().getName(), \"data\");\n-    }\n-\n-    private Path getExpirationLocation() {\n-        return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.expiredLocation(), ctx.getCache().getName(), \"expired\");\n-    }\n-\n-    private WriteOptions dataWriteOptions() {\n-        if (dataWriteOptions == null)\n-            dataWriteOptions = new WriteOptions().setDisableWAL(false);\n-        return dataWriteOptions;\n-    }\n-\n-    protected DBOptions dataDbOptions() {\n-        DBOptions dbOptions;\n-        if (databaseProperties != null) {\n-            dbOptions = DBOptions.getDBOptionsFromProps(databaseProperties);\n-            if (dbOptions == null) {\n-                throw log.rocksDBUnknownPropertiesSupplied(databaseProperties.toString());\n-            }\n-        } else {\n-            dbOptions = new DBOptions();\n-        }\n-        return dbOptions\n-              .setCreateIfMissing(true)\n-              // We have to create missing column families on open.\n-              // Otherwise when we start we won't know what column families this database had if any - thus\n-              // we must specify all of them and later remove them.\n-              .setCreateMissingColumnFamilies(true);\n-    }\n-\n-    protected Options expiredDbOptions() {\n-        return new Options()\n-              .setCreateIfMissing(true)\n-              // Make sure keys are sorted by bytes - we use this sorting to remove entries that have expired most recently\n-              .setComparator(BuiltinComparator.BYTEWISE_COMPARATOR);\n-    }\n-\n-    /**\n-     * Creates database if it doesn't exist.\n-     */\n-    protected RocksDB openDatabase(Path location, Options options) throws RocksDBException {\n-        File dir = location.toFile();\n-        dir.mkdirs();\n-        return RocksDB.open(options, location.toString());\n-    }\n-\n-    @Override\n-    public void stop() {\n-        try {\n-            semaphore.acquire(Integer.MAX_VALUE);\n-        } catch (InterruptedException e) {\n-            throw new PersistenceException(\"Cannot acquire semaphore\", e);\n-        }\n-        try {\n-            handler.close();\n-            expiredDb.close();\n-        } finally {\n-            stopped = true;\n-            semaphore.release(Integer.MAX_VALUE);\n-        }\n-    }\n-\n-    @Override\n-    public void destroy() {\n-        stop();\n-        Util.recursiveFileRemove(getLocation().toFile());\n-        Util.recursiveFileRemove(getExpirationLocation().toFile());\n-    }\n-\n-    @Override\n-    public boolean isAvailable() {\n-        return getLocation().toFile().exists() && getExpirationLocation().toFile().exists();\n-    }\n-\n-    @Override\n-    public void clear() {\n-        handler.clear(null);\n-    }\n-\n-    @Override\n-    public void clear(IntSet segments) {\n-        handler.clear(segments);\n-    }\n-\n-    @Override\n-    public int size() {\n-        return handler.size(null);\n-    }\n-\n-    @Override\n-    public int size(IntSet segments) {\n-        return handler.size(segments);\n-    }\n-\n-    @Override\n-    public boolean contains(Object key) {\n-        return handler.contains(-1, key);\n-    }\n-\n-    @Override\n-    public boolean contains(int segment, Object key) {\n-        return handler.contains(segment, key);\n-    }\n-\n-    @Override\n-    public Publisher<K> publishKeys(Predicate<? super K> filter) {\n-        return handler.publishKeys(null, filter);\n-    }\n-\n-    @Override\n-    public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n-        return handler.publishKeys(segments, filter);\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> entryPublisher(Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata) {\n-        return handler.publishEntries(null, filter, fetchValue, fetchMetadata);\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> entryPublisher(IntSet segments, Predicate<? super K> filter,\n-                                                             boolean fetchValue, boolean fetchMetadata) {\n-        return handler.publishEntries(segments, filter, fetchValue, fetchMetadata);\n-    }\n-\n-    @Override\n-    public boolean delete(Object key) {\n-        return handler.delete(-1, key);\n-    }\n-\n-    @Override\n-    public boolean delete(int segment, Object key) {\n-        return handler.delete(segment, key);\n-    }\n-\n-    @Override\n-    public void write(MarshallableEntry entry) {\n-        handler.write(-1, entry);\n-    }\n-\n-    @Override\n-    public void write(int segment, MarshallableEntry<? extends K, ? extends V> entry) {\n-        handler.write(segment, entry);\n-    }\n-\n-    @Override\n-    public MarshallableEntry loadEntry(Object key) {\n-        return handler.load(-1, key);\n-    }\n-\n-    @Override\n-    public MarshallableEntry<K, V> get(int segment, Object key) {\n-        return handler.load(segment, key);\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> bulkUpdate(Publisher<MarshallableEntry<? extends K, ? extends V>> publisher) {\n-        return handler.writeBatch(publisher);\n-    }\n-\n-    @Override\n-    public void deleteBatch(Iterable<Object> keys) {\n-        handler.deleteBatch(keys);\n-    }\n-\n-    private void putExpireDbData(ExpiryEntry entry) throws InterruptedException, RocksDBException, IOException,\n-       ClassNotFoundException {\n-        final byte[] expiryBytes = marshall(entry.expiry);\n-        final byte[] existingBytes = expiredDb.get(expiryBytes);\n-\n-        if (existingBytes != null) {\n-            // in the case of collision make the value a List ...\n-            final Object existing = unmarshall(existingBytes);\n-            if (existing instanceof ExpiryBucket) {\n-                ((ExpiryBucket) existing).entries.add(entry.keyBytes);\n-                expiredDb.put(expiryBytes, marshall(existing));\n-            } else {\n-                ExpiryBucket bucket = new ExpiryBucket(existingBytes, entry.keyBytes);\n-                expiredDb.put(expiryBytes, marshall(bucket));\n-            }\n-        } else {\n-            expiredDb.put(expiryBytes, entry.keyBytes);\n-        }\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    @Override\n-    public void purge(Executor executor, PurgeListener purgeListener) {\n-        try {\n-            semaphore.acquire();\n-        } catch (InterruptedException e) {\n-            throw new PersistenceException(\"Cannot acquire semaphore: CacheStore is likely stopped.\", e);\n-        }\n-        try (ReadOptions readOptions = new ReadOptions().setFillCache(false)) {\n-            if (stopped) {\n-                throw new PersistenceException(\"RocksDB is stopped\");\n-            }\n-            long now = ctx.getTimeService().wallClockTime();\n-            RocksIterator iterator = expiredDb.newIterator(readOptions);\n-            if (iterator != null) {\n-                try (RocksIterator it = iterator) {\n-                    List<Long> times = new ArrayList<>();\n-                    List<Object> keys = new ArrayList<>();\n-                    List<byte[]> marshalledKeys = new ArrayList<>();\n-\n-                    for (it.seekToFirst(); it.isValid(); it.next()) {\n-                        Long time = (Long) unmarshall(it.key());\n+         }\n+      }, \"rocksdb-open\");\n+   }\n+\n+   private Path getLocation() {\n+      return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.location(), ctx.getCache().getName(), \"data\");\n+   }\n+\n+   private Path getExpirationLocation() {\n+      return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.expiredLocation(), ctx.getCache().getName(), \"expired\");\n+   }\n+\n+   protected DBOptions dataDbOptions() {\n+      DBOptions dbOptions;\n+      if (databaseProperties != null) {\n+         dbOptions = DBOptions.getDBOptionsFromProps(databaseProperties);\n+         if (dbOptions == null) {\n+            throw log.rocksDBUnknownPropertiesSupplied(databaseProperties.toString());\n+         }\n+      } else {\n+         dbOptions = new DBOptions();\n+      }\n+      return dbOptions\n+            .setCreateIfMissing(true)\n+            // We have to create missing column families on open.\n+            // Otherwise when we start we won't know what column families this database had if any - thus\n+            // we must specify all of them and later remove them.\n+            .setCreateMissingColumnFamilies(true);\n+   }\n+\n+   protected Options expiredDbOptions() {\n+      return new Options()\n+            .setCreateIfMissing(true)\n+            // Make sure keys are sorted by bytes - we use this sorting to remove entries that have expired most recently\n+            .setComparator(BuiltinComparator.BYTEWISE_COMPARATOR);\n+   }\n+\n+   /**\n+    * Creates database if it doesn't exist.\n+    */\n+   protected RocksDB openDatabase(Path location, Options options) throws RocksDBException {\n+      File dir = location.toFile();\n+      dir.mkdirs();\n+      return RocksDB.open(options, location.toString());\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> stop() {\n+      return blockingManager.runBlocking(() -> {\n+         handler.close();\n+         expiredDb.close();\n+      }, \"rocksdb-stop\");\n+   }\n+\n+   @Override\n+   public Set<Characteristic> characteristics() {\n+      return EnumSet.of(Characteristic.BULK_READ, Characteristic.EXPIRATION, Characteristic.SEGMENTABLE);\n+   }\n+\n+   @Override\n+   public CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n+      return handler.load(segment, key);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> entry) {\n+      return handler.write(segment, entry);\n+   }\n+\n+   @Override\n+   public CompletionStage<Boolean> delete(int segment, Object key) {\n+      return handler.delete(segment, key);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> batch(int publisherCount, Publisher<SegmentedPublisher<Object>> removePublisher,\n+         Publisher<SegmentedPublisher<MarshallableEntry<K, V>>> writePublisher) {\n+      WriteBatch batch = new WriteBatch();\n+      Set<MarshallableEntry<K, V>> expirableEntries = new HashSet<>();\n+      Flowable.fromPublisher(removePublisher)\n+            .subscribe(sp -> {\n+               ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n+               Flowable.fromPublisher(sp)\n+                     .subscribe(removed -> batch.delete(handle, marshall(removed)));\n+            });\n+      Flowable.fromPublisher(writePublisher)\n+            .subscribe(sp -> {\n+               ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n+               Flowable.fromPublisher(sp)\n+                     .subscribe(me -> {\n+                        batch.put(handle, marshall(me.getKey()), marshall(me.getMarshalledValue()));\n+                        if (me.expiryTime() > -1) {\n+                           expirableEntries.add(me);\n+                        }\n+                     });\n+            });\n+      if (batch.count() <= 0) {\n+         batch.close();\n+         return CompletableFutures.completedNull();\n+      }\n+      return blockingManager.runBlocking(() -> {\n+         try {\n+            db.write(dataWriteOptions(), batch);\n+            for (MarshallableEntry<K, V> me : expirableEntries) {\n+               addNewExpiry(me);\n+            }\n+         } catch (RocksDBException e) {\n+            throw new PersistenceException(e);\n+         }\n+      }, \"rocksdb-batch\").whenComplete((ignore, t) -> batch.close());\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> clear() {\n+      return handler.clear();\n+   }\n+\n+   @Override\n+   public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n+      return Flowable.fromPublisher(handler.publishEntries(segments, filter, false))\n+            .map(MarshallableEntry::getKey);\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n+      return handler.publishEntries(segments, filter, includeValues);\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> size(IntSet segments) {\n+      return handler.size(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> approximateSize(IntSet segments) {\n+      return handler.approximateSize(segments);\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> purgeExpired() {\n+      return Flowable.defer(() -> {\n+         UnicastProcessor<MarshallableEntry<K, V>> processor = UnicastProcessor.create();\n+         blockingManager.runBlocking(() -> {\n+            try (ReadOptions readOptions = new ReadOptions().setFillCache(false)) {\n+               long now = ctx.getTimeService().wallClockTime();\n+               RocksIterator iterator = expiredDb.newIterator(readOptions);\n+               if (iterator != null) {\n+                  try (RocksIterator it = iterator) {\n+                     List<Long> times = new ArrayList<>();\n+                     List<Object> keys = new ArrayList<>();\n+                     List<byte[]> marshalledKeys = new ArrayList<>();\n+\n+                     for (it.seekToFirst(); it.isValid(); it.next()) {\n+                        Long time = unmarshall(it.key());\n                         if (time > now)\n-                            break;\n+                           break;\n                         times.add(time);\n                         byte[] marshalledKey = it.value();\n                         Object key = unmarshall(marshalledKey);\n                         if (key instanceof ExpiryBucket) {\n-                            for (byte[] bytes : ((ExpiryBucket) key).entries) {\n-                                marshalledKeys.add(bytes);\n-                                keys.add(unmarshall(bytes));\n-                            }\n+                           for (byte[] bytes : ((ExpiryBucket) key).entries) {\n+                              marshalledKeys.add(bytes);\n+                              keys.add(unmarshall(bytes));\n+                           }\n                         } else {\n-                            keys.add(key);\n-                            marshalledKeys.add(marshalledKey);\n+                           keys.add(key);\n+                           marshalledKeys.add(marshalledKey);\n                         }\n-                    }\n+                     }\n \n-                    for (Long time : times) {\n+                     for (Long time : times) {\n                         expiredDb.delete(marshall(time));\n-                    }\n+                     }\n \n-                    if (!keys.isEmpty())\n+                     if (!keys.isEmpty())\n                         log.debugf(\"purge (up to) %d entries\", keys.size());\n-                    int count = 0;\n-                    for (int i = 0; i < keys.size(); i++) {\n+                     int count = 0;\n+                     for (int i = 0; i < keys.size(); i++) {\n                         Object key = keys.get(i);\n                         byte[] keyBytes = marshalledKeys.get(i);\n-                        int segment = handler.calculateSegment(key);\n \n-                        ColumnFamilyHandle handle = handler.getHandle(segment);\n+                        ColumnFamilyHandle handle = handler.getHandle(key);\n                         byte[] valueBytes = db.get(handle, keyBytes);\n                         if (valueBytes == null)\n-                            continue;\n+                           continue;\n \n-                        MarshalledValue mv = (MarshalledValue) unmarshall(valueBytes);\n+                        MarshalledValue mv = unmarshall(valueBytes);\n                         if (mv != null) {\n-                            // TODO race condition: the entry could be updated between the get and delete!\n-                            Metadata metadata = (Metadata) unmarshall(MarshallUtil.toByteArray(mv.getMetadataBytes()));\n-                            if (MarshallableEntryImpl.isExpired(metadata, now, mv.getCreated(), mv.getLastUsed())) {\n-                                // somewhat inefficient to FIND then REMOVE...\n-                                db.delete(handle, keyBytes);\n-                                purgeListener.entryPurged(key);\n-                                count++;\n-                            }\n+                           // TODO race condition: the entry could be updated between the get and delete!\n+                           Metadata metadata = unmarshall(MarshallUtil.toByteArray(mv.getMetadataBytes()));\n+                           if (MarshallableEntryImpl.isExpired(metadata, now, mv.getCreated(), mv.getLastUsed())) {\n+                              // somewhat inefficient to FIND then REMOVE... but required if the value is updated\n+                              db.delete(handle, keyBytes);\n+                              processor.onNext(entryFactory.create(key, mv));\n+                              count++;\n+                           }\n                         }\n-                    }\n-                    if (count != 0)\n+                     }\n+                     if (count != 0)\n                         log.debugf(\"purged %d entries\", count);\n-                } catch (Exception e) {\n-                    throw new PersistenceException(e);\n-                } finally {\n-                    readOptions.close();\n-                }\n-            }\n-        } catch (PersistenceException e) {\n-            throw e;\n-        } catch (Exception e) {\n-            throw new PersistenceException(e);\n-        } finally {\n-            semaphore.release();\n-        }\n-    }\n-\n-    @Override\n-    public void addSegments(IntSet segments) {\n-        handler.addSegments(segments);\n-    }\n-\n-    @Override\n-    public void removeSegments(IntSet segments) {\n-        handler.removeSegments(segments);\n-    }\n-\n-    private byte[] marshall(Object entry) throws IOException, InterruptedException {\n-        return marshaller.objectToByteBuffer(entry);\n-    }\n-\n-    private Object unmarshall(byte[] bytes) throws IOException, ClassNotFoundException {\n-        if (bytes == null)\n-            return null;\n-\n-        return marshaller.objectFromByteBuffer(bytes);\n-    }\n-\n-    private MarshallableEntry<K, V> valueToMarshallableEntry(Object key, byte[] valueBytes, boolean fetchMeta) throws IOException, ClassNotFoundException {\n-        MarshalledValue value = (MarshalledValue) unmarshall(valueBytes);\n-        if (value == null) return null;\n-\n-        ByteBuffer metadataBytes = fetchMeta ? value.getMetadataBytes() : null;\n-        return entryFactory.create(key, value.getValueBytes(), metadataBytes, value.getInternalMetadataBytes(), value.getCreated(), value.getLastUsed());\n-    }\n-\n-    private void addNewExpiry(MarshallableEntry entry) throws RocksDBException, IOException, ClassNotFoundException {\n-        long expiry = entry.expiryTime();\n-        long maxIdle = entry.getMetadata().maxIdle();\n-        if (maxIdle > 0) {\n-            // Coding getExpiryTime() for transient entries has the risk of being a moving target\n-            // which could lead to unexpected results, hence, InternalCacheEntry calls are required\n-            expiry = maxIdle + ctx.getTimeService().wallClockTime();\n-        }\n-        try {\n-            byte[] keyBytes = entry.getKeyBytes().copy().getBuf();\n-            putExpireDbData(new ExpiryEntry(expiry, keyBytes));\n-        } catch (InterruptedException e) {\n-            Thread.currentThread().interrupt(); // Restore interruption status\n-        }\n-    }\n-\n-    @ProtoTypeId(ProtoStreamTypeIds.ROCKSDB_EXPIRY_BUCKET)\n-    static final class ExpiryBucket {\n-        @ProtoField(number = 1, collectionImplementation = ArrayList.class)\n-        List<byte[]> entries;\n-\n-        ExpiryBucket(){}\n-\n-        ExpiryBucket(byte[] existingKey, byte[] newKey) {\n-            entries = new ArrayList<>(2);\n-            entries.add(existingKey);\n-            entries.add(newKey);\n-        }\n-    }\n-\n-    private static final class ExpiryEntry {\n-\n-        final long expiry;\n-        final byte[] keyBytes;\n-\n-        ExpiryEntry(long expiry, byte[] keyBytes) {\n-            this.expiry = expiry;\n-            this.keyBytes = keyBytes;\n-        }\n-\n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o) return true;\n-            if (o == null || getClass() != o.getClass()) return false;\n-            ExpiryEntry that = (ExpiryEntry) o;\n-            return expiry == that.expiry &&\n-                  Arrays.equals(keyBytes, that.keyBytes);\n-        }\n-\n-        @Override\n-        public int hashCode() {\n-            int result = Objects.hash(expiry);\n-            result = 31 * result + Arrays.hashCode(keyBytes);\n-            return result;\n-        }\n-    }\n-\n-    private class RocksKeyIterator extends AbstractIterator<K> {\n-        private final RocksIterator it;\n-        private final Predicate<? super K> filter;\n-\n-        public RocksKeyIterator(RocksIterator it, Predicate<? super K> filter) {\n-            this.it = it;\n-            this.filter = filter;\n-        }\n-\n-        @Override\n-        protected K getNext() {\n-            K key = null;\n-            try {\n-                while (key == null && it.isValid()) {\n-                    K testKey = (K) unmarshall(it.key());\n-                    if (filter == null || filter.test(testKey)) {\n-                        key = testKey;\n-                    }\n-                    it.next();\n-                }\n-            } catch (IOException | ClassNotFoundException e) {\n-                throw new CacheException(e);\n-            }\n-            return key;\n-        }\n-    }\n-\n-    private class RocksEntryIterator extends AbstractIterator<MarshallableEntry<K, V>> {\n-        private final RocksIterator it;\n-        private final Predicate<? super K> filter;\n-        private final boolean fetchValue;\n-        private final boolean fetchMetadata;\n-        private final long now;\n-\n-        public RocksEntryIterator(RocksIterator it, Predicate<? super K> filter, boolean fetchValue,\n-              boolean fetchMetadata, long now) {\n-            this.it = it;\n-            this.filter = filter;\n-            this.fetchValue = fetchValue;\n-            this.fetchMetadata = fetchMetadata;\n-            this.now = now;\n-        }\n-\n-        @Override\n-        protected MarshallableEntry<K, V> getNext() {\n-            MarshallableEntry<K, V> entry = null;\n-            try {\n-                while (entry == null && it.isValid()) {\n-                    K key = (K) unmarshall(it.key());\n-                    if (filter == null || filter.test(key)) {\n-                        if (fetchValue || fetchMetadata) {\n-                            MarshallableEntry<K, V> me = valueToMarshallableEntry(key, it.value(), fetchMetadata);\n-                            if (me != null && !me.isExpired(now)) {\n-                                entry = me;\n-                            }\n-                        } else {\n-                            entry = entryFactory.create(key);\n-                        }\n-                    }\n-                    it.next();\n-                }\n-            } catch (IOException | ClassNotFoundException e) {\n-                throw new CacheException(e);\n-            }\n-            return entry;\n-        }\n-    }\n-\n-    private abstract class RocksDBHandler {\n-\n-        abstract RocksDB open(Path location, DBOptions options) throws RocksDBException;\n-\n-        abstract void close();\n-\n-        abstract ColumnFamilyHandle getHandle(int segment);\n-\n-        final ColumnFamilyHandle getHandle(int segment, Object key) {\n-            if (segment < 0) {\n-                segment = calculateSegment(key);\n-            }\n-            return getHandle(segment);\n-        }\n-\n-        abstract int calculateSegment(Object key);\n-\n-        ColumnFamilyDescriptor newDescriptor(byte[] name) {\n-            ColumnFamilyOptions columnFamilyOptions;\n-            if (columnFamilyProperties != null) {\n-                columnFamilyOptions = ColumnFamilyOptions.getColumnFamilyOptionsFromProps(columnFamilyProperties);\n-                if (columnFamilyOptions == null) {\n-                    throw log.rocksDBUnknownPropertiesSupplied(columnFamilyProperties.toString());\n-                }\n+                  } catch (Exception e) {\n+                     throw new PersistenceException(e);\n+                  } finally {\n+                     readOptions.close();\n+                  }\n+               }\n+            }\n+         }, \"rocksdb-purgeExpired\").whenComplete((ignore, t) -> {\n+            if (t != null) {\n+               processor.onError(t);\n             } else {\n-                columnFamilyOptions = new ColumnFamilyOptions();\n-            }\n-            return new ColumnFamilyDescriptor(name,\n-                  columnFamilyOptions.setCompressionType(CompressionType.getCompressionType(configuration.compressionType().toString())));\n-        }\n-\n-        boolean contains(int segment, Object key) {\n-            // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n-            return load(segment, key) != null;\n-        }\n-\n-        MarshallableEntry<K, V> load(int segment, Object key) {\n-            ColumnFamilyHandle handle = getHandle(segment, key);\n-            if (handle == null) {\n-                log.trace(\"Ignoring load as handle is not currently configured\");\n-                return null;\n-            }\n-            try {\n-                byte[] entryBytes;\n-                semaphore.acquire();\n-                try {\n-                    if (stopped) {\n-                        throw new PersistenceException(\"RocksDB is stopped\");\n-                    }\n-\n-                    entryBytes = db.get(handle, marshall(key));\n-                } finally {\n-                    semaphore.release();\n-                }\n-                MarshallableEntry<K, V> me = valueToMarshallableEntry(key, entryBytes, true);\n-                if (me == null || me.isExpired(timeService.wallClockTime())) {\n-                    return null;\n-                }\n-                return me;\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n-\n-        void write(int segment, MarshallableEntry<? extends K, ? extends V> me) {\n-            Object key = me.getKey();\n-            ColumnFamilyHandle handle = getHandle(segment, key);\n-            if (handle == null) {\n-                log.trace(\"Ignoring write as handle is not currently configured\");\n-                return;\n-            }\n-            try {\n-                byte[] marshalledKey = MarshallUtil.toByteArray(me.getKeyBytes());\n-                byte[] marshalledValue = marshall(me.getMarshalledValue());\n-                semaphore.acquire();\n-                try {\n-                    if (stopped) {\n-                        throw new PersistenceException(\"RocksDB is stopped\");\n-                    }\n-                    db.put(handle, marshalledKey, marshalledValue);\n-                } finally {\n-                    semaphore.release();\n-                }\n-                if (me.expiryTime() > -1) {\n-                    addNewExpiry(me);\n-                }\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n-\n-        boolean delete(int segment, Object key) {\n-            try {\n-                byte[] keyBytes = marshall(key);\n-                semaphore.acquire();\n-                try {\n-                    if (stopped) {\n-                        throw new PersistenceException(\"RocksDB is stopped\");\n-                    }\n-                    if (db.get(getHandle(segment, key), keyBytes) == null) {\n-                        return false;\n-                    }\n-                    db.delete(getHandle(segment, key), keyBytes);\n-                } finally {\n-                    semaphore.release();\n-                }\n-                return true;\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n-\n-        CompletionStage<Void> writeBatch(Publisher<MarshallableEntry<? extends K, ? extends V>> publisher) {\n-            return Flowable.fromPublisher(publisher)\n-                  .buffer(configuration.maxBatchSize())\n-                  .doOnNext(entries -> {\n-                      WriteBatch batch = new WriteBatch();\n-                      for (MarshallableEntry<? extends K, ? extends V> entry : entries) {\n-                          int segment = calculateSegment(entry.getKey());\n-                          byte[] keyBytes = MarshallUtil.toByteArray(entry.getKeyBytes());\n-                          batch.put(getHandle(segment), keyBytes, marshall(entry.getMarshalledValue()));\n-                      }\n-                      writeBatch(batch);\n-\n-                      // Add metadata only after batch has been written\n-                      for (MarshallableEntry entry : entries) {\n-                          if (entry.expiryTime() > -1)\n-                              addNewExpiry(entry);\n-                      }\n-                  })\n-                  .doOnError(e -> {\n-                      throw new PersistenceException(e);\n-                  })\n-                  .ignoreElements()\n-                  .toCompletionStage(null);\n-        }\n-\n-        void deleteBatch(Iterable<Object> keys) {\n-            try {\n-                int batchSize = 0;\n-                WriteBatch batch = new WriteBatch();\n-                for (Object key : keys) {\n-                    batch.remove(getHandle(calculateSegment(key)), marshall(key));\n-                    batchSize++;\n-\n-                    if (batchSize == configuration.maxBatchSize()) {\n-                        batchSize = 0;\n-                        writeBatch(batch);\n-                        batch = new WriteBatch();\n-                    }\n-                }\n-\n-                if (batchSize != 0)\n-                    writeBatch(batch);\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n-\n-        abstract void clear(IntSet segments);\n-\n-        abstract Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter);\n-\n-        abstract Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                                                                   boolean fetchValue, boolean fetchMetadata);\n+               processor.onComplete();\n+            }\n+         });\n+         return processor;\n+      });\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> addSegments(IntSet segments) {\n+      return handler.addSegments(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> removeSegments(IntSet segments) {\n+      return handler.removeSegments(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Boolean> isAvailable() {\n+      return blockingManager.supplyBlocking(() -> getLocation().toFile().exists() && getExpirationLocation().toFile().exists(),\n+            \"rocksdb-available\");\n+   }\n+\n+   private abstract class RocksDBHandler {\n+\n+      abstract RocksDB open(Path location, DBOptions options) throws RocksDBException;\n+\n+      abstract void close();\n+\n+      abstract ColumnFamilyHandle getHandle(int segment);\n+\n+      abstract ColumnFamilyHandle getHandle(Object key);\n+\n+      ColumnFamilyDescriptor newDescriptor(byte[] name) {\n+         ColumnFamilyOptions columnFamilyOptions;\n+         if (columnFamilyProperties != null) {\n+            columnFamilyOptions = ColumnFamilyOptions.getColumnFamilyOptionsFromProps(columnFamilyProperties);\n+            if (columnFamilyOptions == null) {\n+               throw log.rocksDBUnknownPropertiesSupplied(columnFamilyProperties.toString());\n+            }\n+         } else {\n+            columnFamilyOptions = new ColumnFamilyOptions();\n+         }\n+         return new ColumnFamilyDescriptor(name,\n+               columnFamilyOptions.setCompressionType(CompressionType.getCompressionType(configuration.compressionType().toString())));\n+      }\n+\n+      CompletionStage<Boolean> contains(int segment, Object key) {", "originalCommit": "be24dc2231fa8baed421fe4a622b5347481a67a9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2MTM1MQ==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r434761351", "bodyText": "Fixed, moved this to the actual store method for better visibility.", "author": "wburns", "createdAt": "2020-06-03T18:12:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY0OTU4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY1MzgzNA==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r434653834", "bodyText": "Never used.", "author": "ryanemerson", "createdAt": "2020-06-03T15:26:28Z", "path": "persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java", "diffHunk": "@@ -67,1091 +67,933 @@\n import org.rocksdb.WriteOptions;\n \n import io.reactivex.rxjava3.core.Flowable;\n-\n-@Store\n-@ConfiguredBy(RocksDBStoreConfiguration.class)\n-public class RocksDBStore<K,V> implements SegmentedAdvancedLoadWriteStore<K,V> {\n-    private static final Log log = LogFactory.getLog(RocksDBStore.class, Log.class);\n-    static final String DATABASE_PROPERTY_NAME_WITH_SUFFIX = \"database.\";\n-    static final String COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX = \"data.\";\n-\n-    protected RocksDBStoreConfiguration configuration;\n-    private RocksDB db;\n-    private RocksDB expiredDb;\n-    private InitializationContext ctx;\n-    private TimeService timeService;\n-    private Semaphore semaphore;\n-    private WriteOptions dataWriteOptions;\n-    private RocksDBHandler handler;\n-    private Properties databaseProperties;\n-    private Properties columnFamilyProperties;\n-    private Marshaller marshaller;\n-    private MarshallableEntryFactory<K, V> entryFactory;\n-    private volatile boolean stopped = true;\n-\n-    @Override\n-    public void init(InitializationContext ctx) {\n-        this.configuration = ctx.getConfiguration();\n-        this.ctx = ctx;\n-        this.timeService = ctx.getTimeService();\n-        this.marshaller = ctx.getPersistenceMarshaller();\n-        this.semaphore = new Semaphore(Integer.MAX_VALUE, true);\n-        this.entryFactory = ctx.getMarshallableEntryFactory();\n-        ctx.getPersistenceMarshaller().register(new PersistenceContextInitializerImpl());\n-    }\n-\n-    @Override\n-    public void start() {\n-\n-        AdvancedCache cache = ctx.getCache().getAdvancedCache();\n-        KeyPartitioner keyPartitioner = cache.getComponentRegistry().getComponent(KeyPartitioner.class);\n-        if (configuration.segmented()) {\n-            handler = new SegmentedRocksDBHandler(cache.getCacheConfiguration().clustering().hash().numSegments(),\n-                  keyPartitioner);\n-        } else {\n-            handler = new NonSegmentedRocksDBHandler(keyPartitioner);\n-        }\n-\n-        // Has to be done before we open the database, so we can pass the properties\n-        Properties allProperties = configuration.properties();\n-        for (Map.Entry<Object, Object> entry : allProperties.entrySet()) {\n-            String key = entry.getKey().toString();\n-            if (key.startsWith(DATABASE_PROPERTY_NAME_WITH_SUFFIX)) {\n-                if (databaseProperties == null) {\n-                    databaseProperties = new Properties();\n-                }\n-                databaseProperties.setProperty(key.substring(DATABASE_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n-            } else if (key.startsWith(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX)) {\n-                if (columnFamilyProperties == null) {\n-                    columnFamilyProperties = new Properties();\n-                }\n-                columnFamilyProperties.setProperty(key.substring(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n-            }\n-        }\n-\n-        try {\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+\n+public class RocksDBStore<K, V> implements NonBlockingStore<K, V> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass(), Log.class);\n+   private static final boolean trace = log.isTraceEnabled();\n+\n+   static final String DATABASE_PROPERTY_NAME_WITH_SUFFIX = \"database.\";\n+   static final String COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX = \"data.\";\n+\n+   protected RocksDBStoreConfiguration configuration;\n+   private RocksDB db;\n+   private RocksDB expiredDb;\n+   private InitializationContext ctx;\n+   private TimeService timeService;\n+   private WriteOptions dataWriteOptions;\n+   private RocksDBHandler handler;\n+   private Properties databaseProperties;\n+   private Properties columnFamilyProperties;\n+   private Marshaller marshaller;\n+   private KeyPartitioner keyPartitioner;\n+   private MarshallableEntryFactory<K, V> entryFactory;\n+   private BlockingManager blockingManager;\n+\n+   @Override\n+   public CompletionStage<Void> start(InitializationContext ctx) {\n+      this.configuration = ctx.getConfiguration();\n+      this.ctx = ctx;\n+      this.timeService = ctx.getTimeService();\n+      this.marshaller = ctx.getPersistenceMarshaller();\n+      this.entryFactory = ctx.getMarshallableEntryFactory();\n+      this.blockingManager = ctx.getBlockingManager();\n+      this.keyPartitioner = ctx.getKeyPartitioner();\n+\n+      ctx.getPersistenceMarshaller().register(new PersistenceContextInitializerImpl());\n+\n+      AdvancedCache cache = ctx.getCache().getAdvancedCache();\n+      if (configuration.segmented()) {\n+         handler = new SegmentedRocksDBHandler(cache.getCacheConfiguration().clustering().hash().numSegments());\n+      } else {\n+         handler = new NonSegmentedRocksDBHandler(keyPartitioner);\n+      }\n+\n+      // Has to be done before we open the database, so we can pass the properties\n+      Properties allProperties = configuration.properties();\n+      for (Map.Entry<Object, Object> entry : allProperties.entrySet()) {\n+         String key = entry.getKey().toString();\n+         if (key.startsWith(DATABASE_PROPERTY_NAME_WITH_SUFFIX)) {\n+            if (databaseProperties == null) {\n+               databaseProperties = new Properties();\n+            }\n+            databaseProperties.setProperty(key.substring(DATABASE_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n+         } else if (key.startsWith(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX)) {\n+            if (columnFamilyProperties == null) {\n+               columnFamilyProperties = new Properties();\n+            }\n+            columnFamilyProperties.setProperty(key.substring(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n+         }\n+      }\n+\n+      return blockingManager.runBlocking(() -> {\n+         try {\n             db = handler.open(getLocation(), dataDbOptions());\n             expiredDb = openDatabase(getExpirationLocation(), expiredDbOptions());\n-            stopped = false;\n-        } catch (Exception e) {\n+         } catch (Exception e) {\n             throw new CacheConfigurationException(\"Unable to open database\", e);\n-        }\n-    }\n-\n-    private Path getLocation() {\n-        return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.location(), ctx.getCache().getName(), \"data\");\n-    }\n-\n-    private Path getExpirationLocation() {\n-        return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.expiredLocation(), ctx.getCache().getName(), \"expired\");\n-    }\n-\n-    private WriteOptions dataWriteOptions() {\n-        if (dataWriteOptions == null)\n-            dataWriteOptions = new WriteOptions().setDisableWAL(false);\n-        return dataWriteOptions;\n-    }\n-\n-    protected DBOptions dataDbOptions() {\n-        DBOptions dbOptions;\n-        if (databaseProperties != null) {\n-            dbOptions = DBOptions.getDBOptionsFromProps(databaseProperties);\n-            if (dbOptions == null) {\n-                throw log.rocksDBUnknownPropertiesSupplied(databaseProperties.toString());\n-            }\n-        } else {\n-            dbOptions = new DBOptions();\n-        }\n-        return dbOptions\n-              .setCreateIfMissing(true)\n-              // We have to create missing column families on open.\n-              // Otherwise when we start we won't know what column families this database had if any - thus\n-              // we must specify all of them and later remove them.\n-              .setCreateMissingColumnFamilies(true);\n-    }\n-\n-    protected Options expiredDbOptions() {\n-        return new Options()\n-              .setCreateIfMissing(true)\n-              // Make sure keys are sorted by bytes - we use this sorting to remove entries that have expired most recently\n-              .setComparator(BuiltinComparator.BYTEWISE_COMPARATOR);\n-    }\n-\n-    /**\n-     * Creates database if it doesn't exist.\n-     */\n-    protected RocksDB openDatabase(Path location, Options options) throws RocksDBException {\n-        File dir = location.toFile();\n-        dir.mkdirs();\n-        return RocksDB.open(options, location.toString());\n-    }\n-\n-    @Override\n-    public void stop() {\n-        try {\n-            semaphore.acquire(Integer.MAX_VALUE);\n-        } catch (InterruptedException e) {\n-            throw new PersistenceException(\"Cannot acquire semaphore\", e);\n-        }\n-        try {\n-            handler.close();\n-            expiredDb.close();\n-        } finally {\n-            stopped = true;\n-            semaphore.release(Integer.MAX_VALUE);\n-        }\n-    }\n-\n-    @Override\n-    public void destroy() {\n-        stop();\n-        Util.recursiveFileRemove(getLocation().toFile());\n-        Util.recursiveFileRemove(getExpirationLocation().toFile());\n-    }\n-\n-    @Override\n-    public boolean isAvailable() {\n-        return getLocation().toFile().exists() && getExpirationLocation().toFile().exists();\n-    }\n-\n-    @Override\n-    public void clear() {\n-        handler.clear(null);\n-    }\n-\n-    @Override\n-    public void clear(IntSet segments) {\n-        handler.clear(segments);\n-    }\n-\n-    @Override\n-    public int size() {\n-        return handler.size(null);\n-    }\n-\n-    @Override\n-    public int size(IntSet segments) {\n-        return handler.size(segments);\n-    }\n-\n-    @Override\n-    public boolean contains(Object key) {\n-        return handler.contains(-1, key);\n-    }\n-\n-    @Override\n-    public boolean contains(int segment, Object key) {\n-        return handler.contains(segment, key);\n-    }\n-\n-    @Override\n-    public Publisher<K> publishKeys(Predicate<? super K> filter) {\n-        return handler.publishKeys(null, filter);\n-    }\n-\n-    @Override\n-    public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n-        return handler.publishKeys(segments, filter);\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> entryPublisher(Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata) {\n-        return handler.publishEntries(null, filter, fetchValue, fetchMetadata);\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> entryPublisher(IntSet segments, Predicate<? super K> filter,\n-                                                             boolean fetchValue, boolean fetchMetadata) {\n-        return handler.publishEntries(segments, filter, fetchValue, fetchMetadata);\n-    }\n-\n-    @Override\n-    public boolean delete(Object key) {\n-        return handler.delete(-1, key);\n-    }\n-\n-    @Override\n-    public boolean delete(int segment, Object key) {\n-        return handler.delete(segment, key);\n-    }\n-\n-    @Override\n-    public void write(MarshallableEntry entry) {\n-        handler.write(-1, entry);\n-    }\n-\n-    @Override\n-    public void write(int segment, MarshallableEntry<? extends K, ? extends V> entry) {\n-        handler.write(segment, entry);\n-    }\n-\n-    @Override\n-    public MarshallableEntry loadEntry(Object key) {\n-        return handler.load(-1, key);\n-    }\n-\n-    @Override\n-    public MarshallableEntry<K, V> get(int segment, Object key) {\n-        return handler.load(segment, key);\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> bulkUpdate(Publisher<MarshallableEntry<? extends K, ? extends V>> publisher) {\n-        return handler.writeBatch(publisher);\n-    }\n-\n-    @Override\n-    public void deleteBatch(Iterable<Object> keys) {\n-        handler.deleteBatch(keys);\n-    }\n-\n-    private void putExpireDbData(ExpiryEntry entry) throws InterruptedException, RocksDBException, IOException,\n-       ClassNotFoundException {\n-        final byte[] expiryBytes = marshall(entry.expiry);\n-        final byte[] existingBytes = expiredDb.get(expiryBytes);\n-\n-        if (existingBytes != null) {\n-            // in the case of collision make the value a List ...\n-            final Object existing = unmarshall(existingBytes);\n-            if (existing instanceof ExpiryBucket) {\n-                ((ExpiryBucket) existing).entries.add(entry.keyBytes);\n-                expiredDb.put(expiryBytes, marshall(existing));\n-            } else {\n-                ExpiryBucket bucket = new ExpiryBucket(existingBytes, entry.keyBytes);\n-                expiredDb.put(expiryBytes, marshall(bucket));\n-            }\n-        } else {\n-            expiredDb.put(expiryBytes, entry.keyBytes);\n-        }\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    @Override\n-    public void purge(Executor executor, PurgeListener purgeListener) {\n-        try {\n-            semaphore.acquire();\n-        } catch (InterruptedException e) {\n-            throw new PersistenceException(\"Cannot acquire semaphore: CacheStore is likely stopped.\", e);\n-        }\n-        try (ReadOptions readOptions = new ReadOptions().setFillCache(false)) {\n-            if (stopped) {\n-                throw new PersistenceException(\"RocksDB is stopped\");\n-            }\n-            long now = ctx.getTimeService().wallClockTime();\n-            RocksIterator iterator = expiredDb.newIterator(readOptions);\n-            if (iterator != null) {\n-                try (RocksIterator it = iterator) {\n-                    List<Long> times = new ArrayList<>();\n-                    List<Object> keys = new ArrayList<>();\n-                    List<byte[]> marshalledKeys = new ArrayList<>();\n-\n-                    for (it.seekToFirst(); it.isValid(); it.next()) {\n-                        Long time = (Long) unmarshall(it.key());\n+         }\n+      }, \"rocksdb-open\");\n+   }\n+\n+   private Path getLocation() {\n+      return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.location(), ctx.getCache().getName(), \"data\");\n+   }\n+\n+   private Path getExpirationLocation() {\n+      return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.expiredLocation(), ctx.getCache().getName(), \"expired\");\n+   }\n+\n+   protected DBOptions dataDbOptions() {\n+      DBOptions dbOptions;\n+      if (databaseProperties != null) {\n+         dbOptions = DBOptions.getDBOptionsFromProps(databaseProperties);\n+         if (dbOptions == null) {\n+            throw log.rocksDBUnknownPropertiesSupplied(databaseProperties.toString());\n+         }\n+      } else {\n+         dbOptions = new DBOptions();\n+      }\n+      return dbOptions\n+            .setCreateIfMissing(true)\n+            // We have to create missing column families on open.\n+            // Otherwise when we start we won't know what column families this database had if any - thus\n+            // we must specify all of them and later remove them.\n+            .setCreateMissingColumnFamilies(true);\n+   }\n+\n+   protected Options expiredDbOptions() {\n+      return new Options()\n+            .setCreateIfMissing(true)\n+            // Make sure keys are sorted by bytes - we use this sorting to remove entries that have expired most recently\n+            .setComparator(BuiltinComparator.BYTEWISE_COMPARATOR);\n+   }\n+\n+   /**\n+    * Creates database if it doesn't exist.\n+    */\n+   protected RocksDB openDatabase(Path location, Options options) throws RocksDBException {\n+      File dir = location.toFile();\n+      dir.mkdirs();\n+      return RocksDB.open(options, location.toString());\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> stop() {\n+      return blockingManager.runBlocking(() -> {\n+         handler.close();\n+         expiredDb.close();\n+      }, \"rocksdb-stop\");\n+   }\n+\n+   @Override\n+   public Set<Characteristic> characteristics() {\n+      return EnumSet.of(Characteristic.BULK_READ, Characteristic.EXPIRATION, Characteristic.SEGMENTABLE);\n+   }\n+\n+   @Override\n+   public CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n+      return handler.load(segment, key);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> entry) {\n+      return handler.write(segment, entry);\n+   }\n+\n+   @Override\n+   public CompletionStage<Boolean> delete(int segment, Object key) {\n+      return handler.delete(segment, key);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> batch(int publisherCount, Publisher<SegmentedPublisher<Object>> removePublisher,\n+         Publisher<SegmentedPublisher<MarshallableEntry<K, V>>> writePublisher) {\n+      WriteBatch batch = new WriteBatch();\n+      Set<MarshallableEntry<K, V>> expirableEntries = new HashSet<>();\n+      Flowable.fromPublisher(removePublisher)\n+            .subscribe(sp -> {\n+               ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n+               Flowable.fromPublisher(sp)\n+                     .subscribe(removed -> batch.delete(handle, marshall(removed)));\n+            });\n+      Flowable.fromPublisher(writePublisher)\n+            .subscribe(sp -> {\n+               ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n+               Flowable.fromPublisher(sp)\n+                     .subscribe(me -> {\n+                        batch.put(handle, marshall(me.getKey()), marshall(me.getMarshalledValue()));\n+                        if (me.expiryTime() > -1) {\n+                           expirableEntries.add(me);\n+                        }\n+                     });\n+            });\n+      if (batch.count() <= 0) {\n+         batch.close();\n+         return CompletableFutures.completedNull();\n+      }\n+      return blockingManager.runBlocking(() -> {\n+         try {\n+            db.write(dataWriteOptions(), batch);\n+            for (MarshallableEntry<K, V> me : expirableEntries) {\n+               addNewExpiry(me);\n+            }\n+         } catch (RocksDBException e) {\n+            throw new PersistenceException(e);\n+         }\n+      }, \"rocksdb-batch\").whenComplete((ignore, t) -> batch.close());\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> clear() {\n+      return handler.clear();\n+   }\n+\n+   @Override\n+   public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n+      return Flowable.fromPublisher(handler.publishEntries(segments, filter, false))\n+            .map(MarshallableEntry::getKey);\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n+      return handler.publishEntries(segments, filter, includeValues);\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> size(IntSet segments) {\n+      return handler.size(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> approximateSize(IntSet segments) {\n+      return handler.approximateSize(segments);\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> purgeExpired() {\n+      return Flowable.defer(() -> {\n+         UnicastProcessor<MarshallableEntry<K, V>> processor = UnicastProcessor.create();\n+         blockingManager.runBlocking(() -> {\n+            try (ReadOptions readOptions = new ReadOptions().setFillCache(false)) {\n+               long now = ctx.getTimeService().wallClockTime();\n+               RocksIterator iterator = expiredDb.newIterator(readOptions);\n+               if (iterator != null) {\n+                  try (RocksIterator it = iterator) {\n+                     List<Long> times = new ArrayList<>();\n+                     List<Object> keys = new ArrayList<>();\n+                     List<byte[]> marshalledKeys = new ArrayList<>();\n+\n+                     for (it.seekToFirst(); it.isValid(); it.next()) {\n+                        Long time = unmarshall(it.key());\n                         if (time > now)\n-                            break;\n+                           break;\n                         times.add(time);\n                         byte[] marshalledKey = it.value();\n                         Object key = unmarshall(marshalledKey);\n                         if (key instanceof ExpiryBucket) {\n-                            for (byte[] bytes : ((ExpiryBucket) key).entries) {\n-                                marshalledKeys.add(bytes);\n-                                keys.add(unmarshall(bytes));\n-                            }\n+                           for (byte[] bytes : ((ExpiryBucket) key).entries) {\n+                              marshalledKeys.add(bytes);\n+                              keys.add(unmarshall(bytes));\n+                           }\n                         } else {\n-                            keys.add(key);\n-                            marshalledKeys.add(marshalledKey);\n+                           keys.add(key);\n+                           marshalledKeys.add(marshalledKey);\n                         }\n-                    }\n+                     }\n \n-                    for (Long time : times) {\n+                     for (Long time : times) {\n                         expiredDb.delete(marshall(time));\n-                    }\n+                     }\n \n-                    if (!keys.isEmpty())\n+                     if (!keys.isEmpty())\n                         log.debugf(\"purge (up to) %d entries\", keys.size());\n-                    int count = 0;\n-                    for (int i = 0; i < keys.size(); i++) {\n+                     int count = 0;\n+                     for (int i = 0; i < keys.size(); i++) {\n                         Object key = keys.get(i);\n                         byte[] keyBytes = marshalledKeys.get(i);\n-                        int segment = handler.calculateSegment(key);\n \n-                        ColumnFamilyHandle handle = handler.getHandle(segment);\n+                        ColumnFamilyHandle handle = handler.getHandle(key);\n                         byte[] valueBytes = db.get(handle, keyBytes);\n                         if (valueBytes == null)\n-                            continue;\n+                           continue;\n \n-                        MarshalledValue mv = (MarshalledValue) unmarshall(valueBytes);\n+                        MarshalledValue mv = unmarshall(valueBytes);\n                         if (mv != null) {\n-                            // TODO race condition: the entry could be updated between the get and delete!\n-                            Metadata metadata = (Metadata) unmarshall(MarshallUtil.toByteArray(mv.getMetadataBytes()));\n-                            if (MarshallableEntryImpl.isExpired(metadata, now, mv.getCreated(), mv.getLastUsed())) {\n-                                // somewhat inefficient to FIND then REMOVE...\n-                                db.delete(handle, keyBytes);\n-                                purgeListener.entryPurged(key);\n-                                count++;\n-                            }\n+                           // TODO race condition: the entry could be updated between the get and delete!\n+                           Metadata metadata = unmarshall(MarshallUtil.toByteArray(mv.getMetadataBytes()));\n+                           if (MarshallableEntryImpl.isExpired(metadata, now, mv.getCreated(), mv.getLastUsed())) {\n+                              // somewhat inefficient to FIND then REMOVE... but required if the value is updated\n+                              db.delete(handle, keyBytes);\n+                              processor.onNext(entryFactory.create(key, mv));\n+                              count++;\n+                           }\n                         }\n-                    }\n-                    if (count != 0)\n+                     }\n+                     if (count != 0)\n                         log.debugf(\"purged %d entries\", count);\n-                } catch (Exception e) {\n-                    throw new PersistenceException(e);\n-                } finally {\n-                    readOptions.close();\n-                }\n-            }\n-        } catch (PersistenceException e) {\n-            throw e;\n-        } catch (Exception e) {\n-            throw new PersistenceException(e);\n-        } finally {\n-            semaphore.release();\n-        }\n-    }\n-\n-    @Override\n-    public void addSegments(IntSet segments) {\n-        handler.addSegments(segments);\n-    }\n-\n-    @Override\n-    public void removeSegments(IntSet segments) {\n-        handler.removeSegments(segments);\n-    }\n-\n-    private byte[] marshall(Object entry) throws IOException, InterruptedException {\n-        return marshaller.objectToByteBuffer(entry);\n-    }\n-\n-    private Object unmarshall(byte[] bytes) throws IOException, ClassNotFoundException {\n-        if (bytes == null)\n-            return null;\n-\n-        return marshaller.objectFromByteBuffer(bytes);\n-    }\n-\n-    private MarshallableEntry<K, V> valueToMarshallableEntry(Object key, byte[] valueBytes, boolean fetchMeta) throws IOException, ClassNotFoundException {\n-        MarshalledValue value = (MarshalledValue) unmarshall(valueBytes);\n-        if (value == null) return null;\n-\n-        ByteBuffer metadataBytes = fetchMeta ? value.getMetadataBytes() : null;\n-        return entryFactory.create(key, value.getValueBytes(), metadataBytes, value.getInternalMetadataBytes(), value.getCreated(), value.getLastUsed());\n-    }\n-\n-    private void addNewExpiry(MarshallableEntry entry) throws RocksDBException, IOException, ClassNotFoundException {\n-        long expiry = entry.expiryTime();\n-        long maxIdle = entry.getMetadata().maxIdle();\n-        if (maxIdle > 0) {\n-            // Coding getExpiryTime() for transient entries has the risk of being a moving target\n-            // which could lead to unexpected results, hence, InternalCacheEntry calls are required\n-            expiry = maxIdle + ctx.getTimeService().wallClockTime();\n-        }\n-        try {\n-            byte[] keyBytes = entry.getKeyBytes().copy().getBuf();\n-            putExpireDbData(new ExpiryEntry(expiry, keyBytes));\n-        } catch (InterruptedException e) {\n-            Thread.currentThread().interrupt(); // Restore interruption status\n-        }\n-    }\n-\n-    @ProtoTypeId(ProtoStreamTypeIds.ROCKSDB_EXPIRY_BUCKET)\n-    static final class ExpiryBucket {\n-        @ProtoField(number = 1, collectionImplementation = ArrayList.class)\n-        List<byte[]> entries;\n-\n-        ExpiryBucket(){}\n-\n-        ExpiryBucket(byte[] existingKey, byte[] newKey) {\n-            entries = new ArrayList<>(2);\n-            entries.add(existingKey);\n-            entries.add(newKey);\n-        }\n-    }\n-\n-    private static final class ExpiryEntry {\n-\n-        final long expiry;\n-        final byte[] keyBytes;\n-\n-        ExpiryEntry(long expiry, byte[] keyBytes) {\n-            this.expiry = expiry;\n-            this.keyBytes = keyBytes;\n-        }\n-\n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o) return true;\n-            if (o == null || getClass() != o.getClass()) return false;\n-            ExpiryEntry that = (ExpiryEntry) o;\n-            return expiry == that.expiry &&\n-                  Arrays.equals(keyBytes, that.keyBytes);\n-        }\n-\n-        @Override\n-        public int hashCode() {\n-            int result = Objects.hash(expiry);\n-            result = 31 * result + Arrays.hashCode(keyBytes);\n-            return result;\n-        }\n-    }\n-\n-    private class RocksKeyIterator extends AbstractIterator<K> {\n-        private final RocksIterator it;\n-        private final Predicate<? super K> filter;\n-\n-        public RocksKeyIterator(RocksIterator it, Predicate<? super K> filter) {\n-            this.it = it;\n-            this.filter = filter;\n-        }\n-\n-        @Override\n-        protected K getNext() {\n-            K key = null;\n-            try {\n-                while (key == null && it.isValid()) {\n-                    K testKey = (K) unmarshall(it.key());\n-                    if (filter == null || filter.test(testKey)) {\n-                        key = testKey;\n-                    }\n-                    it.next();\n-                }\n-            } catch (IOException | ClassNotFoundException e) {\n-                throw new CacheException(e);\n-            }\n-            return key;\n-        }\n-    }\n-\n-    private class RocksEntryIterator extends AbstractIterator<MarshallableEntry<K, V>> {\n-        private final RocksIterator it;\n-        private final Predicate<? super K> filter;\n-        private final boolean fetchValue;\n-        private final boolean fetchMetadata;\n-        private final long now;\n-\n-        public RocksEntryIterator(RocksIterator it, Predicate<? super K> filter, boolean fetchValue,\n-              boolean fetchMetadata, long now) {\n-            this.it = it;\n-            this.filter = filter;\n-            this.fetchValue = fetchValue;\n-            this.fetchMetadata = fetchMetadata;\n-            this.now = now;\n-        }\n-\n-        @Override\n-        protected MarshallableEntry<K, V> getNext() {\n-            MarshallableEntry<K, V> entry = null;\n-            try {\n-                while (entry == null && it.isValid()) {\n-                    K key = (K) unmarshall(it.key());\n-                    if (filter == null || filter.test(key)) {\n-                        if (fetchValue || fetchMetadata) {\n-                            MarshallableEntry<K, V> me = valueToMarshallableEntry(key, it.value(), fetchMetadata);\n-                            if (me != null && !me.isExpired(now)) {\n-                                entry = me;\n-                            }\n-                        } else {\n-                            entry = entryFactory.create(key);\n-                        }\n-                    }\n-                    it.next();\n-                }\n-            } catch (IOException | ClassNotFoundException e) {\n-                throw new CacheException(e);\n-            }\n-            return entry;\n-        }\n-    }\n-\n-    private abstract class RocksDBHandler {\n-\n-        abstract RocksDB open(Path location, DBOptions options) throws RocksDBException;\n-\n-        abstract void close();\n-\n-        abstract ColumnFamilyHandle getHandle(int segment);\n-\n-        final ColumnFamilyHandle getHandle(int segment, Object key) {\n-            if (segment < 0) {\n-                segment = calculateSegment(key);\n-            }\n-            return getHandle(segment);\n-        }\n-\n-        abstract int calculateSegment(Object key);\n-\n-        ColumnFamilyDescriptor newDescriptor(byte[] name) {\n-            ColumnFamilyOptions columnFamilyOptions;\n-            if (columnFamilyProperties != null) {\n-                columnFamilyOptions = ColumnFamilyOptions.getColumnFamilyOptionsFromProps(columnFamilyProperties);\n-                if (columnFamilyOptions == null) {\n-                    throw log.rocksDBUnknownPropertiesSupplied(columnFamilyProperties.toString());\n-                }\n+                  } catch (Exception e) {\n+                     throw new PersistenceException(e);\n+                  } finally {\n+                     readOptions.close();\n+                  }\n+               }\n+            }\n+         }, \"rocksdb-purgeExpired\").whenComplete((ignore, t) -> {\n+            if (t != null) {\n+               processor.onError(t);\n             } else {\n-                columnFamilyOptions = new ColumnFamilyOptions();\n-            }\n-            return new ColumnFamilyDescriptor(name,\n-                  columnFamilyOptions.setCompressionType(CompressionType.getCompressionType(configuration.compressionType().toString())));\n-        }\n-\n-        boolean contains(int segment, Object key) {\n-            // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n-            return load(segment, key) != null;\n-        }\n-\n-        MarshallableEntry<K, V> load(int segment, Object key) {\n-            ColumnFamilyHandle handle = getHandle(segment, key);\n-            if (handle == null) {\n-                log.trace(\"Ignoring load as handle is not currently configured\");\n-                return null;\n-            }\n-            try {\n-                byte[] entryBytes;\n-                semaphore.acquire();\n-                try {\n-                    if (stopped) {\n-                        throw new PersistenceException(\"RocksDB is stopped\");\n-                    }\n-\n-                    entryBytes = db.get(handle, marshall(key));\n-                } finally {\n-                    semaphore.release();\n-                }\n-                MarshallableEntry<K, V> me = valueToMarshallableEntry(key, entryBytes, true);\n-                if (me == null || me.isExpired(timeService.wallClockTime())) {\n-                    return null;\n-                }\n-                return me;\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n-\n-        void write(int segment, MarshallableEntry<? extends K, ? extends V> me) {\n-            Object key = me.getKey();\n-            ColumnFamilyHandle handle = getHandle(segment, key);\n-            if (handle == null) {\n-                log.trace(\"Ignoring write as handle is not currently configured\");\n-                return;\n-            }\n-            try {\n-                byte[] marshalledKey = MarshallUtil.toByteArray(me.getKeyBytes());\n-                byte[] marshalledValue = marshall(me.getMarshalledValue());\n-                semaphore.acquire();\n-                try {\n-                    if (stopped) {\n-                        throw new PersistenceException(\"RocksDB is stopped\");\n-                    }\n-                    db.put(handle, marshalledKey, marshalledValue);\n-                } finally {\n-                    semaphore.release();\n-                }\n-                if (me.expiryTime() > -1) {\n-                    addNewExpiry(me);\n-                }\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n-\n-        boolean delete(int segment, Object key) {\n-            try {\n-                byte[] keyBytes = marshall(key);\n-                semaphore.acquire();\n-                try {\n-                    if (stopped) {\n-                        throw new PersistenceException(\"RocksDB is stopped\");\n-                    }\n-                    if (db.get(getHandle(segment, key), keyBytes) == null) {\n-                        return false;\n-                    }\n-                    db.delete(getHandle(segment, key), keyBytes);\n-                } finally {\n-                    semaphore.release();\n-                }\n-                return true;\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n-\n-        CompletionStage<Void> writeBatch(Publisher<MarshallableEntry<? extends K, ? extends V>> publisher) {\n-            return Flowable.fromPublisher(publisher)\n-                  .buffer(configuration.maxBatchSize())\n-                  .doOnNext(entries -> {\n-                      WriteBatch batch = new WriteBatch();\n-                      for (MarshallableEntry<? extends K, ? extends V> entry : entries) {\n-                          int segment = calculateSegment(entry.getKey());\n-                          byte[] keyBytes = MarshallUtil.toByteArray(entry.getKeyBytes());\n-                          batch.put(getHandle(segment), keyBytes, marshall(entry.getMarshalledValue()));\n-                      }\n-                      writeBatch(batch);\n-\n-                      // Add metadata only after batch has been written\n-                      for (MarshallableEntry entry : entries) {\n-                          if (entry.expiryTime() > -1)\n-                              addNewExpiry(entry);\n-                      }\n-                  })\n-                  .doOnError(e -> {\n-                      throw new PersistenceException(e);\n-                  })\n-                  .ignoreElements()\n-                  .toCompletionStage(null);\n-        }\n-\n-        void deleteBatch(Iterable<Object> keys) {\n-            try {\n-                int batchSize = 0;\n-                WriteBatch batch = new WriteBatch();\n-                for (Object key : keys) {\n-                    batch.remove(getHandle(calculateSegment(key)), marshall(key));\n-                    batchSize++;\n-\n-                    if (batchSize == configuration.maxBatchSize()) {\n-                        batchSize = 0;\n-                        writeBatch(batch);\n-                        batch = new WriteBatch();\n-                    }\n-                }\n-\n-                if (batchSize != 0)\n-                    writeBatch(batch);\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n-\n-        abstract void clear(IntSet segments);\n-\n-        abstract Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter);\n-\n-        abstract Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                                                                   boolean fetchValue, boolean fetchMetadata);\n+               processor.onComplete();\n+            }\n+         });\n+         return processor;\n+      });\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> addSegments(IntSet segments) {\n+      return handler.addSegments(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> removeSegments(IntSet segments) {\n+      return handler.removeSegments(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Boolean> isAvailable() {\n+      return blockingManager.supplyBlocking(() -> getLocation().toFile().exists() && getExpirationLocation().toFile().exists(),\n+            \"rocksdb-available\");\n+   }\n+\n+   private abstract class RocksDBHandler {\n+\n+      abstract RocksDB open(Path location, DBOptions options) throws RocksDBException;\n+\n+      abstract void close();\n+\n+      abstract ColumnFamilyHandle getHandle(int segment);\n+\n+      abstract ColumnFamilyHandle getHandle(Object key);\n+\n+      ColumnFamilyDescriptor newDescriptor(byte[] name) {\n+         ColumnFamilyOptions columnFamilyOptions;\n+         if (columnFamilyProperties != null) {\n+            columnFamilyOptions = ColumnFamilyOptions.getColumnFamilyOptionsFromProps(columnFamilyProperties);\n+            if (columnFamilyOptions == null) {\n+               throw log.rocksDBUnknownPropertiesSupplied(columnFamilyProperties.toString());\n+            }\n+         } else {\n+            columnFamilyOptions = new ColumnFamilyOptions();\n+         }\n+         return new ColumnFamilyDescriptor(name,\n+               columnFamilyOptions.setCompressionType(CompressionType.getCompressionType(configuration.compressionType().toString())));\n+      }\n+\n+      CompletionStage<Boolean> contains(int segment, Object key) {\n+         // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n+         return load(segment, key)\n+               .thenApply(Objects::nonNull);\n+      }\n+\n+      CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n+         ColumnFamilyHandle handle = getHandle(segment);\n+         if (handle == null) {\n+            log.trace(\"Ignoring load as handle is not currently configured\");\n+            return CompletableFutures.completedNull();\n+         }\n+         try {\n+            CompletionStage<byte[]> entryByteStage = blockingManager.supplyBlocking(() -> {\n+               try {\n+                  return db.get(handle, marshall(key));\n+               } catch (RocksDBException e) {\n+                  throw new CompletionException(e);\n+               }\n+            }, \"rocksdb-load\");\n+            return entryByteStage.thenApply(entryBytes -> {\n+               MarshallableEntry<K, V> me = unmarshallEntry(key, entryBytes);\n+               if (me == null || me.isExpired(timeService.wallClockTime())) {\n+                  return null;\n+               }\n+               return me;\n+            });\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n+\n+      CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> me) {\n+         ColumnFamilyHandle handle = getHandle(segment);\n+         if (handle == null) {\n+            log.trace(\"Ignoring write as handle is not currently configured\");\n+            return CompletableFutures.completedNull();\n+         }\n+         try {\n+            byte[] marshalledKey = MarshallUtil.toByteArray(me.getKeyBytes());\n+            byte[] marshalledValue = marshall(me.getMarshalledValue());\n+            return blockingManager.runBlocking(() -> {\n+               try {\n+                  db.put(handle, marshalledKey, marshalledValue);\n+                  if (me.expiryTime() > -1) {\n+                     addNewExpiry(me);\n+                  }\n+               } catch (RocksDBException e) {\n+                  throw new PersistenceException(e);\n+               }\n+            }, \"rocksdb-write\");\n+\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n+\n+      CompletionStage<Boolean> delete(int segment, Object key) {\n+         try {\n+            byte[] keyBytes = marshall(key);\n+            ColumnFamilyHandle handle = getHandle(segment);\n+            return blockingManager.supplyBlocking(() -> {\n+               try {\n+                  if (db.get(handle, keyBytes) == null) {\n+                     return Boolean.FALSE;\n+                  }\n+                  db.delete(handle, keyBytes);\n+                  return Boolean.TRUE;\n+               } catch (RocksDBException e) {\n+                  throw new PersistenceException(e);\n+               }\n+            }, \"rocksdb-delete\");\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n \n-        int size(IntSet segments) {\n-            CompletionStage<Long> stage = Flowable.fromPublisher(publishKeys(segments, null))\n-                  .count().toCompletionStage();\n+      abstract CompletionStage<Void> clear();\n \n-            long count = CompletionStages.join(stage);\n-            if (count > Integer.MAX_VALUE) {\n-                return Integer.MAX_VALUE;\n-            }\n-            return (int) count;\n-        }\n-\n-        <P> Flowable<P> publish(int segment, Function<RocksIterator, Flowable<P>> function) {\n-            ReadOptions readOptions = new ReadOptions().setFillCache(false);\n-            return Flowable.using(() -> {\n-                semaphore.acquire();\n-                if (stopped) {\n-                    throw new PersistenceException(\"RocksDB is stopped\");\n-                }\n-                return wrapIterator(db, readOptions, segment);\n-            }, iterator -> {\n-                if (iterator == null) {\n-                    return Flowable.empty();\n-                }\n-                iterator.seekToFirst();\n-                return function.apply(iterator);\n-            }, iterator -> {\n-                if (iterator != null) {\n-                    iterator.close();\n-                }\n-                readOptions.close();\n-                semaphore.release();\n-            });\n-        }\n+      abstract Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n+            boolean fetchValue);\n \n-        abstract RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment);\n+      CompletionStage<Long> size(IntSet segments) {\n+         return Flowable.fromPublisher(publishKeys(segments, null))\n+               .count().toCompletionStage();\n+      }\n \n-        private void writeBatch(WriteBatch batch) throws InterruptedException, RocksDBException {\n-            semaphore.acquire();\n-            try {\n-                if (stopped)\n-                    throw new PersistenceException(\"RocksDB is stopped\");\n+      abstract CompletionStage<Long> approximateSize(IntSet segments);\n \n-                db.write(dataWriteOptions(), batch);\n-            } finally {\n-                batch.close();\n-                semaphore.release();\n+      <P> Publisher<P> publish(int segment, Function<RocksIterator, Flowable<P>> function) {\n+         ReadOptions readOptions = new ReadOptions().setFillCache(false);\n+         return blockingManager.blockingPublisher(Flowable.using(() -> wrapIterator(db, readOptions, segment), iterator -> {\n+            if (iterator == null) {\n+               return Flowable.<P>empty();\n             }\n-        }\n-\n-        abstract void addSegments(IntSet segments);\n-\n-        abstract void removeSegments(IntSet segments);\n-    }\n-\n-    private final class NonSegmentedRocksDBHandler extends RocksDBHandler {\n-        private final KeyPartitioner keyPartitioner;\n-        private ColumnFamilyHandle defaultColumnFamilyHandle;\n-\n-        public NonSegmentedRocksDBHandler(KeyPartitioner keyPartitioner) {\n-            this.keyPartitioner = keyPartitioner;\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandle(int segment) {\n-            return defaultColumnFamilyHandle;\n-        }\n-\n-        @Override\n-        int calculateSegment(Object key) {\n-            // Segment not used\n-            return 0;\n-        }\n-\n-        @Override\n-        RocksDB open(Path location, DBOptions options) throws RocksDBException {\n-            File dir = location.toFile();\n-            dir.mkdirs();\n-            List<ColumnFamilyHandle> handles = new ArrayList<>(1);\n-            RocksDB rocksDB = RocksDB.open(options, location.toString(),\n-                  Collections.singletonList(newDescriptor(RocksDB.DEFAULT_COLUMN_FAMILY)),\n-                  handles);\n-            defaultColumnFamilyHandle = handles.get(0);\n-            return rocksDB;\n-        }\n-\n-        @Override\n-        void clear(IntSet segments) {\n+            iterator.seekToFirst();\n+            return function.apply(iterator);\n+         }, iterator -> {\n+            if (iterator != null) {\n+               iterator.close();\n+            }\n+            readOptions.close();\n+         }));\n+      }\n+\n+      abstract RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment);\n+\n+      private void writeBatch(WriteBatch batch) throws RocksDBException {", "originalCommit": "be24dc2231fa8baed421fe4a622b5347481a67a9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "7bd3d9b10de1991887422123439cfdb9bd29e3ea", "url": "https://github.com/infinispan/infinispan/commit/7bd3d9b10de1991887422123439cfdb9bd29e3ea", "message": "ISPN-11931 Add blockhound to rocksdb module", "committedDate": "2020-06-03T18:05:53Z", "type": "forcePushed"}, {"oid": "5fbbd75ca94c8e4512a9a8bbe7f3b020bc90be03", "url": "https://github.com/infinispan/infinispan/commit/5fbbd75ca94c8e4512a9a8bbe7f3b020bc90be03", "message": "ISPN-11931 Add blockhound to rocksdb module", "committedDate": "2020-06-03T18:14:30Z", "type": "forcePushed"}, {"oid": "778113107c54287a4d8b96fbb31e08400de3304d", "url": "https://github.com/infinispan/infinispan/commit/778113107c54287a4d8b96fbb31e08400de3304d", "message": "ISPN-11931 Add blockhound to rocksdb module", "committedDate": "2020-06-03T20:38:24Z", "type": "forcePushed"}, {"oid": "0af866533d6dbaa15977c7cd57ed5e92e165865d", "url": "https://github.com/infinispan/infinispan/commit/0af866533d6dbaa15977c7cd57ed5e92e165865d", "message": "ISPN-11931 Add blockhound to rocksdb module", "committedDate": "2020-06-03T20:44:16Z", "type": "forcePushed"}, {"oid": "dfd1106c672cbba56b2c6b3ae7632a5624edec0c", "url": "https://github.com/infinispan/infinispan/commit/dfd1106c672cbba56b2c6b3ae7632a5624edec0c", "message": "ISPN-11931 Add blockhound to rocksdb module", "committedDate": "2020-06-04T02:15:11Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgwMjM4Mw==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r435802383", "bodyText": "Performing a full load seems wasteful here, as it means that the value has to be unmarshalled even though it is never used.", "author": "ryanemerson", "createdAt": "2020-06-05T09:29:00Z", "path": "persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java", "diffHunk": "@@ -187,271 +190,268 @@ protected RocksDB openDatabase(Path location, Options options) throws RocksDBExc\n     }\n \n     @Override\n-    public void stop() {\n-        try {\n-            semaphore.acquire(Integer.MAX_VALUE);\n-        } catch (InterruptedException e) {\n-            throw new PersistenceException(\"Cannot acquire semaphore\", e);\n-        }\n-        try {\n+    public CompletionStage<Void> stop() {\n+        return blockingManager.runBlocking(() -> {\n             handler.close();\n             expiredDb.close();\n-        } finally {\n-            stopped = true;\n-            semaphore.release(Integer.MAX_VALUE);\n-        }\n-    }\n-\n-    @Override\n-    public void destroy() {\n-        stop();\n-        Util.recursiveFileRemove(getLocation().toFile());\n-        Util.recursiveFileRemove(getExpirationLocation().toFile());\n+        }, \"rocksdb-stop\");\n     }\n \n     @Override\n-    public boolean isAvailable() {\n-        return getLocation().toFile().exists() && getExpirationLocation().toFile().exists();\n+    public Set<Characteristic> characteristics() {\n+        return EnumSet.of(Characteristic.BULK_READ, Characteristic.EXPIRATION, Characteristic.SEGMENTABLE);\n     }\n \n     @Override\n-    public void clear() {\n-        handler.clear(null);\n+    public CompletionStage<Boolean> isAvailable() {\n+        return blockingManager.supplyBlocking(() -> getLocation().toFile().exists() && getExpirationLocation().toFile().exists(),\n+                \"rocksdb-available\");\n     }\n \n     @Override\n-    public void clear(IntSet segments) {\n-        handler.clear(segments);\n+    public CompletionStage<Void> clear() {\n+        return handler.clear();\n     }\n \n     @Override\n-    public int size() {\n-        return handler.size(null);\n-    }\n-\n-    @Override\n-    public int size(IntSet segments) {\n+    public CompletionStage<Long> size(IntSet segments) {\n         return handler.size(segments);\n     }\n \n     @Override\n-    public boolean contains(Object key) {\n-        return handler.contains(-1, key);\n+    public CompletionStage<Long> approximateSize(IntSet segments) {\n+        return handler.approximateSize(segments);\n     }\n \n     @Override\n-    public boolean contains(int segment, Object key) {\n-        return handler.contains(segment, key);\n-    }\n-\n-    @Override\n-    public Publisher<K> publishKeys(Predicate<? super K> filter) {\n-        return handler.publishKeys(null, filter);\n+    public CompletionStage<Boolean> containsKey(int segment, Object key) {\n+        // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n+        return load(segment, key)", "originalCommit": "e8e91c3ad807c6bdba1406c83b12f4bf79650577", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE0MTExNA==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r436141114", "bodyText": "This is the same as it was prior, but I can see if I can add it easily enough.", "author": "wburns", "createdAt": "2020-06-05T20:08:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgwMjM4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgwOTAxMg==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r435809012", "bodyText": "As publishEntries utilises the RocksEntryIterator we're also unnecessarily unmarshalling the value. In this case I think we can simply add a boolean to the RocksEntryIterator constructor and pass the fetchValue parameter to it.", "author": "ryanemerson", "createdAt": "2020-06-05T09:41:04Z", "path": "persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java", "diffHunk": "@@ -187,271 +190,268 @@ protected RocksDB openDatabase(Path location, Options options) throws RocksDBExc\n     }\n \n     @Override\n-    public void stop() {\n-        try {\n-            semaphore.acquire(Integer.MAX_VALUE);\n-        } catch (InterruptedException e) {\n-            throw new PersistenceException(\"Cannot acquire semaphore\", e);\n-        }\n-        try {\n+    public CompletionStage<Void> stop() {\n+        return blockingManager.runBlocking(() -> {\n             handler.close();\n             expiredDb.close();\n-        } finally {\n-            stopped = true;\n-            semaphore.release(Integer.MAX_VALUE);\n-        }\n-    }\n-\n-    @Override\n-    public void destroy() {\n-        stop();\n-        Util.recursiveFileRemove(getLocation().toFile());\n-        Util.recursiveFileRemove(getExpirationLocation().toFile());\n+        }, \"rocksdb-stop\");\n     }\n \n     @Override\n-    public boolean isAvailable() {\n-        return getLocation().toFile().exists() && getExpirationLocation().toFile().exists();\n+    public Set<Characteristic> characteristics() {\n+        return EnumSet.of(Characteristic.BULK_READ, Characteristic.EXPIRATION, Characteristic.SEGMENTABLE);\n     }\n \n     @Override\n-    public void clear() {\n-        handler.clear(null);\n+    public CompletionStage<Boolean> isAvailable() {\n+        return blockingManager.supplyBlocking(() -> getLocation().toFile().exists() && getExpirationLocation().toFile().exists(),\n+                \"rocksdb-available\");\n     }\n \n     @Override\n-    public void clear(IntSet segments) {\n-        handler.clear(segments);\n+    public CompletionStage<Void> clear() {\n+        return handler.clear();\n     }\n \n     @Override\n-    public int size() {\n-        return handler.size(null);\n-    }\n-\n-    @Override\n-    public int size(IntSet segments) {\n+    public CompletionStage<Long> size(IntSet segments) {\n         return handler.size(segments);\n     }\n \n     @Override\n-    public boolean contains(Object key) {\n-        return handler.contains(-1, key);\n+    public CompletionStage<Long> approximateSize(IntSet segments) {\n+        return handler.approximateSize(segments);\n     }\n \n     @Override\n-    public boolean contains(int segment, Object key) {\n-        return handler.contains(segment, key);\n-    }\n-\n-    @Override\n-    public Publisher<K> publishKeys(Predicate<? super K> filter) {\n-        return handler.publishKeys(null, filter);\n+    public CompletionStage<Boolean> containsKey(int segment, Object key) {\n+        // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n+        return load(segment, key)\n+                .thenApply(Objects::nonNull);\n     }\n \n     @Override\n     public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n-        return handler.publishKeys(segments, filter);\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> entryPublisher(Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata) {\n-        return handler.publishEntries(null, filter, fetchValue, fetchMetadata);\n+        return Flowable.fromPublisher(handler.publishEntries(segments, filter, false))\n+                .map(MarshallableEntry::getKey);", "originalCommit": "e8e91c3ad807c6bdba1406c83b12f4bf79650577", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE0MjMzMA==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r436142330", "bodyText": "I looked closely at this one, unfortunately due to the contract of keyPublisher with expiration and the way values are stored we have to always unmarshall the value to get the metadata, so I just consolidated it.", "author": "wburns", "createdAt": "2020-06-05T20:12:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgwOTAxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgxMDc0NQ==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r435810745", "bodyText": "I missed this before, but I don't think boolean includeValues should be part of the publishEntries parameters. If you don't require the value, then you should just utilise publishKeys.\nProbably irrelavant now, but boolean includeValues has no Javadocs in the interface.", "author": "ryanemerson", "createdAt": "2020-06-05T09:44:03Z", "path": "persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java", "diffHunk": "@@ -187,271 +190,268 @@ protected RocksDB openDatabase(Path location, Options options) throws RocksDBExc\n     }\n \n     @Override\n-    public void stop() {\n-        try {\n-            semaphore.acquire(Integer.MAX_VALUE);\n-        } catch (InterruptedException e) {\n-            throw new PersistenceException(\"Cannot acquire semaphore\", e);\n-        }\n-        try {\n+    public CompletionStage<Void> stop() {\n+        return blockingManager.runBlocking(() -> {\n             handler.close();\n             expiredDb.close();\n-        } finally {\n-            stopped = true;\n-            semaphore.release(Integer.MAX_VALUE);\n-        }\n-    }\n-\n-    @Override\n-    public void destroy() {\n-        stop();\n-        Util.recursiveFileRemove(getLocation().toFile());\n-        Util.recursiveFileRemove(getExpirationLocation().toFile());\n+        }, \"rocksdb-stop\");\n     }\n \n     @Override\n-    public boolean isAvailable() {\n-        return getLocation().toFile().exists() && getExpirationLocation().toFile().exists();\n+    public Set<Characteristic> characteristics() {\n+        return EnumSet.of(Characteristic.BULK_READ, Characteristic.EXPIRATION, Characteristic.SEGMENTABLE);\n     }\n \n     @Override\n-    public void clear() {\n-        handler.clear(null);\n+    public CompletionStage<Boolean> isAvailable() {\n+        return blockingManager.supplyBlocking(() -> getLocation().toFile().exists() && getExpirationLocation().toFile().exists(),\n+                \"rocksdb-available\");\n     }\n \n     @Override\n-    public void clear(IntSet segments) {\n-        handler.clear(segments);\n+    public CompletionStage<Void> clear() {\n+        return handler.clear();\n     }\n \n     @Override\n-    public int size() {\n-        return handler.size(null);\n-    }\n-\n-    @Override\n-    public int size(IntSet segments) {\n+    public CompletionStage<Long> size(IntSet segments) {\n         return handler.size(segments);\n     }\n \n     @Override\n-    public boolean contains(Object key) {\n-        return handler.contains(-1, key);\n+    public CompletionStage<Long> approximateSize(IntSet segments) {\n+        return handler.approximateSize(segments);\n     }\n \n     @Override\n-    public boolean contains(int segment, Object key) {\n-        return handler.contains(segment, key);\n-    }\n-\n-    @Override\n-    public Publisher<K> publishKeys(Predicate<? super K> filter) {\n-        return handler.publishKeys(null, filter);\n+    public CompletionStage<Boolean> containsKey(int segment, Object key) {\n+        // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n+        return load(segment, key)\n+                .thenApply(Objects::nonNull);\n     }\n \n     @Override\n     public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n-        return handler.publishKeys(segments, filter);\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> entryPublisher(Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata) {\n-        return handler.publishEntries(null, filter, fetchValue, fetchMetadata);\n+        return Flowable.fromPublisher(handler.publishEntries(segments, filter, false))\n+                .map(MarshallableEntry::getKey);\n     }\n \n     @Override\n-    public Publisher<MarshallableEntry<K, V>> entryPublisher(IntSet segments, Predicate<? super K> filter,\n-                                                             boolean fetchValue, boolean fetchMetadata) {\n-        return handler.publishEntries(segments, filter, fetchValue, fetchMetadata);\n+    public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {", "originalCommit": "e8e91c3ad807c6bdba1406c83b12f4bf79650577", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE0MjYyMg==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r436142622", "bodyText": "You can use publishEntries with includeValues as false to get the metadata still (although  most stores may still return the value).", "author": "wburns", "createdAt": "2020-06-05T20:12:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgxMDc0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjU3OTgxMQ==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r436579811", "bodyText": "Ok, in that case publishEntries is still missing the JavDoc for the includeValues parameter. As part of the main javadocs \"body\" I think you should include your explanation of why !includeValues is useful and that some stores may still return the value in MarshallableEntry.", "author": "ryanemerson", "createdAt": "2020-06-08T09:51:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgxMDc0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgxNTMwMQ==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r435815301", "bodyText": "Nitpick, but IMO it's better to return early if it's not valid to reduce nesting.\n    if (!iterator.isValid())\n        return null;", "author": "ryanemerson", "createdAt": "2020-06-05T09:52:48Z", "path": "persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java", "diffHunk": "@@ -187,271 +190,268 @@ protected RocksDB openDatabase(Path location, Options options) throws RocksDBExc\n     }\n \n     @Override\n-    public void stop() {\n-        try {\n-            semaphore.acquire(Integer.MAX_VALUE);\n-        } catch (InterruptedException e) {\n-            throw new PersistenceException(\"Cannot acquire semaphore\", e);\n-        }\n-        try {\n+    public CompletionStage<Void> stop() {\n+        return blockingManager.runBlocking(() -> {\n             handler.close();\n             expiredDb.close();\n-        } finally {\n-            stopped = true;\n-            semaphore.release(Integer.MAX_VALUE);\n-        }\n-    }\n-\n-    @Override\n-    public void destroy() {\n-        stop();\n-        Util.recursiveFileRemove(getLocation().toFile());\n-        Util.recursiveFileRemove(getExpirationLocation().toFile());\n+        }, \"rocksdb-stop\");\n     }\n \n     @Override\n-    public boolean isAvailable() {\n-        return getLocation().toFile().exists() && getExpirationLocation().toFile().exists();\n+    public Set<Characteristic> characteristics() {\n+        return EnumSet.of(Characteristic.BULK_READ, Characteristic.EXPIRATION, Characteristic.SEGMENTABLE);\n     }\n \n     @Override\n-    public void clear() {\n-        handler.clear(null);\n+    public CompletionStage<Boolean> isAvailable() {\n+        return blockingManager.supplyBlocking(() -> getLocation().toFile().exists() && getExpirationLocation().toFile().exists(),\n+                \"rocksdb-available\");\n     }\n \n     @Override\n-    public void clear(IntSet segments) {\n-        handler.clear(segments);\n+    public CompletionStage<Void> clear() {\n+        return handler.clear();\n     }\n \n     @Override\n-    public int size() {\n-        return handler.size(null);\n-    }\n-\n-    @Override\n-    public int size(IntSet segments) {\n+    public CompletionStage<Long> size(IntSet segments) {\n         return handler.size(segments);\n     }\n \n     @Override\n-    public boolean contains(Object key) {\n-        return handler.contains(-1, key);\n+    public CompletionStage<Long> approximateSize(IntSet segments) {\n+        return handler.approximateSize(segments);\n     }\n \n     @Override\n-    public boolean contains(int segment, Object key) {\n-        return handler.contains(segment, key);\n-    }\n-\n-    @Override\n-    public Publisher<K> publishKeys(Predicate<? super K> filter) {\n-        return handler.publishKeys(null, filter);\n+    public CompletionStage<Boolean> containsKey(int segment, Object key) {\n+        // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n+        return load(segment, key)\n+                .thenApply(Objects::nonNull);\n     }\n \n     @Override\n     public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n-        return handler.publishKeys(segments, filter);\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> entryPublisher(Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata) {\n-        return handler.publishEntries(null, filter, fetchValue, fetchMetadata);\n+        return Flowable.fromPublisher(handler.publishEntries(segments, filter, false))\n+                .map(MarshallableEntry::getKey);\n     }\n \n     @Override\n-    public Publisher<MarshallableEntry<K, V>> entryPublisher(IntSet segments, Predicate<? super K> filter,\n-                                                             boolean fetchValue, boolean fetchMetadata) {\n-        return handler.publishEntries(segments, filter, fetchValue, fetchMetadata);\n+    public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n+        return handler.publishEntries(segments, filter, includeValues);\n     }\n \n     @Override\n-    public boolean delete(Object key) {\n-        return handler.delete(-1, key);\n-    }\n-\n-    @Override\n-    public boolean delete(int segment, Object key) {\n+    public CompletionStage<Boolean> delete(int segment, Object key) {\n         return handler.delete(segment, key);\n     }\n \n     @Override\n-    public void write(MarshallableEntry entry) {\n-        handler.write(-1, entry);\n-    }\n-\n-    @Override\n-    public void write(int segment, MarshallableEntry<? extends K, ? extends V> entry) {\n-        handler.write(segment, entry);\n+    public CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> entry) {\n+        return handler.write(segment, entry);\n     }\n \n     @Override\n-    public MarshallableEntry loadEntry(Object key) {\n-        return handler.load(-1, key);\n-    }\n-\n-    @Override\n-    public MarshallableEntry<K, V> get(int segment, Object key) {\n+    public CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n         return handler.load(segment, key);\n     }\n \n     @Override\n-    public CompletionStage<Void> bulkUpdate(Publisher<MarshallableEntry<? extends K, ? extends V>> publisher) {\n-        return handler.writeBatch(publisher);\n+    public CompletionStage<Void> batch(int publisherCount, Publisher<SegmentedPublisher<Object>> removePublisher,\n+            Publisher<SegmentedPublisher<MarshallableEntry<K, V>>> writePublisher) {\n+        WriteBatch batch = new WriteBatch();\n+        Set<MarshallableEntry<K, V>> expirableEntries = new HashSet<>();\n+        Flowable.fromPublisher(removePublisher)\n+                .subscribe(sp -> {\n+                    ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n+                    Flowable.fromPublisher(sp)\n+                            .subscribe(removed -> batch.delete(handle, marshall(removed)));\n+                });\n+        Flowable.fromPublisher(writePublisher)\n+                .subscribe(sp -> {\n+                    ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n+                    Flowable.fromPublisher(sp)\n+                            .subscribe(me -> {\n+                                batch.put(handle, marshall(me.getKey()), marshall(me.getMarshalledValue()));\n+                                if (me.expiryTime() > -1) {\n+                                    expirableEntries.add(me);\n+                                }\n+                            });\n+                });\n+        if (batch.count() <= 0) {\n+            batch.close();\n+            return CompletableFutures.completedNull();\n+        }\n+        return blockingManager.runBlocking(() -> {\n+            try {\n+                db.write(dataWriteOptions(), batch);\n+                for (MarshallableEntry<K, V> me : expirableEntries) {\n+                    addNewExpiry(me);\n+                }\n+            } catch (RocksDBException e) {\n+                throw new PersistenceException(e);\n+            }\n+        }, \"rocksdb-batch\").whenComplete((ignore, t) -> batch.close());\n     }\n \n     @Override\n-    public void deleteBatch(Iterable<Object> keys) {\n-        handler.deleteBatch(keys);\n+    public Publisher<MarshallableEntry<K, V>> purgeExpired() {\n+        Publisher<List<MarshallableEntry<K, V>>> purgedBatches = blockingManager.blockingPublisher(Flowable.defer(() -> {\n+            // We check expiration based on time of subscription only\n+            long now = timeService.wallClockTime();\n+            return actualPurgeExpired(now)\n+                    // We return a buffer of expired entries emitted to the non blocking thread\n+                    // This prevents waking up the non blocking thread for every entry as they will most likely be\n+                    // consumed much faster than emission (since each emission performs a get and remove)\n+                    .buffer(16);\n+        }));\n+\n+        return Flowable.fromPublisher(purgedBatches)\n+                .concatMap(Flowable::fromIterable);\n     }\n \n-    private void putExpireDbData(ExpiryEntry entry) throws InterruptedException, RocksDBException, IOException,\n-       ClassNotFoundException {\n-        final byte[] expiryBytes = marshall(entry.expiry);\n-        final byte[] existingBytes = expiredDb.get(expiryBytes);\n-\n-        if (existingBytes != null) {\n-            // in the case of collision make the value a List ...\n-            final Object existing = unmarshall(existingBytes);\n-            if (existing instanceof ExpiryBucket) {\n-                ((ExpiryBucket) existing).entries.add(entry.keyBytes);\n-                expiredDb.put(expiryBytes, marshall(existing));\n+    private Flowable<MarshallableEntry<K, V>> actualPurgeExpired(long now) {\n+        // The following flowable is responsible for emitting entries that have expired from expiredDb and removing the\n+        // given entries\n+        Flowable<byte[]> expiredFlowable = Flowable.using(() -> {\n+            ReadOptions readOptions = new ReadOptions().setFillCache(false);\n+            return new AbstractMap.SimpleImmutableEntry<>(readOptions, expiredDb.newIterator(readOptions));\n+        }, entry -> {\n+            if (entry.getValue() == null) {\n+                return Flowable.empty();\n+            }\n+            RocksIterator iterator = entry.getValue();\n+            iterator.seekToFirst();\n+\n+            return Flowable.fromIterable(() ->\n+                    new AbstractIterator<byte[]>() {\n+                        @Override\n+                        protected byte[] getNext() {\n+                            if (iterator.isValid()) {", "originalCommit": "e8e91c3ad807c6bdba1406c83b12f4bf79650577", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgyOTg5NA==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r435829894", "bodyText": "For a segmented store there is no need to unmarshall all of the keys during the size calculation. Instead we can just return a  \"empty\" MarshalledEntry instance from RocksEntryIterator in order for count() to still be utilised.", "author": "ryanemerson", "createdAt": "2020-06-05T10:21:21Z", "path": "persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java", "diffHunk": "@@ -594,161 +549,92 @@ ColumnFamilyDescriptor newDescriptor(byte[] name) {\n                   columnFamilyOptions.setCompressionType(CompressionType.getCompressionType(configuration.compressionType().toString())));\n         }\n \n-        boolean contains(int segment, Object key) {\n-            // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n-            return load(segment, key) != null;\n-        }\n-\n-        MarshallableEntry<K, V> load(int segment, Object key) {\n-            ColumnFamilyHandle handle = getHandle(segment, key);\n+        CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n+            ColumnFamilyHandle handle = getHandle(segment);\n             if (handle == null) {\n                 log.trace(\"Ignoring load as handle is not currently configured\");\n-                return null;\n+                return CompletableFutures.completedNull();\n             }\n             try {\n-                byte[] entryBytes;\n-                semaphore.acquire();\n-                try {\n-                    if (stopped) {\n-                        throw new PersistenceException(\"RocksDB is stopped\");\n+                CompletionStage<byte[]> entryByteStage = blockingManager.supplyBlocking(() -> {\n+                    try {\n+                        return db.get(handle, marshall(key));\n+                    } catch (RocksDBException e) {\n+                        throw new CompletionException(e);\n                     }\n-\n-                    entryBytes = db.get(handle, marshall(key));\n-                } finally {\n-                    semaphore.release();\n-                }\n-                MarshallableEntry<K, V> me = valueToMarshallableEntry(key, entryBytes, true);\n-                if (me == null || me.isExpired(timeService.wallClockTime())) {\n-                    return null;\n-                }\n-                return me;\n+                }, \"rocksdb-load\");\n+                return entryByteStage.thenApply(entryBytes -> {\n+                    MarshallableEntry<K, V> me = unmarshallEntry(key, entryBytes);\n+                    if (me == null || me.isExpired(timeService.wallClockTime())) {\n+                        return null;\n+                    }\n+                    return me;\n+                });\n             } catch (Exception e) {\n                 throw new PersistenceException(e);\n             }\n         }\n \n-        void write(int segment, MarshallableEntry<? extends K, ? extends V> me) {\n-            Object key = me.getKey();\n-            ColumnFamilyHandle handle = getHandle(segment, key);\n+        CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> me) {\n+            ColumnFamilyHandle handle = getHandle(segment);\n             if (handle == null) {\n                 log.trace(\"Ignoring write as handle is not currently configured\");\n-                return;\n+                return CompletableFutures.completedNull();\n             }\n             try {\n                 byte[] marshalledKey = MarshallUtil.toByteArray(me.getKeyBytes());\n                 byte[] marshalledValue = marshall(me.getMarshalledValue());\n-                semaphore.acquire();\n-                try {\n-                    if (stopped) {\n-                        throw new PersistenceException(\"RocksDB is stopped\");\n+                return blockingManager.runBlocking(() -> {\n+                    try {\n+                        db.put(handle, marshalledKey, marshalledValue);\n+                        if (me.expiryTime() > -1) {\n+                            addNewExpiry(me);\n+                        }\n+                    } catch (RocksDBException e) {\n+                        throw new PersistenceException(e);\n                     }\n-                    db.put(handle, marshalledKey, marshalledValue);\n-                } finally {\n-                    semaphore.release();\n-                }\n-                if (me.expiryTime() > -1) {\n-                    addNewExpiry(me);\n-                }\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n+                }, \"rocksdb-write\");\n \n-        boolean delete(int segment, Object key) {\n-            try {\n-                byte[] keyBytes = marshall(key);\n-                semaphore.acquire();\n-                try {\n-                    if (stopped) {\n-                        throw new PersistenceException(\"RocksDB is stopped\");\n-                    }\n-                    if (db.get(getHandle(segment, key), keyBytes) == null) {\n-                        return false;\n-                    }\n-                    db.delete(getHandle(segment, key), keyBytes);\n-                } finally {\n-                    semaphore.release();\n-                }\n-                return true;\n             } catch (Exception e) {\n                 throw new PersistenceException(e);\n             }\n         }\n \n-        CompletionStage<Void> writeBatch(Publisher<MarshallableEntry<? extends K, ? extends V>> publisher) {\n-            return Flowable.fromPublisher(publisher)\n-                  .buffer(configuration.maxBatchSize())\n-                  .doOnNext(entries -> {\n-                      WriteBatch batch = new WriteBatch();\n-                      for (MarshallableEntry<? extends K, ? extends V> entry : entries) {\n-                          int segment = calculateSegment(entry.getKey());\n-                          byte[] keyBytes = MarshallUtil.toByteArray(entry.getKeyBytes());\n-                          batch.put(getHandle(segment), keyBytes, marshall(entry.getMarshalledValue()));\n-                      }\n-                      writeBatch(batch);\n-\n-                      // Add metadata only after batch has been written\n-                      for (MarshallableEntry entry : entries) {\n-                          if (entry.expiryTime() > -1)\n-                              addNewExpiry(entry);\n-                      }\n-                  })\n-                  .doOnError(e -> {\n-                      throw new PersistenceException(e);\n-                  })\n-                  .ignoreElements()\n-                  .toCompletionStage(null);\n-        }\n-\n-        void deleteBatch(Iterable<Object> keys) {\n+        CompletionStage<Boolean> delete(int segment, Object key) {\n             try {\n-                int batchSize = 0;\n-                WriteBatch batch = new WriteBatch();\n-                for (Object key : keys) {\n-                    batch.remove(getHandle(calculateSegment(key)), marshall(key));\n-                    batchSize++;\n-\n-                    if (batchSize == configuration.maxBatchSize()) {\n-                        batchSize = 0;\n-                        writeBatch(batch);\n-                        batch = new WriteBatch();\n+                byte[] keyBytes = marshall(key);\n+                ColumnFamilyHandle handle = getHandle(segment);\n+                return blockingManager.supplyBlocking(() -> {\n+                    try {\n+                        if (db.get(handle, keyBytes) == null) {\n+                            return Boolean.FALSE;\n+                        }\n+                        db.delete(handle, keyBytes);\n+                        return Boolean.TRUE;\n+                    } catch (RocksDBException e) {\n+                        throw new PersistenceException(e);\n                     }\n-                }\n-\n-                if (batchSize != 0)\n-                    writeBatch(batch);\n+                }, \"rocksdb-delete\");\n             } catch (Exception e) {\n                 throw new PersistenceException(e);\n             }\n         }\n \n-        abstract void clear(IntSet segments);\n-\n-        abstract Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter);\n+        abstract CompletionStage<Void> clear();\n \n         abstract Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                                                                   boolean fetchValue, boolean fetchMetadata);\n-\n-        int size(IntSet segments) {\n-            CompletionStage<Long> stage = Flowable.fromPublisher(publishKeys(segments, null))\n-                  .count().toCompletionStage();\n+                boolean fetchValue);\n \n-            long count = CompletionStages.join(stage);\n-            if (count > Integer.MAX_VALUE) {\n-                return Integer.MAX_VALUE;\n-            }\n-            return (int) count;\n+        CompletionStage<Long> size(IntSet segments) {\n+            return Flowable.fromPublisher(publishKeys(segments, null))", "originalCommit": "e8e91c3ad807c6bdba1406c83b12f4bf79650577", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE0NDIzOQ==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r436144239", "bodyText": "Unfortunately, expiration rears its ugly head again (we need to get at the metadata for expiration) - and at that point we require unmarshalling the value - so I don't think it is worth wasting code to not unmarshall the key.\nThis is one reason I was very glad to add in the approximateSize method (which won't work for non shared non segmented stores - shared or segmented should always work).", "author": "wburns", "createdAt": "2020-06-05T20:16:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgyOTg5NA=="}], "type": "inlineReview"}, {"oid": "e8fa6bdc48daafc58e4263bc903bb422571478b4", "url": "https://github.com/infinispan/infinispan/commit/e8fa6bdc48daafc58e4263bc903bb422571478b4", "message": "fixup", "committedDate": "2020-06-05T20:21:18Z", "type": "forcePushed"}, {"oid": "bda05885fa9402829f90bb45dc8230ebe6f66057", "url": "https://github.com/infinispan/infinispan/commit/bda05885fa9402829f90bb45dc8230ebe6f66057", "message": "Iterator indentation", "committedDate": "2020-06-05T20:23:05Z", "type": "forcePushed"}, {"oid": "b49b25911980f3f11c9534e811567496fe192ea9", "url": "https://github.com/infinispan/infinispan/commit/b49b25911980f3f11c9534e811567496fe192ea9", "message": "ISPN-11931 Add blockhound to rocksdb module", "committedDate": "2020-06-08T14:04:04Z", "type": "forcePushed"}, {"oid": "4f2898b3fc33c76e1ce14577dd980a7335274f68", "url": "https://github.com/infinispan/infinispan/commit/4f2898b3fc33c76e1ce14577dd980a7335274f68", "message": "ISPN-11930 Convert RocksDBStore to new Store SPI\n\n* Fix indentation of existing file (no actual code changes)", "committedDate": "2020-06-09T13:25:30Z", "type": "commit"}, {"oid": "4a22922d3209fb73a9e935dbff122953a63212a7", "url": "https://github.com/infinispan/infinispan/commit/4a22922d3209fb73a9e935dbff122953a63212a7", "message": "ISPN-11930 Convert RocksDBStore to new Store SPI", "committedDate": "2020-06-09T13:45:24Z", "type": "commit"}, {"oid": "efae2dba4fa4dfb9393e67e90706a2fc277521e9", "url": "https://github.com/infinispan/infinispan/commit/efae2dba4fa4dfb9393e67e90706a2fc277521e9", "message": "ISPN-11931 Add blockhound to rocksdb module", "committedDate": "2020-06-09T13:45:24Z", "type": "commit"}, {"oid": "efae2dba4fa4dfb9393e67e90706a2fc277521e9", "url": "https://github.com/infinispan/infinispan/commit/efae2dba4fa4dfb9393e67e90706a2fc277521e9", "message": "ISPN-11931 Add blockhound to rocksdb module", "committedDate": "2020-06-09T13:45:24Z", "type": "forcePushed"}]}