{"pr_number": 1436, "pr_title": "Feature/Adds result types ArrowStream and ArrowFile", "pr_createdAt": "2020-11-18T17:49:49Z", "pr_url": "https://github.com/bakdata/conquery/pull/1436", "timeline": [{"oid": "d733e454016f97a01b74eeea10d4f95c47016386", "url": "https://github.com/bakdata/conquery/commit/d733e454016f97a01b74eeea10d4f95c47016386", "message": "wip arrow test", "committedDate": "2020-11-11T23:38:28Z", "type": "commit"}, {"oid": "08b444adfcddb5ea01d90c23b5b0ea2dc51ab0f3", "url": "https://github.com/bakdata/conquery/commit/08b444adfcddb5ea01d90c23b5b0ea2dc51ab0f3", "message": "more work on the test", "committedDate": "2020-11-13T08:33:31Z", "type": "commit"}, {"oid": "a52dcf10b18bf33b81caf8c79efec7a417cfc95d", "url": "https://github.com/bakdata/conquery/commit/a52dcf10b18bf33b81caf8c79efec7a417cfc95d", "message": "put writers into a pipeline for save fiel access", "committedDate": "2020-11-13T15:23:37Z", "type": "commit"}, {"oid": "7e09526c7a6b1e020773dee3ff3b2bcc0b666c8a", "url": "https://github.com/bakdata/conquery/commit/7e09526c7a6b1e020773dee3ff3b2bcc0b666c8a", "message": "added support for all ResultTypes", "committedDate": "2020-11-17T10:45:16Z", "type": "commit"}, {"oid": "d10ce157e9321c4b26ceea85ab321fcc7ba5a07a", "url": "https://github.com/bakdata/conquery/commit/d10ce157e9321c4b26ceea85ab321fcc7ba5a07a", "message": "adds vector generation for idmapping and query renderer", "committedDate": "2020-11-17T15:52:16Z", "type": "commit"}, {"oid": "893c0a6bdb2d6f1834d90bfa88210488d6ced15a", "url": "https://github.com/bakdata/conquery/commit/893c0a6bdb2d6f1834d90bfa88210488d6ced15a", "message": "more complete testing and cleanup", "committedDate": "2020-11-17T17:42:30Z", "type": "commit"}, {"oid": "7a6a9bc00d51b22283020f4d06e1730d163a88eb", "url": "https://github.com/bakdata/conquery/commit/7a6a9bc00d51b22283020f4d06e1730d163a88eb", "message": "abstract the arrow writer", "committedDate": "2020-11-17T17:52:18Z", "type": "commit"}, {"oid": "ed32480b94170beb897d6a397ef7803ed68e4a6e", "url": "https://github.com/bakdata/conquery/commit/ed32480b94170beb897d6a397ef7803ed68e4a6e", "message": "add multiline result to test", "committedDate": "2020-11-18T17:48:29Z", "type": "commit"}, {"oid": "eb7c023789b2b369ebe08677af2b55a55083b5c2", "url": "https://github.com/bakdata/conquery/commit/eb7c023789b2b369ebe08677af2b55a55083b5c2", "message": "add endpoints", "committedDate": "2020-11-18T17:48:43Z", "type": "commit"}, {"oid": "8916b9fc06909b7698cbb6edfccb6c7d38240f43", "url": "https://github.com/bakdata/conquery/commit/8916b9fc06909b7698cbb6edfccb6c7d38240f43", "message": "make members explicitly static as the compiler maven uses has problems with @UtilityClass", "committedDate": "2020-11-18T18:03:11Z", "type": "commit"}, {"oid": "6058a202853514cc128ba7d9c0418f984aef031c", "url": "https://github.com/bakdata/conquery/commit/6058a202853514cc128ba7d9c0418f984aef031c", "message": "automatic update to docs", "committedDate": "2020-11-18T18:05:36Z", "type": "commit"}, {"oid": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d", "url": "https://github.com/bakdata/conquery/commit/3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d", "message": "adds new endpoints to test", "committedDate": "2020-11-19T08:45:36Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUxNzYxOA==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527517618", "bodyText": "Die sind nirgends in Arrow codiert?", "author": "awildturtok", "createdAt": "2020-11-20T08:22:24Z", "path": "backend/src/main/java/com/bakdata/conquery/apiv1/AdditionalMediaTypes.java", "diffHunk": "@@ -5,4 +5,8 @@\n \tstatic final String JSON = \"application/json; charset=utf-8\";\n \n \tstatic final String CSV = \"text/csv; charset=utf-8\";\n+\t\n+\t// As proposed in https://issues.apache.org/jira/browse/ARROW-7396", "originalCommit": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU4NjkwNg==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527586906", "bodyText": "Ist ja erstmal nur ein vorschlag. Das ist eine politische Entscheidung den Typ zu standardisieren", "author": "thoniTUB", "createdAt": "2020-11-20T10:11:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUxNzYxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUxODQxOQ==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527518419", "bodyText": "Das solltest du am besten auch von au\u00dfen reinreichen, und dann via DI an die richtigen Stellen injecten", "author": "awildturtok", "createdAt": "2020-11-20T08:24:01Z", "path": "backend/src/main/java/com/bakdata/conquery/io/result/ResultUtil.java", "diffHunk": "@@ -0,0 +1,29 @@\n+package com.bakdata.conquery.io.result;\n+\n+import java.util.Arrays;\n+import java.util.Collection;\n+\n+import com.bakdata.conquery.models.config.ConqueryConfig;\n+import com.bakdata.conquery.models.dictionary.DirectDictionary;\n+import com.bakdata.conquery.models.identifiable.mapping.CsvEntityId;\n+import com.bakdata.conquery.models.identifiable.mapping.ExternalEntityId;\n+import com.bakdata.conquery.models.identifiable.mapping.IdMappingConfig;\n+import com.bakdata.conquery.models.identifiable.mapping.IdMappingState;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.bakdata.conquery.models.worker.Namespace;\n+\n+public class ResultUtil {\n+\n+\tpublic static final IdMappingConfig ID_MAPPING = ConqueryConfig.getInstance().getIdMapping();", "originalCommit": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUxODgwMg==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527518802", "bodyText": "Config?", "author": "awildturtok", "createdAt": "2020-11-20T08:24:43Z", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;", "originalCommit": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYyMjAxMg==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527622012", "bodyText": "kann jetzt in der config eingestellt werden", "author": "thoniTUB", "createdAt": "2020-11-20T11:13:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUxODgwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUxOTY2Mg==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527519662", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\tlog.info(\"Writing final batch\");\n          \n          \n            \n            \t\tlog.debug(\"Writing final batch\");", "author": "awildturtok", "createdAt": "2020-11-20T08:26:13Z", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");", "originalCommit": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyMTQ1Ng==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527521456", "bodyText": "den null check kannst du mit komposition vorziehen, es w\u00fcrde sich denke ich auch anbieten die Methoden in einer Enum zu kapseln?", "author": "awildturtok", "createdAt": "2020-11-20T08:29:23Z", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {", "originalCommit": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzY3MDU3MA==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527670570", "bodyText": "Da die methoden vector.setNull(..) vector.setSafe(..) immer nur auf dem konkreten FieldVector definiert sind und nicht vererbt werden f\u00e4llt mir das schwer. Hast du eine Idee?", "author": "thoniTUB", "createdAt": "2020-11-20T12:51:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyMTQ1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzcwNDMxMw==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527704313", "bodyText": "ouh, das ist nat\u00fcrlich nervig :/", "author": "awildturtok", "createdAt": "2020-11-20T13:53:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyMTQ1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzcwODM2MQ==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527708361", "bodyText": "Die kommen von BaseFixed-/BaseVariableWidthVector, vlt kannst du da in einer parent klasse einen check drauf machen? aber ja sch\u00f6n ists nicht", "author": "awildturtok", "createdAt": "2020-11-20T13:59:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyMTQ1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzc4Njk3MQ==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527786971", "bodyText": "Bei vector.setNull(..) stimmt das bei setSafe(..) leider nicht. Ich w\u00fcrde es erstmal so lassen.", "author": "thoniTUB", "createdAt": "2020-11-20T15:56:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyMTQ1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyNDAyNQ==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527524025", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\tif(vector instanceof IntVector) {\n          \n          \n            \n                                    //TODO When Pattern-matching lands, clean this up. (Think Java 12?)\n          \n          \n            \n            \t\t\tif(vector instanceof IntVector) {", "author": "awildturtok", "createdAt": "2020-11-20T08:34:23Z", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer start = (n, r) -> {};\n+\t\tfor (int vecI = vectorOffset, resultPos = 0; vecI < root.getFieldVectors().size() && vecI < vectorOffset + numVectors; vecI++, resultPos++) {\n+\t\t\tfinal int pos = resultPos;\n+\t\t\tfinal FieldVector vector = root.getVector(vecI);\n+\t\t\t\n+\t\t\tif(vector instanceof IntVector) {", "originalCommit": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyNDk4OA==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527524988", "bodyText": "Ich kann mir vorstellen, dass dieses Method chaining sehr langsam ist, du k\u00f6nntest das invertieren indem du eine Liste an Funktionen pro Spalte hast.", "author": "awildturtok", "createdAt": "2020-11-20T08:36:09Z", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer start = (n, r) -> {};", "originalCommit": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzY1NzUwNQ==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527657505", "bodyText": "Ich benutze jetzt ein Array anstatt der Verkettung", "author": "thoniTUB", "createdAt": "2020-11-20T12:25:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyNDk4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyNjM1NQ==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527526355", "bodyText": "Ich glaube ich w\u00fcrde diesen switch in einer Map<Type,Function..> kapseln, bin mir aber nicht sicher, wie viel besser das ist. Es bietet sich aber auf jeden Fall an, das mapping in eine separate Funktion zu schieben.", "author": "awildturtok", "createdAt": "2020-11-20T08:38:48Z", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer start = (n, r) -> {};\n+\t\tfor (int vecI = vectorOffset, resultPos = 0; vecI < root.getFieldVectors().size() && vecI < vectorOffset + numVectors; vecI++, resultPos++) {\n+\t\t\tfinal int pos = resultPos;\n+\t\t\tfinal FieldVector vector = root.getVector(vecI);\n+\t\t\t\n+\t\t\tif(vector instanceof IntVector) {\n+\t\t\t\tstart = start.andThen(intVectorFiller((IntVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif(vector instanceof VarCharVector) {\n+\t\t\t\tstart = start.andThen(varCharVectorFiller((VarCharVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof BitVector) {\n+\t\t\t\tstart = start.andThen(bitVectorFiller((BitVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof Float4Vector) {\n+\t\t\t\tstart = start.andThen(float4VectorFiller((Float4Vector)vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof Float8Vector) {\n+\t\t\t\tstart = start.andThen(float8VectorFiller((Float8Vector)vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof DateDayVector) {\n+\t\t\t\tstart = start.andThen(dateDayVectorFiller((DateDayVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tthrow new UnsupportedOperationException(\"Vector type for writing result: \"+ vector.getClass());\n+\t\t}\n+\t\treturn start;\n+\t\t\n+\t}\n+\t\n+\tpublic static List<Field> generateFieldsFromIdMapping(String[] idHeaders){\n+\t\tPreconditions.checkArgument(idHeaders != null && idHeaders.length > 0, \"No id headers given\");\n+\n+\t\tImmutableList.Builder<Field> fields = ImmutableList.builder();\n+\t\t\n+\t\tfor(String header : idHeaders) {\n+\t\t\tfields.add(new Field(header, FieldType.nullable(new ArrowType.Utf8()), null));\n+\t\t}\n+\t\t\n+\t\treturn fields.build();\n+\t}\n+\t\n+\tpublic static List<Field> generateFieldsFromResultType(@NonNull List<ResultInfo> infos, PrintSettings settings) {\n+\t\t\n+\t\tImmutableList.Builder<Field> fields = ImmutableList.builder();\n+\t\t\n+\t\tfor(ResultInfo info : infos) {\n+\t\t\tswitch(info.getType()) {\n+\t\t\t\tcase BOOLEAN:", "originalCommit": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYzMDIxMQ==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527630211", "bodyText": "Ich extrahiere es aber lasse es erstmal beim switch. Kann dann auch durch Switch Expressions in java 12 ersetzt werden.", "author": "thoniTUB", "createdAt": "2020-11-20T11:29:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyNjM1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyNjY1Mw==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527526653", "bodyText": "was ist denn der letzte Parameter, der immer null ist?", "author": "awildturtok", "createdAt": "2020-11-20T08:39:28Z", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,250 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\n+\tprivate static final int BATCH_SIZE = 10;\n+\t\n+\tpublic static void renderToStream(Function<VectorSchemaRoot, ArrowWriter> writerProducer, PrintSettings cfg, ManagedQuery query, Function<ContainedEntityResult,String[]> idMapper, String[] idHeaders) throws IOException {\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer idPipeline = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer valuePipeline = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idPipeline, valuePipeline, idMapper, results);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(ArrowWriter writer, VectorSchemaRoot root, RowConsumer idPipeline, RowConsumer valuePipeline, Function<ContainedEntityResult,String[]> idMapper, List<ContainedEntityResult> results) throws IOException {\n+\t\tlog.info(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(BATCH_SIZE);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\t// Write id information\n+\t\t\t\tidPipeline.accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t// Write values\n+\t\t\t\tvaluePipeline.accept(batchLineCount, line);\n+\t\t\t\tbatchLineCount++;\n+\t\t\t}\n+\t\t\tif(batchLineCount >= BATCH_SIZE) {\t\t\t\t\n+\t\t\t\twriter.writeBatch();\n+\t\t\t\tbatchLineCount = 0;\n+\t\t\t}\n+\t\t}\n+\t\tlog.info(\"Writing final batch\");\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t}\n+\t\tlog.info(\"Finishing result write\");\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer start = (n, r) -> {};\n+\t\tfor (int vecI = vectorOffset, resultPos = 0; vecI < root.getFieldVectors().size() && vecI < vectorOffset + numVectors; vecI++, resultPos++) {\n+\t\t\tfinal int pos = resultPos;\n+\t\t\tfinal FieldVector vector = root.getVector(vecI);\n+\t\t\t\n+\t\t\tif(vector instanceof IntVector) {\n+\t\t\t\tstart = start.andThen(intVectorFiller((IntVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif(vector instanceof VarCharVector) {\n+\t\t\t\tstart = start.andThen(varCharVectorFiller((VarCharVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof BitVector) {\n+\t\t\t\tstart = start.andThen(bitVectorFiller((BitVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof Float4Vector) {\n+\t\t\t\tstart = start.andThen(float4VectorFiller((Float4Vector)vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof Float8Vector) {\n+\t\t\t\tstart = start.andThen(float8VectorFiller((Float8Vector)vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof DateDayVector) {\n+\t\t\t\tstart = start.andThen(dateDayVectorFiller((DateDayVector) vector, pos));\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tthrow new UnsupportedOperationException(\"Vector type for writing result: \"+ vector.getClass());\n+\t\t}\n+\t\treturn start;\n+\t\t\n+\t}\n+\t\n+\tpublic static List<Field> generateFieldsFromIdMapping(String[] idHeaders){\n+\t\tPreconditions.checkArgument(idHeaders != null && idHeaders.length > 0, \"No id headers given\");\n+\n+\t\tImmutableList.Builder<Field> fields = ImmutableList.builder();\n+\t\t\n+\t\tfor(String header : idHeaders) {\n+\t\t\tfields.add(new Field(header, FieldType.nullable(new ArrowType.Utf8()), null));\n+\t\t}\n+\t\t\n+\t\treturn fields.build();\n+\t}\n+\t\n+\tpublic static List<Field> generateFieldsFromResultType(@NonNull List<ResultInfo> infos, PrintSettings settings) {\n+\t\t\n+\t\tImmutableList.Builder<Field> fields = ImmutableList.builder();\n+\t\t\n+\t\tfor(ResultInfo info : infos) {\n+\t\t\tswitch(info.getType()) {\n+\t\t\t\tcase BOOLEAN:\n+\t\t\t\t\tfields.add(new Field(info.getUniqueName(settings), FieldType.nullable(ArrowType.Bool.INSTANCE), null));\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase CATEGORICAL:\n+\t\t\t\t\tfields.add(new Field(info.getUniqueName(settings), FieldType.nullable(new ArrowType.Utf8()), null));\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase DATE:\n+\t\t\t\t\tfields.add(NAMED_FIELD_DATE_DAY.apply(info.getUniqueName(settings)));\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase INTEGER:\n+\t\t\t\t\tfields.add(new Field(info.getUniqueName(settings), FieldType.nullable(new ArrowType.Int(32, true)), null));\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase MONEY:\n+\t\t\t\t\tfields.add(new Field(info.getUniqueName(settings), FieldType.nullable(new ArrowType.Int(32, true)), null));\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase NUMERIC:\n+\t\t\t\t\tfields.add(new Field(info.getUniqueName(settings), FieldType.nullable(new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE)), null));", "originalCommit": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU4ODgxMQ==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527588811", "bodyText": "Das sind die Felder von nested/complex Feldern. Wir haben aber erstmal noch keine Listen Typen oder Objekte", "author": "thoniTUB", "createdAt": "2020-11-20T10:14:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUyNjY1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUzNDYzOQ==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527534639", "bodyText": "yo-dawg", "author": "awildturtok", "createdAt": "2020-11-20T08:53:58Z", "path": "backend/src/main/java/com/bakdata/conquery/models/execution/ResultProcessor.java", "diffHunk": "@@ -61,6 +81,73 @@ public static ResponseBuilder getResult(User user, DatasetId datasetId, ManagedE\n \t\t\tConqueryMDC.clearLocation();\n \t\t}\n \t}\n+\t\n+\tpublic Response getArrowStreamResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowStreamWriter(root, new DictionaryProvider.MapDictionaryProvider(), output),", "originalCommit": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYzNjA3OQ==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527636079", "bodyText": "mini Abstraktion ;)", "author": "thoniTUB", "createdAt": "2020-11-20T11:41:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUzNDYzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUzNTQwMA==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527535400", "bodyText": "w\u00e4re das nicht die Aufgabe des callers?", "author": "awildturtok", "createdAt": "2020-11-20T08:55:15Z", "path": "backend/src/main/java/com/bakdata/conquery/models/execution/ResultProcessor.java", "diffHunk": "@@ -61,6 +81,73 @@ public static ResponseBuilder getResult(User user, DatasetId datasetId, ManagedE\n \t\t\tConqueryMDC.clearLocation();\n \t\t}\n \t}\n+\t\n+\tpublic Response getArrowStreamResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowStreamWriter(root, new DictionaryProvider.MapDictionaryProvider(), output),\n+\t\t\tuser,\n+\t\t\tqueryId,\n+\t\t\tdatasetId,\n+\t\t\tdatasetRegistry,\n+\t\t\tpretty);\n+\t}\n+\t\n+\tpublic Response getArrowFileResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowFileWriter(root, new DictionaryProvider.MapDictionaryProvider(), Channels.newChannel(output)),\n+\t\t\tuser,\n+\t\t\tqueryId,\n+\t\t\tdatasetId,\n+\t\t\tdatasetRegistry,\n+\t\t\tpretty);\n+\t}\n+\t\n+\t\n+\tprivate Response getArrowResult(\n+\t\tFunction<OutputStream, Function<VectorSchemaRoot,ArrowWriter>> writerProducer,\n+\t\tUser user,\n+\t\tManagedExecutionId queryId,\n+\t\tDatasetId datasetId,\n+\t\tDatasetRegistry datasetRegistry,\n+\t\tboolean pretty) {\n+\t\t\n+\t\tConqueryMDC.setLocation(user.getName());\n+\t\tlog.info(\"Downloading results for {} on dataset {}\", queryId, datasetId);", "originalCommit": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU4OTE4MA==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527589180", "bodyText": "Jap kann es hochziehen", "author": "thoniTUB", "createdAt": "2020-11-20T10:14:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUzNTQwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzYzNjk3Ng==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527636976", "bodyText": "Ist doch besser wenn es hier ist, da die Methoden potenziell aus verschiedenen Dateien aufgerufen werden", "author": "thoniTUB", "createdAt": "2020-11-20T11:43:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUzNTQwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzcxMjAxMA==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527712010", "bodyText": "jo hatte gedacht, dass du das dispatch in einer method machst, aber du hast unterschiedliche klassen", "author": "awildturtok", "createdAt": "2020-11-20T14:06:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUzNTQwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUzNTU4Nw==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527535587", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\t\t// TODO Auto-generated method stub", "author": "awildturtok", "createdAt": "2020-11-20T08:55:36Z", "path": "backend/src/main/java/com/bakdata/conquery/models/execution/ResultProcessor.java", "diffHunk": "@@ -61,6 +81,73 @@ public static ResponseBuilder getResult(User user, DatasetId datasetId, ManagedE\n \t\t\tConqueryMDC.clearLocation();\n \t\t}\n \t}\n+\t\n+\tpublic Response getArrowStreamResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowStreamWriter(root, new DictionaryProvider.MapDictionaryProvider(), output),\n+\t\t\tuser,\n+\t\t\tqueryId,\n+\t\t\tdatasetId,\n+\t\t\tdatasetRegistry,\n+\t\t\tpretty);\n+\t}\n+\t\n+\tpublic Response getArrowFileResult(User user, ManagedExecutionId queryId, DatasetId datasetId, boolean pretty) {\n+\t\treturn getArrowResult(\n+\t\t\t(output) -> (root) -> new ArrowFileWriter(root, new DictionaryProvider.MapDictionaryProvider(), Channels.newChannel(output)),\n+\t\t\tuser,\n+\t\t\tqueryId,\n+\t\t\tdatasetId,\n+\t\t\tdatasetRegistry,\n+\t\t\tpretty);\n+\t}\n+\t\n+\t\n+\tprivate Response getArrowResult(\n+\t\tFunction<OutputStream, Function<VectorSchemaRoot,ArrowWriter>> writerProducer,\n+\t\tUser user,\n+\t\tManagedExecutionId queryId,\n+\t\tDatasetId datasetId,\n+\t\tDatasetRegistry datasetRegistry,\n+\t\tboolean pretty) {\n+\t\t\n+\t\tConqueryMDC.setLocation(user.getName());\n+\t\tlog.info(\"Downloading results for {} on dataset {}\", queryId, datasetId);\n+\t\tauthorize(user, datasetId, Ability.READ);\n+\t\tauthorize(user, queryId, Ability.READ);\n+\n+\t\tManagedExecution<?> exec = datasetRegistry.getMetaStorage().getExecution(queryId);\n+\t\t\n+\t\t// Check if user is permitted to download on all datasets that were referenced by the query\n+\t\tauthorizeDownloadDatasets(user, exec);\n+\t\t\n+\t\tif(!(exec instanceof ManagedQuery)) {\n+\t\t\treturn Response.notAcceptable(null).build();\n+\t\t}\n+\t\tManagedQuery mquery = (ManagedQuery) exec;\n+\n+\t\t// Get the locale extracted by the LocaleFilter\n+\t\tPrintSettings settings = new PrintSettings(pretty, I18n.LOCALE.get(), datasetRegistry);\n+\t\t\n+\t\tIdMappingConfig idMappingConf = config.getIdMapping();\n+\t\tDirectDictionary primaryDict = datasetRegistry.get(datasetId).getStorage().getPrimaryDictionary();\n+\t\tPersistentIdMap idMapping = datasetRegistry.get(datasetId).getStorage().getIdMapping();\n+\t\t\n+\t\tStreamingOutput out = new StreamingOutput() {\n+\t\t\t\n+\t\t\t@Override\n+\t\t\tpublic void write(OutputStream output) throws IOException, WebApplicationException {\n+\t\t\t\t// TODO Auto-generated method stub", "originalCommit": "3cb8fd9132ef94f5e34ac6f7f4d88acbf129084d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "f70d90eab9d3763fc134f6c248e130f6d05135a0", "url": "https://github.com/bakdata/conquery/commit/f70d90eab9d3763fc134f6c248e130f6d05135a0", "message": "inject idmapper from outside", "committedDate": "2020-11-20T11:10:30Z", "type": "commit"}, {"oid": "edc2b44f7da16e7397561b9a5e84fdc6968f2c3d", "url": "https://github.com/bakdata/conquery/commit/edc2b44f7da16e7397561b9a5e84fdc6968f2c3d", "message": "makes batch size configurable", "committedDate": "2020-11-20T11:15:22Z", "type": "commit"}, {"oid": "edc2b44f7da16e7397561b9a5e84fdc6968f2c3d", "url": "https://github.com/bakdata/conquery/commit/edc2b44f7da16e7397561b9a5e84fdc6968f2c3d", "message": "makes batch size configurable", "committedDate": "2020-11-20T11:15:22Z", "type": "forcePushed"}, {"oid": "69d53f27064c077ffaf69f0aa20a11b3666d4407", "url": "https://github.com/bakdata/conquery/commit/69d53f27064c077ffaf69f0aa20a11b3666d4407", "message": "clean up logging", "committedDate": "2020-11-20T11:21:40Z", "type": "commit"}, {"oid": "033357b80c1b61a5fd5e4adf8652d30c8e1847c8", "url": "https://github.com/bakdata/conquery/commit/033357b80c1b61a5fd5e4adf8652d30c8e1847c8", "message": "extract switch statement", "committedDate": "2020-11-20T11:31:57Z", "type": "commit"}, {"oid": "65c8104710f88c2d519fef84e8878f35def16674", "url": "https://github.com/bakdata/conquery/commit/65c8104710f88c2d519fef84e8878f35def16674", "message": "fix batchwrite invocation", "committedDate": "2020-11-20T11:41:14Z", "type": "commit"}, {"oid": "a0317efe3c3478138a763cf2b7a79acbf990f908", "url": "https://github.com/bakdata/conquery/commit/a0317efe3c3478138a763cf2b7a79acbf990f908", "message": "Update backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java\n\nCo-authored-by: awildturtok <1553491+awildturtok@users.noreply.github.com>", "committedDate": "2020-11-20T12:51:14Z", "type": "commit"}, {"oid": "1ecff18ec12248938920f13e18574e00f2445265", "url": "https://github.com/bakdata/conquery/commit/1ecff18ec12248938920f13e18574e00f2445265", "message": "dont chain cell fillers but have them in a list.", "committedDate": "2020-11-20T12:51:35Z", "type": "commit"}, {"oid": "cb60a08f1cd224cfb1946520cd23ed46626af72b", "url": "https://github.com/bakdata/conquery/commit/cb60a08f1cd224cfb1946520cd23ed46626af72b", "message": "Merge branch 'develop' into feature/arrow-result-output\n\n# Conflicts:\n#\tbackend/src/main/java/com/bakdata/conquery/models/execution/ResultProcessor.java\n#\tbackend/src/main/java/com/bakdata/conquery/models/query/ManagedQuery.java\n#\tbackend/src/main/java/com/bakdata/conquery/resources/api/ResultCSVResource.java", "committedDate": "2020-11-20T13:07:11Z", "type": "commit"}, {"oid": "ee4a0eedb62410ea25c6d80aea9990d42564b4d4", "url": "https://github.com/bakdata/conquery/commit/ee4a0eedb62410ea25c6d80aea9990d42564b4d4", "message": "automatic update to docs", "committedDate": "2020-11-20T13:10:05Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzcwMjkzOQ==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527702939", "bodyText": "Ich denke, dass du das sogar als Stream behalten kannst, dann sparst du dir die potentiell riesige allokation", "author": "awildturtok", "createdAt": "2020-11-20T13:50:55Z", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg, ManagedQuery query, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());", "originalCommit": "ee4a0eedb62410ea25c6d80aea9990d42564b4d4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzc5MzM3NQ==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527793375", "bodyText": "Ich spare mir das filtern hier und \u00fcberspringe beim schreiben einfach die die notContained sind.\nIch sortiere das ergebnis jetzt noch nicht, meinst du das ist wichtig?", "author": "thoniTUB", "createdAt": "2020-11-20T16:06:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzcwMjkzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzcwNTQ0MA==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527705440", "bodyText": "Intellij sagt mit, dass du dir hier alle for(int ... zu for ( : ) machen kannst", "author": "awildturtok", "createdAt": "2020-11-20T13:55:02Z", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg, ManagedQuery query, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idWriters, valueWriter, idMapper, results, batchsize);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(\n+\t\tArrowWriter writer, \n+\t\tVectorSchemaRoot root, \n+\t\tRowConsumer[] idWriter, \n+\t\tRowConsumer[] valueWriter, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tList<ContainedEntityResult> results,\n+\t\tint batchSize) throws IOException \n+\t{\n+\t\tPreconditions.checkArgument(batchSize > 0, \"Batchsize needs be larger than 0.\");\n+\t\t// TODO add time metric for writing\n+\t\t\n+\t\tlog.trace(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchCount = 0;\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(batchSize);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {", "originalCommit": "ee4a0eedb62410ea25c6d80aea9990d42564b4d4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzc5NDEwNw==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527794107", "bodyText": "Da hats recht", "author": "thoniTUB", "createdAt": "2020-11-20T16:07:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzcwNTQ0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzcxMDQ1MQ==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r527710451", "bodyText": "public static List<Field> generateFieldsFromResultType(@NonNull List<ResultInfo> infos, PrintSettings settings) {\n\t\treturn  infos.stream()\n\t\t\t\t\t .map(info -> getFieldForResultInfo(info, settings))\n\t\t\t\t\t .collect(Collectors.toUnmodifiableList());\n\t\t\t\t\n\t}", "author": "awildturtok", "createdAt": "2020-11-20T14:03:21Z", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg, ManagedQuery query, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tList<ResultInfo> resultInfos = query.collectResultInfos().getInfos();\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t\n+\t\tList<ContainedEntityResult> results = query.getResults().stream().filter(ContainedEntityResult.class::isInstance).map(ContainedEntityResult.class::cast).collect(Collectors.toList());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idWriters, valueWriter, idMapper, results, batchsize);\n+\t\t}\n+\t\t\n+\t}\n+\n+\tpublic static void write(\n+\t\tArrowWriter writer, \n+\t\tVectorSchemaRoot root, \n+\t\tRowConsumer[] idWriter, \n+\t\tRowConsumer[] valueWriter, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tList<ContainedEntityResult> results,\n+\t\tint batchSize) throws IOException \n+\t{\n+\t\tPreconditions.checkArgument(batchSize > 0, \"Batchsize needs be larger than 0.\");\n+\t\t// TODO add time metric for writing\n+\t\t\n+\t\tlog.trace(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchCount = 0;\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(batchSize);\n+\t\tfor (int resultCount = 0; resultCount < results.size(); resultCount++) {\n+\t\t\tContainedEntityResult result = results.get(resultCount);\n+\t\t\tfor (Object[] line : result.listResultLines()) {\n+\t\t\t\tfor(int cellIndex = 0; cellIndex < idWriter.length; cellIndex++) {\n+\t\t\t\t\t// Write id information\n+\t\t\t\t\tidWriter[cellIndex].accept(batchLineCount, idMapper.apply(result));\n+\t\t\t\t}\n+\t\t\t\tfor(int cellIndex = 0; cellIndex < valueWriter.length; cellIndex++) {\n+\t\t\t\t\t// Write values\n+\t\t\t\t\tvalueWriter[cellIndex].accept(batchLineCount, line);\t\t\t\t\t\n+\t\t\t\t}\n+\t\t\t\tbatchLineCount++;\n+\t\t\t\t\n+\t\t\t\tif(batchLineCount >= batchSize) {\t\t\t\t\n+\t\t\t\t\twriter.writeBatch();\n+\t\t\t\t\tbatchLineCount = 0;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t\tbatchCount++;\n+\t\t}\n+\t\tlog.trace(\"Wrote {} batches of size {} (last batch might be smaller)\", batchCount, batchSize);\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text((String) line[pos]));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer[] generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer[] builder = new RowConsumer[numVectors];\n+\t\t\n+\t\tfor (int vecI = vectorOffset, resultPos = 0; vecI < root.getFieldVectors().size() && vecI < vectorOffset + numVectors; vecI++, resultPos++) {\n+\t\t\tfinal int pos = resultPos;\n+\t\t\tfinal FieldVector vector = root.getVector(vecI);\n+\t\t\t\n+                        //TODO When Pattern-matching lands, clean this up. (Think Java 12?)\n+\t\t\tif(vector instanceof IntVector) {\n+\t\t\t\tbuilder[resultPos]  = intVectorFiller((IntVector) vector, pos);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\n+\t\t\tif(vector instanceof VarCharVector) {\n+\t\t\t\tbuilder[resultPos]  = varCharVectorFiller((VarCharVector) vector, pos);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof BitVector) {\n+\t\t\t\tbuilder[resultPos]  = bitVectorFiller((BitVector) vector, pos);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof Float4Vector) {\n+\t\t\t\tbuilder[resultPos]  = float4VectorFiller((Float4Vector)vector, pos);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof Float8Vector) {\n+\t\t\t\tbuilder[resultPos]  = float8VectorFiller((Float8Vector)vector, pos);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif(vector instanceof DateDayVector) {\n+\t\t\t\tbuilder[resultPos]  = dateDayVectorFiller((DateDayVector) vector, pos);\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\t\n+\t\t\tthrow new UnsupportedOperationException(\"Vector type for writing result: \"+ vector.getClass());\n+\t\t}\n+\t\treturn builder;\n+\t\t\n+\t}\n+\t\n+\tpublic static List<Field> generateFieldsFromIdMapping(String[] idHeaders){", "originalCommit": "ee4a0eedb62410ea25c6d80aea9990d42564b4d4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "a45364b11191a5ab60b67c171133c52d0ac92650", "url": "https://github.com/bakdata/conquery/commit/a45364b11191a5ab60b67c171133c52d0ac92650", "message": "fix wrong parameterization in arrow result url path", "committedDate": "2020-11-20T15:28:18Z", "type": "commit"}, {"oid": "7b21684803eab64fdee7f5f8c43e221cd29632fb", "url": "https://github.com/bakdata/conquery/commit/7b21684803eab64fdee7f5f8c43e221cd29632fb", "message": "review changes", "committedDate": "2020-11-20T16:08:49Z", "type": "commit"}, {"oid": "1ffbf06f457458e0804da722f149f186071852b0", "url": "https://github.com/bakdata/conquery/commit/1ffbf06f457458e0804da722f149f186071852b0", "message": "better ErrorCode, when executtion cannot be rendered", "committedDate": "2020-11-23T15:51:53Z", "type": "commit"}, {"oid": "a6423b628d97243b145987cf7f208cf1c14ec237", "url": "https://github.com/bakdata/conquery/commit/a6423b628d97243b145987cf7f208cf1c14ec237", "message": "call toString on any object that is flagged as string", "committedDate": "2020-11-23T15:53:11Z", "type": "commit"}, {"oid": "207be338c3e486fce054446f8559b24101e1d233", "url": "https://github.com/bakdata/conquery/commit/207be338c3e486fce054446f8559b24101e1d233", "message": "add render support for Forms", "committedDate": "2020-11-23T15:53:35Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODg0MzU2Mw==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r528843563", "bodyText": "in Methode kapseln, das collectResultInfos m\u00fcsstest du doch eigentlich nicht so umst\u00e4ndlich benutzen?", "author": "awildturtok", "createdAt": "2020-11-23T16:40:16Z", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,285 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.bakdata.conquery.models.execution.ManagedExecution;\n+import com.bakdata.conquery.models.forms.managed.ManagedForm;\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.bakdata.conquery.models.query.results.EntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg,\n+\t\tManagedExecution<?> exec, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\t\t// Test the execution if the result is renderable into one table\n+\t\tStream<EntityResult> results;\n+\t\tList<ResultInfo> resultInfos;\n+\t\tif(exec instanceof ManagedQuery) {", "originalCommit": "207be338c3e486fce054446f8559b24101e1d233", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTI4ODQ2OA==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r529288468", "bodyText": "okay ich mache split phase. Aber ja die Result infos sind nur auf der ManagedQuery definiert", "author": "thoniTUB", "createdAt": "2020-11-24T08:30:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODg0MzU2Mw=="}], "type": "inlineReview"}, {"oid": "54358b1388d08bd462e18a584c62f2967001137a", "url": "https://github.com/bakdata/conquery/commit/54358b1388d08bd462e18a584c62f2967001137a", "message": "review changes", "committedDate": "2020-11-24T13:46:12Z", "type": "commit"}, {"oid": "3f416c8159c5d0d17cf33cef11d7228e6217dbd0", "url": "https://github.com/bakdata/conquery/commit/3f416c8159c5d0d17cf33cef11d7228e6217dbd0", "message": "Merge branch 'develop' into feature/arrow-result-output\n\n# Conflicts:\n#\tbackend/src/main/java/com/bakdata/conquery/models/execution/ResultProcessor.java", "committedDate": "2020-11-24T15:10:02Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTY1OTQyMA==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r529659420", "bodyText": "Kannst du nicht direkt returnen?", "author": "awildturtok", "createdAt": "2020-11-24T15:45:11Z", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,307 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.bakdata.conquery.models.execution.ManagedExecution;\n+import com.bakdata.conquery.models.forms.managed.ManagedForm;\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.bakdata.conquery.models.query.results.EntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg,\n+\t\tManagedExecution<?> exec, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\t\t// Test the execution if the result is renderable into one table\n+\t\tStream<EntityResult> results = getResults(exec);\n+\t\tList<ResultInfo> resultInfos = getResultInfos(exec);\n+\t\t\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idWriters, valueWriter, idMapper, results, batchsize);\n+\t\t}\n+\t\t\n+\t}\n+\t\n+\tprivate static Stream<EntityResult> getResults(ManagedExecution<?> exec) {\n+\t\tStream<EntityResult> results;\n+\t\tif(exec instanceof ManagedQuery) {\n+\t\t\tresults = ((ManagedQuery)exec).getResults().stream();\n+\t\t}\n+\t\telse if(exec instanceof ManagedForm && ((ManagedForm)exec).getSubQueries().size() == 1) {\n+\t\t\tresults = ((ManagedForm)exec).getSubQueries().values().iterator().next().stream().flatMap(mq -> mq.getResults().stream());\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new IllegalStateException(\"The provided execution cannot be rendered as a single table. Was: \" + exec.getId());\n+\t\t}\n+\t\treturn results;", "originalCommit": "3f416c8159c5d0d17cf33cef11d7228e6217dbd0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTY1OTc4Mw==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r529659783", "bodyText": "return reinziehen", "author": "awildturtok", "createdAt": "2020-11-24T15:45:25Z", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,307 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.bakdata.conquery.models.execution.ManagedExecution;\n+import com.bakdata.conquery.models.forms.managed.ManagedForm;\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.bakdata.conquery.models.query.results.EntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg,\n+\t\tManagedExecution<?> exec, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\t\t// Test the execution if the result is renderable into one table\n+\t\tStream<EntityResult> results = getResults(exec);\n+\t\tList<ResultInfo> resultInfos = getResultInfos(exec);\n+\t\t\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idWriters, valueWriter, idMapper, results, batchsize);\n+\t\t}\n+\t\t\n+\t}\n+\t\n+\tprivate static Stream<EntityResult> getResults(ManagedExecution<?> exec) {\n+\t\tStream<EntityResult> results;\n+\t\tif(exec instanceof ManagedQuery) {\n+\t\t\tresults = ((ManagedQuery)exec).getResults().stream();\n+\t\t}\n+\t\telse if(exec instanceof ManagedForm && ((ManagedForm)exec).getSubQueries().size() == 1) {\n+\t\t\tresults = ((ManagedForm)exec).getSubQueries().values().iterator().next().stream().flatMap(mq -> mq.getResults().stream());\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new IllegalStateException(\"The provided execution cannot be rendered as a single table. Was: \" + exec.getId());\n+\t\t}\n+\t\treturn results;\n+\t}\n+\t\n+\tprivate static List<ResultInfo> getResultInfos(ManagedExecution<?> exec) {\n+\t\tList<ResultInfo> resultInfos;\n+\t\tif(exec instanceof ManagedQuery) {\n+\t\t\tresultInfos = ((ManagedQuery)exec).collectResultInfos().getInfos();\n+\t\t}\n+\t\telse if(exec instanceof ManagedForm && ((ManagedForm)exec).getSubQueries().size() == 1) {\n+\t\t\tresultInfos = ((ManagedForm)exec).getSubQueries().values().iterator().next().get(0).collectResultInfos().getInfos();\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new IllegalStateException(\"The provided execution cannot be rendered as a single table. Was: \" + exec.getId());\n+\t\t}\n+\t\treturn resultInfos;", "originalCommit": "3f416c8159c5d0d17cf33cef11d7228e6217dbd0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTY2MzY4NQ==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r529663685", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\tint vecI = vectorOffset, resultPos = 0; \n          \n          \n            \n            \t\t\t( vecI < root.getFieldVectors().size() ) && ( vecI < vectorOffset + numVectors );\n          \n          \n            \n            \t\t\tvecI++, resultPos++\n          \n          \n            \n            \t\t\t) {\n          \n          \n            \n            \t\t\tfinal int pos = resultPos;\n          \n          \n            \n            \t\t\tint vecI = vectorOffset; \n          \n          \n            \n            \t\t\t( vecI < root.getFieldVectors().size() ) && ( vecI < vectorOffset + numVectors );\n          \n          \n            \n            \t\t\tvecI++\n          \n          \n            \n            \t\t\t) {\n          \n          \n            \n            \t\t\tfinal int pos = vecI - vectorOffset;", "author": "awildturtok", "createdAt": "2020-11-24T15:48:18Z", "path": "backend/src/main/java/com/bakdata/conquery/io/result/arrow/ArrowRenderer.java", "diffHunk": "@@ -0,0 +1,307 @@\n+package com.bakdata.conquery.io.result.arrow;\n+\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.NAMED_FIELD_DATE_DAY;\n+import static com.bakdata.conquery.io.result.arrow.ArrowUtil.ROOT_ALLOCATOR;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import com.bakdata.conquery.models.execution.ManagedExecution;\n+import com.bakdata.conquery.models.forms.managed.ManagedForm;\n+import com.bakdata.conquery.models.query.ManagedQuery;\n+import com.bakdata.conquery.models.query.PrintSettings;\n+import com.bakdata.conquery.models.query.resultinfo.ResultInfo;\n+import com.bakdata.conquery.models.query.results.ContainedEntityResult;\n+import com.bakdata.conquery.models.query.results.EntityResult;\n+import com.google.common.collect.ImmutableList;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.ipc.ArrowWriter;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.arrow.vector.util.Text;\n+\n+@Slf4j\n+public class ArrowRenderer {\n+\t\n+\tpublic static void renderToStream(\n+\t\tFunction<VectorSchemaRoot, ArrowWriter> writerProducer, \n+\t\tPrintSettings cfg,\n+\t\tManagedExecution<?> exec, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tString[] idHeaders,\t\n+\t\tint batchsize) throws IOException\n+\t{\n+\t\t// Test the execution if the result is renderable into one table\n+\t\tStream<EntityResult> results = getResults(exec);\n+\t\tList<ResultInfo> resultInfos = getResultInfos(exec);\n+\t\t\n+\t\t// Combine id and value Fields to one vector to build a schema\n+\t\tList<Field> fields = new ArrayList<>(generateFieldsFromIdMapping(idHeaders));\n+\t\tfields.addAll(generateFieldsFromResultType(resultInfos, cfg));\n+\t\tVectorSchemaRoot root = VectorSchemaRoot.create(new Schema(fields, null), ROOT_ALLOCATOR);\n+\t\t\n+\t\t// Build separate pipelines for id and value, as they have different sources but the same target\n+\t\tRowConsumer[] idWriters = generateWriterPipeline(root, 0, idHeaders.length);\n+\t\tRowConsumer[] valueWriter = generateWriterPipeline(root, idHeaders.length, resultInfos.size());\n+\n+\t\t// Write the data\n+\t\ttry(ArrowWriter writer = writerProducer.apply(root)) {\t\t\t\n+\t\t\twrite(writer, root, idWriters, valueWriter, idMapper, results, batchsize);\n+\t\t}\n+\t\t\n+\t}\n+\t\n+\tprivate static Stream<EntityResult> getResults(ManagedExecution<?> exec) {\n+\t\tStream<EntityResult> results;\n+\t\tif(exec instanceof ManagedQuery) {\n+\t\t\tresults = ((ManagedQuery)exec).getResults().stream();\n+\t\t}\n+\t\telse if(exec instanceof ManagedForm && ((ManagedForm)exec).getSubQueries().size() == 1) {\n+\t\t\tresults = ((ManagedForm)exec).getSubQueries().values().iterator().next().stream().flatMap(mq -> mq.getResults().stream());\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new IllegalStateException(\"The provided execution cannot be rendered as a single table. Was: \" + exec.getId());\n+\t\t}\n+\t\treturn results;\n+\t}\n+\t\n+\tprivate static List<ResultInfo> getResultInfos(ManagedExecution<?> exec) {\n+\t\tList<ResultInfo> resultInfos;\n+\t\tif(exec instanceof ManagedQuery) {\n+\t\t\tresultInfos = ((ManagedQuery)exec).collectResultInfos().getInfos();\n+\t\t}\n+\t\telse if(exec instanceof ManagedForm && ((ManagedForm)exec).getSubQueries().size() == 1) {\n+\t\t\tresultInfos = ((ManagedForm)exec).getSubQueries().values().iterator().next().get(0).collectResultInfos().getInfos();\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new IllegalStateException(\"The provided execution cannot be rendered as a single table. Was: \" + exec.getId());\n+\t\t}\n+\t\treturn resultInfos;\n+\t}\n+\t\n+\n+\tpublic static void write(\n+\t\tArrowWriter writer, \n+\t\tVectorSchemaRoot root, \n+\t\tRowConsumer[] idWriter, \n+\t\tRowConsumer[] valueWriter, \n+\t\tFunction<ContainedEntityResult,String[]> idMapper, \n+\t\tStream<EntityResult> results,\n+\t\tint batchSize) throws IOException \n+\t{\n+\t\tPreconditions.checkArgument(batchSize > 0, \"Batchsize needs be larger than 0.\");\n+\t\t// TODO add time metric for writing\n+\t\t\n+\t\tlog.trace(\"Starting result write\");\n+\t\twriter.start();\n+\t\tint batchCount = 0;\n+\t\tint batchLineCount = 0;\n+\t\troot.setRowCount(batchSize);\n+\t\tIterator<EntityResult> resultIter = results.iterator();\n+\t\twhile(resultIter.hasNext()) {\n+\t\t\tEntityResult result = resultIter.next();\n+\t\t\tif (!result.isContained()) {\n+\t\t\t\tcontinue;\n+\t\t\t}\n+\t\t\tContainedEntityResult cer = result.asContained();\n+\t\t\tfor (Object[] line : cer.listResultLines()) {\n+\t\t\t\tfor(int cellIndex = 0; cellIndex < idWriter.length; cellIndex++) {\n+\t\t\t\t\t// Write id information\n+\t\t\t\t\tidWriter[cellIndex].accept(batchLineCount, idMapper.apply(cer));\n+\t\t\t\t}\n+\t\t\t\tfor(int cellIndex = 0; cellIndex < valueWriter.length; cellIndex++) {\n+\t\t\t\t\t// Write values\n+\t\t\t\t\tvalueWriter[cellIndex].accept(batchLineCount, line);\t\t\t\t\t\n+\t\t\t\t}\n+\t\t\t\tbatchLineCount++;\n+\t\t\t\t\n+\t\t\t\tif(batchLineCount >= batchSize) {\t\t\t\t\n+\t\t\t\t\twriter.writeBatch();\n+\t\t\t\t\tbatchLineCount = 0;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tif(batchLineCount > 0) {\n+\t\t\troot.setRowCount(batchLineCount);\n+\t\t\twriter.writeBatch();\n+\t\t\tbatchCount++;\n+\t\t}\n+\t\tlog.trace(\"Wrote {} batches of size {} (last batch might be smaller)\", batchCount, batchSize);\n+\t\twriter.end();\n+\t}\n+\t\n+\tprivate static RowConsumer intVectorFiller(IntVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, (int) line[pos]);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer bitVectorFiller(BitVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Boolean) line[pos])? 1 : 0);\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float8VectorFiller(Float8Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).doubleValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer float4VectorFiller(Float4Vector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).floatValue());\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer varCharVectorFiller(VarCharVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, new Text(Objects.toString(line[pos])));\n+\t\t};\n+\t}\n+\t\n+\tprivate static RowConsumer dateDayVectorFiller(DateDayVector vector, int pos) {\n+\t\treturn (rowNumber, line) -> {\n+\t\t\tif (line[pos] == null) {\n+\t\t\t\tvector.setNull(rowNumber);\n+\t\t\t\treturn;\n+\t\t\t}\n+\t\t\tvector.setSafe(rowNumber, ((Number) line[pos]).intValue());\n+\t\t};\n+\t}\n+\n+\t\n+\tpublic static RowConsumer[] generateWriterPipeline(VectorSchemaRoot root, int vectorOffset, int numVectors){\n+\t\tPreconditions.checkArgument(vectorOffset >= 0, \"Offset was negativ: %s\", vectorOffset);\n+\t\tPreconditions.checkArgument(numVectors >= 0, \"Number of vectors was negativ: %s\", numVectors);\n+\t\t\n+\t\tRowConsumer[] builder = new RowConsumer[numVectors];\n+\t\t\n+\t\tfor (\n+\t\t\tint vecI = vectorOffset, resultPos = 0; \n+\t\t\t( vecI < root.getFieldVectors().size() ) && ( vecI < vectorOffset + numVectors );\n+\t\t\tvecI++, resultPos++\n+\t\t\t) {\n+\t\t\tfinal int pos = resultPos;", "originalCommit": "3f416c8159c5d0d17cf33cef11d7228e6217dbd0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTY2NDc4MQ==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r529664781", "bodyText": "ah, das ist nicht ganz richtig, aber w\u00fcrde nur eine laufvariable verwenden, gerade weil die voneinander abh\u00e4ngen", "author": "awildturtok", "createdAt": "2020-11-24T15:49:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTY2MzY4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTY4NTkyMw==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r529685923", "bodyText": "Was meinst du mit, dass es nicht ganz richtig ist?", "author": "thoniTUB", "createdAt": "2020-11-24T16:03:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTY2MzY4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTY5MTQzOQ==", "url": "https://github.com/bakdata/conquery/pull/1436#discussion_r529691439", "bodyText": "neuer PR #1436", "author": "thoniTUB", "createdAt": "2020-11-24T16:07:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTY2MzY4NQ=="}], "type": "inlineReview"}]}