{"pr_number": 1231, "pr_title": "Terminal State Job Purging", "pr_createdAt": "2020-08-07T17:07:45Z", "pr_url": "https://github.com/apache/helix/pull/1231", "timeline": [{"oid": "594fdc1341460f2105ab3337e40885e09a7afe32", "url": "https://github.com/apache/helix/commit/594fdc1341460f2105ab3337e40885e09a7afe32", "message": "Add purging logic", "committedDate": "2020-08-06T01:27:55Z", "type": "commit"}, {"oid": "dab99125126f8ad611a7f176b44d5d415df71b75", "url": "https://github.com/apache/helix/commit/dab99125126f8ad611a7f176b44d5d415df71b75", "message": "modify rebalance schedule", "committedDate": "2020-08-06T06:24:43Z", "type": "commit"}, {"oid": "5c13b9e14584a997dec86b3ca4c4974e266780d0", "url": "https://github.com/apache/helix/commit/5c13b9e14584a997dec86b3ca4c4974e266780d0", "message": "added test case", "committedDate": "2020-08-06T22:31:31Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzIwNDMyMw==", "url": "https://github.com/apache/helix/pull/1231#discussion_r467204323", "bodyText": "This may not be a good idea for that. It means user has to add a new config for each their job. And the existing running jobs and tasks are not benefiting from that.\nIf we really want to support that, my suggestion is to have a global TASK config similar to cluster config. Let them enable for failed/timeout job purge. That would be much easier for user.", "author": "junkaixue", "createdAt": "2020-08-07T18:31:38Z", "path": "helix-core/src/main/java/org/apache/helix/task/JobConfig.java", "diffHunk": "@@ -147,10 +147,17 @@\n     StartTime,\n \n     /**\n-     * The expiration time for the job\n+     * The expiration time for the job if it's completed; once the expiry is reached and the job is\n+     * completed, the job will be purged\n      */\n     Expiry,\n \n+    /**\n+     * The expiration time for the job if it's failed or timed out; once the expiry is reached and\n+     * the job has failed or timed out, the job will be purged\n+     */\n+    TerminalStateExpiry,", "originalCommit": "5c13b9e14584a997dec86b3ca4c4974e266780d0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzIyMDA3Mg==", "url": "https://github.com/apache/helix/pull/1231#discussion_r467220072", "bodyText": "The current expiry config we're using is on per-job level, though. I imagine there's a reason behind that?", "author": "NealSun96", "createdAt": "2020-08-07T19:05:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzIwNDMyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMxMjQ2Ng==", "url": "https://github.com/apache/helix/pull/1231#discussion_r467312466", "bodyText": "I think it makes more sense to keep this config in the as job config because we are treating this config similar to Expiry. If anybody want to use it, they can add this config.", "author": "alirezazamani", "createdAt": "2020-08-07T22:28:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzIwNDMyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODE4NDA0NQ==", "url": "https://github.com/apache/helix/pull/1231#discussion_r468184045", "bodyText": "Then there could be too many expiry to configured by user. Even if we want to support this at per job level, we need a boolean flag to show whether we should clean them after expiry when it fails or timed out. From time perspective, I dont think user has any difference with time number.\nBut from user migration perspective, adding to job level is very cost and roll out.", "author": "junkaixue", "createdAt": "2020-08-10T21:02:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzIwNDMyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODE5NjQ0Mg==", "url": "https://github.com/apache/helix/pull/1231#discussion_r468196442", "bodyText": "The current expiry config we're using is on per-job level, though. I imagine there's a reason behind that?\n\nAt job level is make sense that different job could have different time to persist for further investigation.\n@alirezazamani And we dont have to make the behavior to fine-grained at  job level. Because we defined the default behavior not purge failed/timeout jobs gloabally. That's why I am suggesting not adding the options at job level.", "author": "junkaixue", "createdAt": "2020-08-10T21:28:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzIwNDMyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIyMTY2MA==", "url": "https://github.com/apache/helix/pull/1231#discussion_r468221660", "bodyText": "@dasahcc The default behavior being not purging is already implemented: not specifying the expiry = not purging; we don't need an extra boolean flag.\nI understand where your came from, but the flexibility should be a good thing; after all, wouldn't users be more inclined to investigate failed/timed-out jobs comparing to completed jobs? The investigation flexibility should be more suited for the terminal state expiry.\nFor what its worth, if a user want to \"keep an eye out for Job0 if it fails, but don't care if the other jobs fail\", then it's easy to config that way: give Job0 no expiry (not purging), and give other jobs an expiry.", "author": "NealSun96", "createdAt": "2020-08-10T22:30:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzIwNDMyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQ2NzA2OA==", "url": "https://github.com/apache/helix/pull/1231#discussion_r469467068", "bodyText": "@NealSun96 Even we dont setup the expiry, there is a default value for expiry is one day. That's why I am saying the default behavior is cleaning up successful job. That's our assumption.\nWe need to balance the operability and flexibility. If you really consider flexibility, we should have cluster level config + job level. Then the logic will be much complicated.\nMy suggestion is still having a global change flag instead of to fine-grained, because we dont see any fine-grained control request from any of our customers. And from operability perspective, it will be very hard for them roll out since it requires code level change and redeploy all their scheduler code. But if we support it from cluster level, it they can dynamically change it with our latest controller deployment. That will be much simpler.", "author": "junkaixue", "createdAt": "2020-08-12T18:45:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzIwNDMyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTYyMzM5NQ==", "url": "https://github.com/apache/helix/pull/1231#discussion_r469623395", "bodyText": "Resolved offline.", "author": "NealSun96", "createdAt": "2020-08-13T00:24:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzIwNDMyMw=="}], "type": "inlineReview"}, {"oid": "c21e30fb8a600c49d5ca07c8a4da07fec75f2cf2", "url": "https://github.com/apache/helix/commit/c21e30fb8a600c49d5ca07c8a4da07fec75f2cf2", "message": "Flaky tests", "committedDate": "2020-08-07T19:08:26Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMxMzYzMA==", "url": "https://github.com/apache/helix/pull/1231#discussion_r467313630", "bodyText": "maybe it is better to change this comment to failed, complete and time_out. Because there are other terminal states as well.", "author": "alirezazamani", "createdAt": "2020-08-07T22:33:12Z", "path": "helix-core/src/main/java/org/apache/helix/task/JobDispatcher.java", "diffHunk": "@@ -87,9 +87,10 @@ public ResourceAssignment processJobStatusUpdateAndAssignment(String jobName,\n     TaskState jobState = workflowCtx.getJobState(jobName);\n     // The job is already in a final state (completed/failed).\n     if (workflowState == TaskState.FAILED || workflowState == TaskState.COMPLETED\n-        || jobState == TaskState.FAILED || jobState == TaskState.COMPLETED) {\n+        || jobState == TaskState.FAILED || jobState == TaskState.COMPLETED\n+        || jobState == TaskState.TIMED_OUT) {\n       LOG.info(String.format(\n-          \"Workflow %s or job %s is already failed or completed, workflow state (%s), job state (%s), clean up job IS.\",\n+          \"Workflow %s or job %s is already in final state, workflow state (%s), job state (%s), clean up job IS.\",", "originalCommit": "c21e30fb8a600c49d5ca07c8a4da07fec75f2cf2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMxNzM0Mg==", "url": "https://github.com/apache/helix/pull/1231#discussion_r467317342", "bodyText": "The thing is Workflow = timed_out is not checked here.", "author": "NealSun96", "createdAt": "2020-08-07T22:48:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMxMzYzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMxNDM1NA==", "url": "https://github.com/apache/helix/pull/1231#discussion_r467314354", "bodyText": "Can you confirm if jobState here is not null always?", "author": "alirezazamani", "createdAt": "2020-08-07T22:35:59Z", "path": "helix-core/src/main/java/org/apache/helix/task/TaskUtil.java", "diffHunk": "@@ -733,10 +734,29 @@ protected static boolean removeJobsFromWorkflow(final HelixDataAccessor dataAcce\n     if (workflowContext != null) {\n       Map<String, TaskState> jobStates = workflowContext.getJobStates();\n       for (String job : workflowConfig.getJobDag().getAllNodes()) {\n+        if (expiredJobs.contains(job)) {\n+          continue;\n+        }\n         JobConfig jobConfig = TaskUtil.getJobConfig(dataAccessor, job);\n         JobContext jobContext = TaskUtil.getJobContext(propertyStore, job);\n-        if (isJobExpired(job, jobConfig, jobContext, jobStates.get(job))) {\n+        TaskState jobState = jobStates.get(job);", "originalCommit": "c21e30fb8a600c49d5ca07c8a4da07fec75f2cf2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMxOTA3OQ==", "url": "https://github.com/apache/helix/pull/1231#discussion_r467319079", "bodyText": "jobState could be null in an erroneous state where a job is in the DAG but not in jobState map, but is that a situation we need to consider? This logic here wouldn't interact with such a job as it fails the jobState check conditions. I think it's fine to not deal with that case.", "author": "NealSun96", "createdAt": "2020-08-07T22:55:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMxNDM1NA=="}], "type": "inlineReview"}, {"oid": "60f08cf84baf4d10abe573b7644820494b7d7ed9", "url": "https://github.com/apache/helix/commit/60f08cf84baf4d10abe573b7644820494b7d7ed9", "message": "Nit change", "committedDate": "2020-08-13T18:53:56Z", "type": "commit"}]}