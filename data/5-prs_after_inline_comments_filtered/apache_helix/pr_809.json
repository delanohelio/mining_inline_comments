{"pr_number": 809, "pr_title": "Add system property options to config write size limit for ZNRecord Serializer", "pr_createdAt": "2020-02-26T02:00:10Z", "pr_url": "https://github.com/apache/helix/pull/809", "timeline": [{"oid": "a844abe838743838306712294b05aaf81cc1748a", "url": "https://github.com/apache/helix/commit/a844abe838743838306712294b05aaf81cc1748a", "message": "Add options to config auto compression", "committedDate": "2020-02-26T01:40:36Z", "type": "forcePushed"}, {"oid": "3d772ffd135b8d648ec914fa253002dce9b5616c", "url": "https://github.com/apache/helix/commit/3d772ffd135b8d648ec914fa253002dce9b5616c", "message": "Add options to config auto compression", "committedDate": "2020-02-26T03:47:59Z", "type": "commit"}, {"oid": "3d772ffd135b8d648ec914fa253002dce9b5616c", "url": "https://github.com/apache/helix/commit/3d772ffd135b8d648ec914fa253002dce9b5616c", "message": "Add options to config auto compression", "committedDate": "2020-02-26T03:47:59Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY2MDMzNQ==", "url": "https://github.com/apache/helix/pull/809#discussion_r384660335", "bodyText": "What if user inputs a typo?", "author": "junkaixue", "createdAt": "2020-02-26T17:50:45Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/datamodel/serializer/ZNRecordSerializer.java", "diffHunk": "@@ -38,6 +39,17 @@\n public class ZNRecordSerializer implements ZkSerializer {\n   private static Logger logger = LoggerFactory.getLogger(ZNRecordSerializer.class);\n \n+  // Reads from system property and represents whether auto compression is enabled or not.\n+  // If and only if this property is set to \"true\", auto compression is enabled.\n+  private final boolean autoCompressionEnabled =\n+      Boolean.getBoolean(ZkSystemPropertyKeys.ZK_SERIALIZER_AUTO_COMPRESSION_ENABLED);\n+\n+  // Reads from system property and represents the data size threshold in bytes for\n+  // auto compression.\n+  private final int autoCompressionThreshold = Integer", "originalCommit": "3d772ffd135b8d648ec914fa253002dce9b5616c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY2NDYyMw==", "url": "https://github.com/apache/helix/pull/809#discussion_r384664623", "bodyText": "If there is no property of the specified name, if the property does not have the correct numeric format, or if the specified name is empty or null, the default value ZNRecord.SIZE_LIMIT would be used.", "author": "huizhilu", "createdAt": "2020-02-26T17:58:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY2MDMzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY2MDcxNg==", "url": "https://github.com/apache/helix/pull/809#discussion_r384660716", "bodyText": "Is default value true with this statement?", "author": "junkaixue", "createdAt": "2020-02-26T17:51:25Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/datamodel/serializer/ZNRecordSerializer.java", "diffHunk": "@@ -38,6 +39,17 @@\n public class ZNRecordSerializer implements ZkSerializer {\n   private static Logger logger = LoggerFactory.getLogger(ZNRecordSerializer.class);\n \n+  // Reads from system property and represents whether auto compression is enabled or not.\n+  // If and only if this property is set to \"true\", auto compression is enabled.\n+  private final boolean autoCompressionEnabled =", "originalCommit": "3d772ffd135b8d648ec914fa253002dce9b5616c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY2MzA0Mg==", "url": "https://github.com/apache/helix/pull/809#discussion_r384663042", "bodyText": "No. By default: not set or empty or invalid string, it is false. Like the comment said, If and only if this property is set to \"true\", auto compression is enabled.", "author": "huizhilu", "createdAt": "2020-02-26T17:55:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY2MDcxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDc0MzAxMw==", "url": "https://github.com/apache/helix/pull/809#discussion_r384743013", "bodyText": "We need migration steps. So the first step will be let the default value to be true.", "author": "junkaixue", "createdAt": "2020-02-26T20:22:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY2MDcxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY4MDk3OA==", "url": "https://github.com/apache/helix/pull/809#discussion_r384680978", "bodyText": "It's actually the ZNRecord serializer, not zkSerializer, right? I don't think we support auto-compression in ZkSerializer?", "author": "narendly", "createdAt": "2020-02-26T18:28:48Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/constant/ZkSystemPropertyKeys.java", "diffHunk": "@@ -0,0 +1,39 @@\n+package org.apache.helix.zookeeper.constant;\n+\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+public class ZkSystemPropertyKeys {\n+  /**\n+   * Setting this property to true in system properties enables auto compression in ZK serializer.\n+   * The data will be automatically compressed by\n+   * {@link org.apache.helix.zookeeper.util.GZipCompressionUtil} when being written to Zookeeper\n+   * if size of serialized data exceeds the configured threshold.\n+   */\n+  public static final String ZK_SERIALIZER_AUTO_COMPRESSION_ENABLED =\n+      \"zk.serializer.auto-compression.enabled\";", "originalCommit": "3d772ffd135b8d648ec914fa253002dce9b5616c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY4MTAyNg==", "url": "https://github.com/apache/helix/pull/809#discussion_r384681026", "bodyText": "It's actually the ZNRecord serializer, not zkSerializer, right? I don't think we support auto-compression in ZkSerializer?", "author": "narendly", "createdAt": "2020-02-26T18:28:54Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/constant/ZkSystemPropertyKeys.java", "diffHunk": "@@ -0,0 +1,39 @@\n+package org.apache.helix.zookeeper.constant;\n+\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+public class ZkSystemPropertyKeys {\n+  /**\n+   * Setting this property to true in system properties enables auto compression in ZK serializer.\n+   * The data will be automatically compressed by\n+   * {@link org.apache.helix.zookeeper.util.GZipCompressionUtil} when being written to Zookeeper\n+   * if size of serialized data exceeds the configured threshold.\n+   */\n+  public static final String ZK_SERIALIZER_AUTO_COMPRESSION_ENABLED =\n+      \"zk.serializer.auto-compression.enabled\";\n+\n+  /**\n+   * This is property that defines the threshold in bytes for auto compression in ZK serializer.\n+   * Given auto compression is enabled, if the size of data exceeds this configured threshold,\n+   * the data will be automatically compressed when being written to Zookeeper.\n+   */\n+  public static final String ZK_SERIALIZER_AUTO_COMPRESSION_THRESHOLD_BYTES =\n+      \"zk.serializer.auto-compression.threshold.bytes\";", "originalCommit": "3d772ffd135b8d648ec914fa253002dce9b5616c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY4NjU3MQ==", "url": "https://github.com/apache/helix/pull/809#discussion_r384686571", "bodyText": "Initially I had the same thought to include znrecord-serializer. and had a conversation with @dasahcc. His opinions:\n\nthe config name would be too long\nusers don't know what znrecord is so we don't want to expose znrecord to the config.\nusers may not use implements of zkSerializer other than ZNRecordSerializer", "author": "huizhilu", "createdAt": "2020-02-26T18:39:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY4MTAyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDc5MzI2NA==", "url": "https://github.com/apache/helix/pull/809#discussion_r384793264", "bodyText": "As we discussed, we want this config to apply to ZNRecordSerializers only to achieve the following:\n\nIt serves only one clear purpose without ambiguity (only applies to ZNRecord serializers, not to some potential serializers we may or may not add in the future)\nIt makes it harder to misuse the variable. Has ZNRecord in the name, so shouldn't be used for any other purposes.", "author": "narendly", "createdAt": "2020-02-26T22:00:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY4MTAyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY4MjgyMg==", "url": "https://github.com/apache/helix/pull/809#discussion_r384682822", "bodyText": "ZnRecordStreamingSerializer also has this logic. It would be great if you could refactor this piece of logic out so that we support this in all of our ZNRecord serializers :)", "author": "narendly", "createdAt": "2020-02-26T18:32:13Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/datamodel/serializer/ZNRecordSerializer.java", "diffHunk": "@@ -86,7 +98,8 @@ private static int getListFieldBound(ZNRecord record) {\n       mapper.writeValue(baos, data);\n       serializedBytes = baos.toByteArray();\n       // apply compression if needed\n-      if (record.getBooleanField(\"enableCompression\", false) || serializedBytes.length > ZNRecord.SIZE_LIMIT) {\n+      if (record.getBooleanField(\"enableCompression\", false) || (autoCompressionEnabled\n+          && serializedBytes.length > autoCompressionThreshold)) {", "originalCommit": "3d772ffd135b8d648ec914fa253002dce9b5616c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY4NzM0NQ==", "url": "https://github.com/apache/helix/pull/809#discussion_r384687345", "bodyText": "Right already realized that. Just need to confirm with the team on this, as I was initially working on ZNRecordSerializer. But also found the same logic in ZnRecordStreamingSerializer.", "author": "huizhilu", "createdAt": "2020-02-26T18:40:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY4MjgyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDc0NjIwMQ==", "url": "https://github.com/apache/helix/pull/809#discussion_r384746201", "bodyText": "This logic could go to the ZNRecord class. isAutoCompression()", "author": "jiajunwang", "createdAt": "2020-02-26T20:28:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY4MjgyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDg2MjExNQ==", "url": "https://github.com/apache/helix/pull/809#discussion_r384862115", "bodyText": "@jiajunwang Actually I don't think it could go the ZNRecord as you said. We actually have 2 conditions:\nif (record.getBooleanField(\"enableCompression\", false) || (autoCompressionEnabled\n           && serializedBytes.length > autoCompressionThreshold))\n\nWe can not only rely on the configs of enableCompression and autoCompressionEnabled, but also the serializedBytes' length. Current behavior is that if enableCompression is enabled, we ignore length. If enableCompression is disabled, we still have to check the length.\nWe can't simply do\nznrecord.autoCompressionEnabled() {\n    return record.getBooleanField(\"enableCompression\", false) || autoCompressionEnabled;\n}\n\nMaybe you meat this:\nznrecord.autoCompressionEnabled(serializedBytes.length) {\n    return record.getBooleanField(\"enableCompression\", false) || (autoCompressionEnabled\n           && serializedBytes.length > autoCompressionThreshold));\n}\n\nWith above, I don't see benefit.\nDiscussed offline, we decided to add the logic as a static util method.", "author": "huizhilu", "createdAt": "2020-02-27T01:14:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY4MjgyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDc0Mzc0Ng==", "url": "https://github.com/apache/helix/pull/809#discussion_r384743746", "bodyText": "Just call it ZNRECORD_AUTO_COMPRESSION_ENABLED?", "author": "jiajunwang", "createdAt": "2020-02-26T20:23:32Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/constant/ZkSystemPropertyKeys.java", "diffHunk": "@@ -0,0 +1,39 @@\n+package org.apache.helix.zookeeper.constant;\n+\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+public class ZkSystemPropertyKeys {\n+  /**\n+   * Setting this property to true in system properties enables auto compression in ZK serializer.\n+   * The data will be automatically compressed by\n+   * {@link org.apache.helix.zookeeper.util.GZipCompressionUtil} when being written to Zookeeper\n+   * if size of serialized data exceeds the configured threshold.\n+   */\n+  public static final String ZK_SERIALIZER_AUTO_COMPRESSION_ENABLED =", "originalCommit": "3d772ffd135b8d648ec914fa253002dce9b5616c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDc0Mzg3NQ==", "url": "https://github.com/apache/helix/pull/809#discussion_r384743875", "bodyText": "Same here, ZK_SERIALIZER -> ZNRECORD", "author": "jiajunwang", "createdAt": "2020-02-26T20:23:52Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/constant/ZkSystemPropertyKeys.java", "diffHunk": "@@ -0,0 +1,39 @@\n+package org.apache.helix.zookeeper.constant;\n+\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+public class ZkSystemPropertyKeys {\n+  /**\n+   * Setting this property to true in system properties enables auto compression in ZK serializer.\n+   * The data will be automatically compressed by\n+   * {@link org.apache.helix.zookeeper.util.GZipCompressionUtil} when being written to Zookeeper\n+   * if size of serialized data exceeds the configured threshold.\n+   */\n+  public static final String ZK_SERIALIZER_AUTO_COMPRESSION_ENABLED =\n+      \"zk.serializer.auto-compression.enabled\";\n+\n+  /**\n+   * This is property that defines the threshold in bytes for auto compression in ZK serializer.\n+   * Given auto compression is enabled, if the size of data exceeds this configured threshold,\n+   * the data will be automatically compressed when being written to Zookeeper.\n+   */\n+  public static final String ZK_SERIALIZER_AUTO_COMPRESSION_THRESHOLD_BYTES =", "originalCommit": "3d772ffd135b8d648ec914fa253002dce9b5616c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTM3MzMxNA==", "url": "https://github.com/apache/helix/pull/809#discussion_r385373314", "bodyText": "The default should be ENABLED if this is for Pinot, right?", "author": "lei-xia", "createdAt": "2020-02-27T21:11:16Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/datamodel/serializer/ZNRecordSerializer.java", "diffHunk": "@@ -38,6 +39,17 @@\n public class ZNRecordSerializer implements ZkSerializer {\n   private static Logger logger = LoggerFactory.getLogger(ZNRecordSerializer.class);\n \n+  // Reads from system property and represents whether auto compression is enabled or not.\n+  // If and only if this property is set to \"true\", auto compression is enabled.\n+  private final boolean autoCompressionEnabled =\n+      Boolean.getBoolean(ZkSystemPropertyKeys.ZK_SERIALIZER_AUTO_COMPRESSION_ENABLED);", "originalCommit": "3d772ffd135b8d648ec914fa253002dce9b5616c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ2MzQ0NA==", "url": "https://github.com/apache/helix/pull/809#discussion_r385463444", "bodyText": "Yes. I had a misunderstanding and with current implementation, we make it enabled by default.", "author": "huizhilu", "createdAt": "2020-02-28T01:26:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTM3MzMxNA=="}], "type": "inlineReview"}, {"oid": "7f8d8874d850409b42bb2fb31fbe3cf8d295c06b", "url": "https://github.com/apache/helix/commit/7f8d8874d850409b42bb2fb31fbe3cf8d295c06b", "message": "Enable auto compression by default", "committedDate": "2020-02-27T23:09:20Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQyNDk5NA==", "url": "https://github.com/apache/helix/pull/809#discussion_r385424994", "bodyText": "Remove it.", "author": "junkaixue", "createdAt": "2020-02-27T23:13:32Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/datamodel/ZNRecord.java", "diffHunk": "@@ -49,6 +50,16 @@\n   @JsonIgnore(true)\n   public static final String LIST_FIELD_BOUND = \"listField.bound\";\n \n+  @JsonIgnore\n+  public static final String ENABLE_COMPRESSION_BOOLEAN_FIELD = \"enableCompression\";\n+\n+  /**\n+   * Default value for system property\n+   * {@link ZkSystemPropertyKeys#ZNRECORD_SERIALIZER_COMPRESS}\n+   */\n+  @JsonIgnore\n+  public static final String ZNRECORD_SERIALIZER_COMPRESS_DEFAULT_VALUE = \"true\";", "originalCommit": "7f8d8874d850409b42bb2fb31fbe3cf8d295c06b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQyNTM1Mw==", "url": "https://github.com/apache/helix/pull/809#discussion_r385425353", "bodyText": "We should not allow 0", "author": "junkaixue", "createdAt": "2020-02-27T23:14:43Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/datamodel/ZNRecord.java", "diffHunk": "@@ -621,6 +632,22 @@ public void subtract(ZNRecord value) {\n     }\n   }\n \n+  /**\n+   * Returns compression threshold in bytes. The threshold is a smaller number determined by the\n+   * configured threshold and {@link ZNRecord#SIZE_LIMIT}.\n+   *\n+   * @return compress threshold in bytes\n+   */\n+  @JsonIgnore\n+  public int getCompressThreshold() {\n+    Integer threshold =\n+        Integer.getInteger(ZkSystemPropertyKeys.ZNRECORD_SERIALIZER_COMPRESS_THRESHOLD_BYTES);\n+    if (threshold == null || threshold < 0 || threshold > ZNRecord.SIZE_LIMIT) {", "originalCommit": "7f8d8874d850409b42bb2fb31fbe3cf8d295c06b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNTgyOQ==", "url": "https://github.com/apache/helix/pull/809#discussion_r385435829", "bodyText": "Done.", "author": "huizhilu", "createdAt": "2020-02-27T23:48:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQyNTM1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQyNjEyNA==", "url": "https://github.com/apache/helix/pull/809#discussion_r385426124", "bodyText": "Move it to util", "author": "junkaixue", "createdAt": "2020-02-27T23:17:04Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/datamodel/ZNRecord.java", "diffHunk": "@@ -621,6 +632,22 @@ public void subtract(ZNRecord value) {\n     }\n   }\n \n+  /**\n+   * Returns compression threshold in bytes. The threshold is a smaller number determined by the\n+   * configured threshold and {@link ZNRecord#SIZE_LIMIT}.\n+   *\n+   * @return compress threshold in bytes\n+   */\n+  @JsonIgnore\n+  public int getCompressThreshold() {", "originalCommit": "7f8d8874d850409b42bb2fb31fbe3cf8d295c06b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQyNzUxNw==", "url": "https://github.com/apache/helix/pull/809#discussion_r385427517", "bodyText": "Let's put a TODO here for further clean up for the logs.", "author": "junkaixue", "createdAt": "2020-02-27T23:21:20Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/datamodel/serializer/ZNRecordSerializer.java", "diffHunk": "@@ -81,24 +82,38 @@ private static int getListFieldBound(ZNRecord record) {\n     serializationConfig.set(SerializationConfig.Feature.AUTO_DETECT_FIELDS, true);\n     serializationConfig.set(SerializationConfig.Feature.CAN_OVERRIDE_ACCESS_MODIFIERS, true);\n     ByteArrayOutputStream baos = new ByteArrayOutputStream();\n-    byte[] serializedBytes;\n+    byte[] serializedBytes = new byte[0];\n     try {\n       mapper.writeValue(baos, data);\n       serializedBytes = baos.toByteArray();\n+\n       // apply compression if needed\n-      if (record.getBooleanField(\"enableCompression\", false) || serializedBytes.length > ZNRecord.SIZE_LIMIT) {\n+      if (ZNRecordUtil.shouldCompress(record, serializedBytes.length)) {\n         serializedBytes = GZipCompressionUtil.compress(serializedBytes);\n       }\n     } catch (Exception e) {\n-      logger.error(\"Exception during data serialization. Will not write to zk. Data (first 1k): \"\n-          + new String(baos.toByteArray()).substring(0, 1024), e);\n+      if (serializedBytes.length == 0 || GZipCompressionUtil.isCompressed(serializedBytes)) {\n+        serializedBytes = baos.toByteArray();\n+      }\n+      int firstBytesLength = Math.min(serializedBytes.length, 1024);\n+      LOG.error(\"Exception during data serialization. Will not write to zk.\"\n+              + \" The first {} bytes of data: {}\", firstBytesLength,\n+          new String(serializedBytes, 0, firstBytesLength), e);\n       throw new ZkClientException(e);\n     }\n-    if (serializedBytes.length > ZNRecord.SIZE_LIMIT) {\n-      logger.error(\"Data size larger than 1M, ZNRecord.id: \" + record.getId()\n-          + \". Will not write to zk. Data (first 1k): \"\n-          + new String(serializedBytes).substring(0, 1024));\n-      throw new ZkClientException(\"Data size larger than 1M, ZNRecord.id: \" + record.getId());\n+\n+    int compressThreshold = record.getCompressThreshold();\n+    if (serializedBytes.length > compressThreshold) {\n+      if (GZipCompressionUtil.isCompressed(serializedBytes)) {", "originalCommit": "7f8d8874d850409b42bb2fb31fbe3cf8d295c06b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNTg4OQ==", "url": "https://github.com/apache/helix/pull/809#discussion_r385435889", "bodyText": "Done.", "author": "huizhilu", "createdAt": "2020-02-27T23:48:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQyNzUxNw=="}], "type": "inlineReview"}, {"oid": "1ed24303840f68aa50a1b95625806e200c453978", "url": "https://github.com/apache/helix/commit/1ed24303840f68aa50a1b95625806e200c453978", "message": "Address comments.", "committedDate": "2020-02-27T23:52:28Z", "type": "forcePushed"}, {"oid": "0c85da48438242469357f81f5bd8bdda0e0d0734", "url": "https://github.com/apache/helix/commit/0c85da48438242469357f81f5bd8bdda0e0d0734", "message": "Address comments.", "committedDate": "2020-02-27T23:56:01Z", "type": "forcePushed"}, {"oid": "8bf289f1aeb8d8b4ff2a5b0d8c6f18f1547260a7", "url": "https://github.com/apache/helix/commit/8bf289f1aeb8d8b4ff2a5b0d8c6f18f1547260a7", "message": "Address comments.", "committedDate": "2020-02-28T01:08:48Z", "type": "forcePushed"}, {"oid": "99b29378b98550fbda2da2588f8bedf4a469a039", "url": "https://github.com/apache/helix/commit/99b29378b98550fbda2da2588f8bedf4a469a039", "message": "Add tests", "committedDate": "2020-02-28T01:12:41Z", "type": "commit"}, {"oid": "99b29378b98550fbda2da2588f8bedf4a469a039", "url": "https://github.com/apache/helix/commit/99b29378b98550fbda2da2588f8bedf4a469a039", "message": "Add tests", "committedDate": "2020-02-28T01:12:41Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ2NDQwNQ==", "url": "https://github.com/apache/helix/pull/809#discussion_r385464405", "bodyText": "nit: getCompressionThreshold", "author": "narendly", "createdAt": "2020-02-28T01:29:54Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/util/ZNRecordUtil.java", "diffHunk": "@@ -0,0 +1,59 @@\n+package org.apache.helix.zookeeper.util;\n+\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+import org.apache.helix.zookeeper.constant.ZkSystemPropertyKeys;\n+import org.apache.helix.zookeeper.datamodel.ZNRecord;\n+\n+\n+/**\n+ * This utility class contains various methods for manipulating ZNRecord.\n+ */\n+public class ZNRecordUtil {\n+\n+  /**\n+   * Checks whether or not a serialized ZNRecord bytes should be compressed before being written to\n+   * Zookeeper.\n+   *\n+   * @param record raw ZNRecord before being serialized\n+   * @param serializedLength length of the serialized bytes array\n+   * @return\n+   */\n+  public static boolean shouldCompress(ZNRecord record, int serializedLength) {\n+    if (record.getBooleanField(ZNRecord.ENABLE_COMPRESSION_BOOLEAN_FIELD, false)) {\n+      return true;\n+    }\n+\n+    return serializedLength > getCompressThreshold();\n+  }\n+\n+  /**\n+   * Returns compression threshold in bytes. The threshold is a smaller number determined by the\n+   * configured threshold and {@link ZNRecord#SIZE_LIMIT}.\n+   */\n+  public static int getCompressThreshold() {", "originalCommit": "99b29378b98550fbda2da2588f8bedf4a469a039", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ2ODIxMg==", "url": "https://github.com/apache/helix/pull/809#discussion_r385468212", "bodyText": "I was thinking of both. I made it to follow the config \u201ccompress.threshold\u201d which I intentionally make it a short name. But either way works for me.", "author": "huizhilu", "createdAt": "2020-02-28T01:45:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ2NDQwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ2NTc3NQ==", "url": "https://github.com/apache/helix/pull/809#discussion_r385465775", "bodyText": "I think it might be a good idea to also log and include in the exception message whether the ZNRecord was compressed or not. Adding a boolean isCompressed would do?", "author": "narendly", "createdAt": "2020-02-28T01:35:12Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/datamodel/serializer/ZNRecordStreamingSerializer.java", "diffHunk": "@@ -154,20 +155,31 @@ private static int getListFieldBound(ZNRecord record) {\n       g.close();\n       serializedBytes = baos.toByteArray();\n       // apply compression if needed\n-      if (record.getBooleanField(\"enableCompression\", false) || serializedBytes.length > ZNRecord.SIZE_LIMIT) {\n+      if (ZNRecordUtil.shouldCompress(record, serializedBytes.length)) {\n         serializedBytes = GZipCompressionUtil.compress(serializedBytes);\n       }\n     } catch (Exception e) {\n-      LOG.error(\"Exception during data serialization. Will not write to zk. Data (first 1k): \"\n-          + new String(baos.toByteArray()).substring(0, 1024), e);\n+      if (serializedBytes.length == 0 || GZipCompressionUtil.isCompressed(serializedBytes)) {\n+        serializedBytes = baos.toByteArray();\n+      }\n+      int firstBytesLength = Math.min(serializedBytes.length, 1024);\n+      // TODO: remove logging first N bytes of data to reduce log size.\n+      LOG.error(\"Exception during data serialization. Will not write to zk.\"\n+              + \" The first {} bytes of data: {}\", firstBytesLength,\n+          new String(serializedBytes, 0, firstBytesLength), e);\n       throw new ZkClientException(e);\n     }\n     // check size\n-    if (serializedBytes.length > ZNRecord.SIZE_LIMIT) {\n-      LOG.error(\"Data size larger than 1M, ZNRecord.id: \" + record.getId()\n-          + \". Will not write to zk. Data (first 1k): \"\n-          + new String(serializedBytes).substring(0, 1024));\n-      throw new ZkClientException(\"Data size larger than 1M, ZNRecord.id: \" + record.getId());\n+    int compressThreshold = ZNRecordUtil.getCompressThreshold();\n+    if (serializedBytes.length > compressThreshold) {\n+      int firstBytesLength = Math.min(serializedBytes.length, 1024);\n+      // TODO: remove logging first N bytes of data to reduce log size.\n+      LOG.error(\"Data size: {} is greater than {} bytes, ZNRecord.id: {}.\"\n+              + \" Data will not be written to Zookeeper. The first {} bytes of data: {}\",\n+          serializedBytes.length, compressThreshold, record.getId(), firstBytesLength,\n+          new String(serializedBytes, 0, firstBytesLength));\n+      throw new ZkClientException(\"Data size: \" + serializedBytes.length + \" is greater than \"\n+          + compressThreshold + \" bytes, ZNRecord.id: \" + record.getId());", "originalCommit": "99b29378b98550fbda2da2588f8bedf4a469a039", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ3MjM3Mg==", "url": "https://github.com/apache/helix/pull/809#discussion_r385472372", "bodyText": "Same with above reply.", "author": "huizhilu", "createdAt": "2020-02-28T02:01:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ2NTc3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ2NTgzMA==", "url": "https://github.com/apache/helix/pull/809#discussion_r385465830", "bodyText": "I think it might be a good idea to also log and include in the exception message whether the ZNRecord was compressed or not. Adding a boolean isCompressed would do?", "author": "narendly", "createdAt": "2020-02-28T01:35:22Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/datamodel/serializer/ZNRecordSerializer.java", "diffHunk": "@@ -81,24 +82,39 @@ private static int getListFieldBound(ZNRecord record) {\n     serializationConfig.set(SerializationConfig.Feature.AUTO_DETECT_FIELDS, true);\n     serializationConfig.set(SerializationConfig.Feature.CAN_OVERRIDE_ACCESS_MODIFIERS, true);\n     ByteArrayOutputStream baos = new ByteArrayOutputStream();\n-    byte[] serializedBytes;\n+    byte[] serializedBytes = new byte[0];\n     try {\n       mapper.writeValue(baos, data);\n       serializedBytes = baos.toByteArray();\n       // apply compression if needed\n-      if (record.getBooleanField(\"enableCompression\", false) || serializedBytes.length > ZNRecord.SIZE_LIMIT) {\n+      if (ZNRecordUtil.shouldCompress(record, serializedBytes.length)) {\n         serializedBytes = GZipCompressionUtil.compress(serializedBytes);\n       }\n     } catch (Exception e) {\n-      logger.error(\"Exception during data serialization. Will not write to zk. Data (first 1k): \"\n-          + new String(baos.toByteArray()).substring(0, 1024), e);\n+      if (serializedBytes.length == 0 || GZipCompressionUtil.isCompressed(serializedBytes)) {\n+        serializedBytes = baos.toByteArray();\n+      }\n+      int firstBytesLength = Math.min(serializedBytes.length, 1024);\n+      // TODO: remove logging first N bytes of data to reduce log size.\n+      LOG.error(\"Exception during data serialization. Will not write to zk.\"\n+              + \" The first {} bytes of data: {}\", firstBytesLength,\n+          new String(serializedBytes, 0, firstBytesLength), e);\n       throw new ZkClientException(e);\n     }\n-    if (serializedBytes.length > ZNRecord.SIZE_LIMIT) {\n-      logger.error(\"Data size larger than 1M, ZNRecord.id: \" + record.getId()\n-          + \". Will not write to zk. Data (first 1k): \"\n-          + new String(serializedBytes).substring(0, 1024));\n-      throw new ZkClientException(\"Data size larger than 1M, ZNRecord.id: \" + record.getId());\n+\n+    int compressThreshold = ZNRecordUtil.getCompressThreshold();\n+    if (serializedBytes.length > compressThreshold) {\n+      if (GZipCompressionUtil.isCompressed(serializedBytes)) {\n+        serializedBytes = baos.toByteArray();\n+      }\n+      int firstBytesLength = Math.min(serializedBytes.length, 1024);\n+      // TODO: remove logging first N bytes of data to reduce log size.\n+      LOG.error(\"Data size: {} is greater than {} bytes, ZNRecord.id: {}.\"\n+              + \" Data will not be written to Zookeeper. The first {} bytes of data: {}\",\n+          serializedBytes.length, compressThreshold, record.getId(), firstBytesLength,\n+          new String(serializedBytes, 0, firstBytesLength));\n+      throw new ZkClientException(\"Data size: \" + serializedBytes.length + \" is greater than \"\n+          + compressThreshold + \" bytes, ZNRecord.id: \" + record.getId());", "originalCommit": "99b29378b98550fbda2da2588f8bedf4a469a039", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ3MjIyNw==", "url": "https://github.com/apache/helix/pull/809#discussion_r385472227", "bodyText": "I thought about that. But with current logic, it must has been compressed if we get this exception because currently we are using the same threshold for compression and Znode limit.. I agree that isCompressed could help, if we are using a different threshold for Znode size limit.\nUpdate: with updated logic that provides a config to limit serialized znrecord size, we log being compressed or not.", "author": "huizhilu", "createdAt": "2020-02-28T02:01:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ2NTgzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ2NjI4MQ==", "url": "https://github.com/apache/helix/pull/809#discussion_r385466281", "bodyText": "Nit: Good to add a TestHelper check here given how often deleteRecursively could fail. To make sure things have been really cleaned up.", "author": "narendly", "createdAt": "2020-02-28T01:37:05Z", "path": "helix-core/src/test/java/org/apache/helix/manager/zk/TestZNRecordSizeLimit.java", "diffHunk": "@@ -290,4 +292,176 @@ public void testZNRecordSizeLimitUseZNRecordStreamingSerializer() {\n     System.out.println(\"END testZNRecordSizeLimitUseZNRecordStreamingSerializer at \" + new Date(\n         System.currentTimeMillis()));\n   }\n+\n+  /*\n+   * Tests ZNRecordSerializer auto compression threshold.\n+   * Two cases:\n+   * 1. serialized data size is less than threshold and could be written to ZK.\n+   * 2. serialized data size is greater than threshold, so ZkClientException is thrown.\n+   */\n+  @Test(dependsOnMethods = \"testZNRecordSizeLimitUseZNRecordStreamingSerializer\")\n+  public void testZNRecordSerializerCompressThreshold() {\n+    // Backup properties for later resetting.\n+    final String compressionThresholdProperty =\n+        System.getProperty(ZkSystemPropertyKeys.ZNRECORD_SERIALIZER_COMPRESS_THRESHOLD_BYTES);\n+\n+    ZNRecordSerializer serializer = new ZNRecordSerializer();\n+\n+    String root = getShortClassName();\n+\n+    byte[] buf = new byte[1024];\n+    for (int i = 0; i < 1024; i++) {\n+      buf[i] = 'a';\n+    }\n+    String bufStr = new String(buf);\n+\n+    // 1. legal-sized data gets written to zk\n+    // write a znode of size less than threshold\n+    int rawZnRecordSize = 900;\n+    int thresholdKB = 800;\n+    int compressionThreshold = thresholdKB * 1024;\n+    System.setProperty(ZkSystemPropertyKeys.ZNRECORD_SERIALIZER_COMPRESS_THRESHOLD_BYTES,\n+        String.valueOf(compressionThreshold));\n+\n+    final ZNRecord normalSizeRecord = new ZNRecord(\"normal-size\");\n+    for (int i = 0; i < rawZnRecordSize; i++) {\n+      normalSizeRecord.setSimpleField(Integer.toString(i), bufStr);\n+    }\n+\n+    String path = \"/\" + root + \"/normal\";\n+    _gZkClient.createPersistent(path, true);\n+    _gZkClient.writeData(path, normalSizeRecord);\n+\n+    ZNRecord record = _gZkClient.readData(path);\n+\n+    // Successfully reads the same data.\n+    Assert.assertEquals(normalSizeRecord, record);\n+\n+    int length = serializer.serialize(record).length;\n+\n+    // Less than compression threshold so it is written to ZK.\n+    Assert.assertTrue(length < compressionThreshold);\n+\n+    // 2. Large size data is not allowed to write to ZK\n+    // Set raw record size to be large enough so its compressed data exceeds the threshold.\n+    rawZnRecordSize = 5000;\n+    // Set the threshold to very small so compressed data size exceeds the threshold.\n+    thresholdKB = 1;\n+    compressionThreshold = thresholdKB * 1024;\n+    System.setProperty(ZkSystemPropertyKeys.ZNRECORD_SERIALIZER_COMPRESS_THRESHOLD_BYTES,\n+        String.valueOf(compressionThreshold));\n+\n+    final ZNRecord largeRecord = new ZNRecord(\"large-size\");\n+    for (int i = 0; i < rawZnRecordSize; i++) {\n+      largeRecord.setSimpleField(Integer.toString(i), bufStr);\n+    }\n+\n+    path = \"/\" + root + \"/large\";\n+    _gZkClient.createPersistent(path, true);\n+\n+    try {\n+      _gZkClient.writeData(path, largeRecord);\n+      Assert.fail(\"Data should not written to ZK because data size exceeds threshold!\");\n+    } catch (ZkClientException expected) {\n+      Assert.assertTrue(\n+          expected.getMessage().contains(\" is greater than \" + compressionThreshold + \" bytes\"));\n+    }\n+\n+    // Delete the nodes.\n+    _gZkClient.deleteRecursively(\"/\" + root);", "originalCommit": "99b29378b98550fbda2da2588f8bedf4a469a039", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ2NjM5OQ==", "url": "https://github.com/apache/helix/pull/809#discussion_r385466399", "bodyText": "Nit: Good to add a TestHelper check here given how often deleteRecursively could fail. To make sure things have been really cleaned up.", "author": "narendly", "createdAt": "2020-02-28T01:37:32Z", "path": "helix-core/src/test/java/org/apache/helix/manager/zk/TestZNRecordSizeLimit.java", "diffHunk": "@@ -290,4 +292,176 @@ public void testZNRecordSizeLimitUseZNRecordStreamingSerializer() {\n     System.out.println(\"END testZNRecordSizeLimitUseZNRecordStreamingSerializer at \" + new Date(\n         System.currentTimeMillis()));\n   }\n+\n+  /*\n+   * Tests ZNRecordSerializer auto compression threshold.\n+   * Two cases:\n+   * 1. serialized data size is less than threshold and could be written to ZK.\n+   * 2. serialized data size is greater than threshold, so ZkClientException is thrown.\n+   */\n+  @Test(dependsOnMethods = \"testZNRecordSizeLimitUseZNRecordStreamingSerializer\")\n+  public void testZNRecordSerializerCompressThreshold() {\n+    // Backup properties for later resetting.\n+    final String compressionThresholdProperty =\n+        System.getProperty(ZkSystemPropertyKeys.ZNRECORD_SERIALIZER_COMPRESS_THRESHOLD_BYTES);\n+\n+    ZNRecordSerializer serializer = new ZNRecordSerializer();\n+\n+    String root = getShortClassName();\n+\n+    byte[] buf = new byte[1024];\n+    for (int i = 0; i < 1024; i++) {\n+      buf[i] = 'a';\n+    }\n+    String bufStr = new String(buf);\n+\n+    // 1. legal-sized data gets written to zk\n+    // write a znode of size less than threshold\n+    int rawZnRecordSize = 900;\n+    int thresholdKB = 800;\n+    int compressionThreshold = thresholdKB * 1024;\n+    System.setProperty(ZkSystemPropertyKeys.ZNRECORD_SERIALIZER_COMPRESS_THRESHOLD_BYTES,\n+        String.valueOf(compressionThreshold));\n+\n+    final ZNRecord normalSizeRecord = new ZNRecord(\"normal-size\");\n+    for (int i = 0; i < rawZnRecordSize; i++) {\n+      normalSizeRecord.setSimpleField(Integer.toString(i), bufStr);\n+    }\n+\n+    String path = \"/\" + root + \"/normal\";\n+    _gZkClient.createPersistent(path, true);\n+    _gZkClient.writeData(path, normalSizeRecord);\n+\n+    ZNRecord record = _gZkClient.readData(path);\n+\n+    // Successfully reads the same data.\n+    Assert.assertEquals(normalSizeRecord, record);\n+\n+    int length = serializer.serialize(record).length;\n+\n+    // Less than compression threshold so it is written to ZK.\n+    Assert.assertTrue(length < compressionThreshold);\n+\n+    // 2. Large size data is not allowed to write to ZK\n+    // Set raw record size to be large enough so its compressed data exceeds the threshold.\n+    rawZnRecordSize = 5000;\n+    // Set the threshold to very small so compressed data size exceeds the threshold.\n+    thresholdKB = 1;\n+    compressionThreshold = thresholdKB * 1024;\n+    System.setProperty(ZkSystemPropertyKeys.ZNRECORD_SERIALIZER_COMPRESS_THRESHOLD_BYTES,\n+        String.valueOf(compressionThreshold));\n+\n+    final ZNRecord largeRecord = new ZNRecord(\"large-size\");\n+    for (int i = 0; i < rawZnRecordSize; i++) {\n+      largeRecord.setSimpleField(Integer.toString(i), bufStr);\n+    }\n+\n+    path = \"/\" + root + \"/large\";\n+    _gZkClient.createPersistent(path, true);\n+\n+    try {\n+      _gZkClient.writeData(path, largeRecord);\n+      Assert.fail(\"Data should not written to ZK because data size exceeds threshold!\");\n+    } catch (ZkClientException expected) {\n+      Assert.assertTrue(\n+          expected.getMessage().contains(\" is greater than \" + compressionThreshold + \" bytes\"));\n+    }\n+\n+    // Delete the nodes.\n+    _gZkClient.deleteRecursively(\"/\" + root);\n+\n+    // Reset: add the properties back to system properties if they were originally available.\n+    if (compressionThresholdProperty != null) {\n+      System.setProperty(ZkSystemPropertyKeys.ZNRECORD_SERIALIZER_COMPRESS_THRESHOLD_BYTES,\n+          compressionThresholdProperty);\n+    } else {\n+      System.clearProperty(ZkSystemPropertyKeys.ZNRECORD_SERIALIZER_COMPRESS_THRESHOLD_BYTES);\n+    }\n+  }\n+\n+  /*\n+   * Tests ZNRecordStreamingSerializer auto compression threshold.\n+   * Two cases:\n+   * 1. serialized data size is less than threshold and could be written to ZK.\n+   * 2. serialized data size is greater than threshold, so ZkClientException is thrown.\n+   */\n+  @Test(dependsOnMethods = \"testZNRecordSerializerCompressThreshold\")\n+  public void testZNRecordStreamingSerializerCompressThreshold() {\n+    // Backup properties for later resetting.\n+    final String compressionThresholdProperty =\n+        System.getProperty(ZkSystemPropertyKeys.ZNRECORD_SERIALIZER_COMPRESS_THRESHOLD_BYTES);\n+\n+    ZNRecordStreamingSerializer serializer = new ZNRecordStreamingSerializer();\n+\n+    String root = getShortClassName();\n+\n+    byte[] buf = new byte[1024];\n+    for (int i = 0; i < 1024; i++) {\n+      buf[i] = 'a';\n+    }\n+    String bufStr = new String(buf);\n+\n+    // 1. legal-sized data gets written to zk\n+    // write a znode of size less than threshold\n+    int rawZnRecordSize = 900;\n+    int thresholdKB = 800;\n+    int compressionThreshold = thresholdKB * 1024;\n+    System.setProperty(ZkSystemPropertyKeys.ZNRECORD_SERIALIZER_COMPRESS_THRESHOLD_BYTES,\n+        String.valueOf(compressionThreshold));\n+\n+    final ZNRecord normalSizeRecord = new ZNRecord(\"normal-size\");\n+    for (int i = 0; i < rawZnRecordSize; i++) {\n+      normalSizeRecord.setSimpleField(Integer.toString(i), bufStr);\n+    }\n+\n+    String path = \"/\" + root + \"/normal\";\n+    _gZkClient.createPersistent(path, true);\n+    _gZkClient.writeData(path, normalSizeRecord);\n+\n+    ZNRecord record = _gZkClient.readData(path);\n+\n+    // Successfully reads the same data.\n+    Assert.assertEquals(normalSizeRecord, record);\n+\n+    int length = serializer.serialize(record).length;\n+\n+    // Less than compression threshold so it is written to ZK.\n+    Assert.assertTrue(length < compressionThreshold);\n+\n+    // 2. Large size data is not allowed to write to ZK\n+    // Set raw record size to be large enough so its compressed data exceeds the threshold.\n+    rawZnRecordSize = 5000;\n+    // Set the threshold to very small so compressed data size exceeds the threshold.\n+    thresholdKB = 1;\n+    compressionThreshold = thresholdKB * 1024;\n+    System.setProperty(ZkSystemPropertyKeys.ZNRECORD_SERIALIZER_COMPRESS_THRESHOLD_BYTES,\n+        String.valueOf(compressionThreshold));\n+\n+    final ZNRecord largeRecord = new ZNRecord(\"large-size\");\n+    for (int i = 0; i < rawZnRecordSize; i++) {\n+      largeRecord.setSimpleField(Integer.toString(i), bufStr);\n+    }\n+\n+    path = \"/\" + root + \"/large\";\n+    _gZkClient.createPersistent(path, true);\n+\n+    try {\n+      _gZkClient.writeData(path, largeRecord);\n+      Assert.fail(\"Data should not written to ZK because data size exceeds threshold!\");\n+    } catch (ZkClientException expected) {\n+      Assert.assertTrue(\n+          expected.getMessage().contains(\" is greater than \" + compressionThreshold + \" bytes\"));\n+    }\n+\n+    // Delete the nodes.\n+    _gZkClient.deleteRecursively(\"/\" + root);", "originalCommit": "99b29378b98550fbda2da2588f8bedf4a469a039", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ5MjY3OA==", "url": "https://github.com/apache/helix/pull/809#discussion_r385492678", "bodyText": "Done.", "author": "huizhilu", "createdAt": "2020-02-28T03:34:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ2NjM5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ2ODQyNg==", "url": "https://github.com/apache/helix/pull/809#discussion_r385468426", "bodyText": "Shouldn't this be\nserializedBytes.length > 1MB (or ZK's Djute.maxbuffer)? \nA case to consider:\nSuppose some user wants every ZNode compressed. So sets this threshold to be 1 byte.\nI write 1 byte, and this 1 byte gets compressed (and after compression, say it's greater than or equal to 1 byte), and this write will not go through even after compression because you won't ever allow any writes larger than 1 byte.\nNow I am confused because I set the \"compression threshold\", not the \"ZNode size threshold\"?\nI'm pretty sure this isn't the intended behavior?\nSame thing applies to ZNRecordSerializer.\nHint: I think it might be a good idea to decouple compression threshold from ZNode size limit.", "author": "narendly", "createdAt": "2020-02-28T01:45:48Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/datamodel/serializer/ZNRecordStreamingSerializer.java", "diffHunk": "@@ -154,20 +155,31 @@ private static int getListFieldBound(ZNRecord record) {\n       g.close();\n       serializedBytes = baos.toByteArray();\n       // apply compression if needed\n-      if (record.getBooleanField(\"enableCompression\", false) || serializedBytes.length > ZNRecord.SIZE_LIMIT) {\n+      if (ZNRecordUtil.shouldCompress(record, serializedBytes.length)) {\n         serializedBytes = GZipCompressionUtil.compress(serializedBytes);\n       }\n     } catch (Exception e) {\n-      LOG.error(\"Exception during data serialization. Will not write to zk. Data (first 1k): \"\n-          + new String(baos.toByteArray()).substring(0, 1024), e);\n+      if (serializedBytes.length == 0 || GZipCompressionUtil.isCompressed(serializedBytes)) {\n+        serializedBytes = baos.toByteArray();\n+      }\n+      int firstBytesLength = Math.min(serializedBytes.length, 1024);\n+      // TODO: remove logging first N bytes of data to reduce log size.\n+      LOG.error(\"Exception during data serialization. Will not write to zk.\"\n+              + \" The first {} bytes of data: {}\", firstBytesLength,\n+          new String(serializedBytes, 0, firstBytesLength), e);\n       throw new ZkClientException(e);\n     }\n     // check size\n-    if (serializedBytes.length > ZNRecord.SIZE_LIMIT) {\n-      LOG.error(\"Data size larger than 1M, ZNRecord.id: \" + record.getId()\n-          + \". Will not write to zk. Data (first 1k): \"\n-          + new String(serializedBytes).substring(0, 1024));\n-      throw new ZkClientException(\"Data size larger than 1M, ZNRecord.id: \" + record.getId());\n+    int compressThreshold = ZNRecordUtil.getCompressThreshold();\n+    if (serializedBytes.length > compressThreshold) {", "originalCommit": "99b29378b98550fbda2da2588f8bedf4a469a039", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ5MzEzMA==", "url": "https://github.com/apache/helix/pull/809#discussion_r385493130", "bodyText": "Discussed offline. We provide a config to limit serialized data size. Keep the old behavior for compression.", "author": "huizhilu", "createdAt": "2020-02-28T03:36:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ2ODQyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ2OTA3Mg==", "url": "https://github.com/apache/helix/pull/809#discussion_r385469072", "bodyText": "Nit: did you mean to make this @JsonIgnore(true) ?", "author": "narendly", "createdAt": "2020-02-28T01:48:23Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/datamodel/ZNRecord.java", "diffHunk": "@@ -49,6 +50,10 @@\n   @JsonIgnore(true)\n   public static final String LIST_FIELD_BOUND = \"listField.bound\";\n \n+  /** A field name in ZNRecord's boolean fields to enable compression in ZNRecord serializers. */\n+  @JsonIgnore", "originalCommit": "99b29378b98550fbda2da2588f8bedf4a469a039", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ3MTU3Mg==", "url": "https://github.com/apache/helix/pull/809#discussion_r385471572", "bodyText": "It is redundant to add true because it is true by default.", "author": "huizhilu", "createdAt": "2020-02-28T01:58:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ2OTA3Mg=="}], "type": "inlineReview"}, {"oid": "f72feac2e51b3bd38503d3841374be13dfb8c348", "url": "https://github.com/apache/helix/commit/f72feac2e51b3bd38503d3841374be13dfb8c348", "message": "Change config to limit serialized znrecord size.", "committedDate": "2020-02-28T03:34:11Z", "type": "commit"}, {"oid": "f72feac2e51b3bd38503d3841374be13dfb8c348", "url": "https://github.com/apache/helix/commit/f72feac2e51b3bd38503d3841374be13dfb8c348", "message": "Change config to limit serialized znrecord size.", "committedDate": "2020-02-28T03:34:11Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTU0NDAwNQ==", "url": "https://github.com/apache/helix/pull/809#discussion_r385544005", "bodyText": "nit: do we want to use long here?", "author": "narendly", "createdAt": "2020-02-28T07:30:13Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/datamodel/serializer/ZNRecordSerializer.java", "diffHunk": "@@ -81,24 +82,44 @@ private static int getListFieldBound(ZNRecord record) {\n     serializationConfig.set(SerializationConfig.Feature.AUTO_DETECT_FIELDS, true);\n     serializationConfig.set(SerializationConfig.Feature.CAN_OVERRIDE_ACCESS_MODIFIERS, true);\n     ByteArrayOutputStream baos = new ByteArrayOutputStream();\n-    byte[] serializedBytes;\n+    byte[] serializedBytes = new byte[0];\n+    boolean isCompressed = false;\n+\n     try {\n       mapper.writeValue(baos, data);\n       serializedBytes = baos.toByteArray();\n       // apply compression if needed\n-      if (record.getBooleanField(\"enableCompression\", false) || serializedBytes.length > ZNRecord.SIZE_LIMIT) {\n+      if (ZNRecordUtil.shouldCompress(record, serializedBytes.length)) {\n         serializedBytes = GZipCompressionUtil.compress(serializedBytes);\n+        isCompressed = true;\n       }\n     } catch (Exception e) {\n-      logger.error(\"Exception during data serialization. Will not write to zk. Data (first 1k): \"\n-          + new String(baos.toByteArray()).substring(0, 1024), e);\n+      if (serializedBytes.length == 0 || GZipCompressionUtil.isCompressed(serializedBytes)) {\n+        serializedBytes = baos.toByteArray();\n+      }\n+      int firstBytesLength = Math.min(serializedBytes.length, 1024);\n+      // TODO: remove logging first N bytes of data to reduce log size.\n+      LOG.error(\"Exception during data serialization. Will not write to zk.\"\n+              + \" The first {} bytes of data: {}\", firstBytesLength,\n+          new String(serializedBytes, 0, firstBytesLength), e);\n       throw new ZkClientException(e);\n     }\n-    if (serializedBytes.length > ZNRecord.SIZE_LIMIT) {\n-      logger.error(\"Data size larger than 1M, ZNRecord.id: \" + record.getId()\n-          + \". Will not write to zk. Data (first 1k): \"\n-          + new String(serializedBytes).substring(0, 1024));\n-      throw new ZkClientException(\"Data size larger than 1M, ZNRecord.id: \" + record.getId());\n+\n+    int threshold = ZNRecordUtil.getSerializerThreshold();", "originalCommit": "f72feac2e51b3bd38503d3841374be13dfb8c348", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTU1NDY3OA==", "url": "https://github.com/apache/helix/pull/809#discussion_r385554678", "bodyText": "I don't think it is necessary because we have a 1 MB as base. I don't think people would set it larger than a max integer. And we do have a check for it. If it is larger than max integer, the property would be parsed to a null and we will use 1 MB.", "author": "huizhilu", "createdAt": "2020-02-28T08:03:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTU0NDAwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTU0NDcyOA==", "url": "https://github.com/apache/helix/pull/809#discussion_r385544728", "bodyText": "Either use length throughout or just use serializedBytes.length?", "author": "narendly", "createdAt": "2020-02-28T07:32:11Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/datamodel/serializer/ZNRecordSerializer.java", "diffHunk": "@@ -81,24 +82,44 @@ private static int getListFieldBound(ZNRecord record) {\n     serializationConfig.set(SerializationConfig.Feature.AUTO_DETECT_FIELDS, true);\n     serializationConfig.set(SerializationConfig.Feature.CAN_OVERRIDE_ACCESS_MODIFIERS, true);\n     ByteArrayOutputStream baos = new ByteArrayOutputStream();\n-    byte[] serializedBytes;\n+    byte[] serializedBytes = new byte[0];\n+    boolean isCompressed = false;\n+\n     try {\n       mapper.writeValue(baos, data);\n       serializedBytes = baos.toByteArray();\n       // apply compression if needed\n-      if (record.getBooleanField(\"enableCompression\", false) || serializedBytes.length > ZNRecord.SIZE_LIMIT) {\n+      if (ZNRecordUtil.shouldCompress(record, serializedBytes.length)) {\n         serializedBytes = GZipCompressionUtil.compress(serializedBytes);\n+        isCompressed = true;\n       }\n     } catch (Exception e) {\n-      logger.error(\"Exception during data serialization. Will not write to zk. Data (first 1k): \"\n-          + new String(baos.toByteArray()).substring(0, 1024), e);\n+      if (serializedBytes.length == 0 || GZipCompressionUtil.isCompressed(serializedBytes)) {\n+        serializedBytes = baos.toByteArray();\n+      }\n+      int firstBytesLength = Math.min(serializedBytes.length, 1024);\n+      // TODO: remove logging first N bytes of data to reduce log size.\n+      LOG.error(\"Exception during data serialization. Will not write to zk.\"\n+              + \" The first {} bytes of data: {}\", firstBytesLength,\n+          new String(serializedBytes, 0, firstBytesLength), e);\n       throw new ZkClientException(e);\n     }\n-    if (serializedBytes.length > ZNRecord.SIZE_LIMIT) {\n-      logger.error(\"Data size larger than 1M, ZNRecord.id: \" + record.getId()\n-          + \". Will not write to zk. Data (first 1k): \"\n-          + new String(serializedBytes).substring(0, 1024));\n-      throw new ZkClientException(\"Data size larger than 1M, ZNRecord.id: \" + record.getId());\n+\n+    int threshold = ZNRecordUtil.getSerializerThreshold();\n+    if (serializedBytes.length > threshold) {\n+      int length = serializedBytes.length;", "originalCommit": "f72feac2e51b3bd38503d3841374be13dfb8c348", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTU1NDgzNQ==", "url": "https://github.com/apache/helix/pull/809#discussion_r385554835", "bodyText": "// serializedBytes may be compressed and its length is different from byte array.\n// To log the compressed data size, use this length as the compressed data length.", "author": "huizhilu", "createdAt": "2020-02-28T08:03:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTU0NDcyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTU0NTQwNw==", "url": "https://github.com/apache/helix/pull/809#discussion_r385545407", "bodyText": "Nit: Just plain THRESHOLD is a little vague. Could we say\nWRITE_THRESHOLD or WRITE_SIZE_LIMIT?", "author": "narendly", "createdAt": "2020-02-28T07:34:34Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/constant/ZkSystemPropertyKeys.java", "diffHunk": "@@ -0,0 +1,39 @@\n+package org.apache.helix.zookeeper.constant;\n+\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+/**\n+ * This class contains various ZK system property keys.\n+ */\n+public class ZkSystemPropertyKeys {\n+\n+  /**\n+   * This is property that defines the threshold in bytes for ZKRecord's two serializers before\n+   * serialized data is ready to be written to ZK. This property applies to\n+   * 1. {@link org.apache.helix.zookeeper.datamodel.serializer.ZNRecordSerializer}\n+   * 2. {@link org.apache.helix.zookeeper.datamodel.serializer.ZNRecordStreamingSerializer}.\n+   * <p>\n+   * If the size of serialized data exceeds this configured threshold, the data will NOT be written\n+   * to Zookeeper. Default value is 1024000 (1 MB). If the configured threshold is greater than\n+   * 1 MB or less than or equal to 0 byte, 1 MB will be used.\n+   */\n+  public static final String ZNRECORD_SERIALIZER_THRESHOLD_BYTES =", "originalCommit": "f72feac2e51b3bd38503d3841374be13dfb8c348", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTU1MjU4MQ==", "url": "https://github.com/apache/helix/pull/809#discussion_r385552581", "bodyText": "I was thinking of write.threshold.bytes but I didn't think the serializer is kind of an IO writer to ZK. So I didn't take it. And I just thought of a name might fit better: output.limit.bytes which indicates the behavior and it role. What do you think?", "author": "huizhilu", "createdAt": "2020-02-28T07:56:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTU0NTQwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTgyOTc1Mw==", "url": "https://github.com/apache/helix/pull/809#discussion_r385829753", "bodyText": "As a user, I wouldn't care whether it's output or what - that's an internal implementation detail. I still vote we use 'write.threshold' or 'write.size.limit', so users know what this config is used for.", "author": "narendly", "createdAt": "2020-02-28T17:37:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTU0NTQwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTU0NTY0NQ==", "url": "https://github.com/apache/helix/pull/809#discussion_r385545645", "bodyText": "Nit: just an issue with naming. Can we call this getSerializerWriteSizeLimit()? or WriteSizeThreshold?", "author": "narendly", "createdAt": "2020-02-28T07:35:28Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/datamodel/serializer/ZNRecordStreamingSerializer.java", "diffHunk": "@@ -154,20 +157,34 @@ private static int getListFieldBound(ZNRecord record) {\n       g.close();\n       serializedBytes = baos.toByteArray();\n       // apply compression if needed\n-      if (record.getBooleanField(\"enableCompression\", false) || serializedBytes.length > ZNRecord.SIZE_LIMIT) {\n+      if (ZNRecordUtil.shouldCompress(record, serializedBytes.length)) {\n         serializedBytes = GZipCompressionUtil.compress(serializedBytes);\n+        isCompressed = true;\n       }\n     } catch (Exception e) {\n-      LOG.error(\"Exception during data serialization. Will not write to zk. Data (first 1k): \"\n-          + new String(baos.toByteArray()).substring(0, 1024), e);\n+      if (serializedBytes.length == 0 || GZipCompressionUtil.isCompressed(serializedBytes)) {\n+        serializedBytes = baos.toByteArray();\n+      }\n+      int firstBytesLength = Math.min(serializedBytes.length, 1024);\n+      // TODO: remove logging first N bytes of data to reduce log size.\n+      LOG.error(\"Exception during data serialization. Will not write to zk.\"\n+              + \" The first {} bytes of data: {}\", firstBytesLength,\n+          new String(serializedBytes, 0, firstBytesLength), e);\n       throw new ZkClientException(e);\n     }\n     // check size\n-    if (serializedBytes.length > ZNRecord.SIZE_LIMIT) {\n-      LOG.error(\"Data size larger than 1M, ZNRecord.id: \" + record.getId()\n-          + \". Will not write to zk. Data (first 1k): \"\n-          + new String(serializedBytes).substring(0, 1024));\n-      throw new ZkClientException(\"Data size larger than 1M, ZNRecord.id: \" + record.getId());\n+    int threshold = ZNRecordUtil.getSerializerThreshold();", "originalCommit": "f72feac2e51b3bd38503d3841374be13dfb8c348", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTU0NjA1MQ==", "url": "https://github.com/apache/helix/pull/809#discussion_r385546051", "bodyText": "What threshold is it? There could be thresholds for a whole bunch of things.\nI vote to use getSerializerWriteSizeLimit() or getSerializerWriteSizeThreshold(). This way, it's not ambiguous and clear about what this value stands for.\nAlso should we use long instead of int here? Is overflow possible?", "author": "narendly", "createdAt": "2020-02-28T07:36:45Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/util/ZNRecordUtil.java", "diffHunk": "@@ -0,0 +1,61 @@\n+package org.apache.helix.zookeeper.util;\n+\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+import org.apache.helix.zookeeper.constant.ZkSystemPropertyKeys;\n+import org.apache.helix.zookeeper.datamodel.ZNRecord;\n+\n+\n+/**\n+ * This utility class contains various methods for manipulating ZNRecord.\n+ */\n+public class ZNRecordUtil {\n+\n+  /**\n+   * Checks whether or not a serialized ZNRecord bytes should be compressed before being written to\n+   * Zookeeper.\n+   *\n+   * @param record raw ZNRecord before being serialized\n+   * @param serializedLength length of the serialized bytes array\n+   * @return\n+   */\n+  public static boolean shouldCompress(ZNRecord record, int serializedLength) {\n+    if (record.getBooleanField(ZNRecord.ENABLE_COMPRESSION_BOOLEAN_FIELD, false)) {\n+      return true;\n+    }\n+\n+    return serializedLength > ZNRecord.SIZE_LIMIT;\n+  }\n+\n+  /**\n+   * Returns compression threshold in bytes. The threshold is a smaller number determined by the\n+   * configured threshold and {@link ZNRecord#SIZE_LIMIT}.\n+   */\n+  public static int getSerializerThreshold() {", "originalCommit": "f72feac2e51b3bd38503d3841374be13dfb8c348", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTU1NjQ3NA==", "url": "https://github.com/apache/helix/pull/809#discussion_r385556474", "bodyText": "Used output.limit\nreplied above. int is good enough.", "author": "huizhilu", "createdAt": "2020-02-28T08:08:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTU0NjA1MQ=="}], "type": "inlineReview"}, {"oid": "21d443ed9a89630b29006ac581446e212d28e6c9", "url": "https://github.com/apache/helix/commit/21d443ed9a89630b29006ac581446e212d28e6c9", "message": "Rename property to znrecord.serializer.output.limit.bytes", "committedDate": "2020-02-28T08:09:19Z", "type": "commit"}, {"oid": "8378d7a36ab6158274e228089b0c483a6f2c6436", "url": "https://github.com/apache/helix/commit/8378d7a36ab6158274e228089b0c483a6f2c6436", "message": "Add test.", "committedDate": "2020-02-28T23:51:38Z", "type": "commit"}, {"oid": "a7a0b2ac56d4ae4a4a935ce433860076f6cc2158", "url": "https://github.com/apache/helix/commit/a7a0b2ac56d4ae4a4a935ce433860076f6cc2158", "message": "Remove first 1kb data logging", "committedDate": "2020-02-29T00:24:11Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTk3OTY4MQ==", "url": "https://github.com/apache/helix/pull/809#discussion_r385979681", "bodyText": "This is what we are going to decide. We should reduce this default value.", "author": "huizhilu", "createdAt": "2020-02-29T00:26:22Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/datamodel/ZNRecord.java", "diffHunk": "@@ -49,6 +50,10 @@\n   @JsonIgnore(true)\n   public static final String LIST_FIELD_BOUND = \"listField.bound\";\n \n+  /** A field name in ZNRecord's boolean fields to enable compression in ZNRecord serializers. */\n+  @JsonIgnore\n+  public static final String ENABLE_COMPRESSION_BOOLEAN_FIELD = \"enableCompression\";\n+\n   @JsonIgnore(true)\n   public static final int SIZE_LIMIT = 1000 * 1024; // leave a margin out of 1M", "originalCommit": "a7a0b2ac56d4ae4a4a935ce433860076f6cc2158", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTk3NDA1Ng==", "url": "https://github.com/apache/helix/pull/809#discussion_r385974056", "bodyText": "nit, regardless of the java field name, should the content be zk.serializer.znrecord.write.size.limit.bytes?", "author": "jiajunwang", "createdAt": "2020-02-28T23:56:48Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/constant/ZkSystemPropertyKeys.java", "diffHunk": "@@ -0,0 +1,39 @@\n+package org.apache.helix.zookeeper.constant;\n+\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+/**\n+ * This class contains various ZK system property keys.\n+ */\n+public class ZkSystemPropertyKeys {\n+\n+  /**\n+   * This is property that defines the maximum write size in bytes for ZKRecord's two serializers\n+   * before serialized data is ready to be written to ZK. This property applies to\n+   * 1. {@link org.apache.helix.zookeeper.datamodel.serializer.ZNRecordSerializer}\n+   * 2. {@link org.apache.helix.zookeeper.datamodel.serializer.ZNRecordStreamingSerializer}.\n+   * <p>\n+   * If the size of serialized data exceeds this configured limit, the data will NOT be written\n+   * to Zookeeper. Default value is 1 MB. If the configured limit is greater than\n+   * 1 MB or less than or equal to 0 byte, 1 MB will be used.\n+   */\n+  public static final String ZNRECORD_SERIALIZER_WRITE_SIZE_LIMIT_BYTES =\n+      \"znrecord.serializer.write.size.limit.bytes\";", "originalCommit": "8378d7a36ab6158274e228089b0c483a6f2c6436", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTk3NDQ3Ng==", "url": "https://github.com/apache/helix/pull/809#discussion_r385974476", "bodyText": "nit, The field name....\nAnd what does \"boolean fields\" mean? Should it be simple fields?", "author": "jiajunwang", "createdAt": "2020-02-28T23:58:44Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/datamodel/ZNRecord.java", "diffHunk": "@@ -49,6 +50,10 @@\n   @JsonIgnore(true)\n   public static final String LIST_FIELD_BOUND = \"listField.bound\";\n \n+  /** A field name in ZNRecord's boolean fields to enable compression in ZNRecord serializers. */", "originalCommit": "8378d7a36ab6158274e228089b0c483a6f2c6436", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTk5MjE2NA==", "url": "https://github.com/apache/helix/pull/809#discussion_r385992164", "bodyText": "Changed", "author": "huizhilu", "createdAt": "2020-02-29T02:15:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTk3NDQ3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTk3NDg0OQ==", "url": "https://github.com/apache/helix/pull/809#discussion_r385974849", "bodyText": "Can we follow the convention that has an enum for the properties?", "author": "jiajunwang", "createdAt": "2020-02-29T00:00:35Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/datamodel/ZNRecord.java", "diffHunk": "@@ -49,6 +50,10 @@\n   @JsonIgnore(true)\n   public static final String LIST_FIELD_BOUND = \"listField.bound\";\n \n+  /** A field name in ZNRecord's boolean fields to enable compression in ZNRecord serializers. */\n+  @JsonIgnore\n+  public static final String ENABLE_COMPRESSION_BOOLEAN_FIELD = \"enableCompression\";", "originalCommit": "8378d7a36ab6158274e228089b0c483a6f2c6436", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTk5MjMwNQ==", "url": "https://github.com/apache/helix/pull/809#discussion_r385992305", "bodyText": "I prefer to do it later as there are other fields in ZNRecord: LIST_FIELD_BOUND.", "author": "huizhilu", "createdAt": "2020-02-29T02:16:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTk3NDg0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTk3NjQzMg==", "url": "https://github.com/apache/helix/pull/809#discussion_r385976432", "bodyText": "This is a potential bug, \"serializedBytes = baos.toByteArray();\" is called in the try block. As we discussed, either split the try block, or just don't try to read the content in catch block.", "author": "jiajunwang", "createdAt": "2020-02-29T00:08:37Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/datamodel/serializer/ZNRecordSerializer.java", "diffHunk": "@@ -81,25 +82,48 @@ private static int getListFieldBound(ZNRecord record) {\n     serializationConfig.set(SerializationConfig.Feature.AUTO_DETECT_FIELDS, true);\n     serializationConfig.set(SerializationConfig.Feature.CAN_OVERRIDE_ACCESS_MODIFIERS, true);\n     ByteArrayOutputStream baos = new ByteArrayOutputStream();\n-    byte[] serializedBytes;\n+    byte[] serializedBytes = new byte[0];\n+    boolean isCompressed = false;\n+\n     try {\n       mapper.writeValue(baos, data);\n       serializedBytes = baos.toByteArray();\n       // apply compression if needed\n-      if (record.getBooleanField(\"enableCompression\", false) || serializedBytes.length > ZNRecord.SIZE_LIMIT) {\n+      if (ZNRecordUtil.shouldCompress(record, serializedBytes.length)) {\n         serializedBytes = GZipCompressionUtil.compress(serializedBytes);\n+        isCompressed = true;\n       }\n     } catch (Exception e) {\n-      logger.error(\"Exception during data serialization. Will not write to zk. Data (first 1k): \"\n-          + new String(baos.toByteArray()).substring(0, 1024), e);\n+      if (serializedBytes.length == 0 || GZipCompressionUtil.isCompressed(serializedBytes)) {\n+        serializedBytes = baos.toByteArray();", "originalCommit": "8378d7a36ab6158274e228089b0c483a6f2c6436", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTk5MzAwMg==", "url": "https://github.com/apache/helix/pull/809#discussion_r385993002", "bodyText": "Removed the logs.", "author": "huizhilu", "createdAt": "2020-02-29T02:26:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTk3NjQzMg=="}], "type": "inlineReview"}, {"oid": "2933d7d2f53f88674f71b72048b7397452484a4f", "url": "https://github.com/apache/helix/commit/2933d7d2f53f88674f71b72048b7397452484a4f", "message": "Add auto compress enabled config.", "committedDate": "2020-02-29T02:17:02Z", "type": "commit"}, {"oid": "2933d7d2f53f88674f71b72048b7397452484a4f", "url": "https://github.com/apache/helix/commit/2933d7d2f53f88674f71b72048b7397452484a4f", "message": "Add auto compress enabled config.", "committedDate": "2020-02-29T02:17:02Z", "type": "forcePushed"}, {"oid": "8dea00710f12c43f130f047a41800c1e95c3b6e3", "url": "https://github.com/apache/helix/commit/8dea00710f12c43f130f047a41800c1e95c3b6e3", "message": "Format style", "committedDate": "2020-02-29T04:33:08Z", "type": "commit"}, {"oid": "8dea00710f12c43f130f047a41800c1e95c3b6e3", "url": "https://github.com/apache/helix/commit/8dea00710f12c43f130f047a41800c1e95c3b6e3", "message": "Format style", "committedDate": "2020-02-29T04:33:08Z", "type": "forcePushed"}, {"oid": "4a307ff73534fec27f13ef5accede9d2b068707f", "url": "https://github.com/apache/helix/commit/4a307ff73534fec27f13ef5accede9d2b068707f", "message": "Include tests in zookeeper-api module", "committedDate": "2020-02-29T04:37:09Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwNjUzOQ==", "url": "https://github.com/apache/helix/pull/809#discussion_r386006539", "bodyText": "Nit: clarify that this \"write size limit\" refers to \"zk.serializer.znrecord.write.size.limit.bytes\"?", "author": "narendly", "createdAt": "2020-02-29T06:32:12Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/constant/ZkSystemPropertyKeys.java", "diffHunk": "@@ -0,0 +1,48 @@\n+package org.apache.helix.zookeeper.constant;\n+\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+/**\n+ * This class contains various ZK system property keys.\n+ */\n+public class ZkSystemPropertyKeys {\n+\n+  /**\n+   * Setting this property to true in system properties enables auto compression in ZK serializer.\n+   * The data will be automatically compressed by\n+   * {@link org.apache.helix.zookeeper.util.GZipCompressionUtil} when being written to Zookeeper\n+   * if size of serialized data exceeds the write size limit. The default value is enabled.", "originalCommit": "4a307ff73534fec27f13ef5accede9d2b068707f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwNjgwOQ==", "url": "https://github.com/apache/helix/pull/809#discussion_r386006809", "bodyText": "It doesn\u2019t have to be this size limit config because it has 1 MB as default. This is why I did not explicitly refer to that config.", "author": "huizhilu", "createdAt": "2020-02-29T06:37:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwNjUzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwNzM2Mw==", "url": "https://github.com/apache/helix/pull/809#discussion_r386007363", "bodyText": "What if the user sets their zookeeper's jute maxbuffer to some value greater than 1MB?", "author": "narendly", "createdAt": "2020-02-29T06:46:53Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/constant/ZkSystemPropertyKeys.java", "diffHunk": "@@ -0,0 +1,48 @@\n+package org.apache.helix.zookeeper.constant;\n+\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+/**\n+ * This class contains various ZK system property keys.\n+ */\n+public class ZkSystemPropertyKeys {\n+\n+  /**\n+   * Setting this property to true in system properties enables auto compression in ZK serializer.\n+   * The data will be automatically compressed by\n+   * {@link org.apache.helix.zookeeper.util.GZipCompressionUtil} when being written to Zookeeper\n+   * if size of serialized data exceeds the write size limit. The default value is enabled.\n+   */\n+  public static final String ZK_SERIALIZER_ZNRECORD_AUTO_COMPRESS_ENABLED =\n+      \"zk.serializer.znrecord.auto-compress.enabled\";\n+\n+  /**\n+   * This is property that defines the maximum write size in bytes for ZKRecord's two serializers\n+   * before serialized data is ready to be written to ZK. This property applies to\n+   * 1. {@link org.apache.helix.zookeeper.datamodel.serializer.ZNRecordSerializer}\n+   * 2. {@link org.apache.helix.zookeeper.datamodel.serializer.ZNRecordStreamingSerializer}.\n+   * <p>\n+   * If the size of serialized data exceeds this configured limit, the data will NOT be written\n+   * to Zookeeper. Default value is 1 MB. If the configured limit is greater than\n+   * 1 MB or less than or equal to 0 byte, 1 MB will be used.", "originalCommit": "4a307ff73534fec27f13ef5accede9d2b068707f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwNzM4Ng==", "url": "https://github.com/apache/helix/pull/809#discussion_r386007386", "bodyText": "Lets make it clear that this will apply no matter whether the data has been compressed or not", "author": "narendly", "createdAt": "2020-02-29T06:47:30Z", "path": "zookeeper-api/src/main/java/org/apache/helix/zookeeper/constant/ZkSystemPropertyKeys.java", "diffHunk": "@@ -0,0 +1,48 @@\n+package org.apache.helix.zookeeper.constant;\n+\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+/**\n+ * This class contains various ZK system property keys.\n+ */\n+public class ZkSystemPropertyKeys {\n+\n+  /**\n+   * Setting this property to true in system properties enables auto compression in ZK serializer.\n+   * The data will be automatically compressed by\n+   * {@link org.apache.helix.zookeeper.util.GZipCompressionUtil} when being written to Zookeeper\n+   * if size of serialized data exceeds the write size limit. The default value is enabled.\n+   */\n+  public static final String ZK_SERIALIZER_ZNRECORD_AUTO_COMPRESS_ENABLED =\n+      \"zk.serializer.znrecord.auto-compress.enabled\";\n+\n+  /**\n+   * This is property that defines the maximum write size in bytes for ZKRecord's two serializers\n+   * before serialized data is ready to be written to ZK. This property applies to\n+   * 1. {@link org.apache.helix.zookeeper.datamodel.serializer.ZNRecordSerializer}\n+   * 2. {@link org.apache.helix.zookeeper.datamodel.serializer.ZNRecordStreamingSerializer}.\n+   * <p>\n+   * If the size of serialized data exceeds this configured limit, the data will NOT be written", "originalCommit": "4a307ff73534fec27f13ef5accede9d2b068707f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwNzk0Mw==", "url": "https://github.com/apache/helix/pull/809#discussion_r386007943", "bodyText": "Resolved.", "author": "huizhilu", "createdAt": "2020-02-29T06:59:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjAwNzM4Ng=="}], "type": "inlineReview"}, {"oid": "efc43a2ee57d7fe52884dbca58e759768f087f02", "url": "https://github.com/apache/helix/commit/efc43a2ee57d7fe52884dbca58e759768f087f02", "message": "Comments.", "committedDate": "2020-02-29T07:03:31Z", "type": "commit"}, {"oid": "efc43a2ee57d7fe52884dbca58e759768f087f02", "url": "https://github.com/apache/helix/commit/efc43a2ee57d7fe52884dbca58e759768f087f02", "message": "Comments.", "committedDate": "2020-02-29T07:03:31Z", "type": "forcePushed"}, {"oid": "4b4f1742b5f99564fc6aa9fb12d43e9ebab18fb7", "url": "https://github.com/apache/helix/commit/4b4f1742b5f99564fc6aa9fb12d43e9ebab18fb7", "message": "Format lines.", "committedDate": "2020-02-29T07:08:15Z", "type": "commit"}]}