{"pr_number": 1076, "pr_title": "Recover Workflow Garbage Collection Logic", "pr_createdAt": "2020-06-09T05:06:35Z", "pr_url": "https://github.com/apache/helix/pull/1076", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MjgzMA==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437152830", "bodyText": "Why are we starting with a space?", "author": "narendly", "createdAt": "2020-06-09T05:51:50Z", "path": "helix-core/src/main/java/org/apache/helix/controller/stages/TaskGarbageCollectionStage.java", "diffHunk": "@@ -23,34 +27,53 @@ public AsyncWorkerType getAsyncWorkerType() {\n   }\n \n   @Override\n-  public void execute(ClusterEvent event) {\n+  public void process(ClusterEvent event) throws Exception {\n     WorkflowControllerDataProvider dataProvider =\n         event.getAttribute(AttributeName.ControllerDataProvider.name());\n+    event.addAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name(),\n+        dataProvider.getWorkflowConfigMap());\n+    event.addAttribute(AttributeName.RESOURCE_CONTEXT_MAP.name(), dataProvider.getContexts());\n+\n+    super.process(event);\n+  }\n+\n+  @Override\n+  public void execute(ClusterEvent event) {\n+    Map<String, WorkflowConfig> workflowConfigMap =\n+        event.getAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name());\n+    Map<String, ZNRecord> resourceContextMap =\n+        event.getAttribute(AttributeName.RESOURCE_CONTEXT_MAP.name());\n     HelixManager manager = event.getAttribute(AttributeName.helixmanager.name());\n \n-    if (dataProvider == null || manager == null) {\n+    if (manager == null) {\n       LOG.warn(\n-          \"ResourceControllerDataProvider or HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n+          \" HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",", "originalCommit": "a91b75b9d415ebbc56062ba4c36f17fc160428ca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MzI4OQ==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437153289", "bodyText": "How would this affect the memory footprint of each event?\nAlso, is a soft copy okay here? I guess if soft copy is okay, then it won't be too much overhead because this would just be two more references.", "author": "narendly", "createdAt": "2020-06-09T05:53:12Z", "path": "helix-core/src/main/java/org/apache/helix/controller/stages/TaskGarbageCollectionStage.java", "diffHunk": "@@ -23,34 +27,53 @@ public AsyncWorkerType getAsyncWorkerType() {\n   }\n \n   @Override\n-  public void execute(ClusterEvent event) {\n+  public void process(ClusterEvent event) throws Exception {\n     WorkflowControllerDataProvider dataProvider =\n         event.getAttribute(AttributeName.ControllerDataProvider.name());\n+    event.addAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name(),\n+        dataProvider.getWorkflowConfigMap());\n+    event.addAttribute(AttributeName.RESOURCE_CONTEXT_MAP.name(), dataProvider.getContexts());", "originalCommit": "a91b75b9d415ebbc56062ba4c36f17fc160428ca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY2NDA2Nw==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437664067", "bodyText": "I don't think a reference to the map object itself here is OK. My understanding is that you want to persist in the cached map before it is modified in the later stages. In this case, you need to copy the map, not just pass the reference.\nAlso, if this change passes our current test, then it may indicate the test does not cover the race condition case. To justify that this PR really fixes the race condition, please add the corresponding test case which fails because of the race condition without the fix.", "author": "jiajunwang", "createdAt": "2020-06-09T19:22:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MzI4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzczNzE5Ng==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437737196", "bodyText": "This is a very good point for reviewing. First, my mistake: this should be a deep copy. Now, with the concerns raised here, we have a couple options:\n\nWe still do deep copies. I'm only copying the maps that are necessary for this stage to run, which is the 2 maps for WorkflowConfig and Context.\nPro: Doesn't change existing logic\nCon: Uses more memory. @narendly I'd like to know how concerned are you about the memory usage for them?\nWe do what @jiajunwang suggests; we only pass in the ones that need to be GC'ed.\nPro: Less memory usage.\nCon: More time spent in the main thread. It's actually not a fast thing to do because getExpiredJobs() contains Zk read operations. This could add delay to the main pipeline.\n@dasahcc proposed another approach: the event is like a trigger, and the garbage collection stage figures out what to GC by itself. The stage performs 2 Zk reads to get WorkflowConfigs and WorkflowContexts. Imo 2 Zk reads are not many comparing to the existing Zk reads this stage does (per job level).\n\nI'm more inclined towards the 3rd option. Feel free to let me know what you guys think.\n@jiajunwang About the test comment: I agree with what you said 100% - there should have been a test. The race condition was introduced due to cache refresh, but whatever approach we take, this stage will now be completely unrelated to the cache. Should we still write a test that involves cache refresh()? I don't think it would make sense NOW because this stage is no longer related to it; it would make sense before.", "author": "NealSun96", "createdAt": "2020-06-09T21:37:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MzI4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc3MjE0Nw==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437772147", "bodyText": "I agree with what you replied about the test. Let's address the other comments.", "author": "jiajunwang", "createdAt": "2020-06-09T23:12:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MzI4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc5ODQ3Mw==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437798473", "bodyText": "Closing this thread as deep copy is no longer a concern anymore with the new approach @jiajunwang proposed.", "author": "NealSun96", "createdAt": "2020-06-10T00:44:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MzI4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NDA2Nw==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437154067", "bodyText": "If we assumed soft copy was okay from above, then why are we creating a new set? Also this is not a hard copy either is it?", "author": "narendly", "createdAt": "2020-06-09T05:55:34Z", "path": "helix-core/src/main/java/org/apache/helix/controller/stages/TaskGarbageCollectionStage.java", "diffHunk": "@@ -23,34 +27,53 @@ public AsyncWorkerType getAsyncWorkerType() {\n   }\n \n   @Override\n-  public void execute(ClusterEvent event) {\n+  public void process(ClusterEvent event) throws Exception {\n     WorkflowControllerDataProvider dataProvider =\n         event.getAttribute(AttributeName.ControllerDataProvider.name());\n+    event.addAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name(),\n+        dataProvider.getWorkflowConfigMap());\n+    event.addAttribute(AttributeName.RESOURCE_CONTEXT_MAP.name(), dataProvider.getContexts());\n+\n+    super.process(event);\n+  }\n+\n+  @Override\n+  public void execute(ClusterEvent event) {\n+    Map<String, WorkflowConfig> workflowConfigMap =\n+        event.getAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name());\n+    Map<String, ZNRecord> resourceContextMap =\n+        event.getAttribute(AttributeName.RESOURCE_CONTEXT_MAP.name());\n     HelixManager manager = event.getAttribute(AttributeName.helixmanager.name());\n \n-    if (dataProvider == null || manager == null) {\n+    if (manager == null) {\n       LOG.warn(\n-          \"ResourceControllerDataProvider or HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n+          \" HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n           event.getEventId(), event.getEventType(), event.getClusterName());\n       return;\n     }\n \n-    Set<WorkflowConfig> existingWorkflows =\n-        new HashSet<>(dataProvider.getWorkflowConfigMap().values());\n+    Set<WorkflowConfig> existingWorkflows = new HashSet<>(workflowConfigMap.values());", "originalCommit": "a91b75b9d415ebbc56062ba4c36f17fc160428ca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ5MTY2MA==", "url": "https://github.com/apache/helix/pull/1076#discussion_r442491660", "bodyText": "Outdated", "author": "NealSun96", "createdAt": "2020-06-18T20:44:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NDA2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NDUwNA==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437154504", "bodyText": "Aren't these two checks same checks? Why are we doing the same thing twice?", "author": "narendly", "createdAt": "2020-06-09T05:56:39Z", "path": "helix-core/src/main/java/org/apache/helix/controller/stages/TaskGarbageCollectionStage.java", "diffHunk": "@@ -23,34 +27,53 @@ public AsyncWorkerType getAsyncWorkerType() {\n   }\n \n   @Override\n-  public void execute(ClusterEvent event) {\n+  public void process(ClusterEvent event) throws Exception {\n     WorkflowControllerDataProvider dataProvider =\n         event.getAttribute(AttributeName.ControllerDataProvider.name());\n+    event.addAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name(),\n+        dataProvider.getWorkflowConfigMap());\n+    event.addAttribute(AttributeName.RESOURCE_CONTEXT_MAP.name(), dataProvider.getContexts());\n+\n+    super.process(event);\n+  }\n+\n+  @Override\n+  public void execute(ClusterEvent event) {\n+    Map<String, WorkflowConfig> workflowConfigMap =\n+        event.getAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name());\n+    Map<String, ZNRecord> resourceContextMap =\n+        event.getAttribute(AttributeName.RESOURCE_CONTEXT_MAP.name());\n     HelixManager manager = event.getAttribute(AttributeName.helixmanager.name());\n \n-    if (dataProvider == null || manager == null) {\n+    if (manager == null) {\n       LOG.warn(\n-          \"ResourceControllerDataProvider or HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n+          \" HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n           event.getEventId(), event.getEventType(), event.getClusterName());\n       return;\n     }\n \n-    Set<WorkflowConfig> existingWorkflows =\n-        new HashSet<>(dataProvider.getWorkflowConfigMap().values());\n+    Set<WorkflowConfig> existingWorkflows = new HashSet<>(workflowConfigMap.values());\n     for (WorkflowConfig workflowConfig : existingWorkflows) {\n       // clean up the expired jobs if it is a queue.\n       if (workflowConfig != null && (!workflowConfig.isTerminable() || workflowConfig\n           .isJobQueue())) {\n-        try {\n-          TaskUtil.purgeExpiredJobs(workflowConfig.getWorkflowId(), workflowConfig,\n-              dataProvider.getWorkflowContext(workflowConfig.getWorkflowId()), manager,\n-              _rebalanceScheduler);\n-        } catch (Exception e) {\n-          LOG.warn(String.format(\"Failed to purge job for workflow %s with reason %s\",\n-              workflowConfig.getWorkflowId(), e.toString()));\n+        String workflowId = workflowConfig.getWorkflowId();\n+        if (resourceContextMap.containsKey(workflowId)\n+            && resourceContextMap.get(workflowId) != null) {", "originalCommit": "a91b75b9d415ebbc56062ba4c36f17fc160428ca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NDY0OQ==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437154649", "bodyText": "Use parameterized logging.", "author": "narendly", "createdAt": "2020-06-09T05:57:09Z", "path": "helix-core/src/main/java/org/apache/helix/controller/stages/TaskGarbageCollectionStage.java", "diffHunk": "@@ -23,34 +27,53 @@ public AsyncWorkerType getAsyncWorkerType() {\n   }\n \n   @Override\n-  public void execute(ClusterEvent event) {\n+  public void process(ClusterEvent event) throws Exception {\n     WorkflowControllerDataProvider dataProvider =\n         event.getAttribute(AttributeName.ControllerDataProvider.name());\n+    event.addAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name(),\n+        dataProvider.getWorkflowConfigMap());\n+    event.addAttribute(AttributeName.RESOURCE_CONTEXT_MAP.name(), dataProvider.getContexts());\n+\n+    super.process(event);\n+  }\n+\n+  @Override\n+  public void execute(ClusterEvent event) {\n+    Map<String, WorkflowConfig> workflowConfigMap =\n+        event.getAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name());\n+    Map<String, ZNRecord> resourceContextMap =\n+        event.getAttribute(AttributeName.RESOURCE_CONTEXT_MAP.name());\n     HelixManager manager = event.getAttribute(AttributeName.helixmanager.name());\n \n-    if (dataProvider == null || manager == null) {\n+    if (manager == null) {\n       LOG.warn(\n-          \"ResourceControllerDataProvider or HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n+          \" HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n           event.getEventId(), event.getEventType(), event.getClusterName());\n       return;\n     }\n \n-    Set<WorkflowConfig> existingWorkflows =\n-        new HashSet<>(dataProvider.getWorkflowConfigMap().values());\n+    Set<WorkflowConfig> existingWorkflows = new HashSet<>(workflowConfigMap.values());\n     for (WorkflowConfig workflowConfig : existingWorkflows) {\n       // clean up the expired jobs if it is a queue.\n       if (workflowConfig != null && (!workflowConfig.isTerminable() || workflowConfig\n           .isJobQueue())) {\n-        try {\n-          TaskUtil.purgeExpiredJobs(workflowConfig.getWorkflowId(), workflowConfig,\n-              dataProvider.getWorkflowContext(workflowConfig.getWorkflowId()), manager,\n-              _rebalanceScheduler);\n-        } catch (Exception e) {\n-          LOG.warn(String.format(\"Failed to purge job for workflow %s with reason %s\",\n-              workflowConfig.getWorkflowId(), e.toString()));\n+        String workflowId = workflowConfig.getWorkflowId();\n+        if (resourceContextMap.containsKey(workflowId)\n+            && resourceContextMap.get(workflowId) != null) {\n+          try {\n+            TaskUtil.purgeExpiredJobs(workflowId, workflowConfig,\n+                new WorkflowContext(resourceContextMap.get(workflowId)), manager,\n+                _rebalanceScheduler);\n+          } catch (Exception e) {\n+            LOG.warn(String.format(\"Failed to purge job for workflow %s with reason %s\", workflowId,\n+                e.toString()));", "originalCommit": "a91b75b9d415ebbc56062ba4c36f17fc160428ca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NDY5MQ==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437154691", "bodyText": "Use parameterized logging.", "author": "narendly", "createdAt": "2020-06-09T05:57:20Z", "path": "helix-core/src/main/java/org/apache/helix/controller/stages/TaskGarbageCollectionStage.java", "diffHunk": "@@ -23,34 +27,53 @@ public AsyncWorkerType getAsyncWorkerType() {\n   }\n \n   @Override\n-  public void execute(ClusterEvent event) {\n+  public void process(ClusterEvent event) throws Exception {\n     WorkflowControllerDataProvider dataProvider =\n         event.getAttribute(AttributeName.ControllerDataProvider.name());\n+    event.addAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name(),\n+        dataProvider.getWorkflowConfigMap());\n+    event.addAttribute(AttributeName.RESOURCE_CONTEXT_MAP.name(), dataProvider.getContexts());\n+\n+    super.process(event);\n+  }\n+\n+  @Override\n+  public void execute(ClusterEvent event) {\n+    Map<String, WorkflowConfig> workflowConfigMap =\n+        event.getAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name());\n+    Map<String, ZNRecord> resourceContextMap =\n+        event.getAttribute(AttributeName.RESOURCE_CONTEXT_MAP.name());\n     HelixManager manager = event.getAttribute(AttributeName.helixmanager.name());\n \n-    if (dataProvider == null || manager == null) {\n+    if (manager == null) {\n       LOG.warn(\n-          \"ResourceControllerDataProvider or HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n+          \" HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n           event.getEventId(), event.getEventType(), event.getClusterName());\n       return;\n     }\n \n-    Set<WorkflowConfig> existingWorkflows =\n-        new HashSet<>(dataProvider.getWorkflowConfigMap().values());\n+    Set<WorkflowConfig> existingWorkflows = new HashSet<>(workflowConfigMap.values());\n     for (WorkflowConfig workflowConfig : existingWorkflows) {\n       // clean up the expired jobs if it is a queue.\n       if (workflowConfig != null && (!workflowConfig.isTerminable() || workflowConfig\n           .isJobQueue())) {\n-        try {\n-          TaskUtil.purgeExpiredJobs(workflowConfig.getWorkflowId(), workflowConfig,\n-              dataProvider.getWorkflowContext(workflowConfig.getWorkflowId()), manager,\n-              _rebalanceScheduler);\n-        } catch (Exception e) {\n-          LOG.warn(String.format(\"Failed to purge job for workflow %s with reason %s\",\n-              workflowConfig.getWorkflowId(), e.toString()));\n+        String workflowId = workflowConfig.getWorkflowId();\n+        if (resourceContextMap.containsKey(workflowId)\n+            && resourceContextMap.get(workflowId) != null) {\n+          try {\n+            TaskUtil.purgeExpiredJobs(workflowId, workflowConfig,\n+                new WorkflowContext(resourceContextMap.get(workflowId)), manager,\n+                _rebalanceScheduler);\n+          } catch (Exception e) {\n+            LOG.warn(String.format(\"Failed to purge job for workflow %s with reason %s\", workflowId,\n+                e.toString()));\n+          }\n+        } else {\n+          LOG.warn(String.format(\"Workflow %s context does not exist!\", workflowId));", "originalCommit": "a91b75b9d415ebbc56062ba4c36f17fc160428ca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1NTA5NQ==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437155095", "bodyText": "Redundant line", "author": "narendly", "createdAt": "2020-06-09T05:58:29Z", "path": "helix-core/src/test/java/org/apache/helix/integration/task/TestWorkflowContextWithoutConfig.java", "diffHunk": "@@ -103,6 +103,73 @@ public void testWorkflowContextWithoutConfig() throws Exception {\n     Assert.assertTrue(workflowContextNotCreated);\n   }\n \n+  @Test\n+  public void testWorkflowContextGarbageCollection() throws Exception {\n+    String workflowName = TestHelper.getTestMethodName();\n+    Workflow.Builder builder1 = createSimpleWorkflowBuilder(workflowName);\n+    _driver.start(builder1.build());\n+\n+    // Wait until workflow is created and IN_PROGRESS state.\n+    _driver.pollForWorkflowState(workflowName, TaskState.IN_PROGRESS);\n+\n+    // Check that WorkflowConfig, WorkflowContext, and IdealState are indeed created for this\n+    // workflow\n+    Assert.assertNotNull(_driver.getWorkflowConfig(workflowName));\n+    Assert.assertNotNull(_driver.getWorkflowContext(workflowName));\n+    Assert.assertNotNull(_admin.getResourceIdealState(CLUSTER_NAME, workflowName));\n+\n+    String workflowContextPath =\n+        \"/\" + CLUSTER_NAME + \"/PROPERTYSTORE/TaskRebalancer/\" + workflowName + \"/Context\";\n+\n+    ZNRecord record = _manager.getHelixDataAccessor().getBaseDataAccessor().get(workflowContextPath,\n+        null, AccessOption.PERSISTENT);\n+    Assert.assertNotNull(record);\n+\n+    // Wait until workflow is completed.\n+    _driver.pollForWorkflowState(workflowName, TaskState.COMPLETED);\n+\n+    // Verify that WorkflowConfig, WorkflowContext, and IdealState are removed after workflow got\n+    // expired.\n+    boolean workflowExpired = TestHelper.verify(() -> {\n+      WorkflowContext wCtx = _driver.getWorkflowContext(workflowName);\n+      WorkflowConfig wCfg = _driver.getWorkflowConfig(workflowName);\n+      IdealState idealState = _admin.getResourceIdealState(CLUSTER_NAME, workflowName);\n+      return (wCtx == null && wCfg == null && idealState == null);\n+    }, TestHelper.WAIT_DURATION);\n+    Assert.assertTrue(workflowExpired);\n+\n+", "originalCommit": "a91b75b9d415ebbc56062ba4c36f17fc160428ca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzYzMjA2Nw==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437632067", "bodyText": "Instead of having 2 additional maps that duplicate the fields of ControllerDataProvider, can we have a specific class presenting the to be GCed objects?", "author": "jiajunwang", "createdAt": "2020-06-09T18:24:45Z", "path": "helix-core/src/main/java/org/apache/helix/controller/stages/AttributeName.java", "diffHunk": "@@ -40,5 +40,7 @@\n   PipelineType,\n   LastRebalanceFinishTimeStamp,\n   ControllerDataProvider,\n-  STATEFUL_REBALANCER\n+  STATEFUL_REBALANCER,\n+  WORKFLOW_CONFIG_MAP,", "originalCommit": "a91b75b9d415ebbc56062ba4c36f17fc160428ca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ5MTc0MA==", "url": "https://github.com/apache/helix/pull/1076#discussion_r442491740", "bodyText": "Outdated", "author": "NealSun96", "createdAt": "2020-06-18T20:44:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzYzMjA2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY2MTMyMA==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437661320", "bodyText": "Is this tool related to the TaskGarbageCollectionStage?\nIt would be better to split the PR if they are not strictly related.", "author": "jiajunwang", "createdAt": "2020-06-09T19:17:42Z", "path": "helix-core/src/main/java/org/apache/helix/task/TaskUtil.java", "diffHunk": "@@ -1037,6 +1037,53 @@ public static void purgeExpiredJobs(String workflow, WorkflowConfig workflowConf\n     setNextJobPurgeTime(workflow, currentTime, purgeInterval, rebalanceScheduler, manager);\n   }\n \n+  /**\n+   * The function that loops through the all existing workflow contexts and removes IdealState and\n+   * workflow context of the workflow whose workflow config does not exist.\n+   * @param workflowConfigMap\n+   * @param resourceContextMap\n+   * @param manager\n+   */\n+  public static void workflowGarbageCollection(final Map<String, WorkflowConfig> workflowConfigMap,", "originalCommit": "890aa1fbc90e852e6a2beea5e9ca89cdc7639483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzczODM0Nw==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437738347", "bodyText": "This is the logic I'm recovering with this PR; it's also the logic that @alirezazamani deleted in the linked PR in the description. :)", "author": "NealSun96", "createdAt": "2020-06-09T21:40:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY2MTMyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc0MTI1Mg==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437741252", "bodyText": "You are not answering the question. Sorry but I don't have that context.\nSo is it related to the stage code, please?", "author": "jiajunwang", "createdAt": "2020-06-09T21:47:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY2MTMyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc0OTI3NA==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437749274", "bodyText": "I see the confusion: this function is called on line 75 in the stage, please see above.\nI was trying to give you the context: this function was deleted in #803, and recovering it is the sole purpose of this PR.", "author": "NealSun96", "createdAt": "2020-06-09T22:06:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY2MTMyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc3MDgwMA==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437770800", "bodyText": "Got you.", "author": "jiajunwang", "createdAt": "2020-06-09T23:08:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY2MTMyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY2NTE4Ng==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437665186", "bodyText": "Just curious, why are we adding all the workflow/resource contexts here?\nIMHO, a better design would be after all workflows, jobs being processed, add the rest unprocessed workflows, jobs to the async GC thread. Will that be better?", "author": "jiajunwang", "createdAt": "2020-06-09T19:25:03Z", "path": "helix-core/src/main/java/org/apache/helix/controller/stages/TaskGarbageCollectionStage.java", "diffHunk": "@@ -23,34 +27,53 @@ public AsyncWorkerType getAsyncWorkerType() {\n   }\n \n   @Override\n-  public void execute(ClusterEvent event) {\n+  public void process(ClusterEvent event) throws Exception {\n     WorkflowControllerDataProvider dataProvider =\n         event.getAttribute(AttributeName.ControllerDataProvider.name());\n+    event.addAttribute(AttributeName.WORKFLOW_CONFIG_MAP.name(),\n+        dataProvider.getWorkflowConfigMap());", "originalCommit": "890aa1fbc90e852e6a2beea5e9ca89cdc7639483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzczODg0MQ==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437738841", "bodyText": "I have also explained it in an earlier comment: the logic of this stage is to loop through every workflow and figure out what to purge. We could do what you said before this stage - we figure out what need to be purged, but that adds delay to the main pipeline.", "author": "NealSun96", "createdAt": "2020-06-09T21:41:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY2NTE4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc0MzcxNQ==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437743715", "bodyText": "So which way do you think would be better? And what is the additional latency we are talking about here?\nSo what I proposed is that we finish the main rebalance calculation first. During that computing, we know what workflow/job is no longer alive. Then we record them in the TaskCache. Then at the end of the pipeline, pass that list to the GC stage which will remove thing async. I think it would be helpful because,\n\nno additional calculation to check if an object needs to be purged, since the rebalance pipeline will have to check for this anyway.\nno parallel logic to determine if a job or workflow needs to be purged separately. So less conflict, and fewer race conditions.\n\nThere might be TF logic prevents us from doing that, but please investigate and we can talk if there are any blocks.", "author": "jiajunwang", "createdAt": "2020-06-09T21:53:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY2NTE4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc2NjE2Ng==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437766166", "bodyText": "I understand your proposal better now, and after looking at it carefully, we can ditch the Zk reads and preprocess lightweight logic (or keep the Zk reads in async thread, which is also okay). In the preprocess section of the stage, we can loop through all WorkflowContext and figure out 1. what workflows need to be garbage collected, and 2. what jobs need to be purged (we can do step 2 in async thread if we use Zk reads instead of cache). All work can be done based on cache.\nFor your point 1 you did mention that we can do the calculation in \"rebalance pipeline\" (I assume is the synonym for TaskSchedulingStage\"). I prefer doing the calculation in GarbageCollectionStage for code clarity, and the overhead isn't much because it's 1 loop through workflows using in-memory operations. What do you think?", "author": "NealSun96", "createdAt": "2020-06-09T22:54:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY2NTE4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc3MDQ2Nw==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437770467", "bodyText": "I prefer doing the calculation in GarbageCollectionStage for code clarity, and the overhead isn't much because it's 1 loop through workflows using in-memory operations. What do you think?\n\nAs long as there is no duplicate logic, I'm fine. The concern is that in TaskSchedulingStage, we may have the logic to loop all the tasks for the non-removable ones, then in the GarbageCollectionStage, we loop all the tasks for the removable ones. That would be concerning because of the duplicate logic and potential conflicts.", "author": "jiajunwang", "createdAt": "2020-06-09T23:07:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY2NTE4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc3MzMwOQ==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437773309", "bodyText": "On my second thought, we should do the check in the async GC thread. The reason is that we may have a job with the same name re-created. So it is possible that when TaskSchedulingStage is running, JOBA should be GCed, but when it is the time to GC, JOBA has already been recreated.\nTo prevent problems in this kind of scenario, I think we should do the check inside GC thread. And also delete the jobs inside the GC thread only after the check.", "author": "jiajunwang", "createdAt": "2020-06-09T23:15:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY2NTE4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc4NTYwMw==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437785603", "bodyText": "So you're right that looping is done twice, but there is no exact duplication for sure. My point is that TaskSchedulingStage is doing a lot of things, to a point where I think it's better for the code base if we allow the other stage to do work just to improve clarity. Is the delay of running a loop a matter of concern? If so we should do it in the TaskSchedulingStage loop.\nAbout your second thought: 1. even if we do the check in async thread, same job name is still an issue because the operation is not atomic. It could be recreated just before deletion; 2. if this is a concern for jobs, this should also be a concern for workflow? If we need to do the check for workflow in async thread, we face the same problem again.", "author": "NealSun96", "createdAt": "2020-06-09T23:56:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY2NTE4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc4OTc4OA==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437789788", "bodyText": "About your second thought: 1. even if we do the check in async thread, same job name is still an issue because the operation is not atomic. It could be recreated just before deletion; 2. if this is a concern for jobs, this should also be a concern for workflow? If we need to do the check for workflow in async thread, we face the same problem again.\n\nProbably easier to check and avoid the race conditions when the logics are in the same block. But I do think this is an issue that we want to solve when you claim to address race condition. Are you implying we do this in the future?", "author": "jiajunwang", "createdAt": "2020-06-10T00:11:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY2NTE4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc5ODI1MQ==", "url": "https://github.com/apache/helix/pull/1076#discussion_r437798251", "bodyText": "@jiajunwang Thank you for the offline discussion!\nConclusion: 1. we will do preprocessing in GarbageCollectionStage. We will sacrifice a bit of performance for better code clarity. All garbage collection logic is better to be in 1 class.\n2. There is no good way to deal with the situation described in this thread (same job name). We are not expanding the scope of this PR to cover that, and therefore it will be left as a TODO item for now.", "author": "NealSun96", "createdAt": "2020-06-10T00:43:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY2NTE4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU2MDA0NQ==", "url": "https://github.com/apache/helix/pull/1076#discussion_r442560045", "bodyText": "We can name it consistently.", "author": "junkaixue", "createdAt": "2020-06-18T23:50:50Z", "path": "helix-core/src/main/java/org/apache/helix/controller/stages/AttributeName.java", "diffHunk": "@@ -40,5 +40,9 @@\n   PipelineType,\n   LastRebalanceFinishTimeStamp,\n   ControllerDataProvider,\n-  STATEFUL_REBALANCER\n+  STATEFUL_REBALANCER,\n+  // This attribute should only be used in TaskGarbageCollectionStage, misuse could cause race conditions.\n+  WORKFLOWS_TO_BE_DELETED,\n+  // This attribute should only be used in TaskGarbageCollectionStage, misuse could cause race conditions.\n+  EXPIRED_JOBS_MAP", "originalCommit": "c089b286ffab0a320c7d997b88cdd77614c5aa65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2OTM0MQ==", "url": "https://github.com/apache/helix/pull/1076#discussion_r443069341", "bodyText": "Could you elaborate please?", "author": "NealSun96", "createdAt": "2020-06-19T22:45:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU2MDA0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUyMzY5Ng==", "url": "https://github.com/apache/helix/pull/1076#discussion_r444523696", "bodyText": "You can make it:\nWORKFLOW_TO_BE_DELETED with JOB_TO_BE_DELETED. Or EXPIRED_WORKFLOWS_MAP with EXPIRED_JOBS_MAP. Logically, they belong to same group.", "author": "junkaixue", "createdAt": "2020-06-23T21:38:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU2MDA0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQzMzQzNA==", "url": "https://github.com/apache/helix/pull/1076#discussion_r446433434", "bodyText": "Ok, renaming.", "author": "NealSun96", "createdAt": "2020-06-26T22:14:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU2MDA0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAyMTExNA==", "url": "https://github.com/apache/helix/pull/1076#discussion_r443021114", "bodyText": "If you do garbage collection. This triggering is not necessary.", "author": "junkaixue", "createdAt": "2020-06-19T19:59:24Z", "path": "helix-core/src/main/java/org/apache/helix/controller/stages/TaskGarbageCollectionStage.java", "diffHunk": "@@ -23,34 +29,89 @@ public AsyncWorkerType getAsyncWorkerType() {\n   }\n \n   @Override\n-  public void execute(ClusterEvent event) {\n-    WorkflowControllerDataProvider dataProvider =\n-        event.getAttribute(AttributeName.ControllerDataProvider.name());\n+  public void process(ClusterEvent event) throws Exception {\n+    // Use main thread to compute what jobs need to be purged, and what workflows need to be gc'ed.\n+    // This is to avoid race conditions since the cache will be modified. After this work, then the\n+    // async work will happen.\n     HelixManager manager = event.getAttribute(AttributeName.helixmanager.name());\n-\n-    if (dataProvider == null || manager == null) {\n+    if (manager == null) {\n       LOG.warn(\n-          \"ResourceControllerDataProvider or HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n+          \"HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n           event.getEventId(), event.getEventType(), event.getClusterName());\n       return;\n     }\n \n-    Set<WorkflowConfig> existingWorkflows =\n-        new HashSet<>(dataProvider.getWorkflowConfigMap().values());\n-    for (WorkflowConfig workflowConfig : existingWorkflows) {\n-      // clean up the expired jobs if it is a queue.\n+    Map<String, Set<String>> expiredJobsMap = new HashMap<>();\n+    Set<String> workflowsToBeDeleted = new HashSet<>();\n+    WorkflowControllerDataProvider dataProvider =\n+        event.getAttribute(AttributeName.ControllerDataProvider.name());\n+    for (Map.Entry<String, ZNRecord> entry : dataProvider.getContexts().entrySet()) {\n+      WorkflowConfig workflowConfig = dataProvider.getWorkflowConfig(entry.getKey());\n       if (workflowConfig != null && (!workflowConfig.isTerminable() || workflowConfig\n           .isJobQueue())) {\n-        try {\n-          TaskUtil.purgeExpiredJobs(workflowConfig.getWorkflowId(), workflowConfig,\n-              dataProvider.getWorkflowContext(workflowConfig.getWorkflowId()), manager,\n-              _rebalanceScheduler);\n-        } catch (Exception e) {\n-          LOG.warn(String.format(\"Failed to purge job for workflow %s with reason %s\",\n-              workflowConfig.getWorkflowId(), e.toString()));\n+        WorkflowContext workflowContext = dataProvider.getWorkflowContext(entry.getKey());\n+        long purgeInterval = workflowConfig.getJobPurgeInterval();\n+        long currentTime = System.currentTimeMillis();\n+        if (purgeInterval > 0\n+            && workflowContext.getLastJobPurgeTime() + purgeInterval <= currentTime) {\n+          // Find jobs that are ready to be purged\n+          Set<String> expiredJobs =\n+              TaskUtil.getExpiredJobsFromCache(dataProvider, workflowConfig, workflowContext);\n+          if (!expiredJobs.isEmpty()) {\n+            expiredJobsMap.put(workflowConfig.getWorkflowId(), expiredJobs);\n+          }\n+          scheduleNextJobPurge(workflowConfig.getWorkflowId(), currentTime, purgeInterval,", "originalCommit": "c089b286ffab0a320c7d997b88cdd77614c5aa65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2OTYzOQ==", "url": "https://github.com/apache/helix/pull/1076#discussion_r443069639", "bodyText": "This line is moved from the original purgeExpiredJob and moved here. The reason is that by triggering, we no longer need to pass purgeInterval to the async thread. I will elaborate on this in another comment.\nI think it's necessary, right? The goal of this is to trigger a purge even if there is no event, I believe.", "author": "NealSun96", "createdAt": "2020-06-19T22:47:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAyMTExNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MDA5NQ==", "url": "https://github.com/apache/helix/pull/1076#discussion_r443070095", "bodyText": "This function is moved from here to TaskGarbageCollectionStage's main thread. The reason for the move is that purgeInterval no longer needs to be passed to the async thread. I believe the trade-off is worth it because the frequency isn't critical. FYI: @dasahcc", "author": "NealSun96", "createdAt": "2020-06-19T22:49:40Z", "path": "helix-core/src/main/java/org/apache/helix/task/TaskUtil.java", "diffHunk": "@@ -977,72 +1011,69 @@ public static boolean isJobStarted(String job, WorkflowContext workflowContext)\n   }\n \n   /**\n-   * Clean up all jobs that are COMPLETED and passes its expiry time.\n-   * @param workflowConfig\n-   * @param workflowContext\n+   * Clean up all jobs that are marked as expired.\n    */\n-  public static void purgeExpiredJobs(String workflow, WorkflowConfig workflowConfig,\n-      WorkflowContext workflowContext, HelixManager manager,\n-      RebalanceScheduler rebalanceScheduler) {\n-    if (workflowContext == null) {\n-      LOG.warn(String.format(\"Workflow %s context does not exist!\", workflow));\n-      return;\n+  public static void purgeExpiredJobs(String workflow, Set<String> expiredJobs,\n+      HelixManager manager, RebalanceScheduler rebalanceScheduler) {\n+    Set<String> failedJobRemovals = new HashSet<>();\n+    for (String job : expiredJobs) {\n+      if (!TaskUtil\n+          .removeJob(manager.getHelixDataAccessor(), manager.getHelixPropertyStore(), job)) {\n+        failedJobRemovals.add(job);\n+        LOG.warn(\"Failed to clean up expired and completed jobs from workflow \" + workflow);\n+      }\n+      rebalanceScheduler.removeScheduledRebalance(job);\n     }\n-    long purgeInterval = workflowConfig.getJobPurgeInterval();\n-    long currentTime = System.currentTimeMillis();\n-    final Set<String> expiredJobs = Sets.newHashSet();\n-    if (purgeInterval > 0 && workflowContext.getLastJobPurgeTime() + purgeInterval <= currentTime) {\n-      expiredJobs.addAll(TaskUtil.getExpiredJobs(manager.getHelixDataAccessor(),\n-          manager.getHelixPropertyStore(), workflowConfig, workflowContext));\n-      if (expiredJobs.isEmpty()) {\n-        LOG.info(\"No job to purge for the queue \" + workflow);\n-      } else {\n-        LOG.info(\"Purge jobs \" + expiredJobs + \" from queue \" + workflow);\n-        Set<String> failedJobRemovals = new HashSet<>();\n-        for (String job : expiredJobs) {\n-          if (!TaskUtil.removeJob(manager.getHelixDataAccessor(), manager.getHelixPropertyStore(),\n-              job)) {\n-            failedJobRemovals.add(job);\n-            LOG.warn(\"Failed to clean up expired and completed jobs from workflow \" + workflow);\n-          }\n-          rebalanceScheduler.removeScheduledRebalance(job);\n-        }\n \n-        // If the job removal failed, make sure we do NOT prematurely delete it from DAG so that the\n-        // removal will be tried again at next purge\n-        expiredJobs.removeAll(failedJobRemovals);\n+    // If the job removal failed, make sure we do NOT prematurely delete it from DAG so that the\n+    // removal will be tried again at next purge\n+    expiredJobs.removeAll(failedJobRemovals);\n \n-        if (!TaskUtil.removeJobsFromDag(manager.getHelixDataAccessor(), workflow, expiredJobs,\n-            true)) {\n-          LOG.warn(\"Error occurred while trying to remove jobs + \" + expiredJobs\n-              + \" from the workflow \" + workflow);\n-        }\n+    if (!TaskUtil.removeJobsFromDag(manager.getHelixDataAccessor(), workflow, expiredJobs, true)) {\n+      LOG.warn(\"Error occurred while trying to remove jobs + \" + expiredJobs + \" from the workflow \"\n+          + workflow);\n+    }\n \n-        if (expiredJobs.size() > 0) {\n-          // Update workflow context will be in main pipeline not here. Otherwise, it will cause\n-          // concurrent write issue. It is possible that jobs got purged but there is no event to\n-          // trigger the pipeline to clean context.\n-          HelixDataAccessor accessor = manager.getHelixDataAccessor();\n-          List<String> resourceConfigs =\n-              accessor.getChildNames(accessor.keyBuilder().resourceConfigs());\n-          if (resourceConfigs.size() > 0) {\n-            RebalanceUtil.scheduleOnDemandPipeline(manager.getClusterName(), 0L);\n-          } else {\n-            LOG.warn(\n-                \"No resource config to trigger rebalance for clean up contexts for\" + expiredJobs);\n-          }\n-        }\n+    if (expiredJobs.size() > 0) {\n+      // Update workflow context will be in main pipeline not here. Otherwise, it will cause\n+      // concurrent write issue. It is possible that jobs got purged but there is no event to\n+      // trigger the pipeline to clean context.\n+      HelixDataAccessor accessor = manager.getHelixDataAccessor();\n+      List<String> resourceConfigs =\n+          accessor.getChildNames(accessor.keyBuilder().resourceConfigs());\n+      if (resourceConfigs.size() > 0) {\n+        RebalanceUtil.scheduleOnDemandPipeline(manager.getClusterName(), 0L);\n+      } else {\n+        LOG.warn(\"No resource config to trigger rebalance for clean up contexts for\" + expiredJobs);\n       }\n     }\n-    setNextJobPurgeTime(workflow, currentTime, purgeInterval, rebalanceScheduler, manager);", "originalCommit": "c089b286ffab0a320c7d997b88cdd77614c5aa65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUyNDc1MA==", "url": "https://github.com/apache/helix/pull/1076#discussion_r444524750", "bodyText": "One concern here is that if we move the logic from function to the stage. We make the clean up logic dedicated to pipeline. Say some one they just want to call the purge from REST and do a job purge. You will not be able to do it.", "author": "junkaixue", "createdAt": "2020-06-23T21:41:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MDA5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM1NTc3Mw==", "url": "https://github.com/apache/helix/pull/1076#discussion_r446355773", "bodyText": "That's a good point, but I think you may have gotten this backwards? setNextJobPurgeTime should be a part of the pipeline, so it makes more sense to be in the stage instead of the function, right? If there is ever a REST endpoint that triggers the function, we don't want to schedule a rebalance.", "author": "NealSun96", "createdAt": "2020-06-26T18:54:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MDA5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYyNDIzMA==", "url": "https://github.com/apache/helix/pull/1076#discussion_r454624230", "bodyText": "Resolved offline.", "author": "NealSun96", "createdAt": "2020-07-14T20:27:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MDA5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUyNTAyOA==", "url": "https://github.com/apache/helix/pull/1076#discussion_r444525028", "bodyText": "You can mark a TODO here. We dont need it in the future since TF is not relying on IS/EV anymore.", "author": "junkaixue", "createdAt": "2020-06-23T21:41:50Z", "path": "helix-core/src/main/java/org/apache/helix/task/TaskUtil.java", "diffHunk": "@@ -977,72 +1011,70 @@ public static boolean isJobStarted(String job, WorkflowContext workflowContext)\n   }\n \n   /**\n-   * Clean up all jobs that are COMPLETED and passes its expiry time.\n-   * @param workflowConfig\n-   * @param workflowContext\n+   * Clean up all jobs that are marked as expired.\n    */\n-  public static void purgeExpiredJobs(String workflow, WorkflowConfig workflowConfig,\n-      WorkflowContext workflowContext, HelixManager manager,\n-      RebalanceScheduler rebalanceScheduler) {\n-    if (workflowContext == null) {\n-      LOG.warn(String.format(\"Workflow %s context does not exist!\", workflow));\n-      return;\n+  public static void purgeExpiredJobs(String workflow, Set<String> expiredJobs,\n+      HelixManager manager, RebalanceScheduler rebalanceScheduler) {\n+    Set<String> failedJobRemovals = new HashSet<>();\n+    for (String job : expiredJobs) {\n+      if (!TaskUtil\n+          .removeJob(manager.getHelixDataAccessor(), manager.getHelixPropertyStore(), job)) {\n+        failedJobRemovals.add(job);\n+        LOG.warn(\"Failed to clean up expired and completed jobs from workflow {}!\", workflow);\n+      }\n+      rebalanceScheduler.removeScheduledRebalance(job);\n     }\n-    long purgeInterval = workflowConfig.getJobPurgeInterval();\n-    long currentTime = System.currentTimeMillis();\n-    final Set<String> expiredJobs = Sets.newHashSet();\n-    if (purgeInterval > 0 && workflowContext.getLastJobPurgeTime() + purgeInterval <= currentTime) {\n-      expiredJobs.addAll(TaskUtil.getExpiredJobs(manager.getHelixDataAccessor(),\n-          manager.getHelixPropertyStore(), workflowConfig, workflowContext));\n-      if (expiredJobs.isEmpty()) {\n-        LOG.info(\"No job to purge for the queue \" + workflow);\n-      } else {\n-        LOG.info(\"Purge jobs \" + expiredJobs + \" from queue \" + workflow);\n-        Set<String> failedJobRemovals = new HashSet<>();\n-        for (String job : expiredJobs) {\n-          if (!TaskUtil.removeJob(manager.getHelixDataAccessor(), manager.getHelixPropertyStore(),\n-              job)) {\n-            failedJobRemovals.add(job);\n-            LOG.warn(\"Failed to clean up expired and completed jobs from workflow \" + workflow);\n-          }\n-          rebalanceScheduler.removeScheduledRebalance(job);\n-        }\n \n-        // If the job removal failed, make sure we do NOT prematurely delete it from DAG so that the\n-        // removal will be tried again at next purge\n-        expiredJobs.removeAll(failedJobRemovals);\n+    // If the job removal failed, make sure we do NOT prematurely delete it from DAG so that the\n+    // removal will be tried again at next purge\n+    expiredJobs.removeAll(failedJobRemovals);\n \n-        if (!TaskUtil.removeJobsFromDag(manager.getHelixDataAccessor(), workflow, expiredJobs,\n-            true)) {\n-          LOG.warn(\"Error occurred while trying to remove jobs + \" + expiredJobs\n-              + \" from the workflow \" + workflow);\n-        }\n+    if (!TaskUtil.removeJobsFromDag(manager.getHelixDataAccessor(), workflow, expiredJobs, true)) {\n+      LOG.warn(\"Error occurred while trying to remove jobs {} from the workflow {}!\", expiredJobs,\n+          workflow);\n+    }\n \n-        if (expiredJobs.size() > 0) {\n-          // Update workflow context will be in main pipeline not here. Otherwise, it will cause\n-          // concurrent write issue. It is possible that jobs got purged but there is no event to\n-          // trigger the pipeline to clean context.\n-          HelixDataAccessor accessor = manager.getHelixDataAccessor();\n-          List<String> resourceConfigs =\n-              accessor.getChildNames(accessor.keyBuilder().resourceConfigs());\n-          if (resourceConfigs.size() > 0) {\n-            RebalanceUtil.scheduleOnDemandPipeline(manager.getClusterName(), 0L);\n-          } else {\n-            LOG.warn(\n-                \"No resource config to trigger rebalance for clean up contexts for\" + expiredJobs);\n-          }\n-        }\n+    if (expiredJobs.size() > 0) {\n+      // Update workflow context will be in main pipeline not here. Otherwise, it will cause\n+      // concurrent write issue. It is possible that jobs got purged but there is no event to\n+      // trigger the pipeline to clean context.\n+      HelixDataAccessor accessor = manager.getHelixDataAccessor();\n+      List<String> resourceConfigs =\n+          accessor.getChildNames(accessor.keyBuilder().resourceConfigs());\n+      if (resourceConfigs.size() > 0) {\n+        RebalanceUtil.scheduleOnDemandPipeline(manager.getClusterName(), 0L);\n+      } else {\n+        LOG.warn(\"No resource config to trigger rebalance for clean up contexts for {}!\",\n+            expiredJobs);\n       }\n     }\n-    setNextJobPurgeTime(workflow, currentTime, purgeInterval, rebalanceScheduler, manager);\n   }\n \n-  private static void setNextJobPurgeTime(String workflow, long currentTime, long purgeInterval,\n-      RebalanceScheduler rebalanceScheduler, HelixManager manager) {\n-    long nextPurgeTime = currentTime + purgeInterval;\n-    long currentScheduledTime = rebalanceScheduler.getRebalanceTime(workflow);\n-    if (currentScheduledTime == -1 || currentScheduledTime > nextPurgeTime) {\n-      rebalanceScheduler.scheduleRebalance(manager, workflow, nextPurgeTime);\n+  /**\n+   * The function that removes IdealStates and workflow contexts of the workflows that need to be\n+   * deleted.\n+   * @param toBeDeletedWorkflows\n+   * @param manager\n+   */\n+  public static void workflowGarbageCollection(final Set<String> toBeDeletedWorkflows,\n+      final HelixManager manager) {\n+    HelixDataAccessor accessor = manager.getHelixDataAccessor();\n+    HelixPropertyStore<ZNRecord> propertyStore = manager.getHelixPropertyStore();\n+\n+    for (String workflowName : toBeDeletedWorkflows) {\n+      LOG.warn(\n+          \"WorkflowContext exists for workflow {}. However, Workflow Config is missing! Deleting the WorkflowConfig and IdealState!!\",\n+          workflowName);\n+\n+      if (!cleanupWorkflowIdealStateExtView(accessor, workflowName)) {\n+        LOG.warn(\"Error occurred while trying to remove workflow idealstate/externalview for {}.\",\n+            workflowName);\n+        continue;\n+      }", "originalCommit": "42afe9ded7b7cafdab5eaaaa864965dc1efda414", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAyNDkzNg==", "url": "https://github.com/apache/helix/pull/1076#discussion_r448024936", "bodyText": "Can we have workflow config without having context? Do we need to do null check?", "author": "alirezazamani", "createdAt": "2020-06-30T22:59:09Z", "path": "helix-core/src/main/java/org/apache/helix/controller/stages/TaskGarbageCollectionStage.java", "diffHunk": "@@ -23,34 +29,87 @@ public AsyncWorkerType getAsyncWorkerType() {\n   }\n \n   @Override\n-  public void execute(ClusterEvent event) {\n-    WorkflowControllerDataProvider dataProvider =\n-        event.getAttribute(AttributeName.ControllerDataProvider.name());\n+  public void process(ClusterEvent event) throws Exception {\n+    // Use main thread to compute what jobs need to be purged, and what workflows need to be gc'ed.\n+    // This is to avoid race conditions since the cache will be modified. After this work, then the\n+    // async work will happen.\n     HelixManager manager = event.getAttribute(AttributeName.helixmanager.name());\n-\n-    if (dataProvider == null || manager == null) {\n+    if (manager == null) {\n       LOG.warn(\n-          \"ResourceControllerDataProvider or HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n+          \"HelixManager is null for event {}({}) in cluster {}. Skip TaskGarbageCollectionStage.\",\n           event.getEventId(), event.getEventType(), event.getClusterName());\n       return;\n     }\n \n-    Set<WorkflowConfig> existingWorkflows =\n-        new HashSet<>(dataProvider.getWorkflowConfigMap().values());\n-    for (WorkflowConfig workflowConfig : existingWorkflows) {\n-      // clean up the expired jobs if it is a queue.\n+    Map<String, Set<String>> expiredJobsMap = new HashMap<>();\n+    Set<String> workflowsToBeDeleted = new HashSet<>();\n+    WorkflowControllerDataProvider dataProvider =\n+        event.getAttribute(AttributeName.ControllerDataProvider.name());\n+    for (Map.Entry<String, ZNRecord> entry : dataProvider.getContexts().entrySet()) {\n+      WorkflowConfig workflowConfig = dataProvider.getWorkflowConfig(entry.getKey());\n       if (workflowConfig != null && (!workflowConfig.isTerminable() || workflowConfig\n           .isJobQueue())) {\n-        try {\n-          TaskUtil.purgeExpiredJobs(workflowConfig.getWorkflowId(), workflowConfig,\n-              dataProvider.getWorkflowContext(workflowConfig.getWorkflowId()), manager,\n-              _rebalanceScheduler);\n-        } catch (Exception e) {\n-          LOG.warn(String.format(\"Failed to purge job for workflow %s with reason %s\",\n-              workflowConfig.getWorkflowId(), e.toString()));\n+        WorkflowContext workflowContext = dataProvider.getWorkflowContext(entry.getKey());", "originalCommit": "986a9ee0aaf27f629129d37433eea7b67096bbcd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA3MDU5MA==", "url": "https://github.com/apache/helix/pull/1076#discussion_r448070590", "bodyText": "I don't believe it's necessary as entry come from dataProvider.getContexts().entrySet()", "author": "NealSun96", "createdAt": "2020-07-01T01:46:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAyNDkzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA3MTUwNg==", "url": "https://github.com/apache/helix/pull/1076#discussion_r448071506", "bodyText": "Ok makes sense.", "author": "alirezazamani", "createdAt": "2020-07-01T01:50:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAyNDkzNg=="}], "type": "inlineReview"}, {"oid": "a882840ce44096c5a6e099ceca86d326de69b8d5", "url": "https://github.com/apache/helix/commit/a882840ce44096c5a6e099ceca86d326de69b8d5", "message": "Recover logics", "committedDate": "2020-07-01T17:55:27Z", "type": "commit"}, {"oid": "e11a983683cc00f4642d8809ec41fe284863aa49", "url": "https://github.com/apache/helix/commit/e11a983683cc00f4642d8809ec41fe284863aa49", "message": "Fix broken test", "committedDate": "2020-07-01T17:55:27Z", "type": "commit"}, {"oid": "a6e0f441d8f532e00d0c82d06e00fab134ac0886", "url": "https://github.com/apache/helix/commit/a6e0f441d8f532e00d0c82d06e00fab134ac0886", "message": "Address some comments", "committedDate": "2020-07-01T17:55:27Z", "type": "commit"}, {"oid": "5dcf29923e9669d25bc62a3e2b242de8efd91955", "url": "https://github.com/apache/helix/commit/5dcf29923e9669d25bc62a3e2b242de8efd91955", "message": "Temporary changes", "committedDate": "2020-07-01T17:55:27Z", "type": "commit"}, {"oid": "7cca49ab1f69e38b448b16ee49fe3e22a1ebeca0", "url": "https://github.com/apache/helix/commit/7cca49ab1f69e38b448b16ee49fe3e22a1ebeca0", "message": "New direction", "committedDate": "2020-07-01T17:55:28Z", "type": "commit"}, {"oid": "87f86bcb2427ec83e4ff66ecc5639571d31fe4d7", "url": "https://github.com/apache/helix/commit/87f86bcb2427ec83e4ff66ecc5639571d31fe4d7", "message": "Add comments etc", "committedDate": "2020-07-01T17:55:28Z", "type": "commit"}, {"oid": "3ec883ed1204dec8309e07303ea3750843281b36", "url": "https://github.com/apache/helix/commit/3ec883ed1204dec8309e07303ea3750843281b36", "message": "New test and nits", "committedDate": "2020-07-01T17:55:28Z", "type": "commit"}, {"oid": "8d327f92bc676d5609cf8cab8dbfd530b954ca30", "url": "https://github.com/apache/helix/commit/8d327f92bc676d5609cf8cab8dbfd530b954ca30", "message": "address comments", "committedDate": "2020-07-01T17:55:28Z", "type": "commit"}, {"oid": "8d327f92bc676d5609cf8cab8dbfd530b954ca30", "url": "https://github.com/apache/helix/commit/8d327f92bc676d5609cf8cab8dbfd530b954ca30", "message": "address comments", "committedDate": "2020-07-01T17:55:28Z", "type": "forcePushed"}, {"oid": "b7df8665602af2368a2224dc85b89af1fda3d2a7", "url": "https://github.com/apache/helix/commit/b7df8665602af2368a2224dc85b89af1fda3d2a7", "message": "Renaming", "committedDate": "2020-07-14T00:14:21Z", "type": "commit"}]}