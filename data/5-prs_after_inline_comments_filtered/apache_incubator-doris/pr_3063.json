{"pr_number": 3063, "pr_title": "(#3061) [Spark Load] Doris Support Using Hive Table to Build Global Dict", "pr_createdAt": "2020-03-09T11:31:23Z", "pr_url": "https://github.com/apache/incubator-doris/pull/3063", "timeline": [{"oid": "535d89d3aba0ff43300c524cfb8fb1efa9f94d59", "url": "https://github.com/apache/incubator-doris/commit/535d89d3aba0ff43300c524cfb8fb1efa9f94d59", "message": "(#3061) [Spark Load] Doris Support Using Hive Table to Build Global Dict", "committedDate": "2020-03-09T11:29:32Z", "type": "commit"}, {"oid": "f085241d895664298443fda908ff564f93b7b12c", "url": "https://github.com/apache/incubator-doris/commit/f085241d895664298443fda908ff564f93b7b12c", "message": "1 use bigint instead of int in global dict\n2 add bigint overflow check", "committedDate": "2020-03-10T06:43:04Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjgwODcyOQ==", "url": "https://github.com/apache/incubator-doris/pull/3063#discussion_r392808729", "bodyText": "How about GlobalDictBuilder?", "author": "morningman", "createdAt": "2020-03-16T06:48:14Z", "path": "fe/src/main/java/org/apache/doris/load/loadv2/BuildGlobalDict.java", "diffHunk": "@@ -0,0 +1,272 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalog.Column;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+/**\n+ *  used for build hive global dict and encode source hive table\n+ *\n+ *  input: a source hive table\n+ *  output: a intermediate hive table whose distinct column is encode with int value\n+ *\n+ *  usage example\n+ *  step1,create a intermediate hive table\n+ *      BuildGlobalDict.createHiveIntermediateTable()\n+ *  step2, get distinct column's value\n+ *      BuildGlobalDict.extractDistinctColumn()\n+ *  step3, build global dict\n+ *      BuildGlobalDict.buildGlobalDict()\n+ *  step4, encode intermediate hive table with global dict\n+ *      BuildGlobalDict.encodeDorisIntermediateHiveTable()\n+ */\n+\n+public class BuildGlobalDict {", "originalCommit": "f085241d895664298443fda908ff564f93b7b12c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM0NDI3MA==", "url": "https://github.com/apache/incubator-doris/pull/3063#discussion_r398344270", "bodyText": "sounds great", "author": "wangbo", "createdAt": "2020-03-26T06:35:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjgwODcyOQ=="}], "type": "inlineReview"}, {"oid": "9d269085567c10b5c049da10550e17e5acb111b0", "url": "https://github.com/apache/incubator-doris/commit/9d269085567c10b5c049da10550e17e5acb111b0", "message": "tmp submit", "committedDate": "2020-03-27T02:50:55Z", "type": "commit"}, {"oid": "2bd83043e19beb58e2e9af11e784dc2065e584e9", "url": "https://github.com/apache/incubator-doris/commit/2bd83043e19beb58e2e9af11e784dc2065e584e9", "message": "[Spark load] build global dict for spark load", "committedDate": "2020-05-29T10:19:50Z", "type": "commit"}, {"oid": "18c55e6d8e3059542b34138e1595965e88a87aa7", "url": "https://github.com/apache/incubator-doris/commit/18c55e6d8e3059542b34138e1595965e88a87aa7", "message": "update comment", "committedDate": "2020-05-29T10:35:14Z", "type": "commit"}, {"oid": "18c55e6d8e3059542b34138e1595965e88a87aa7", "url": "https://github.com/apache/incubator-doris/commit/18c55e6d8e3059542b34138e1595965e88a87aa7", "message": "update comment", "committedDate": "2020-05-29T10:35:14Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM4MTM1NQ==", "url": "https://github.com/apache/incubator-doris/pull/3063#discussion_r439381355", "bodyText": "if column name of doris table is upper, this will be wrong.\nMaybe you can use TreeMap for source hive table columns.", "author": "wyb", "createdAt": "2020-06-12T12:11:55Z", "path": "fe/src/main/java/org/apache/doris/load/loadv2/dpp/GlobalDictBuilder.java", "diffHunk": "@@ -0,0 +1,413 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2.dpp;\n+\n+import org.apache.commons.collections.map.MultiValueMap;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalog.Column;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+/**\n+ *  used for build hive global dict and encode source hive table\n+ *\n+ *  input: a source hive table\n+ *  output: a intermediate hive table whose distinct column is encode with int value\n+ *\n+ *  usage example\n+ *  step1,create a intermediate hive table\n+ *      BuildGlobalDict.createHiveIntermediateTable()\n+ *  step2, get distinct column's value\n+ *      BuildGlobalDict.extractDistinctColumn()\n+ *  step3, build global dict\n+ *      BuildGlobalDict.buildGlobalDict()\n+ *  step4, encode intermediate hive table with global dict\n+ *      BuildGlobalDict.encodeDorisIntermediateHiveTable()\n+ */\n+\n+public class GlobalDictBuilder {\n+\n+    protected static final Logger LOG = LoggerFactory.getLogger(GlobalDictBuilder.class);\n+\n+    // name of the column in doris table which need to build global dict\n+    // for example: some dict columns a,b,c\n+    // case 1: all dict columns has no relation, then the map is as below\n+    //     [a=null, b=null, c=null]\n+    // case 2: column a's value can reuse column b's value which means column a's value is a subset of column b's value\n+    //  [b=a,c=null]\n+    private MultiValueMap dictColumn;\n+    // target doris table columns in current spark load job\n+    private List<String> dorisOlapTableColumnList;\n+\n+    // distinct columns which need to use map join to solve data skew in encodeDorisIntermediateHiveTable()\n+    // we needn't to specify it until data skew happends\n+    private List<String> mapSideJoinColumns;\n+\n+    // hive table datasource,format is db.table\n+    private String sourceHiveDBTableName;\n+    // user-specified filter when query sourceHiveDBTable\n+    private String sourceHiveFilter;\n+    // intermediate hive table to store the distinct value of distinct column\n+    private String distinctKeyTableName;\n+    // current doris table's global dict hive table\n+    private String globalDictTableName;\n+\n+    // used for next step to read\n+    private String dorisIntermediateHiveTable;\n+    private SparkSession spark;\n+\n+    // key=doris column name,value=column type\n+    private Map<String, String> dorisColumnNameTypeMap = new HashMap<>();\n+\n+    // column in this list means need split distinct value and then encode respectively\n+    // to avoid the performance bottleneck to transfer origin value to dict value\n+    private List<String> veryHighCardinalityColumn;\n+    // determine the split num of new distinct value,better can be divisible by 1\n+    private int veryHighCardinalityColumnSplitNum;\n+\n+    private ExecutorService pool;\n+\n+    private StructType distinctValueSchema;\n+\n+    public GlobalDictBuilder(MultiValueMap dictColumn,\n+                             List<String> dorisOlapTableColumnList,\n+                             List<String> mapSideJoinColumns,\n+                             String sourceHiveDBTableName,\n+                             String sourceHiveFilter,\n+                             String dorisHiveDB,\n+                             String distinctKeyTableName,\n+                             String globalDictTableName,\n+                             String dorisIntermediateHiveTable,\n+                             int buildConcurrency,\n+                             List<String> veryHighCardinalityColumn,\n+                             int veryHighCardinalityColumnSplitNum,\n+                             SparkSession spark) {\n+        this.dictColumn = dictColumn;\n+        this.dorisOlapTableColumnList = dorisOlapTableColumnList;\n+        this.mapSideJoinColumns = mapSideJoinColumns;\n+        this.sourceHiveDBTableName = sourceHiveDBTableName;\n+        this.sourceHiveFilter = sourceHiveFilter;\n+        this.distinctKeyTableName = distinctKeyTableName;\n+        this.globalDictTableName = globalDictTableName;\n+        this.dorisIntermediateHiveTable = dorisIntermediateHiveTable;\n+        this.spark = spark;\n+        this.pool = Executors.newFixedThreadPool(buildConcurrency < 0 ? 1 : buildConcurrency);\n+        this.veryHighCardinalityColumn = veryHighCardinalityColumn;\n+        this.veryHighCardinalityColumnSplitNum = veryHighCardinalityColumnSplitNum;\n+\n+        spark.sql(\"use \" + dorisHiveDB);\n+    }\n+\n+    public void createHiveIntermediateTable() throws AnalysisException {\n+        Map<String, String> sourceHiveTableColumn = spark.catalog()\n+                .listColumns(sourceHiveDBTableName)\n+                .collectAsList()\n+                .stream().collect(Collectors.toMap(Column::name, Column::dataType));\n+\n+        Map<String, String> sourceHiveTableColumnInLowercase = new HashMap<>();\n+        for (Map.Entry<String, String> entry : sourceHiveTableColumn.entrySet()) {\n+            sourceHiveTableColumnInLowercase.put(entry.getKey().toLowerCase(), entry.getValue().toLowerCase());\n+        }\n+\n+        // check and get doris column type in hive\n+        dorisOlapTableColumnList.stream().forEach(columnName -> {\n+            String columnType = sourceHiveTableColumnInLowercase.get(columnName);", "originalCommit": "53bfc0239d8e9e557af0028a8525b816d242e01a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM4MTk5Nw==", "url": "https://github.com/apache/incubator-doris/pull/3063#discussion_r439381997", "bodyText": "two spaces", "author": "wyb", "createdAt": "2020-06-12T12:13:21Z", "path": "fe/src/main/java/org/apache/doris/load/loadv2/dpp/GlobalDictBuilder.java", "diffHunk": "@@ -0,0 +1,413 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2.dpp;\n+\n+import org.apache.commons.collections.map.MultiValueMap;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalog.Column;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+/**\n+ *  used for build hive global dict and encode source hive table\n+ *\n+ *  input: a source hive table\n+ *  output: a intermediate hive table whose distinct column is encode with int value\n+ *\n+ *  usage example\n+ *  step1,create a intermediate hive table\n+ *      BuildGlobalDict.createHiveIntermediateTable()\n+ *  step2, get distinct column's value\n+ *      BuildGlobalDict.extractDistinctColumn()\n+ *  step3, build global dict\n+ *      BuildGlobalDict.buildGlobalDict()\n+ *  step4, encode intermediate hive table with global dict\n+ *      BuildGlobalDict.encodeDorisIntermediateHiveTable()\n+ */\n+\n+public class GlobalDictBuilder {\n+\n+    protected static final Logger LOG = LoggerFactory.getLogger(GlobalDictBuilder.class);\n+\n+    // name of the column in doris table which need to build global dict\n+    // for example: some dict columns a,b,c\n+    // case 1: all dict columns has no relation, then the map is as below\n+    //     [a=null, b=null, c=null]\n+    // case 2: column a's value can reuse column b's value which means column a's value is a subset of column b's value\n+    //  [b=a,c=null]\n+    private MultiValueMap dictColumn;\n+    // target doris table columns in current spark load job\n+    private List<String> dorisOlapTableColumnList;\n+\n+    // distinct columns which need to use map join to solve data skew in encodeDorisIntermediateHiveTable()\n+    // we needn't to specify it until data skew happends\n+    private List<String> mapSideJoinColumns;\n+\n+    // hive table datasource,format is db.table\n+    private String sourceHiveDBTableName;\n+    // user-specified filter when query sourceHiveDBTable\n+    private String sourceHiveFilter;\n+    // intermediate hive table to store the distinct value of distinct column\n+    private String distinctKeyTableName;\n+    // current doris table's global dict hive table\n+    private String globalDictTableName;\n+\n+    // used for next step to read\n+    private String dorisIntermediateHiveTable;\n+    private SparkSession spark;\n+\n+    // key=doris column name,value=column type\n+    private Map<String, String> dorisColumnNameTypeMap = new HashMap<>();\n+\n+    // column in this list means need split distinct value and then encode respectively\n+    // to avoid the performance bottleneck to transfer origin value to dict value\n+    private List<String> veryHighCardinalityColumn;\n+    // determine the split num of new distinct value,better can be divisible by 1\n+    private int veryHighCardinalityColumnSplitNum;\n+\n+    private ExecutorService pool;\n+\n+    private StructType distinctValueSchema;\n+\n+    public GlobalDictBuilder(MultiValueMap dictColumn,\n+                             List<String> dorisOlapTableColumnList,\n+                             List<String> mapSideJoinColumns,\n+                             String sourceHiveDBTableName,\n+                             String sourceHiveFilter,\n+                             String dorisHiveDB,\n+                             String distinctKeyTableName,\n+                             String globalDictTableName,\n+                             String dorisIntermediateHiveTable,\n+                             int buildConcurrency,\n+                             List<String> veryHighCardinalityColumn,\n+                             int veryHighCardinalityColumnSplitNum,\n+                             SparkSession spark) {\n+        this.dictColumn = dictColumn;\n+        this.dorisOlapTableColumnList = dorisOlapTableColumnList;\n+        this.mapSideJoinColumns = mapSideJoinColumns;\n+        this.sourceHiveDBTableName = sourceHiveDBTableName;\n+        this.sourceHiveFilter = sourceHiveFilter;\n+        this.distinctKeyTableName = distinctKeyTableName;\n+        this.globalDictTableName = globalDictTableName;\n+        this.dorisIntermediateHiveTable = dorisIntermediateHiveTable;\n+        this.spark = spark;\n+        this.pool = Executors.newFixedThreadPool(buildConcurrency < 0 ? 1 : buildConcurrency);\n+        this.veryHighCardinalityColumn = veryHighCardinalityColumn;\n+        this.veryHighCardinalityColumnSplitNum = veryHighCardinalityColumnSplitNum;\n+\n+        spark.sql(\"use \" + dorisHiveDB);\n+    }\n+\n+    public void createHiveIntermediateTable() throws AnalysisException {\n+        Map<String, String> sourceHiveTableColumn = spark.catalog()\n+                .listColumns(sourceHiveDBTableName)\n+                .collectAsList()\n+                .stream().collect(Collectors.toMap(Column::name, Column::dataType));\n+\n+        Map<String, String> sourceHiveTableColumnInLowercase = new HashMap<>();\n+        for (Map.Entry<String, String> entry : sourceHiveTableColumn.entrySet()) {\n+            sourceHiveTableColumnInLowercase.put(entry.getKey().toLowerCase(), entry.getValue().toLowerCase());\n+        }\n+\n+        // check and get doris column type in hive\n+        dorisOlapTableColumnList.stream().forEach(columnName -> {\n+            String columnType = sourceHiveTableColumnInLowercase.get(columnName);\n+            if (StringUtils.isEmpty(columnType)) {\n+                throw new RuntimeException(String.format(\"doris column %s not in source hive table\", columnName));\n+            }\n+            dorisColumnNameTypeMap.put(columnName, columnType);\n+        });\n+\n+        spark.sql(String.format(\"drop table if exists %s \", dorisIntermediateHiveTable));\n+        // create IntermediateHiveTable\n+        spark.sql(getCreateIntermediateHiveTableSql());\n+\n+        // insert data to IntermediateHiveTable\n+        spark.sql(getInsertIntermediateHiveTableSql());\n+    }\n+\n+    public void extractDistinctColumn() {\n+        // create distinct tables\n+        spark.sql(getCreateDistinctKeyTableSql());\n+\n+        // extract distinct column\n+        List<GlobalDictBuildWorker> workerList = new ArrayList<>();\n+        // For the column in dictColumns's valueSet, their value is a subset of column in keyset,\n+        // so we don't need to extract distinct value of column in valueSet\n+        for (Object column : dictColumn.keySet()) {\n+            workerList.add(()->{\n+                spark.sql(getInsertDistinctKeyTableSql(column.toString(), dorisIntermediateHiveTable));\n+            });\n+        }\n+\n+        submitWorker(workerList);\n+    }\n+\n+    public void buildGlobalDict() throws ExecutionException, InterruptedException {\n+        // create global dict hive table\n+        spark.sql(getCreateGlobalDictHiveTableSql());\n+\n+        List<GlobalDictBuildWorker> globalDictBuildWorkers = new ArrayList<>();\n+        for (Object distinctColumnNameOrigin : dictColumn.keySet()) {\n+            String distinctColumnNameTmp = distinctColumnNameOrigin.toString();\n+            globalDictBuildWorkers.add(()->{\n+                // get global dict max value\n+                List<Row> maxGlobalDictValueRow = spark.sql(getMaxGlobalDictValueSql(distinctColumnNameTmp)).collectAsList();\n+                if (maxGlobalDictValueRow.size() == 0) {\n+                    throw new RuntimeException(String.format(\"get max dict value failed: %s\", distinctColumnNameTmp));\n+                }\n+\n+                long maxDictValue = 0;\n+                long minDictValue = 0;\n+                Row row = maxGlobalDictValueRow.get(0);\n+                if (row != null && row.get(0) != null) {\n+                    maxDictValue = (long)row.get(0);\n+                    minDictValue = (long)row.get(1);\n+                }\n+                LOG.info(\" column {} 's max value in dict is {} , min value is {}\", distinctColumnNameTmp, maxDictValue, minDictValue);\n+                // maybe never happened, but we need detect it\n+                if (minDictValue < 0) {\n+                    throw new RuntimeException(String.format(\" column %s 's cardinality has exceed bigint's max value\", distinctColumnNameTmp));\n+                }\n+\n+                if (veryHighCardinalityColumn.contains(distinctColumnNameTmp) && veryHighCardinalityColumnSplitNum > 1) {\n+                    // split distinct key first and then encode with count\n+                    buildGlobalDictBySplit(maxDictValue, distinctColumnNameTmp);\n+                } else {\n+                    // build global dict directly\n+                    spark.sql(getBuildGlobalDictSql(maxDictValue, distinctColumnNameTmp));\n+                }\n+\n+            });\n+        }\n+        submitWorker(globalDictBuildWorkers);\n+    }\n+\n+    // encode dorisIntermediateHiveTable's distinct column\n+    public void encodeDorisIntermediateHiveTable() {\n+        for (Object distinctColumnObj  : dictColumn.keySet()) {", "originalCommit": "53bfc0239d8e9e557af0028a8525b816d242e01a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM4MjU5NQ==", "url": "https://github.com/apache/incubator-doris/pull/3063#discussion_r439382595", "bodyText": "better using List<String> childColumn", "author": "wyb", "createdAt": "2020-06-12T12:14:55Z", "path": "fe/src/main/java/org/apache/doris/load/loadv2/dpp/GlobalDictBuilder.java", "diffHunk": "@@ -0,0 +1,413 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2.dpp;\n+\n+import org.apache.commons.collections.map.MultiValueMap;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalog.Column;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+/**\n+ *  used for build hive global dict and encode source hive table\n+ *\n+ *  input: a source hive table\n+ *  output: a intermediate hive table whose distinct column is encode with int value\n+ *\n+ *  usage example\n+ *  step1,create a intermediate hive table\n+ *      BuildGlobalDict.createHiveIntermediateTable()\n+ *  step2, get distinct column's value\n+ *      BuildGlobalDict.extractDistinctColumn()\n+ *  step3, build global dict\n+ *      BuildGlobalDict.buildGlobalDict()\n+ *  step4, encode intermediate hive table with global dict\n+ *      BuildGlobalDict.encodeDorisIntermediateHiveTable()\n+ */\n+\n+public class GlobalDictBuilder {\n+\n+    protected static final Logger LOG = LoggerFactory.getLogger(GlobalDictBuilder.class);\n+\n+    // name of the column in doris table which need to build global dict\n+    // for example: some dict columns a,b,c\n+    // case 1: all dict columns has no relation, then the map is as below\n+    //     [a=null, b=null, c=null]\n+    // case 2: column a's value can reuse column b's value which means column a's value is a subset of column b's value\n+    //  [b=a,c=null]\n+    private MultiValueMap dictColumn;\n+    // target doris table columns in current spark load job\n+    private List<String> dorisOlapTableColumnList;\n+\n+    // distinct columns which need to use map join to solve data skew in encodeDorisIntermediateHiveTable()\n+    // we needn't to specify it until data skew happends\n+    private List<String> mapSideJoinColumns;\n+\n+    // hive table datasource,format is db.table\n+    private String sourceHiveDBTableName;\n+    // user-specified filter when query sourceHiveDBTable\n+    private String sourceHiveFilter;\n+    // intermediate hive table to store the distinct value of distinct column\n+    private String distinctKeyTableName;\n+    // current doris table's global dict hive table\n+    private String globalDictTableName;\n+\n+    // used for next step to read\n+    private String dorisIntermediateHiveTable;\n+    private SparkSession spark;\n+\n+    // key=doris column name,value=column type\n+    private Map<String, String> dorisColumnNameTypeMap = new HashMap<>();\n+\n+    // column in this list means need split distinct value and then encode respectively\n+    // to avoid the performance bottleneck to transfer origin value to dict value\n+    private List<String> veryHighCardinalityColumn;\n+    // determine the split num of new distinct value,better can be divisible by 1\n+    private int veryHighCardinalityColumnSplitNum;\n+\n+    private ExecutorService pool;\n+\n+    private StructType distinctValueSchema;\n+\n+    public GlobalDictBuilder(MultiValueMap dictColumn,\n+                             List<String> dorisOlapTableColumnList,\n+                             List<String> mapSideJoinColumns,\n+                             String sourceHiveDBTableName,\n+                             String sourceHiveFilter,\n+                             String dorisHiveDB,\n+                             String distinctKeyTableName,\n+                             String globalDictTableName,\n+                             String dorisIntermediateHiveTable,\n+                             int buildConcurrency,\n+                             List<String> veryHighCardinalityColumn,\n+                             int veryHighCardinalityColumnSplitNum,\n+                             SparkSession spark) {\n+        this.dictColumn = dictColumn;\n+        this.dorisOlapTableColumnList = dorisOlapTableColumnList;\n+        this.mapSideJoinColumns = mapSideJoinColumns;\n+        this.sourceHiveDBTableName = sourceHiveDBTableName;\n+        this.sourceHiveFilter = sourceHiveFilter;\n+        this.distinctKeyTableName = distinctKeyTableName;\n+        this.globalDictTableName = globalDictTableName;\n+        this.dorisIntermediateHiveTable = dorisIntermediateHiveTable;\n+        this.spark = spark;\n+        this.pool = Executors.newFixedThreadPool(buildConcurrency < 0 ? 1 : buildConcurrency);\n+        this.veryHighCardinalityColumn = veryHighCardinalityColumn;\n+        this.veryHighCardinalityColumnSplitNum = veryHighCardinalityColumnSplitNum;\n+\n+        spark.sql(\"use \" + dorisHiveDB);\n+    }\n+\n+    public void createHiveIntermediateTable() throws AnalysisException {\n+        Map<String, String> sourceHiveTableColumn = spark.catalog()\n+                .listColumns(sourceHiveDBTableName)\n+                .collectAsList()\n+                .stream().collect(Collectors.toMap(Column::name, Column::dataType));\n+\n+        Map<String, String> sourceHiveTableColumnInLowercase = new HashMap<>();\n+        for (Map.Entry<String, String> entry : sourceHiveTableColumn.entrySet()) {\n+            sourceHiveTableColumnInLowercase.put(entry.getKey().toLowerCase(), entry.getValue().toLowerCase());\n+        }\n+\n+        // check and get doris column type in hive\n+        dorisOlapTableColumnList.stream().forEach(columnName -> {\n+            String columnType = sourceHiveTableColumnInLowercase.get(columnName);\n+            if (StringUtils.isEmpty(columnType)) {\n+                throw new RuntimeException(String.format(\"doris column %s not in source hive table\", columnName));\n+            }\n+            dorisColumnNameTypeMap.put(columnName, columnType);\n+        });\n+\n+        spark.sql(String.format(\"drop table if exists %s \", dorisIntermediateHiveTable));\n+        // create IntermediateHiveTable\n+        spark.sql(getCreateIntermediateHiveTableSql());\n+\n+        // insert data to IntermediateHiveTable\n+        spark.sql(getInsertIntermediateHiveTableSql());\n+    }\n+\n+    public void extractDistinctColumn() {\n+        // create distinct tables\n+        spark.sql(getCreateDistinctKeyTableSql());\n+\n+        // extract distinct column\n+        List<GlobalDictBuildWorker> workerList = new ArrayList<>();\n+        // For the column in dictColumns's valueSet, their value is a subset of column in keyset,\n+        // so we don't need to extract distinct value of column in valueSet\n+        for (Object column : dictColumn.keySet()) {\n+            workerList.add(()->{\n+                spark.sql(getInsertDistinctKeyTableSql(column.toString(), dorisIntermediateHiveTable));\n+            });\n+        }\n+\n+        submitWorker(workerList);\n+    }\n+\n+    public void buildGlobalDict() throws ExecutionException, InterruptedException {\n+        // create global dict hive table\n+        spark.sql(getCreateGlobalDictHiveTableSql());\n+\n+        List<GlobalDictBuildWorker> globalDictBuildWorkers = new ArrayList<>();\n+        for (Object distinctColumnNameOrigin : dictColumn.keySet()) {\n+            String distinctColumnNameTmp = distinctColumnNameOrigin.toString();\n+            globalDictBuildWorkers.add(()->{\n+                // get global dict max value\n+                List<Row> maxGlobalDictValueRow = spark.sql(getMaxGlobalDictValueSql(distinctColumnNameTmp)).collectAsList();\n+                if (maxGlobalDictValueRow.size() == 0) {\n+                    throw new RuntimeException(String.format(\"get max dict value failed: %s\", distinctColumnNameTmp));\n+                }\n+\n+                long maxDictValue = 0;\n+                long minDictValue = 0;\n+                Row row = maxGlobalDictValueRow.get(0);\n+                if (row != null && row.get(0) != null) {\n+                    maxDictValue = (long)row.get(0);\n+                    minDictValue = (long)row.get(1);\n+                }\n+                LOG.info(\" column {} 's max value in dict is {} , min value is {}\", distinctColumnNameTmp, maxDictValue, minDictValue);\n+                // maybe never happened, but we need detect it\n+                if (minDictValue < 0) {\n+                    throw new RuntimeException(String.format(\" column %s 's cardinality has exceed bigint's max value\", distinctColumnNameTmp));\n+                }\n+\n+                if (veryHighCardinalityColumn.contains(distinctColumnNameTmp) && veryHighCardinalityColumnSplitNum > 1) {\n+                    // split distinct key first and then encode with count\n+                    buildGlobalDictBySplit(maxDictValue, distinctColumnNameTmp);\n+                } else {\n+                    // build global dict directly\n+                    spark.sql(getBuildGlobalDictSql(maxDictValue, distinctColumnNameTmp));\n+                }\n+\n+            });\n+        }\n+        submitWorker(globalDictBuildWorkers);\n+    }\n+\n+    // encode dorisIntermediateHiveTable's distinct column\n+    public void encodeDorisIntermediateHiveTable() {\n+        for (Object distinctColumnObj  : dictColumn.keySet()) {\n+            spark.sql(getEncodeDorisIntermediateHiveTableSql(distinctColumnObj.toString(), (ArrayList)dictColumn.get(distinctColumnObj.toString())));\n+        }\n+    }\n+\n+    private String getCreateIntermediateHiveTableSql() {\n+        StringBuilder sql = new StringBuilder();\n+        sql.append(\"create table if not exists \" + dorisIntermediateHiveTable + \" ( \");\n+\n+        Set<String> allDictColumn = new HashSet<>();\n+        allDictColumn.addAll(dictColumn.keySet());\n+        allDictColumn.addAll(dictColumn.values());\n+        dorisOlapTableColumnList.stream().forEach(columnName -> {\n+            sql.append(columnName).append(\" \");\n+            if (allDictColumn.contains(columnName)) {\n+                sql.append(\" string ,\");\n+            } else {\n+                sql.append(dorisColumnNameTypeMap.get(columnName)).append(\" ,\");\n+            }\n+        });\n+        return sql.deleteCharAt(sql.length() - 1).append(\" )\").append(\" stored as sequencefile \").toString();\n+    }\n+\n+    private String getInsertIntermediateHiveTableSql() {\n+        StringBuilder sql = new StringBuilder();\n+        sql.append(\"insert overwrite table \").append(dorisIntermediateHiveTable).append(\" select \");\n+        dorisOlapTableColumnList.stream().forEach(columnName -> {\n+            sql.append(columnName).append(\" ,\");\n+        });\n+        sql.deleteCharAt(sql.length() - 1)\n+                .append(\" from \").append(sourceHiveDBTableName);\n+        if (!StringUtils.isEmpty(sourceHiveFilter)) {\n+            sql.append(\" where \").append(sourceHiveFilter);\n+        }\n+        return sql.toString();\n+    }\n+\n+    private String getCreateDistinctKeyTableSql() {\n+        return \"create table if not exists \" + distinctKeyTableName + \"(dict_key string) partitioned by (dict_column string) stored as sequencefile \";\n+    }\n+\n+    private String getInsertDistinctKeyTableSql(String distinctColumnName, String sourceHiveTable) {\n+        StringBuilder sql = new StringBuilder();\n+        sql.append(\"insert overwrite table \").append(distinctKeyTableName)\n+                .append(\" partition(dict_column='\").append(distinctColumnName).append(\"')\")\n+                .append(\" select \").append(distinctColumnName)\n+                .append(\" from \").append(sourceHiveTable)\n+                .append(\" group by \").append(distinctColumnName);\n+        return sql.toString();\n+    }\n+\n+    private String getCreateGlobalDictHiveTableSql() {\n+        return \"create table if not exists \" + globalDictTableName\n+                + \"(dict_key string, dict_value bigint) partitioned by(dict_column string) stored as sequencefile \";\n+    }\n+\n+    private String getMaxGlobalDictValueSql(String distinctColumnName) {\n+        return \"select max(dict_value) as max_value,min(dict_value) as min_value from \" + globalDictTableName + \" where dict_column='\" + distinctColumnName + \"'\";\n+    }\n+\n+    private void buildGlobalDictBySplit(long maxGlobalDictValue, String distinctColumnName) {\n+        // 1. get distinct value\n+        Dataset<Row> newDistinctValue = spark.sql(getNewDistinctValue(distinctColumnName));\n+\n+        // 2. split the newDistinctValue to avoid window functions' single node bottleneck\n+        Dataset<Row>[] splitedDistinctValue = newDistinctValue.randomSplit(getRandomSplitWeights());\n+        long currentMaxDictValue = maxGlobalDictValue;\n+        Map<String, Long> distinctKeyMap = new HashMap<>();\n+\n+        for (int i = 0; i < splitedDistinctValue.length; i++) {\n+            long currentDatasetStartDictValue = currentMaxDictValue;\n+            long splitDistinctValueCount = splitedDistinctValue[i].count();\n+            currentMaxDictValue += splitDistinctValueCount;\n+            String tmpDictTableName = String.format(\"%s_%s_tmp_dict_%s\", i, currentDatasetStartDictValue, distinctColumnName);\n+            distinctKeyMap.put(tmpDictTableName, currentDatasetStartDictValue);\n+            Dataset<Row> distinctValueFrame = spark.createDataFrame(splitedDistinctValue[i].toJavaRDD(), getDistinctValueSchema());\n+            distinctValueFrame.createOrReplaceTempView(tmpDictTableName);\n+        }\n+\n+        spark.sql(getSplitBuildGlobalDictSql(distinctKeyMap, distinctColumnName));\n+\n+    }\n+\n+    private String getSplitBuildGlobalDictSql(Map<String, Long> distinctKeyMap, String distinctColumnName) {\n+        StringBuilder sql = new StringBuilder();\n+        sql.append(\"insert overwrite table \").append(globalDictTableName).append(\" partition(dict_column='\").append(distinctColumnName).append(\"') \")\n+                .append(\" select dict_key,dict_value from \").append(globalDictTableName).append(\" where dict_column='\").append(distinctColumnName).append(\"' \");\n+        for (Map.Entry<String, Long> entry : distinctKeyMap.entrySet()) {\n+            sql.append(\" union all select dict_key, (row_number() over(order by dict_key)) \")\n+                    .append(String.format(\" +(%s) as dict_value from %s\", entry.getValue(), entry.getKey()));\n+        }\n+        return sql.toString();\n+    }\n+\n+    private StructType getDistinctValueSchema() {\n+        if (distinctValueSchema == null) {\n+            List<StructField> fieldList = new ArrayList<>();\n+            fieldList.add(DataTypes.createStructField(\"dict_key\", DataTypes.StringType, false));\n+            distinctValueSchema = DataTypes.createStructType(fieldList);\n+        }\n+        return distinctValueSchema;\n+    }\n+\n+    private double[] getRandomSplitWeights() {\n+        double[] weights = new double[veryHighCardinalityColumnSplitNum];\n+        double weight = 1 / Double.parseDouble(String.valueOf(veryHighCardinalityColumnSplitNum));\n+        Arrays.fill(weights, weight);\n+        return weights;\n+    }\n+\n+    private String getBuildGlobalDictSql(long maxGlobalDictValue, String distinctColumnName) {\n+        return \"insert overwrite table \" + globalDictTableName + \" partition(dict_column='\" + distinctColumnName + \"') \"\n+                + \" select dict_key,dict_value from \" + globalDictTableName + \" where dict_column='\" + distinctColumnName + \"' \"\n+                + \" union all select t1.dict_key as dict_key,(row_number() over(order by t1.dict_key)) + (\" + maxGlobalDictValue + \") as dict_value from \"\n+                + \"(select dict_key from \" + distinctKeyTableName + \" where dict_column='\" + distinctColumnName + \"' and dict_key is not null)t1 left join \"\n+                + \" (select dict_key,dict_value from \" + globalDictTableName + \" where dict_column='\" + distinctColumnName + \"' )t2 \" +\n+                \"on t1.dict_key = t2.dict_key where t2.dict_value is null\";\n+    }\n+\n+    private String getNewDistinctValue(String distinctColumnName) {\n+        return  \"select t1.dict_key from \" +\n+                \" (select dict_key from \" + distinctKeyTableName + \" where dict_column='\" + distinctColumnName + \"' and dict_key is not null)t1 left join \" +\n+                \" (select dict_key,dict_value from \" + globalDictTableName + \" where dict_column='\" + distinctColumnName + \"' )t2 \" +\n+                \"on t1.dict_key = t2.dict_key where t2.dict_value is null\";\n+\n+    }\n+\n+    private String getEncodeDorisIntermediateHiveTableSql(String dictColumn, ArrayList<String> childColumn) {", "originalCommit": "53bfc0239d8e9e557af0028a8525b816d242e01a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ1NjQyNg==", "url": "https://github.com/apache/incubator-doris/pull/3063#discussion_r439456426", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             *      BuildGlobalDict.createHiveIntermediateTable()\n          \n          \n            \n             *      GlobalDictBuilder.createHiveIntermediateTable()", "author": "morningman", "createdAt": "2020-06-12T14:33:48Z", "path": "fe/src/main/java/org/apache/doris/load/loadv2/dpp/GlobalDictBuilder.java", "diffHunk": "@@ -0,0 +1,413 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2.dpp;\n+\n+import org.apache.commons.collections.map.MultiValueMap;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalog.Column;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+/**\n+ *  used for build hive global dict and encode source hive table\n+ *\n+ *  input: a source hive table\n+ *  output: a intermediate hive table whose distinct column is encode with int value\n+ *\n+ *  usage example\n+ *  step1,create a intermediate hive table\n+ *      BuildGlobalDict.createHiveIntermediateTable()", "originalCommit": "53bfc0239d8e9e557af0028a8525b816d242e01a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "66d0ba2f32e79a46b18a231550a1b342069fd837", "url": "https://github.com/apache/incubator-doris/commit/66d0ba2f32e79a46b18a231550a1b342069fd837", "message": "little change", "committedDate": "2020-06-15T11:26:27Z", "type": "forcePushed"}, {"oid": "8ba812e3b97c84cf8132e32cf8aed0c2776df583", "url": "https://github.com/apache/incubator-doris/commit/8ba812e3b97c84cf8132e32cf8aed0c2776df583", "message": "1 support dict reuse to reduce shuffle frequency\n\n2 little change\n\n3 change pom", "committedDate": "2020-06-17T07:12:54Z", "type": "commit"}, {"oid": "8ba812e3b97c84cf8132e32cf8aed0c2776df583", "url": "https://github.com/apache/incubator-doris/commit/8ba812e3b97c84cf8132e32cf8aed0c2776df583", "message": "1 support dict reuse to reduce shuffle frequency\n\n2 little change\n\n3 change pom", "committedDate": "2020-06-17T07:12:54Z", "type": "forcePushed"}, {"oid": "ec980b9b28a489796e972faeeab7ef0ec0dded32", "url": "https://github.com/apache/incubator-doris/commit/ec980b9b28a489796e972faeeab7ef0ec0dded32", "message": "change pom", "committedDate": "2020-06-17T07:17:16Z", "type": "commit"}, {"oid": "d049f6d70d9787d9d4d2916474f5d8ff4c286ed8", "url": "https://github.com/apache/incubator-doris/commit/d049f6d70d9787d9d4d2916474f5d8ff4c286ed8", "message": "change logger", "committedDate": "2020-06-22T04:04:32Z", "type": "commit"}]}