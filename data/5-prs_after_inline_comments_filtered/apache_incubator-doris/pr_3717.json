{"pr_number": 3717, "pr_title": "[Spark load][Fe 6/6] Fe process etl and loading state job", "pr_createdAt": "2020-05-28T17:14:04Z", "pr_url": "https://github.com/apache/incubator-doris/pull/3717", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgzNTA1Mw==", "url": "https://github.com/apache/incubator-doris/pull/3717#discussion_r432835053", "bodyText": "Why is this ScanNode needed?\nSeems that this scanNode will be destruct after this function.", "author": "imay", "createdAt": "2020-05-30T11:47:19Z", "path": "fe/src/main/java/org/apache/doris/load/loadv2/SparkLoadJob.java", "diffHunk": "@@ -0,0 +1,700 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import org.apache.doris.analysis.Analyzer;\n+import org.apache.doris.analysis.BrokerDesc;\n+import org.apache.doris.analysis.CastExpr;\n+import org.apache.doris.analysis.DescriptorTable;\n+import org.apache.doris.analysis.ResourceDesc;\n+import org.apache.doris.analysis.Expr;\n+import org.apache.doris.analysis.SlotDescriptor;\n+import org.apache.doris.analysis.SlotRef;\n+import org.apache.doris.analysis.TupleDescriptor;\n+import org.apache.doris.catalog.Catalog;\n+import org.apache.doris.catalog.Column;\n+import org.apache.doris.catalog.Database;\n+import org.apache.doris.catalog.FsBroker;\n+import org.apache.doris.catalog.MaterializedIndex;\n+import org.apache.doris.catalog.MaterializedIndex.IndexExtState;\n+import org.apache.doris.catalog.OlapTable;\n+import org.apache.doris.catalog.Partition;\n+import org.apache.doris.catalog.PrimitiveType;\n+import org.apache.doris.catalog.Replica;\n+import org.apache.doris.catalog.ScalarType;\n+import org.apache.doris.catalog.SparkResource;\n+import org.apache.doris.catalog.Tablet;\n+import org.apache.doris.catalog.Type;\n+import org.apache.doris.common.Config;\n+import org.apache.doris.common.LoadException;\n+import org.apache.doris.common.MetaNotFoundException;\n+import org.apache.doris.common.Pair;\n+import org.apache.doris.common.UserException;\n+import org.apache.doris.common.util.LogBuilder;\n+import org.apache.doris.common.util.LogKey;\n+import org.apache.doris.load.EtlJobType;\n+import org.apache.doris.load.EtlStatus;\n+import org.apache.doris.load.FailMsg;\n+import org.apache.doris.load.loadv2.etl.EtlJobConfig;\n+import org.apache.doris.planner.PlanNodeId;\n+import org.apache.doris.planner.ScanNode;\n+import org.apache.doris.qe.OriginStatement;\n+import org.apache.doris.system.Backend;\n+import org.apache.doris.task.AgentBatchTask;\n+import org.apache.doris.task.AgentTaskExecutor;\n+import org.apache.doris.task.AgentTaskQueue;\n+import org.apache.doris.task.PushTask;\n+import org.apache.doris.thrift.TBrokerRangeDesc;\n+import org.apache.doris.thrift.TBrokerScanRange;\n+import org.apache.doris.thrift.TBrokerScanRangeParams;\n+import org.apache.doris.thrift.TDescriptorTable;\n+import org.apache.doris.thrift.TFileFormatType;\n+import org.apache.doris.thrift.TFileType;\n+import org.apache.doris.thrift.TNetworkAddress;\n+import org.apache.doris.thrift.TPlanNode;\n+import org.apache.doris.thrift.TPriority;\n+import org.apache.doris.thrift.TPushType;\n+import org.apache.doris.thrift.TScanRangeLocations;\n+import org.apache.doris.transaction.TabletCommitInfo;\n+import org.apache.doris.transaction.TabletQuorumFailedException;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.spark.launcher.SparkAppHandle;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+import com.google.gson.annotations.SerializedName;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * There are 4 steps in SparkLoadJob:\n+ * Step1: SparkLoadPendingTask will be created by unprotectedExecuteJob method and submit spark etl job.\n+ * Step2: LoadEtlChecker will check spark etl job status periodly and send push tasks to be when spark etl job is finished.\n+ * Step3: LoadLoadingChecker will check loading status periodly and commit transaction when push tasks are finished.\n+ * Step4: PublishVersionDaemon will send publish version tasks to be and finish transaction.\n+ */\n+public class SparkLoadJob extends BulkLoadJob {\n+    private static final Logger LOG = LogManager.getLogger(SparkLoadJob.class);\n+\n+    // for global dict\n+    public static final String BITMAP_DATA_PROPERTY = \"bitmap_data\";\n+\n+    // --- members below need persist ---\n+    // create from resourceDesc when job created\n+    private SparkResource sparkResource;\n+    // members below updated when job state changed to etl\n+    private long etlStartTimestamp = -1;\n+    // for spark yarn\n+    private String appId = \"\";\n+    // spark job outputPath\n+    private String etlOutputPath = \"\";\n+    // members below updated when job state changed to loading\n+    // { tableId.partitionId.indexId.bucket.schemaHash -> (etlFilePath, etlFileSize) }\n+    private Map<String, Pair<String, Long>> tabletMetaToFileInfo = Maps.newHashMap();\n+\n+    // --- members below not persist ---\n+    // temporary use\n+    // one SparkLoadJob has only one table to load\n+    // hivedb.table for global dict\n+    private String hiveTableName = \"\";\n+    private ResourceDesc resourceDesc;\n+    // for spark standalone\n+    private SparkAppHandle sparkAppHandle;\n+    // for straggler wait long time to commit transaction\n+    private long quorumFinishTimestamp = -1;\n+    // below for push task\n+    private Map<Long, Set<Long>> tableToLoadPartitions = Maps.newHashMap();\n+    private Map<Long, PushBrokerScannerParams> indexToPushBrokerReaderParams = Maps.newHashMap();\n+    private Map<Long, Integer> indexToSchemaHash = Maps.newHashMap();\n+    private Map<Long, Map<Long, PushTask>> tabletToSentReplicaPushTask = Maps.newHashMap();\n+    private Set<Long> finishedReplicas = Sets.newHashSet();\n+    private Set<Long> quorumTablets = Sets.newHashSet();\n+    private Set<Long> fullTablets = Sets.newHashSet();\n+\n+    private static class PushBrokerScannerParams {\n+        TBrokerScanRange tBrokerScanRange;\n+        TDescriptorTable tDescriptorTable;\n+\n+        public void init(List<Column> columns, BrokerDesc brokerDesc) throws UserException {\n+            Analyzer analyzer = new Analyzer(null, null);\n+            // Generate tuple descriptor\n+            DescriptorTable descTable = analyzer.getDescTbl();\n+            TupleDescriptor destTupleDesc = descTable.createTupleDescriptor();\n+            // use index schema to fill the descriptor table\n+            for (Column column : columns) {\n+                SlotDescriptor destSlotDesc = descTable.addSlotDescriptor(destTupleDesc);\n+                destSlotDesc.setIsMaterialized(true);\n+                destSlotDesc.setColumn(column);\n+                if (column.isAllowNull()) {\n+                    destSlotDesc.setIsNullable(true);\n+                } else {\n+                    destSlotDesc.setIsNullable(false);\n+                }\n+            }\n+            // Push broker scan node\n+            PushBrokerScanNode scanNode = new PushBrokerScanNode(destTupleDesc);", "originalCommit": "ed946d2f9cf0c62c73ab0d9d67b9625fd21a1182", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgzNTI5NA==", "url": "https://github.com/apache/incubator-doris/pull/3717#discussion_r432835294", "bodyText": "When is this assigned?", "author": "imay", "createdAt": "2020-05-30T11:51:04Z", "path": "fe/src/main/java/org/apache/doris/load/loadv2/SparkLoadJob.java", "diffHunk": "@@ -0,0 +1,700 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import org.apache.doris.analysis.Analyzer;\n+import org.apache.doris.analysis.BrokerDesc;\n+import org.apache.doris.analysis.CastExpr;\n+import org.apache.doris.analysis.DescriptorTable;\n+import org.apache.doris.analysis.ResourceDesc;\n+import org.apache.doris.analysis.Expr;\n+import org.apache.doris.analysis.SlotDescriptor;\n+import org.apache.doris.analysis.SlotRef;\n+import org.apache.doris.analysis.TupleDescriptor;\n+import org.apache.doris.catalog.Catalog;\n+import org.apache.doris.catalog.Column;\n+import org.apache.doris.catalog.Database;\n+import org.apache.doris.catalog.FsBroker;\n+import org.apache.doris.catalog.MaterializedIndex;\n+import org.apache.doris.catalog.MaterializedIndex.IndexExtState;\n+import org.apache.doris.catalog.OlapTable;\n+import org.apache.doris.catalog.Partition;\n+import org.apache.doris.catalog.PrimitiveType;\n+import org.apache.doris.catalog.Replica;\n+import org.apache.doris.catalog.ScalarType;\n+import org.apache.doris.catalog.SparkResource;\n+import org.apache.doris.catalog.Tablet;\n+import org.apache.doris.catalog.Type;\n+import org.apache.doris.common.Config;\n+import org.apache.doris.common.LoadException;\n+import org.apache.doris.common.MetaNotFoundException;\n+import org.apache.doris.common.Pair;\n+import org.apache.doris.common.UserException;\n+import org.apache.doris.common.util.LogBuilder;\n+import org.apache.doris.common.util.LogKey;\n+import org.apache.doris.load.EtlJobType;\n+import org.apache.doris.load.EtlStatus;\n+import org.apache.doris.load.FailMsg;\n+import org.apache.doris.load.loadv2.etl.EtlJobConfig;\n+import org.apache.doris.planner.PlanNodeId;\n+import org.apache.doris.planner.ScanNode;\n+import org.apache.doris.qe.OriginStatement;\n+import org.apache.doris.system.Backend;\n+import org.apache.doris.task.AgentBatchTask;\n+import org.apache.doris.task.AgentTaskExecutor;\n+import org.apache.doris.task.AgentTaskQueue;\n+import org.apache.doris.task.PushTask;\n+import org.apache.doris.thrift.TBrokerRangeDesc;\n+import org.apache.doris.thrift.TBrokerScanRange;\n+import org.apache.doris.thrift.TBrokerScanRangeParams;\n+import org.apache.doris.thrift.TDescriptorTable;\n+import org.apache.doris.thrift.TFileFormatType;\n+import org.apache.doris.thrift.TFileType;\n+import org.apache.doris.thrift.TNetworkAddress;\n+import org.apache.doris.thrift.TPlanNode;\n+import org.apache.doris.thrift.TPriority;\n+import org.apache.doris.thrift.TPushType;\n+import org.apache.doris.thrift.TScanRangeLocations;\n+import org.apache.doris.transaction.TabletCommitInfo;\n+import org.apache.doris.transaction.TabletQuorumFailedException;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.spark.launcher.SparkAppHandle;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+import com.google.gson.annotations.SerializedName;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * There are 4 steps in SparkLoadJob:\n+ * Step1: SparkLoadPendingTask will be created by unprotectedExecuteJob method and submit spark etl job.\n+ * Step2: LoadEtlChecker will check spark etl job status periodly and send push tasks to be when spark etl job is finished.\n+ * Step3: LoadLoadingChecker will check loading status periodly and commit transaction when push tasks are finished.\n+ * Step4: PublishVersionDaemon will send publish version tasks to be and finish transaction.\n+ */\n+public class SparkLoadJob extends BulkLoadJob {\n+    private static final Logger LOG = LogManager.getLogger(SparkLoadJob.class);\n+\n+    // for global dict\n+    public static final String BITMAP_DATA_PROPERTY = \"bitmap_data\";\n+\n+    // --- members below need persist ---\n+    // create from resourceDesc when job created\n+    private SparkResource sparkResource;\n+    // members below updated when job state changed to etl\n+    private long etlStartTimestamp = -1;\n+    // for spark yarn\n+    private String appId = \"\";\n+    // spark job outputPath\n+    private String etlOutputPath = \"\";\n+    // members below updated when job state changed to loading\n+    // { tableId.partitionId.indexId.bucket.schemaHash -> (etlFilePath, etlFileSize) }\n+    private Map<String, Pair<String, Long>> tabletMetaToFileInfo = Maps.newHashMap();\n+\n+    // --- members below not persist ---\n+    // temporary use\n+    // one SparkLoadJob has only one table to load\n+    // hivedb.table for global dict\n+    private String hiveTableName = \"\";\n+    private ResourceDesc resourceDesc;\n+    // for spark standalone\n+    private SparkAppHandle sparkAppHandle;", "originalCommit": "ed946d2f9cf0c62c73ab0d9d67b9625fd21a1182", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzk4NjAxOQ==", "url": "https://github.com/apache/incubator-doris/pull/3717#discussion_r433986019", "bodyText": "setJobProperties in SparkLoadJob.\nThis is for temporary use. I am investigating load from hive table, and i will update it recently.", "author": "wyb", "createdAt": "2020-06-02T15:53:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgzNTI5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgzNTY0OQ==", "url": "https://github.com/apache/incubator-doris/pull/3717#discussion_r432835649", "bodyText": "why not get lock in the outer public function? In most scenario, there is only one writer.\nIf you put this lock in a private function, there is a case that multiple threads will do proceeEtlFinish concurrently.", "author": "imay", "createdAt": "2020-05-30T11:56:52Z", "path": "fe/src/main/java/org/apache/doris/load/loadv2/SparkLoadJob.java", "diffHunk": "@@ -0,0 +1,700 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import org.apache.doris.analysis.Analyzer;\n+import org.apache.doris.analysis.BrokerDesc;\n+import org.apache.doris.analysis.CastExpr;\n+import org.apache.doris.analysis.DescriptorTable;\n+import org.apache.doris.analysis.ResourceDesc;\n+import org.apache.doris.analysis.Expr;\n+import org.apache.doris.analysis.SlotDescriptor;\n+import org.apache.doris.analysis.SlotRef;\n+import org.apache.doris.analysis.TupleDescriptor;\n+import org.apache.doris.catalog.Catalog;\n+import org.apache.doris.catalog.Column;\n+import org.apache.doris.catalog.Database;\n+import org.apache.doris.catalog.FsBroker;\n+import org.apache.doris.catalog.MaterializedIndex;\n+import org.apache.doris.catalog.MaterializedIndex.IndexExtState;\n+import org.apache.doris.catalog.OlapTable;\n+import org.apache.doris.catalog.Partition;\n+import org.apache.doris.catalog.PrimitiveType;\n+import org.apache.doris.catalog.Replica;\n+import org.apache.doris.catalog.ScalarType;\n+import org.apache.doris.catalog.SparkResource;\n+import org.apache.doris.catalog.Tablet;\n+import org.apache.doris.catalog.Type;\n+import org.apache.doris.common.Config;\n+import org.apache.doris.common.LoadException;\n+import org.apache.doris.common.MetaNotFoundException;\n+import org.apache.doris.common.Pair;\n+import org.apache.doris.common.UserException;\n+import org.apache.doris.common.util.LogBuilder;\n+import org.apache.doris.common.util.LogKey;\n+import org.apache.doris.load.EtlJobType;\n+import org.apache.doris.load.EtlStatus;\n+import org.apache.doris.load.FailMsg;\n+import org.apache.doris.load.loadv2.etl.EtlJobConfig;\n+import org.apache.doris.planner.PlanNodeId;\n+import org.apache.doris.planner.ScanNode;\n+import org.apache.doris.qe.OriginStatement;\n+import org.apache.doris.system.Backend;\n+import org.apache.doris.task.AgentBatchTask;\n+import org.apache.doris.task.AgentTaskExecutor;\n+import org.apache.doris.task.AgentTaskQueue;\n+import org.apache.doris.task.PushTask;\n+import org.apache.doris.thrift.TBrokerRangeDesc;\n+import org.apache.doris.thrift.TBrokerScanRange;\n+import org.apache.doris.thrift.TBrokerScanRangeParams;\n+import org.apache.doris.thrift.TDescriptorTable;\n+import org.apache.doris.thrift.TFileFormatType;\n+import org.apache.doris.thrift.TFileType;\n+import org.apache.doris.thrift.TNetworkAddress;\n+import org.apache.doris.thrift.TPlanNode;\n+import org.apache.doris.thrift.TPriority;\n+import org.apache.doris.thrift.TPushType;\n+import org.apache.doris.thrift.TScanRangeLocations;\n+import org.apache.doris.transaction.TabletCommitInfo;\n+import org.apache.doris.transaction.TabletQuorumFailedException;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.spark.launcher.SparkAppHandle;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import com.google.common.collect.Sets;\n+import com.google.gson.annotations.SerializedName;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * There are 4 steps in SparkLoadJob:\n+ * Step1: SparkLoadPendingTask will be created by unprotectedExecuteJob method and submit spark etl job.\n+ * Step2: LoadEtlChecker will check spark etl job status periodly and send push tasks to be when spark etl job is finished.\n+ * Step3: LoadLoadingChecker will check loading status periodly and commit transaction when push tasks are finished.\n+ * Step4: PublishVersionDaemon will send publish version tasks to be and finish transaction.\n+ */\n+public class SparkLoadJob extends BulkLoadJob {\n+    private static final Logger LOG = LogManager.getLogger(SparkLoadJob.class);\n+\n+    // for global dict\n+    public static final String BITMAP_DATA_PROPERTY = \"bitmap_data\";\n+\n+    // --- members below need persist ---\n+    // create from resourceDesc when job created\n+    private SparkResource sparkResource;\n+    // members below updated when job state changed to etl\n+    private long etlStartTimestamp = -1;\n+    // for spark yarn\n+    private String appId = \"\";\n+    // spark job outputPath\n+    private String etlOutputPath = \"\";\n+    // members below updated when job state changed to loading\n+    // { tableId.partitionId.indexId.bucket.schemaHash -> (etlFilePath, etlFileSize) }\n+    private Map<String, Pair<String, Long>> tabletMetaToFileInfo = Maps.newHashMap();\n+\n+    // --- members below not persist ---\n+    // temporary use\n+    // one SparkLoadJob has only one table to load\n+    // hivedb.table for global dict\n+    private String hiveTableName = \"\";\n+    private ResourceDesc resourceDesc;\n+    // for spark standalone\n+    private SparkAppHandle sparkAppHandle;\n+    // for straggler wait long time to commit transaction\n+    private long quorumFinishTimestamp = -1;\n+    // below for push task\n+    private Map<Long, Set<Long>> tableToLoadPartitions = Maps.newHashMap();\n+    private Map<Long, PushBrokerScannerParams> indexToPushBrokerReaderParams = Maps.newHashMap();\n+    private Map<Long, Integer> indexToSchemaHash = Maps.newHashMap();\n+    private Map<Long, Map<Long, PushTask>> tabletToSentReplicaPushTask = Maps.newHashMap();\n+    private Set<Long> finishedReplicas = Sets.newHashSet();\n+    private Set<Long> quorumTablets = Sets.newHashSet();\n+    private Set<Long> fullTablets = Sets.newHashSet();\n+\n+    private static class PushBrokerScannerParams {\n+        TBrokerScanRange tBrokerScanRange;\n+        TDescriptorTable tDescriptorTable;\n+\n+        public void init(List<Column> columns, BrokerDesc brokerDesc) throws UserException {\n+            Analyzer analyzer = new Analyzer(null, null);\n+            // Generate tuple descriptor\n+            DescriptorTable descTable = analyzer.getDescTbl();\n+            TupleDescriptor destTupleDesc = descTable.createTupleDescriptor();\n+            // use index schema to fill the descriptor table\n+            for (Column column : columns) {\n+                SlotDescriptor destSlotDesc = descTable.addSlotDescriptor(destTupleDesc);\n+                destSlotDesc.setIsMaterialized(true);\n+                destSlotDesc.setColumn(column);\n+                if (column.isAllowNull()) {\n+                    destSlotDesc.setIsNullable(true);\n+                } else {\n+                    destSlotDesc.setIsNullable(false);\n+                }\n+            }\n+            // Push broker scan node\n+            PushBrokerScanNode scanNode = new PushBrokerScanNode(destTupleDesc);\n+            scanNode.setLoadInfo(columns, brokerDesc);\n+            scanNode.init(analyzer);\n+            tBrokerScanRange = scanNode.getTBrokerScanRange();\n+\n+            // descTable\n+            descTable.computeMemLayout();\n+            tDescriptorTable = descTable.toThrift();\n+        }\n+    }\n+\n+    private static class PushBrokerScanNode extends ScanNode {\n+        private TBrokerScanRange tBrokerScanRange;\n+        private List<Column> columns;\n+        private BrokerDesc brokerDesc;\n+\n+        public PushBrokerScanNode(TupleDescriptor destTupleDesc) {\n+            super(new PlanNodeId(0), destTupleDesc, \"PushBrokerScanNode\");\n+            this.tBrokerScanRange = new TBrokerScanRange();\n+        }\n+\n+        public void setLoadInfo(List<Column> columns, BrokerDesc brokerDesc) {\n+            this.columns = columns;\n+            this.brokerDesc = brokerDesc;\n+        }\n+\n+        public void init(Analyzer analyzer) throws UserException {\n+            super.init(analyzer);\n+\n+            // scan range params\n+            TBrokerScanRangeParams params = new TBrokerScanRangeParams();\n+            params.setStrict_mode(false);\n+            params.setProperties(brokerDesc.getProperties());\n+            TupleDescriptor srcTupleDesc = analyzer.getDescTbl().createTupleDescriptor();\n+            Map<String, SlotDescriptor> srcSlotDescByName = Maps.newHashMap();\n+            for (Column column : columns) {\n+                SlotDescriptor srcSlotDesc = analyzer.getDescTbl().addSlotDescriptor(srcTupleDesc);\n+                srcSlotDesc.setType(ScalarType.createType(PrimitiveType.VARCHAR));\n+                srcSlotDesc.setIsMaterialized(true);\n+                srcSlotDesc.setIsNullable(true);\n+                srcSlotDesc.setColumn(new Column(column.getName(), PrimitiveType.VARCHAR));\n+                params.addToSrc_slot_ids(srcSlotDesc.getId().asInt());\n+                srcSlotDescByName.put(column.getName(), srcSlotDesc);\n+            }\n+\n+            TupleDescriptor destTupleDesc = desc;\n+            Map<Integer, Integer> destSidToSrcSidWithoutTrans = Maps.newHashMap();\n+            for (SlotDescriptor destSlotDesc : destTupleDesc.getSlots()) {\n+                if (!destSlotDesc.isMaterialized()) {\n+                    continue;\n+                }\n+\n+                SlotDescriptor srcSlotDesc = srcSlotDescByName.get(destSlotDesc.getColumn().getName());\n+                destSidToSrcSidWithoutTrans.put(destSlotDesc.getId().asInt(), srcSlotDesc.getId().asInt());\n+                Expr expr = new SlotRef(srcSlotDesc);\n+                if (destSlotDesc.getType().getPrimitiveType() == PrimitiveType.BOOLEAN) {\n+                    // there is no cast string to boolean function\n+                    // so we cast string to tinyint first, then cast tinyint to boolean\n+                    expr = new CastExpr(Type.BOOLEAN, new CastExpr(Type.TINYINT, expr));\n+                } else {\n+                    expr = castToSlot(destSlotDesc, expr);\n+                }\n+                params.putToExpr_of_dest_slot(destSlotDesc.getId().asInt(), expr.treeToThrift());\n+            }\n+            params.setDest_sid_to_src_sid_without_trans(destSidToSrcSidWithoutTrans);\n+            params.setSrc_tuple_id(srcTupleDesc.getId().asInt());\n+            params.setDest_tuple_id(destTupleDesc.getId().asInt());\n+            tBrokerScanRange.setParams(params);\n+\n+            // broker address updated for each replica\n+            tBrokerScanRange.setBroker_addresses(Lists.newArrayList());\n+\n+            // broker range desc\n+            TBrokerRangeDesc tBrokerRangeDesc = new TBrokerRangeDesc();\n+            tBrokerRangeDesc.setFile_type(TFileType.FILE_BROKER);\n+            tBrokerRangeDesc.setFormat_type(TFileFormatType.FORMAT_PARQUET);\n+            tBrokerRangeDesc.setSplittable(false);\n+            tBrokerRangeDesc.setStart_offset(0);\n+            tBrokerRangeDesc.setSize(-1);\n+            // path and file size updated for each replica\n+            tBrokerScanRange.setRanges(Lists.newArrayList(tBrokerRangeDesc));\n+        }\n+\n+        public TBrokerScanRange getTBrokerScanRange() {\n+            return tBrokerScanRange;\n+        }\n+\n+        @Override\n+        public List<TScanRangeLocations> getScanRangeLocations(long maxScanRangeLength) {\n+            return null;\n+        }\n+\n+        @Override\n+        protected void toThrift(TPlanNode msg) {}\n+    }\n+\n+    // only for log replay\n+    public SparkLoadJob() {\n+        super();\n+        jobType = EtlJobType.SPARK;\n+    }\n+\n+    public SparkLoadJob(long dbId, String label, ResourceDesc resourceDesc, OriginStatement originStmt)\n+            throws MetaNotFoundException {\n+        super(dbId, label, originStmt);\n+        this.resourceDesc = resourceDesc;\n+        timeoutSecond = Config.spark_load_default_timeout_second;\n+        jobType = EtlJobType.SPARK;\n+    }\n+\n+    private boolean checkState(JobState expectState) {\n+        readLock();\n+        try {\n+            if (state == expectState) {\n+                return true;\n+            }\n+            return false;\n+        } finally {\n+            readUnlock();\n+        }\n+    }\n+\n+    public void updateEtlStatus() throws Exception {\n+        if (!checkState(JobState.ETL)) {\n+            return;\n+        }\n+\n+        // get etl status\n+        SparkEtlJobHandler handler = new SparkEtlJobHandler();\n+        EtlStatus status = handler.getEtlJobStatus(sparkAppHandle, appId, id, etlOutputPath, sparkResource, brokerDesc);\n+        switch (status.getState()) {\n+            case RUNNING:\n+                updateEtlStatusInternal(status);\n+                break;\n+            case FINISHED:\n+                processEtlFinish(status, handler);\n+                break;\n+            case CANCELLED:\n+                throw new LoadException(\"spark etl job failed. msg: \" + status.getFailMsg());\n+            default:\n+                LOG.warn(\"unknown etl state: {}\", status.getState().name());\n+                break;\n+        }\n+    }\n+\n+    private void updateEtlStatusInternal(EtlStatus etlStatus) {\n+        writeLock();\n+        try {\n+            loadingStatus = etlStatus;\n+            progress = etlStatus.getProgress();\n+            if (!sparkResource.isYarnMaster()) {\n+                loadingStatus.setTrackingUrl(appId);\n+            }\n+\n+            // TODO(wyb): spark-load\n+            /*\n+            DppResult dppResult = etlStatus.getDppResult();\n+            if (dppResult != null) {\n+                // update load statistic and counters when spark etl job finished\n+                // fe gets these infos from spark dpp, so we use dummy load id and dummy backend id here\n+                loadStatistic.fileNum = (int) dppResult.fileNumber;\n+                loadStatistic.totalFileSizeB = dppResult.fileSize;\n+                TUniqueId dummyId = new TUniqueId(0, 0);\n+                long dummyBackendId = -1L;\n+                loadStatistic.initLoad(dummyId, Sets.newHashSet(dummyId), Lists.newArrayList(dummyBackendId));\n+                loadStatistic.updateLoadProgress(dummyBackendId, dummyId, dummyId, dppResult.scannedRows, true);\n+\n+                Map<String, String> counters = loadingStatus.getCounters();\n+                counters.put(DPP_NORMAL_ALL, String.valueOf(dppResult.normalRows));\n+                counters.put(DPP_ABNORMAL_ALL, String.valueOf(dppResult.abnormalRows));\n+                counters.put(UNSELECTED_ROWS, String.valueOf(dppResult.unselectRows));\n+            }\n+            */\n+        } finally {\n+            writeUnlock();\n+        }\n+    }\n+\n+    private void processEtlFinish(EtlStatus etlStatus, SparkEtlJobHandler handler) throws Exception {\n+        updateEtlStatusInternal(etlStatus);\n+        // checkDataQuality\n+        if (!checkDataQuality()) {\n+            cancelJobWithoutCheck(new FailMsg(FailMsg.CancelType.ETL_QUALITY_UNSATISFIED, QUALITY_FAIL_MSG),\n+                                  true, true);\n+            return;\n+        }\n+\n+        // get etl output files and update loading state\n+        updateToLoadingState(etlStatus, handler.getEtlFilePaths(etlOutputPath, brokerDesc));\n+        // log loading state\n+        logUpdateStateInfo();\n+\n+        // create push tasks\n+        prepareLoadingInfos();\n+        submitPushTasks();\n+    }\n+\n+    private void updateToLoadingState(EtlStatus etlStatus, Map<String, Long> filePathToSize) throws LoadException {\n+        writeLock();", "originalCommit": "ed946d2f9cf0c62c73ab0d9d67b9625fd21a1182", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b0aaf1ef45b058c74b42ddc450bcb4f8dcb0ab11", "url": "https://github.com/apache/incubator-doris/commit/b0aaf1ef45b058c74b42ddc450bcb4f8dcb0ab11", "message": "Fe process etl and loading state job", "committedDate": "2020-06-19T10:40:45Z", "type": "forcePushed"}, {"oid": "1da49652c852457e44e425ded669c2d95a05dfbc", "url": "https://github.com/apache/incubator-doris/commit/1da49652c852457e44e425ded669c2d95a05dfbc", "message": "Fe process etl and loading state job", "committedDate": "2020-06-19T11:06:27Z", "type": "commit"}, {"oid": "1da49652c852457e44e425ded669c2d95a05dfbc", "url": "https://github.com/apache/incubator-doris/commit/1da49652c852457e44e425ded669c2d95a05dfbc", "message": "Fe process etl and loading state job", "committedDate": "2020-06-19T11:06:27Z", "type": "forcePushed"}, {"oid": "a2e7e1280631fe642a5c8975de4d031d2b43f4b6", "url": "https://github.com/apache/incubator-doris/commit/a2e7e1280631fe642a5c8975de4d031d2b43f4b6", "message": "Add missing class import", "committedDate": "2020-06-19T11:16:19Z", "type": "commit"}, {"oid": "c57352ee8768da107dd48c2883f9ddbb9b8cc697", "url": "https://github.com/apache/incubator-doris/commit/c57352ee8768da107dd48c2883f9ddbb9b8cc697", "message": "Fix ut", "committedDate": "2020-06-19T15:53:10Z", "type": "commit"}]}