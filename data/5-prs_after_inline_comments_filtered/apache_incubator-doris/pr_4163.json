{"pr_number": 4163, "pr_title": "[Spark Load]Create spark load's repository in HDFS for dependencies", "pr_createdAt": "2020-07-23T13:01:52Z", "pr_url": "https://github.com/apache/incubator-doris/pull/4163", "timeline": [{"oid": "b74ca33e8cc3a2263fb17240cabd3c9aa576119f", "url": "https://github.com/apache/incubator-doris/commit/b74ca33e8cc3a2263fb17240cabd3c9aa576119f", "message": "save code", "committedDate": "2020-07-23T09:43:16Z", "type": "commit"}, {"oid": "1c9f256148d59fd146cf26f705770db6eb96e777", "url": "https://github.com/apache/incubator-doris/commit/1c9f256148d59fd146cf26f705770db6eb96e777", "message": "save code", "committedDate": "2020-07-23T09:43:16Z", "type": "commit"}, {"oid": "3a777e1fcf3fe83f24201106af1091a992005797", "url": "https://github.com/apache/incubator-doris/commit/3a777e1fcf3fe83f24201106af1091a992005797", "message": "save code", "committedDate": "2020-07-23T09:43:16Z", "type": "commit"}, {"oid": "56f8c9b8b35cb069b4e3b0819d80737a72448153", "url": "https://github.com/apache/incubator-doris/commit/56f8c9b8b35cb069b4e3b0819d80737a72448153", "message": "save code", "committedDate": "2020-07-23T09:43:16Z", "type": "commit"}, {"oid": "d2be4f2c20057773825fc14a7e38c3fecf7b51e0", "url": "https://github.com/apache/incubator-doris/commit/d2be4f2c20057773825fc14a7e38c3fecf7b51e0", "message": "remove log", "committedDate": "2020-07-23T13:21:15Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQ0Mjc5NA==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r459442794", "bodyText": "Better to move it to the Config.java", "author": "morningman", "createdAt": "2020-07-23T13:20:17Z", "path": "fe/fe-core/src/main/java/org/apache/doris/common/FeConstants.java", "diffHunk": "@@ -39,6 +39,9 @@\n     // dpp version\n     public static String dpp_version = \"3_2_0\";\n \n+    // spark load version\n+    public static String spark_dpp_version = \"1_0_0\";", "originalCommit": "56f8c9b8b35cb069b4e3b0819d80737a72448153", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDA4MzQ4NA==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r460083484", "bodyText": "done", "author": "xy720", "createdAt": "2020-07-24T14:21:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQ0Mjc5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQ0NTk5NA==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r459445994", "bodyText": "Make /spark-dpp/spark-dpp.jar as a static final string", "author": "morningman", "createdAt": "2020-07-23T13:24:57Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkRepository.java", "diffHunk": "@@ -0,0 +1,379 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import org.apache.doris.PaloFe;\n+import org.apache.doris.analysis.BrokerDesc;\n+import org.apache.doris.common.Config;\n+import org.apache.doris.common.FeConstants;\n+import org.apache.doris.common.LoadException;\n+import org.apache.doris.common.UserException;\n+import org.apache.doris.common.util.BrokerUtil;\n+import org.apache.doris.thrift.TBrokerFileStatus;\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.Lists;\n+\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+\n+/*\n+ * SparkRepository represents the remote repository for spark archives uploaded by spark\n+ * The organization in repository is:\n+ *\n+ * * __spark_repository__/\n+ *   * __archive_1_0_0/\n+ *     * __lib_990325d2c0d1d5e45bf675e54e44fb16_spark-dpp.jar\n+ *     * __lib_7670c29daf535efe3c9b923f778f61fc_spark-2x.zip\n+ *   * __archive_2_2_0/\n+ *     * __lib_64d5696f99c379af2bee28c1c84271d5_spark-dpp.jar\n+ *     * __lib_1bbb74bb6b264a270bc7fca3e964160f_spark-2x.zip\n+ *   * __archive_3_2_0/\n+ *     * ...\n+ */\n+public class SparkRepository {\n+    private static final Logger LOG = LogManager.getLogger(SparkRepository.class);\n+\n+    public static final String REPOSITORY_DIR = \"__spark_repository__\";\n+    public static final String PREFIX_ARCHIVE = \"__archive_\";\n+    public static final String PREFIX_LIB = \"__lib_\";\n+    public static final String SPARK_DPP = \"spark-dpp\";\n+    public static final String SPARK_2X = \"spark-2x\";\n+    public static final String SUFFIX = \".zip\";\n+\n+    private static final String PATH_DELIMITER = \"/\";\n+    private static final String FILE_NAME_SEPARATOR = \"_\";\n+\n+    private String remoteRepositoryPath;\n+    private BrokerDesc brokerDesc;\n+\n+    private String localDppPath;\n+    private String localSpark2xPath;\n+\n+    // Version of the spark dpp program in this cluster\n+    private String currentDppVersion;\n+    // Archive that current dpp version pointed to\n+    private SparkArchive currentArchive;\n+\n+    private ReentrantReadWriteLock rwLock;\n+    private boolean isInit;\n+\n+    public SparkRepository(String remoteRepositoryPath, BrokerDesc brokerDesc) {\n+        this.remoteRepositoryPath = remoteRepositoryPath;\n+        this.brokerDesc = brokerDesc;\n+        this.currentDppVersion = FeConstants.spark_dpp_version;\n+        currentArchive = new SparkArchive(getRemoteArchivePath(currentDppVersion), currentDppVersion);\n+        this.rwLock = new ReentrantReadWriteLock();\n+        this.isInit = false;\n+        this.localDppPath = PaloFe.DORIS_HOME_DIR + \"/spark-dpp/spark-dpp.jar\";", "originalCommit": "56f8c9b8b35cb069b4e3b0819d80737a72448153", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQ0ODYwMw==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r459448603", "bodyText": "What is this for?", "author": "morningman", "createdAt": "2020-07-23T13:28:33Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java", "diffHunk": "@@ -114,7 +142,9 @@ public void submitEtlJob(long loadJobId, String loadLabel, EtlJobConfig etlJobCo\n                 .setAppResource(appResourceHdfsPath)\n                 .setMainClass(SparkEtlJob.class.getCanonicalName())\n                 .setAppName(String.format(ETL_JOB_NAME, loadLabel))\n+                .setSparkHome(spark_home)", "originalCommit": "56f8c9b8b35cb069b4e3b0819d80737a72448153", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDA4NDE4Mg==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r460084182", "bodyText": "The SparkHome argument of sparkLaucher", "author": "xy720", "createdAt": "2020-07-24T14:22:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQ0ODYwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQ1NTQ2OA==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r459455468", "bodyText": "the whole initRepository need to be lock protected", "author": "morningman", "createdAt": "2020-07-23T13:38:55Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkRepository.java", "diffHunk": "@@ -0,0 +1,379 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import org.apache.doris.PaloFe;\n+import org.apache.doris.analysis.BrokerDesc;\n+import org.apache.doris.common.Config;\n+import org.apache.doris.common.FeConstants;\n+import org.apache.doris.common.LoadException;\n+import org.apache.doris.common.UserException;\n+import org.apache.doris.common.util.BrokerUtil;\n+import org.apache.doris.thrift.TBrokerFileStatus;\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.Lists;\n+\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+\n+/*\n+ * SparkRepository represents the remote repository for spark archives uploaded by spark\n+ * The organization in repository is:\n+ *\n+ * * __spark_repository__/\n+ *   * __archive_1_0_0/\n+ *     * __lib_990325d2c0d1d5e45bf675e54e44fb16_spark-dpp.jar\n+ *     * __lib_7670c29daf535efe3c9b923f778f61fc_spark-2x.zip\n+ *   * __archive_2_2_0/\n+ *     * __lib_64d5696f99c379af2bee28c1c84271d5_spark-dpp.jar\n+ *     * __lib_1bbb74bb6b264a270bc7fca3e964160f_spark-2x.zip\n+ *   * __archive_3_2_0/\n+ *     * ...\n+ */\n+public class SparkRepository {\n+    private static final Logger LOG = LogManager.getLogger(SparkRepository.class);\n+\n+    public static final String REPOSITORY_DIR = \"__spark_repository__\";\n+    public static final String PREFIX_ARCHIVE = \"__archive_\";\n+    public static final String PREFIX_LIB = \"__lib_\";\n+    public static final String SPARK_DPP = \"spark-dpp\";\n+    public static final String SPARK_2X = \"spark-2x\";\n+    public static final String SUFFIX = \".zip\";\n+\n+    private static final String PATH_DELIMITER = \"/\";\n+    private static final String FILE_NAME_SEPARATOR = \"_\";\n+\n+    private String remoteRepositoryPath;\n+    private BrokerDesc brokerDesc;\n+\n+    private String localDppPath;\n+    private String localSpark2xPath;\n+\n+    // Version of the spark dpp program in this cluster\n+    private String currentDppVersion;\n+    // Archive that current dpp version pointed to\n+    private SparkArchive currentArchive;\n+\n+    private ReentrantReadWriteLock rwLock;\n+    private boolean isInit;\n+\n+    public SparkRepository(String remoteRepositoryPath, BrokerDesc brokerDesc) {\n+        this.remoteRepositoryPath = remoteRepositoryPath;\n+        this.brokerDesc = brokerDesc;\n+        this.currentDppVersion = FeConstants.spark_dpp_version;\n+        currentArchive = new SparkArchive(getRemoteArchivePath(currentDppVersion), currentDppVersion);\n+        this.rwLock = new ReentrantReadWriteLock();\n+        this.isInit = false;\n+        this.localDppPath = PaloFe.DORIS_HOME_DIR + \"/spark-dpp/spark-dpp.jar\";\n+        if (!Strings.isNullOrEmpty(Config.spark_resource_path)) {\n+            this.localSpark2xPath = Config.spark_resource_path;\n+        } else {\n+            this.localSpark2xPath = Config.spark_home_default_dir + \"/jars/spark-2x.zip\";\n+        }\n+    }\n+\n+    public boolean prepare() throws LoadException {\n+        if (!isInit) {\n+            initRepository();\n+        }\n+        return isInit;\n+    }\n+\n+    private void initRepository() throws LoadException {\n+        LOG.info(\"start to init remote repository\");", "originalCommit": "56f8c9b8b35cb069b4e3b0819d80737a72448153", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDA4NzA2NA==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r460087064", "bodyText": "Added a synchronized lock in SparkEtlHandler for cluster id.\nNow initRepository operations are protected by lock if they are in same cluster", "author": "xy720", "createdAt": "2020-07-24T14:26:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQ1NTQ2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQ1NjA4NQ==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r459456085", "bodyText": "debug", "author": "morningman", "createdAt": "2020-07-23T13:39:47Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkRepository.java", "diffHunk": "@@ -0,0 +1,379 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import org.apache.doris.PaloFe;\n+import org.apache.doris.analysis.BrokerDesc;\n+import org.apache.doris.common.Config;\n+import org.apache.doris.common.FeConstants;\n+import org.apache.doris.common.LoadException;\n+import org.apache.doris.common.UserException;\n+import org.apache.doris.common.util.BrokerUtil;\n+import org.apache.doris.thrift.TBrokerFileStatus;\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.Lists;\n+\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+\n+/*\n+ * SparkRepository represents the remote repository for spark archives uploaded by spark\n+ * The organization in repository is:\n+ *\n+ * * __spark_repository__/\n+ *   * __archive_1_0_0/\n+ *     * __lib_990325d2c0d1d5e45bf675e54e44fb16_spark-dpp.jar\n+ *     * __lib_7670c29daf535efe3c9b923f778f61fc_spark-2x.zip\n+ *   * __archive_2_2_0/\n+ *     * __lib_64d5696f99c379af2bee28c1c84271d5_spark-dpp.jar\n+ *     * __lib_1bbb74bb6b264a270bc7fca3e964160f_spark-2x.zip\n+ *   * __archive_3_2_0/\n+ *     * ...\n+ */\n+public class SparkRepository {\n+    private static final Logger LOG = LogManager.getLogger(SparkRepository.class);\n+\n+    public static final String REPOSITORY_DIR = \"__spark_repository__\";\n+    public static final String PREFIX_ARCHIVE = \"__archive_\";\n+    public static final String PREFIX_LIB = \"__lib_\";\n+    public static final String SPARK_DPP = \"spark-dpp\";\n+    public static final String SPARK_2X = \"spark-2x\";\n+    public static final String SUFFIX = \".zip\";\n+\n+    private static final String PATH_DELIMITER = \"/\";\n+    private static final String FILE_NAME_SEPARATOR = \"_\";\n+\n+    private String remoteRepositoryPath;\n+    private BrokerDesc brokerDesc;\n+\n+    private String localDppPath;\n+    private String localSpark2xPath;\n+\n+    // Version of the spark dpp program in this cluster\n+    private String currentDppVersion;\n+    // Archive that current dpp version pointed to\n+    private SparkArchive currentArchive;\n+\n+    private ReentrantReadWriteLock rwLock;\n+    private boolean isInit;\n+\n+    public SparkRepository(String remoteRepositoryPath, BrokerDesc brokerDesc) {\n+        this.remoteRepositoryPath = remoteRepositoryPath;\n+        this.brokerDesc = brokerDesc;\n+        this.currentDppVersion = FeConstants.spark_dpp_version;\n+        currentArchive = new SparkArchive(getRemoteArchivePath(currentDppVersion), currentDppVersion);\n+        this.rwLock = new ReentrantReadWriteLock();\n+        this.isInit = false;\n+        this.localDppPath = PaloFe.DORIS_HOME_DIR + \"/spark-dpp/spark-dpp.jar\";\n+        if (!Strings.isNullOrEmpty(Config.spark_resource_path)) {\n+            this.localSpark2xPath = Config.spark_resource_path;\n+        } else {\n+            this.localSpark2xPath = Config.spark_home_default_dir + \"/jars/spark-2x.zip\";\n+        }\n+    }\n+\n+    public boolean prepare() throws LoadException {\n+        if (!isInit) {\n+            initRepository();\n+        }\n+        return isInit;\n+    }\n+\n+    private void initRepository() throws LoadException {\n+        LOG.info(\"start to init remote repository\");\n+        boolean needUpload = false;\n+        boolean needReplace = false;\n+        CHECK: {\n+            if (Strings.isNullOrEmpty(remoteRepositoryPath) || brokerDesc == null) {\n+                break CHECK;\n+            }\n+\n+            if (!checkCurrentArchiveExists()) {\n+                needUpload = true;\n+                break CHECK;\n+            }\n+\n+            // init current archive\n+            String remoteArchivePath = getRemoteArchivePath(currentDppVersion);\n+            List<SparkLibrary> libraries = Lists.newArrayList();\n+            getLibraries(remoteArchivePath, libraries);\n+            if (libraries.size() != 2) {\n+                needUpload = true;\n+                needReplace = true;\n+                break CHECK;\n+            }\n+            currentArchive.libraries.addAll(libraries);\n+            for (SparkLibrary library : currentArchive.libraries) {\n+                String localMd5sum = null;\n+                switch (library.libType) {\n+                    case DPP:\n+                        localMd5sum = getMd5String(localDppPath);\n+                        break;\n+                    case SPARK2X:\n+                        localMd5sum = getMd5String(localSpark2xPath);\n+                        break;\n+                    default:\n+                        Preconditions.checkState(false, \"wrong library type: \" + library.libType);\n+                        break;\n+                }\n+                if (!localMd5sum.equals(library.md5sum)) {\n+                    needUpload = true;\n+                    needReplace = true;\n+                    break;\n+                }\n+            }\n+        }\n+\n+        if (needUpload) {\n+            uploadArchive(needReplace);\n+        }\n+        isInit = true;\n+        LOG.info(\"init spark repository success, current dppVersion={}, archive path={}, libraries size={}\",\n+                currentDppVersion, currentArchive.remotePath, currentArchive.libraries.size());\n+    }\n+\n+    public boolean checkCurrentArchiveExists() {\n+        boolean result = false;\n+        Preconditions.checkNotNull(remoteRepositoryPath);\n+        String remotePath = getRemoteArchivePath(currentDppVersion);\n+        readLock();\n+        try {\n+            result = BrokerUtil.checkPathExist(remotePath, brokerDesc);\n+            LOG.info(\"check archive exists in repository, {}\", result);\n+        } catch (UserException e) {\n+            LOG.warn(\"Failed to check remote archive exist, path={}, version={}\", remotePath, currentDppVersion);\n+        } finally {\n+            readUnlock();\n+        }\n+        return result;\n+    }\n+\n+    private void uploadArchive(boolean isReplace) throws LoadException {\n+        writeLock();\n+        try {\n+            String remoteArchivePath = getRemoteArchivePath(currentDppVersion);\n+            if (isReplace) {\n+                BrokerUtil.deletePath(remoteArchivePath, brokerDesc);\n+                currentArchive.libraries.clear();\n+            }\n+            String srcFilePath = null;\n+            // upload dpp\n+            {\n+                srcFilePath = localDppPath;\n+                String md5sum = getMd5String(srcFilePath);\n+                long size = getFileSize(srcFilePath);\n+                String fileName = getFileName(PATH_DELIMITER, srcFilePath);\n+                String destFilePath = remoteArchivePath + PATH_DELIMITER +\n+                        assemblyFileName(PREFIX_LIB, md5sum, fileName, \"\");\n+                upload(srcFilePath, destFilePath);\n+                currentArchive.libraries.add(new SparkLibrary(destFilePath, md5sum, SparkLibrary.LibType.DPP, size));\n+            }\n+            // upload spark2x\n+            {\n+                srcFilePath = localSpark2xPath;\n+                String md5sum = getMd5String(srcFilePath);\n+                long size = getFileSize(srcFilePath);\n+                String fileName = getFileName(PATH_DELIMITER, srcFilePath);\n+                String destFilePath = remoteArchivePath + PATH_DELIMITER +\n+                        assemblyFileName(PREFIX_LIB, md5sum, fileName, \"\");\n+                upload(srcFilePath, destFilePath);\n+                currentArchive.libraries.add(new SparkLibrary(destFilePath, md5sum, SparkLibrary.LibType.SPARK2X, size));\n+            }\n+            LOG.info(\"finished to upload archive to repository, currentDppVersion={}, path={}\",\n+                    currentDppVersion, remoteArchivePath);\n+        } catch (UserException e) {\n+            throw new LoadException(e.getMessage());\n+        } finally {\n+            writeUnlock();\n+        }\n+    }\n+\n+    private void getLibraries(String remoteArchivePath, List<SparkLibrary> libraries) throws LoadException {\n+        List<TBrokerFileStatus> fileStatuses = Lists.newArrayList();\n+        readLock();\n+        try {\n+            LOG.info(\"input remote archive path, path={}\", remoteArchivePath);\n+            BrokerUtil.parseFile(remoteArchivePath + \"/*\", brokerDesc, fileStatuses);\n+        } catch (UserException e) {\n+            throw new LoadException(e.getMessage());\n+        } finally {\n+            readUnlock();\n+        }\n+\n+        LOG.info(\"get file statuses, size={} \", fileStatuses.size());\n+        for (TBrokerFileStatus fileStatus : fileStatuses) {\n+            LOG.info(\"get file status \" + fileStatus.path);\n+            String fileName = getFileName(PATH_DELIMITER, fileStatus.path);\n+            LOG.info(\"get file name \" + fileName);\n+            if (!fileName.startsWith(PREFIX_LIB)) {\n+                continue;\n+            }\n+            String[] lib_arg = unWrap(PREFIX_LIB, SUFFIX, fileName).split(FILE_NAME_SEPARATOR);\n+            LOG.info(\"get lib arg, length={}, arg[0]={}, arg[1]={}\", lib_arg.length, lib_arg[0], lib_arg[1]);\n+            if (lib_arg.length != 2) {\n+                continue;\n+            }\n+            String md5sum = lib_arg[0];\n+            String type = lib_arg[1];\n+            SparkLibrary.LibType libType = null;\n+            switch (type) {\n+                case SPARK_DPP:\n+                    libType = SparkLibrary.LibType.DPP;\n+                    break;\n+                case SPARK_2X:\n+                    libType = SparkLibrary.LibType.SPARK2X;\n+                    break;\n+                default:\n+                    Preconditions.checkState(false, \"wrong library type: \" + type);\n+                    break;\n+            }\n+            SparkLibrary remoteFile = new SparkLibrary(fileStatus.path, md5sum, libType, fileStatus.size);\n+            libraries.add(remoteFile);\n+            LOG.info(\"get Libraries from remote archive, archive path={}, library={}, md5sum={}, size={}\",\n+                    remoteArchivePath, remoteFile.remotePath, remoteFile.md5sum, remoteFile.size);\n+        }\n+    }\n+\n+    private String getMd5String(String filePath) throws LoadException {\n+        File file = new File(filePath);\n+        String md5sum = null;\n+        try {\n+            md5sum = DigestUtils.md5Hex(new FileInputStream(file));\n+            Preconditions.checkNotNull(md5sum);\n+            LOG.info(\"get md5sum from file {}, md5sum={}\", filePath, md5sum);", "originalCommit": "56f8c9b8b35cb069b4e3b0819d80737a72448153", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQ1ODE0Mw==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r459458143", "bodyText": "better add a cluster_id in path", "author": "morningman", "createdAt": "2020-07-23T13:42:33Z", "path": "fe/fe-core/src/main/java/org/apache/doris/catalog/SparkResource.java", "diffHunk": "@@ -117,6 +117,10 @@ public String getWorkingDir() {\n         return workingDir;\n     }\n \n+    public String getRepositoryDir() {\n+        return workingDir + \"/\" + SparkRepository.REPOSITORY_DIR;", "originalCommit": "56f8c9b8b35cb069b4e3b0819d80737a72448153", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQ2MDc0OQ==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r459460749", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    String spark_home = Config.spark_home_default_dir;\n          \n          \n            \n                    String sparkHome = Config.spark_home_default_dir;", "author": "morningman", "createdAt": "2020-07-23T13:46:13Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java", "diffHunk": "@@ -92,12 +90,42 @@ public void submitEtlJob(long loadJobId, String loadLabel, EtlJobConfig etlJobCo\n         // delete outputPath\n         deleteEtlOutputPath(etlJobConfig.outputPath, brokerDesc);\n \n-        // upload app resource and jobconfig to hdfs\n+        // remote repository\n+        SparkRepository.SparkArchive archive = getRemoteArchive(resource.getRepositoryDir(), brokerDesc);\n+        List<SparkRepository.SparkLibrary> libraries = archive.libraries;\n+        Optional<SparkRepository.SparkLibrary> dppLibrary = libraries.stream().\n+                filter(library -> library.libType == SparkRepository.SparkLibrary.LibType.DPP).findFirst();\n+        Optional<SparkRepository.SparkLibrary> spark2xLibrary = libraries.stream().\n+                filter(library -> library.libType == SparkRepository.SparkLibrary.LibType.SPARK2X).findFirst();\n+        if (!dppLibrary.isPresent() || !spark2xLibrary.isPresent()) {\n+            throw new LoadException(\"failed to get library info from repository\");\n+        }\n+\n+        // spark home\n+        String spark_home = Config.spark_home_default_dir;", "originalCommit": "56f8c9b8b35cb069b4e3b0819d80737a72448153", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQ2MTc4MQ==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r459461781", "bodyText": "What' s this spark.yarn.stage.dir means?", "author": "morningman", "createdAt": "2020-07-23T13:47:41Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java", "diffHunk": "@@ -92,12 +90,42 @@ public void submitEtlJob(long loadJobId, String loadLabel, EtlJobConfig etlJobCo\n         // delete outputPath\n         deleteEtlOutputPath(etlJobConfig.outputPath, brokerDesc);\n \n-        // upload app resource and jobconfig to hdfs\n+        // remote repository\n+        SparkRepository.SparkArchive archive = getRemoteArchive(resource.getRepositoryDir(), brokerDesc);\n+        List<SparkRepository.SparkLibrary> libraries = archive.libraries;\n+        Optional<SparkRepository.SparkLibrary> dppLibrary = libraries.stream().\n+                filter(library -> library.libType == SparkRepository.SparkLibrary.LibType.DPP).findFirst();\n+        Optional<SparkRepository.SparkLibrary> spark2xLibrary = libraries.stream().\n+                filter(library -> library.libType == SparkRepository.SparkLibrary.LibType.SPARK2X).findFirst();\n+        if (!dppLibrary.isPresent() || !spark2xLibrary.isPresent()) {\n+            throw new LoadException(\"failed to get library info from repository\");\n+        }\n+\n+        // spark home\n+        String spark_home = Config.spark_home_default_dir;\n+        // etl config path\n         String configsHdfsDir = etlJobConfig.outputPath + \"/\" + JOB_CONFIG_DIR + \"/\";\n-        String appResourceHdfsPath = configsHdfsDir + APP_RESOURCE_NAME;\n+        // etl config json path\n         String jobConfigHdfsPath = configsHdfsDir + CONFIG_FILE_NAME;\n+        // spark submit app resource path\n+        String appResourceHdfsPath = dppLibrary.get().remotePath;\n+        // spark yarn archive path\n+        String jobArchiveHdfsPath = spark2xLibrary.get().remotePath;\n+        // spark yarn stage dir\n+        String jobStageHdfsPath = resource.getWorkingDir();\n+        LOG.info(\"configsHdfsDir={}, appResourceHdfsPath={}, jobConfigHdfsPath={}, jobArchiveHdfsPath={}\",\n+                configsHdfsDir, appResourceHdfsPath, jobConfigHdfsPath, jobArchiveHdfsPath);\n+\n+        // update archive and stage configs here\n+        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n+        if (Strings.isNullOrEmpty(sparkConfigs.get(\"spark.yarn.archive\"))) {\n+            sparkConfigs.put(\"spark.yarn.archive\", jobArchiveHdfsPath);\n+        }\n+        if (Strings.isNullOrEmpty(sparkConfigs.get(\"spark.yarn.stage.dir\"))) {\n+            sparkConfigs.put(\"spark.yarn.stage.dir\", jobStageHdfsPath);", "originalCommit": "56f8c9b8b35cb069b4e3b0819d80737a72448153", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg0MDA3MA==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r459840070", "bodyText": "A self-generated path by spark in hdfs to save temporary configuration for spark application", "author": "xy720", "createdAt": "2020-07-24T03:52:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQ2MTc4MQ=="}], "type": "inlineReview"}, {"oid": "d042861368ba96027fa94396c1adbb70055a12e2", "url": "https://github.com/apache/incubator-doris/commit/d042861368ba96027fa94396c1adbb70055a12e2", "message": "add lock", "committedDate": "2020-07-24T03:40:18Z", "type": "commit"}, {"oid": "25e6d1af99a24275adab2b33a698576408c8ac8a", "url": "https://github.com/apache/incubator-doris/commit/25e6d1af99a24275adab2b33a698576408c8ac8a", "message": "add ut", "committedDate": "2020-07-24T14:18:38Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDA5Njk3Mw==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r460096973", "bodyText": "Use Catalog.getClusterId() instead of Config.cluster_id.\nAnd how about add SparkResource's name to path? Such as:\n/path/to/work/dir/cluster_id/__spark_repository__my_resource_name/\nNo need to add additional level of dir.", "author": "morningman", "createdAt": "2020-07-24T14:42:51Z", "path": "fe/fe-core/src/main/java/org/apache/doris/catalog/SparkResource.java", "diffHunk": "@@ -144,6 +146,12 @@ public SparkResource getCopiedResource() {\n         return new SparkResource(name, Maps.newHashMap(sparkConfigs), workingDir, broker, brokerProperties);\n     }\n \n+    public SparkRepository getRemoteRepository() {\n+        String remoteRepositoryPath = workingDir + \"/\" + Config.cluster_id + \"/\" + SparkRepository.REPOSITORY_DIR;", "originalCommit": "25e6d1af99a24275adab2b33a698576408c8ac8a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDEyOTU5Ng==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r460129596", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public static String spark_dpp_version = \"1_0_0\";\n          \n          \n            \n                @ConfField\n          \n          \n            \n                public static String spark_dpp_version = \"1_0_0\";\n          \n      \n    \n    \n  \n\nAll config must has @ConfField annotation.", "author": "morningman", "createdAt": "2020-07-24T15:35:17Z", "path": "fe/fe-core/src/main/java/org/apache/doris/common/Config.java", "diffHunk": "@@ -509,12 +509,29 @@\n     @ConfField(mutable = true, masterOnly = true)\n     public static int hadoop_load_default_timeout_second = 86400 * 3; // 3 day\n \n+    // Configurations for spark load\n+    /**\n+     * Default spark dpp version\n+     */\n+    public static String spark_dpp_version = \"1_0_0\";", "originalCommit": "25e6d1af99a24275adab2b33a698576408c8ac8a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDEzMTczMw==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r460131733", "bodyText": "Use SparkResource' name as Key, not ClusterId.\nClusterId is same all the way.\nAnd I think this lock map is unnecessary. You can just call resource.prepareDppArchive().\nAnd prepareDppArchive() is protected by synchronized.", "author": "morningman", "createdAt": "2020-07-24T15:39:00Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java", "diffHunk": "@@ -70,15 +72,16 @@\n public class SparkEtlJobHandler {\n     private static final Logger LOG = LogManager.getLogger(SparkEtlJobHandler.class);\n \n-    private static final String APP_RESOURCE_NAME = \"palo-fe.jar\";\n     private static final String CONFIG_FILE_NAME = \"jobconfig.json\";\n-    private static final String APP_RESOURCE_LOCAL_PATH = PaloFe.DORIS_HOME_DIR + \"/lib/\" + APP_RESOURCE_NAME;\n     private static final String JOB_CONFIG_DIR = \"configs\";\n     private static final String ETL_JOB_NAME = \"doris__%s\";\n     // 5min\n     private static final int GET_APPID_MAX_RETRY_TIMES = 300;\n     private static final int GET_APPID_SLEEP_MS = 1000;\n \n+    // cluster_id -> lock\n+    private static final ConcurrentMap<Integer, Object> DPP_LOCK_MAP = Maps.newConcurrentMap();", "originalCommit": "25e6d1af99a24275adab2b33a698576408c8ac8a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDEzNDQ1OQ==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r460134459", "bodyText": "Each SparkResource has and only has one SparkRepository.\nSo why not just call resource. prepareDppArchive() and return SparkRepository.SparkArchive?", "author": "morningman", "createdAt": "2020-07-24T15:43:32Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java", "diffHunk": "@@ -255,6 +295,17 @@ public void killEtlJob(SparkAppHandle handle, String appId, long loadJobId, Spar\n         }\n     }\n \n+    private SparkRepository.SparkArchive prepareDppArchive(SparkResource resource) throws LoadException {\n+        // remote archive\n+        SparkRepository.SparkArchive archive = null;\n+        SparkRepository repository = resource.getRemoteRepository();", "originalCommit": "25e6d1af99a24275adab2b33a698576408c8ac8a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "16407db3d8a33a4a37b9abd03ecd96b0a771f27c", "url": "https://github.com/apache/incubator-doris/commit/16407db3d8a33a4a37b9abd03ecd96b0a771f27c", "message": "add syncchronized and fix ut", "committedDate": "2020-07-24T17:26:27Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM4NTE2Mg==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r460385162", "bodyText": "This method is unused, can be removed.", "author": "morningman", "createdAt": "2020-07-25T09:14:38Z", "path": "fe/fe-core/src/main/java/org/apache/doris/catalog/SparkResource.java", "diffHunk": "@@ -144,6 +146,28 @@ public SparkResource getCopiedResource() {\n         return new SparkResource(name, Maps.newHashMap(sparkConfigs), workingDir, broker, brokerProperties);\n     }\n \n+    public SparkRepository getRemoteRepository() {", "originalCommit": "16407db3d8a33a4a37b9abd03ecd96b0a771f27c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM4NTI0NA==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r460385244", "bodyText": "The SparkRepository will be created as a new Object for each time. So this isInit is meaningless.", "author": "morningman", "createdAt": "2020-07-25T09:15:33Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkRepository.java", "diffHunk": "@@ -0,0 +1,347 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import org.apache.doris.PaloFe;\n+import org.apache.doris.analysis.BrokerDesc;\n+import org.apache.doris.common.Config;\n+import org.apache.doris.common.LoadException;\n+import org.apache.doris.common.UserException;\n+import org.apache.doris.common.util.BrokerUtil;\n+import org.apache.doris.thrift.TBrokerFileStatus;\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.Lists;\n+\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.util.List;\n+\n+/*\n+ * SparkRepository represents the remote repository for spark archives uploaded by spark\n+ * The organization in repository is:\n+ *\n+ * * __spark_repository__/\n+ *   * __archive_1_0_0/\n+ *     * __lib_990325d2c0d1d5e45bf675e54e44fb16_spark-dpp.jar\n+ *     * __lib_7670c29daf535efe3c9b923f778f61fc_spark-2x.zip\n+ *   * __archive_2_2_0/\n+ *     * __lib_64d5696f99c379af2bee28c1c84271d5_spark-dpp.jar\n+ *     * __lib_1bbb74bb6b264a270bc7fca3e964160f_spark-2x.zip\n+ *   * __archive_3_2_0/\n+ *     * ...\n+ */\n+public class SparkRepository {\n+    private static final Logger LOG = LogManager.getLogger(SparkRepository.class);\n+\n+    public static final String REPOSITORY_DIR = \"__spark_repository__\";\n+    public static final String PREFIX_ARCHIVE = \"__archive_\";\n+    public static final String PREFIX_LIB = \"__lib_\";\n+    public static final String SPARK_DPP = \"spark-dpp\";\n+    public static final String SPARK_2X = \"spark-2x\";\n+    public static final String SUFFIX = \".zip\";\n+\n+    private static final String PATH_DELIMITER = \"/\";\n+    private static final String FILE_NAME_SEPARATOR = \"_\";\n+\n+    private static final String DPP_RESOURCE = \"/spark-dpp/spark-dpp.jar\";\n+    private static final String SPARK_RESOURCE = \"/jars/spark-2x.zip\";\n+\n+    private String remoteRepositoryPath;\n+    private BrokerDesc brokerDesc;\n+    private String localDppPath;\n+    private String localSpark2xPath;\n+\n+    // Version of the spark dpp program in this cluster\n+    private String currentDppVersion;\n+    // Archive that current dpp version pointed to\n+    private SparkArchive currentArchive;\n+\n+    private boolean isInit;", "originalCommit": "16407db3d8a33a4a37b9abd03ecd96b0a771f27c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM4NTc2OA==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r460385768", "bodyText": "This method can be public void, and if error happens, just throw exception", "author": "morningman", "createdAt": "2020-07-25T09:22:14Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkRepository.java", "diffHunk": "@@ -0,0 +1,347 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+package org.apache.doris.load.loadv2;\n+\n+import org.apache.doris.PaloFe;\n+import org.apache.doris.analysis.BrokerDesc;\n+import org.apache.doris.common.Config;\n+import org.apache.doris.common.LoadException;\n+import org.apache.doris.common.UserException;\n+import org.apache.doris.common.util.BrokerUtil;\n+import org.apache.doris.thrift.TBrokerFileStatus;\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import com.google.common.collect.Lists;\n+\n+import org.apache.commons.codec.digest.DigestUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.util.List;\n+\n+/*\n+ * SparkRepository represents the remote repository for spark archives uploaded by spark\n+ * The organization in repository is:\n+ *\n+ * * __spark_repository__/\n+ *   * __archive_1_0_0/\n+ *     * __lib_990325d2c0d1d5e45bf675e54e44fb16_spark-dpp.jar\n+ *     * __lib_7670c29daf535efe3c9b923f778f61fc_spark-2x.zip\n+ *   * __archive_2_2_0/\n+ *     * __lib_64d5696f99c379af2bee28c1c84271d5_spark-dpp.jar\n+ *     * __lib_1bbb74bb6b264a270bc7fca3e964160f_spark-2x.zip\n+ *   * __archive_3_2_0/\n+ *     * ...\n+ */\n+public class SparkRepository {\n+    private static final Logger LOG = LogManager.getLogger(SparkRepository.class);\n+\n+    public static final String REPOSITORY_DIR = \"__spark_repository__\";\n+    public static final String PREFIX_ARCHIVE = \"__archive_\";\n+    public static final String PREFIX_LIB = \"__lib_\";\n+    public static final String SPARK_DPP = \"spark-dpp\";\n+    public static final String SPARK_2X = \"spark-2x\";\n+    public static final String SUFFIX = \".zip\";\n+\n+    private static final String PATH_DELIMITER = \"/\";\n+    private static final String FILE_NAME_SEPARATOR = \"_\";\n+\n+    private static final String DPP_RESOURCE = \"/spark-dpp/spark-dpp.jar\";\n+    private static final String SPARK_RESOURCE = \"/jars/spark-2x.zip\";\n+\n+    private String remoteRepositoryPath;\n+    private BrokerDesc brokerDesc;\n+    private String localDppPath;\n+    private String localSpark2xPath;\n+\n+    // Version of the spark dpp program in this cluster\n+    private String currentDppVersion;\n+    // Archive that current dpp version pointed to\n+    private SparkArchive currentArchive;\n+\n+    private boolean isInit;\n+\n+    public SparkRepository(String remoteRepositoryPath, BrokerDesc brokerDesc) {\n+        this.remoteRepositoryPath = remoteRepositoryPath;\n+        this.brokerDesc = brokerDesc;\n+        this.currentDppVersion = Config.spark_dpp_version;\n+        this.currentArchive = new SparkArchive(getRemoteArchivePath(currentDppVersion), currentDppVersion);\n+        this.localDppPath = PaloFe.DORIS_HOME_DIR + DPP_RESOURCE;\n+        if (!Strings.isNullOrEmpty(Config.spark_resource_path)) {\n+            this.localSpark2xPath = Config.spark_resource_path;\n+        } else {\n+            this.localSpark2xPath = Config.spark_home_default_dir + SPARK_RESOURCE;\n+        }\n+        this.isInit = false;\n+    }\n+\n+    public boolean prepare() throws LoadException {", "originalCommit": "16407db3d8a33a4a37b9abd03ecd96b0a771f27c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM4NTg3MA==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r460385870", "bodyText": "isPrepare is meaningless, we can just call repository.prepare() and then call archive = repository.getCurrentArchive();\nIf error happens, exception will be thrown, and this method will never return null.", "author": "morningman", "createdAt": "2020-07-25T09:23:24Z", "path": "fe/fe-core/src/main/java/org/apache/doris/catalog/SparkResource.java", "diffHunk": "@@ -144,6 +146,28 @@ public SparkResource getCopiedResource() {\n         return new SparkResource(name, Maps.newHashMap(sparkConfigs), workingDir, broker, brokerProperties);\n     }\n \n+    public SparkRepository getRemoteRepository() {\n+        String remoteRepositoryPath = workingDir + \"/\" + Catalog.getCurrentCatalog().getClusterId()\n+                + \"/\" + SparkRepository.REPOSITORY_DIR;\n+        BrokerDesc brokerDesc = new BrokerDesc(broker, getBrokerPropertiesWithoutPrefix());\n+        return new SparkRepository(remoteRepositoryPath, brokerDesc);\n+    }\n+\n+    // Each SparkResource has and only has one SparkRepository.\n+    // This method get the remote archive which matches the dpp version from remote repository\n+    public synchronized SparkRepository.SparkArchive prepareArchive() throws LoadException {\n+        SparkRepository.SparkArchive archive = null;\n+        String remoteRepositoryPath = workingDir + \"/\" + Catalog.getCurrentCatalog().getClusterId()\n+                + \"/\" + SparkRepository.REPOSITORY_DIR + name;\n+        BrokerDesc brokerDesc = new BrokerDesc(broker, getBrokerPropertiesWithoutPrefix());\n+        SparkRepository repository = new SparkRepository(remoteRepositoryPath, brokerDesc);\n+        boolean isPrepare = repository.prepare();", "originalCommit": "16407db3d8a33a4a37b9abd03ecd96b0a771f27c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUzMzQxNA==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r460533414", "bodyText": "done", "author": "xy720", "createdAt": "2020-07-26T14:19:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM4NTg3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM4NjQ4MQ==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r460386481", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    Pair<TPaloBrokerService.Client, TNetworkAddress> pair = new Pair<TPaloBrokerService.Client, TNetworkAddress>(null, null);\n          \n          \n            \n                    Pair<TPaloBrokerService.Client, TNetworkAddress> pair = getBrokerAddressAndClient(brokerDesc);", "author": "morningman", "createdAt": "2020-07-25T09:31:03Z", "path": "fe/fe-core/src/main/java/org/apache/doris/common/util/BrokerUtil.java", "diffHunk": "@@ -349,6 +351,39 @@ public static void deletePath(String path, BrokerDesc brokerDesc) throws UserExc\n         }\n     }\n \n+    public static boolean checkPathExist(String remotePath, BrokerDesc brokerDesc) throws UserException {\n+        Pair<TPaloBrokerService.Client, TNetworkAddress> pair = new Pair<TPaloBrokerService.Client, TNetworkAddress>(null, null);", "originalCommit": "16407db3d8a33a4a37b9abd03ecd96b0a771f27c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM4Njc0Ng==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r460386746", "bodyText": "These checks can be put into the resource.prepareArchive() to make the caller logic more simple.\nprepareArchive() only return the right results, or exception will be thrown.", "author": "morningman", "createdAt": "2020-07-25T09:34:45Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java", "diffHunk": "@@ -92,12 +90,41 @@ public void submitEtlJob(long loadJobId, String loadLabel, EtlJobConfig etlJobCo\n         // delete outputPath\n         deleteEtlOutputPath(etlJobConfig.outputPath, brokerDesc);\n \n-        // upload app resource and jobconfig to hdfs\n+        // prepare dpp archive\n+        SparkRepository.SparkArchive archive = resource.prepareArchive();\n+        Preconditions.checkNotNull(archive);\n+        List<SparkRepository.SparkLibrary> libraries = archive.libraries;\n+        Optional<SparkRepository.SparkLibrary> dppLibrary = libraries.stream().\n+                filter(library -> library.libType == SparkRepository.SparkLibrary.LibType.DPP).findFirst();\n+        Optional<SparkRepository.SparkLibrary> spark2xLibrary = libraries.stream().\n+                filter(library -> library.libType == SparkRepository.SparkLibrary.LibType.SPARK2X).findFirst();\n+        if (!dppLibrary.isPresent() || !spark2xLibrary.isPresent()) {", "originalCommit": "16407db3d8a33a4a37b9abd03ecd96b0a771f27c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM4NjkyOQ==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r460386929", "bodyText": "In what situation, the spark.yarn.archive config will NOT be empty?", "author": "morningman", "createdAt": "2020-07-25T09:37:17Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java", "diffHunk": "@@ -92,12 +90,41 @@ public void submitEtlJob(long loadJobId, String loadLabel, EtlJobConfig etlJobCo\n         // delete outputPath\n         deleteEtlOutputPath(etlJobConfig.outputPath, brokerDesc);\n \n-        // upload app resource and jobconfig to hdfs\n+        // prepare dpp archive\n+        SparkRepository.SparkArchive archive = resource.prepareArchive();\n+        Preconditions.checkNotNull(archive);\n+        List<SparkRepository.SparkLibrary> libraries = archive.libraries;\n+        Optional<SparkRepository.SparkLibrary> dppLibrary = libraries.stream().\n+                filter(library -> library.libType == SparkRepository.SparkLibrary.LibType.DPP).findFirst();\n+        Optional<SparkRepository.SparkLibrary> spark2xLibrary = libraries.stream().\n+                filter(library -> library.libType == SparkRepository.SparkLibrary.LibType.SPARK2X).findFirst();\n+        if (!dppLibrary.isPresent() || !spark2xLibrary.isPresent()) {\n+            throw new LoadException(\"failed to get library from remote archive\");\n+        }\n+\n+        // spark home\n+        String sparkHome = Config.spark_home_default_dir;\n+        // etl config path\n         String configsHdfsDir = etlJobConfig.outputPath + \"/\" + JOB_CONFIG_DIR + \"/\";\n-        String appResourceHdfsPath = configsHdfsDir + APP_RESOURCE_NAME;\n+        // etl config json path\n         String jobConfigHdfsPath = configsHdfsDir + CONFIG_FILE_NAME;\n+        // spark submit app resource path\n+        String appResourceHdfsPath = dppLibrary.get().remotePath;\n+        // spark yarn archive path\n+        String jobArchiveHdfsPath = spark2xLibrary.get().remotePath;\n+        // spark yarn stage dir\n+        String jobStageHdfsPath = resource.getWorkingDir();\n+\n+        // update archive and stage configs here\n+        Map<String, String> sparkConfigs = resource.getSparkConfigs();\n+        if (Strings.isNullOrEmpty(sparkConfigs.get(\"spark.yarn.archive\"))) {", "originalCommit": "16407db3d8a33a4a37b9abd03ecd96b0a771f27c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUzMTgwMQ==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r460531801", "bodyText": "If user has set spark.yarn.archivein resource, we prefer to use the archive set by user, otherwise we use archive generated by SparkRepository.", "author": "xy720", "createdAt": "2020-07-26T14:04:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM4NjkyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM4Njk0Mw==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r460386943", "bodyText": "Why need to set spark home here?\nIs it compatible with open source spark env?", "author": "morningman", "createdAt": "2020-07-25T09:37:33Z", "path": "fe/fe-core/src/main/java/org/apache/doris/load/loadv2/SparkEtlJobHandler.java", "diffHunk": "@@ -114,7 +141,9 @@ public void submitEtlJob(long loadJobId, String loadLabel, EtlJobConfig etlJobCo\n                 .setAppResource(appResourceHdfsPath)\n                 .setMainClass(SparkEtlJob.class.getCanonicalName())\n                 .setAppName(String.format(ETL_JOB_NAME, loadLabel))\n+                .setSparkHome(sparkHome)", "originalCommit": "16407db3d8a33a4a37b9abd03ecd96b0a771f27c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUzMjU5MA==", "url": "https://github.com/apache/incubator-doris/pull/4163#discussion_r460532590", "bodyText": "This spark home is configurable. Users in open source environment need to configure this parameter in fe.conf", "author": "xy720", "createdAt": "2020-07-26T14:11:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM4Njk0Mw=="}], "type": "inlineReview"}, {"oid": "f622cf39bd72f64ba14258c5e48b7a9c10a2d4f1", "url": "https://github.com/apache/incubator-doris/commit/f622cf39bd72f64ba14258c5e48b7a9c10a2d4f1", "message": "save code", "committedDate": "2020-07-26T14:17:55Z", "type": "commit"}]}