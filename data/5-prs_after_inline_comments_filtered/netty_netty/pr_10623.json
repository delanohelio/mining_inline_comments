{"pr_number": 10623, "pr_title": "Fix performance regression on HttpPost RequestDecoder", "pr_createdAt": "2020-09-30T09:16:13Z", "pr_url": "https://github.com/netty/netty/pull/10623", "timeline": [{"oid": "4b2f1db62ce5bab675dc9d82e9bb00d1bd855633", "url": "https://github.com/netty/netty/commit/4b2f1db62ce5bab675dc9d82e9bb00d1bd855633", "message": "Fix for performance regression on HttpPost RequestDecoder\n\nFix issue #10508 where PARANOID mode slow down about 1000 times compared to ADVANCED.\n\nReasons were:\n\nInstead of allocating with no upper limit buffers, try to alocate as much as possible with effective and coherent limit\n\nThe performances are better in all conditions (even PARANOID):\n- Without this patch, the very same test gives 300 to 450 ms each check, except in PARANOID mode where it is about 105.000 ms\n- With this patch, the timers are about 100 to 200 ms each check (so almost 2 times better), and 1000 to 1300 ms in PARANOID mode (so about 100 times better)\n\nSo this might be a good idea considering better times without PARANOID level, and allowing to keep PARANOID in all tests.\n\nConsider that the included test shall not be keeped as it does nothing except performance check.", "committedDate": "2020-10-01T10:16:30Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODI0NjI1NA==", "url": "https://github.com/netty/netty/pull/10623#discussion_r498246254", "bodyText": "@fredericBregier can you explain why setting an upper limit improves things here ?", "author": "normanmaurer", "createdAt": "2020-10-01T13:31:39Z", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoder.java", "diffHunk": "@@ -1035,7 +1035,7 @@ private static String readLine(ByteBuf undecodedChunk, Charset charset) {\n         }\n         SeekAheadOptimize sao = new SeekAheadOptimize(undecodedChunk);\n         int readerIndex = undecodedChunk.readerIndex();\n-        ByteBuf line = undecodedChunk.alloc().heapBuffer(64);\n+        ByteBuf line = undecodedChunk.alloc().heapBuffer(64, 64);", "originalCommit": "4b2f1db62ce5bab675dc9d82e9bb00d1bd855633", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODI3MDAyOA==", "url": "https://github.com/netty/netty/pull/10623#discussion_r498270028", "bodyText": "@normanmaurer\nI don't have any real clue, but it seems it does. That's why I'm asking you to look at this why.\nThe change is huge on performances in PARANOID mode (using the example test given by the final user), about 10 times better... I test several times, and without it, the times are about 1000ms, while with it, it gives about 100ms.\nUsing my old test (a bit different, where a lot of items, about 6000, are passed and not only one file), the improvement is there also on PARANOID mode, but not that much (not 10 times better, comparing 146.000 old code vs 105.000 new code).\nThere, the issue (and change) is on the undecodedChunk.write(buf).\nBut if I change it to something like wrappedBuffers(undecodedChunk, buf), the time is close to 1000ms again in PARANOID mode, but increases sensibly in other modes (500ms against 200ms without any change).\nIt was strange for me to see that comparing:\n\nundecodedChunk = isLast? ... : buf.alloc.buffer(buf.readableSize(), buf.readableSize()).write(buf) and undecodedChunk = Unpooled.wrapped(undecodedChunk, buf.alloc.buffer(buf.readableSize(), buf.readableSize()).write(buf))\nvs original ones (still in place at lines 338 and 340\n\ngives impact performances (better for PARANOID, about 1000 times, but worst in other modes, about 2 times). I feel like it was the same in both cases (allocation and writing), but it seems not. Note that in current code, undecodedChunk in line 338 is still allocated using no upper bound.\nThe main \"same root cause\" seems to be \"no upper bound for buffer\" when using PARANOID mode, but in different ways.\nI feel like this proposal is safe as it enhances a bit the general performances and takes down the PARANOID issue, at least for the user's example.\nBut I feel also that there is something else, since in my test (high number of items) this is really not enough.\nSo the reason I try to test it and produce at least this in order to get help.", "author": "fredericBregier", "createdAt": "2020-10-01T14:04:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODI0NjI1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODYyODA0MA==", "url": "https://github.com/netty/netty/pull/10623#discussion_r498628040", "bodyText": "@normanmaurer\nI added a fake commit to let you run the different cases at once:\nThis commit is only intend to show the result on tests using 3 cases:\n\nHttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 1: original case\nHttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 2: where only upper bound is modified\nHttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 3: where write is replaced by wrapped and the last \"no upper bounded\" buffer is replaced too\n\nTest to run is testRegressionMultipleLevelLeakDetector.\nOn my side results are:\nTimer: DISABLED NotUsingDisk1 => 220.995585\nTimer: SIMPLE NotUsingDisk1 => 222.134248\nTimer: ADVANCED NotUsingDisk1 => 208.68989\nTimer: PARANOID NotUsingDisk1 => 100044.293876\nTimer: DISABLED UsingDisk1 => 707.480873\nTimer: DISABLED UsingDisk1 => 518.695771\nTimer: SIMPLE UsingDisk1 => 522.790849\nTimer: ADVANCED UsingDisk1 => 585.759164\nTimer: PARANOID UsingDisk1 => 105090.135276\nTimer: DISABLED NotUsingDisk2 => 136.461289\nTimer: SIMPLE NotUsingDisk2 => 126.793194\nTimer: ADVANCED NotUsingDisk2 => 129.771282\nTimer: PARANOID NotUsingDisk2 => 973.38734\nTimer: DISABLED UsingDisk2 => 74.279745\nTimer: DISABLED UsingDisk2 => 71.332363\nTimer: SIMPLE UsingDisk2 => 114.811827\nTimer: ADVANCED UsingDisk2 => 89.290387\nTimer: PARANOID UsingDisk2 => 1155.278418\nTimer: DISABLED NotUsingDisk3 => 1829.364849\nTimer: SIMPLE NotUsingDisk3 => 1949.512746\nTimer: ADVANCED NotUsingDisk3 => 2120.573799\nTimer: PARANOID NotUsingDisk3 => 2073.506771\nTimer: DISABLED UsingDisk3 => 1998.837595\nTimer: DISABLED UsingDisk3 => 1952.342478\nTimer: SIMPLE UsingDisk3 => 2099.951834\nTimer: ADVANCED UsingDisk3 => 2058.028121\nTimer: PARANOID UsingDisk3 => 2295.847557", "author": "fredericBregier", "createdAt": "2020-10-02T05:53:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODI0NjI1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTIzNTA3OQ==", "url": "https://github.com/netty/netty/pull/10623#discussion_r499235079", "bodyText": "Last fake commit:\nThis commit is only intend to show the result on tests using 3 cases:\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 1: original case\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 2: where only upper bound is modified\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 3: where temptative to reuse more the existing ByteBuf is done\nTest to run is testRegressionMultipleLevelLeakDetectorNoDisk or testHighNumberCheckLeakDetectorVersions\nCurrent results (stability is correct but numbers depend on host)\nI've done various tests:\n\nusing wrapped (or through buf.alloc().compositeByteBuf()): worst timers\nusing fixed allocation (but high enough to fit,the necessary elements, just for test): not good timers (probably because the size is too huge for a lot of cases, and whatever, it is not acceptable to have fixed size, it was just for testing purpose)\nusing various ways to discard read bytes: it gives a bit improvement (TEST_TEMP_ITEM=3): almost best results, but not that much\n\nIn particular, in the \"big\" test (testHighNumberCheckLeakDetectorVersions where more than 16000 items are sent, while the other test is only one item from a big file), I am not able to reduce the timer in PARANOID mode, while in \"unique\" item test ( testRegressionMultipleLevelLeakDetectorNoDisk), the PARANOID decreases a lot.\nEven if I cannot understand why setting the upper bound brings such a better improvement, I feel like this is at least a good change.\nHowever, for the other try using discardReadBytes (or tries using wrapped buffers), it seems to not enhanced the timers.\n@normanmaurer If you have any clue to continue, I will. I would propose to at least have the minimal change (upper bounds) in order to fix a bit this issue in PARANOID mode (and does not changed a lot, better or worst, for other levels).\nHere are my results (I tried to use profiling to understand the reasons, but the profiling just kill all results, therefore I was unable to deep in buffer implementations to see where is the main difference).\ntestHighNumberCheckLeakDetectorVersions\nHighItemNumberDISABLED1=506.43838600000004,\nHighItemNumberDISABLED2=496.729877,\nHighItemNumberDISABLED3=503.72193200000004,\nHighItemNumberSIMPLE1=460.46265600000004,\nHighItemNumberSIMPLE2=475.950721,\nHighItemNumberSIMPLE3=467.69606699999997,\nHighItemNumberADVANCED1=465.508472,\nHighItemNumberADVANCED2=611.797092,\nHighItemNumberADVANCED3=467.812221,\nHighItemNumberPARANOID1=304098.68770899996,\nHighItemNumberPARANOID2=304140.256149,\nHighItemNumberPARANOID3=303301.14581200003,\nBigItemDISABLED1=426.205971,\nBigItemDISABLED2=441.343974,\nBigItemDISABLED3=420.528978,\nBigItemSIMPLE1=420.825118,\nBigItemSIMPLE2=432.968082,\nBigItemSIMPLE3=418.955781,\nBigItemADVANCED1=420.391646,\nBigItemADVANCED2=445.089119,\nBigItemADVANCED3=428.97732599999995,\nBigItemPARANOID1=318339.78715600003\nBigItemPARANOID2=319121.713933\nBigItemPARANOID3=316866.262051\ntestRegressionMultipleLevelLeakDetectorNoDisk\nTimer: DISABLED NotUsingDisk1 => 195.185296\nTimer: DISABLED NotUsingDisk2 => 206.28149\nTimer: DISABLED NotUsingDisk3 => 142.354934\nTimer: SIMPLE NotUsingDisk1 => 185.459894\nTimer: SIMPLE NotUsingDisk2 => 82.243331\nTimer: SIMPLE NotUsingDisk3 => 83.65489\nTimer: ADVANCED NotUsingDisk1 => 175.245971\nTimer: ADVANCED NotUsingDisk2 => 214.114222\nTimer: ADVANCED NotUsingDisk3 => 89.005829\nTimer: PARANOID NotUsingDisk1 => 119286.301819\nTimer: PARANOID NotUsingDisk2 => 1604.135822\nTimer: PARANOID NotUsingDisk3 => 2489.555437", "author": "fredericBregier", "createdAt": "2020-10-04T11:18:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODI0NjI1NA=="}], "type": "inlineReview"}, {"oid": "be0651e98dc09d3056441b30ed90a184a5ccf512", "url": "https://github.com/netty/netty/commit/be0651e98dc09d3056441b30ed90a184a5ccf512", "message": "Fake commit\n\nThis commit is only intend to show the result on tests using 3 cases:\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 1: original case\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 2: where only upper bound is modified\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 3: where temptative to reuse more the existing ByteBuf is done\n\nTest to run is `testRegressionMultipleLevelLeakDetectorNoDisk` or `testHighNumberCheckLeakDetectorVersions`\n\nCurrent results (stability is correct but numbers depend on host)\n\ntestHighNumberCheckLeakDetectorVersions\n=======================================\n\nHighItemNumberDISABLED1=506.43838600000004,\nHighItemNumberDISABLED2=496.729877,\nHighItemNumberDISABLED3=503.72193200000004,\n\nHighItemNumberSIMPLE1=460.46265600000004,\nHighItemNumberSIMPLE2=475.950721,\nHighItemNumberSIMPLE3=467.69606699999997,\n\nHighItemNumberADVANCED1=465.508472,\nHighItemNumberADVANCED2=611.797092,\nHighItemNumberADVANCED3=467.812221,\n\nHighItemNumberPARANOID1=304098.68770899996,\nHighItemNumberPARANOID2=304140.256149,\nHighItemNumberPARANOID3=303301.14581200003,\n\nBigItemDISABLED1=426.205971,\nBigItemDISABLED2=441.343974,\nBigItemDISABLED3=420.528978,\n\nBigItemSIMPLE1=420.825118,\nBigItemSIMPLE2=432.968082,\nBigItemSIMPLE3=418.955781,\n\nBigItemADVANCED1=420.391646,\nBigItemADVANCED2=445.089119,\nBigItemADVANCED3=428.97732599999995,\n\nBigItemPARANOID1=318339.78715600003\nBigItemPARANOID2=319121.713933\nBigItemPARANOID3=316866.262051\n\ntestRegressionMultipleLevelLeakDetectorNoDisk\n=============================================\n\nTimer: DISABLED NotUsingDisk1 => 195.185296\nTimer: DISABLED NotUsingDisk2 => 206.28149\nTimer: DISABLED NotUsingDisk3 => 142.354934\n\nTimer: SIMPLE NotUsingDisk1 => 185.459894\nTimer: SIMPLE NotUsingDisk2 => 82.243331\nTimer: SIMPLE NotUsingDisk3 => 83.65489\n\nTimer: ADVANCED NotUsingDisk1 => 175.245971\nTimer: ADVANCED NotUsingDisk2 => 214.114222\nTimer: ADVANCED NotUsingDisk3 => 89.005829\n\nTimer: PARANOID NotUsingDisk1 => 119286.301819\nTimer: PARANOID NotUsingDisk2 => 1604.135822\nTimer: PARANOID NotUsingDisk3 => 2489.555437", "committedDate": "2020-10-04T11:00:37Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI0MjQ2Mw==", "url": "https://github.com/netty/netty/pull/10623#discussion_r499242463", "bodyText": "Use final here, it would change a lot how the JIT would optimize it.\nUse a sys property to set this and just use different runs with different JVMs (probably using a good profiler + JMH bench would be ideal to be sure of the impact/meaning of changes)", "author": "franz1981", "createdAt": "2020-10-04T12:40:47Z", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoder.java", "diffHunk": "@@ -306,6 +308,7 @@ public InterfaceHttpData getBodyHttpData(String name) {\n         return null;\n     }\n \n+    public static int TEST_TEMP_ITEM = 1;", "originalCommit": "be0651e98dc09d3056441b30ed90a184a5ccf512", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI0NDQ4MA==", "url": "https://github.com/netty/netty/pull/10623#discussion_r499244480", "bodyText": "@franz1981 Thank you !\nI agree that making it final shall be done to achieve final checks. This variable was only intend to allow 3 kinds of tests, not to be keeped of course in final code (and if final, I couldn't change the value of course).\nNow, using a final here, 3 different runs (changing value of course) and a good profiler could be far better and ideal, I agree.\nHowever, as I said, when using a profiler (the one I have is from Oracle - VisualVM-, and I believe it is not the best around, but the only one I've got), the performances drop severely (even in DISCARDED mode), therefore nothing to look at correctly. For my other tasks, this one is great, giving the information I need and with not that much impact. But there, the performances are almost all at the same level than PARANOID, even in DISCARDED or SIMPLE mode.\nCan you suggest some tools (free, sadly) that could help in this research?", "author": "fredericBregier", "createdAt": "2020-10-04T13:02:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI0MjQ2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI0OTA3Ng==", "url": "https://github.com/netty/netty/pull/10623#discussion_r499249076", "bodyText": "Sure!\nhttps://github.com/jvm-profiling-tools/async-profiler is probably one of the most complete, accurate and cheap (;)) ones I know.\nHighly suggested to add -XX:+UnlockDiagnosticVMOptions -XX:+DebugNonSafepoints", "author": "franz1981", "createdAt": "2020-10-04T13:50:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI0MjQ2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI1NjI5NA==", "url": "https://github.com/netty/netty/pull/10623#discussion_r499256294", "bodyText": "@njhill now you read my mind bud?:)", "author": "franz1981", "createdAt": "2020-10-04T15:05:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI0MjQ2Mw=="}], "type": "inlineReview"}, {"oid": "a5cc7f20bd5824638f91b66fcc50b1fc773cb787", "url": "https://github.com/netty/netty/commit/a5cc7f20bd5824638f91b66fcc50b1fc773cb787", "message": "Fix for performance regression on HttpPost RequestDecoder\n\nFix issue #10508 where PARANOID mode slow down about 1000 times compared to ADVANCED.\n\nReasons were:\n\nToo many `readByte()` method calls while other ways exist (such as keep in memory the last scan position when trying to find a delimiter or using `bytesBefore(firstByte)` instead of looping externally).\nIn addition, try to reuse as much as possible already allocated buffers.\n\nThis commit is only intend to show the result on tests using 3 cases:\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 1: original case\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 2: where only undecodedChunk temptatives of reusing already allocated buffers are made\n- HttpPostMultipartRequestDecoder.TEST_TEMP_ITEM = 3: where change on way buffer are parsed: instead of read byte per byte until found delimiter, try to find the delimiter using bytesBefore() and keep the last unfound position to skeep already parsed parts\n\nTest to run is `testRegressionMultipleLevelLeakDetectorNoDisk` or `testHighNumberCheckLeakDetectorVersions`\n\nCurrent results:\n\ntestHighNumberCheckLeakDetectorVersions\n=======================================\n\n{BigItemSIMPLE1=4100, BigItemADVANCED1=4100, BigItemDISABLED1=4100, HighItemNumberADVANCED1=4500, HighItemNumberSIMPLE1=4500, HighItemNumberDISABLED1=4100, HighItemNumberPARANOID1=1480000, BigItemPARANOID1=1450000}\n\n{BigItemDISABLED2=2339, BigItemADVANCED2=2345, BigItemSIMPLE2=2383, HighItemNumberDISABLED2=2840, HighItemNumberSIMPLE2=2864, HighItemNumberADVANCED2=3048, HighItemNumberPARANOID2=1511640, BigItemPARANOID2=1588672}\n\nWithout optim level 2\n{BigItemDISABLED3=423, BigItemADVANCED3=429, BigItemSIMPLE3=453, HighItemNumberSIMPLE3=834, HighItemNumberDISABLED3=936, HighItemNumberADVANCED3=978, BigItemPARANOID3=2720, HighItemNumberPARANOID3=17182}\n\nWith Optim level 2\n{BigItemDISABLED3=420, BigItemSIMPLE3=422, BigItemADVANCED3=451, HighItemNumberSIMPLE3=821, HighItemNumberDISABLED3=930, HighItemNumberADVANCED3=1003, BigItemPARANOID3=2660, HighItemNumberPARANOID3=16712}\n\ntestRegressionMultipleLevelLeakDetectorNoDisk\n=============================================\n\nTimer: DISABLED NotUsingDisk1 => 1903.912934\nTimer: SIMPLE NotUsingDisk1 => 1926.971844\nTimer: ADVANCED NotUsingDisk1 => 26489.845075\nTimer: PARANOID NotUsingDisk1 => 1082819.185679\n\nTimer: DISABLED NotUsingDisk2 => 1538.593234\nTimer: SIMPLE NotUsingDisk2 => 1590.055765\nTimer: ADVANCED NotUsingDisk2 => 1538.786221\nTimer: PARANOID NotUsingDisk2 => 1132602.00827\n\nTimer: DISABLED NotUsingDisk3 => 653.744671\nTimer: SIMPLE NotUsingDisk3 => 785.152185\nTimer: ADVANCED NotUsingDisk3 => 572.136954\nTimer: PARANOID NotUsingDisk3 => 1039.148579\n\nObservations using Async-Profiler:\n\n1) Without optimizations, most of the time (more than 95%) is through `readByte()` method within `loadDataMultipartStandard` method.\n2) With level 2 (reusing as much as possible already allocated buffers and limit the allocation as small as possible), about 0.2% less within `readByte()` for only 0.01% more in `offer()` method (where reusing buffer, possibly rewriting internally using `discardReadBytes()` ).\n3) With level 3 (using `bytesBefore(byte)` instead of `readByte()` to find the delimiter), the `loadDataMultipartStandard` method is going down to 19 to 33% depending on the test used. the `readByte()` method or equivalent `getByte(pos)` method are going down to 15% (from 95%).\n\nTimes are confirming those profiling:\n- V1 vs V2: In SIMPLE mode about 37% better, in ADVANCED mode about 32% better and in PARANOID mode about -2% worst (most of the read access and implicit stacktrace creation are masking the benefit)\n- V1 vs V3: In SIMPLE mode about 82% better, in ADVANCED mode about 79% better and in PARANOID mode about 99% better (most of the duplicate read accesses are removed or make internally through `bytesBefore(byte)` method)", "committedDate": "2020-10-08T18:13:27Z", "type": "forcePushed"}, {"oid": "d515b3eaf67059ff58135ed2c606997ff921d9d4", "url": "https://github.com/netty/netty/commit/d515b3eaf67059ff58135ed2c606997ff921d9d4", "message": "Fix for performance regression on HttpPost RequestDecoder\n\nFix issue #10508 where PARANOID mode slow down about 1000 times compared to ADVANCED.\n\nReasons were:\n\nToo many `readByte()` method calls while other ways exist (such as keep in memory the last scan position when trying to find a delimiter or using `bytesBefore(firstByte)` instead of looping externally).\nIn addition, try to reuse as much as possible already allocated buffers minimizes a bit the memory footprint and allocations.\n\n2 kinds of change were done:\n- undecodedChunk temptatives of reusing already allocated buffers are made\n- major change on way buffer are parsed: instead of read byte per byte until found delimiter, try to find the delimiter using `bytesBefore()` and keep the last unfound position to skeep already parsed parts (algorithms are the same but implementation of scan are different)\n\nObservations using Async-Profiler:\n==================================\n\n1) Without optimizations, most of the time (more than 95%) is through `readByte()` method within `loadDataMultipartStandard` method.\n2) With reusing as much as possible already allocated buffers and limit the allocation as small as possible, about 0.2% less within `readByte()` for only 0.01% more in `offer()` method (where reusing buffer, possibly rewriting internally using `discardReadBytes()` ).\n3) With using `bytesBefore(byte)` instead of `readByte()` to find various delimiter, the `loadDataMultipartStandard` method is going down to 19 to 33% depending on the test used. the `readByte()` method or equivalent `getByte(pos)` method are going down to 15% (from 95%).\n\nTimes are confirming those profiling:\n- With reusing already allocated buffers as much as possible, in SIMPLE mode about 37% better, in ADVANCED mode about 32% better and in PARANOID mode about -2% worst (most of the read access and implicit stacktrace creation are masking the benefit)\n- With all optimizations (including buffer reuse), in SIMPLE mode about 82% better, in ADVANCED mode about 79% better and in PARANOID mode about 99% better (most of the duplicate read accesses are removed or make internally through `bytesBefore(byte)` method)\n\nA benchmark is added to show the behavior of the various cases (one big item, such as File upload, and many items) and various level of detection (Disabled, Simple, Advanced, Paranoid). This benchmark is intend to alert if new implementations make too many differences (such as the previous version where about PARANOID gives about 1000 times slower than other levels, while it is now about at most 10 times).\n\nExtract of Benchmark run:\n=========================\n\nRun complete. Total time: 00:13:27\n\nBenchmark                                                                           Mode  Cnt  Score   Error   Units\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigAdvancedLevel   thrpt    6  2,977 \u00b1 0,843  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigDisabledLevel   thrpt    6  2,997 \u00b1 0,240  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigParanoidLevel   thrpt    6  1,010 \u00b1 1,398  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigSimpleLevel     thrpt    6  2,768 \u00b1 0,072  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighAdvancedLevel  thrpt    6  1,706 \u00b1 0,047  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighDisabledLevel  thrpt    6  1,954 \u00b1 0,041  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighParanoidLevel  thrpt    6  0,197 \u00b1 0,008  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighSimpleLevel    thrpt    6  1,967 \u00b1 0,428  ops/ms", "committedDate": "2020-10-10T15:05:20Z", "type": "forcePushed"}, {"oid": "00565439760dce1a5cfdafc542e94311dfeca68b", "url": "https://github.com/netty/netty/commit/00565439760dce1a5cfdafc542e94311dfeca68b", "message": "Fix for performance regression on HttpPost RequestDecoder\n\nFix issue #10508 where PARANOID mode slow down about 1000 times compared to ADVANCED.\n\nReasons were:\n\nToo many `readByte()` method calls while other ways exist (such as keep in memory the last scan position when trying to find a delimiter or using `bytesBefore(firstByte)` instead of looping externally).\nIn addition, try to reuse as much as possible already allocated buffers minimizes a bit the memory footprint and allocations.\n\n2 kinds of change were done:\n- undecodedChunk temptatives of reusing already allocated buffers are made\n- major change on way buffer are parsed: instead of read byte per byte until found delimiter, try to find the delimiter using `bytesBefore()` and keep the last unfound position to skeep already parsed parts (algorithms are the same but implementation of scan are different)\n\nObservations using Async-Profiler:\n==================================\n\n1) Without optimizations, most of the time (more than 95%) is through `readByte()` method within `loadDataMultipartStandard` method.\n2) With reusing as much as possible already allocated buffers and limit the allocation as small as possible, about 0.2% less within `readByte()` for only 0.01% more in `offer()` method (where reusing buffer, possibly rewriting internally using `discardReadBytes()` ).\n3) With using `bytesBefore(byte)` instead of `readByte()` to find various delimiter, the `loadDataMultipartStandard` method is going down to 19 to 33% depending on the test used. the `readByte()` method or equivalent `getByte(pos)` method are going down to 15% (from 95%).\n\nTimes are confirming those profiling:\n- With reusing already allocated buffers as much as possible, in SIMPLE mode about 37% better, in ADVANCED mode about 32% better and in PARANOID mode about -2% worst (most of the read access and implicit stacktrace creation are masking the benefit)\n- With all optimizations (including buffer reuse), in SIMPLE mode about 82% better, in ADVANCED mode about 79% better and in PARANOID mode about 99% better (most of the duplicate read accesses are removed or make internally through `bytesBefore(byte)` method)\n\nA benchmark is added to show the behavior of the various cases (one big item, such as File upload, and many items) and various level of detection (Disabled, Simple, Advanced, Paranoid). This benchmark is intend to alert if new implementations make too many differences (such as the previous version where about PARANOID gives about 1000 times slower than other levels, while it is now about at most 10 times).\n\nExtract of Benchmark run:\n=========================\n\nRun complete. Total time: 00:13:27\n\nBenchmark                                                                           Mode  Cnt  Score   Error   Units\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigAdvancedLevel   thrpt    6  2,977 \u00b1 0,843  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigDisabledLevel   thrpt    6  2,997 \u00b1 0,240  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigParanoidLevel   thrpt    6  1,010 \u00b1 1,398  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigSimpleLevel     thrpt    6  2,768 \u00b1 0,072  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighAdvancedLevel  thrpt    6  1,706 \u00b1 0,047  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighDisabledLevel  thrpt    6  1,954 \u00b1 0,041  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighParanoidLevel  thrpt    6  0,197 \u00b1 0,008  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighSimpleLevel    thrpt    6  1,967 \u00b1 0,428  ops/ms", "committedDate": "2020-10-10T15:29:33Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI0NDU1MQ==", "url": "https://github.com/netty/netty/pull/10623#discussion_r503244551", "bodyText": "2020", "author": "chrisvest", "createdAt": "2020-10-12T11:56:38Z", "path": "microbench/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoderBenchmark.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Copyright 2019 The Netty Project", "originalCommit": "00565439760dce1a5cfdafc542e94311dfeca68b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzM4MjExOQ==", "url": "https://github.com/netty/netty/pull/10623#discussion_r503382119", "bodyText": "OK", "author": "fredericBregier", "createdAt": "2020-10-12T15:44:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI0NDU1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI0NTc3Mg==", "url": "https://github.com/netty/netty/pull/10623#discussion_r503245772", "bodyText": "It's suspicious to see a while-loop for releasing a reference counted object. Increments and decrements of the ref count are supposed to be pair-wise, following the structure of the code.", "author": "chrisvest", "createdAt": "2020-10-12T11:58:57Z", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -689,7 +695,9 @@ public void destroy() {\n         destroyed = true;\n \n         if (undecodedChunk != null && undecodedChunk.refCnt() > 0) {\n-            undecodedChunk.release();\n+            while (undecodedChunk.refCnt() > 0) {\n+                undecodedChunk.release();\n+            }", "originalCommit": "00565439760dce1a5cfdafc542e94311dfeca68b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzM4MzEyMg==", "url": "https://github.com/netty/netty/pull/10623#discussion_r503383122", "bodyText": "Indeed, and I did not found why in some cases I have a huge number of refCnt(), so this.\nHowever, I will remove it, going back to previous situation (probably bad programming on my side).", "author": "fredericBregier", "createdAt": "2020-10-12T15:46:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI0NTc3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI0NjQxMA==", "url": "https://github.com/netty/netty/pull/10623#discussion_r503246410", "bodyText": "Odd formatting that the constants ended up on their own line.", "author": "chrisvest", "createdAt": "2020-10-12T11:59:59Z", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoder.java", "diffHunk": "@@ -1290,96 +1159,89 @@ private static String readDelimiter(ByteBuf undecodedChunk, String delimiter) {\n     }\n \n     /**\n-     * Load the field value or file data from a Multipart request\n+     * @param undecodedChunk the source where the delimiter is to be found\n+     * @param delimiter the string to find out\n+     * @param offset the offset from readerIndex within the undecodedChunk to\n+     *     start from to find out the delimiter\n      *\n-     * @return {@code true} if the last chunk is loaded (boundary delimiter found), {@code false} if need more chunks\n-     * @throws ErrorDataDecoderException\n+     * @return a number >= 0 if found, else new offset with negative value\n+     *     (to inverse), both from readerIndex\n      */\n-    private static boolean loadDataMultipartStandard(ByteBuf undecodedChunk, String delimiter, HttpData httpData) {\n+    private static int findDelimiter(ByteBuf undecodedChunk, String delimiter,\n+                                     int offset) {\n         final int startReaderIndex = undecodedChunk.readerIndex();\n         final int delimeterLength = delimiter.length();\n-        int index = 0;\n-        int lastPosition = startReaderIndex;\n-        byte prevByte = HttpConstants.LF;\n-        boolean delimiterFound = false;\n-        while (undecodedChunk.isReadable()) {\n-            final byte nextByte = undecodedChunk.readByte();\n-            // Check the delimiter\n-            if (prevByte == HttpConstants.LF && nextByte == delimiter.codePointAt(index)) {\n-                index++;\n-                if (delimeterLength == index) {\n-                    delimiterFound = true;\n+        final int toRead = undecodedChunk.readableBytes();\n+        int newOffset = offset;\n+        boolean delimiterNotFound = true;\n+        while (delimiterNotFound && newOffset + delimeterLength <= toRead) {\n+            int posFirstChar = undecodedChunk\n+                .bytesBefore(startReaderIndex + newOffset, toRead - newOffset,\n+                             (byte) delimiter.codePointAt(0));\n+            if (posFirstChar == -1) {\n+                newOffset = toRead;\n+                return -newOffset;\n+            }\n+            newOffset = posFirstChar + offset;\n+            if (newOffset + delimeterLength > toRead) {\n+                return -newOffset;\n+            }\n+            // assume will found it\n+            delimiterNotFound = false;\n+            for (int index = 1; index < delimeterLength; index++) {\n+                if (undecodedChunk\n+                        .getByte(startReaderIndex + newOffset + index) !=\n+                    delimiter.codePointAt(index)) {\n+                    // ignore first found offset and redo search from next char\n+                    newOffset++;\n+                    delimiterNotFound = true;\n                     break;\n                 }\n-                continue;\n             }\n-            lastPosition = undecodedChunk.readerIndex();\n-            if (nextByte == HttpConstants.LF) {\n-                index = 0;\n-                lastPosition -= (prevByte == HttpConstants.CR)? 2 : 1;\n-            }\n-            prevByte = nextByte;\n-        }\n-        if (prevByte == HttpConstants.CR) {\n-            lastPosition--;\n         }\n-        ByteBuf content = undecodedChunk.retainedSlice(startReaderIndex, lastPosition - startReaderIndex);\n-        try {\n-            httpData.addContent(content, delimiterFound);\n-        } catch (IOException e) {\n-            throw new ErrorDataDecoderException(e);\n+        if (delimiterNotFound || newOffset + delimeterLength > toRead) {\n+            return -newOffset;\n         }\n-        undecodedChunk.readerIndex(lastPosition);\n-        return delimiterFound;\n+        return newOffset;\n     }\n \n     /**\n      * Load the field value from a Multipart request\n      *\n      * @return {@code true} if the last chunk is loaded (boundary delimiter found), {@code false} if need more chunks\n+     *\n      * @throws ErrorDataDecoderException\n      */\n-    private static boolean loadDataMultipart(ByteBuf undecodedChunk, String delimiter, HttpData httpData) {\n-        if (!undecodedChunk.hasArray()) {\n-            return loadDataMultipartStandard(undecodedChunk, delimiter, httpData);\n-        }\n-        final SeekAheadOptimize sao = new SeekAheadOptimize(undecodedChunk);\n+    private boolean loadDataMultipart(ByteBuf undecodedChunk, String delimiter,\n+                                      HttpData httpData) {\n         final int startReaderIndex = undecodedChunk.readerIndex();\n-        final int delimeterLength = delimiter.length();\n-        int index = 0;\n-        int lastRealPos = sao.pos;\n-        byte prevByte = HttpConstants.LF;\n-        boolean delimiterFound = false;\n-        while (sao.pos < sao.limit) {\n-            final byte nextByte = sao.bytes[sao.pos++];\n-            // Check the delimiter\n-            if (prevByte == HttpConstants.LF && nextByte == delimiter.codePointAt(index)) {\n-                index++;\n-                if (delimeterLength == index) {\n-                    delimiterFound = true;\n-                    break;\n-                }\n-                continue;\n-            }\n-            lastRealPos = sao.pos;\n-            if (nextByte == HttpConstants.LF) {\n-                index = 0;\n-                lastRealPos -= (prevByte == HttpConstants.CR)? 2 : 1;\n-            }\n-            prevByte = nextByte;\n+        int newOffset =\n+            findDelimiter(undecodedChunk, delimiter, lastDataPosition);\n+        if (newOffset < 0) {\n+            // delimiter not found\n+            lastDataPosition = -newOffset;\n+            return false;\n         }\n-        if (prevByte == HttpConstants.CR) {\n-            lastRealPos--;\n+        // found delimiter but still need to check if CRLF before\n+        int startDelimiter = newOffset;\n+        if (undecodedChunk.getByte(startReaderIndex + startDelimiter - 1) ==\n+            HttpConstants.LF) {\n+            startDelimiter--;\n+            if (undecodedChunk.getByte(startReaderIndex + startDelimiter - 1) ==\n+                HttpConstants.CR) {", "originalCommit": "00565439760dce1a5cfdafc542e94311dfeca68b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzM4MzMxMA==", "url": "https://github.com/netty/netty/pull/10623#discussion_r503383310", "bodyText": "Right, wrong indentation through IJ", "author": "fredericBregier", "createdAt": "2020-10-12T15:46:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI0NjQxMA=="}], "type": "inlineReview"}, {"oid": "785e9d99946ad25e2cfc1b2b248dfdadb04d59d4", "url": "https://github.com/netty/netty/commit/785e9d99946ad25e2cfc1b2b248dfdadb04d59d4", "message": "Fix for performance regression on HttpPost RequestDecoder\n\nFix issue #10508 where PARANOID mode slow down about 1000 times compared to ADVANCED.\n\nReasons were:\n\nToo many `readByte()` method calls while other ways exist (such as keep in memory the last scan position when trying to find a delimiter or using `bytesBefore(firstByte)` instead of looping externally).\n\nChanges done:\n- major change on way buffer are parsed: instead of read byte per byte until found delimiter, try to find the delimiter using `bytesBefore()` and keep the last unfound position to skeep already parsed parts (algorithms are the same but implementation of scan are different)\n\nObservations using Async-Profiler:\n==================================\n\n1) Without optimizations, most of the time (more than 95%) is through `readByte()` method within `loadDataMultipartStandard` method.\n2) With using `bytesBefore(byte)` instead of `readByte()` to find various delimiter, the `loadDataMultipartStandard` method is going down to 19 to 33% depending on the test used. the `readByte()` method or equivalent `getByte(pos)` method are going down to 15% (from 95%).\n\nTimes are confirming those profiling:\n- With all optimizations (including buffer reuse), in SIMPLE mode about 82% better, in ADVANCED mode about 79% better and in PARANOID mode about 99% better (most of the duplicate read accesses are removed or make internally through `bytesBefore(byte)` method)\n\nA benchmark is added to show the behavior of the various cases (one big item, such as File upload, and many items) and various level of detection (Disabled, Simple, Advanced, Paranoid). This benchmark is intend to alert if new implementations make too many differences (such as the previous version where about PARANOID gives about 1000 times slower than other levels, while it is now about at most 10 times).\n\nExtract of Benchmark run:\n=========================\n\nRun complete. Total time: 00:13:27\n\nBenchmark                                                                           Mode  Cnt  Score   Error   Units\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigAdvancedLevel   thrpt    6  2,925 \u00b1 0,233  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigDisabledLevel   thrpt    6  3,498 \u00b1 1,484  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigParanoidLevel   thrpt    6  1,443 \u00b1 0,027  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigSimpleLevel     thrpt    6  2,983 \u00b1 0,103  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighAdvancedLevel  thrpt    6  1,690 \u00b1 0,060  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighDisabledLevel  thrpt    6  1,935 \u00b1 0,030  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighParanoidLevel  thrpt    6  0,201 \u00b1 0,005  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighSimpleLevel    thrpt    6  1,858 \u00b1 0,055  ops/ms", "committedDate": "2020-10-12T16:15:34Z", "type": "forcePushed"}, {"oid": "7b14cfb7c113f916dd019d623912c19fff66ba04", "url": "https://github.com/netty/netty/commit/7b14cfb7c113f916dd019d623912c19fff66ba04", "message": "Create package-info.java", "committedDate": "2020-10-26T13:23:13Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk1OTAzOA==", "url": "https://github.com/netty/netty/pull/10623#discussion_r511959038", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             *   http://www.apache.org/licenses/LICENSE-2.0\n          \n          \n            \n             *   https://www.apache.org/licenses/LICENSE-2.0", "author": "chrisvest", "createdAt": "2020-10-26T13:29:38Z", "path": "microbench/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoderBenchmark.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Copyright 2020 The Netty Project\n+ *\n+ * The Netty Project licenses this file to you under the Apache License,\n+ * version 2.0 (the \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at:\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0", "originalCommit": "7b14cfb7c113f916dd019d623912c19fff66ba04", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MTAyOA==", "url": "https://github.com/netty/netty/pull/10623#discussion_r511961028", "bodyText": "Already done ;-)", "author": "fredericBregier", "createdAt": "2020-10-26T13:32:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk1OTAzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MjY0Ng==", "url": "https://github.com/netty/netty/pull/10623#discussion_r511962646", "bodyText": "nit: we use 4 spaces ... Please change everywhere to be consistent with our code-styling", "author": "normanmaurer", "createdAt": "2020-10-26T13:34:46Z", "path": "microbench/src/main/java/io/netty/handler/codec/http/multipart/HttpPostMultipartRequestDecoderBenchmark.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Copyright 2020 The Netty Project\n+ *\n+ * The Netty Project licenses this file to you under the Apache License,\n+ * version 2.0 (the \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at:\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package io.netty.handler.codec.http.multipart;\n+\n+import io.netty.buffer.ByteBuf;\n+import io.netty.buffer.Unpooled;\n+import io.netty.handler.codec.http.DefaultHttpContent;\n+import io.netty.handler.codec.http.DefaultHttpRequest;\n+import io.netty.handler.codec.http.DefaultLastHttpContent;\n+import io.netty.handler.codec.http.HttpHeaderNames;\n+import io.netty.handler.codec.http.HttpMethod;\n+import io.netty.handler.codec.http.HttpVersion;\n+import io.netty.microbench.util.AbstractMicrobenchmark;\n+import io.netty.util.ResourceLeakDetector;\n+import io.netty.util.ResourceLeakDetector.Level;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Threads;\n+import org.openjdk.jmh.annotations.Warmup;\n+\n+import java.util.concurrent.TimeUnit;\n+\n+\n+@Threads(1)\n+@Warmup(iterations = 2)\n+@Measurement(iterations = 3)\n+@OutputTimeUnit(TimeUnit.MILLISECONDS)\n+public class HttpPostMultipartRequestDecoderBenchmark\n+    extends AbstractMicrobenchmark {\n+\n+  public double testHighNumberChunks(boolean big, boolean noDisk) {\n+    String BOUNDARY = \"01f136d9282f\";\n+    int size = 8 * 1024;\n+    int chunkNumber = 64;\n+    StringBuilder stringBuilder = new StringBuilder(size);\n+    stringBuilder.setLength(size);\n+    String data = stringBuilder.toString();\n+\n+    byte[] bodyStartBytes = (\"--\" + BOUNDARY + \"\\n\" +\n+                             \"Content-Disposition: form-data; name=\\\"msg_id\\\"\\n\\n15200\\n--\" +\n+                             BOUNDARY +\n+                             \"\\nContent-Disposition: form-data; name=\\\"msg1\\\"; filename=\\\"file1.txt\\\"\\n\\n\" +\n+                             data).getBytes();\n+    byte[] bodyPartBigBytes = data.getBytes();\n+    byte[] intermediaryBytes = (\"\\n--\" + BOUNDARY +\n+                                \"\\nContent-Disposition: form-data; name=\\\"msg2\\\"; filename=\\\"file2.txt\\\"\\n\\n\" +\n+                                data).getBytes();\n+    byte[] finalBigBytes = (\"\\n\" + \"--\" + BOUNDARY + \"--\\n\").getBytes();\n+    ByteBuf firstBuf = Unpooled.wrappedBuffer(bodyStartBytes);\n+    ByteBuf finalBuf = Unpooled.wrappedBuffer(finalBigBytes);\n+    ByteBuf nextBuf;\n+    if (big) {\n+      nextBuf = Unpooled.wrappedBuffer(bodyPartBigBytes);\n+    } else {\n+      nextBuf = Unpooled.wrappedBuffer(intermediaryBytes);\n+    }\n+    DefaultHttpRequest req =\n+        new DefaultHttpRequest(HttpVersion.HTTP_1_0, HttpMethod.POST, \"/up\");\n+    req.headers().add(HttpHeaderNames.CONTENT_TYPE,\n+                      \"multipart/form-data; boundary=\" + BOUNDARY);\n+\n+    long start = System.nanoTime();\n+\n+    DefaultHttpDataFactory defaultHttpDataFactory =\n+        new DefaultHttpDataFactory(noDisk? 1024 * 1024 : 16 * 1024);\n+    HttpPostRequestDecoder decoder =\n+        new HttpPostRequestDecoder(defaultHttpDataFactory, req);\n+    firstBuf.retain();\n+    decoder.offer(new DefaultHttpContent(firstBuf));\n+    firstBuf.release();\n+    for (int i = 1; i < chunkNumber; i++) {\n+      nextBuf.retain();\n+      decoder.offer(new DefaultHttpContent(nextBuf));\n+      nextBuf.release();\n+      nextBuf.readerIndex(0);\n+    }\n+    finalBuf.retain();\n+    decoder.offer(new DefaultLastHttpContent(finalBuf));\n+    finalBuf.release();\n+    while (decoder.hasNext()) {\n+      InterfaceHttpData httpData = decoder.next();\n+    }\n+    while (finalBuf.refCnt() > 0) {\n+      finalBuf.release();\n+    }\n+    while (nextBuf.refCnt() > 0) {\n+      nextBuf.release();\n+    }\n+    while (finalBuf.refCnt() > 0) {\n+      finalBuf.release();\n+    }\n+    long stop = System.nanoTime();\n+    double time = (stop - start) / 1000000.0;\n+    defaultHttpDataFactory.cleanAllHttpData();\n+    defaultHttpDataFactory.cleanRequestHttpData(req);\n+    decoder.destroy();\n+    return time;\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderHighDisabledLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();\n+    try {\n+      ResourceLeakDetector.setLevel(Level.DISABLED);\n+      return testHighNumberChunks(false, true);\n+    } finally {\n+      ResourceLeakDetector.setLevel(level);\n+    }\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderBigDisabledLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();\n+    try {\n+      ResourceLeakDetector.setLevel(Level.DISABLED);\n+      return testHighNumberChunks(true, true);\n+    } finally {\n+      ResourceLeakDetector.setLevel(level);\n+    }\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderHighSimpleLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();\n+    try {\n+      ResourceLeakDetector.setLevel(Level.SIMPLE);\n+      return testHighNumberChunks(false, true);\n+    } finally {\n+      ResourceLeakDetector.setLevel(level);\n+    }\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderBigSimpleLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();\n+    try {\n+      ResourceLeakDetector.setLevel(Level.SIMPLE);\n+      return testHighNumberChunks(true, true);\n+    } finally {\n+      ResourceLeakDetector.setLevel(level);\n+    }\n+  }\n+\n+  @Benchmark\n+  public double multipartRequestDecoderHighAdvancedLevel() {\n+    final Level level = ResourceLeakDetector.getLevel();", "originalCommit": "7b14cfb7c113f916dd019d623912c19fff66ba04", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2NTY0OA==", "url": "https://github.com/netty/netty/pull/10623#discussion_r511965648", "bodyText": "OK, I will double check.", "author": "fredericBregier", "createdAt": "2020-10-26T13:39:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MjY0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjk3MDg3MA==", "url": "https://github.com/netty/netty/pull/10623#discussion_r512970870", "bodyText": "Done", "author": "fredericBregier", "createdAt": "2020-10-27T19:30:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MjY0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzM1MA==", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963350", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "author": "normanmaurer", "createdAt": "2020-10-26T13:35:42Z", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -438,7 +441,7 @@ private void parseBodyAttributesStandard() {\n                     if (read == '&') {\n                         currentStatus = MultiPartStatus.DISPOSITION;\n                         ampersandpos = currentpos - 1;\n-                        setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                        setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "originalCommit": "7b14cfb7c113f916dd019d623912c19fff66ba04", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk3MDM1NQ==", "url": "https://github.com/netty/netty/pull/10623#discussion_r511970355", "bodyText": "As I wrote, there is a \"old\" bug, rarely catched, when the buffer undecodedChunk exceed the memory limit. When it reached this limit, there is a call to undecodedChunk.discardReadBytes();. I found this bug accidently, through my other new calls to the very same methods and errors in unit testing.\nWhen this call occurs, the underlying buffers are changed, changing all HttpData values (of course, incorrectly).\nTo prevent this, I propose to copy it. I know this is higher in memory copy, but correct from final values, instead of current status.\nAlso, I changed this \"high limit only\" call to \"each time it is relevant\" (when new data can be added while there are enough space within the current buffer to fill it with this new data).\nAnother way would be to igonre totally this high limit and therefore not calling, nowhere, this undecodedChunk.discardReadBytes();, in order to keep final consistency.\nWDYT?", "author": "fredericBregier", "createdAt": "2020-10-26T13:45:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzM1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk3NTkxMg==", "url": "https://github.com/netty/netty/pull/10623#discussion_r511975912", "bodyText": "Of course, the second option (no more calls to discardReadBytes()) would lead to higher memory usage since the underlying buffer will be kept until the end.\nUsing \"to disk\" (or mixed way) management of HttpData can decrease this memory pressure, espcially for File upload (but not limited to), but as in mixed way, some HttpData could stay using the original undecodedChunk, this could lead in wrong data at the end, so my original proposition.\nAny direction or idea ?", "author": "fredericBregier", "createdAt": "2020-10-26T13:53:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzM1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjE4OTAwMQ==", "url": "https://github.com/netty/netty/pull/10623#discussion_r512189001", "bodyText": "I think we should only call undecoded.discardReadBytes() if refCnt() == 1.", "author": "normanmaurer", "createdAt": "2020-10-26T18:43:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzM1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjIwNzA5NQ==", "url": "https://github.com/netty/netty/pull/10623#discussion_r512207095", "bodyText": "It could be a good idea !\nI know this will be almost never true (due to retainedSlice() that will increment refCnt()), but at least, it will give a chance for it (as with SeekAhead algorithm that is almost never usable) and obviously limits the copy as much as possible.\nI will git it a try. Thanks @normanmaurer for this idea !", "author": "fredericBregier", "createdAt": "2020-10-26T19:14:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzM1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjk3MTEzOQ==", "url": "https://github.com/netty/netty/pull/10623#discussion_r512971139", "bodyText": "Done:\nNew benchmark results:\nBenchmark                                                                           Mode  Cnt  Score   Error   Units\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigAdvancedLevel   thrpt    6  2,248 \u00b1 0,198  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigDisabledLevel   thrpt    6  2,067 \u00b1 1,219  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigParanoidLevel   thrpt    6  1,109 \u00b1 0,038  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigSimpleLevel     thrpt    6  2,326 \u00b1 0,314  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighAdvancedLevel  thrpt    6  1,444 \u00b1 0,226  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighDisabledLevel  thrpt    6  1,462 \u00b1 0,642  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighParanoidLevel  thrpt    6  0,159 \u00b1 0,003  ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighSimpleLevel    thrpt    6  1,522 \u00b1 0,049  ops/ms\nAlmost the same as in previous code.", "author": "fredericBregier", "createdAt": "2020-10-27T19:30:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzM1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzM5NA==", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963394", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "author": "normanmaurer", "createdAt": "2020-10-26T13:35:47Z", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -461,7 +464,7 @@ private void parseBodyAttributesStandard() {\n                     } else if (read == HttpConstants.LF) {\n                         currentStatus = MultiPartStatus.PREEPILOGUE;\n                         ampersandpos = currentpos - 1;\n-                        setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                        setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "originalCommit": "7b14cfb7c113f916dd019d623912c19fff66ba04", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzQ1Mg==", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963452", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "author": "normanmaurer", "createdAt": "2020-10-26T13:35:53Z", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -475,16 +478,15 @@ private void parseBodyAttributesStandard() {\n                 // special case\n                 ampersandpos = currentpos;\n                 if (ampersandpos > firstpos) {\n-                    setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                    setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "originalCommit": "7b14cfb7c113f916dd019d623912c19fff66ba04", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzUwOQ==", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963509", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "author": "normanmaurer", "createdAt": "2020-10-26T13:35:58Z", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -475,16 +478,15 @@ private void parseBodyAttributesStandard() {\n                 // special case\n                 ampersandpos = currentpos;\n                 if (ampersandpos > firstpos) {\n-                    setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                    setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));\n                 } else if (!currentAttribute.isCompleted()) {\n                     setFinalBuffer(Unpooled.EMPTY_BUFFER);\n                 }\n                 firstpos = currentpos;\n                 currentStatus = MultiPartStatus.EPILOGUE;\n             } else if (contRead && currentAttribute != null && currentStatus == MultiPartStatus.FIELD) {\n                 // reset index except if to continue in case of FIELD getStatus\n-                currentAttribute.addContent(undecodedChunk.retainedSlice(firstpos, currentpos - firstpos),\n-                                            false);\n+                currentAttribute.addContent(undecodedChunk.copy(firstpos, currentpos - firstpos), false);", "originalCommit": "7b14cfb7c113f916dd019d623912c19fff66ba04", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzU4MQ==", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963581", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "author": "normanmaurer", "createdAt": "2020-10-26T13:36:03Z", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -558,7 +560,7 @@ private void parseBodyAttributes() {\n                     if (read == '&') {\n                         currentStatus = MultiPartStatus.DISPOSITION;\n                         ampersandpos = currentpos - 1;\n-                        setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                        setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "originalCommit": "7b14cfb7c113f916dd019d623912c19fff66ba04", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2Mzc1Mw==", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963753", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "author": "normanmaurer", "createdAt": "2020-10-26T13:36:19Z", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -569,7 +571,7 @@ private void parseBodyAttributes() {\n                                 currentStatus = MultiPartStatus.PREEPILOGUE;\n                                 ampersandpos = currentpos - 2;\n                                 sao.setReadPosition(0);\n-                                setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                                setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "originalCommit": "7b14cfb7c113f916dd019d623912c19fff66ba04", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzgxNA==", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963814", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "author": "normanmaurer", "createdAt": "2020-10-26T13:36:24Z", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -587,7 +589,7 @@ private void parseBodyAttributes() {\n                         currentStatus = MultiPartStatus.PREEPILOGUE;\n                         ampersandpos = currentpos - 1;\n                         sao.setReadPosition(0);\n-                        setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                        setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "originalCommit": "7b14cfb7c113f916dd019d623912c19fff66ba04", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2Mzg2OQ==", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963869", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "author": "normanmaurer", "createdAt": "2020-10-26T13:36:29Z", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -604,16 +606,15 @@ private void parseBodyAttributes() {\n                 // special case\n                 ampersandpos = currentpos;\n                 if (ampersandpos > firstpos) {\n-                    setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                    setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));", "originalCommit": "7b14cfb7c113f916dd019d623912c19fff66ba04", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2MzkxNA==", "url": "https://github.com/netty/netty/pull/10623#discussion_r511963914", "bodyText": "hmm... retainedSlice(...) should be fine. This will do an extra memory copy which I think is not needed.", "author": "normanmaurer", "createdAt": "2020-10-26T13:36:34Z", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -604,16 +606,15 @@ private void parseBodyAttributes() {\n                 // special case\n                 ampersandpos = currentpos;\n                 if (ampersandpos > firstpos) {\n-                    setFinalBuffer(undecodedChunk.retainedSlice(firstpos, ampersandpos - firstpos));\n+                    setFinalBuffer(undecodedChunk.copy(firstpos, ampersandpos - firstpos));\n                 } else if (!currentAttribute.isCompleted()) {\n                     setFinalBuffer(Unpooled.EMPTY_BUFFER);\n                 }\n                 firstpos = currentpos;\n                 currentStatus = MultiPartStatus.EPILOGUE;\n             } else if (contRead && currentAttribute != null && currentStatus == MultiPartStatus.FIELD) {\n                 // reset index except if to continue in case of FIELD getStatus\n-                currentAttribute.addContent(undecodedChunk.retainedSlice(firstpos, currentpos - firstpos),\n-                                            false);\n+                currentAttribute.addContent(undecodedChunk.copy(firstpos, currentpos - firstpos), false);", "originalCommit": "7b14cfb7c113f916dd019d623912c19fff66ba04", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2NDI1Ng==", "url": "https://github.com/netty/netty/pull/10623#discussion_r511964256", "bodyText": "I think the b.readable() for the second arg is not needed.", "author": "normanmaurer", "createdAt": "2020-10-26T13:37:02Z", "path": "codec-http/src/main/java/io/netty/handler/codec/http/multipart/HttpPostStandardRequestDecoder.java", "diffHunk": "@@ -661,7 +662,7 @@ private static ByteBuf decodeAttribute(ByteBuf b, Charset charset) {\n             return null; // nothing to decode\n         }\n \n-        ByteBuf buf = b.alloc().buffer(b.readableBytes());\n+        ByteBuf buf = b.alloc().buffer(b.readableBytes(), b.readableBytes());", "originalCommit": "7b14cfb7c113f916dd019d623912c19fff66ba04", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk3MTI3Ng==", "url": "https://github.com/netty/netty/pull/10623#discussion_r511971276", "bodyText": "Not mandatory, but there, it is valid (there is no need to allocate more than this in this specific case).\nIf you prefer, I could remove it (while valid).", "author": "fredericBregier", "createdAt": "2020-10-26T13:46:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk2NDI1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzIzMg==", "url": "https://github.com/netty/netty/pull/10623#discussion_r517433232", "bodyText": "seems like a leak in the test that was not related to your change... correct ?", "author": "normanmaurer", "createdAt": "2020-11-04T15:36:30Z", "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -704,6 +710,7 @@ public void testDecodeMalformedEmptyContentTypeFieldParameters() throws Exceptio\n         assertTrue(part1 instanceof FileUpload);\n         FileUpload fileUpload = (FileUpload) part1;\n         assertEquals(\"tmp-0.txt\", fileUpload.getFilename());\n+        req.release();", "originalCommit": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQ4MDUzNQ==", "url": "https://github.com/netty/netty/pull/10623#discussion_r517480535", "bodyText": "Yes, not related. I was just getting from time to time (before and after changes) an error in PARANOID mode, so I tried to fix as much as possible the tests to be more coherent.\nThe reason is: as the test is not going through network but directly, so there is no handler that take care of incoming messages, there is no clean of the request after receiving it by the main handler as it should.", "author": "fredericBregier", "createdAt": "2020-11-04T16:41:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzIzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzMyMw==", "url": "https://github.com/netty/netty/pull/10623#discussion_r517433323", "bodyText": "seems like a leak in the test that was not related to your change... correct ?", "author": "normanmaurer", "createdAt": "2020-11-04T15:36:40Z", "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -427,6 +432,7 @@ public void testMultipartRequestWithoutContentTypeBody() {\n         // Create decoder instance to test without any exception.\n         final HttpPostRequestDecoder decoder = new HttpPostRequestDecoder(inMemoryFactory, req);\n         assertFalse(decoder.getBodyHttpDatas().isEmpty());\n+        req.release();", "originalCommit": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQ4MDk1Ng==", "url": "https://github.com/netty/netty/pull/10623#discussion_r517480956", "bodyText": "Same answear: not related, just fix the test", "author": "fredericBregier", "createdAt": "2020-11-04T16:41:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzMyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzM1Ng==", "url": "https://github.com/netty/netty/pull/10623#discussion_r517433356", "bodyText": "seems like a leak in the test that was not related to your change... correct ?", "author": "normanmaurer", "createdAt": "2020-11-04T15:36:44Z", "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -397,6 +401,7 @@ public void testFilenameContainingSemicolon2() throws Exception {\n         assertTrue(part1 instanceof FileUpload);\n         FileUpload fileUpload = (FileUpload) part1;\n         assertEquals(\"tmp 0.txt\", fileUpload.getFilename());\n+        req.release();", "originalCommit": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQ4MDg2Nw==", "url": "https://github.com/netty/netty/pull/10623#discussion_r517480867", "bodyText": "Same answear: not related, just fix the test", "author": "fredericBregier", "createdAt": "2020-11-04T16:41:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzM1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzM5Ng==", "url": "https://github.com/netty/netty/pull/10623#discussion_r517433396", "bodyText": "seems like a leak in the test that was not related to your change... correct ?", "author": "normanmaurer", "createdAt": "2020-11-04T15:36:48Z", "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -368,6 +371,7 @@ public void testFilenameContainingSemicolon() throws Exception {\n         // Create decoder instance to test.\n         final HttpPostRequestDecoder decoder = new HttpPostRequestDecoder(inMemoryFactory, req);\n         assertFalse(decoder.getBodyHttpDatas().isEmpty());\n+        req.release();", "originalCommit": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQ4MTA1MQ==", "url": "https://github.com/netty/netty/pull/10623#discussion_r517481051", "bodyText": "Same answear: not related, just fix the test", "author": "fredericBregier", "createdAt": "2020-11-04T16:42:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzM5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzQ0Mw==", "url": "https://github.com/netty/netty/pull/10623#discussion_r517433443", "bodyText": "seems like a leak in the test that was not related to your change... correct ?", "author": "normanmaurer", "createdAt": "2020-11-04T15:36:52Z", "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -211,6 +213,7 @@ public void testQuotedBoundary() throws Exception {\n         // Create decoder instance to test.\n         final HttpPostRequestDecoder decoder = new HttpPostRequestDecoder(inMemoryFactory, req);\n         assertFalse(decoder.getBodyHttpDatas().isEmpty());\n+        req.release();", "originalCommit": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQ4MTExNA==", "url": "https://github.com/netty/netty/pull/10623#discussion_r517481114", "bodyText": "Same answear: not related, just fix the test", "author": "fredericBregier", "createdAt": "2020-11-04T16:42:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzQ0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzQ4Mw==", "url": "https://github.com/netty/netty/pull/10623#discussion_r517433483", "bodyText": "seems like a leak in the test that was not related to your change... correct ?", "author": "normanmaurer", "createdAt": "2020-11-04T15:36:55Z", "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -178,6 +179,7 @@ public void testMultipartCodecWithCRasEndOfAttribute() throws Exception {\n             assertNotNull(datar);\n             assertEquals(datas[i].getBytes(CharsetUtil.UTF_8).length, datar.length);\n \n+            req.release();", "originalCommit": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQ4MTE4Mw==", "url": "https://github.com/netty/netty/pull/10623#discussion_r517481183", "bodyText": "Same answear: not related, just fix the test", "author": "fredericBregier", "createdAt": "2020-11-04T16:42:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzQ4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzU0NQ==", "url": "https://github.com/netty/netty/pull/10623#discussion_r517433545", "bodyText": "seems like a leak in the test that was not related to your change... correct ?", "author": "normanmaurer", "createdAt": "2020-11-04T15:36:59Z", "path": "codec-http/src/test/java/io/netty/handler/codec/http/multipart/HttpPostRequestDecoderTest.java", "diffHunk": "@@ -132,6 +132,7 @@ public void testFullHttpRequestUpload() throws Exception {\n         // Create decoder instance to test.\n         final HttpPostRequestDecoder decoder = new HttpPostRequestDecoder(inMemoryFactory, req);\n         assertFalse(decoder.getBodyHttpDatas().isEmpty());\n+        req.release();", "originalCommit": "1f97ccc08d4c50c814d826fa89539d4eb0b932fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQ4MTI2MA==", "url": "https://github.com/netty/netty/pull/10623#discussion_r517481260", "bodyText": "Same answear: not related, just fix the test", "author": "fredericBregier", "createdAt": "2020-11-04T16:42:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQzMzU0NQ=="}], "type": "inlineReview"}, {"oid": "03ef3d0090a51cd79ada6fbbed65cda147e0d595", "url": "https://github.com/netty/netty/commit/03ef3d0090a51cd79ada6fbbed65cda147e0d595", "message": "Fix for performance regression on HttpPost RequestDecoder\n\nFix issue #10508 where PARANOID mode slow down about 1000 times compared to ADVANCED.\nAlso fix a rare issue when internal buffer was growing over a limit, it was partially discarded\nusing `discardReadBytes()` which causes bad changes within previously discovered HttpData.\n\nReasons were:\n\nToo many `readByte()` method calls while other ways exist (such as keep in memory the last scan position when trying to find a delimiter or using `bytesBefore(firstByte)` instead of looping externally).\n\nChanges done:\n- major change on way buffer are parsed: instead of read byte per byte until found delimiter, try to find the delimiter using `bytesBefore()` and keep the last unfound position to skeep already parsed parts (algorithms are the same but implementation of scan are different)\n- Change the condition to discard read bytes when refCnt is at most 1.\n\nObservations using Async-Profiler:\n==================================\n\n1) Without optimizations, most of the time (more than 95%) is through `readByte()` method within `loadDataMultipartStandard` method.\n2) With using `bytesBefore(byte)` instead of `readByte()` to find various delimiter, the `loadDataMultipartStandard` method is going down to 19 to 33% depending on the test used. the `readByte()` method or equivalent `getByte(pos)` method are going down to 15% (from 95%).\n\nTimes are confirming those profiling:\n- With optimizations, in SIMPLE mode about 82% better, in ADVANCED mode about 79% better and in PARANOID mode about 99% better (most of the duplicate read accesses are removed or make internally through `bytesBefore(byte)` method)\n\nA benchmark is added to show the behavior of the various cases (one big item, such as File upload, and many items) and various level of detection (Disabled, Simple, Advanced, Paranoid). This benchmark is intend to alert if new implementations make too many differences (such as the previous version where about PARANOID gives about 1000 times slower than other levels, while it is now about at most 10 times).\n\nExtract of Benchmark run:\n=========================\n\nRun complete. Total time: 00:13:27\n\nBenchmark                                                                           Mode  Cnt  Score   Error   Units\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigAdvancedLevel   thrpt    6  2,248 \u00b1 0,198 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigDisabledLevel   thrpt    6  2,067 \u00b1 1,219 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigParanoidLevel   thrpt    6  1,109 \u00b1 0,038 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigSimpleLevel     thrpt    6  2,326 \u00b1 0,314 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighAdvancedLevel  thrpt    6  1,444 \u00b1 0,226 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighDisabledLevel  thrpt    6  1,462 \u00b1 0,642 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighParanoidLevel  thrpt    6  0,159 \u00b1 0,003 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighSimpleLevel    thrpt    6  1,522 \u00b1 0,049 ops/ms", "committedDate": "2020-11-18T14:21:49Z", "type": "commit"}, {"oid": "03ef3d0090a51cd79ada6fbbed65cda147e0d595", "url": "https://github.com/netty/netty/commit/03ef3d0090a51cd79ada6fbbed65cda147e0d595", "message": "Fix for performance regression on HttpPost RequestDecoder\n\nFix issue #10508 where PARANOID mode slow down about 1000 times compared to ADVANCED.\nAlso fix a rare issue when internal buffer was growing over a limit, it was partially discarded\nusing `discardReadBytes()` which causes bad changes within previously discovered HttpData.\n\nReasons were:\n\nToo many `readByte()` method calls while other ways exist (such as keep in memory the last scan position when trying to find a delimiter or using `bytesBefore(firstByte)` instead of looping externally).\n\nChanges done:\n- major change on way buffer are parsed: instead of read byte per byte until found delimiter, try to find the delimiter using `bytesBefore()` and keep the last unfound position to skeep already parsed parts (algorithms are the same but implementation of scan are different)\n- Change the condition to discard read bytes when refCnt is at most 1.\n\nObservations using Async-Profiler:\n==================================\n\n1) Without optimizations, most of the time (more than 95%) is through `readByte()` method within `loadDataMultipartStandard` method.\n2) With using `bytesBefore(byte)` instead of `readByte()` to find various delimiter, the `loadDataMultipartStandard` method is going down to 19 to 33% depending on the test used. the `readByte()` method or equivalent `getByte(pos)` method are going down to 15% (from 95%).\n\nTimes are confirming those profiling:\n- With optimizations, in SIMPLE mode about 82% better, in ADVANCED mode about 79% better and in PARANOID mode about 99% better (most of the duplicate read accesses are removed or make internally through `bytesBefore(byte)` method)\n\nA benchmark is added to show the behavior of the various cases (one big item, such as File upload, and many items) and various level of detection (Disabled, Simple, Advanced, Paranoid). This benchmark is intend to alert if new implementations make too many differences (such as the previous version where about PARANOID gives about 1000 times slower than other levels, while it is now about at most 10 times).\n\nExtract of Benchmark run:\n=========================\n\nRun complete. Total time: 00:13:27\n\nBenchmark                                                                           Mode  Cnt  Score   Error   Units\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigAdvancedLevel   thrpt    6  2,248 \u00b1 0,198 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigDisabledLevel   thrpt    6  2,067 \u00b1 1,219 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigParanoidLevel   thrpt    6  1,109 \u00b1 0,038 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderBigSimpleLevel     thrpt    6  2,326 \u00b1 0,314 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighAdvancedLevel  thrpt    6  1,444 \u00b1 0,226 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighDisabledLevel  thrpt    6  1,462 \u00b1 0,642 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighParanoidLevel  thrpt    6  0,159 \u00b1 0,003 ops/ms\nHttpPostMultipartRequestDecoderBenchmark.multipartRequestDecoderHighSimpleLevel    thrpt    6  1,522 \u00b1 0,049 ops/ms", "committedDate": "2020-11-18T14:21:49Z", "type": "forcePushed"}]}