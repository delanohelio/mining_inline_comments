{"pr_number": 10082, "pr_title": "Let LzfEncoder support length aware ability.", "pr_createdAt": "2020-03-04T03:18:20Z", "pr_url": "https://github.com/netty/netty/pull/10082", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDM0OTM5Ng==", "url": "https://github.com/netty/netty/pull/10082#discussion_r390349396", "bodyText": "we need to also keep the old constructor for backward combat.", "author": "normanmaurer", "createdAt": "2020-03-10T14:22:17Z", "path": "codec/src/main/java/io/netty/handler/codec/compression/LzfEncoder.java", "diffHunk": "@@ -55,58 +70,73 @@\n      * non-standard platforms it may be necessary to use {@link #LzfEncoder(boolean)} with {@code true} param.\n      */\n     public LzfEncoder() {\n-        this(false, MAX_CHUNK_LEN);\n+        this(false, MAX_CHUNK_LEN, -1);\n     }\n \n     /**\n      * Creates a new LZF encoder with specified encoding instance.\n      *\n-     * @param safeInstance\n-     *        If {@code true} encoder will use {@link ChunkEncoder} that only uses standard JDK access methods,\n-     *        and should work on all Java platforms and JVMs.\n-     *        Otherwise encoder will try to use highly optimized {@link ChunkEncoder} implementation that uses\n-     *        Sun JDK's {@link sun.misc.Unsafe} class (which may be included by other JDK's as well).\n+     * @param safeInstance If {@code true} encoder will use {@link ChunkEncoder} that only uses\n+     *                     standard JDK access methods, and should work on all Java platforms and JVMs.\n+     *                     Otherwise encoder will try to use highly optimized {@link ChunkEncoder}\n+     *                     implementation that uses Sun JDK's {@link sun.misc.Unsafe}\n+     *                     class (which may be included by other JDK's as well).\n      */\n     public LzfEncoder(boolean safeInstance) {\n-        this(safeInstance, MAX_CHUNK_LEN);\n+        this(safeInstance, MAX_CHUNK_LEN, -1);\n+    }\n+\n+    /**\n+     * Creates a new LZF encoder with specified encoding instance and compressThreshold.\n+     *\n+     * @param safeInstance      If {@code true} encoder will use {@link ChunkEncoder} that only uses standard\n+     *                          JDK access methods, and should work on all Java platforms and JVMs.\n+     *                          Otherwise encoder will try to use highly optimized {@link ChunkEncoder}\n+     *                          implementation that uses Sun JDK's {@link sun.misc.Unsafe}\n+     *                          class (which may be included by other JDK's as well).\n+     * @param compressThreshold compress threshold for compression. see {@link #compressThreshold}.\n+     */\n+    public LzfEncoder(boolean safeInstance, int compressThreshold) {\n+        this(safeInstance, MAX_CHUNK_LEN, compressThreshold);\n     }\n \n     /**\n      * Creates a new LZF encoder with specified total length of encoded chunk. You can configure it to encode\n      * your data flow more efficient if you know the average size of messages that you send.\n      *\n-     * @param totalLength\n-     *        Expected total length of content to compress; only matters for outgoing messages that is smaller\n-     *        than maximum chunk size (64k), to optimize encoding hash tables.\n+     * @param totalLength Expected total length of content to compress;\n+     *                    only matters for outgoing messages that is smaller than maximum chunk size (64k),\n+     *                    to optimize encoding hash tables.\n      */\n     public LzfEncoder(int totalLength) {\n-        this(false, totalLength);\n+        this(false, totalLength, -1);\n     }\n \n     /**\n      * Creates a new LZF encoder with specified settings.\n      *\n-     * @param safeInstance\n-     *        If {@code true} encoder will use {@link ChunkEncoder} that only uses standard JDK access methods,\n-     *        and should work on all Java platforms and JVMs.\n-     *        Otherwise encoder will try to use highly optimized {@link ChunkEncoder} implementation that uses\n-     *        Sun JDK's {@link sun.misc.Unsafe} class (which may be included by other JDK's as well).\n-     * @param totalLength\n-     *        Expected total length of content to compress; only matters for outgoing messages that is smaller\n-     *        than maximum chunk size (64k), to optimize encoding hash tables.\n+     * @param safeInstance If {@code true} encoder will use {@link ChunkEncoder} that only uses standard JDK\n+     *                     access methods, and should work on all Java platforms and JVMs.\n+     *                     Otherwise encoder will try to use highly optimized {@link ChunkEncoder}\n+     *                     implementation that uses Sun JDK's {@link sun.misc.Unsafe}\n+     *                     class (which may be included by other JDK's as well).\n+     * @param totalLength  Expected total length of content to compress; only matters for outgoing messages\n+     *                     that is smaller than maximum chunk size (64k), to optimize encoding hash tables.\n      */\n-    public LzfEncoder(boolean safeInstance, int totalLength) {\n+    public LzfEncoder(boolean safeInstance, int totalLength, int compressThreshold) {", "originalCommit": "9b3ff30a4df57d3ceda77a19d33af709ad50e3be", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDM1MDcyOA==", "url": "https://github.com/netty/netty/pull/10082#discussion_r390350728", "bodyText": "I would prefer to not allow this for now... I think there is really no good reason to adjust this on the fly.", "author": "normanmaurer", "createdAt": "2020-03-10T14:23:57Z", "path": "codec/src/main/java/io/netty/handler/codec/compression/LzfEncoder.java", "diffHunk": "@@ -138,6 +176,48 @@ protected void encode(ChannelHandlerContext ctx, ByteBuf in, ByteBuf out) throws\n         }\n     }\n \n+    private int encodeCompress(byte[] input, int inputPtr, int length, byte[] output, int outputPtr) {\n+        return LZFEncoder.appendEncoded(encoder,\n+                input, inputPtr, length, output, outputPtr) - outputPtr;\n+    }\n+\n+    /**\n+     * Use lzf uncompressed format to encode a piece of input.\n+     */\n+    private static int encodeNonCompress(byte[] input, int inputPtr, int length, byte[] output, int outputPtr) {\n+        int left = length;\n+        int chunkLen = Math.min(LZFChunk.MAX_CHUNK_LEN, left);\n+        outputPtr = LZFChunk.appendNonCompressed(input, inputPtr, length, output, outputPtr);\n+        left -= chunkLen;\n+        if (left < 1) {\n+            return outputPtr;\n+        }\n+        inputPtr += chunkLen;\n+        do {\n+            chunkLen = Math.min(left, LZFChunk.MAX_CHUNK_LEN);\n+            outputPtr = LZFChunk.appendNonCompressed(input, inputPtr, length, output, outputPtr);\n+            inputPtr += chunkLen;\n+            left -= chunkLen;\n+        } while (left > 0);\n+        return outputPtr;\n+    }\n+\n+    public int getCompressThreshold() {\n+        return compressThreshold;\n+    }\n+\n+    /**\n+     * Since we could set this threshold at runtime, so we keep set method.\n+     */\n+    public void setCompressThreshold(int compressThreshold) {", "originalCommit": "9b3ff30a4df57d3ceda77a19d33af709ad50e3be", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDM1MTA5OA==", "url": "https://github.com/netty/netty/pull/10082#discussion_r390351098", "bodyText": "remove...", "author": "normanmaurer", "createdAt": "2020-03-10T14:24:24Z", "path": "codec/src/test/java/io/netty/handler/codec/compression/LengthAwareLzfIntegrationTest.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Copyright 2020 The Netty Project\n+ *\n+ * The Netty Project licenses this file to you under the Apache License,\n+ * version 2.0 (the \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at:\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package io.netty.handler.codec.compression;\n+\n+import io.netty.channel.embedded.EmbeddedChannel;\n+\n+/**\n+ * LengthAwareLzfIntegrationTest", "originalCommit": "9b3ff30a4df57d3ceda77a19d33af709ad50e3be", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "8dc27b7884b8a7653a0c3700c6e499ff6428f2af", "url": "https://github.com/netty/netty/commit/8dc27b7884b8a7653a0c3700c6e499ff6428f2af", "message": "Fix bug", "committedDate": "2020-03-11T03:27:21Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDgxODY2NA==", "url": "https://github.com/netty/netty/pull/10082#discussion_r390818664", "bodyText": "should we validate this on construction and if so throw ?", "author": "normanmaurer", "createdAt": "2020-03-11T08:48:06Z", "path": "codec/src/main/java/io/netty/handler/codec/compression/LzfEncoder.java", "diffHunk": "@@ -17,27 +17,42 @@\n \n import com.ning.compress.BufferRecycler;\n import com.ning.compress.lzf.ChunkEncoder;\n+import com.ning.compress.lzf.LZFChunk;\n import com.ning.compress.lzf.LZFEncoder;\n import com.ning.compress.lzf.util.ChunkEncoderFactory;\n import io.netty.buffer.ByteBuf;\n import io.netty.channel.ChannelHandlerContext;\n import io.netty.handler.codec.MessageToByteEncoder;\n+import io.netty.util.internal.logging.InternalLogger;\n+import io.netty.util.internal.logging.InternalLoggerFactory;\n \n-import static com.ning.compress.lzf.LZFChunk.*;\n+import static com.ning.compress.lzf.LZFChunk.MAX_CHUNK_LEN;\n \n /**\n  * Compresses a {@link ByteBuf} using the LZF format.\n- *\n+ * <p>\n  * See original <a href=\"http://oldhome.schmorp.de/marc/liblzf.html\">LZF package</a>\n  * and <a href=\"https://github.com/ning/compress/wiki/LZFFormat\">LZF format</a> for full description.\n  */\n public class LzfEncoder extends MessageToByteEncoder<ByteBuf> {\n+\n+    private static final InternalLogger logger = InternalLoggerFactory.getInstance(LzfEncoder.class);\n+\n     /**\n      * Minimum block size ready for compression. Blocks with length\n      * less than {@link #MIN_BLOCK_TO_COMPRESS} will write as uncompressed.\n      */\n     private static final int MIN_BLOCK_TO_COMPRESS = 16;\n \n+    /**\n+     * Compress threshold for LZF format. When the amount of input data is less than compressThreshold,\n+     * we will construct an uncompressed output according to the LZF format.\n+     * <p>\n+     * When the value is less than {@see ChunkEncoder#MIN_BLOCK_TO_COMPRESS}, since LZF will not compress data\n+     * that is less than {@see ChunkEncoder#MIN_BLOCK_TO_COMPRESS}, compressThreshold will not work.", "originalCommit": "8dc27b7884b8a7653a0c3700c6e499ff6428f2af", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDgyNTk1Nw==", "url": "https://github.com/netty/netty/pull/10082#discussion_r390825957", "bodyText": "Currently I checked compressThreshold <MIN_BLOCK_TO_COMPRESS. If so, compressThreshold = MIN_BLOCK_TO_COMPRESS, and print a log.\nIt seems that the startup phase informs the user that this is an inappropriate value and is a good choice. fixed...", "author": "carryxyh", "createdAt": "2020-03-11T09:02:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDgxODY2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDgxODkxMA==", "url": "https://github.com/netty/netty/pull/10082#discussion_r390818910", "bodyText": "final", "author": "normanmaurer", "createdAt": "2020-03-11T08:48:35Z", "path": "codec/src/main/java/io/netty/handler/codec/compression/LzfEncoder.java", "diffHunk": "@@ -17,27 +17,42 @@\n \n import com.ning.compress.BufferRecycler;\n import com.ning.compress.lzf.ChunkEncoder;\n+import com.ning.compress.lzf.LZFChunk;\n import com.ning.compress.lzf.LZFEncoder;\n import com.ning.compress.lzf.util.ChunkEncoderFactory;\n import io.netty.buffer.ByteBuf;\n import io.netty.channel.ChannelHandlerContext;\n import io.netty.handler.codec.MessageToByteEncoder;\n+import io.netty.util.internal.logging.InternalLogger;\n+import io.netty.util.internal.logging.InternalLoggerFactory;\n \n-import static com.ning.compress.lzf.LZFChunk.*;\n+import static com.ning.compress.lzf.LZFChunk.MAX_CHUNK_LEN;\n \n /**\n  * Compresses a {@link ByteBuf} using the LZF format.\n- *\n+ * <p>\n  * See original <a href=\"http://oldhome.schmorp.de/marc/liblzf.html\">LZF package</a>\n  * and <a href=\"https://github.com/ning/compress/wiki/LZFFormat\">LZF format</a> for full description.\n  */\n public class LzfEncoder extends MessageToByteEncoder<ByteBuf> {\n+\n+    private static final InternalLogger logger = InternalLoggerFactory.getInstance(LzfEncoder.class);\n+\n     /**\n      * Minimum block size ready for compression. Blocks with length\n      * less than {@link #MIN_BLOCK_TO_COMPRESS} will write as uncompressed.\n      */\n     private static final int MIN_BLOCK_TO_COMPRESS = 16;\n \n+    /**\n+     * Compress threshold for LZF format. When the amount of input data is less than compressThreshold,\n+     * we will construct an uncompressed output according to the LZF format.\n+     * <p>\n+     * When the value is less than {@see ChunkEncoder#MIN_BLOCK_TO_COMPRESS}, since LZF will not compress data\n+     * that is less than {@see ChunkEncoder#MIN_BLOCK_TO_COMPRESS}, compressThreshold will not work.\n+     */\n+    private int compressThreshold;", "originalCommit": "8dc27b7884b8a7653a0c3700c6e499ff6428f2af", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDgxOTE4MA==", "url": "https://github.com/netty/netty/pull/10082#discussion_r390819180", "bodyText": "add javadocs for param compressThreshold as well.", "author": "normanmaurer", "createdAt": "2020-03-11T08:49:07Z", "path": "codec/src/main/java/io/netty/handler/codec/compression/LzfEncoder.java", "diffHunk": "@@ -55,58 +70,79 @@\n      * non-standard platforms it may be necessary to use {@link #LzfEncoder(boolean)} with {@code true} param.\n      */\n     public LzfEncoder() {\n-        this(false, MAX_CHUNK_LEN);\n+        this(false, MAX_CHUNK_LEN, -1);\n     }\n \n     /**\n      * Creates a new LZF encoder with specified encoding instance.\n      *\n-     * @param safeInstance\n-     *        If {@code true} encoder will use {@link ChunkEncoder} that only uses standard JDK access methods,\n-     *        and should work on all Java platforms and JVMs.\n-     *        Otherwise encoder will try to use highly optimized {@link ChunkEncoder} implementation that uses\n-     *        Sun JDK's {@link sun.misc.Unsafe} class (which may be included by other JDK's as well).\n+     * @param safeInstance If {@code true} encoder will use {@link ChunkEncoder} that only uses\n+     *                     standard JDK access methods, and should work on all Java platforms and JVMs.\n+     *                     Otherwise encoder will try to use highly optimized {@link ChunkEncoder}\n+     *                     implementation that uses Sun JDK's {@link sun.misc.Unsafe}\n+     *                     class (which may be included by other JDK's as well).\n      */\n     public LzfEncoder(boolean safeInstance) {\n-        this(safeInstance, MAX_CHUNK_LEN);\n+        this(safeInstance, MAX_CHUNK_LEN, -1);\n+    }\n+\n+    /**\n+     * Creates a new LZF encoder with specified encoding instance and compressThreshold.\n+     *\n+     * @param safeInstance      If {@code true} encoder will use {@link ChunkEncoder} that only uses standard\n+     *                          JDK access methods, and should work on all Java platforms and JVMs.\n+     *                          Otherwise encoder will try to use highly optimized {@link ChunkEncoder}\n+     *                          implementation that uses Sun JDK's {@link sun.misc.Unsafe}\n+     *                          class (which may be included by other JDK's as well).\n+     * @param totalLength       Expected total length of content to compress; only matters for outgoing messages\n+     *                          that is smaller than maximum chunk size (64k), to optimize encoding hash tables.\n+     */\n+    public LzfEncoder(boolean safeInstance, int totalLength) {\n+        this(safeInstance, totalLength, -1);\n     }\n \n     /**\n      * Creates a new LZF encoder with specified total length of encoded chunk. You can configure it to encode\n      * your data flow more efficient if you know the average size of messages that you send.\n      *\n-     * @param totalLength\n-     *        Expected total length of content to compress; only matters for outgoing messages that is smaller\n-     *        than maximum chunk size (64k), to optimize encoding hash tables.\n+     * @param totalLength Expected total length of content to compress;\n+     *                    only matters for outgoing messages that is smaller than maximum chunk size (64k),\n+     *                    to optimize encoding hash tables.\n      */\n     public LzfEncoder(int totalLength) {\n-        this(false, totalLength);\n+        this(false, totalLength, -1);\n     }\n \n     /**\n      * Creates a new LZF encoder with specified settings.\n      *\n-     * @param safeInstance\n-     *        If {@code true} encoder will use {@link ChunkEncoder} that only uses standard JDK access methods,\n-     *        and should work on all Java platforms and JVMs.\n-     *        Otherwise encoder will try to use highly optimized {@link ChunkEncoder} implementation that uses\n-     *        Sun JDK's {@link sun.misc.Unsafe} class (which may be included by other JDK's as well).\n-     * @param totalLength\n-     *        Expected total length of content to compress; only matters for outgoing messages that is smaller\n-     *        than maximum chunk size (64k), to optimize encoding hash tables.\n+     * @param safeInstance If {@code true} encoder will use {@link ChunkEncoder} that only uses standard JDK\n+     *                     access methods, and should work on all Java platforms and JVMs.\n+     *                     Otherwise encoder will try to use highly optimized {@link ChunkEncoder}\n+     *                     implementation that uses Sun JDK's {@link sun.misc.Unsafe}\n+     *                     class (which may be included by other JDK's as well).\n+     * @param totalLength  Expected total length of content to compress; only matters for outgoing messages\n+     *                     that is smaller than maximum chunk size (64k), to optimize encoding hash tables.", "originalCommit": "8dc27b7884b8a7653a0c3700c6e499ff6428f2af", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDgyNzQzMQ==", "url": "https://github.com/netty/netty/pull/10082#discussion_r390827431", "bodyText": "I think we should not use -1 as a special number here and use MIN_BLOCK_TO_COMPRESS  as our default.", "author": "normanmaurer", "createdAt": "2020-03-11T09:04:58Z", "path": "codec/src/main/java/io/netty/handler/codec/compression/LzfEncoder.java", "diffHunk": "@@ -131,10 +133,10 @@ public LzfEncoder(boolean safeInstance, int totalLength, int compressThreshold)\n                     \" (expected: \" + MIN_BLOCK_TO_COMPRESS + '-' + MAX_CHUNK_LEN + ')');\n         }\n \n-        if (compressThreshold >= 0 && compressThreshold < MIN_BLOCK_TO_COMPRESS) {\n+        if (compressThreshold > 0 && compressThreshold < MIN_BLOCK_TO_COMPRESS) {", "originalCommit": "173bbde9794eb304239181903052980c2e8846a2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDgzNDIyOA==", "url": "https://github.com/netty/netty/pull/10082#discussion_r390834228", "bodyText": "Fixed.", "author": "carryxyh", "createdAt": "2020-03-11T09:17:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDgyNzQzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDgzODIyMw==", "url": "https://github.com/netty/netty/pull/10082#discussion_r390838223", "bodyText": "nit: make this final", "author": "normanmaurer", "createdAt": "2020-03-11T09:24:07Z", "path": "codec/src/main/java/io/netty/handler/codec/compression/LzfEncoder.java", "diffHunk": "@@ -128,8 +166,16 @@ protected void encode(ChannelHandlerContext ctx, ByteBuf in, ByteBuf out) throws\n         out.ensureWritable(maxOutputLength);\n         final byte[] output = out.array();\n         final int outputPtr = out.arrayOffset() + out.writerIndex();\n-        final int outputLength = LZFEncoder.appendEncoded(encoder,\n-                        input, inputPtr, length,  output, outputPtr) - outputPtr;\n+\n+        int outputLength;", "originalCommit": "5d706c6245b71a84c4aede2c3906d90ef50b331c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDkyNDc4OQ==", "url": "https://github.com/netty/netty/pull/10082#discussion_r390924789", "bodyText": "@carryxyh sorry I missed this before... Please remove this declaration and the related imports as its not used anymore.", "author": "normanmaurer", "createdAt": "2020-03-11T12:05:21Z", "path": "codec/src/main/java/io/netty/handler/codec/compression/LzfEncoder.java", "diffHunk": "@@ -17,27 +17,42 @@\n \n import com.ning.compress.BufferRecycler;\n import com.ning.compress.lzf.ChunkEncoder;\n+import com.ning.compress.lzf.LZFChunk;\n import com.ning.compress.lzf.LZFEncoder;\n import com.ning.compress.lzf.util.ChunkEncoderFactory;\n import io.netty.buffer.ByteBuf;\n import io.netty.channel.ChannelHandlerContext;\n import io.netty.handler.codec.MessageToByteEncoder;\n+import io.netty.util.internal.logging.InternalLogger;\n+import io.netty.util.internal.logging.InternalLoggerFactory;\n \n-import static com.ning.compress.lzf.LZFChunk.*;\n+import static com.ning.compress.lzf.LZFChunk.MAX_CHUNK_LEN;\n \n /**\n  * Compresses a {@link ByteBuf} using the LZF format.\n- *\n+ * <p>\n  * See original <a href=\"http://oldhome.schmorp.de/marc/liblzf.html\">LZF package</a>\n  * and <a href=\"https://github.com/ning/compress/wiki/LZFFormat\">LZF format</a> for full description.\n  */\n public class LzfEncoder extends MessageToByteEncoder<ByteBuf> {\n+\n+    private static final InternalLogger logger = InternalLoggerFactory.getInstance(LzfEncoder.class);", "originalCommit": "05f61dc01343b56135db5bf28bc8c22f9e5e1d02", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDkyNjQ3MQ==", "url": "https://github.com/netty/netty/pull/10082#discussion_r390926471", "bodyText": "Careful. fixed..  :)", "author": "carryxyh", "createdAt": "2020-03-11T12:09:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDkyNDc4OQ=="}], "type": "inlineReview"}, {"oid": "c7270b921ab1ff1c15cc966c0322388b74a38db7", "url": "https://github.com/netty/netty/commit/c7270b921ab1ff1c15cc966c0322388b74a38db7", "message": "Let LzfEncoder support length aware ability.\n\nMotivation:\n\nSince the LZF support non-compress and compress format, we can let LzfEncoder support length aware ability. It can let the user control compress.\n\nModification:\n\nWhen the data length over compressThreshold, LzfEncoder use compress format to compress data. Otherwise, only use non-compress format. Whatever compress format the encoder use, the LzfDecoder can decompress data well.", "committedDate": "2020-03-11T13:46:37Z", "type": "forcePushed"}, {"oid": "96a65a003881f73d4896ba104644f796217fe82f", "url": "https://github.com/netty/netty/commit/96a65a003881f73d4896ba104644f796217fe82f", "message": "Let LzfEncoder support length aware ability.\n\nMotivation:\n\nSince the LZF support non-compress and compress format, we can let LzfEncoder support length aware ability. It can let the user control compress.\n\nModification:\n\nWhen the data length over compressThreshold, LzfEncoder use compress format to compress data. Otherwise, only use non-compress format. Whatever compress format the encoder use, the LzfDecoder can decompress data well.", "committedDate": "2020-03-11T13:50:56Z", "type": "commit"}, {"oid": "96a65a003881f73d4896ba104644f796217fe82f", "url": "https://github.com/netty/netty/commit/96a65a003881f73d4896ba104644f796217fe82f", "message": "Let LzfEncoder support length aware ability.\n\nMotivation:\n\nSince the LZF support non-compress and compress format, we can let LzfEncoder support length aware ability. It can let the user control compress.\n\nModification:\n\nWhen the data length over compressThreshold, LzfEncoder use compress format to compress data. Otherwise, only use non-compress format. Whatever compress format the encoder use, the LzfDecoder can decompress data well.", "committedDate": "2020-03-11T13:50:56Z", "type": "forcePushed"}]}