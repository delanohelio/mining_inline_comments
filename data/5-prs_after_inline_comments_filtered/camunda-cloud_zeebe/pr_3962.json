{"pr_number": 3962, "pr_title": "chore(broker): move compaction responsibility from LogStream to broker", "pr_createdAt": "2020-03-01T15:04:15Z", "pr_url": "https://github.com/camunda-cloud/zeebe/pull/3962", "timeline": [{"oid": "b27315872a6123ac70bdf8c9f99540f3024ae09e", "url": "https://github.com/camunda-cloud/zeebe/commit/b27315872a6123ac70bdf8c9f99540f3024ae09e", "message": "chore(qa): minor quality improvement", "committedDate": "2020-03-05T13:42:03Z", "type": "forcePushed"}, {"oid": "1f1b559a4b719c0817dc54c8078d703b05bcc134", "url": "https://github.com/camunda-cloud/zeebe/commit/1f1b559a4b719c0817dc54c8078d703b05bcc134", "message": "chore(qa): minor quality improvement", "committedDate": "2020-03-05T13:48:48Z", "type": "forcePushed"}, {"oid": "418636af04343eba2fda893654a5e71701131ae4", "url": "https://github.com/camunda-cloud/zeebe/commit/418636af04343eba2fda893654a5e71701131ae4", "message": "chore(qa): minor quality improvement", "committedDate": "2020-03-05T14:23:05Z", "type": "forcePushed"}, {"oid": "ad926660ad5edcce342cb273aacfeb2a360335d2", "url": "https://github.com/camunda-cloud/zeebe/commit/ad926660ad5edcce342cb273aacfeb2a360335d2", "message": "chore(engine): fetch last processed position on async snapshot director start", "committedDate": "2020-03-06T09:36:29Z", "type": "forcePushed"}, {"oid": "b818c647036dc181ed1ed41dee1683b9a711fd15", "url": "https://github.com/camunda-cloud/zeebe/commit/b818c647036dc181ed1ed41dee1683b9a711fd15", "message": "chore(engine): do not access deprecated getLastValidSnapshotPosition API", "committedDate": "2020-03-10T07:55:41Z", "type": "commit"}, {"oid": "f24ff33d0bc1226c5c94771b1e25bd43b73c479a", "url": "https://github.com/camunda-cloud/zeebe/commit/f24ff33d0bc1226c5c94771b1e25bd43b73c479a", "message": "chore(engine): fetch last processed position on async snapshot director start", "committedDate": "2020-03-10T07:57:51Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDEyMzc1OQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390123759", "bodyText": "Why is this method called onCompaction but only called on error? A bit unclear to me. As you wrote in the comment I think it make sense to complete the future exceptionally ? \ud83e\udd14", "author": "Zelldon", "createdAt": "2020-03-10T07:00:39Z", "path": "broker/src/main/java/io/zeebe/broker/logstreams/AtomixLogCompactor.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Copyright Camunda Services GmbH and/or licensed to Camunda Services GmbH under\n+ * one or more contributor license agreements. See the NOTICE file distributed\n+ * with this work for additional information regarding copyright ownership.\n+ * Licensed under the Zeebe Community License 1.0. You may not use this file\n+ * except in compliance with the Zeebe Community License 1.0.\n+ */\n+package io.zeebe.broker.logstreams;\n+\n+import io.atomix.protocols.raft.partition.impl.RaftPartitionServer;\n+import io.zeebe.broker.Loggers;\n+import java.util.concurrent.CompletableFuture;\n+\n+public final class AtomixLogCompactor implements LogCompactor {\n+  private final RaftPartitionServer partitionServer;\n+\n+  public AtomixLogCompactor(final RaftPartitionServer partitionServer) {\n+    this.partitionServer = partitionServer;\n+  }\n+\n+  /**\n+   * Sets the compactable index on the Atomix side and triggers compaction. On failure will log the\n+   * error but will return a \"successful\" future - arguable if this is desired behavior.\n+   *\n+   * @param compactionBound the upper index compaction bound\n+   * @return a future which is completed after compaction is finished\n+   */\n+  @Override\n+  public CompletableFuture<Void> compactLog(final long compactionBound) {\n+    Loggers.DELETION_SERVICE.debug(\"Compacting Atomix log up to index {}\", compactionBound);\n+    partitionServer.setCompactableIndex(compactionBound);\n+    return partitionServer.snapshot().exceptionally(error -> onCompaction(compactionBound, error));\n+  }\n+\n+  private Void onCompaction(final long compactionBound, final Throwable error) {", "originalCommit": "ad926660ad5edcce342cb273aacfeb2a360335d2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE2MjUwOA==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390162508", "bodyText": "As you wrote in the comment I think it make sense to complete the future exceptionally?\n\nI wasn't sure here, since what exactly would we do with it? Right now nothing, except maybe log it, which we already do here. Though it might be more intuitive if here we return a failed future, and the consumer can decide to log it or not, and we don't log it here, I guess.", "author": "npepinpe", "createdAt": "2020-03-10T08:44:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDEyMzc1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE4OTY1Mg==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390189652", "bodyText": "Though it might be more intuitive if here we return a failed future, and the consumer can decide to log it or not, and we don't log it here, I guess.\n\nExactly", "author": "Zelldon", "createdAt": "2020-03-10T09:35:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDEyMzc1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDEyNDQ2Mw==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390124463", "bodyText": "\ud83d\udc4d", "author": "Zelldon", "createdAt": "2020-03-10T07:03:20Z", "path": "broker/src/main/java/io/zeebe/broker/system/partitions/ZeebePartition.java", "diffHunk": "@@ -294,10 +295,9 @@ private void transitionToLeader(final CompletableActorFuture<Void> transitionCom\n           e);\n     }\n \n-    final StatePositionSupplier positionSupplier = new StatePositionSupplier(LOG);", "originalCommit": "ad926660ad5edcce342cb273aacfeb2a360335d2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0NDU4Mw==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390144583", "bodyText": "Why do you think they are null?", "author": "Zelldon", "createdAt": "2020-03-10T08:04:17Z", "path": "broker/src/test/java/io/zeebe/broker/logstreams/AtomixLogDeletionServiceTest.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Copyright Camunda Services GmbH and/or licensed to Camunda Services GmbH under\n+ * one or more contributor license agreements. See the NOTICE file distributed\n+ * with this work for additional information regarding copyright ownership.\n+ * Licensed under the Zeebe Community License 1.0. You may not use this file\n+ * except in compliance with the Zeebe Community License 1.0.\n+ */\n+package io.zeebe.broker.logstreams;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import io.atomix.protocols.raft.storage.RaftStorage;\n+import io.atomix.protocols.raft.storage.log.RaftLogReader;\n+import io.atomix.storage.journal.Indexed;\n+import io.atomix.storage.journal.JournalReader.Mode;\n+import io.atomix.storage.journal.JournalSegmentDescriptor;\n+import io.atomix.utils.time.WallClockTimestamp;\n+import io.zeebe.broker.clustering.atomix.storage.snapshot.AtomixRecordEntrySupplierImpl;\n+import io.zeebe.broker.clustering.atomix.storage.snapshot.AtomixSnapshotStorage;\n+import io.zeebe.broker.clustering.atomix.storage.snapshot.DbSnapshotStore;\n+import io.zeebe.logstreams.state.SnapshotMetrics;\n+import io.zeebe.logstreams.storage.atomix.AtomixLogStorageReader;\n+import io.zeebe.logstreams.util.AtomixLogStorageRule;\n+import io.zeebe.util.sched.testing.ActorSchedulerRule;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.TimeUnit;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.RuleChain;\n+import org.junit.rules.TemporaryFolder;\n+\n+public final class AtomixLogDeletionServiceTest {\n+\n+  private static final ByteBuffer DATA = ByteBuffer.allocate(Integer.BYTES).putInt(0, 1);\n+  private static final int PARTITION_ID = 1;\n+\n+  private final ActorSchedulerRule actorScheduler = new ActorSchedulerRule();\n+  private final TemporaryFolder temporaryFolder = new TemporaryFolder();\n+  private final AtomixLogStorageRule logStorageRule =\n+      new AtomixLogStorageRule(temporaryFolder, PARTITION_ID, b -> builder(b, temporaryFolder));\n+\n+  @Rule\n+  public final RuleChain chain =\n+      RuleChain.outerRule(temporaryFolder).around(actorScheduler).around(logStorageRule);\n+\n+  private AtomixSnapshotStorage snapshotStorage;\n+  private AtomixLogStorageReader storageReader;\n+  private LogDeletionService deletionService;\n+  private Compactor compactor;\n+\n+  @Before\n+  public void setUp() {\n+    storageReader =\n+        new AtomixLogStorageReader(logStorageRule.getRaftLog().openReader(-1, Mode.COMMITS));\n+    snapshotStorage =\n+        new AtomixSnapshotStorage(\n+            null,\n+            logStorageRule.getSnapshotStore(),\n+            new AtomixRecordEntrySupplierImpl(storageReader),\n+            1,\n+            new SnapshotMetrics(PARTITION_ID));\n+    compactor = new Compactor();\n+\n+    deletionService = new LogDeletionService(0, PARTITION_ID, compactor, snapshotStorage);\n+    actorScheduler.submitActor(deletionService).join();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    if (storageReader != null) {", "originalCommit": "c575fc393063e4d4276fe84791d3d478d73b8710", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE1NzI1NA==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390157254", "bodyText": "Force of habit, in case something went wrong before it was initialized and the @After still runs - just defensive programming in general.", "author": "npepinpe", "createdAt": "2020-03-10T08:33:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0NDU4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE5MTQ2MQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390191461", "bodyText": "Hm not sure if this make sense here, it is just noise on reading the test.", "author": "Zelldon", "createdAt": "2020-03-10T09:38:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0NDU4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0NTYwNw==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390145607", "bodyText": "Not sure whether this makes sense or not. This would mean if we get a snapshot with index 5 we keep 3, so actually we have an gap in between. Hopefully we will start not  at the beginning \ud83d\ude05", "author": "Zelldon", "createdAt": "2020-03-10T08:06:50Z", "path": "broker/src/test/java/io/zeebe/broker/logstreams/AtomixLogDeletionServiceTest.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Copyright Camunda Services GmbH and/or licensed to Camunda Services GmbH under\n+ * one or more contributor license agreements. See the NOTICE file distributed\n+ * with this work for additional information regarding copyright ownership.\n+ * Licensed under the Zeebe Community License 1.0. You may not use this file\n+ * except in compliance with the Zeebe Community License 1.0.\n+ */\n+package io.zeebe.broker.logstreams;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import io.atomix.protocols.raft.storage.RaftStorage;\n+import io.atomix.protocols.raft.storage.log.RaftLogReader;\n+import io.atomix.storage.journal.Indexed;\n+import io.atomix.storage.journal.JournalReader.Mode;\n+import io.atomix.storage.journal.JournalSegmentDescriptor;\n+import io.atomix.utils.time.WallClockTimestamp;\n+import io.zeebe.broker.clustering.atomix.storage.snapshot.AtomixRecordEntrySupplierImpl;\n+import io.zeebe.broker.clustering.atomix.storage.snapshot.AtomixSnapshotStorage;\n+import io.zeebe.broker.clustering.atomix.storage.snapshot.DbSnapshotStore;\n+import io.zeebe.logstreams.state.SnapshotMetrics;\n+import io.zeebe.logstreams.storage.atomix.AtomixLogStorageReader;\n+import io.zeebe.logstreams.util.AtomixLogStorageRule;\n+import io.zeebe.util.sched.testing.ActorSchedulerRule;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.TimeUnit;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.RuleChain;\n+import org.junit.rules.TemporaryFolder;\n+\n+public final class AtomixLogDeletionServiceTest {\n+\n+  private static final ByteBuffer DATA = ByteBuffer.allocate(Integer.BYTES).putInt(0, 1);\n+  private static final int PARTITION_ID = 1;\n+\n+  private final ActorSchedulerRule actorScheduler = new ActorSchedulerRule();\n+  private final TemporaryFolder temporaryFolder = new TemporaryFolder();\n+  private final AtomixLogStorageRule logStorageRule =\n+      new AtomixLogStorageRule(temporaryFolder, PARTITION_ID, b -> builder(b, temporaryFolder));\n+\n+  @Rule\n+  public final RuleChain chain =\n+      RuleChain.outerRule(temporaryFolder).around(actorScheduler).around(logStorageRule);\n+\n+  private AtomixSnapshotStorage snapshotStorage;\n+  private AtomixLogStorageReader storageReader;\n+  private LogDeletionService deletionService;\n+  private Compactor compactor;\n+\n+  @Before\n+  public void setUp() {\n+    storageReader =\n+        new AtomixLogStorageReader(logStorageRule.getRaftLog().openReader(-1, Mode.COMMITS));\n+    snapshotStorage =\n+        new AtomixSnapshotStorage(\n+            null,\n+            logStorageRule.getSnapshotStore(),\n+            new AtomixRecordEntrySupplierImpl(storageReader),\n+            1,\n+            new SnapshotMetrics(PARTITION_ID));\n+    compactor = new Compactor();\n+\n+    deletionService = new LogDeletionService(0, PARTITION_ID, compactor, snapshotStorage);\n+    actorScheduler.submitActor(deletionService).join();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    if (storageReader != null) {\n+      storageReader.close();\n+      storageReader = null;\n+    }\n+\n+    if (snapshotStorage != null) {\n+      snapshotStorage.close();\n+      snapshotStorage = null;\n+    }\n+\n+    if (deletionService != null) {\n+      deletionService.close();\n+      deletionService = null;\n+    }\n+  }\n+\n+  @Test\n+  public void shouldDeleteUpToCompactionBound() {\n+    // given\n+    final var reader = logStorageRule.getRaftLog().openReader(-1);\n+\n+    // when\n+    logStorageRule.appendEntry(1, 1, DATA).index();\n+    logStorageRule.appendEntry(2, 2, DATA).index();\n+    logStorageRule.appendEntry(3, 3, DATA).index();\n+    createSnapshot(2);\n+\n+    // then\n+    compactor.awaitCompaction(2L, Duration.ofSeconds(5));\n+    reader.reset();\n+    final var entries = readAllEntries(reader);\n+    assertThat(entries).isNotEmpty().hasSize(2).extracting(Indexed::index).containsExactly(2L, 3L);\n+  }\n+\n+  @Test\n+  public void shouldNotDeleteOnLowerCompactionBound() {\n+    // given\n+    final var reader = logStorageRule.getRaftLog().openReader(-1);\n+\n+    // when\n+    logStorageRule.appendEntry(1, 1, DATA).index();\n+    logStorageRule.appendEntry(2, 2, DATA).index();\n+    logStorageRule.appendEntry(3, 3, DATA).index();\n+    createSnapshot(0);\n+\n+    // then\n+    compactor.awaitCompaction(0L, Duration.ofSeconds(5));\n+    reader.reset();\n+    final var entries = readAllEntries(reader);\n+    assertThat(entries)\n+        .isNotEmpty()\n+        .hasSize(3)\n+        .extracting(Indexed::index)\n+        .containsExactly(1L, 2L, 3L);\n+  }\n+\n+  @Test\n+  public void shouldDeleteLowerEntriesEvenIfIndexNotFound() {\n+    // given\n+    final var reader = logStorageRule.getRaftLog().openReader(-1);\n+\n+    // when\n+    logStorageRule.appendEntry(1, 1, DATA).index();\n+    logStorageRule.appendEntry(2, 2, DATA).index();\n+    logStorageRule.appendEntry(3, 3, DATA).index();\n+    createSnapshot(5L);\n+\n+    // then - expect exactly one segment left with entry 3\n+    compactor.awaitCompaction(5L, Duration.ofSeconds(5));", "originalCommit": "c575fc393063e4d4276fe84791d3d478d73b8710", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE2MzUwNg==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390163506", "bodyText": "This is already the case, though - I just replicated current feature parity \ud83d\ude05 This is more a side effect of the journal design in Atomix how we use it - there is always one segment left, so even if you try to compact because you received a snapshot ahead of your log, you'll always have some data left over =/", "author": "npepinpe", "createdAt": "2020-03-10T08:46:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0NTYwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE5MTkzNg==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390191936", "bodyText": "but with truncate we delete it right?", "author": "Zelldon", "createdAt": "2020-03-10T09:39:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0NTYwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE5MzU3MA==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390193570", "bodyText": "I'll double check, but afaik in Atomix, on receiving a snapshot, we reset the head to that index (meaning we truncate down to that index), and start appending. So if we had 1, 2, 3, and we receive snapshot 25, then we truncate nothing and the log will be 1,2,3,26,27, etc.", "author": "npepinpe", "createdAt": "2020-03-10T09:42:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0NTYwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDM2MzU2Mw==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390363563", "bodyText": "Hm so we can't start from a clean state", "author": "Zelldon", "createdAt": "2020-03-10T14:40:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0NTYwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0NjUyNQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390146525", "bodyText": "Is it correct to continue from here?", "author": "Zelldon", "createdAt": "2020-03-10T08:09:13Z", "path": "engine/src/main/java/io/zeebe/engine/processor/AsyncSnapshotDirector.java", "diffHunk": "@@ -78,17 +78,75 @@ protected void onActorStarting() {\n     commitCondition = actor.onCondition(getConditionNameForPosition(), this::onCommitCheck);\n     logStream.registerOnCommitPositionUpdatedCondition(commitCondition);\n \n-    lastValidSnapshotPosition = snapshotController.getLastValidSnapshotPosition();\n-    LOG.debug(\n-        \"The position of the last valid snapshot is '{}'. Taking snapshots beyond this position.\",\n-        lastValidSnapshotPosition);\n+    lastValidSnapshotPosition = -1;\n+    actor.runOnCompletionBlockingCurrentPhase(\n+        streamProcessor.getLastProcessedPositionAsync(),\n+        (position, error) -> {\n+          if (error == null) {\n+            lastValidSnapshotPosition = position;\n+            LOG.debug(\n+                \"The position of the last valid snapshot is '{}'. Taking snapshots beyond this position.\",\n+                lastValidSnapshotPosition);\n+          } else {\n+            LOG.error(ERROR_MSG_ON_RESOLVE_PROCESSED_POS, error);", "originalCommit": "c575fc393063e4d4276fe84791d3d478d73b8710", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE1ODM3Mw==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390158373", "bodyText": "The worse that can happen is that we might take a snapshot when we don't need to, since we fallback to using -1 as the last valid snapshot position. So if we had a snapshot at position 5, and we process nothing, we will have two snapshots at position 5 (but one with a higher timestamp so they can still be distinguished). It's useless work but I think it's not incorrect. We could also fail, but I'm not sure how is the recovery at the moment, so this seemed like a good compromise.", "author": "npepinpe", "createdAt": "2020-03-10T08:36:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0NjUyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE5MjQ4NA==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390192484", "bodyText": "So would could also just skip this and determine the position on the first next snapshot we take? Less complexity", "author": "Zelldon", "createdAt": "2020-03-10T09:40:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0NjUyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE5NDQ4Mw==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390194483", "bodyText": "How would you determine this later? Once we want to take a snapshot, how do we distinguish whether the stream processor has processed events or not?", "author": "npepinpe", "createdAt": "2020-03-10T09:43:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0NjUyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE5NTE3MQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390195171", "bodyText": "Maybe we dont need to?", "author": "Zelldon", "createdAt": "2020-03-10T09:45:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0NjUyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0NzQwNg==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390147406", "bodyText": "We have to check our format settings again. In the latest reviews I often saw methods rearrangements. It is is really hard to follow.", "author": "Zelldon", "createdAt": "2020-03-10T08:11:24Z", "path": "engine/src/main/java/io/zeebe/engine/processor/AsyncSnapshotDirector.java", "diffHunk": "@@ -78,17 +78,75 @@ protected void onActorStarting() {\n     commitCondition = actor.onCondition(getConditionNameForPosition(), this::onCommitCheck);\n     logStream.registerOnCommitPositionUpdatedCondition(commitCondition);\n \n-    lastValidSnapshotPosition = snapshotController.getLastValidSnapshotPosition();\n-    LOG.debug(\n-        \"The position of the last valid snapshot is '{}'. Taking snapshots beyond this position.\",\n-        lastValidSnapshotPosition);\n+    lastValidSnapshotPosition = -1;\n+    actor.runOnCompletionBlockingCurrentPhase(\n+        streamProcessor.getLastProcessedPositionAsync(),\n+        (position, error) -> {\n+          if (error == null) {\n+            lastValidSnapshotPosition = position;\n+            LOG.debug(\n+                \"The position of the last valid snapshot is '{}'. Taking snapshots beyond this position.\",\n+                lastValidSnapshotPosition);\n+          } else {\n+            LOG.error(ERROR_MSG_ON_RESOLVE_PROCESSED_POS, error);\n+          }\n+        });\n   }\n \n   @Override\n   protected void onActorCloseRequested() {\n     logStream.removeOnCommitPositionUpdatedCondition(commitCondition);\n   }\n \n+  @Override\n+  public ActorFuture<Void> closeAsync() {", "originalCommit": "c575fc393063e4d4276fe84791d3d478d73b8710", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE1ODU0NA==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390158544", "bodyText": "Yes, I reverted those, not sure why it rearranged them :s", "author": "npepinpe", "createdAt": "2020-03-10T08:36:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0NzQwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0ODIyOA==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390148228", "bodyText": "When we could get rid of this limitation then we would also get updates from exporter position.", "author": "Zelldon", "createdAt": "2020-03-10T08:13:34Z", "path": "engine/src/test/java/io/zeebe/engine/processor/StreamProcessorTest.java", "diffHunk": "@@ -575,7 +572,7 @@ public void onRecovered(final ReadonlyProcessingContext context) {\n   }\n \n   @Test\n-  public void shouldNotCreateSnapshotsIfNoProcessorProcessEvent() throws Exception {\n+  public void shouldNotCreateSnapshotsIfNoProcessorProcessEvent() {", "originalCommit": "c575fc393063e4d4276fe84791d3d478d73b8710", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE1OTIyMA==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390159220", "bodyText": "What limitation are you referring to?", "author": "npepinpe", "createdAt": "2020-03-10T08:37:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0ODIyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE5MzEwMQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390193101", "bodyText": "That we do not take snapshots if the position has not changed. I mean we could also just do snapshot every 15min regardless of the position", "author": "Zelldon", "createdAt": "2020-03-10T09:41:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0ODIyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE5NTYxMQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390195611", "bodyText": "This is kind of expensive right now because we push snapshots, so with large states this is a lot of extra work for little gain.\nThat said, it's really tempting to remove it - then we dont have the issue with the exporters. Maybe we should discuss this a bit more thoroughly, it's a little out of scope here imho but a very good point", "author": "npepinpe", "createdAt": "2020-03-10T09:45:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0ODIyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE5NjgzMQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390196831", "bodyText": "Yes we can do it separately. Came just in my mind when I saw this code \ud83d\ude05", "author": "Zelldon", "createdAt": "2020-03-10T09:47:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0ODIyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDIwOTAyNQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390209025", "bodyText": "#4017", "author": "npepinpe", "createdAt": "2020-03-10T10:08:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0ODIyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0ODQ1Nw==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390148457", "bodyText": "This could cause merge conflicts - I think I renamed it already :D", "author": "Zelldon", "createdAt": "2020-03-10T08:14:08Z", "path": "logstreams/src/main/java/io/zeebe/logstreams/impl/log/LogStreamImpl.java", "diffHunk": "@@ -335,13 +324,13 @@ private int determineInitialPartitionId() {\n       final long lastPosition = logReader.seekToEnd();\n \n       // dispatcher needs to generate positions greater than the last position\n-      int partitionId = 0;\n+      int dispatcherPartitionId = 0;", "originalCommit": "c575fc393063e4d4276fe84791d3d478d73b8710", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE1OTMyOQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390159329", "bodyText": "\ud83d\ude48", "author": "npepinpe", "createdAt": "2020-03-10T08:38:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0ODQ1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0OTU1Ng==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390149556", "bodyText": "\ud83d\ude48 This is often really hard to spot", "author": "Zelldon", "createdAt": "2020-03-10T08:16:44Z", "path": "logstreams/src/main/java/io/zeebe/logstreams/state/StateSnapshotController.java", "diffHunk": "@@ -119,7 +115,7 @@ public long recover() throws Exception {\n         // open database to verify that the snapshot is recoverable\n         openDb();\n         LOG.debug(\"Recovered state from snapshot '{}'\", snapshot);\n-        lowerBoundSnapshotPosition = snapshot.getPosition();\n+        break;", "originalCommit": "c575fc393063e4d4276fe84791d3d478d73b8710", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE1OTg2Mg==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390159862", "bodyText": "Hm, the other option is to use a boolean flag, I guess - I'm usually fine with a break but we can use that instead.", "author": "npepinpe", "createdAt": "2020-03-10T08:39:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0OTU1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE5MzgyNg==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390193826", "bodyText": "Leave it for now - we will probably rewrite this method in the next issues, because we remove multiple snapshots.", "author": "Zelldon", "createdAt": "2020-03-10T09:42:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0OTU1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE1MDc2MQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390150761", "bodyText": "Should we fail then? I'm wondering what we should do in the processor actually, if we have no snapshot to open (because they are corrupted)", "author": "Zelldon", "createdAt": "2020-03-10T08:19:30Z", "path": "logstreams/src/test/java/io/zeebe/logstreams/state/StateSnapshotControllerTest.java", "diffHunk": "@@ -73,14 +74,14 @@ public void shouldTakeSnapshot() throws Exception {\n   }\n \n   @Test\n-  public void shouldOpenNewDatabaseIfNoSnapshotsToRecoverFrom() throws Exception {\n+  public void shouldDoNothingIfNoSnapshotsToRecoverFrom() throws Exception {\n     // given\n \n     // when\n-    final long lowerBoundSnapshotPosition = snapshotController.recover();\n+    snapshotController.recover();", "originalCommit": "c575fc393063e4d4276fe84791d3d478d73b8710", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE2MDczOQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390160739", "bodyText": "This is probably a different issue - current behavior is not to fail. If we have no snapshots at all, then \u2714\ufe0f  If we had some corrupted snapshots, then probably we should step down and throw away our state, but that's a bigger issue, I guess", "author": "npepinpe", "createdAt": "2020-03-10T08:40:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE1MDc2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE5NDMyNw==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390194327", "bodyText": "Had a look in the code - if the snapshot was corrupt and the last one we throw an exception, where the processor goes into failed state then. so should be fine", "author": "Zelldon", "createdAt": "2020-03-10T09:43:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE1MDc2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE1MjQ0OA==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390152448", "bodyText": "\ud83d\ude06 \ud83d\udc4d", "author": "Zelldon", "createdAt": "2020-03-10T08:23:21Z", "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/SnapshotReplicationTest.java", "diffHunk": "@@ -28,7 +29,7 @@\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n-import java.util.zip.CRC32;\n+import java.util.zip.CRC32C;", "originalCommit": "c575fc393063e4d4276fe84791d3d478d73b8710", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE1MjU5Nw==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390152597", "bodyText": "We can also use that in the replication chunks right?", "author": "Zelldon", "createdAt": "2020-03-10T08:23:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE1MjQ0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE2MDk1Nw==", "url": "https://github.com/camunda-cloud/zeebe/pull/3962#discussion_r390160957", "bodyText": "Yes - and also do a checksum of the full snapshot. I'll write an issue for it.", "author": "npepinpe", "createdAt": "2020-03-10T08:41:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE1MjQ0OA=="}], "type": "inlineReview"}, {"oid": "8ee58d11f155056b7df806ff44fefd1b634f9659", "url": "https://github.com/camunda-cloud/zeebe/commit/8ee58d11f155056b7df806ff44fefd1b634f9659", "message": "chore(broker): move compaction error log to service", "committedDate": "2020-03-10T09:00:37Z", "type": "forcePushed"}, {"oid": "43543afe7748f57a2022254bea04fa86b3507955", "url": "https://github.com/camunda-cloud/zeebe/commit/43543afe7748f57a2022254bea04fa86b3507955", "message": "chore(logstreams): remove deletion responsibility from LogStream", "committedDate": "2020-03-11T09:15:30Z", "type": "commit"}, {"oid": "e51197a6b3bd54d61b82b13a168378b768efe828", "url": "https://github.com/camunda-cloud/zeebe/commit/e51197a6b3bd54d61b82b13a168378b768efe828", "message": "chore(qa): minor quality improvement", "committedDate": "2020-03-11T09:15:30Z", "type": "commit"}, {"oid": "f7c55b849a5a2880627338b5da6e22a4b0d6a0b8", "url": "https://github.com/camunda-cloud/zeebe/commit/f7c55b849a5a2880627338b5da6e22a4b0d6a0b8", "message": "chore(broker): introduce LogDeletion service\n\n- adds AtomixLogDeletionService a specialized class to compaction the\n  log based on a given snapshot upper compaction bound", "committedDate": "2020-03-11T09:15:30Z", "type": "commit"}, {"oid": "2063146e61bbc73636be83534de44bdddc69466d", "url": "https://github.com/camunda-cloud/zeebe/commit/2063146e61bbc73636be83534de44bdddc69466d", "message": "chore(engine): fetch last processed position on async snapshot director start", "committedDate": "2020-03-11T09:15:30Z", "type": "commit"}, {"oid": "2063146e61bbc73636be83534de44bdddc69466d", "url": "https://github.com/camunda-cloud/zeebe/commit/2063146e61bbc73636be83534de44bdddc69466d", "message": "chore(engine): fetch last processed position on async snapshot director start", "committedDate": "2020-03-11T09:15:30Z", "type": "forcePushed"}]}