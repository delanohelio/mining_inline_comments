{"pr_number": 5614, "pr_title": "Try fix flaky HealthMonitoringTest and DeploymentClusteredTest", "pr_createdAt": "2020-10-16T16:09:54Z", "pr_url": "https://github.com/camunda-cloud/zeebe/pull/5614", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA1OTk3MA==", "url": "https://github.com/camunda-cloud/zeebe/pull/5614#discussion_r509059970", "bodyText": "@korthout do you think it makes sense to replace the boolean flags with just having a future for starting and stopping?\nIf starting future is null, you know it hasn't been started etc. If you call stop twice you just return the stopping future. I think it is not necessary to return an exceptionally future, for me it is not an error to close something what haven't been started. What do you think?", "author": "Zelldon", "createdAt": "2020-10-21T07:46:29Z", "path": "atomix/cluster/src/main/java/io/atomix/raft/impl/DefaultRaftServer.java", "diffHunk": "@@ -123,22 +124,27 @@ public void removeFailureListener(final Runnable failureListener) {\n    *\n    * @return A completable future to be completed once the server has been shutdown.\n    */\n+  @Override\n   public CompletableFuture<Void> shutdown() {\n-    if (!started) {\n+    if (!started && !stopped) {\n       return Futures.exceptionalFuture(new IllegalStateException(\"Server not running\"));\n     }\n \n+    if (stopped) {", "originalCommit": "8e93d01d391a66ac637acb000099b543f61830de", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDExNzIxOA==", "url": "https://github.com/camunda-cloud/zeebe/pull/5614#discussion_r510117218", "bodyText": "It makes sense but since you can start, stop and leave async this gets very complex. It requires keeping track of a queue of futures (or something similar to ZeebePartitionTransition). It would be easier if we could just remove leaving behaviour, since we don't use this anyways. I'll create an issue for it.", "author": "korthout", "createdAt": "2020-10-22T12:24:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA1OTk3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDEyMjkxOQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/5614#discussion_r510122919", "bodyText": "Created here: #5663", "author": "korthout", "createdAt": "2020-10-22T12:33:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA1OTk3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA2MDI0Nw==", "url": "https://github.com/camunda-cloud/zeebe/pull/5614#discussion_r509060247", "bodyText": "Thanks for replacing that \ud83d\udc4d", "author": "Zelldon", "createdAt": "2020-10-21T07:46:53Z", "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/health/HealthMonitoringTest.java", "diffHunk": "@@ -56,7 +46,7 @@ public void shouldReportUnhealthyWhenRaftInactive() {\n     raftPartition.getServer().stop();\n \n     // then\n-    waitUntil(() -> !isBrokerHealthy());\n+    Awaitility.waitAtMost(Duration.ofMinutes(1)).until(() -> !isBrokerHealthy());", "originalCommit": "8e93d01d391a66ac637acb000099b543f61830de", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "e9ffae19c65cabcabc051fcb4f46f397d39f5512", "url": "https://github.com/camunda-cloud/zeebe/commit/e9ffae19c65cabcabc051fcb4f46f397d39f5512", "message": "refactor(broker): Cleanup HealthMonitoringTest\n\nThis test contained a lot of unnecessary things as remnants\nfrom previous tests that existed in this test class. Since they are no\nlonger there, they could simply be removed.", "committedDate": "2020-10-22T12:36:28Z", "type": "commit"}, {"oid": "9753fcc1d2457cc7333cf9ea543e62b939ad7eae", "url": "https://github.com/camunda-cloud/zeebe/commit/9753fcc1d2457cc7333cf9ea543e62b939ad7eae", "message": "fix(atomix): Unflake multiple ITs\n\nHealthMonitoringTest and DeploymentClusteredTest are both flaky. The\ntest timesout due to the outerrule (timeout) wrapped around the\nembeddedbrokerrule.\n\nAt least for the health monitoring test it was clear that a lot of time\nwas spend waiting for the broker to finish, because the raft partition\nserver was already stopped inside the test and could then later not be\nstopped by the after of the embeddedbrokerrule.\n\nThis commit allows calling the shutdown method of DefaultRaftServer\nmultiple times by keeping track of a stopped flag. The server is only\nallowed to stop if it is started and not already stopped. If it is\nalready stopped it can just return.\n\nCo-authored-by: Peter Ihme <peter.ihme@camunda.com>", "committedDate": "2020-10-22T12:36:28Z", "type": "commit"}, {"oid": "9753fcc1d2457cc7333cf9ea543e62b939ad7eae", "url": "https://github.com/camunda-cloud/zeebe/commit/9753fcc1d2457cc7333cf9ea543e62b939ad7eae", "message": "fix(atomix): Unflake multiple ITs\n\nHealthMonitoringTest and DeploymentClusteredTest are both flaky. The\ntest timesout due to the outerrule (timeout) wrapped around the\nembeddedbrokerrule.\n\nAt least for the health monitoring test it was clear that a lot of time\nwas spend waiting for the broker to finish, because the raft partition\nserver was already stopped inside the test and could then later not be\nstopped by the after of the embeddedbrokerrule.\n\nThis commit allows calling the shutdown method of DefaultRaftServer\nmultiple times by keeping track of a stopped flag. The server is only\nallowed to stop if it is started and not already stopped. If it is\nalready stopped it can just return.\n\nCo-authored-by: Peter Ihme <peter.ihme@camunda.com>", "committedDate": "2020-10-22T12:36:28Z", "type": "forcePushed"}]}