{"pr_number": 6652, "pr_title": "de-sparkify SV discovery", "pr_createdAt": "2020-06-09T17:22:51Z", "pr_url": "https://github.com/broadinstitute/gatk/pull/6652", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDQzOTY0MA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r454439640", "bodyText": "I think someone from the engine team should probably review this part of PR (This interface and its uses).", "author": "cwhelan", "createdAt": "2020-07-14T15:23:03Z", "path": "src/main/java/org/broadinstitute/hellbender/engine/BasicReference.java", "diffHunk": "@@ -0,0 +1,15 @@\n+package org.broadinstitute.hellbender.engine;\n+\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+/**\n+ * A source of reference base calls.\n+ */\n+public interface BasicReference {", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE1MTkwNw==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460151907", "bodyText": "This seems OK, but it's a shame to add yet another reference interface to the mess of reference interfaces / classes that already exist.  It seems like the basic need is to be able to unify ReferenceContext and the mess of ReferenceSparkSources?  I don't see an easy way to fix it without adding something new or doing more substantial rework to the spark reference classes.  (I wouldn't be opposed to substantially reworking those in the future but it's probably out of scope here.)\nI would like to clarify all of them into a smaller set of classes / interfaces that can be used between spark/nonspark but we never get around to it.\nSeems ok unless @droazen has comments.", "author": "lbergelson", "createdAt": "2020-07-24T16:13:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDQzOTY0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAwNzc3NA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459007774", "bodyText": "Could you rename simpleMap and complexMap to something like evidenceForSimpleNovelAdjacencies and contigsForComplexVariants, or something similar? I found these names to be too generic for me to understand their uses in the code below.", "author": "cwhelan", "createdAt": "2020-07-22T18:46:10Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package org.broadinstitute.hellbender.tools;\n+\n+import htsjdk.samtools.SAMFileHeader;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.BasicReference;\n+import org.broadinstitute.hellbender.engine.FeatureContext;\n+import org.broadinstitute.hellbender.engine.ReadWalker;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignmentInterval;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInducingAssemblyContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.MultiSegmentsCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.RelevantAttributes;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.ZeroAndOneSegmentCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleChimera;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyAndChimericAlignmentEvidence;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.CNVInputReader;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVIntervalTree;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVUtils;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVVCFWriter;\n+import org.broadinstitute.hellbender.utils.BaseUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import scala.Tuple2;\n+\n+import java.io.BufferedWriter;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.*;\n+\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        oneLineSummary = \"(Internal) Examines aligned contigs from local assemblies and calls structural variants or their breakpoints\",\n+        summary =\n+            \"This tool takes a file containing the alignments of assembled contigs and searches it for contigs with\" +\n+            \" split alignments or large gaps indicating the presence of structural variation breakpoints.\" +\n+            \" Variations' types are determined by analyzing the signatures of the split alignments,\" +\n+            \" and are written to a VCF file.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class)\n+public class StructuralVariantDiscoverer extends ReadWalker {\n+    @Argument(doc = \"Name of output VCF.\", shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            fullName = \"outputVCFName\")\n+    private static String outputVCFName;\n+\n+    @Argument(doc = \"file containing non-canonical chromosome names (e.g chrUn_KI270588v1) in the reference, \" +\n+            \"human reference (hg19 or hg38) assumed when omitted\", shortName = \"alt-tigs\",\n+            fullName = \"non-canonical-contig-names-file\", optional = true)\n+    private static String nonCanonicalChromosomeNamesFile;\n+\n+    @ArgumentCollection\n+    private static final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoverStageArgs =\n+            new DiscoverVariantsFromContigAlignmentsSparkArgumentCollection();\n+\n+    private static final double SCORE_DIFF_TOLERANCE = 0.;\n+\n+    private String sampleId;\n+    private SAMSequenceDictionary refDict;\n+    private BasicReference reference;\n+    private SVIntervalTree<VariantContext> cnvCalls;\n+    private Set<String> canonicalChromosomes;\n+\n+    private String currentContigName = null;\n+    private final List<GATKRead> readsForCurrentContig = new ArrayList<>();\n+    private final Map<NovelAdjacencyAndAltHaplotype, SimpleNovelAdjacencyAndChimericAlignmentEvidence> simpleMap =", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAxMTQ0MA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459011440", "bodyText": "Maybe call redoMap novelAdjacenciesToReinterpret?", "author": "cwhelan", "createdAt": "2020-07-22T18:52:12Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package org.broadinstitute.hellbender.tools;\n+\n+import htsjdk.samtools.SAMFileHeader;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.BasicReference;\n+import org.broadinstitute.hellbender.engine.FeatureContext;\n+import org.broadinstitute.hellbender.engine.ReadWalker;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignmentInterval;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInducingAssemblyContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.MultiSegmentsCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.RelevantAttributes;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.ZeroAndOneSegmentCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleChimera;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyAndChimericAlignmentEvidence;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.CNVInputReader;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVIntervalTree;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVUtils;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVVCFWriter;\n+import org.broadinstitute.hellbender.utils.BaseUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import scala.Tuple2;\n+\n+import java.io.BufferedWriter;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.*;\n+\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        oneLineSummary = \"(Internal) Examines aligned contigs from local assemblies and calls structural variants or their breakpoints\",\n+        summary =\n+            \"This tool takes a file containing the alignments of assembled contigs and searches it for contigs with\" +\n+            \" split alignments or large gaps indicating the presence of structural variation breakpoints.\" +\n+            \" Variations' types are determined by analyzing the signatures of the split alignments,\" +\n+            \" and are written to a VCF file.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class)\n+public class StructuralVariantDiscoverer extends ReadWalker {\n+    @Argument(doc = \"Name of output VCF.\", shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            fullName = \"outputVCFName\")\n+    private static String outputVCFName;\n+\n+    @Argument(doc = \"file containing non-canonical chromosome names (e.g chrUn_KI270588v1) in the reference, \" +\n+            \"human reference (hg19 or hg38) assumed when omitted\", shortName = \"alt-tigs\",\n+            fullName = \"non-canonical-contig-names-file\", optional = true)\n+    private static String nonCanonicalChromosomeNamesFile;\n+\n+    @ArgumentCollection\n+    private static final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoverStageArgs =\n+            new DiscoverVariantsFromContigAlignmentsSparkArgumentCollection();\n+\n+    private static final double SCORE_DIFF_TOLERANCE = 0.;\n+\n+    private String sampleId;\n+    private SAMSequenceDictionary refDict;\n+    private BasicReference reference;\n+    private SVIntervalTree<VariantContext> cnvCalls;\n+    private Set<String> canonicalChromosomes;\n+\n+    private String currentContigName = null;\n+    private final List<GATKRead> readsForCurrentContig = new ArrayList<>();\n+    private final Map<NovelAdjacencyAndAltHaplotype, SimpleNovelAdjacencyAndChimericAlignmentEvidence> simpleMap =\n+            new HashMap<>(10000);\n+    private final Map<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> complexMap =\n+            new HashMap<>(1000);\n+    private final List<AlignedContig> complexContigs = new ArrayList<>(1000);\n+\n+    @Override public boolean requiresReads() { return true; }\n+    @Override public boolean requiresReference() { return true; }\n+\n+    @Override public List<ReadFilter> getDefaultReadFilters() {\n+        return Arrays.asList(ReadFilterLibrary.MAPPED, ReadFilterLibrary.NOT_SECONDARY_ALIGNMENT);\n+    }\n+\n+    @Override public void onTraversalStart() {\n+        final SAMFileHeader header = getHeaderForReads();\n+        if ( header.getSortOrder() != SAMFileHeader.SortOrder.queryname ) {\n+            throw new UserException(\"This tool requires a queryname-sorted source of reads.\");\n+        }\n+        sampleId = SVUtils.getSampleId(header);\n+        refDict = header.getSequenceDictionary();\n+        cnvCalls = discoverStageArgs.cnvCallsFile == null ? null :\n+                CNVInputReader.loadCNVCalls(discoverStageArgs.cnvCallsFile, header);\n+        canonicalChromosomes = SVUtils.getCanonicalChromosomes(nonCanonicalChromosomeNamesFile, refDict);\n+    }\n+\n+    @Override public void apply( GATKRead read, ReferenceContext referenceContext, FeatureContext featureContext ) {\n+        reference = referenceContext;\n+        final String contigName = read.getName();\n+        if ( !contigName.equals(currentContigName) ) {\n+            if ( !readsForCurrentContig.isEmpty() ) {\n+                processContigAlignments(readsForCurrentContig);\n+                readsForCurrentContig.clear();\n+            }\n+            currentContigName = contigName;\n+        }\n+        readsForCurrentContig.add(read);\n+    }\n+\n+    @Override public Object onTraversalSuccess() {\n+        final Object result = super.onTraversalSuccess();\n+\n+        if ( !readsForCurrentContig.isEmpty() ) {\n+            processContigAlignments(readsForCurrentContig);\n+        }\n+\n+        final List<VariantContext> variants = new ArrayList<>(2 * simpleMap.size());\n+        for ( final SimpleNovelAdjacencyAndChimericAlignmentEvidence novelAdjacencyAndEvidence : simpleMap.values() ) {\n+            final List<SvType> svTypes =\n+                    novelAdjacencyAndEvidence.getNovelAdjacencyReferenceLocations().toSimpleOrBNDTypes(reference);\n+            variants.addAll(novelAdjacencyAndEvidence.turnIntoVariantContexts(svTypes, sampleId, refDict, cnvCalls));\n+        }\n+        final ZeroAndOneSegmentCpxVariantExtractor zeroAndOneSegmentCpxVariantExtractor =\n+                new ZeroAndOneSegmentCpxVariantExtractor();\n+        final List<VariantContext> multiSegmentVariants = new ArrayList<>(complexMap.size());\n+        for ( final Map.Entry<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> entry :\n+                complexMap.entrySet() ) {\n+            final VariantContext variantContext =\n+                    CpxVariantInterpreter.turnIntoVariantContext(entry.getKey(), entry.getValue(), reference);\n+            final int refSegs =\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CPX_SV_REF_SEGMENTS).size();\n+            if ( refSegs < 2 ) {\n+                variants.addAll(zeroAndOneSegmentCpxVariantExtractor.extract(variantContext, reference));\n+            } else {\n+                multiSegmentVariants.add(variantContext);\n+            }\n+        }\n+        final Map<String, RelevantAttributes> contigNameToCpxVariantAttributes =\n+                new HashMap<>(2 * multiSegmentVariants.size());\n+        for ( final VariantContext variantContext : multiSegmentVariants ) {\n+            final RelevantAttributes relevantAttributes = new RelevantAttributes(variantContext);\n+            for ( final String contigName :\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CONTIG_NAMES) ) {\n+                contigNameToCpxVariantAttributes.put(contigName, relevantAttributes);\n+            }\n+        }\n+        final Map<NovelAdjacencyAndAltHaplotype, List<SimpleChimera>> redoMap = new HashMap<>();", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAxMjAyMw==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459012023", "bodyText": "mapVal -> chimerasForNovelAdjacency?", "author": "cwhelan", "createdAt": "2020-07-22T18:53:13Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package org.broadinstitute.hellbender.tools;\n+\n+import htsjdk.samtools.SAMFileHeader;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.BasicReference;\n+import org.broadinstitute.hellbender.engine.FeatureContext;\n+import org.broadinstitute.hellbender.engine.ReadWalker;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignmentInterval;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInducingAssemblyContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.MultiSegmentsCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.RelevantAttributes;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.ZeroAndOneSegmentCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleChimera;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyAndChimericAlignmentEvidence;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.CNVInputReader;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVIntervalTree;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVUtils;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVVCFWriter;\n+import org.broadinstitute.hellbender.utils.BaseUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import scala.Tuple2;\n+\n+import java.io.BufferedWriter;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.*;\n+\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        oneLineSummary = \"(Internal) Examines aligned contigs from local assemblies and calls structural variants or their breakpoints\",\n+        summary =\n+            \"This tool takes a file containing the alignments of assembled contigs and searches it for contigs with\" +\n+            \" split alignments or large gaps indicating the presence of structural variation breakpoints.\" +\n+            \" Variations' types are determined by analyzing the signatures of the split alignments,\" +\n+            \" and are written to a VCF file.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class)\n+public class StructuralVariantDiscoverer extends ReadWalker {\n+    @Argument(doc = \"Name of output VCF.\", shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            fullName = \"outputVCFName\")\n+    private static String outputVCFName;\n+\n+    @Argument(doc = \"file containing non-canonical chromosome names (e.g chrUn_KI270588v1) in the reference, \" +\n+            \"human reference (hg19 or hg38) assumed when omitted\", shortName = \"alt-tigs\",\n+            fullName = \"non-canonical-contig-names-file\", optional = true)\n+    private static String nonCanonicalChromosomeNamesFile;\n+\n+    @ArgumentCollection\n+    private static final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoverStageArgs =\n+            new DiscoverVariantsFromContigAlignmentsSparkArgumentCollection();\n+\n+    private static final double SCORE_DIFF_TOLERANCE = 0.;\n+\n+    private String sampleId;\n+    private SAMSequenceDictionary refDict;\n+    private BasicReference reference;\n+    private SVIntervalTree<VariantContext> cnvCalls;\n+    private Set<String> canonicalChromosomes;\n+\n+    private String currentContigName = null;\n+    private final List<GATKRead> readsForCurrentContig = new ArrayList<>();\n+    private final Map<NovelAdjacencyAndAltHaplotype, SimpleNovelAdjacencyAndChimericAlignmentEvidence> simpleMap =\n+            new HashMap<>(10000);\n+    private final Map<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> complexMap =\n+            new HashMap<>(1000);\n+    private final List<AlignedContig> complexContigs = new ArrayList<>(1000);\n+\n+    @Override public boolean requiresReads() { return true; }\n+    @Override public boolean requiresReference() { return true; }\n+\n+    @Override public List<ReadFilter> getDefaultReadFilters() {\n+        return Arrays.asList(ReadFilterLibrary.MAPPED, ReadFilterLibrary.NOT_SECONDARY_ALIGNMENT);\n+    }\n+\n+    @Override public void onTraversalStart() {\n+        final SAMFileHeader header = getHeaderForReads();\n+        if ( header.getSortOrder() != SAMFileHeader.SortOrder.queryname ) {\n+            throw new UserException(\"This tool requires a queryname-sorted source of reads.\");\n+        }\n+        sampleId = SVUtils.getSampleId(header);\n+        refDict = header.getSequenceDictionary();\n+        cnvCalls = discoverStageArgs.cnvCallsFile == null ? null :\n+                CNVInputReader.loadCNVCalls(discoverStageArgs.cnvCallsFile, header);\n+        canonicalChromosomes = SVUtils.getCanonicalChromosomes(nonCanonicalChromosomeNamesFile, refDict);\n+    }\n+\n+    @Override public void apply( GATKRead read, ReferenceContext referenceContext, FeatureContext featureContext ) {\n+        reference = referenceContext;\n+        final String contigName = read.getName();\n+        if ( !contigName.equals(currentContigName) ) {\n+            if ( !readsForCurrentContig.isEmpty() ) {\n+                processContigAlignments(readsForCurrentContig);\n+                readsForCurrentContig.clear();\n+            }\n+            currentContigName = contigName;\n+        }\n+        readsForCurrentContig.add(read);\n+    }\n+\n+    @Override public Object onTraversalSuccess() {\n+        final Object result = super.onTraversalSuccess();\n+\n+        if ( !readsForCurrentContig.isEmpty() ) {\n+            processContigAlignments(readsForCurrentContig);\n+        }\n+\n+        final List<VariantContext> variants = new ArrayList<>(2 * simpleMap.size());\n+        for ( final SimpleNovelAdjacencyAndChimericAlignmentEvidence novelAdjacencyAndEvidence : simpleMap.values() ) {\n+            final List<SvType> svTypes =\n+                    novelAdjacencyAndEvidence.getNovelAdjacencyReferenceLocations().toSimpleOrBNDTypes(reference);\n+            variants.addAll(novelAdjacencyAndEvidence.turnIntoVariantContexts(svTypes, sampleId, refDict, cnvCalls));\n+        }\n+        final ZeroAndOneSegmentCpxVariantExtractor zeroAndOneSegmentCpxVariantExtractor =\n+                new ZeroAndOneSegmentCpxVariantExtractor();\n+        final List<VariantContext> multiSegmentVariants = new ArrayList<>(complexMap.size());\n+        for ( final Map.Entry<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> entry :\n+                complexMap.entrySet() ) {\n+            final VariantContext variantContext =\n+                    CpxVariantInterpreter.turnIntoVariantContext(entry.getKey(), entry.getValue(), reference);\n+            final int refSegs =\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CPX_SV_REF_SEGMENTS).size();\n+            if ( refSegs < 2 ) {\n+                variants.addAll(zeroAndOneSegmentCpxVariantExtractor.extract(variantContext, reference));\n+            } else {\n+                multiSegmentVariants.add(variantContext);\n+            }\n+        }\n+        final Map<String, RelevantAttributes> contigNameToCpxVariantAttributes =\n+                new HashMap<>(2 * multiSegmentVariants.size());\n+        for ( final VariantContext variantContext : multiSegmentVariants ) {\n+            final RelevantAttributes relevantAttributes = new RelevantAttributes(variantContext);\n+            for ( final String contigName :\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CONTIG_NAMES) ) {\n+                contigNameToCpxVariantAttributes.put(contigName, relevantAttributes);\n+            }\n+        }\n+        final Map<NovelAdjacencyAndAltHaplotype, List<SimpleChimera>> redoMap = new HashMap<>();\n+        for ( final AlignedContig alignedContig : complexContigs ) {\n+            if ( contigNameToCpxVariantAttributes.containsKey(alignedContig.getContigName()) &&\n+                    alignedContig.getAlignments().size() > 1 ) {\n+                final List<SimpleChimera> chimeras =\n+                        ContigChimericAlignmentIterativeInterpreter.parseOneContig(\n+                                alignedContig,\n+                                refDict,\n+                                true,\n+                                DiscoverVariantsFromContigAlignmentsSparkArgumentCollection.DEFAULT_MIN_ALIGNMENT_LENGTH,\n+                                DiscoverVariantsFromContigAlignmentsSparkArgumentCollection.CHIMERIC_ALIGNMENTS_HIGHMQ_THRESHOLD,\n+                                true);\n+                for ( final SimpleChimera simpleChimera : chimeras ) {\n+                    final NovelAdjacencyAndAltHaplotype novelAdjacency =\n+                            new NovelAdjacencyAndAltHaplotype(simpleChimera, alignedContig.getContigSequence(), refDict);\n+                    final List<SimpleChimera> mapVal = redoMap.get(novelAdjacency);", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAxNDUyOA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459014528", "bodyText": "filterMergedVariantList would be a more accurate name", "author": "cwhelan", "createdAt": "2020-07-22T18:57:29Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package org.broadinstitute.hellbender.tools;\n+\n+import htsjdk.samtools.SAMFileHeader;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.BasicReference;\n+import org.broadinstitute.hellbender.engine.FeatureContext;\n+import org.broadinstitute.hellbender.engine.ReadWalker;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignmentInterval;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInducingAssemblyContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.MultiSegmentsCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.RelevantAttributes;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.ZeroAndOneSegmentCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleChimera;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyAndChimericAlignmentEvidence;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.CNVInputReader;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVIntervalTree;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVUtils;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVVCFWriter;\n+import org.broadinstitute.hellbender.utils.BaseUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import scala.Tuple2;\n+\n+import java.io.BufferedWriter;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.*;\n+\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        oneLineSummary = \"(Internal) Examines aligned contigs from local assemblies and calls structural variants or their breakpoints\",\n+        summary =\n+            \"This tool takes a file containing the alignments of assembled contigs and searches it for contigs with\" +\n+            \" split alignments or large gaps indicating the presence of structural variation breakpoints.\" +\n+            \" Variations' types are determined by analyzing the signatures of the split alignments,\" +\n+            \" and are written to a VCF file.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class)\n+public class StructuralVariantDiscoverer extends ReadWalker {\n+    @Argument(doc = \"Name of output VCF.\", shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            fullName = \"outputVCFName\")\n+    private static String outputVCFName;\n+\n+    @Argument(doc = \"file containing non-canonical chromosome names (e.g chrUn_KI270588v1) in the reference, \" +\n+            \"human reference (hg19 or hg38) assumed when omitted\", shortName = \"alt-tigs\",\n+            fullName = \"non-canonical-contig-names-file\", optional = true)\n+    private static String nonCanonicalChromosomeNamesFile;\n+\n+    @ArgumentCollection\n+    private static final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoverStageArgs =\n+            new DiscoverVariantsFromContigAlignmentsSparkArgumentCollection();\n+\n+    private static final double SCORE_DIFF_TOLERANCE = 0.;\n+\n+    private String sampleId;\n+    private SAMSequenceDictionary refDict;\n+    private BasicReference reference;\n+    private SVIntervalTree<VariantContext> cnvCalls;\n+    private Set<String> canonicalChromosomes;\n+\n+    private String currentContigName = null;\n+    private final List<GATKRead> readsForCurrentContig = new ArrayList<>();\n+    private final Map<NovelAdjacencyAndAltHaplotype, SimpleNovelAdjacencyAndChimericAlignmentEvidence> simpleMap =\n+            new HashMap<>(10000);\n+    private final Map<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> complexMap =\n+            new HashMap<>(1000);\n+    private final List<AlignedContig> complexContigs = new ArrayList<>(1000);\n+\n+    @Override public boolean requiresReads() { return true; }\n+    @Override public boolean requiresReference() { return true; }\n+\n+    @Override public List<ReadFilter> getDefaultReadFilters() {\n+        return Arrays.asList(ReadFilterLibrary.MAPPED, ReadFilterLibrary.NOT_SECONDARY_ALIGNMENT);\n+    }\n+\n+    @Override public void onTraversalStart() {\n+        final SAMFileHeader header = getHeaderForReads();\n+        if ( header.getSortOrder() != SAMFileHeader.SortOrder.queryname ) {\n+            throw new UserException(\"This tool requires a queryname-sorted source of reads.\");\n+        }\n+        sampleId = SVUtils.getSampleId(header);\n+        refDict = header.getSequenceDictionary();\n+        cnvCalls = discoverStageArgs.cnvCallsFile == null ? null :\n+                CNVInputReader.loadCNVCalls(discoverStageArgs.cnvCallsFile, header);\n+        canonicalChromosomes = SVUtils.getCanonicalChromosomes(nonCanonicalChromosomeNamesFile, refDict);\n+    }\n+\n+    @Override public void apply( GATKRead read, ReferenceContext referenceContext, FeatureContext featureContext ) {\n+        reference = referenceContext;\n+        final String contigName = read.getName();\n+        if ( !contigName.equals(currentContigName) ) {\n+            if ( !readsForCurrentContig.isEmpty() ) {\n+                processContigAlignments(readsForCurrentContig);\n+                readsForCurrentContig.clear();\n+            }\n+            currentContigName = contigName;\n+        }\n+        readsForCurrentContig.add(read);\n+    }\n+\n+    @Override public Object onTraversalSuccess() {\n+        final Object result = super.onTraversalSuccess();\n+\n+        if ( !readsForCurrentContig.isEmpty() ) {\n+            processContigAlignments(readsForCurrentContig);\n+        }\n+\n+        final List<VariantContext> variants = new ArrayList<>(2 * simpleMap.size());\n+        for ( final SimpleNovelAdjacencyAndChimericAlignmentEvidence novelAdjacencyAndEvidence : simpleMap.values() ) {\n+            final List<SvType> svTypes =\n+                    novelAdjacencyAndEvidence.getNovelAdjacencyReferenceLocations().toSimpleOrBNDTypes(reference);\n+            variants.addAll(novelAdjacencyAndEvidence.turnIntoVariantContexts(svTypes, sampleId, refDict, cnvCalls));\n+        }\n+        final ZeroAndOneSegmentCpxVariantExtractor zeroAndOneSegmentCpxVariantExtractor =\n+                new ZeroAndOneSegmentCpxVariantExtractor();\n+        final List<VariantContext> multiSegmentVariants = new ArrayList<>(complexMap.size());\n+        for ( final Map.Entry<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> entry :\n+                complexMap.entrySet() ) {\n+            final VariantContext variantContext =\n+                    CpxVariantInterpreter.turnIntoVariantContext(entry.getKey(), entry.getValue(), reference);\n+            final int refSegs =\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CPX_SV_REF_SEGMENTS).size();\n+            if ( refSegs < 2 ) {\n+                variants.addAll(zeroAndOneSegmentCpxVariantExtractor.extract(variantContext, reference));\n+            } else {\n+                multiSegmentVariants.add(variantContext);\n+            }\n+        }\n+        final Map<String, RelevantAttributes> contigNameToCpxVariantAttributes =\n+                new HashMap<>(2 * multiSegmentVariants.size());\n+        for ( final VariantContext variantContext : multiSegmentVariants ) {\n+            final RelevantAttributes relevantAttributes = new RelevantAttributes(variantContext);\n+            for ( final String contigName :\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CONTIG_NAMES) ) {\n+                contigNameToCpxVariantAttributes.put(contigName, relevantAttributes);\n+            }\n+        }\n+        final Map<NovelAdjacencyAndAltHaplotype, List<SimpleChimera>> redoMap = new HashMap<>();\n+        for ( final AlignedContig alignedContig : complexContigs ) {\n+            if ( contigNameToCpxVariantAttributes.containsKey(alignedContig.getContigName()) &&\n+                    alignedContig.getAlignments().size() > 1 ) {\n+                final List<SimpleChimera> chimeras =\n+                        ContigChimericAlignmentIterativeInterpreter.parseOneContig(\n+                                alignedContig,\n+                                refDict,\n+                                true,\n+                                DiscoverVariantsFromContigAlignmentsSparkArgumentCollection.DEFAULT_MIN_ALIGNMENT_LENGTH,\n+                                DiscoverVariantsFromContigAlignmentsSparkArgumentCollection.CHIMERIC_ALIGNMENTS_HIGHMQ_THRESHOLD,\n+                                true);\n+                for ( final SimpleChimera simpleChimera : chimeras ) {\n+                    final NovelAdjacencyAndAltHaplotype novelAdjacency =\n+                            new NovelAdjacencyAndAltHaplotype(simpleChimera, alignedContig.getContigSequence(), refDict);\n+                    final List<SimpleChimera> mapVal = redoMap.get(novelAdjacency);\n+                    if ( mapVal != null ) {\n+                        mapVal.add(simpleChimera);\n+                    } else {\n+                        final List<SimpleChimera> newList = new ArrayList<>(2);\n+                        newList.add(simpleChimera);\n+                        redoMap.put(novelAdjacency, newList);\n+                    }\n+                }\n+            }\n+        }\n+        final List<VariantContext> reinterpretedVariants = new ArrayList<>(redoMap.size());\n+        for ( Map.Entry<NovelAdjacencyAndAltHaplotype, List<SimpleChimera>> entry : redoMap.entrySet() ) {\n+            reinterpretedVariants.add(\n+                new SimpleNovelAdjacencyAndChimericAlignmentEvidence(entry.getKey(), entry.getValue())\n+                        .produceAnnotatedVcFromAssemblyEvidence(\n+                                ContigChimericAlignmentIterativeInterpreter\n+                                        .inferSimpleTypeFromNovelAdjacency(entry.getKey(), reference),\n+                                refDict, cnvCalls, sampleId).make());\n+        }\n+        final List<VariantContext> extractedMultiSegmentVariants = new ArrayList<>(multiSegmentVariants.size());\n+        final MultiSegmentsCpxVariantExtractor multiSegmentsCpxVariantExtractor =\n+                new MultiSegmentsCpxVariantExtractor();\n+        for ( final VariantContext variantContext : multiSegmentVariants ) {\n+            extractedMultiSegmentVariants.addAll(multiSegmentsCpxVariantExtractor.extract(variantContext, reference));\n+        }\n+        final List<VariantContext> consistentVariants =\n+                SegmentedCpxVariantSimpleVariantExtractor\n+                        .filterForConsistency(reinterpretedVariants, contigNameToCpxVariantAttributes, reference);\n+        variants.addAll(SegmentedCpxVariantSimpleVariantExtractor.removeDuplicates(extractedMultiSegmentVariants, consistentVariants));\n+\n+        final List<VariantContext> filteredVariants = AnnotatedVariantProducer.filterMergedVCF(variants, discoverStageArgs);", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAxNjc2Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459016762", "bodyText": "I know this wasn't your method name, but could you rename this method, notDiscardForBadMQ, to hasGoodMQ? Double negatives made my head hurt here.", "author": "cwhelan", "createdAt": "2020-07-22T19:01:20Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package org.broadinstitute.hellbender.tools;\n+\n+import htsjdk.samtools.SAMFileHeader;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.BasicReference;\n+import org.broadinstitute.hellbender.engine.FeatureContext;\n+import org.broadinstitute.hellbender.engine.ReadWalker;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignmentInterval;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInducingAssemblyContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.MultiSegmentsCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.RelevantAttributes;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.ZeroAndOneSegmentCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleChimera;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyAndChimericAlignmentEvidence;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.CNVInputReader;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVIntervalTree;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVUtils;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVVCFWriter;\n+import org.broadinstitute.hellbender.utils.BaseUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import scala.Tuple2;\n+\n+import java.io.BufferedWriter;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.*;\n+\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        oneLineSummary = \"(Internal) Examines aligned contigs from local assemblies and calls structural variants or their breakpoints\",\n+        summary =\n+            \"This tool takes a file containing the alignments of assembled contigs and searches it for contigs with\" +\n+            \" split alignments or large gaps indicating the presence of structural variation breakpoints.\" +\n+            \" Variations' types are determined by analyzing the signatures of the split alignments,\" +\n+            \" and are written to a VCF file.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class)\n+public class StructuralVariantDiscoverer extends ReadWalker {\n+    @Argument(doc = \"Name of output VCF.\", shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            fullName = \"outputVCFName\")\n+    private static String outputVCFName;\n+\n+    @Argument(doc = \"file containing non-canonical chromosome names (e.g chrUn_KI270588v1) in the reference, \" +\n+            \"human reference (hg19 or hg38) assumed when omitted\", shortName = \"alt-tigs\",\n+            fullName = \"non-canonical-contig-names-file\", optional = true)\n+    private static String nonCanonicalChromosomeNamesFile;\n+\n+    @ArgumentCollection\n+    private static final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoverStageArgs =\n+            new DiscoverVariantsFromContigAlignmentsSparkArgumentCollection();\n+\n+    private static final double SCORE_DIFF_TOLERANCE = 0.;\n+\n+    private String sampleId;\n+    private SAMSequenceDictionary refDict;\n+    private BasicReference reference;\n+    private SVIntervalTree<VariantContext> cnvCalls;\n+    private Set<String> canonicalChromosomes;\n+\n+    private String currentContigName = null;\n+    private final List<GATKRead> readsForCurrentContig = new ArrayList<>();\n+    private final Map<NovelAdjacencyAndAltHaplotype, SimpleNovelAdjacencyAndChimericAlignmentEvidence> simpleMap =\n+            new HashMap<>(10000);\n+    private final Map<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> complexMap =\n+            new HashMap<>(1000);\n+    private final List<AlignedContig> complexContigs = new ArrayList<>(1000);\n+\n+    @Override public boolean requiresReads() { return true; }\n+    @Override public boolean requiresReference() { return true; }\n+\n+    @Override public List<ReadFilter> getDefaultReadFilters() {\n+        return Arrays.asList(ReadFilterLibrary.MAPPED, ReadFilterLibrary.NOT_SECONDARY_ALIGNMENT);\n+    }\n+\n+    @Override public void onTraversalStart() {\n+        final SAMFileHeader header = getHeaderForReads();\n+        if ( header.getSortOrder() != SAMFileHeader.SortOrder.queryname ) {\n+            throw new UserException(\"This tool requires a queryname-sorted source of reads.\");\n+        }\n+        sampleId = SVUtils.getSampleId(header);\n+        refDict = header.getSequenceDictionary();\n+        cnvCalls = discoverStageArgs.cnvCallsFile == null ? null :\n+                CNVInputReader.loadCNVCalls(discoverStageArgs.cnvCallsFile, header);\n+        canonicalChromosomes = SVUtils.getCanonicalChromosomes(nonCanonicalChromosomeNamesFile, refDict);\n+    }\n+\n+    @Override public void apply( GATKRead read, ReferenceContext referenceContext, FeatureContext featureContext ) {\n+        reference = referenceContext;\n+        final String contigName = read.getName();\n+        if ( !contigName.equals(currentContigName) ) {\n+            if ( !readsForCurrentContig.isEmpty() ) {\n+                processContigAlignments(readsForCurrentContig);\n+                readsForCurrentContig.clear();\n+            }\n+            currentContigName = contigName;\n+        }\n+        readsForCurrentContig.add(read);\n+    }\n+\n+    @Override public Object onTraversalSuccess() {\n+        final Object result = super.onTraversalSuccess();\n+\n+        if ( !readsForCurrentContig.isEmpty() ) {\n+            processContigAlignments(readsForCurrentContig);\n+        }\n+\n+        final List<VariantContext> variants = new ArrayList<>(2 * simpleMap.size());\n+        for ( final SimpleNovelAdjacencyAndChimericAlignmentEvidence novelAdjacencyAndEvidence : simpleMap.values() ) {\n+            final List<SvType> svTypes =\n+                    novelAdjacencyAndEvidence.getNovelAdjacencyReferenceLocations().toSimpleOrBNDTypes(reference);\n+            variants.addAll(novelAdjacencyAndEvidence.turnIntoVariantContexts(svTypes, sampleId, refDict, cnvCalls));\n+        }\n+        final ZeroAndOneSegmentCpxVariantExtractor zeroAndOneSegmentCpxVariantExtractor =\n+                new ZeroAndOneSegmentCpxVariantExtractor();\n+        final List<VariantContext> multiSegmentVariants = new ArrayList<>(complexMap.size());\n+        for ( final Map.Entry<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> entry :\n+                complexMap.entrySet() ) {\n+            final VariantContext variantContext =\n+                    CpxVariantInterpreter.turnIntoVariantContext(entry.getKey(), entry.getValue(), reference);\n+            final int refSegs =\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CPX_SV_REF_SEGMENTS).size();\n+            if ( refSegs < 2 ) {\n+                variants.addAll(zeroAndOneSegmentCpxVariantExtractor.extract(variantContext, reference));\n+            } else {\n+                multiSegmentVariants.add(variantContext);\n+            }\n+        }\n+        final Map<String, RelevantAttributes> contigNameToCpxVariantAttributes =\n+                new HashMap<>(2 * multiSegmentVariants.size());\n+        for ( final VariantContext variantContext : multiSegmentVariants ) {\n+            final RelevantAttributes relevantAttributes = new RelevantAttributes(variantContext);\n+            for ( final String contigName :\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CONTIG_NAMES) ) {\n+                contigNameToCpxVariantAttributes.put(contigName, relevantAttributes);\n+            }\n+        }\n+        final Map<NovelAdjacencyAndAltHaplotype, List<SimpleChimera>> redoMap = new HashMap<>();\n+        for ( final AlignedContig alignedContig : complexContigs ) {\n+            if ( contigNameToCpxVariantAttributes.containsKey(alignedContig.getContigName()) &&\n+                    alignedContig.getAlignments().size() > 1 ) {\n+                final List<SimpleChimera> chimeras =\n+                        ContigChimericAlignmentIterativeInterpreter.parseOneContig(\n+                                alignedContig,\n+                                refDict,\n+                                true,\n+                                DiscoverVariantsFromContigAlignmentsSparkArgumentCollection.DEFAULT_MIN_ALIGNMENT_LENGTH,\n+                                DiscoverVariantsFromContigAlignmentsSparkArgumentCollection.CHIMERIC_ALIGNMENTS_HIGHMQ_THRESHOLD,\n+                                true);\n+                for ( final SimpleChimera simpleChimera : chimeras ) {\n+                    final NovelAdjacencyAndAltHaplotype novelAdjacency =\n+                            new NovelAdjacencyAndAltHaplotype(simpleChimera, alignedContig.getContigSequence(), refDict);\n+                    final List<SimpleChimera> mapVal = redoMap.get(novelAdjacency);\n+                    if ( mapVal != null ) {\n+                        mapVal.add(simpleChimera);\n+                    } else {\n+                        final List<SimpleChimera> newList = new ArrayList<>(2);\n+                        newList.add(simpleChimera);\n+                        redoMap.put(novelAdjacency, newList);\n+                    }\n+                }\n+            }\n+        }\n+        final List<VariantContext> reinterpretedVariants = new ArrayList<>(redoMap.size());\n+        for ( Map.Entry<NovelAdjacencyAndAltHaplotype, List<SimpleChimera>> entry : redoMap.entrySet() ) {\n+            reinterpretedVariants.add(\n+                new SimpleNovelAdjacencyAndChimericAlignmentEvidence(entry.getKey(), entry.getValue())\n+                        .produceAnnotatedVcFromAssemblyEvidence(\n+                                ContigChimericAlignmentIterativeInterpreter\n+                                        .inferSimpleTypeFromNovelAdjacency(entry.getKey(), reference),\n+                                refDict, cnvCalls, sampleId).make());\n+        }\n+        final List<VariantContext> extractedMultiSegmentVariants = new ArrayList<>(multiSegmentVariants.size());\n+        final MultiSegmentsCpxVariantExtractor multiSegmentsCpxVariantExtractor =\n+                new MultiSegmentsCpxVariantExtractor();\n+        for ( final VariantContext variantContext : multiSegmentVariants ) {\n+            extractedMultiSegmentVariants.addAll(multiSegmentsCpxVariantExtractor.extract(variantContext, reference));\n+        }\n+        final List<VariantContext> consistentVariants =\n+                SegmentedCpxVariantSimpleVariantExtractor\n+                        .filterForConsistency(reinterpretedVariants, contigNameToCpxVariantAttributes, reference);\n+        variants.addAll(SegmentedCpxVariantSimpleVariantExtractor.removeDuplicates(extractedMultiSegmentVariants, consistentVariants));\n+\n+        final List<VariantContext> filteredVariants = AnnotatedVariantProducer.filterMergedVCF(variants, discoverStageArgs);\n+        SVVCFWriter.writeVCF(filteredVariants, outputVCFName, refDict, getDefaultToolVCFHeaderLines(), logger);\n+        return result;\n+    }\n+\n+    private void processContigAlignments( final List<GATKRead> contigAlignments ) {\n+        final List<AlignmentInterval> alignmentIntervals = new ArrayList<>(contigAlignments.size());\n+        String contigName = null;\n+        byte[] contigSequence = null;\n+        for ( final GATKRead read : contigAlignments ) {\n+            contigName = read.getName();\n+            if ( !read.isSupplementaryAlignment() ) {\n+                contigSequence = read.getBasesNoCopy();\n+                if ( read.isReverseStrand() ) {\n+                    contigSequence = BaseUtils.simpleReverseComplement(contigSequence);\n+                }\n+            }\n+            alignmentIntervals.add(new AlignmentInterval(read));\n+        }\n+        if ( contigSequence == null ) {\n+            throw new UserException(\"No primary line for \" + contigName);\n+        }\n+        final AlignedContig alignedContig = new AlignedContig(contigName, contigSequence, alignmentIntervals);\n+        if ( !alignedContig.notDiscardForBadMQ() ) return;", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAyMDI2Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459020263", "bodyText": "Since it looks like you moved pickBestConfigurations into this method (reConstructContigFromPickedConfiguration) could you call this reconstructContigFromBestConfiguration (assuming that we picked the best one)?", "author": "cwhelan", "createdAt": "2020-07-22T19:07:29Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package org.broadinstitute.hellbender.tools;\n+\n+import htsjdk.samtools.SAMFileHeader;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.BasicReference;\n+import org.broadinstitute.hellbender.engine.FeatureContext;\n+import org.broadinstitute.hellbender.engine.ReadWalker;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignmentInterval;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInducingAssemblyContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.MultiSegmentsCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.RelevantAttributes;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.ZeroAndOneSegmentCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleChimera;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyAndChimericAlignmentEvidence;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.CNVInputReader;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVIntervalTree;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVUtils;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVVCFWriter;\n+import org.broadinstitute.hellbender.utils.BaseUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import scala.Tuple2;\n+\n+import java.io.BufferedWriter;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.*;\n+\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        oneLineSummary = \"(Internal) Examines aligned contigs from local assemblies and calls structural variants or their breakpoints\",\n+        summary =\n+            \"This tool takes a file containing the alignments of assembled contigs and searches it for contigs with\" +\n+            \" split alignments or large gaps indicating the presence of structural variation breakpoints.\" +\n+            \" Variations' types are determined by analyzing the signatures of the split alignments,\" +\n+            \" and are written to a VCF file.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class)\n+public class StructuralVariantDiscoverer extends ReadWalker {\n+    @Argument(doc = \"Name of output VCF.\", shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            fullName = \"outputVCFName\")\n+    private static String outputVCFName;\n+\n+    @Argument(doc = \"file containing non-canonical chromosome names (e.g chrUn_KI270588v1) in the reference, \" +\n+            \"human reference (hg19 or hg38) assumed when omitted\", shortName = \"alt-tigs\",\n+            fullName = \"non-canonical-contig-names-file\", optional = true)\n+    private static String nonCanonicalChromosomeNamesFile;\n+\n+    @ArgumentCollection\n+    private static final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoverStageArgs =\n+            new DiscoverVariantsFromContigAlignmentsSparkArgumentCollection();\n+\n+    private static final double SCORE_DIFF_TOLERANCE = 0.;\n+\n+    private String sampleId;\n+    private SAMSequenceDictionary refDict;\n+    private BasicReference reference;\n+    private SVIntervalTree<VariantContext> cnvCalls;\n+    private Set<String> canonicalChromosomes;\n+\n+    private String currentContigName = null;\n+    private final List<GATKRead> readsForCurrentContig = new ArrayList<>();\n+    private final Map<NovelAdjacencyAndAltHaplotype, SimpleNovelAdjacencyAndChimericAlignmentEvidence> simpleMap =\n+            new HashMap<>(10000);\n+    private final Map<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> complexMap =\n+            new HashMap<>(1000);\n+    private final List<AlignedContig> complexContigs = new ArrayList<>(1000);\n+\n+    @Override public boolean requiresReads() { return true; }\n+    @Override public boolean requiresReference() { return true; }\n+\n+    @Override public List<ReadFilter> getDefaultReadFilters() {\n+        return Arrays.asList(ReadFilterLibrary.MAPPED, ReadFilterLibrary.NOT_SECONDARY_ALIGNMENT);\n+    }\n+\n+    @Override public void onTraversalStart() {\n+        final SAMFileHeader header = getHeaderForReads();\n+        if ( header.getSortOrder() != SAMFileHeader.SortOrder.queryname ) {\n+            throw new UserException(\"This tool requires a queryname-sorted source of reads.\");\n+        }\n+        sampleId = SVUtils.getSampleId(header);\n+        refDict = header.getSequenceDictionary();\n+        cnvCalls = discoverStageArgs.cnvCallsFile == null ? null :\n+                CNVInputReader.loadCNVCalls(discoverStageArgs.cnvCallsFile, header);\n+        canonicalChromosomes = SVUtils.getCanonicalChromosomes(nonCanonicalChromosomeNamesFile, refDict);\n+    }\n+\n+    @Override public void apply( GATKRead read, ReferenceContext referenceContext, FeatureContext featureContext ) {\n+        reference = referenceContext;\n+        final String contigName = read.getName();\n+        if ( !contigName.equals(currentContigName) ) {\n+            if ( !readsForCurrentContig.isEmpty() ) {\n+                processContigAlignments(readsForCurrentContig);\n+                readsForCurrentContig.clear();\n+            }\n+            currentContigName = contigName;\n+        }\n+        readsForCurrentContig.add(read);\n+    }\n+\n+    @Override public Object onTraversalSuccess() {\n+        final Object result = super.onTraversalSuccess();\n+\n+        if ( !readsForCurrentContig.isEmpty() ) {\n+            processContigAlignments(readsForCurrentContig);\n+        }\n+\n+        final List<VariantContext> variants = new ArrayList<>(2 * simpleMap.size());\n+        for ( final SimpleNovelAdjacencyAndChimericAlignmentEvidence novelAdjacencyAndEvidence : simpleMap.values() ) {\n+            final List<SvType> svTypes =\n+                    novelAdjacencyAndEvidence.getNovelAdjacencyReferenceLocations().toSimpleOrBNDTypes(reference);\n+            variants.addAll(novelAdjacencyAndEvidence.turnIntoVariantContexts(svTypes, sampleId, refDict, cnvCalls));\n+        }\n+        final ZeroAndOneSegmentCpxVariantExtractor zeroAndOneSegmentCpxVariantExtractor =\n+                new ZeroAndOneSegmentCpxVariantExtractor();\n+        final List<VariantContext> multiSegmentVariants = new ArrayList<>(complexMap.size());\n+        for ( final Map.Entry<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> entry :\n+                complexMap.entrySet() ) {\n+            final VariantContext variantContext =\n+                    CpxVariantInterpreter.turnIntoVariantContext(entry.getKey(), entry.getValue(), reference);\n+            final int refSegs =\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CPX_SV_REF_SEGMENTS).size();\n+            if ( refSegs < 2 ) {\n+                variants.addAll(zeroAndOneSegmentCpxVariantExtractor.extract(variantContext, reference));\n+            } else {\n+                multiSegmentVariants.add(variantContext);\n+            }\n+        }\n+        final Map<String, RelevantAttributes> contigNameToCpxVariantAttributes =\n+                new HashMap<>(2 * multiSegmentVariants.size());\n+        for ( final VariantContext variantContext : multiSegmentVariants ) {\n+            final RelevantAttributes relevantAttributes = new RelevantAttributes(variantContext);\n+            for ( final String contigName :\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CONTIG_NAMES) ) {\n+                contigNameToCpxVariantAttributes.put(contigName, relevantAttributes);\n+            }\n+        }\n+        final Map<NovelAdjacencyAndAltHaplotype, List<SimpleChimera>> redoMap = new HashMap<>();\n+        for ( final AlignedContig alignedContig : complexContigs ) {\n+            if ( contigNameToCpxVariantAttributes.containsKey(alignedContig.getContigName()) &&\n+                    alignedContig.getAlignments().size() > 1 ) {\n+                final List<SimpleChimera> chimeras =\n+                        ContigChimericAlignmentIterativeInterpreter.parseOneContig(\n+                                alignedContig,\n+                                refDict,\n+                                true,\n+                                DiscoverVariantsFromContigAlignmentsSparkArgumentCollection.DEFAULT_MIN_ALIGNMENT_LENGTH,\n+                                DiscoverVariantsFromContigAlignmentsSparkArgumentCollection.CHIMERIC_ALIGNMENTS_HIGHMQ_THRESHOLD,\n+                                true);\n+                for ( final SimpleChimera simpleChimera : chimeras ) {\n+                    final NovelAdjacencyAndAltHaplotype novelAdjacency =\n+                            new NovelAdjacencyAndAltHaplotype(simpleChimera, alignedContig.getContigSequence(), refDict);\n+                    final List<SimpleChimera> mapVal = redoMap.get(novelAdjacency);\n+                    if ( mapVal != null ) {\n+                        mapVal.add(simpleChimera);\n+                    } else {\n+                        final List<SimpleChimera> newList = new ArrayList<>(2);\n+                        newList.add(simpleChimera);\n+                        redoMap.put(novelAdjacency, newList);\n+                    }\n+                }\n+            }\n+        }\n+        final List<VariantContext> reinterpretedVariants = new ArrayList<>(redoMap.size());\n+        for ( Map.Entry<NovelAdjacencyAndAltHaplotype, List<SimpleChimera>> entry : redoMap.entrySet() ) {\n+            reinterpretedVariants.add(\n+                new SimpleNovelAdjacencyAndChimericAlignmentEvidence(entry.getKey(), entry.getValue())\n+                        .produceAnnotatedVcFromAssemblyEvidence(\n+                                ContigChimericAlignmentIterativeInterpreter\n+                                        .inferSimpleTypeFromNovelAdjacency(entry.getKey(), reference),\n+                                refDict, cnvCalls, sampleId).make());\n+        }\n+        final List<VariantContext> extractedMultiSegmentVariants = new ArrayList<>(multiSegmentVariants.size());\n+        final MultiSegmentsCpxVariantExtractor multiSegmentsCpxVariantExtractor =\n+                new MultiSegmentsCpxVariantExtractor();\n+        for ( final VariantContext variantContext : multiSegmentVariants ) {\n+            extractedMultiSegmentVariants.addAll(multiSegmentsCpxVariantExtractor.extract(variantContext, reference));\n+        }\n+        final List<VariantContext> consistentVariants =\n+                SegmentedCpxVariantSimpleVariantExtractor\n+                        .filterForConsistency(reinterpretedVariants, contigNameToCpxVariantAttributes, reference);\n+        variants.addAll(SegmentedCpxVariantSimpleVariantExtractor.removeDuplicates(extractedMultiSegmentVariants, consistentVariants));\n+\n+        final List<VariantContext> filteredVariants = AnnotatedVariantProducer.filterMergedVCF(variants, discoverStageArgs);\n+        SVVCFWriter.writeVCF(filteredVariants, outputVCFName, refDict, getDefaultToolVCFHeaderLines(), logger);\n+        return result;\n+    }\n+\n+    private void processContigAlignments( final List<GATKRead> contigAlignments ) {\n+        final List<AlignmentInterval> alignmentIntervals = new ArrayList<>(contigAlignments.size());\n+        String contigName = null;\n+        byte[] contigSequence = null;\n+        for ( final GATKRead read : contigAlignments ) {\n+            contigName = read.getName();\n+            if ( !read.isSupplementaryAlignment() ) {\n+                contigSequence = read.getBasesNoCopy();\n+                if ( read.isReverseStrand() ) {\n+                    contigSequence = BaseUtils.simpleReverseComplement(contigSequence);\n+                }\n+            }\n+            alignmentIntervals.add(new AlignmentInterval(read));\n+        }\n+        if ( contigSequence == null ) {\n+            throw new UserException(\"No primary line for \" + contigName);\n+        }\n+        final AlignedContig alignedContig = new AlignedContig(contigName, contigSequence, alignmentIntervals);\n+        if ( !alignedContig.notDiscardForBadMQ() ) return;\n+\n+        final List<AssemblyContigWithFineTunedAlignments> fineTunedAlignmentsList =\n+            alignedContig.reConstructContigFromPickedConfiguration(canonicalChromosomes, SCORE_DIFF_TOLERANCE);", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAyNDM0NA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459024344", "bodyText": "It's not clear to me from reading this code what these two values are \"more relaxed\" than. Could you just call them SPLIT_PAIR_EVIDENCE_MQ_THRESHOLD and SPLIT_PAIR_EVIDENCE_ALIGNMENT_LENGTH_THRESHOLD, unless you have a better name?", "author": "cwhelan", "createdAt": "2020-07-22T19:15:04Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleChimera.java", "diffHunk": "@@ -25,6 +25,9 @@\n @DefaultSerializer(SimpleChimera.Serializer.class)\n public class SimpleChimera {\n \n+    static final int MORE_RELAXED_ALIGNMENT_MIN_LENGTH = 30;", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAyNTc1OA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459025758", "bodyText": "The comment for this method refers to DiscoverVariantsFromContigAlignmentsSAMSpark#nextAlignmentMayBeInsertion but I think that should actually be ContigChimericAlignmentIterativeInterpreter#nextAlignmentMayBeInsertion. Could you update?", "author": "cwhelan", "createdAt": "2020-07-22T19:17:46Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleChimera.java", "diffHunk": "@@ -102,18 +105,16 @@ public SimpleChimera(final AlignmentInterval intervalWithLowerCoordOnContig, fin\n      *  2) either alignment may consume only a \"short\" part of the contig, or if assuming that the alignment consumes\n      *     roughly the same amount of ref bases and read bases, has isAlignment that is too short\n      */\n-    static boolean splitPairStrongEnoughEvidenceForCA(final AlignmentInterval intervalOne,\n-                                                      final AlignmentInterval intervalTwo,\n-                                                      final int mapQThresholdInclusive,\n-                                                      final int alignmentLengthThresholdInclusive) {\n+    public static boolean splitPairStrongEnoughEvidenceForCA(final AlignmentInterval intervalOne,", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAxNzMxOQ==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r468017319", "bodyText": "How in the world did you catch that.  Impressive!", "author": "tedsharpe", "createdAt": "2020-08-10T16:08:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTAyNTc1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTA1NzU2MA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459057560", "bodyText": "As mentioned above, change this to filterMergedLVariantList", "author": "cwhelan", "createdAt": "2020-07-22T20:17:22Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java", "diffHunk": "@@ -282,4 +105,51 @@ static String produceCIInterval(final int point, final SVInterval ciInterval) {\n                 String.valueOf(ciInterval.getStart() - point),\n                 String.valueOf(ciInterval.getEnd() - point));\n     }\n+\n+    /**\n+     * Apply filters (that implements {@link SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter}) given list of variants,\n+     * and write the variants to a single VCF file.\n+     * @param variants variants to which filters are to be applied and written to file\n+     */\n+    public static List<VariantContext> filterMergedVCF( final List<VariantContext> variants,", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTA1ODkwOA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459058908", "bodyText": "Maybe it would be worth moving this interface up to not be an inner class anymore.", "author": "cwhelan", "createdAt": "2020-07-22T20:19:57Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java", "diffHunk": "@@ -282,4 +105,51 @@ static String produceCIInterval(final int point, final SVInterval ciInterval) {\n                 String.valueOf(ciInterval.getStart() - point),\n                 String.valueOf(ciInterval.getEnd() - point));\n     }\n+\n+    /**\n+     * Apply filters (that implements {@link SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter}) given list of variants,\n+     * and write the variants to a single VCF file.\n+     * @param variants variants to which filters are to be applied and written to file\n+     */\n+    public static List<VariantContext> filterMergedVCF( final List<VariantContext> variants,\n+                                                        final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoveryArgs ) {\n+        final List<VariantContext> variantsWithFilterApplied = new ArrayList<>(variants.size());\n+        final List<SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter> filters = Arrays.asList(", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTA1OTM0Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459059346", "bodyText": "What do you think about renaming this argument collection to remove Spark from the name, if it's going to be used in non-spark code paths?", "author": "cwhelan", "createdAt": "2020-07-22T20:20:40Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java", "diffHunk": "@@ -282,4 +105,51 @@ static String produceCIInterval(final int point, final SVInterval ciInterval) {\n                 String.valueOf(ciInterval.getStart() - point),\n                 String.valueOf(ciInterval.getEnd() - point));\n     }\n+\n+    /**\n+     * Apply filters (that implements {@link SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter}) given list of variants,\n+     * and write the variants to a single VCF file.\n+     * @param variants variants to which filters are to be applied and written to file\n+     */\n+    public static List<VariantContext> filterMergedVCF( final List<VariantContext> variants,\n+                                                        final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoveryArgs ) {", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTA2MDk1OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459060959", "bodyText": "Maybe worth pushing this up to top level too?", "author": "cwhelan", "createdAt": "2020-07-22T20:23:45Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/AnnotatedVariantProducer.java", "diffHunk": "@@ -282,4 +105,51 @@ static String produceCIInterval(final int point, final SVInterval ciInterval) {\n                 String.valueOf(ciInterval.getStart() - point),\n                 String.valueOf(ciInterval.getEnd() - point));\n     }\n+\n+    /**\n+     * Apply filters (that implements {@link SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter}) given list of variants,\n+     * and write the variants to a single VCF file.\n+     * @param variants variants to which filters are to be applied and written to file\n+     */\n+    public static List<VariantContext> filterMergedVCF( final List<VariantContext> variants,\n+                                                        final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoveryArgs ) {\n+        final List<VariantContext> variantsWithFilterApplied = new ArrayList<>(variants.size());\n+        final List<SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter> filters = Arrays.asList(\n+                new SvDiscoverFromLocalAssemblyContigAlignmentsSpark.SVMappingQualityFilter(discoveryArgs.minMQ),\n+                new SvDiscoverFromLocalAssemblyContigAlignmentsSpark.SVAlignmentLengthFilter(discoveryArgs.minAlignLength));\n+        for (final VariantContext variant : variants) {\n+            String svType = variant.getAttributeAsString(GATKSVVCFConstants.SVTYPE, \"\");\n+            if (svType.equals(GATKSVVCFConstants.SYMB_ALT_ALLELE_DEL) || svType.equals(GATKSVVCFConstants.SYMB_ALT_ALLELE_INS) || svType.equals(GATKSVVCFConstants.SYMB_ALT_ALLELE_DUP)) {\n+                if (Math.abs(variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0)) < StructuralVariationDiscoveryArgumentCollection.STRUCTURAL_VARIANT_SIZE_LOWER_BOUND )\n+                    continue;\n+            }\n+            variantsWithFilterApplied.add(applyFilters(variant, filters));\n+        }\n+        return variantsWithFilterApplied;\n+    }\n+\n+    /**\n+     * Filters out variants by testing against provided\n+     * filter key, threshold.\n+     *\n+     * Variants with value below specified threshold (or null value)\n+     * are filtered out citing given reason.\n+     *\n+     * @throws ClassCastException if the value corresponding to provided key cannot be casted as a {@link Double}\n+     */\n+    private static VariantContext applyFilters(final VariantContext variantContext,\n+                                               final List<SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter> filters) {\n+\n+        final Set<String> appliedFilters = new HashSet<>();\n+        for (final SvDiscoverFromLocalAssemblyContigAlignmentsSpark.StructuralVariantFilter filter : filters) {", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTUxMjkyNg==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459512926", "bodyText": "This parameter doesn't exist any more, can you update the comment, ie explain that you're now doing the pickAndFilterConfigurations call inside this method?", "author": "cwhelan", "createdAt": "2020-07-23T14:55:57Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/AlignedContig.java", "diffHunk": "@@ -93,28 +134,688 @@ public boolean isUnmapped() {\n         return alignmentIntervals;\n     }\n \n-    @Override\n-    public String toString() {\n-        return formatContigInfo(\n-                new Tuple2<>(contigName, alignmentIntervals.stream().map(AlignmentInterval::toPackedString).collect(Collectors.toList())));\n+\n+    /**\n+     * Idea is to keep mapped contig that\n+     * either has at least two alignments over {@link #ALIGNMENT_MQ_THRESHOLD},\n+     * or in the case of a single alignment, it must be MQ > {@link #ALIGNMENT_MQ_THRESHOLD}.\n+     * Note that we are not simply filtering out contigs with only 1 alignment because\n+     * they might contain large (> 50) gaps hence should be kept.\n+     * <p>\n+     * a point that could use improvements:\n+     * the current implementation exhaustively checks the power set of all possible alignments of each assembly contig,\n+     * which is computationally impossible for contigs having many-but-barely-any-good alignments, yet bringing in no value,\n+     * hence this primitive filtering step to get rid of these bad assembly contigs.\n+     */\n+    public boolean notDiscardForBadMQ() {\n+        if ( alignmentIntervals.size() < 2 ) {\n+            return (!alignmentIntervals.isEmpty()) && alignmentIntervals.get(0).mapQual > ALIGNMENT_MQ_THRESHOLD;\n+        } else {\n+            int notBadMappingsCount = 0; // either more than 1 non-bad mappings, or at least 1 non-bad mapping containing large gap\n+            for ( final AlignmentInterval alignment : alignmentIntervals ) {\n+                if ( alignment.mapQual > ALIGNMENT_MQ_THRESHOLD ) {\n+                    if ( alignment.containsGapOfEqualOrLargerSize(GAPPED_ALIGNMENT_BREAK_DEFAULT_SENSITIVITY) ) {\n+                        return true;// early return when a not-bad one contains a large gap\n+                    } else {\n+                        ++notBadMappingsCount;\n+                    }\n+                }\n+            }\n+            return notBadMappingsCount > 1;\n+        }\n+    }\n+\n+    /**\n+     * Reconstructs (possibly more than one) {@link AssemblyContigWithFineTunedAlignments} based on\n+     * the given best-scored configuration(s) in {@code nameSeqAndBestConfigurationsOfOneAssemblyContig}.", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTUzMjg1NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459532855", "bodyText": "All that seems to be left in this class is a couple of Spark-specific RDD methods and the SAMFormattedContigAlignmentParser, which has some RDD processing stuff and one method that does non-RDD specific logic (parseReadsAndOptionallySplitGappedAlignments). I might suggest moving the latter method to a non-spark-specific class, and renaming this class to something like AssemblyContigAlignmentsRDDProcessor to indicate that it's full of spark specific stuff.", "author": "cwhelan", "createdAt": "2020-07-23T15:22:54Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/AssemblyContigAlignmentsConfigPicker.java", "diffHunk": "@@ -32,49 +31,13 @@\n  */\n public class AssemblyContigAlignmentsConfigPicker {", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU0NDgxMg==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459544812", "bodyText": "I may have just been bogged down in the number of classes that have moved here somehow, but the old code path (in, for example, StructuralVariationDiscoveryPipelineSpark.filterAndConvertToAlignedContigViaSAM) used to go through SvDiscoverFromLocalAssemblyContigAlignmentsSpark.SAMFormattedContigAlignmentParserand particularly the methodparseReadsAndOptionallySplitGappedAlignments` which does some logic processing on the reads. Do you still need to send the reads through that, or is there some replacement for that logic here that I'm missing?", "author": "cwhelan", "createdAt": "2020-07-23T15:40:05Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package org.broadinstitute.hellbender.tools;\n+\n+import htsjdk.samtools.SAMFileHeader;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.BasicReference;\n+import org.broadinstitute.hellbender.engine.FeatureContext;\n+import org.broadinstitute.hellbender.engine.ReadWalker;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignmentInterval;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInducingAssemblyContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.MultiSegmentsCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.RelevantAttributes;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.ZeroAndOneSegmentCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleChimera;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyAndChimericAlignmentEvidence;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.CNVInputReader;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVIntervalTree;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVUtils;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVVCFWriter;\n+import org.broadinstitute.hellbender.utils.BaseUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import scala.Tuple2;\n+\n+import java.io.BufferedWriter;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.*;\n+\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        oneLineSummary = \"(Internal) Examines aligned contigs from local assemblies and calls structural variants or their breakpoints\",\n+        summary =\n+            \"This tool takes a file containing the alignments of assembled contigs and searches it for contigs with\" +\n+            \" split alignments or large gaps indicating the presence of structural variation breakpoints.\" +\n+            \" Variations' types are determined by analyzing the signatures of the split alignments,\" +\n+            \" and are written to a VCF file.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class)\n+public class StructuralVariantDiscoverer extends ReadWalker {\n+    @Argument(doc = \"Name of output VCF.\", shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            fullName = \"outputVCFName\")\n+    private static String outputVCFName;\n+\n+    @Argument(doc = \"file containing non-canonical chromosome names (e.g chrUn_KI270588v1) in the reference, \" +\n+            \"human reference (hg19 or hg38) assumed when omitted\", shortName = \"alt-tigs\",\n+            fullName = \"non-canonical-contig-names-file\", optional = true)\n+    private static String nonCanonicalChromosomeNamesFile;\n+\n+    @ArgumentCollection\n+    private static final DiscoverVariantsFromContigAlignmentsSparkArgumentCollection discoverStageArgs =\n+            new DiscoverVariantsFromContigAlignmentsSparkArgumentCollection();\n+\n+    private static final double SCORE_DIFF_TOLERANCE = 0.;\n+\n+    private String sampleId;\n+    private SAMSequenceDictionary refDict;\n+    private BasicReference reference;\n+    private SVIntervalTree<VariantContext> cnvCalls;\n+    private Set<String> canonicalChromosomes;\n+\n+    private String currentContigName = null;\n+    private final List<GATKRead> readsForCurrentContig = new ArrayList<>();\n+    private final Map<NovelAdjacencyAndAltHaplotype, SimpleNovelAdjacencyAndChimericAlignmentEvidence> simpleMap =\n+            new HashMap<>(10000);\n+    private final Map<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> complexMap =\n+            new HashMap<>(1000);\n+    private final List<AlignedContig> complexContigs = new ArrayList<>(1000);\n+\n+    @Override public boolean requiresReads() { return true; }\n+    @Override public boolean requiresReference() { return true; }\n+\n+    @Override public List<ReadFilter> getDefaultReadFilters() {\n+        return Arrays.asList(ReadFilterLibrary.MAPPED, ReadFilterLibrary.NOT_SECONDARY_ALIGNMENT);\n+    }\n+\n+    @Override public void onTraversalStart() {\n+        final SAMFileHeader header = getHeaderForReads();\n+        if ( header.getSortOrder() != SAMFileHeader.SortOrder.queryname ) {\n+            throw new UserException(\"This tool requires a queryname-sorted source of reads.\");\n+        }\n+        sampleId = SVUtils.getSampleId(header);\n+        refDict = header.getSequenceDictionary();\n+        cnvCalls = discoverStageArgs.cnvCallsFile == null ? null :\n+                CNVInputReader.loadCNVCalls(discoverStageArgs.cnvCallsFile, header);\n+        canonicalChromosomes = SVUtils.getCanonicalChromosomes(nonCanonicalChromosomeNamesFile, refDict);\n+    }\n+\n+    @Override public void apply( GATKRead read, ReferenceContext referenceContext, FeatureContext featureContext ) {\n+        reference = referenceContext;\n+        final String contigName = read.getName();\n+        if ( !contigName.equals(currentContigName) ) {\n+            if ( !readsForCurrentContig.isEmpty() ) {\n+                processContigAlignments(readsForCurrentContig);\n+                readsForCurrentContig.clear();\n+            }\n+            currentContigName = contigName;\n+        }\n+        readsForCurrentContig.add(read);\n+    }\n+\n+    @Override public Object onTraversalSuccess() {\n+        final Object result = super.onTraversalSuccess();\n+\n+        if ( !readsForCurrentContig.isEmpty() ) {\n+            processContigAlignments(readsForCurrentContig);\n+        }\n+\n+        final List<VariantContext> variants = new ArrayList<>(2 * simpleMap.size());\n+        for ( final SimpleNovelAdjacencyAndChimericAlignmentEvidence novelAdjacencyAndEvidence : simpleMap.values() ) {\n+            final List<SvType> svTypes =\n+                    novelAdjacencyAndEvidence.getNovelAdjacencyReferenceLocations().toSimpleOrBNDTypes(reference);\n+            variants.addAll(novelAdjacencyAndEvidence.turnIntoVariantContexts(svTypes, sampleId, refDict, cnvCalls));\n+        }\n+        final ZeroAndOneSegmentCpxVariantExtractor zeroAndOneSegmentCpxVariantExtractor =\n+                new ZeroAndOneSegmentCpxVariantExtractor();\n+        final List<VariantContext> multiSegmentVariants = new ArrayList<>(complexMap.size());\n+        for ( final Map.Entry<CpxVariantCanonicalRepresentation, List<CpxVariantInducingAssemblyContig>> entry :\n+                complexMap.entrySet() ) {\n+            final VariantContext variantContext =\n+                    CpxVariantInterpreter.turnIntoVariantContext(entry.getKey(), entry.getValue(), reference);\n+            final int refSegs =\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CPX_SV_REF_SEGMENTS).size();\n+            if ( refSegs < 2 ) {\n+                variants.addAll(zeroAndOneSegmentCpxVariantExtractor.extract(variantContext, reference));\n+            } else {\n+                multiSegmentVariants.add(variantContext);\n+            }\n+        }\n+        final Map<String, RelevantAttributes> contigNameToCpxVariantAttributes =\n+                new HashMap<>(2 * multiSegmentVariants.size());\n+        for ( final VariantContext variantContext : multiSegmentVariants ) {\n+            final RelevantAttributes relevantAttributes = new RelevantAttributes(variantContext);\n+            for ( final String contigName :\n+                    SVUtils.getAttributeAsStringList(variantContext, GATKSVVCFConstants.CONTIG_NAMES) ) {\n+                contigNameToCpxVariantAttributes.put(contigName, relevantAttributes);\n+            }\n+        }\n+        final Map<NovelAdjacencyAndAltHaplotype, List<SimpleChimera>> redoMap = new HashMap<>();\n+        for ( final AlignedContig alignedContig : complexContigs ) {\n+            if ( contigNameToCpxVariantAttributes.containsKey(alignedContig.getContigName()) &&\n+                    alignedContig.getAlignments().size() > 1 ) {\n+                final List<SimpleChimera> chimeras =\n+                        ContigChimericAlignmentIterativeInterpreter.parseOneContig(\n+                                alignedContig,\n+                                refDict,\n+                                true,\n+                                DiscoverVariantsFromContigAlignmentsSparkArgumentCollection.DEFAULT_MIN_ALIGNMENT_LENGTH,\n+                                DiscoverVariantsFromContigAlignmentsSparkArgumentCollection.CHIMERIC_ALIGNMENTS_HIGHMQ_THRESHOLD,\n+                                true);\n+                for ( final SimpleChimera simpleChimera : chimeras ) {\n+                    final NovelAdjacencyAndAltHaplotype novelAdjacency =\n+                            new NovelAdjacencyAndAltHaplotype(simpleChimera, alignedContig.getContigSequence(), refDict);\n+                    final List<SimpleChimera> mapVal = redoMap.get(novelAdjacency);\n+                    if ( mapVal != null ) {\n+                        mapVal.add(simpleChimera);\n+                    } else {\n+                        final List<SimpleChimera> newList = new ArrayList<>(2);\n+                        newList.add(simpleChimera);\n+                        redoMap.put(novelAdjacency, newList);\n+                    }\n+                }\n+            }\n+        }\n+        final List<VariantContext> reinterpretedVariants = new ArrayList<>(redoMap.size());\n+        for ( Map.Entry<NovelAdjacencyAndAltHaplotype, List<SimpleChimera>> entry : redoMap.entrySet() ) {\n+            reinterpretedVariants.add(\n+                new SimpleNovelAdjacencyAndChimericAlignmentEvidence(entry.getKey(), entry.getValue())\n+                        .produceAnnotatedVcFromAssemblyEvidence(\n+                                ContigChimericAlignmentIterativeInterpreter\n+                                        .inferSimpleTypeFromNovelAdjacency(entry.getKey(), reference),\n+                                refDict, cnvCalls, sampleId).make());\n+        }\n+        final List<VariantContext> extractedMultiSegmentVariants = new ArrayList<>(multiSegmentVariants.size());\n+        final MultiSegmentsCpxVariantExtractor multiSegmentsCpxVariantExtractor =\n+                new MultiSegmentsCpxVariantExtractor();\n+        for ( final VariantContext variantContext : multiSegmentVariants ) {\n+            extractedMultiSegmentVariants.addAll(multiSegmentsCpxVariantExtractor.extract(variantContext, reference));\n+        }\n+        final List<VariantContext> consistentVariants =\n+                SegmentedCpxVariantSimpleVariantExtractor\n+                        .filterForConsistency(reinterpretedVariants, contigNameToCpxVariantAttributes, reference);\n+        variants.addAll(SegmentedCpxVariantSimpleVariantExtractor.removeDuplicates(extractedMultiSegmentVariants, consistentVariants));\n+\n+        final List<VariantContext> filteredVariants = AnnotatedVariantProducer.filterMergedVCF(variants, discoverStageArgs);\n+        SVVCFWriter.writeVCF(filteredVariants, outputVCFName, refDict, getDefaultToolVCFHeaderLines(), logger);\n+        return result;\n+    }\n+\n+    private void processContigAlignments( final List<GATKRead> contigAlignments ) {", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODE0NjgyNw==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r468146827", "bodyText": "The existing code was dealing with SAMRecords, but in a ReadWalker I really needed to be dealing with GATKReads.  So I rewrote parseReadsAndOptionallySplitGappedAlignments, and that's inlined here.  It's the first 17 lines, up to the place where I create a new AlignedContig.  The default read filters have already peeled off unmapped and secondary alignments, and the option to split gaps was always being passed as \"false\"--that actually happens downstream--so the logic is quite a bit simpler in this method than in the original.", "author": "tedsharpe", "createdAt": "2020-08-10T19:57:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU0NDgxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODYzNzUyNg==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r468637526", "bodyText": "Got it, thanks for the explanation.", "author": "cwhelan", "createdAt": "2020-08-11T14:45:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU0NDgxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0NTA0MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459645041", "bodyText": "Do you need to call convertTerminalInsertionToSoftClip as in the old code's splitGappedAlignment method?", "author": "cwhelan", "createdAt": "2020-07-23T18:26:42Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/ContigAlignmentsModifier.java", "diffHunk": "@@ -237,6 +237,120 @@ public String toString() {\n         }\n     }\n \n+    private static class GapSplitter {\n+        private static final int NOT_SET = -1;\n+\n+        int alignmentStartContig = NOT_SET;\n+        int alignmentStartIdx = NOT_SET;\n+        int alignmentStartRef = NOT_SET;\n+        int alignmentEndContig = NOT_SET;\n+        int alignmentEndIdx = NOT_SET;\n+        int alignmentEndRef = NOT_SET;\n+\n+        final AlignmentInterval oneRegion;\n+        final int unclippedContigLen;\n+\n+        public GapSplitter( final AlignmentInterval oneRegion, final int unclippedContigLen ) {\n+            this.oneRegion = oneRegion;\n+            this.unclippedContigLen = unclippedContigLen;\n+        }\n+\n+        public List<AlignmentInterval> splitGaps( final int sensitivity ) {\n+            final List<CigarElement> elements = oneRegion.cigarAlong5to3DirectionOfContig.getCigarElements();", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODE4OTI5NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r468189295", "bodyText": "No, this is completely rewritten to circumvent the exception that was being thrown on cigars with adjacent insertion and deletion operators.  In the new code, leading and trailing indels are never included in the cigar pieces because the starting and ending positions are updated only on aligned bits (M operators), never on indels.", "author": "tedsharpe", "createdAt": "2020-08-10T21:13:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0NTA0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY2Nzg0MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459667841", "bodyText": "Could you add some little comments here to say what these are? It took me a while to figure out that this was an index into the cigar element list, etc.", "author": "cwhelan", "createdAt": "2020-07-23T19:08:02Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/ContigAlignmentsModifier.java", "diffHunk": "@@ -237,6 +237,120 @@ public String toString() {\n         }\n     }\n \n+    private static class GapSplitter {\n+        private static final int NOT_SET = -1;\n+\n+        int alignmentStartContig = NOT_SET;\n+        int alignmentStartIdx = NOT_SET;", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY3MTMyMw==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r459671323", "bodyText": "Stylistically I think it would make it a bit easier to understand this method if you passed the alignment indices (alignmentStartIdx, etc.) to this method as parameters. Conversely, it feels to me like elements is the thing that should be accessed via an instance variable since it's immutable over the lifespan of this GapSplitter object. But that might be just my own style preference, if you like it better this way that's fine.", "author": "cwhelan", "createdAt": "2020-07-23T19:14:51Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/alignment/ContigAlignmentsModifier.java", "diffHunk": "@@ -237,6 +237,120 @@ public String toString() {\n         }\n     }\n \n+    private static class GapSplitter {\n+        private static final int NOT_SET = -1;\n+\n+        int alignmentStartContig = NOT_SET;\n+        int alignmentStartIdx = NOT_SET;\n+        int alignmentStartRef = NOT_SET;\n+        int alignmentEndContig = NOT_SET;\n+        int alignmentEndIdx = NOT_SET;\n+        int alignmentEndRef = NOT_SET;\n+\n+        final AlignmentInterval oneRegion;\n+        final int unclippedContigLen;\n+\n+        public GapSplitter( final AlignmentInterval oneRegion, final int unclippedContigLen ) {\n+            this.oneRegion = oneRegion;\n+            this.unclippedContigLen = unclippedContigLen;\n+        }\n+\n+        public List<AlignmentInterval> splitGaps( final int sensitivity ) {\n+            final List<CigarElement> elements = oneRegion.cigarAlong5to3DirectionOfContig.getCigarElements();\n+            int nElements = elements.size();\n+            if ( nElements <= 1 ) {\n+                return new ArrayList<>( Collections.singletonList(oneRegion) );\n+            }\n+\n+            int contigOffset = 0;\n+            int elementIdx = 0;\n+            if ( elements.get(0).getOperator() == CigarOperator.H ) {\n+                contigOffset += elements.get(0).getLength();\n+                elementIdx += 1;\n+            }\n+\n+            if ( elements.get(nElements - 1).getOperator() == CigarOperator.H ) {\n+                nElements -= 1;\n+            }\n+\n+            int refOffset = oneRegion.referenceSpan.getStart();\n+            if ( !oneRegion.forwardStrand ) refOffset = -oneRegion.referenceSpan.getEnd();\n+\n+            final List<AlignmentInterval> result = new ArrayList<>();\n+            while ( elementIdx < nElements ) {\n+                final CigarElement element = elements.get(elementIdx);\n+                final CigarOperator op = element.getOperator();\n+                final int len = element.getLength();\n+                if ( op.isAlignment() ) {\n+                    if ( alignmentStartContig == NOT_SET ) {\n+                        alignmentStartContig = contigOffset;\n+                        alignmentStartIdx = elementIdx;\n+                        alignmentStartRef = refOffset;\n+                    }\n+                    alignmentEndContig = contigOffset + len;\n+                    alignmentEndIdx = elementIdx + 1;\n+                    alignmentEndRef = refOffset + len;\n+                } else if ( op.isIndel() && len >= sensitivity && alignmentStartContig != -1 ) {\n+                    result.add(createInterval(elements));\n+                    alignmentStartContig = NOT_SET;\n+                }\n+                if ( op.consumesReadBases() ) {\n+                    contigOffset += len;\n+                }\n+                if ( op.consumesReferenceBases() ) {\n+                    refOffset += len;\n+                }\n+                elementIdx += 1;\n+            }\n+\n+            if ( alignmentStartContig != -1 ) {\n+                result.add(createInterval(elements));\n+            }\n+\n+            if ( result.size() < 2 ) {\n+                return new ArrayList<>(Collections.singletonList(oneRegion));\n+            }\n+            return result;\n+        }\n+\n+        private AlignmentInterval createInterval( final List<CigarElement> elements ) {", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODE5NzY4NA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r468197684", "bodyText": "I agree it was counter-intuitive to pass elements as an argument.  (It was a silly performance hack that really shouldn't matter--getting the elements from a Cigar creates a copy.)  I've made the cigar elements an instance variable so they're still only retrieved once, but don't have to be passed.  And I renamed the createInterval method grabCurrentInterval to make it clearer that it's using object state.", "author": "tedsharpe", "createdAt": "2020-08-10T21:31:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY3MTMyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDA5ODAxMw==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460098013", "bodyText": "While you're moving this, want to rename it to something with one fewer word like createVariantContext or even just toVariantContext?", "author": "cwhelan", "createdAt": "2020-07-24T14:44:31Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/CpxVariantInterpreter.java", "diffHunk": "@@ -269,16 +271,14 @@ static boolean yieldOverlapToAlignmentTwo(final AlignmentInterval one, final Ali\n \n     // =================================================================================================================\n \n-    @VisibleForTesting\n-    static VariantContext turnIntoVariantContext(final Tuple2<CpxVariantCanonicalRepresentation, Iterable<CpxVariantInducingAssemblyContig>> pair,\n-                                                 final ReferenceMultiSparkSource reference)\n-            throws IOException {\n-\n-        final CpxVariantCanonicalRepresentation cpxVariantCanonicalRepresentation = pair._1;\n-        final byte[] refBases = getRefBases(reference, cpxVariantCanonicalRepresentation);\n+    public static VariantContext turnIntoVariantContext( final CpxVariantCanonicalRepresentation cpxVariantCanonicalRepresentation,", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDEwMDQyMA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460100420", "bodyText": "Again, what about renaming to createVariantContexts or toVariantContexts while we're moving stuff around?", "author": "cwhelan", "createdAt": "2020-07-24T14:48:15Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleNovelAdjacencyAndChimericAlignmentEvidence.java", "diffHunk": "@@ -48,22 +65,163 @@ private SimpleNovelAdjacencyAndChimericAlignmentEvidence(final Kryo kryo, final\n         }\n     }\n \n-    private void serialize(final Kryo kryo, final Output output) {\n-        narlSerializer.write(kryo, output, novelAdjacencyAndAltHaplotype);\n-        output.writeInt(alignmentEvidence.size());\n-        alignmentEvidence.forEach(ev -> alignmentEvidenceSerializer.write(kryo, output, ev));\n+    /**\n+     * This implementation is the 1st step going towards allowing re-interpretation,\n+     * below we simply take the inferred type and turn it to a VC,\n+     * future implementation may integrate other types of evidence and re-interpret if necessary\n+     */\n+    public List<VariantContext> turnIntoVariantContexts( final List<SvType> svTypes,", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDEwNTYyMg==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460105622", "bodyText": "Maybe make this method static?", "author": "cwhelan", "createdAt": "2020-07-24T14:56:29Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/discovery/inference/SimpleNovelAdjacencyAndChimericAlignmentEvidence.java", "diffHunk": "@@ -48,22 +65,163 @@ private SimpleNovelAdjacencyAndChimericAlignmentEvidence(final Kryo kryo, final\n         }\n     }\n \n-    private void serialize(final Kryo kryo, final Output output) {\n-        narlSerializer.write(kryo, output, novelAdjacencyAndAltHaplotype);\n-        output.writeInt(alignmentEvidence.size());\n-        alignmentEvidence.forEach(ev -> alignmentEvidenceSerializer.write(kryo, output, ev));\n+    /**\n+     * This implementation is the 1st step going towards allowing re-interpretation,\n+     * below we simply take the inferred type and turn it to a VC,\n+     * future implementation may integrate other types of evidence and re-interpret if necessary\n+     */\n+    public List<VariantContext> turnIntoVariantContexts( final List<SvType> svTypes,\n+                                                         final String sampleId,\n+                                                         final SAMSequenceDictionary refDict,\n+                                                         final SVIntervalTree<VariantContext> cnvCalls) {\n+        if( svTypes.isEmpty() || svTypes.size() > 2 ) {\n+            throw new GATKException(\"Wrong number of variants sent for analysis: \" + svTypes.toString() +\n+                    \"\\nWe currently only support 1 (symbolic simple or CPX) or 2 (BND mate pairs) variants for producing annotated variants.\");\n+        }\n+        if (svTypes.size() == 2) {\n+            final SvType firstVar = svTypes.get(0);\n+            final SvType secondVar = svTypes.get(1);\n+            final String linkKey = firstVar instanceof BreakEndVariantType ? GATKSVVCFConstants.BND_MATEID_STR : GATKSVVCFConstants.LINK;\n+            return produceLinkedAssemblyBasedVariants(firstVar, secondVar, refDict, cnvCalls, sampleId, linkKey);\n+        } else {\n+            final VariantContext variantContext =\n+                    produceAnnotatedVcFromAssemblyEvidence(svTypes.get(0), refDict, cnvCalls, sampleId).make();\n+            return Collections.singletonList(variantContext);\n+        }\n     }\n \n-    public static final class Serializer extends com.esotericsoftware.kryo.Serializer<SimpleNovelAdjacencyAndChimericAlignmentEvidence> {\n-        @Override\n-        public void write(final Kryo kryo, final Output output, final SimpleNovelAdjacencyAndChimericAlignmentEvidence novelAdjacencyReferenceLocations ) {\n-            novelAdjacencyReferenceLocations.serialize(kryo, output);\n+    /**\n+     * Produces a VC from a {@link NovelAdjacencyAndAltHaplotype}\n+     * (consensus among different assemblies if they all point to the same breakpoint).\n+     * @param inferredType                      inferred type of variant\n+     * @param sequenceDictionary                reference sequence dictionary\n+     * @param cnvCalls                          external CNV calls, if available, will be used for annotating the assembly based calls\n+     * @param sampleId                          sample identifier of the current sample\n+     */\n+    @VisibleForTesting\n+    public VariantContextBuilder produceAnnotatedVcFromAssemblyEvidence( final SvType inferredType,\n+                                                                         final SAMSequenceDictionary sequenceDictionary,\n+                                                                         final SVIntervalTree<VariantContext> cnvCalls,\n+                                                                         final String sampleId) {\n+\n+        // basic information and attributes\n+        final VariantContextBuilder vcBuilder = inferredType.getBasicInformation();\n+\n+        // attributes from complications\n+        novelAdjacencyAndAltHaplotype.getComplication().toVariantAttributes().forEach(vcBuilder::attribute);\n+\n+        // evidence used for producing the novel adjacency\n+        getAssemblyEvidenceRelatedAnnotations(alignmentEvidence).forEach(vcBuilder::attribute);\n+\n+        // alt seq for non-BND variants, and if available or not empty\n+        final byte[] altHaplotypeSequence = novelAdjacencyAndAltHaplotype.getAltHaplotypeSequence();\n+        if (inferredType instanceof BreakEndVariantType) {\n+            return annotateWithExternalCNVCalls(inferredType.getVariantChromosome(), inferredType.getVariantStart(), inferredType.getVariantStop(),\n+                    vcBuilder, sequenceDictionary, cnvCalls, sampleId);\n         }\n \n-        @Override\n-        public SimpleNovelAdjacencyAndChimericAlignmentEvidence read(final Kryo kryo, final Input input, final Class<SimpleNovelAdjacencyAndChimericAlignmentEvidence> klass ) {\n-            return new SimpleNovelAdjacencyAndChimericAlignmentEvidence(kryo, input);\n+        if (altHaplotypeSequence != null && altHaplotypeSequence.length != 0)\n+            vcBuilder.attribute(GATKSVVCFConstants.SEQ_ALT_HAPLOTYPE, StringUtil.bytesToString(altHaplotypeSequence));\n+\n+        return annotateWithExternalCNVCalls(inferredType.getVariantChromosome(), inferredType.getVariantStart(), inferredType.getVariantStop(),\n+                vcBuilder, sequenceDictionary, cnvCalls, sampleId);\n+    }\n+\n+    private static VariantContextBuilder annotateWithExternalCNVCalls(final String recordContig,\n+                                                                      final int pos,\n+                                                                      final int end,\n+                                                                      final VariantContextBuilder inputBuilder,\n+                                                                      final SAMSequenceDictionary sequenceDictionary,\n+                                                                      final SVIntervalTree<VariantContext> cnvCalls,\n+                                                                      final String sampleId) {\n+        if (cnvCalls == null)\n+            return inputBuilder;\n+\n+        final SVInterval variantInterval = new SVInterval(sequenceDictionary.getSequenceIndex(recordContig), pos, end);\n+        final String cnvCallAnnotation =\n+                Utils.stream(cnvCalls.overlappers(variantInterval))\n+                        .map(overlapper -> formatExternalCNVCallAnnotation(overlapper.getValue(), sampleId))\n+                        .collect(Collectors.joining(VCFConstants.INFO_FIELD_ARRAY_SEPARATOR));\n+        if (!cnvCallAnnotation.isEmpty()) {\n+            return inputBuilder.attribute(GATKSVVCFConstants.EXTERNAL_CNV_CALLS, cnvCallAnnotation);\n+        } else\n+            return inputBuilder;\n+    }\n+\n+    private static String formatExternalCNVCallAnnotation(final VariantContext externalCNVCall, String sampleId) {\n+        return externalCNVCall.getID() + \":\"\n+                + externalCNVCall.getGenotype(sampleId).getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT) + \":\"\n+                + externalCNVCall.getGenotype(sampleId).getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_QUALITY_FORMAT);\n+    }\n+\n+    /**\n+     * Given novel adjacency and inferred variant types that should be linked together,\n+     * produce annotated, and linked VCF records.\n+     */\n+    public List<VariantContext> produceLinkedAssemblyBasedVariants(final SvType svType1,\n+                                                                   final SvType svType2,\n+                                                                   final SAMSequenceDictionary sequenceDictionary,\n+                                                                   final SVIntervalTree<VariantContext> cnvCalls,\n+                                                                   final String sampleId,\n+                                                                   final String linkKey) {\n+\n+        final VariantContext firstVar = produceAnnotatedVcFromAssemblyEvidence(\n+                    svType1, sequenceDictionary, cnvCalls, sampleId).make();\n+        final VariantContext secondVar = produceAnnotatedVcFromAssemblyEvidence(\n+                    svType2, sequenceDictionary, cnvCalls, sampleId).make();\n+\n+        final VariantContextBuilder builder1 = new VariantContextBuilder(firstVar);\n+        builder1.attribute(linkKey, secondVar.getID());\n+\n+        final VariantContextBuilder builder2 = new VariantContextBuilder(secondVar);\n+        builder2.attribute(linkKey, firstVar.getID());\n+\n+        // manually remove inserted sequence information from RPL event-produced DEL, when it can be linked with an INS\n+        if (svType1 instanceof SimpleSVType.Deletion)\n+            return Arrays.asList(builder1.rmAttribute(GATKSVVCFConstants.INSERTED_SEQUENCE)\n+                            .rmAttribute(GATKSVVCFConstants.INSERTED_SEQUENCE_LENGTH)\n+                            .rmAttribute(GATKSVVCFConstants.SEQ_ALT_HAPLOTYPE)\n+                            .make(),\n+                    builder2.make());\n+        else if (svType2 instanceof SimpleSVType.Deletion) {\n+            return Arrays.asList(builder1.make(),\n+                    builder2.rmAttribute(GATKSVVCFConstants.INSERTED_SEQUENCE)\n+                            .rmAttribute(GATKSVVCFConstants.INSERTED_SEQUENCE_LENGTH)\n+                            .rmAttribute(GATKSVVCFConstants.SEQ_ALT_HAPLOTYPE)\n+                            .make());\n+        } else\n+            return Arrays.asList(builder1.make(), builder2.make());\n+    }\n+\n+    private Map<String, Object> getAssemblyEvidenceRelatedAnnotations( final List<SimpleChimera> splitAlignmentEvidence ) {", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDExOTg1Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460119852", "bodyText": "Great to see this test changed to passing.", "author": "cwhelan", "createdAt": "2020-07-24T15:19:06Z", "path": "src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java", "diffHunk": "@@ -139,17 +141,31 @@ public void testPutReadsWithSameNameInSamePartition(int numPairs, int numPartiti\n         return reads;\n     }\n \n-    @Test(expectedExceptions = GATKException.class)", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE4ODExOA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460188118", "bodyText": "\ud83d\udc4d Did you run into this in the wild?", "author": "lbergelson", "createdAt": "2020-07-24T17:23:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDExOTg1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIwMTE3MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r468201171", "bodyText": "@lbergelson Yes, I did see the empty partition issue in the wild.  Situation was a very small BAM containing large contigs from a local assembly with lots of supplemental alignments.", "author": "tedsharpe", "createdAt": "2020-08-10T21:39:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDExOTg1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDEyNDUxNg==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460124516", "bodyText": "It might be nice to also assert that each read name you expect is still there, since the fix you implemented involves removing some groups and re-adding them to different partitions.", "author": "cwhelan", "createdAt": "2020-07-24T15:26:34Z", "path": "src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java", "diffHunk": "@@ -139,17 +141,31 @@ public void testPutReadsWithSameNameInSamePartition(int numPairs, int numPartiti\n         return reads;\n     }\n \n-    @Test(expectedExceptions = GATKException.class)\n-    public void testReadsPairsSpanningMultiplePartitionsCrash() {\n+    @Test\n+    public void testReadsPairsSpanningMultiplePartitions() {\n         JavaSparkContext ctx = SparkContextFactory.getTestSparkContext();\n         SAMFileHeader header = ArtificialReadUtils.createArtificialSamHeader();\n         header.setSortOrder(SAMFileHeader.SortOrder.queryname);\n         List<GATKRead> reads = createPairedReads(header, 40, 2);\n-        // Creating one group in the middle that should cause problems\n-        reads.addAll(40, createPairedReads(header, 1, 30));\n+        reads.addAll(createPairedReads(header, 1, 30));\n+        reads.sort( (r1, r2) -> r1.getName().compareTo(r2.getName()) );\n \n         JavaRDD<GATKRead> problemReads = ctx.parallelize(reads,5 );\n-        SparkUtils.putReadsWithTheSameNameInTheSamePartition(header, problemReads, ctx);\n+        JavaRDD<GATKRead> fixedReads =\n+                SparkUtils.putReadsWithTheSameNameInTheSamePartition(header, problemReads, ctx);\n+        final List<Tuple2<String, Iterable<Integer>>> xPartitionPairs = fixedReads\n+                .mapPartitionsWithIndex(( idx, itr ) -> {\n+                    final Set<Tuple2<String, Integer>> readLocs = new HashSet<>();\n+                    while ( itr.hasNext() ) readLocs.add(new Tuple2<>(itr.next().getName(), idx));\n+                    return readLocs.iterator();\n+                }, false)\n+                .mapToPair(t -> t).groupByKey().filter(t -> {\n+                    int count = 0;\n+                    for ( final int idx : t._2() ) ++count;\n+                    return count > 1;\n+                })\n+                .collect();\n+        Assert.assertEquals(xPartitionPairs.size(), 0);", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE1Mjk4MA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460152980", "bodyText": "It's really weird to make arguments static.  That seems like a mistake to me.", "author": "lbergelson", "createdAt": "2020-07-24T16:15:32Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/StructuralVariantDiscoverer.java", "diffHunk": "@@ -0,0 +1,268 @@\n+package org.broadinstitute.hellbender.tools;\n+\n+import htsjdk.samtools.SAMFileHeader;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.ArgumentCollection;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.BasicReference;\n+import org.broadinstitute.hellbender.engine.FeatureContext;\n+import org.broadinstitute.hellbender.engine.ReadWalker;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n+import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryArgumentCollection.DiscoverVariantsFromContigAlignmentsSparkArgumentCollection;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.AnnotatedVariantProducer;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignedContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AlignmentInterval;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.AssemblyContigWithFineTunedAlignments.AlignmentSignatureBasicType;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInducingAssemblyContig;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.MultiSegmentsCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.RelevantAttributes;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SegmentedCpxVariantSimpleVariantExtractor.ZeroAndOneSegmentCpxVariantExtractor;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleChimera;\n+import org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyAndChimericAlignmentEvidence;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.CNVInputReader;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVIntervalTree;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVUtils;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.SVVCFWriter;\n+import org.broadinstitute.hellbender.utils.BaseUtils;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import scala.Tuple2;\n+\n+import java.io.BufferedWriter;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.util.*;\n+\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        oneLineSummary = \"(Internal) Examines aligned contigs from local assemblies and calls structural variants or their breakpoints\",\n+        summary =\n+            \"This tool takes a file containing the alignments of assembled contigs and searches it for contigs with\" +\n+            \" split alignments or large gaps indicating the presence of structural variation breakpoints.\" +\n+            \" Variations' types are determined by analyzing the signatures of the split alignments,\" +\n+            \" and are written to a VCF file.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class)\n+public class StructuralVariantDiscoverer extends ReadWalker {\n+    @Argument(doc = \"Name of output VCF.\", shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            fullName = \"outputVCFName\")\n+    private static String outputVCFName;", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE1NjIyNg==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460156226", "bodyText": "Pulling this up seems strange because now we have to serialize the empty list.", "author": "lbergelson", "createdAt": "2020-07-24T16:21:43Z", "path": "src/main/java/org/broadinstitute/hellbender/utils/spark/SparkUtils.java", "diffHunk": "@@ -178,60 +177,69 @@ public static boolean hadoopPathExists(final JavaSparkContext ctx, final URI tar\n      * The RDD must be queryname sorted.  If there are so many reads with the same name that they span multiple partitions\n      * this will throw {@link GATKException}.\n      */\n-    public static JavaRDD<GATKRead> putReadsWithTheSameNameInTheSamePartition(final SAMFileHeader header, final JavaRDD<GATKRead> reads, final JavaSparkContext ctx) {\n+    public static JavaRDD<GATKRead> putReadsWithTheSameNameInTheSamePartition( final SAMFileHeader header,\n+                                                                               final JavaRDD<GATKRead> reads,\n+                                                                               final JavaSparkContext ctx ) {\n         Utils.validateArg(ReadUtils.isReadNameGroupedBam(header), () -> \"Reads must be queryname grouped or sorted. \" +\n-                \"Actual sort:\" + header.getSortOrder() + \"  Actual grouping:\" +header.getGroupOrder());\n-        int numPartitions = reads.getNumPartitions();\n-        final String firstNameInBam = reads.first().getName();\n+                \"Actual sort:\" + header.getSortOrder() + \"  Actual grouping:\" + header.getGroupOrder());\n+\n         // Find the first group in each partition\n-        List<List<GATKRead>> firstReadNameGroupInEachPartition = reads\n-                .mapPartitions(it -> { PeekingIterator<GATKRead> current = Iterators.peekingIterator(it);\n-                                List<GATKRead> firstGroup = new ArrayList<>(2);\n-                                firstGroup.add(current.next());\n-                                String name = firstGroup.get(0).getName();\n-                                while (current.hasNext() && current.peek().getName().equals(name)) {\n-                                    firstGroup.add(current.next());\n-                                }\n-                                return Iterators.singletonIterator(firstGroup);\n-                                })\n+        final List<GATKRead> empty = Collections.emptyList();", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE4NTUxMQ==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460185511", "bodyText": "You know how I feel about {}", "author": "lbergelson", "createdAt": "2020-07-24T17:17:46Z", "path": "src/main/java/org/broadinstitute/hellbender/utils/spark/SparkUtils.java", "diffHunk": "@@ -178,60 +177,69 @@ public static boolean hadoopPathExists(final JavaSparkContext ctx, final URI tar\n      * The RDD must be queryname sorted.  If there are so many reads with the same name that they span multiple partitions\n      * this will throw {@link GATKException}.\n      */\n-    public static JavaRDD<GATKRead> putReadsWithTheSameNameInTheSamePartition(final SAMFileHeader header, final JavaRDD<GATKRead> reads, final JavaSparkContext ctx) {\n+    public static JavaRDD<GATKRead> putReadsWithTheSameNameInTheSamePartition( final SAMFileHeader header,\n+                                                                               final JavaRDD<GATKRead> reads,\n+                                                                               final JavaSparkContext ctx ) {\n         Utils.validateArg(ReadUtils.isReadNameGroupedBam(header), () -> \"Reads must be queryname grouped or sorted. \" +\n-                \"Actual sort:\" + header.getSortOrder() + \"  Actual grouping:\" +header.getGroupOrder());\n-        int numPartitions = reads.getNumPartitions();\n-        final String firstNameInBam = reads.first().getName();\n+                \"Actual sort:\" + header.getSortOrder() + \"  Actual grouping:\" + header.getGroupOrder());\n+\n         // Find the first group in each partition\n-        List<List<GATKRead>> firstReadNameGroupInEachPartition = reads\n-                .mapPartitions(it -> { PeekingIterator<GATKRead> current = Iterators.peekingIterator(it);\n-                                List<GATKRead> firstGroup = new ArrayList<>(2);\n-                                firstGroup.add(current.next());\n-                                String name = firstGroup.get(0).getName();\n-                                while (current.hasNext() && current.peek().getName().equals(name)) {\n-                                    firstGroup.add(current.next());\n-                                }\n-                                return Iterators.singletonIterator(firstGroup);\n-                                })\n+        final List<GATKRead> empty = Collections.emptyList();\n+        final List<List<GATKRead>> firstReadNameGroupInEachPartition = reads\n+                .mapPartitions(it -> {\n+                    if ( !it.hasNext() ) {\n+                        return Iterators.singletonIterator(empty);\n+                    }\n+                    final List<GATKRead> firstGroup = new ArrayList<>(2);\n+                    final GATKRead firstRead = it.next();\n+                    firstGroup.add(firstRead);\n+                    final String groupName = firstRead.getName();\n+                    while ( it.hasNext() ) {\n+                        final GATKRead read = it.next();\n+                        if ( !groupName.equals(read.getName()) ) break;", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE4NzYyNg==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460187626", "bodyText": "brackets brackets everywhere", "author": "lbergelson", "createdAt": "2020-07-24T17:22:04Z", "path": "src/main/java/org/broadinstitute/hellbender/utils/spark/SparkUtils.java", "diffHunk": "@@ -178,60 +177,69 @@ public static boolean hadoopPathExists(final JavaSparkContext ctx, final URI tar\n      * The RDD must be queryname sorted.  If there are so many reads with the same name that they span multiple partitions\n      * this will throw {@link GATKException}.\n      */\n-    public static JavaRDD<GATKRead> putReadsWithTheSameNameInTheSamePartition(final SAMFileHeader header, final JavaRDD<GATKRead> reads, final JavaSparkContext ctx) {\n+    public static JavaRDD<GATKRead> putReadsWithTheSameNameInTheSamePartition( final SAMFileHeader header,\n+                                                                               final JavaRDD<GATKRead> reads,\n+                                                                               final JavaSparkContext ctx ) {\n         Utils.validateArg(ReadUtils.isReadNameGroupedBam(header), () -> \"Reads must be queryname grouped or sorted. \" +\n-                \"Actual sort:\" + header.getSortOrder() + \"  Actual grouping:\" +header.getGroupOrder());\n-        int numPartitions = reads.getNumPartitions();\n-        final String firstNameInBam = reads.first().getName();\n+                \"Actual sort:\" + header.getSortOrder() + \"  Actual grouping:\" + header.getGroupOrder());\n+\n         // Find the first group in each partition\n-        List<List<GATKRead>> firstReadNameGroupInEachPartition = reads\n-                .mapPartitions(it -> { PeekingIterator<GATKRead> current = Iterators.peekingIterator(it);\n-                                List<GATKRead> firstGroup = new ArrayList<>(2);\n-                                firstGroup.add(current.next());\n-                                String name = firstGroup.get(0).getName();\n-                                while (current.hasNext() && current.peek().getName().equals(name)) {\n-                                    firstGroup.add(current.next());\n-                                }\n-                                return Iterators.singletonIterator(firstGroup);\n-                                })\n+        final List<GATKRead> empty = Collections.emptyList();\n+        final List<List<GATKRead>> firstReadNameGroupInEachPartition = reads\n+                .mapPartitions(it -> {\n+                    if ( !it.hasNext() ) {\n+                        return Iterators.singletonIterator(empty);\n+                    }\n+                    final List<GATKRead> firstGroup = new ArrayList<>(2);\n+                    final GATKRead firstRead = it.next();\n+                    firstGroup.add(firstRead);\n+                    final String groupName = firstRead.getName();\n+                    while ( it.hasNext() ) {\n+                        final GATKRead read = it.next();\n+                        if ( !groupName.equals(read.getName()) ) break;\n+                        firstGroup.add(read);\n+                    }\n+                    return Iterators.singletonIterator(firstGroup);\n+                })\n                 .collect();\n \n-        // Checking for pathological cases (read name groups that span more than 2 partitions)\n-        String groupName = null;\n-        for (List<GATKRead> group : firstReadNameGroupInEachPartition) {\n-            if (group!=null && !group.isEmpty()) {\n-                // If a read spans multiple partitions we expect its name to show up multiple times and we don't expect this to work properly\n-                if (groupName != null && group.get(0).getName().equals(groupName)) {\n-                    throw new GATKException(String.format(\"The read name '%s' appeared across multiple partitions this could indicate there was a problem \" +\n-                            \"with the sorting or that the rdd has too many partitions, check that the file is queryname sorted and consider decreasing the number of partitions\", groupName));\n+        // Shift left, so that each partition will be zipped with the first read group from the _next_ partition\n+        final int numPartitions = reads.getNumPartitions();\n+        final List<List<GATKRead>> firstGroupFromNextPartition =\n+                new ArrayList<>(firstReadNameGroupInEachPartition.subList(1, numPartitions));\n+        firstGroupFromNextPartition.add(Collections.emptyList()); // the last partition does not have any reads to add to it\n+\n+        // Take care of the situation where an entire partition contains reads with the same name\n+        // (unlikely, but could happen with very long reads, or very small partitions).\n+        for ( int idx = numPartitions - 1; idx >= 1; --idx ) {\n+            final List<GATKRead> curGroup = firstGroupFromNextPartition.get(idx);\n+            if ( !curGroup.isEmpty() ) {\n+                final String groupName = curGroup.get(0).getName();\n+                int idx2 = idx;\n+                while ( --idx2 >= 0 ) {\n+                    final List<GATKRead> prevGroup = firstGroupFromNextPartition.get(idx2);\n+                    if ( !prevGroup.isEmpty() ) {\n+                        if ( groupName.equals(prevGroup.get(0).getName()) ) {\n+                            prevGroup.addAll(curGroup);\n+                            curGroup.clear();\n+                        }\n+                        break;\n+                    }\n                 }\n-                groupName =  group.get(0).getName();\n             }\n         }\n \n-        // Shift left, so that each partition will be joined with the first read group from the _next_ partition\n-        List<List<GATKRead>> firstReadInNextPartition = new ArrayList<>(firstReadNameGroupInEachPartition.subList(1, numPartitions));\n-        firstReadInNextPartition.add(null); // the last partition does not have any reads to add to it\n+        // Peel off the first group in each partition\n+        final int[] firstGroupSizes = firstReadNameGroupInEachPartition.stream().mapToInt(List::size).toArray();\n+        firstGroupSizes[0] = 0; // first partition has no predecessor to handle its first group of reads\n+        JavaRDD<GATKRead> readsSansFirstGroup = reads.mapPartitionsWithIndex( (idx, itr) ->\n+            { int groupSize = firstGroupSizes[idx];\n+              while ( itr.hasNext() && groupSize-- > 0 ) itr.next();", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE4OTE1OA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460189158", "bodyText": "I like Comparator.comparing(GATKRead::getName) but the lambda is fine too.", "author": "lbergelson", "createdAt": "2020-07-24T17:25:14Z", "path": "src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java", "diffHunk": "@@ -139,17 +141,31 @@ public void testPutReadsWithSameNameInSamePartition(int numPairs, int numPartiti\n         return reads;\n     }\n \n-    @Test(expectedExceptions = GATKException.class)\n-    public void testReadsPairsSpanningMultiplePartitionsCrash() {\n+    @Test\n+    public void testReadsPairsSpanningMultiplePartitions() {\n         JavaSparkContext ctx = SparkContextFactory.getTestSparkContext();\n         SAMFileHeader header = ArtificialReadUtils.createArtificialSamHeader();\n         header.setSortOrder(SAMFileHeader.SortOrder.queryname);\n         List<GATKRead> reads = createPairedReads(header, 40, 2);\n-        // Creating one group in the middle that should cause problems\n-        reads.addAll(40, createPairedReads(header, 1, 30));\n+        reads.addAll(createPairedReads(header, 1, 30));\n+        reads.sort( (r1, r2) -> r1.getName().compareTo(r2.getName()) );", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE5MTQ3NA==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460191474", "bodyText": "I always think it's much more readable to split these operations on multiple lines\n.mapToPari()\n.groupByKey()\n.filter(...)", "author": "lbergelson", "createdAt": "2020-07-24T17:29:53Z", "path": "src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java", "diffHunk": "@@ -139,17 +141,31 @@ public void testPutReadsWithSameNameInSamePartition(int numPairs, int numPartiti\n         return reads;\n     }\n \n-    @Test(expectedExceptions = GATKException.class)\n-    public void testReadsPairsSpanningMultiplePartitionsCrash() {\n+    @Test\n+    public void testReadsPairsSpanningMultiplePartitions() {\n         JavaSparkContext ctx = SparkContextFactory.getTestSparkContext();\n         SAMFileHeader header = ArtificialReadUtils.createArtificialSamHeader();\n         header.setSortOrder(SAMFileHeader.SortOrder.queryname);\n         List<GATKRead> reads = createPairedReads(header, 40, 2);\n-        // Creating one group in the middle that should cause problems\n-        reads.addAll(40, createPairedReads(header, 1, 30));\n+        reads.addAll(createPairedReads(header, 1, 30));\n+        reads.sort( (r1, r2) -> r1.getName().compareTo(r2.getName()) );\n \n         JavaRDD<GATKRead> problemReads = ctx.parallelize(reads,5 );\n-        SparkUtils.putReadsWithTheSameNameInTheSamePartition(header, problemReads, ctx);\n+        JavaRDD<GATKRead> fixedReads =\n+                SparkUtils.putReadsWithTheSameNameInTheSamePartition(header, problemReads, ctx);\n+        final List<Tuple2<String, Iterable<Integer>>> xPartitionPairs = fixedReads\n+                .mapPartitionsWithIndex(( idx, itr ) -> {\n+                    final Set<Tuple2<String, Integer>> readLocs = new HashSet<>();\n+                    while ( itr.hasNext() ) readLocs.add(new Tuple2<>(itr.next().getName(), idx));\n+                    return readLocs.iterator();\n+                }, false)\n+                .mapToPair(t -> t).groupByKey().filter(t -> {", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE5MjMyMg==", "url": "https://github.com/broadinstitute/gatk/pull/6652#discussion_r460192322", "bodyText": "why not Iterators.size(t._2())", "author": "lbergelson", "createdAt": "2020-07-24T17:31:37Z", "path": "src/test/java/org/broadinstitute/hellbender/utils/spark/SparkUtilsUnitTest.java", "diffHunk": "@@ -139,17 +141,31 @@ public void testPutReadsWithSameNameInSamePartition(int numPairs, int numPartiti\n         return reads;\n     }\n \n-    @Test(expectedExceptions = GATKException.class)\n-    public void testReadsPairsSpanningMultiplePartitionsCrash() {\n+    @Test\n+    public void testReadsPairsSpanningMultiplePartitions() {\n         JavaSparkContext ctx = SparkContextFactory.getTestSparkContext();\n         SAMFileHeader header = ArtificialReadUtils.createArtificialSamHeader();\n         header.setSortOrder(SAMFileHeader.SortOrder.queryname);\n         List<GATKRead> reads = createPairedReads(header, 40, 2);\n-        // Creating one group in the middle that should cause problems\n-        reads.addAll(40, createPairedReads(header, 1, 30));\n+        reads.addAll(createPairedReads(header, 1, 30));\n+        reads.sort( (r1, r2) -> r1.getName().compareTo(r2.getName()) );\n \n         JavaRDD<GATKRead> problemReads = ctx.parallelize(reads,5 );\n-        SparkUtils.putReadsWithTheSameNameInTheSamePartition(header, problemReads, ctx);\n+        JavaRDD<GATKRead> fixedReads =\n+                SparkUtils.putReadsWithTheSameNameInTheSamePartition(header, problemReads, ctx);\n+        final List<Tuple2<String, Iterable<Integer>>> xPartitionPairs = fixedReads\n+                .mapPartitionsWithIndex(( idx, itr ) -> {\n+                    final Set<Tuple2<String, Integer>> readLocs = new HashSet<>();\n+                    while ( itr.hasNext() ) readLocs.add(new Tuple2<>(itr.next().getName(), idx));\n+                    return readLocs.iterator();\n+                }, false)\n+                .mapToPair(t -> t).groupByKey().filter(t -> {\n+                    int count = 0;\n+                    for ( final int idx : t._2() ) ++count;", "originalCommit": "ca061541e9c0d387b8572395d5773125db4cf865", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "url": "https://github.com/broadinstitute/gatk/commit/ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "message": "de-sparkify SV discovery", "committedDate": "2020-08-11T19:08:42Z", "type": "commit"}, {"oid": "ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "url": "https://github.com/broadinstitute/gatk/commit/ed68d5b98fe9f4b5de5e66df659a5dad2777a7dd", "message": "de-sparkify SV discovery", "committedDate": "2020-08-11T19:08:42Z", "type": "forcePushed"}]}