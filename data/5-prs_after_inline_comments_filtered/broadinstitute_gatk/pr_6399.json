{"pr_number": 6399, "pr_title": "AH - implement changes for mitochondrial pipeline", "pr_createdAt": "2020-01-21T18:12:08Z", "pr_url": "https://github.com/broadinstitute/gatk/pull/6399", "timeline": [{"oid": "33c2942b7fd44d7c6d70947d8629da6f0dd8cc51", "url": "https://github.com/broadinstitute/gatk/commit/33c2942b7fd44d7c6d70947d8629da6f0dd8cc51", "message": "update documentation", "committedDate": "2020-04-21T21:00:22Z", "type": "commit"}, {"oid": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3", "url": "https://github.com/broadinstitute/gatk/commit/959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3", "message": "minor changes and comments", "committedDate": "2020-01-22T15:06:46Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc3NjQ4OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369776489", "bodyText": "I believe you can return String.join(\",\", alleleValues)", "author": "davidbenjamin", "createdAt": "2020-01-22T20:06:53Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/allelespecific/StrandBiasUtils.java", "diffHunk": "@@ -0,0 +1,160 @@\n+package org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.hellbender.engine.filters.VariantFilter;\n+import org.broadinstitute.hellbender.tools.walkers.annotator.AnnotationUtils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class StrandBiasUtils {\n+    public static final int FORWARD = 0;\n+    public static final int REVERSE = 1;\n+    public static final int MIN_COUNT = 2;\n+    public static final String PRINT_DELIM = \"|\";\n+    private static final List<Integer> ZERO_LIST = new ArrayList<>(Arrays.asList(0,0));\n+\n+    public static Map<String, Object> computeSBAnnotation(VariantContext vc, AlleleLikelihoods<GATKRead, Allele> likelihoods, String key) {\n+        // calculate the annotation from the likelihoods\n+        // likelihoods can come from HaplotypeCaller call to VariantAnnotatorEngine\n+        final Map<String, Object> annotations = new HashMap<>();\n+        final ReducibleAnnotationData<List<Integer>> myData = new AlleleSpecificAnnotationData<>(vc.getAlleles(),null);\n+        getStrandCountsFromLikelihoodMap(vc, likelihoods, myData, MIN_COUNT);\n+        final Map<Allele, List<Integer>> perAlleleValues = myData.getAttributeMap();\n+        final String annotationString = makeRawAnnotationString(vc.getAlleles(), perAlleleValues);\n+        annotations.put(key, annotationString);\n+        return annotations;\n+    }\n+\n+    protected static String makeRawAnnotationString(final List<Allele> vcAlleles, final Map<Allele, List<Integer>> perAlleleValues) {\n+        String annotationString = \"\";\n+        for (final Allele a : vcAlleles) {\n+            if (!annotationString.isEmpty()) {\n+                annotationString += PRINT_DELIM;\n+            }\n+            List<Integer> alleleValues = perAlleleValues.get(a);\n+            if (alleleValues == null) {\n+                alleleValues = ZERO_LIST;\n+            }\n+            annotationString += encode(alleleValues);\n+        }\n+        return annotationString;\n+    }\n+\n+    protected static String encode(List<Integer> alleleValues) {\n+        String annotationString = \"\";", "originalCommit": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc3ODY4MA==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369778680", "bodyText": "This method could be written as\nfinal List<String> alleleStrings = vcAlleles.stream()\n    .map(a -> perAlleleValues.getOrDefault(a, ZERO_LIST))\n    .map(encode)\n    .collect(Collectors.toList());\nreturn String.join(PRINT_DELIM, alleleStrings);", "author": "davidbenjamin", "createdAt": "2020-01-22T20:12:00Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/allelespecific/StrandBiasUtils.java", "diffHunk": "@@ -0,0 +1,160 @@\n+package org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.hellbender.engine.filters.VariantFilter;\n+import org.broadinstitute.hellbender.tools.walkers.annotator.AnnotationUtils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class StrandBiasUtils {\n+    public static final int FORWARD = 0;\n+    public static final int REVERSE = 1;\n+    public static final int MIN_COUNT = 2;\n+    public static final String PRINT_DELIM = \"|\";\n+    private static final List<Integer> ZERO_LIST = new ArrayList<>(Arrays.asList(0,0));\n+\n+    public static Map<String, Object> computeSBAnnotation(VariantContext vc, AlleleLikelihoods<GATKRead, Allele> likelihoods, String key) {\n+        // calculate the annotation from the likelihoods\n+        // likelihoods can come from HaplotypeCaller call to VariantAnnotatorEngine\n+        final Map<String, Object> annotations = new HashMap<>();\n+        final ReducibleAnnotationData<List<Integer>> myData = new AlleleSpecificAnnotationData<>(vc.getAlleles(),null);\n+        getStrandCountsFromLikelihoodMap(vc, likelihoods, myData, MIN_COUNT);\n+        final Map<Allele, List<Integer>> perAlleleValues = myData.getAttributeMap();\n+        final String annotationString = makeRawAnnotationString(vc.getAlleles(), perAlleleValues);\n+        annotations.put(key, annotationString);\n+        return annotations;\n+    }\n+\n+    protected static String makeRawAnnotationString(final List<Allele> vcAlleles, final Map<Allele, List<Integer>> perAlleleValues) {\n+        String annotationString = \"\";\n+        for (final Allele a : vcAlleles) {\n+            if (!annotationString.isEmpty()) {", "originalCommit": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc3OTM3Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369779376", "bodyText": "This is already defined as AnnotationUtils.ALLELE_SPECIFIC_PRINT_DELIM", "author": "davidbenjamin", "createdAt": "2020-01-22T20:13:36Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/allelespecific/StrandBiasUtils.java", "diffHunk": "@@ -0,0 +1,160 @@\n+package org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.hellbender.engine.filters.VariantFilter;\n+import org.broadinstitute.hellbender.tools.walkers.annotator.AnnotationUtils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class StrandBiasUtils {\n+    public static final int FORWARD = 0;\n+    public static final int REVERSE = 1;\n+    public static final int MIN_COUNT = 2;\n+    public static final String PRINT_DELIM = \"|\";", "originalCommit": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4MDE5NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369780195", "bodyText": "How about:\nfinal int strand = read.isReverseStrand() ? REVERSE : FORWARD;\nalleleStrandCounts.set(strand, alleleStrandCounts.get(strand) + 1);", "author": "davidbenjamin", "createdAt": "2020-01-22T20:15:30Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/allelespecific/StrandBiasUtils.java", "diffHunk": "@@ -0,0 +1,160 @@\n+package org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.hellbender.engine.filters.VariantFilter;\n+import org.broadinstitute.hellbender.tools.walkers.annotator.AnnotationUtils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class StrandBiasUtils {\n+    public static final int FORWARD = 0;\n+    public static final int REVERSE = 1;\n+    public static final int MIN_COUNT = 2;\n+    public static final String PRINT_DELIM = \"|\";\n+    private static final List<Integer> ZERO_LIST = new ArrayList<>(Arrays.asList(0,0));\n+\n+    public static Map<String, Object> computeSBAnnotation(VariantContext vc, AlleleLikelihoods<GATKRead, Allele> likelihoods, String key) {\n+        // calculate the annotation from the likelihoods\n+        // likelihoods can come from HaplotypeCaller call to VariantAnnotatorEngine\n+        final Map<String, Object> annotations = new HashMap<>();\n+        final ReducibleAnnotationData<List<Integer>> myData = new AlleleSpecificAnnotationData<>(vc.getAlleles(),null);\n+        getStrandCountsFromLikelihoodMap(vc, likelihoods, myData, MIN_COUNT);\n+        final Map<Allele, List<Integer>> perAlleleValues = myData.getAttributeMap();\n+        final String annotationString = makeRawAnnotationString(vc.getAlleles(), perAlleleValues);\n+        annotations.put(key, annotationString);\n+        return annotations;\n+    }\n+\n+    protected static String makeRawAnnotationString(final List<Allele> vcAlleles, final Map<Allele, List<Integer>> perAlleleValues) {\n+        String annotationString = \"\";\n+        for (final Allele a : vcAlleles) {\n+            if (!annotationString.isEmpty()) {\n+                annotationString += PRINT_DELIM;\n+            }\n+            List<Integer> alleleValues = perAlleleValues.get(a);\n+            if (alleleValues == null) {\n+                alleleValues = ZERO_LIST;\n+            }\n+            annotationString += encode(alleleValues);\n+        }\n+        return annotationString;\n+    }\n+\n+    protected static String encode(List<Integer> alleleValues) {\n+        String annotationString = \"\";\n+        for (int j =0; j < alleleValues.size(); j++) {\n+            annotationString += alleleValues.get(j);\n+            if (j < alleleValues.size()-1) {\n+                annotationString += \",\";\n+            }\n+        }\n+        return annotationString;\n+    }\n+\n+\n+    /**\n+     Allocate and fill a 2x2 strand contingency table.  In the end, it'll look something like this:\n+     *             fw      rc\n+     *   allele1   #       #\n+     *   allele2   #       #\n+     * @return a 2x2 contingency table\n+     */\n+    public static void getStrandCountsFromLikelihoodMap( final VariantContext vc,\n+                                                  final AlleleLikelihoods<GATKRead, Allele> likelihoods,\n+                                                  final ReducibleAnnotationData<List<Integer>> perAlleleValues,\n+                                                  final int minCount) {\n+        if( likelihoods == null || vc == null ) {\n+            return;\n+        }\n+\n+        final Allele ref = vc.getReference();\n+        final List<Allele> allAlts = vc.getAlternateAlleles();\n+\n+        for (final String sample : likelihoods.samples()) {\n+            final ReducibleAnnotationData<List<Integer>> sampleTable = new AlleleSpecificAnnotationData<>(vc.getAlleles(),null);\n+            likelihoods.bestAllelesBreakingTies(sample).stream()\n+                    .filter(ba -> ba.isInformative())\n+                    .forEach(ba -> updateTable(ba.allele, ba.evidence, ref, allAlts, sampleTable));\n+            if (passesMinimumThreshold(sampleTable, minCount)) {\n+                combineAttributeMap(sampleTable, perAlleleValues);\n+            }\n+        }\n+    }\n+\n+    protected static void combineAttributeMap(final ReducibleAnnotationData<List<Integer>> toAdd, final ReducibleAnnotationData<List<Integer>> combined) {\n+        for (final Allele a : combined.getAlleles()) {\n+            if (toAdd.hasAttribute(a) && toAdd.getAttribute(a) != null) {\n+                if (combined.getAttribute(a) != null) {\n+                    combined.getAttribute(a).set(FORWARD, (int) combined.getAttribute(a).get(FORWARD) + (int) toAdd.getAttribute(a).get(FORWARD));\n+                    combined.getAttribute(a).set(REVERSE, (int) combined.getAttribute(a).get(REVERSE) + (int) toAdd.getAttribute(a).get(REVERSE));\n+                }\n+                else {\n+                    List<Integer> alleleData = new ArrayList<>();\n+                    alleleData.add(FORWARD, toAdd.getAttribute(a).get(FORWARD));\n+                    alleleData.add(REVERSE, toAdd.getAttribute(a).get(REVERSE));\n+                    combined.putAttribute(a,alleleData);\n+                }\n+            }\n+        }\n+    }\n+\n+    private static void updateTable(final Allele bestAllele, final GATKRead read, final Allele ref, final List<Allele> allAlts, final ReducibleAnnotationData<List<Integer>> perAlleleValues) {\n+\n+        final boolean matchesRef = bestAllele.equals(ref, true);\n+        final boolean matchesAnyAlt = allAlts.contains(bestAllele);\n+\n+        //can happen if a read's most likely allele has been removed when --max_alternate_alleles is exceeded\n+        if (!( matchesRef || matchesAnyAlt )) {\n+            return;\n+        }\n+\n+        final List<Integer> alleleStrandCounts;\n+        if (perAlleleValues.hasAttribute(bestAllele) && perAlleleValues.getAttribute(bestAllele) != null) {\n+            alleleStrandCounts = perAlleleValues.getAttribute(bestAllele);\n+        } else {\n+            alleleStrandCounts = new ArrayList<>();\n+            alleleStrandCounts.add(0,0);\n+            alleleStrandCounts.add(1,0);\n+        }\n+        final boolean isForward = !read.isReverseStrand();", "originalCommit": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4MzcwMA==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369783700", "bodyText": "As long as you're touching this code, could you put in javadoc that I should have done earlier, specifying that\n\ntumorADs are by alt allele, summed over samples\ntumor logOdds are by alt allele\nartifactProbabilities are by alelle and specifically technical artifact probabilities not including sequencing error, contamination, or germline variation\nnonSomatic probabilities are probabilitie that the variants are real but not somatic ie germline or contamination", "author": "davidbenjamin", "createdAt": "2020-01-22T20:23:01Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/clustering/SomaticClusteringModel.java", "diffHunk": "@@ -96,11 +96,16 @@ public double probabilityOfSequencingError(final Datum datum) {\n         return clusterProbabilities(datum)[SEQUENCING_ERROR_INDEX];\n     }\n \n-    public void record(final int[] tumorADs, final double[] tumorLogOdds, final double artifactProbability, final double nonSomaticProbability, final VariantContext vc) {\n+    public void record(final int[] tumorADs, final double[] tumorLogOdds, final List<Double> artifactProbabilities, final List<Double> nonSomaticProbabilities, final VariantContext vc) {", "originalCommit": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDMzMjA3Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r370332076", "bodyText": "i thought tumor ADs were for all alleles. that's why we sum them for the total AD and then fetch them by i+1 to skip the ref.", "author": "ahaessly", "createdAt": "2020-01-23T20:14:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4MzcwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM1MzMxMg==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r370353312", "bodyText": "My mistake.", "author": "davidbenjamin", "createdAt": "2020-01-23T21:04:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4MzcwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4NDY3NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369784675", "bodyText": "We should skip symbolic alleles -- don't record them in the clustering model or the threshold calculator, don't involve them in any filters that learn their parameters, and don't include them in the sum of ADs here, either.", "author": "davidbenjamin", "createdAt": "2020-01-22T20:25:16Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/clustering/SomaticClusteringModel.java", "diffHunk": "@@ -96,11 +96,16 @@ public double probabilityOfSequencingError(final Datum datum) {\n         return clusterProbabilities(datum)[SEQUENCING_ERROR_INDEX];\n     }\n \n-    public void record(final int[] tumorADs, final double[] tumorLogOdds, final double artifactProbability, final double nonSomaticProbability, final VariantContext vc) {\n+    public void record(final int[] tumorADs, final double[] tumorLogOdds, final List<Double> artifactProbabilities, final List<Double> nonSomaticProbabilities, final VariantContext vc) {\n         final int totalAD = (int) MathUtils.sum(tumorADs);\n+        // TODO: david b: is it important to have data for symbolic alleles?\n+        int numAltAlleles = tumorLogOdds.length;", "originalCommit": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4NTA3OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369785079", "bodyText": "There could conceivably be multiple symbolic alleles (spanning deletion in GVCF mode, for example), so\nfinal int numAltAlleles = tumorLogOdds.alleles().stream().filter(a -> !a.isSymbolic()).count() is safer.", "author": "davidbenjamin", "createdAt": "2020-01-22T20:26:17Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/clustering/SomaticClusteringModel.java", "diffHunk": "@@ -96,11 +96,16 @@ public double probabilityOfSequencingError(final Datum datum) {\n         return clusterProbabilities(datum)[SEQUENCING_ERROR_INDEX];\n     }\n \n-    public void record(final int[] tumorADs, final double[] tumorLogOdds, final double artifactProbability, final double nonSomaticProbability, final VariantContext vc) {\n+    public void record(final int[] tumorADs, final double[] tumorLogOdds, final List<Double> artifactProbabilities, final List<Double> nonSomaticProbabilities, final VariantContext vc) {\n         final int totalAD = (int) MathUtils.sum(tumorADs);\n+        // TODO: david b: is it important to have data for symbolic alleles?\n+        int numAltAlleles = tumorLogOdds.length;\n+        if (vc.hasSymbolicAlleles()) {\n+            numAltAlleles--;", "originalCommit": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4NTcxMw==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369785713", "bodyText": "And we shouldn't assume that symbolic alleles are at the end, so perhaps\nfor (int i = 0; i < tumorLogOdds.length; i++) {\n   if (allele i is not symbolic) {\n      data.add. . .\n   }\n}", "author": "davidbenjamin", "createdAt": "2020-01-22T20:27:54Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/clustering/SomaticClusteringModel.java", "diffHunk": "@@ -96,11 +96,16 @@ public double probabilityOfSequencingError(final Datum datum) {\n         return clusterProbabilities(datum)[SEQUENCING_ERROR_INDEX];\n     }\n \n-    public void record(final int[] tumorADs, final double[] tumorLogOdds, final double artifactProbability, final double nonSomaticProbability, final VariantContext vc) {\n+    public void record(final int[] tumorADs, final double[] tumorLogOdds, final List<Double> artifactProbabilities, final List<Double> nonSomaticProbabilities, final VariantContext vc) {\n         final int totalAD = (int) MathUtils.sum(tumorADs);\n+        // TODO: david b: is it important to have data for symbolic alleles?\n+        int numAltAlleles = tumorLogOdds.length;\n+        if (vc.hasSymbolicAlleles()) {\n+            numAltAlleles--;\n+        }\n         // split into one-vs-all biallelics for clustering\n-        for (int i = 0; i < tumorLogOdds.length; i++) {\n-            data.add(new Datum(tumorLogOdds[i], artifactProbability, nonSomaticProbability, tumorADs[i+1], totalAD, indelLength(vc, i)));\n+        for (int i = 0; i < numAltAlleles; i++) {", "originalCommit": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4NzAzOA==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369787038", "bodyText": "How about\nreturn vc.getAttributeAsIntList(GATKVCFConstants.MEDIAN_BASE_QUALITY_KEY, 0).stream().skip(1)  // skip ref\n   .map(qual -> qual < minMedianBaseQuality).collect(Collectors.toList());", "author": "davidbenjamin", "createdAt": "2020-01-22T20:31:02Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/BaseQualityFilter.java", "diffHunk": "@@ -19,12 +19,10 @@ public BaseQualityFilter(final double minMedianBaseQuality) {\n     public ErrorType errorType() { return ErrorType.ARTIFACT; }\n \n     @Override\n-    public boolean isArtifact(final VariantContext vc, final Mutect2FilteringEngine filteringEngine) {\n-        final List<Integer> baseQualityByAllele = vc.getAttributeAsIntList(GATKVCFConstants.MEDIAN_BASE_QUALITY_KEY, 0);\n-        final double[] tumorLods = Mutect2FilteringEngine.getTumorLogOdds(vc);\n-        final int indexOfMaxTumorLod = MathUtils.maxElementIndex(tumorLods);\n-\n-        return baseQualityByAllele.get(indexOfMaxTumorLod + 1) < minMedianBaseQuality;\n+    public List<Boolean> areAllelesArtifacts(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n+        List<Integer> baseQualityByAllele = vc.getAttributeAsIntList(GATKVCFConstants.MEDIAN_BASE_QUALITY_KEY, 0);", "originalCommit": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4NzQ5Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369787492", "bodyText": "Why does this need to be a method and not a static constant predicate?", "author": "davidbenjamin", "createdAt": "2020-01-22T20:32:09Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ChimericOriginalAlignmentFilter.java", "diffHunk": "@@ -16,15 +20,24 @@ public ChimericOriginalAlignmentFilter(final double maxNuMTFraction) {\n     @Override\n     public ErrorType errorType() { return ErrorType.ARTIFACT; }\n \n+    public Predicate<Genotype> checkPreconditions() {", "originalCommit": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgyMzY5Nw==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369823697", "bodyText": "ah, left over from different design. thanks", "author": "ahaessly", "createdAt": "2020-01-22T21:51:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4NzQ5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4ODMwMA==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369788300", "bodyText": "Do we still need to skip multiallelic sites?", "author": "davidbenjamin", "createdAt": "2020-01-22T20:34:17Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ChimericOriginalAlignmentFilter.java", "diffHunk": "@@ -16,15 +20,24 @@ public ChimericOriginalAlignmentFilter(final double maxNuMTFraction) {\n     @Override\n     public ErrorType errorType() { return ErrorType.ARTIFACT; }\n \n+    public Predicate<Genotype> checkPreconditions() {\n+        return Genotype::hasAD;\n+    }\n+\n+    public List<Integer> getData(Genotype g) {\n+        return Arrays.stream(g.getAD()).boxed().collect(Collectors.toList());\n+    }\n+\n     @Override\n-    public boolean isArtifact(final VariantContext vc, final Mutect2FilteringEngine filteringEngine) {\n+    public List<Boolean> areAllelesArtifacts(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n         if(!vc.isBiallelic()) {\n-            return false;\n+            return Collections.emptyList();", "originalCommit": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4ODg1NA==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369788854", "bodyText": "a list of the depths and posterior pairs of each sample", "author": "davidbenjamin", "createdAt": "2020-01-22T20:35:38Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ContaminationFilter.java", "diffHunk": "@@ -31,8 +32,10 @@ public ContaminationFilter(final List<File> contaminationTables, final double co\n     public ErrorType errorType() { return ErrorType.NON_SOMATIC; }\n \n     @Override\n-    public double calculateErrorProbability(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n-        final List<ImmutablePair<Integer, Double>> depthsAndPosteriors = new ArrayList<>();\n+    public List<Double> calculateErrorProbabilityForAlleles(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n+        // for every alt allele, a list of the depth and posterior pair", "originalCommit": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc4OTYzMA==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369789630", "bodyText": "The +1 offset of the comment seems not to be needed any more now that you copy the array starting at 1, right?", "author": "davidbenjamin", "createdAt": "2020-01-22T20:37:18Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ContaminationFilter.java", "diffHunk": "@@ -41,29 +44,38 @@ public double calculateErrorProbability(final VariantContext vc, final Mutect2Fi\n \n             final double contaminationFromFile = contaminationBySample.getOrDefault(tumorGenotype.getSampleName(), defaultContamination);\n             final double contamination = Math.max(0, Math.min(contaminationFromFile, 1 - EPSILON)); // handle file with contamination == 1\n-            final double[] alleleFractions = GATKProtectedVariantContextUtils.getAttributeAsDoubleArray(tumorGenotype, VCFConstants.ALLELE_FREQUENCY_KEY,\n-                    () -> new double[] {1.0}, 1.0);\n-            final int maxFractionIndex = MathUtils.maxElementIndex(alleleFractions);\n-            final int[] ADs = tumorGenotype.getAD();\n-            final int altCount = ADs[maxFractionIndex + 1];   // AD is all alleles, while AF is alts only, hence the +1 offset\n-            final int depth = (int) MathUtils.sum(ADs);\n+            final int[] ADs = tumorGenotype.getAD(); // AD is all alleles, while AF is alts only, hence the +1 offset", "originalCommit": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc5MTc3MA==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369791770", "bodyText": "This always returns a singleton list, even if there is more than one alt allele. . .", "author": "davidbenjamin", "createdAt": "2020-01-22T20:42:07Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/DuplicatedAltReadFilter.java", "diffHunk": "@@ -20,11 +21,11 @@ public DuplicatedAltReadFilter(final int uniqueAltReadCount) {\n     public ErrorType errorType() { return ErrorType.ARTIFACT; }\n \n     @Override\n-    public boolean isArtifact(final VariantContext vc, final Mutect2FilteringEngine filteringEngine) {\n-        return vc.getAttributeAsInt(UniqueAltReadCount.KEY, 1) <= uniqueAltReadCount;\n+    public List<Boolean> areAllelesArtifacts(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {", "originalCommit": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgyMTg5MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369821891", "bodyText": "good catch. So, UniqueAltReadCount is actually assuming a single alt allele. I guess I should change that class to write an allele specific annotation that can be used in an allele specific way in the filter.", "author": "ahaessly", "createdAt": "2020-01-22T21:47:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc5MTc3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkxNzM5Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r374917393", "bodyText": "Yes.", "author": "davidbenjamin", "createdAt": "2020-02-04T20:55:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc5MTc3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTc5OTAyMA==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369799020", "bodyText": "The VCF spec doesn't require it, so let's be cautious and not assume.", "author": "davidbenjamin", "createdAt": "2020-01-22T20:57:59Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ErrorProbabilities.java", "diffHunk": "@@ -3,36 +3,95 @@\n import htsjdk.variant.variantcontext.VariantContext;\n import org.broadinstitute.hellbender.engine.ReferenceContext;\n \n-import java.util.Arrays;\n-import java.util.EnumMap;\n-import java.util.List;\n-import java.util.Map;\n+import java.util.*;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import static java.util.stream.Collectors.*;\n \n public final class ErrorProbabilities {\n-    private final Map<Mutect2VariantFilter, Double> probabilitiesByFilter;\n-    private final EnumMap<ErrorType, Double> probabilitiesByType;\n-    private final double errorProbability;\n+    private  LinkedHashMap<Mutect2Filter, List<Double>> alleleProbabilitiesByFilter;\n+    private final Map<ErrorType, List<Double>> probabilitiesByTypeAndAllele;\n+    private final List<Double> combinedErrorProbabilitiesByAllele;\n+    private final int numAltAlleles;\n \n \n-    public ErrorProbabilities(final List<Mutect2VariantFilter> filters, final VariantContext vc, final Mutect2FilteringEngine filteringEngine, final ReferenceContext referenceContext) {\n-        probabilitiesByFilter = filters.stream().collect(Collectors.toMap(f -> f, f -> f.errorProbability(vc, filteringEngine, referenceContext)));\n-        probabilitiesByType = Arrays.stream(ErrorType.values()).collect(Collectors.toMap(v -> v, v -> 0.0, (a,b) -> a, () -> new EnumMap<>(ErrorType.class)));\n-        filters.forEach(f -> probabilitiesByType.compute(f.errorType(), (type,prob) -> Math.max(prob, probabilitiesByFilter.get(f))));\n+    public ErrorProbabilities(final List<Mutect2Filter> filters, final VariantContext vc, final Mutect2FilteringEngine filteringEngine, final ReferenceContext referenceContext) {\n+        numAltAlleles = vc.getAlternateAlleles().size();\n+        alleleProbabilitiesByFilter = filters.stream()\n+                .collect(toMap(\n+                        Function.identity(),\n+                        f -> f.errorProbabilities(vc, filteringEngine, referenceContext),\n+                        (a, b) -> a, LinkedHashMap::new))\n+                // remove filters that were not applied. i.e. returned empty list\n+                .entrySet().stream().filter(entry -> !entry.getValue().isEmpty())\n+                .collect(toMap(Map.Entry::getKey, Map.Entry::getValue, (a, b) -> a, LinkedHashMap::new));\n \n-        // treat errors of different types as independent\n-        double trueProbability = 1;\n-        for (final double errorProb : probabilitiesByType.values()) {\n-            trueProbability *= (1 - errorProb);\n+        // if vc has symbolic allele, remove it\n+        if (vc.hasSymbolicAlleles()) {\n+            // can we assume it's the last allele?", "originalCommit": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgwMDU2OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369800569", "bodyText": "Should there be an error thrown inside this block?", "author": "davidbenjamin", "createdAt": "2020-01-22T21:01:23Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ErrorProbabilities.java", "diffHunk": "@@ -3,36 +3,95 @@\n import htsjdk.variant.variantcontext.VariantContext;\n import org.broadinstitute.hellbender.engine.ReferenceContext;\n \n-import java.util.Arrays;\n-import java.util.EnumMap;\n-import java.util.List;\n-import java.util.Map;\n+import java.util.*;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import static java.util.stream.Collectors.*;\n \n public final class ErrorProbabilities {\n-    private final Map<Mutect2VariantFilter, Double> probabilitiesByFilter;\n-    private final EnumMap<ErrorType, Double> probabilitiesByType;\n-    private final double errorProbability;\n+    private  LinkedHashMap<Mutect2Filter, List<Double>> alleleProbabilitiesByFilter;\n+    private final Map<ErrorType, List<Double>> probabilitiesByTypeAndAllele;\n+    private final List<Double> combinedErrorProbabilitiesByAllele;\n+    private final int numAltAlleles;\n \n \n-    public ErrorProbabilities(final List<Mutect2VariantFilter> filters, final VariantContext vc, final Mutect2FilteringEngine filteringEngine, final ReferenceContext referenceContext) {\n-        probabilitiesByFilter = filters.stream().collect(Collectors.toMap(f -> f, f -> f.errorProbability(vc, filteringEngine, referenceContext)));\n-        probabilitiesByType = Arrays.stream(ErrorType.values()).collect(Collectors.toMap(v -> v, v -> 0.0, (a,b) -> a, () -> new EnumMap<>(ErrorType.class)));\n-        filters.forEach(f -> probabilitiesByType.compute(f.errorType(), (type,prob) -> Math.max(prob, probabilitiesByFilter.get(f))));\n+    public ErrorProbabilities(final List<Mutect2Filter> filters, final VariantContext vc, final Mutect2FilteringEngine filteringEngine, final ReferenceContext referenceContext) {\n+        numAltAlleles = vc.getAlternateAlleles().size();\n+        alleleProbabilitiesByFilter = filters.stream()\n+                .collect(toMap(\n+                        Function.identity(),\n+                        f -> f.errorProbabilities(vc, filteringEngine, referenceContext),\n+                        (a, b) -> a, LinkedHashMap::new))\n+                // remove filters that were not applied. i.e. returned empty list\n+                .entrySet().stream().filter(entry -> !entry.getValue().isEmpty())\n+                .collect(toMap(Map.Entry::getKey, Map.Entry::getValue, (a, b) -> a, LinkedHashMap::new));\n \n-        // treat errors of different types as independent\n-        double trueProbability = 1;\n-        for (final double errorProb : probabilitiesByType.values()) {\n-            trueProbability *= (1 - errorProb);\n+        // if vc has symbolic allele, remove it\n+        if (vc.hasSymbolicAlleles()) {\n+            // can we assume it's the last allele?\n+            int symIndex = numAltAlleles - 1;\n+            alleleProbabilitiesByFilter.values().stream().forEach(probList -> probList.remove(symIndex));\n         }\n+        LinkedHashMap<ErrorType, List<List<Double>>> probabilitiesByAllelesForEachFilter = alleleProbabilitiesByFilter.entrySet().stream().collect(\n+                groupingBy(entry -> entry.getKey().errorType(), LinkedHashMap::new, mapping(entry -> entry.getValue(), toList())));\n+        // convert the data so we have a list of probabilities by allele instead of filter\n+        probabilitiesByAllelesForEachFilter.replaceAll((k, v) -> ErrorProbabilities.transpose(v));\n+\n+        // foreach error type, get the max probability for each allele\n+        probabilitiesByTypeAndAllele = probabilitiesByAllelesForEachFilter.entrySet().stream().collect(toMap(\n+                Map.Entry::getKey,\n+                entry -> entry.getValue().stream().map(alleleList -> alleleList.stream().max(Double::compare).orElse(0.0)).collect(Collectors.toList()),\n+                (a,b) -> a, HashMap::new));\n+\n+\n+        // treat errors of different types as independent\n+        // transpose the lists of allele probabilities, so it is now a list per allele that contains the prob for each type\n+        // combine allele-wise\n+        combinedErrorProbabilitiesByAllele = transpose(probabilitiesByTypeAndAllele.values().stream().collect(toList()))\n+                .stream().map(\n+                        alleleProbabilities -> alleleProbabilities.stream().map(p -> 1.0 - p).reduce(1.0, (a, b) -> a * b)).collect(Collectors.toList());\n+        combinedErrorProbabilitiesByAllele.replaceAll(trueProb -> Mutect2FilteringEngine.roundFinitePrecisionErrors(1.0 - trueProb));\n+    }\n+\n+    public List<Double> getCombinedErrorProbabilities() { return combinedErrorProbabilitiesByAllele; }\n+    public List<Double> getTechnicalArtifactProbabilities() { return probabilitiesByTypeAndAllele.get(ErrorType.ARTIFACT); }\n+    public List<Double> getNonSomaticProbabilities() { return probabilitiesByTypeAndAllele.get(ErrorType.NON_SOMATIC); }\n+    public Map<Mutect2Filter, List<Double>> getProbabilitiesByFilter() { return alleleProbabilitiesByFilter; }\n+\n+    // helper functions for the few operations that still differ depending on whether the filter\n+    // is per variant or allele\n+    public Map<Mutect2Filter, List<Double>> getProbabilitiesForAlleleFilters() {\n+        return getPartitionedProbabilitiesByFilter(false);\n+    }\n \n-        errorProbability = Mutect2FilteringEngine.roundFinitePrecisionErrors(1 - trueProbability);\n+    public Map<Mutect2Filter, Double> getProbabilitiesForVariantFilters() {\n+        return getPartitionedProbabilitiesByFilter(true).entrySet().stream()\n+                .filter(entry -> entry.getValue() != null && !entry.getValue().isEmpty())\n+                .collect(toMap(entry -> entry.getKey(), entry -> entry.getValue().get(0)));\n     }\n \n-    public double getErrorProbability() { return errorProbability; }\n-    public double getTechnicalArtifactProbability() { return probabilitiesByType.get(ErrorType.ARTIFACT); }\n-    public double getNonSomaticProbability() { return probabilitiesByType.get(ErrorType.NON_SOMATIC); }\n-    public Map<Mutect2VariantFilter, Double> getProbabilitiesByFilter() { return probabilitiesByFilter; }\n+    private Map<Mutect2Filter, List<Double>> getPartitionedProbabilitiesByFilter(boolean variantOnly) {\n+        Map<Boolean, LinkedHashMap<Mutect2Filter, List<Double>>> groups =\n+                alleleProbabilitiesByFilter.entrySet().stream().collect(Collectors.partitioningBy(\n+                        entry -> Mutect2VariantFilter.class.isAssignableFrom(entry.getKey().getClass()),\n+                        toMap(Map.Entry::getKey, Map.Entry::getValue, (a,b) -> a, LinkedHashMap::new)));\n+        return groups.get(variantOnly);\n+    }\n \n+    // TODO would this be useful in a util class somewhere?\n+    private static <T> List<List<T>> transpose(List<List<T>> list) {\n+        // all lists need to be the same size\n+        final int N = list.stream().mapToInt(l -> l.size()).max().orElse(-1);\n+        if (list.stream().anyMatch(l -> l.size() != N)) {", "originalCommit": "959a4a2c6d5ab1a1f60f897f282cd89f9ccc6cd3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgyMjIyNw==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369822227", "bodyText": "yes, is there an example you can refer me to for the best way to throw this error?", "author": "ahaessly", "createdAt": "2020-01-22T21:48:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgwMDU2OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE3NTkzNw==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r370175937", "bodyText": "I think something like\nUtils.validateArg(list.stream().mapToInt(List::size).distinct().count() == 1, \"lists are not the same size\");\n\nThis throws an IllegalArgumentException, which seems like the right choice to me.", "author": "davidbenjamin", "createdAt": "2020-01-23T15:12:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTgwMDU2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkwODIzOQ==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369908239", "bodyText": "As above, why is this a method and not a constant predicate?", "author": "davidbenjamin", "createdAt": "2020-01-23T02:35:48Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/MinAlleleFractionFilter.java", "diffHunk": "@@ -18,16 +21,21 @@ public MinAlleleFractionFilter(final double minAf) {\n     @Override\n     public ErrorType errorType() { return ErrorType.ARTIFACT; }\n \n+    public Predicate<Genotype> checkPreconditions() {", "originalCommit": "ecdbbe7f71162e09cfdef50a4a42c22463374519", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkwODI5Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369908293", "bodyText": "final Genotype g", "author": "davidbenjamin", "createdAt": "2020-01-23T02:36:09Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/MinAlleleFractionFilter.java", "diffHunk": "@@ -18,16 +21,21 @@ public MinAlleleFractionFilter(final double minAf) {\n     @Override\n     public ErrorType errorType() { return ErrorType.ARTIFACT; }\n \n+    public Predicate<Genotype> checkPreconditions() {\n+        return g -> g.hasExtendedAttribute(GATKVCFConstants.ALLELE_FRACTION_KEY);\n+    }\n+\n+    public List<Double> getAltData(Genotype g) {", "originalCommit": "ecdbbe7f71162e09cfdef50a4a42c22463374519", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkwODU2MA==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369908560", "bodyText": "Instead of the stream, return Doubles.asList(data).", "author": "davidbenjamin", "createdAt": "2020-01-23T02:37:40Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/MinAlleleFractionFilter.java", "diffHunk": "@@ -18,16 +21,21 @@ public MinAlleleFractionFilter(final double minAf) {\n     @Override\n     public ErrorType errorType() { return ErrorType.ARTIFACT; }\n \n+    public Predicate<Genotype> checkPreconditions() {\n+        return g -> g.hasExtendedAttribute(GATKVCFConstants.ALLELE_FRACTION_KEY);\n+    }\n+\n+    public List<Double> getAltData(Genotype g) {\n+        double[] data = GATKProtectedVariantContextUtils.getAttributeAsDoubleArray(g, GATKVCFConstants.ALLELE_FRACTION_KEY, () -> null, 1.0);\n+        return Arrays.stream(data).boxed().collect(Collectors.toList());", "originalCommit": "ecdbbe7f71162e09cfdef50a4a42c22463374519", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxMTQ2OA==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369911468", "bodyText": "There should be logic to \"promote\" an allele-specific filter that applies to every allele to a site-level filter.  Also, if all alleles fail, but for different reasons, what happens to the site filter?  Apologies if those features are here and I missed them.  It looks to me like perhaps this is a side-effect of addFilterStrings, but I'm having trouble following.", "author": "davidbenjamin", "createdAt": "2020-01-23T02:53:08Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2FilteringEngine.java", "diffHunk": "@@ -175,28 +177,89 @@ public VariantContext applyFiltersAndAccumulateOutputStats(final VariantContext\n         final ErrorProbabilities errorProbabilities = new ErrorProbabilities(filters, vc, this, referenceContext);\n         filteringOutputStats.recordCall(errorProbabilities, getThreshold() - EPSILON);\n \n-        final boolean variantFailsFilters = errorProbabilities.getErrorProbability() > Math.min(1 - EPSILON, Math.max(EPSILON, getThreshold()));\n-        final double maxErrorProb = errorProbabilities.getProbabilitiesByFilter().values().stream().mapToDouble(p->p).max().orElse(1);\n+        // error probability must exceed threshold, and just in case threshold is bad, probabilities close to 1 must be filtered\n+        // and probabilities close to 0 must not be filtered\n+        double errorThreshold = Math.min(1 - EPSILON, Math.max(EPSILON, getThreshold()));\n+\n+        Map<String, Double> siteFiltersWithErrorProb = new LinkedHashMap<>();\n+\n+        // apply allele specific filters\n+        List<Iterator<String>> ASFilters =\n+                errorProbabilities.getProbabilitiesForAlleleFilters().entrySet().stream()\n+                        .filter(entry -> !entry.getValue().isEmpty())\n+                        .map(entry -> addFilterStrings(entry.getValue(), siteFiltersWithErrorProb, errorThreshold, entry.getKey().filterName())).collect(Collectors.toList());\n+\n+        List<String> orderedASFilterStrings = vc.getAlleles().stream().map(allele -> allele.isReference() || allele.isSymbolic() ?\n+                VCFConstants.EMPTY_INFO_FIELD : getMergedFilterStringForAllele(ASFilters)).collect(Collectors.toList());\n+        String finalAttrString = AnnotationUtils.encodeAnyASList(orderedASFilterStrings);\n+\n+        vcb.putAttributes(Collections.singletonMap(GATKVCFConstants.AS_FILTER_STATUS_KEY, finalAttrString));\n+\n+\n+        // compute site-only filters\n+        errorProbabilities.getProbabilitiesForVariantFilters().entrySet().stream()", "originalCommit": "ecdbbe7f71162e09cfdef50a4a42c22463374519", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxMTc1OA==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369911758", "bodyText": "I think there should not be such an option.  MIN_REPORTABLE_ERROR_PROBABILITY is fairly low.", "author": "davidbenjamin", "createdAt": "2020-01-23T02:54:46Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2FilteringEngine.java", "diffHunk": "@@ -175,28 +177,89 @@ public VariantContext applyFiltersAndAccumulateOutputStats(final VariantContext\n         final ErrorProbabilities errorProbabilities = new ErrorProbabilities(filters, vc, this, referenceContext);\n         filteringOutputStats.recordCall(errorProbabilities, getThreshold() - EPSILON);\n \n-        final boolean variantFailsFilters = errorProbabilities.getErrorProbability() > Math.min(1 - EPSILON, Math.max(EPSILON, getThreshold()));\n-        final double maxErrorProb = errorProbabilities.getProbabilitiesByFilter().values().stream().mapToDouble(p->p).max().orElse(1);\n+        // error probability must exceed threshold, and just in case threshold is bad, probabilities close to 1 must be filtered\n+        // and probabilities close to 0 must not be filtered\n+        double errorThreshold = Math.min(1 - EPSILON, Math.max(EPSILON, getThreshold()));\n+\n+        Map<String, Double> siteFiltersWithErrorProb = new LinkedHashMap<>();\n+\n+        // apply allele specific filters\n+        List<Iterator<String>> ASFilters =\n+                errorProbabilities.getProbabilitiesForAlleleFilters().entrySet().stream()\n+                        .filter(entry -> !entry.getValue().isEmpty())\n+                        .map(entry -> addFilterStrings(entry.getValue(), siteFiltersWithErrorProb, errorThreshold, entry.getKey().filterName())).collect(Collectors.toList());\n+\n+        List<String> orderedASFilterStrings = vc.getAlleles().stream().map(allele -> allele.isReference() || allele.isSymbolic() ?\n+                VCFConstants.EMPTY_INFO_FIELD : getMergedFilterStringForAllele(ASFilters)).collect(Collectors.toList());\n+        String finalAttrString = AnnotationUtils.encodeAnyASList(orderedASFilterStrings);\n+\n+        vcb.putAttributes(Collections.singletonMap(GATKVCFConstants.AS_FILTER_STATUS_KEY, finalAttrString));\n+\n+\n+        // compute site-only filters\n+        errorProbabilities.getProbabilitiesForVariantFilters().entrySet().stream()\n+                .forEach(entry -> {\n+                    entry.getKey().phredScaledPosteriorAnnotationName().ifPresent(annotation -> {\n+                        if (entry.getKey().requiredAnnotations().stream().allMatch(vc::hasAttribute)) {\n+                            vcb.attribute(annotation, QualityUtils.errorProbToQual(entry.getValue()));\n+                        }\n+                    });\n+                    if (entry.getValue() > errorThreshold) {\n+                        siteFiltersWithErrorProb.put(entry.getKey().filterName(), entry.getValue());\n+                    }\n \n-        for (final Map.Entry<Mutect2VariantFilter, Double> entry : errorProbabilities.getProbabilitiesByFilter().entrySet()) {\n-            final double errorProbability = entry.getValue();\n+                });\n \n-            entry.getKey().phredScaledPosteriorAnnotationName().ifPresent(annotation -> {\n-                if (entry.getKey().requiredAnnotations().stream().allMatch(vc::hasAttribute)) {\n-                    vcb.attribute(annotation, QualityUtils.errorProbToQual(errorProbability));\n-                }\n-            });\n+        // TO reviewers - should there be a flag where this is skipped and all filters are in the output vcf?", "originalCommit": "ecdbbe7f71162e09cfdef50a4a42c22463374519", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxMjY2NA==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369912664", "bodyText": "I think it's the correct default.", "author": "davidbenjamin", "createdAt": "2020-01-23T02:59:21Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2FilteringEngine.java", "diffHunk": "@@ -175,28 +177,89 @@ public VariantContext applyFiltersAndAccumulateOutputStats(final VariantContext\n         final ErrorProbabilities errorProbabilities = new ErrorProbabilities(filters, vc, this, referenceContext);\n         filteringOutputStats.recordCall(errorProbabilities, getThreshold() - EPSILON);\n \n-        final boolean variantFailsFilters = errorProbabilities.getErrorProbability() > Math.min(1 - EPSILON, Math.max(EPSILON, getThreshold()));\n-        final double maxErrorProb = errorProbabilities.getProbabilitiesByFilter().values().stream().mapToDouble(p->p).max().orElse(1);\n+        // error probability must exceed threshold, and just in case threshold is bad, probabilities close to 1 must be filtered\n+        // and probabilities close to 0 must not be filtered\n+        double errorThreshold = Math.min(1 - EPSILON, Math.max(EPSILON, getThreshold()));\n+\n+        Map<String, Double> siteFiltersWithErrorProb = new LinkedHashMap<>();\n+\n+        // apply allele specific filters\n+        List<Iterator<String>> ASFilters =\n+                errorProbabilities.getProbabilitiesForAlleleFilters().entrySet().stream()\n+                        .filter(entry -> !entry.getValue().isEmpty())\n+                        .map(entry -> addFilterStrings(entry.getValue(), siteFiltersWithErrorProb, errorThreshold, entry.getKey().filterName())).collect(Collectors.toList());\n+\n+        List<String> orderedASFilterStrings = vc.getAlleles().stream().map(allele -> allele.isReference() || allele.isSymbolic() ?\n+                VCFConstants.EMPTY_INFO_FIELD : getMergedFilterStringForAllele(ASFilters)).collect(Collectors.toList());\n+        String finalAttrString = AnnotationUtils.encodeAnyASList(orderedASFilterStrings);\n+\n+        vcb.putAttributes(Collections.singletonMap(GATKVCFConstants.AS_FILTER_STATUS_KEY, finalAttrString));\n+\n+\n+        // compute site-only filters\n+        errorProbabilities.getProbabilitiesForVariantFilters().entrySet().stream()\n+                .forEach(entry -> {\n+                    entry.getKey().phredScaledPosteriorAnnotationName().ifPresent(annotation -> {\n+                        if (entry.getKey().requiredAnnotations().stream().allMatch(vc::hasAttribute)) {\n+                            vcb.attribute(annotation, QualityUtils.errorProbToQual(entry.getValue()));\n+                        }\n+                    });\n+                    if (entry.getValue() > errorThreshold) {\n+                        siteFiltersWithErrorProb.put(entry.getKey().filterName(), entry.getValue());\n+                    }\n \n-        for (final Map.Entry<Mutect2VariantFilter, Double> entry : errorProbabilities.getProbabilitiesByFilter().entrySet()) {\n-            final double errorProbability = entry.getValue();\n+                });\n \n-            entry.getKey().phredScaledPosteriorAnnotationName().ifPresent(annotation -> {\n-                if (entry.getKey().requiredAnnotations().stream().allMatch(vc::hasAttribute)) {\n-                    vcb.attribute(annotation, QualityUtils.errorProbToQual(errorProbability));\n-                }\n-            });\n+        // TO reviewers - should there be a flag where this is skipped and all filters are in the output vcf?\n+        // otherwise things may seem erroneous. and should we apply this type of limit on the allele specific filters too?\n \n-            // error probability must exceed threshold, and just in case threshold is bad, probabilities close to 1 must be filtered\n-            // and probabilities close to 0 must not be filtered\n-            if (variantFailsFilters && errorProbability >= Math.min(maxErrorProb, MIN_REPORTABLE_ERROR_PROBABILITY)) {\n-                vcb.filter(entry.getKey().filterName());\n+        // this code limits the number of filters specified for any variant to the highest probability filters\n+        // this will not change the status of whether a variant is actually filtered or not\n+        final double maxErrorProb = siteFiltersWithErrorProb.values().stream().mapToDouble(p->p).max().orElse(1);\n+        siteFiltersWithErrorProb.entrySet().stream().forEach(entry -> {\n+            if (entry.getValue() >= Math.min(maxErrorProb, MIN_REPORTABLE_ERROR_PROBABILITY)) {\n+                vcb.filter(entry.getKey());\n             }\n-        }\n+        });\n \n         return vcb.make();\n     }\n \n+    /**\n+     * Creates a comma separated string of all the filters that apply to the allele. This is basically\n+     * a pivot of the data. we have filterlist -> allele -> filterName. and we want allele -> list of filterName\n+     * @param alleleSpecificFilters all of the allele specific filters with the allele filter info\n+     * @return encoded (comma separated) list of filters that apply to the allele\n+     */\n+    private String getMergedFilterStringForAllele(List<Iterator<String>> alleleSpecificFilters) {\n+        // loop through each filter and pull out the filters the specified allele\n+        List<String> results = alleleSpecificFilters.stream().map(alleleValuesIterator -> alleleValuesIterator.next()).distinct().collect(Collectors.toList());\n+        if (results.size() > 1 && results.contains(VCFConstants.PASSES_FILTERS_v4)) {\n+            results.remove(VCFConstants.PASSES_FILTERS_v4);\n+        } else if (results.isEmpty()) {\n+            results.add(VCFConstants.PASSES_FILTERS_v4);\n+        }\n+        return AnnotationUtils.encodeStringList(results);\n+    }\n+\n+    /**\n+     * For each allele, determine whether the filter should be applied. also determine if the filter should apply to the site\n+     * @param probabilities the probability computed by the filter for the allele\n+     * @param siteFiltersWithErrorProb in/out parameter that is collecting site level filters with the max error probability\n+     * @param errorThreshold the theshold to use to determine whether filter applies\n+     * @param filterName the name of the filter used in the vcf\n+     * @return Iterator of filters for an allele\n+     */\n+    private Iterator<String> addFilterStrings(List<Double> probabilities, Map<String, Double> siteFiltersWithErrorProb, double errorThreshold, String filterName) {\n+        List<String> results = probabilities.stream().map(value -> value > errorThreshold ?\n+                        filterName : VCFConstants.PASSES_FILTERS_v4).collect(Collectors.toList());\n+        if (!results.isEmpty() && results.stream().allMatch(x -> x.equals(filterName))) {\n+            // TODO is this the correct default", "originalCommit": "ecdbbe7f71162e09cfdef50a4a42c22463374519", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxMjc1NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369912755", "bodyText": "What are these test gvcf comments about?", "author": "davidbenjamin", "createdAt": "2020-01-23T02:59:53Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2FilteringEngine.java", "diffHunk": "@@ -214,15 +277,21 @@ private void buildFiltersList(final M2FiltersArgumentCollection MTFAC) {\n         filters.add(new BaseQualityFilter(MTFAC.minMedianBaseQuality));\n         filters.add(new MappingQualityFilter(MTFAC.minMedianMappingQuality, MTFAC.longIndelLength));\n         filters.add(new DuplicatedAltReadFilter(MTFAC.uniqueAltReadCount));\n-        filters.add(new StrandArtifactFilter());\n+        filters.add(new StrandArtifactFilter());  // test gvcf", "originalCommit": "ecdbbe7f71162e09cfdef50a4a42c22463374519", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxMjg3Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369912876", "bodyText": "Address the TODO", "author": "davidbenjamin", "createdAt": "2020-01-23T03:00:33Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2FilteringEngine.java", "diffHunk": "@@ -234,8 +303,8 @@ private void buildFiltersList(final M2FiltersArgumentCollection MTFAC) {\n         }\n \n         if (MTFAC.mitochondria) {\n-            filters.add(new ChimericOriginalAlignmentFilter(MTFAC.maxNuMTFraction));\n-            filters.add(new PolymorphicNuMTFilter(MTFAC.medianAutosomalCoverage));\n+            filters.add(new ChimericOriginalAlignmentFilter(MTFAC.maxNuMTFraction));  // TODO convert!!", "originalCommit": "ecdbbe7f71162e09cfdef50a4a42c22463374519", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxMzE2Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369913162", "bodyText": "This can be achieved with Collections.nCopies()", "author": "davidbenjamin", "createdAt": "2020-01-23T03:02:09Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2VariantFilter.java", "diffHunk": "@@ -1,56 +1,25 @@\n package org.broadinstitute.hellbender.tools.walkers.mutect.filtering;\n \n import htsjdk.variant.variantcontext.VariantContext;\n-import org.apache.commons.lang3.tuple.ImmutablePair;\n import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.utils.IndexRange;\n \n-import java.util.Comparator;\n+import java.util.ArrayList;\n import java.util.List;\n-import java.util.Optional;\n \n-public abstract class Mutect2VariantFilter {\n+public abstract class Mutect2VariantFilter extends Mutect2Filter {\n     public Mutect2VariantFilter() { }\n \n-    public double errorProbability(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n-        final double result = requiredAnnotations().stream().allMatch(vc::hasAttribute) ? calculateErrorProbability(vc, filteringEngine, referenceContext) : 0;\n-        return Mutect2FilteringEngine.roundFinitePrecisionErrors(result);\n-    }\n-\n-    protected abstract double calculateErrorProbability(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext);\n+    @Override\n+    public List<Double> errorProbabilities(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n+        int numAltAlleles = vc.getNAlleles() - 1;\n+        final double result = Mutect2FilteringEngine.roundFinitePrecisionErrors(requiredAnnotations().stream().allMatch(vc::hasAttribute) ?\n+                calculateErrorProbability(vc, filteringEngine, referenceContext) : 0.0);\n+        ArrayList<Double> resultList = new ArrayList<>(numAltAlleles);", "originalCommit": "ecdbbe7f71162e09cfdef50a4a42c22463374519", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxMzcxMw==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369913713", "bodyText": "Is .map(   ).reduce(0, Math::addExact) equivalent to .mapToInt(  ).sum()?", "author": "davidbenjamin", "createdAt": "2020-01-23T03:05:05Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/StrandArtifactFilter.java", "diffHunk": "@@ -43,30 +44,40 @@\n     public ErrorType errorType() { return ErrorType.ARTIFACT; }\n \n     @Override\n-    public double calculateErrorProbability(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n-        final EStep probabilities = calculateArtifactProbabilities(vc, filteringEngine);\n-        return probabilities.forwardArtifactResponsibility + probabilities.reverseArtifactResponsibility;\n+    public List<Double> calculateErrorProbabilityForAlleles(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n+        final List<EStep> alleleProbs = calculateArtifactProbabilities(vc, filteringEngine);\n+        return alleleProbs.isEmpty() ? Collections.emptyList() :\n+                alleleProbs.stream().map(probabilities -> probabilities.forwardArtifactResponsibility + probabilities.reverseArtifactResponsibility).collect(Collectors.toList());\n     }\n \n-    public EStep calculateArtifactProbabilities(final VariantContext vc, final Mutect2FilteringEngine filteringEngine) {\n-        // {fwd ref, rev ref, fwd alt, rev alt}\n-        final int[] counts = filteringEngine.sumStrandCountsOverSamples(vc, true, false);\n-\n-        final int indelSize = Math.abs(vc.getReference().length() - vc.getAlternateAllele(0).length());\n-        if (counts[2] + counts[3] == 0 || indelSize > LONGEST_STRAND_ARTIFACT_INDEL_SIZE) {\n-            return new EStep(0, 0, counts[0] + counts[2], counts[1] + counts[3], counts[2], counts[3]);\n+    public List<EStep> calculateArtifactProbabilities(final VariantContext vc, final Mutect2FilteringEngine filteringEngine) {\n+        // for each allele, forward and reverse count\n+        List<List<Integer>> sbs = StrandBiasUtils.getSBsForAlleles(vc);\n+        if (sbs == null || sbs.isEmpty() || sbs.size() <= 1) {\n+            return Collections.emptyList();\n         }\n \n-\n-        return strandArtifactProbability(strandArtifactPrior, counts[0] + counts[2], counts[1] + counts[3], counts[2], counts[3], indelSize);\n-\n+        final ListIterator<Integer> indelSizeIterator = vc.getAlternateAlleles().stream().map(alt -> Math.abs(vc.getReference().length() - alt.length())).collect(Collectors.toList()).listIterator();\n+        int totalFwd = sbs.stream().map(sb -> sb.get(0)).reduce(0, Math::addExact);", "originalCommit": "ecdbbe7f71162e09cfdef50a4a42c22463374519", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNDI0NA==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369914244", "bodyText": "Incrementing an iterator as a side-effect of a lambda spooks me.  It feels like it's adhering to the letter of the law that requires variables inside a lambda to be final but violating the spirit of functional programming.  I would rather have an indel size list and have an IntStream even at the cost of an extra line altSB = altSBs.get(n).", "author": "davidbenjamin", "createdAt": "2020-01-23T03:07:54Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/StrandArtifactFilter.java", "diffHunk": "@@ -43,30 +44,40 @@\n     public ErrorType errorType() { return ErrorType.ARTIFACT; }\n \n     @Override\n-    public double calculateErrorProbability(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n-        final EStep probabilities = calculateArtifactProbabilities(vc, filteringEngine);\n-        return probabilities.forwardArtifactResponsibility + probabilities.reverseArtifactResponsibility;\n+    public List<Double> calculateErrorProbabilityForAlleles(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n+        final List<EStep> alleleProbs = calculateArtifactProbabilities(vc, filteringEngine);\n+        return alleleProbs.isEmpty() ? Collections.emptyList() :\n+                alleleProbs.stream().map(probabilities -> probabilities.forwardArtifactResponsibility + probabilities.reverseArtifactResponsibility).collect(Collectors.toList());\n     }\n \n-    public EStep calculateArtifactProbabilities(final VariantContext vc, final Mutect2FilteringEngine filteringEngine) {\n-        // {fwd ref, rev ref, fwd alt, rev alt}\n-        final int[] counts = filteringEngine.sumStrandCountsOverSamples(vc, true, false);\n-\n-        final int indelSize = Math.abs(vc.getReference().length() - vc.getAlternateAllele(0).length());\n-        if (counts[2] + counts[3] == 0 || indelSize > LONGEST_STRAND_ARTIFACT_INDEL_SIZE) {\n-            return new EStep(0, 0, counts[0] + counts[2], counts[1] + counts[3], counts[2], counts[3]);\n+    public List<EStep> calculateArtifactProbabilities(final VariantContext vc, final Mutect2FilteringEngine filteringEngine) {\n+        // for each allele, forward and reverse count\n+        List<List<Integer>> sbs = StrandBiasUtils.getSBsForAlleles(vc);\n+        if (sbs == null || sbs.isEmpty() || sbs.size() <= 1) {\n+            return Collections.emptyList();\n         }\n \n-\n-        return strandArtifactProbability(strandArtifactPrior, counts[0] + counts[2], counts[1] + counts[3], counts[2], counts[3], indelSize);\n-\n+        final ListIterator<Integer> indelSizeIterator = vc.getAlternateAlleles().stream().map(alt -> Math.abs(vc.getReference().length() - alt.length())).collect(Collectors.toList()).listIterator();\n+        int totalFwd = sbs.stream().map(sb -> sb.get(0)).reduce(0, Math::addExact);\n+        int totalRev = sbs.stream().map(sb -> sb.get(1)).reduce(0, Math::addExact);\n+        // skip the reference\n+        List<List<Integer>> altSBs = sbs.subList(1, sbs.size());\n+\n+        return altSBs.stream().map(altSB -> {\n+            final int altIndelSize = indelSizeIterator.next();", "originalCommit": "ecdbbe7f71162e09cfdef50a4a42c22463374519", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNDQzNw==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369914437", "bodyText": "Thank you.", "author": "davidbenjamin", "createdAt": "2020-01-23T03:09:02Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/ThresholdCalculator.java", "diffHunk": "@@ -20,7 +20,7 @@\n \n     private double threshold;\n \n-    final List<Double> artifactProbabilities = new ArrayList<>();\n+    final List<Double> errorProbabilities = new ArrayList<>();", "originalCommit": "ecdbbe7f71162e09cfdef50a4a42c22463374519", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNDY4Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369914682", "bodyText": "Why not\nreturn new IndexRange(0, tumorLods.length).mapToDouble(i -> _____);", "author": "davidbenjamin", "createdAt": "2020-01-23T03:10:34Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/TumorEvidenceFilter.java", "diffHunk": "@@ -4,27 +4,30 @@\n import org.broadinstitute.hellbender.engine.ReferenceContext;\n import org.broadinstitute.hellbender.tools.walkers.mutect.clustering.Datum;\n import org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel;\n+import org.broadinstitute.hellbender.utils.IndexRange;\n import org.broadinstitute.hellbender.utils.MathUtils;\n import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n \n-import java.util.Collections;\n-import java.util.List;\n-import java.util.Optional;\n+import java.util.*;\n \n-public class TumorEvidenceFilter extends Mutect2VariantFilter {\n+public class TumorEvidenceFilter extends Mutect2AlleleFilter {\n     @Override\n     public ErrorType errorType() { return ErrorType.SEQUENCING; }\n \n     @Override\n-    public double calculateErrorProbability(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n+    protected List<Double> calculateErrorProbabilityForAlleles(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext)\n+    {\n         final double[] tumorLods = Mutect2FilteringEngine.getTumorLogOdds(vc);\n         final int[] ADs = filteringEngine.sumADsOverSamples(vc, true, false);\n-        final int maxIndex = MathUtils.maxElementIndex(tumorLods);\n-        final int altCount = ADs[maxIndex + 1];\n         final int totalCount = (int) MathUtils.sum(ADs);\n+        SomaticClusteringModel model = filteringEngine.getSomaticClusteringModel();\n \n-        return filteringEngine.getSomaticClusteringModel()\n-                .probabilityOfSequencingError(new Datum(tumorLods[maxIndex], 0, 0, altCount, totalCount, SomaticClusteringModel.indelLength(vc, maxIndex)));\n+        List<Double> altResults = new ArrayList<>();\n+        // 0 is the correct value. problem with threshold\n+        new IndexRange(0, tumorLods.length).forEach(i ->", "originalCommit": "ecdbbe7f71162e09cfdef50a4a42c22463374519", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE1OTg3NA==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r370159874", "bodyText": "this is what i had to do. can you think of a way to simplify?\nreturn Arrays.stream(new IndexRange(0, tumorLods.length).mapToDouble(i ->\n    model.probabilityOfSequencingError(new Datum(tumorLods[i], 0, 0, ADs[i+1],\n        totalCount, SomaticClusteringModel.indelLength(vc, i)))))\n                .boxed().collect(Collectors.toList());", "author": "ahaessly", "createdAt": "2020-01-23T14:46:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNDY4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE4MTA2MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r370181061", "bodyText": "Ah, I see, because IndexRange.mapToDouble gives you a double[], not a list.  In that case how about\nreturn IntStream.range(0, tumorLods.length)\n   .mapToObj(i -> new Datum(tumorLods[i], 0, 0, ADs[i+1], totalCount, SomaticClusteringModel.indelLength(vc, i)))\n   .map(model::probabilityOfSequencingError)\n   .collect(Collectors.toList());", "author": "davidbenjamin", "createdAt": "2020-01-23T15:20:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNDY4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNDk2Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369914962", "bodyText": "I try to keep every M2 filter name at least 8 characters and less than 16.  That way the filtering stats file and, with luck, the VCF, looks better when viewed in a terminal.", "author": "davidbenjamin", "createdAt": "2020-01-23T03:12:20Z", "path": "src/main/java/org/broadinstitute/hellbender/utils/variant/GATKVCFConstants.java", "diffHunk": "@@ -162,7 +162,8 @@ their names (or descriptions) depend on some threshold.  Those filters are not i\n     public final static String N_RATIO_FILTER_NAME =                           \"n_ratio\";\n     public final static String CHIMERIC_ORIGINAL_ALIGNMENT_FILTER_NAME =       \"numt_chimera\"; //mitochondria\n     public final static String ALLELE_FRACTION_FILTER_NAME =                   \"low_allele_frac\";\n-    public static final String POTENTIAL_POLYMORPHIC_NUMT_FILTER_NAME =        \"numt_novel\";\n+    public static final String POSSIBLE_NUMT_FILTER_NAME =                     \"possible_numt\";\n+    public static final String LOW_HET_FILTER_NAME =                            \"low_het\";", "originalCommit": "ecdbbe7f71162e09cfdef50a4a42c22463374519", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTkxNTE2Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r369915166", "bodyText": "What's the status of this comment?", "author": "davidbenjamin", "createdAt": "2020-01-23T03:13:37Z", "path": "src/test/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2IntegrationTest.java", "diffHunk": "@@ -532,21 +550,28 @@ public void testFilterMitochondria(File unfiltered, final double minAlleleFracti\n                 args -> args.addBooleanArgument(StandardArgumentDefinitions.DISABLE_SEQUENCE_DICT_VALIDATION_NAME, true),\n                 args -> args.addNumericArgument(M2FiltersArgumentCollection.MIN_AF_LONG_NAME, minAlleleFraction),\n                 args -> args.addNumericArgument(M2FiltersArgumentCollection.MEDIAN_AUTOSOMAL_COVERAGE_LONG_NAME, autosomalCoverage),\n+                args -> args.addNumericArgument(M2FiltersArgumentCollection.MAX_NUMT_COPIES_IN_AUTOSOME_LONG_NAME, 4.0),\n+                args -> args.addNumericArgument(M2FiltersArgumentCollection.MIN_READS_ON_EACH_STRAND_LONG_NAME, 1),\n                 args -> {\n                     intervals.stream().map(SimpleInterval::new).forEach(args::addInterval);\n                     return args;\n                 });\n \n+        // add tests for DUPLICATE", "originalCommit": "ecdbbe7f71162e09cfdef50a4a42c22463374519", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "194678599cdb8154383cc63f1fefb7ff2cf7ff13", "url": "https://github.com/broadinstitute/gatk/commit/194678599cdb8154383cc63f1fefb7ff2cf7ff13", "message": "changes after rebase and to test FAIL filter", "committedDate": "2020-01-24T15:12:27Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE0NzM5NA==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r370147394", "bodyText": "I'm on the fence about putting this in the allelespecific subpackage.  On the one hand, it is allele-specific, but on the other hand, all those classes have interfaces and/or parent classes that this one doesn't.", "author": "ldgauthier", "createdAt": "2020-01-23T14:26:12Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/AS_StrandBiasMutectAnnotation.java", "diffHunk": "@@ -0,0 +1,44 @@\n+package org.broadinstitute.hellbender.tools.walkers.annotator;", "originalCommit": "ecdbbe7f71162e09cfdef50a4a42c22463374519", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDI5NDk2OA==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r374294968", "bodyText": "hmm, now that I look at that class again, I think I'll add the AlleleSpecificAnnotation interface to it. but, yeah, I wasn't really sure which package it belonged in.", "author": "ahaessly", "createdAt": "2020-02-03T19:28:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE0NzM5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDE1MzYyMg==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r370153622", "bodyText": "Well, sometimes likelihoods come from Mutect2, right?", "author": "ldgauthier", "createdAt": "2020-01-23T14:36:37Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/allelespecific/StrandBiasUtils.java", "diffHunk": "@@ -0,0 +1,160 @@\n+package org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.broadinstitute.hellbender.engine.filters.VariantFilter;\n+import org.broadinstitute.hellbender.tools.walkers.annotator.AnnotationUtils;\n+import org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods;\n+import org.broadinstitute.hellbender.utils.read.GATKRead;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class StrandBiasUtils {\n+    public static final int FORWARD = 0;\n+    public static final int REVERSE = 1;\n+    public static final int MIN_COUNT = 2;\n+    public static final String PRINT_DELIM = \"|\";\n+    private static final List<Integer> ZERO_LIST = new ArrayList<>(Arrays.asList(0,0));\n+\n+    public static Map<String, Object> computeSBAnnotation(VariantContext vc, AlleleLikelihoods<GATKRead, Allele> likelihoods, String key) {\n+        // calculate the annotation from the likelihoods\n+        // likelihoods can come from HaplotypeCaller call to VariantAnnotatorEngine", "originalCommit": "ecdbbe7f71162e09cfdef50a4a42c22463374519", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzUzOTU4OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r373539589", "bodyText": "Does it really create a comma separated string?  Also, it looks like the only duplicate filter you're checking for is PASS, so I would call the method \"removeExtraPassFilters\" or something less general than what's there now.", "author": "ldgauthier", "createdAt": "2020-01-31T15:32:30Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2FilteringEngine.java", "diffHunk": "@@ -176,28 +178,105 @@ public VariantContext applyFiltersAndAccumulateOutputStats(final VariantContext\n         final ErrorProbabilities errorProbabilities = new ErrorProbabilities(filters, vc, this, referenceContext);\n         filteringOutputStats.recordCall(errorProbabilities, getThreshold() - EPSILON);\n \n-        final boolean variantFailsFilters = errorProbabilities.getErrorProbability() > Math.min(1 - EPSILON, Math.max(EPSILON, getThreshold()));\n-        final double maxErrorProb = errorProbabilities.getProbabilitiesByFilter().values().stream().mapToDouble(p->p).max().orElse(1);\n+        // error probability must exceed threshold, and just in case threshold is bad, probabilities close to 1 must be filtered\n+        // and probabilities close to 0 must not be filtered\n+        double errorThreshold = Math.min(1 - EPSILON, Math.max(EPSILON, getThreshold()));\n+\n+        Map<String, Double> siteFiltersWithErrorProb = new LinkedHashMap<>();\n+\n+        // apply allele specific filters\n+        List<List<String>> alleleStatusByFilter =\n+                errorProbabilities.getProbabilitiesForAlleleFilters().entrySet().stream()\n+                        .filter(entry -> !entry.getValue().isEmpty())\n+                        .map(entry -> addFilterStrings(entry.getValue(), errorThreshold, entry.getKey().filterName())).collect(Collectors.toList());\n+\n+        // for each allele, merge all allele specific filters\n+//      List<Iterator<String>> ASFiltersIterator = ASFilters.stream().map(list -> list.listIterator()).collect(Collectors.toList());\n+        List<List<String>> filtersByAllele = ErrorProbabilities.transpose(alleleStatusByFilter);\n+        List<List<String>> distinctFiltersByAllele = filtersByAllele.stream().map(this::getDistinctFiltersForAllele).collect(Collectors.toList());\n+        ListIterator<String> mergedFilterStringByAllele = distinctFiltersByAllele.stream().map(AnnotationUtils::encodeStringList).collect(Collectors.toList()).listIterator();\n+\n+        List<String> orderedASFilterStrings = vc.getAlleles().stream().map(allele -> allele.isReference() || allele.isSymbolic() ?\n+                VCFConstants.EMPTY_INFO_FIELD : mergedFilterStringByAllele.next()).collect(Collectors.toList());\n+        String finalAttrString = AnnotationUtils.encodeAnyASList(orderedASFilterStrings);\n+\n+        vcb.putAttributes(Collections.singletonMap(GATKVCFConstants.AS_FILTER_STATUS_KEY, finalAttrString));\n \n-        for (final Map.Entry<Mutect2VariantFilter, Double> entry : errorProbabilities.getProbabilitiesByFilter().entrySet()) {\n-            final double errorProbability = entry.getValue();\n \n-            entry.getKey().phredScaledPosteriorAnnotationName().ifPresent(annotation -> {\n-                if (entry.getKey().requiredAnnotations().stream().allMatch(vc::hasAttribute)) {\n-                    vcb.attribute(annotation, QualityUtils.errorProbToQual(errorProbability));\n-                }\n-            });\n+        // compute site-only filters\n+        // from allele specific filters\n+         alleleStatusByFilter.stream().forEachOrdered(alleleStatusForFilter -> {\n+            if (!alleleStatusForFilter.isEmpty() && alleleStatusForFilter.stream().distinct().count() == 1 && !alleleStatusForFilter.contains(VCFConstants.PASSES_FILTERS_v4)) {\n+                siteFiltersWithErrorProb.put(alleleStatusForFilter.get(0), 1.0);\n+            }\n+        });\n \n-            // error probability must exceed threshold, and just in case threshold is bad, probabilities close to 1 must be filtered\n-            // and probabilities close to 0 must not be filtered\n-            if (variantFailsFilters && errorProbability >= Math.min(maxErrorProb, MIN_REPORTABLE_ERROR_PROBABILITY)) {\n-                vcb.filter(entry.getKey().filterName());\n+\n+        // from variant filters\n+        errorProbabilities.getProbabilitiesForVariantFilters().entrySet().stream()\n+                .forEach(entry -> {\n+                    entry.getKey().phredScaledPosteriorAnnotationName().ifPresent(annotation -> {\n+                        if (entry.getKey().requiredAnnotations().stream().allMatch(vc::hasAttribute)) {\n+                            vcb.attribute(annotation, QualityUtils.errorProbToQual(entry.getValue()));\n+                        }\n+                    });\n+                    if (entry.getValue() > errorThreshold) {\n+                        siteFiltersWithErrorProb.put(entry.getKey().filterName(), entry.getValue());\n+                    }\n+\n+                });\n+\n+        // if all alleles have been filtered out, but for different reasons, fail the site.\n+        // if the site is only ref and symbolic, no filters will be applied so don't fail\n+        if (siteFiltersWithErrorProb.isEmpty() && !distinctFiltersByAllele.stream().allMatch(List::isEmpty)) {\n+            // if any allele passed, don't fail the site\n+            if (!distinctFiltersByAllele.stream().flatMap(List::stream).anyMatch(f -> f.equals(VCFConstants.PASSES_FILTERS_v4))) {\n+                // we know the allele level filters exceeded their threshold - so set this prob to 1\n+                siteFiltersWithErrorProb.put(GATKVCFConstants.FAIL, 1.0);\n             }\n         }\n \n+        // this code limits the number of filters specified for any variant to the highest probability filters\n+        // this will not change the status of whether a variant is actually filtered or not\n+        final double maxErrorProb = siteFiltersWithErrorProb.values().stream().mapToDouble(p->p).max().orElse(1);\n+        siteFiltersWithErrorProb.entrySet().stream().forEach(entry -> {\n+            if (entry.getValue() >= Math.min(maxErrorProb, MIN_REPORTABLE_ERROR_PROBABILITY)) {\n+                vcb.filter(entry.getKey());\n+            }\n+        });\n+\n         return vcb.make();\n     }\n \n+    /**\n+     * Creates a comma separated string of all the filters that apply to the allele.", "originalCommit": "65d5e19b62f1d768ae4139cb2d13e50e97ec3b2b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDI3OTIzNA==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r374279234", "bodyText": "thanks - didn't update the doc after a refactor.\nthe call to distinct removes any duplicate filters (of which the only possibility should be PASS). the first if statement removes the PASS if there are filters that did not pass.", "author": "ahaessly", "createdAt": "2020-02-03T18:56:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzUzOTU4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzU0MjIwNw==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r373542207", "bodyText": "This does have required annotations, they're just not info annotations.  How important is this check?", "author": "ldgauthier", "createdAt": "2020-01-31T15:37:46Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/NuMTFilter.java", "diffHunk": "@@ -0,0 +1,50 @@\n+package org.broadinstitute.hellbender.tools.walkers.mutect.filtering;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import org.apache.commons.math3.distribution.PoissonDistribution;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFConstants;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+\n+public class NuMTFilter extends HardAlleleFilter {\n+    private static final double LOWER_BOUND_PROB = .01;\n+    private final int maxAltDepthCutoff;\n+\n+    public NuMTFilter(final double medianAutosomalCoverage, final double maxNuMTCopies){\n+        if (maxNuMTCopies > 0 && medianAutosomalCoverage > 0) {\n+            final PoissonDistribution autosomalCoverage = new PoissonDistribution(medianAutosomalCoverage * maxNuMTCopies / 2.0);\n+            maxAltDepthCutoff = autosomalCoverage.inverseCumulativeProbability(1 - LOWER_BOUND_PROB);\n+        } else {\n+            maxAltDepthCutoff = 0;\n+        }\n+    }\n+\n+    @Override\n+    public ErrorType errorType() { return ErrorType.NON_SOMATIC; }\n+\n+    public List<Integer> getData(Genotype g) {\n+        return Arrays.stream(g.getAD()).boxed().collect(Collectors.toList());\n+    }\n+\n+    @Override\n+    public List<Boolean> areAllelesArtifacts(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n+        LinkedHashMap<Allele, List<Integer>> dataByAllele = getDataByAllele(vc, Genotype::hasAD, this::getData, filteringEngine);\n+        return dataByAllele.entrySet().stream()\n+                .filter(entry -> !vc.getReference().equals(entry.getKey()))\n+                .map(entry -> entry.getValue().stream().max(Integer::compare).orElse(0) < maxAltDepthCutoff).collect(Collectors.toList());\n+    }\n+\n+    @Override\n+    public String filterName() {\n+        return GATKVCFConstants.POSSIBLE_NUMT_FILTER_NAME;\n+    }\n+\n+    @Override\n+    protected List<String> requiredAnnotations() { return Collections.emptyList(); }", "originalCommit": "65d5e19b62f1d768ae4139cb2d13e50e97ec3b2b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDI5NzYxNA==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r374297614", "bodyText": "well the code that uses this sees if the Variant Context has the annotations - so this wouldn't work here because it's on the genotype. If there are no required annotations the check passes and the error probabilities are calculated. I could rename the method to requiredInfoAnnotations or requiredVariantAnnotations?", "author": "ahaessly", "createdAt": "2020-02-03T19:34:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzU0MjIwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDg0ODY4MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r374848681", "bodyText": "If the upstream method has a check for INFO annotations, then I'm happy.", "author": "ldgauthier", "createdAt": "2020-02-04T18:35:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzU0MjIwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTMzMDAwMQ==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r375330001", "bodyText": "ok - renaming the method to requiredInfoAnnotations so it's more explicit", "author": "ahaessly", "createdAt": "2020-02-05T15:35:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzU0MjIwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzU0MzAzMA==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r373543030", "bodyText": "Thanks for the cleanup!", "author": "ldgauthier", "createdAt": "2020-01-31T15:39:20Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/StrictStrandBiasFilter.java", "diffHunk": "@@ -20,25 +21,14 @@ public StrictStrandBiasFilter(final int minReadsOnEachStrand) {\n     public ErrorType errorType() { return ErrorType.ARTIFACT; }\n \n     @Override\n-    public boolean isArtifact(final VariantContext vc, final Mutect2FilteringEngine filteringEngine) {\n-        if (minReadsOnEachStrand == 0) {\n-            return false;\n+    public List<Boolean> areAllelesArtifacts(final VariantContext vc, final Mutect2FilteringEngine filteringEngine, ReferenceContext referenceContext) {\n+        List<List<Integer>> sbs = StrandBiasUtils.getSBsForAlleles(vc);", "originalCommit": "65d5e19b62f1d768ae4139cb2d13e50e97ec3b2b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYxNjQwOA==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r373616408", "bodyText": "I would really much rather have these tests read in the VCs and then check the filters (and check the relevant FILTER line in the header) rather than doing an exact match, because annoying things happen like someone changes the VCF header description of something unrelated and now these tests fail.  If that's going to be a huge pain, then we can live with this.  They're very thorough, I just want to save someone a little potential pain the future.", "author": "ldgauthier", "createdAt": "2020-01-31T18:18:33Z", "path": "src/test/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/MTLowHeteroplasmyFilterToolTest.java", "diffHunk": "@@ -0,0 +1,35 @@\n+package org.broadinstitute.hellbender.tools.walkers.mutect.filtering;\n+\n+import org.broadinstitute.hellbender.CommandLineProgramTest;\n+import org.broadinstitute.hellbender.testutils.IntegrationTestSpec;\n+import org.testng.annotations.Test;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+\n+public class MTLowHeteroplasmyFilterToolTest extends CommandLineProgramTest {\n+    private static final File MITO_REF = new File(toolsTestDir, \"mutect/mito/Homo_sapiens_assembly38.mt_only.fasta\");\n+    private static final File NA12878_MITO_FILTERED_VCF = new File(toolsTestDir, \"mutect/mito/filtered.vcf\");\n+\n+    @Test\n+    public void testLowHetVariantWalker() throws IOException {\n+        final IntegrationTestSpec testSpec = new IntegrationTestSpec(\n+                        \" -R \" + MITO_REF.getAbsolutePath()  +\n+                        \" -V \" + NA12878_MITO_FILTERED_VCF +\n+                        \" -O %s\",\n+                Arrays.asList(toolsTestDir + \"mutect/mito/expected_LowHetVariantWalkerIntegrationTest_output.vcf\")\n+        );\n+        testSpec.executeTest(\"testLowHetVariantWalker\", this);", "originalCommit": "65d5e19b62f1d768ae4139cb2d13e50e97ec3b2b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "f07b8091378afc3e0a260f70c478dfb0c73bdd0e", "url": "https://github.com/broadinstitute/gatk/commit/f07b8091378afc3e0a260f70c478dfb0c73bdd0e", "message": "add final tests", "committedDate": "2020-02-19T22:36:50Z", "type": "forcePushed"}, {"oid": "d1039b477108e3c118afb27199e86812c4a7cf14", "url": "https://github.com/broadinstitute/gatk/commit/d1039b477108e3c118afb27199e86812c4a7cf14", "message": "needed to update another test file", "committedDate": "2020-03-04T14:52:43Z", "type": "forcePushed"}, {"oid": "d3aacd3d17c940fdec24648baf70fd3da58a9fe8", "url": "https://github.com/broadinstitute/gatk/commit/d3aacd3d17c940fdec24648baf70fd3da58a9fe8", "message": "change count type for RPA", "committedDate": "2020-03-05T17:54:47Z", "type": "forcePushed"}, {"oid": "10a7c3d176040c0229e51a583f6b682d33eb45fa", "url": "https://github.com/broadinstitute/gatk/commit/10a7c3d176040c0229e51a583f6b682d33eb45fa", "message": "final mods", "committedDate": "2020-03-23T16:47:18Z", "type": "forcePushed"}, {"oid": "f9133abae6e5afd5bd0a5dfcddd0532aed682fc9", "url": "https://github.com/broadinstitute/gatk/commit/f9133abae6e5afd5bd0a5dfcddd0532aed682fc9", "message": "initial impl", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "49875a679a2fd98af6d7e4dd28a657cbf75e8458", "url": "https://github.com/broadinstitute/gatk/commit/49875a679a2fd98af6d7e4dd28a657cbf75e8458", "message": "start impl\n\nnew refactor adding allele specific filters in mutect2\n\nfix headers\n\nAS filter status working - still need to fix site filter", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "e94059e295619ad79ed214a09f527496cd68d3d9", "url": "https://github.com/broadinstitute/gatk/commit/e94059e295619ad79ed214a09f527496cd68d3d9", "message": "wip", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "22345ad2607e53015a919e88a418375ba9477f17", "url": "https://github.com/broadinstitute/gatk/commit/22345ad2607e53015a919e88a418375ba9477f17", "message": "converted base qual and tumor", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "32d4ed25a7dc10690da23694125a90bd41381ef9", "url": "https://github.com/broadinstitute/gatk/commit/32d4ed25a7dc10690da23694125a90bd41381ef9", "message": "converted mapping qual filter", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "1d64edf75c0c927227b345e0f6eeaa78f0cc845d", "url": "https://github.com/broadinstitute/gatk/commit/1d64edf75c0c927227b345e0f6eeaa78f0cc845d", "message": "updated duplicate alt read filter - but need to add tests", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "8b85de2140cc2d4ea9731e84d5781138a99510be", "url": "https://github.com/broadinstitute/gatk/commit/8b85de2140cc2d4ea9731e84d5781138a99510be", "message": "undo bad delim change", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "27b49be53a6cf5c40e11d999498a6aa1d14899fb", "url": "https://github.com/broadinstitute/gatk/commit/27b49be53a6cf5c40e11d999498a6aa1d14899fb", "message": "update min allele fraction filter", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "1568a0e5ba355631ee213d2467754c50aa5a257e", "url": "https://github.com/broadinstitute/gatk/commit/1568a0e5ba355631ee213d2467754c50aa5a257e", "message": "update read pos filter", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "2b045c0a5f5eec8b98360e3461bcf5b2d82d701f", "url": "https://github.com/broadinstitute/gatk/commit/2b045c0a5f5eec8b98360e3461bcf5b2d82d701f", "message": "2 different get data methods", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "409aff292a2daf802920089d4d5559f0da8ee637", "url": "https://github.com/broadinstitute/gatk/commit/409aff292a2daf802920089d4d5559f0da8ee637", "message": "use correct filter list", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "e54983c21a9c950cc0adacbc36d130ecacf087b7", "url": "https://github.com/broadinstitute/gatk/commit/e54983c21a9c950cc0adacbc36d130ecacf087b7", "message": "fix issues with filters containing data for ref", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "9f7891786067a0deb47d882bd6c7cce1991ab964", "url": "https://github.com/broadinstitute/gatk/commit/9f7891786067a0deb47d882bd6c7cce1991ab964", "message": "wip, doens't pass tests, fixing error prob for threshold", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "8e0e7d4cfcbd97d4e5095140ef84d99f8a536370", "url": "https://github.com/broadinstitute/gatk/commit/8e0e7d4cfcbd97d4e5095140ef84d99f8a536370", "message": "implement 2 pass variant walker as post processing filter step for low heteroplasmy", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "660c4f9e0c3b174add645baade6d89f49214d9f1", "url": "https://github.com/broadinstitute/gatk/commit/660c4f9e0c3b174add645baade6d89f49214d9f1", "message": "wip - got error prob to compile", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "eb2a590720f9ab088adac321ac3d7d43f83c7491", "url": "https://github.com/broadinstitute/gatk/commit/eb2a590720f9ab088adac321ac3d7d43f83c7491", "message": "wip - fixed almost all compile errors", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "9938a541588cb95d217369d4d34979690d4c5f72", "url": "https://github.com/broadinstitute/gatk/commit/9938a541588cb95d217369d4d34979690d4c5f72", "message": "wip almost done - one more q for DB", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "63c414706263af4740369dac14af815499f571bd", "url": "https://github.com/broadinstitute/gatk/commit/63c414706263af4740369dac14af815499f571bd", "message": "fixed some bugs", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "e93445385c29693b8440aa536c2dfbbd171d99ad", "url": "https://github.com/broadinstitute/gatk/commit/e93445385c29693b8440aa536c2dfbbd171d99ad", "message": "mito filter tests pass", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "5d756078beb068bfbae3b580b85892a15ad377ff", "url": "https://github.com/broadinstitute/gatk/commit/5d756078beb068bfbae3b580b85892a15ad377ff", "message": "add AS_SB_Table as mutect2 annotation", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "1e3595220073cb91bc8397027b6c3643793c7807", "url": "https://github.com/broadinstitute/gatk/commit/1e3595220073cb91bc8397027b6c3643793c7807", "message": "made strict strand bias allele specific", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "f347db5796c589d4e7d834fe2c1401f794016555", "url": "https://github.com/broadinstitute/gatk/commit/f347db5796c589d4e7d834fe2c1401f794016555", "message": "add test data for AS_SB_TABLE in vcf. format not compatible with annotation in gvcf", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "c7efe6e8b59b998582c7a5fbb11c55fe0d7cb920", "url": "https://github.com/broadinstitute/gatk/commit/c7efe6e8b59b998582c7a5fbb11c55fe0d7cb920", "message": "convert stand artifact - bug with prior list", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "4344277d6ec1a6e2a5e87b226ee976397d88da21", "url": "https://github.com/broadinstitute/gatk/commit/4344277d6ec1a6e2a5e87b226ee976397d88da21", "message": "fixed strand artifact - fix and verify tests", "committedDate": "2020-03-25T19:18:25Z", "type": "commit"}, {"oid": "f11171f505dd5052ed3e91cd029f2791a8835ef4", "url": "https://github.com/broadinstitute/gatk/commit/f11171f505dd5052ed3e91cd029f2791a8835ef4", "message": "fix bug in strand artifact. still need tests", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "72d87ab804cbb2242bec1b6116b3751630ae0d77", "url": "https://github.com/broadinstitute/gatk/commit/72d87ab804cbb2242bec1b6116b3751630ae0d77", "message": "strand artifact - use total from all alleles", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "6bf1ecfbd4148c22837d28e968cffad1e9d98583", "url": "https://github.com/broadinstitute/gatk/commit/6bf1ecfbd4148c22837d28e968cffad1e9d98583", "message": "put AS_SB_TABLE back into gvcf format", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "781064acd987da270153f9ad205ad087e408821c", "url": "https://github.com/broadinstitute/gatk/commit/781064acd987da270153f9ad205ad087e408821c", "message": "fix bug in strict strand filter that was ignoring the minReadsOnEachStrand parameter", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "6f19cc3f3db70e39b3455a0df1cc42393aa243fc", "url": "https://github.com/broadinstitute/gatk/commit/6f19cc3f3db70e39b3455a0df1cc42393aa243fc", "message": "minor changes", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "d9318c08fc0acc63ff8774dac65f30e2bb0e4952", "url": "https://github.com/broadinstitute/gatk/commit/d9318c08fc0acc63ff8774dac65f30e2bb0e4952", "message": "remove generic from Mutect2AlleleFilter and make generic methods static. convert chimera filter to allele specific", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "ec8f63cc274899e5d1322000dfd19cbb4ecf6109", "url": "https://github.com/broadinstitute/gatk/commit/ec8f63cc274899e5d1322000dfd19cbb4ecf6109", "message": "minor changes and comments", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "b5e207a8a30fd7f78c31c3e06be9a9c4a6f01729", "url": "https://github.com/broadinstitute/gatk/commit/b5e207a8a30fd7f78c31c3e06be9a9c4a6f01729", "message": "update from PR feedback", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "03754fc224e39cbfa91afd37e7bf3c3209703a64", "url": "https://github.com/broadinstitute/gatk/commit/03754fc224e39cbfa91afd37e7bf3c3209703a64", "message": "changes after rebase and to test FAIL filter", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "98a9dc86e3686a3f000f62dc5d51eb5f8f54e13e", "url": "https://github.com/broadinstitute/gatk/commit/98a9dc86e3686a3f000f62dc5d51eb5f8f54e13e", "message": "fix issue with null value for SB annotation", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "c8bdeeeabcdea650ab7c1b8d894129282456c464", "url": "https://github.com/broadinstitute/gatk/commit/c8bdeeeabcdea650ab7c1b8d894129282456c464", "message": "fix output files for test", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "fb956fbb0fa11725c7604f9d04286697c6557fc5", "url": "https://github.com/broadinstitute/gatk/commit/fb956fbb0fa11725c7604f9d04286697c6557fc5", "message": "remove warnings", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "5de3575669fbc1719a38d5b14d6200a9250bbb34", "url": "https://github.com/broadinstitute/gatk/commit/5de3575669fbc1719a38d5b14d6200a9250bbb34", "message": "update splitting alleles to include analyzing AS_FilterStatus and setting the correct filter fields", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "2cb9b5c09307e5783d18ff713148e7bf662672a4", "url": "https://github.com/broadinstitute/gatk/commit/2cb9b5c09307e5783d18ff713148e7bf662672a4", "message": "fix extra spaces in filter list", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "53d31a05ae43f99ecdbf39b43fee82132f7f79c9", "url": "https://github.com/broadinstitute/gatk/commit/53d31a05ae43f99ecdbf39b43fee82132f7f79c9", "message": "Fixed AF and SB splitting; also some javadoc I should have done in the\npast and some refactoring I should have done in the past", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "9446699d1197564c4205e6f42f54d8895118198e", "url": "https://github.com/broadinstitute/gatk/commit/9446699d1197564c4205e6f42f54d8895118198e", "message": "fix warning, fixes from PR feedback", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "d6f90067ef110a057978bdee619b5bcfa7eca379", "url": "https://github.com/broadinstitute/gatk/commit/d6f90067ef110a057978bdee619b5bcfa7eca379", "message": "fix another generics issue", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "ffecce735fd7bf927a256bb363c2f7d5e15202c7", "url": "https://github.com/broadinstitute/gatk/commit/ffecce735fd7bf927a256bb363c2f7d5e15202c7", "message": "make unique alt read count allele specific", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "4fc762bdc4d473e1211e5f4848dfd24d15776d6b", "url": "https://github.com/broadinstitute/gatk/commit/4fc762bdc4d473e1211e5f4848dfd24d15776d6b", "message": "fix genotypes not included in vcf", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "0deea4c4bdda3faeadc2c474eea9b3a7d5c69910", "url": "https://github.com/broadinstitute/gatk/commit/0deea4c4bdda3faeadc2c474eea9b3a7d5c69910", "message": "better test for low het filter", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "03aefbfff5251a5b00d5b09215f5a558b042785a", "url": "https://github.com/broadinstitute/gatk/commit/03aefbfff5251a5b00d5b09215f5a558b042785a", "message": "changed getRequiredAnnotations to getRequiredInfoAnnotations to be more explicit", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "851a90c3a7a4ce769feebee42adf9197c38958cc", "url": "https://github.com/broadinstitute/gatk/commit/851a90c3a7a4ce769feebee42adf9197c38958cc", "message": "fix low het filter to ignore ref AF", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "9e59a3fe972d01c837c9788c95ad3b0c8320eed9", "url": "https://github.com/broadinstitute/gatk/commit/9e59a3fe972d01c837c9788c95ad3b0c8320eed9", "message": "remove . in AS_FilterStatus for ref, and change PASS to .", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "5459d4f8d2236245e627ddd4bd897c40254467be", "url": "https://github.com/broadinstitute/gatk/commit/5459d4f8d2236245e627ddd4bd897c40254467be", "message": "fix bug in removing symbolic data", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "30b59dcc25b202e6ab4b1a352d0994ac9058ac36", "url": "https://github.com/broadinstitute/gatk/commit/30b59dcc25b202e6ab4b1a352d0994ac9058ac36", "message": "add test for uniq alt read count", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "e702456f0b219c2ab8c551172177cc91813907f7", "url": "https://github.com/broadinstitute/gatk/commit/e702456f0b219c2ab8c551172177cc91813907f7", "message": "add final tests", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "ff2051ece7f3e4998672625a933f8bc030ae9b91", "url": "https://github.com/broadinstitute/gatk/commit/ff2051ece7f3e4998672625a933f8bc030ae9b91", "message": "fix error in LeftAlignAndTrim after rebase", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "fc585dab814eb16b42568299e6607f8c4c174277", "url": "https://github.com/broadinstitute/gatk/commit/fc585dab814eb16b42568299e6607f8c4c174277", "message": "fix as splitting in left align and trim...", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "343cd5f3929de9dbcf44c4cee531ae94cdc85ba2", "url": "https://github.com/broadinstitute/gatk/commit/343cd5f3929de9dbcf44c4cee531ae94cdc85ba2", "message": "updated test for split multi allelics", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "4697b02c148ff79da35fbbfb5b5ba9761c7317ab", "url": "https://github.com/broadinstitute/gatk/commit/4697b02c148ff79da35fbbfb5b5ba9761c7317ab", "message": "needed to update another test file", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "94fa473fb9fb7aecf7e59a649dfc8e7f09d9b8dc", "url": "https://github.com/broadinstitute/gatk/commit/94fa473fb9fb7aecf7e59a649dfc8e7f09d9b8dc", "message": "fix split multiallelics to work for all info fields", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "8091e0d013d6c3b5c78bd47d8736d8168a17d326", "url": "https://github.com/broadinstitute/gatk/commit/8091e0d013d6c3b5c78bd47d8736d8168a17d326", "message": "change count type for RPA", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "30396adf4fa7175d5a5681ac49edfc7723a32fe3", "url": "https://github.com/broadinstitute/gatk/commit/30396adf4fa7175d5a5681ac49edfc7723a32fe3", "message": "make NuMTFilter its own tool. update constants for MT low het tool", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "a3ed2d3df39a0ea4c962f6b454a5eda6b1997c97", "url": "https://github.com/broadinstitute/gatk/commit/a3ed2d3df39a0ea4c962f6b454a5eda6b1997c97", "message": "change info field count type back to 1, since format is non-standard", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "39d04a7ee10206d713963b09933a3d13237e575e", "url": "https://github.com/broadinstitute/gatk/commit/39d04a7ee10206d713963b09933a3d13237e575e", "message": "fix test", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "8205932da124b265baea203848e53ba8b78565d5", "url": "https://github.com/broadinstitute/gatk/commit/8205932da124b265baea203848e53ba8b78565d5", "message": "change allele specific filter status of . to SITE", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "54bd5db4e986a07e3e292ec2a8458d033daab1f8", "url": "https://github.com/broadinstitute/gatk/commit/54bd5db4e986a07e3e292ec2a8458d033daab1f8", "message": "more output for failing test", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "6b496b85b59e14ba98a6736509e7d996e4a1d739", "url": "https://github.com/broadinstitute/gatk/commit/6b496b85b59e14ba98a6736509e7d996e4a1d739", "message": "make MT filter low het tool allele specific", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "711ae07099bbaa422c0cbc9a2b3f838ba43dfcc4", "url": "https://github.com/broadinstitute/gatk/commit/711ae07099bbaa422c0cbc9a2b3f838ba43dfcc4", "message": "update the passing indicator to SITE", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "2068a7832f734b37011770590b05f10cad52cd92", "url": "https://github.com/broadinstitute/gatk/commit/2068a7832f734b37011770590b05f10cad52cd92", "message": "use already computed AF instead of recomputing in MT Low het tool", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "3ae2795ab7124373023bf3a3391062f03831c7f3", "url": "https://github.com/broadinstitute/gatk/commit/3ae2795ab7124373023bf3a3391062f03831c7f3", "message": "get tests working", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "a9ab22c164e3231fc73704b933726833c93d08c4", "url": "https://github.com/broadinstitute/gatk/commit/a9ab22c164e3231fc73704b933726833c93d08c4", "message": "final mods", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "3076777e9e365eb3ad65026f217d569ac8647ab4", "url": "https://github.com/broadinstitute/gatk/commit/3076777e9e365eb3ad65026f217d569ac8647ab4", "message": "minor change", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "10a4ba7ead50036126e9a0b9bf75555ff09ac0a1", "url": "https://github.com/broadinstitute/gatk/commit/10a4ba7ead50036126e9a0b9bf75555ff09ac0a1", "message": "fix expected files", "committedDate": "2020-03-25T19:18:26Z", "type": "commit"}, {"oid": "10a4ba7ead50036126e9a0b9bf75555ff09ac0a1", "url": "https://github.com/broadinstitute/gatk/commit/10a4ba7ead50036126e9a0b9bf75555ff09ac0a1", "message": "fix expected files", "committedDate": "2020-03-25T19:18:26Z", "type": "forcePushed"}, {"oid": "74fff72231f9e1094cd79010e9547da85d1db76e", "url": "https://github.com/broadinstitute/gatk/commit/74fff72231f9e1094cd79010e9547da85d1db76e", "message": "update to lastest wdl", "committedDate": "2020-03-25T19:47:46Z", "type": "commit"}, {"oid": "272f6b892f61456ca8cc83cd51820732eb6c0611", "url": "https://github.com/broadinstitute/gatk/commit/272f6b892f61456ca8cc83cd51820732eb6c0611", "message": "cleanup", "committedDate": "2020-03-30T19:16:57Z", "type": "commit"}, {"oid": "35ab2849b737c1353a53f1dfb11928359832b8bb", "url": "https://github.com/broadinstitute/gatk/commit/35ab2849b737c1353a53f1dfb11928359832b8bb", "message": "updating wdl", "committedDate": "2020-03-31T14:45:57Z", "type": "commit"}, {"oid": "0dfab1da9c821ca70dc6b5c1f17a58d131c234c9", "url": "https://github.com/broadinstitute/gatk/commit/0dfab1da9c821ca70dc6b5c1f17a58d131c234c9", "message": "fix bug that removes existing filters", "committedDate": "2020-04-01T17:58:47Z", "type": "commit"}, {"oid": "9b8f2118dca98f17e09c856b395b7703b14a9e33", "url": "https://github.com/broadinstitute/gatk/commit/9b8f2118dca98f17e09c856b395b7703b14a9e33", "message": "add one more test", "committedDate": "2020-04-01T18:10:23Z", "type": "commit"}, {"oid": "3b43c103c4366214fd4c2a2125f985485e75f82d", "url": "https://github.com/broadinstitute/gatk/commit/3b43c103c4366214fd4c2a2125f985485e75f82d", "message": "fix overwrite of filter", "committedDate": "2020-04-01T21:19:30Z", "type": "commit"}, {"oid": "8191f3c3f24f6148459295e5597796f26b411f07", "url": "https://github.com/broadinstitute/gatk/commit/8191f3c3f24f6148459295e5597796f26b411f07", "message": "add missing test files", "committedDate": "2020-04-02T18:14:45Z", "type": "commit"}, {"oid": "54b7fc11d85d1500317278dcb77e844633a4f65a", "url": "https://github.com/broadinstitute/gatk/commit/54b7fc11d85d1500317278dcb77e844633a4f65a", "message": "should fix wdl tests", "committedDate": "2020-04-03T16:48:45Z", "type": "commit"}, {"oid": "37c8c81e6284f6ccbf3495892ac4be47d712d8d3", "url": "https://github.com/broadinstitute/gatk/commit/37c8c81e6284f6ccbf3495892ac4be47d712d8d3", "message": "fix wdl tests", "committedDate": "2020-04-03T18:26:55Z", "type": "commit"}, {"oid": "4dcd0885944bbc7528ab7b9703091b5370afe76a", "url": "https://github.com/broadinstitute/gatk/commit/4dcd0885944bbc7528ab7b9703091b5370afe76a", "message": "fix path", "committedDate": "2020-04-03T20:11:33Z", "type": "commit"}, {"oid": "d6b12294d6784347df60894c3112f9c64b03d1ed", "url": "https://github.com/broadinstitute/gatk/commit/d6b12294d6784347df60894c3112f9c64b03d1ed", "message": "fix path again", "committedDate": "2020-04-06T17:37:36Z", "type": "commit"}, {"oid": "e60ed86feb7657a54b65db5fd528fa9f0f725e10", "url": "https://github.com/broadinstitute/gatk/commit/e60ed86feb7657a54b65db5fd528fa9f0f725e10", "message": "remove blacklisted shifted", "committedDate": "2020-04-06T20:25:55Z", "type": "commit"}, {"oid": "717d87fab4fb938aa44737019f62a4ae7fed1172", "url": "https://github.com/broadinstitute/gatk/commit/717d87fab4fb938aa44737019f62a4ae7fed1172", "message": "add error checking for contam file format", "committedDate": "2020-04-07T19:33:00Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzE1MjQ2Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r407152466", "bodyText": "Can you go directly compute symbolic indices via new IndexRange(0, vc.getNAlleles()).filter(n -> vc.getAllele(n).isSymbolic());?", "author": "davidbenjamin", "createdAt": "2020-04-12T06:16:12Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/clustering/SomaticClusteringModel.java", "diffHunk": "@@ -88,19 +89,33 @@ public SomaticClusteringModel(final M2FiltersArgumentCollection MTFAC, final Lis\n         logClusterWeights = new double[] {Math.log1p(INITIAL_HIGH_AF_WEIGHT), Math.log(INITIAL_HIGH_AF_WEIGHT)};\n     }\n \n-    public void record(final int[] tumorADs, final double[] tumorLogOdds, final double artifactProbability, final double nonSomaticProbability, final VariantContext vc) {\n-        // things that are definitely not somatic don't need to go in the somatic clustering model\n-        if (artifactProbability > OBVIOUS_ARTIFACT_PROBABILITY_THRESHOLD) {\n-            obviousArtifactCount.increment();\n-            return;\n-        } else if (nonSomaticProbability > OBVIOUS_ARTIFACT_PROBABILITY_THRESHOLD) {\n-            return;\n-        }\n-\n+    /**\n+     * Adds data to the model for every alternate allele\n+     * @param tumorADs for all alleles, summed over samples\n+     * @param tumorLogOdds for alt alleles only\n+     * @param artifactProbabilities by alt allele, specifically technical artifact probabilities not including sequencing error, contamination, or germline variation\n+     * @param nonSomaticProbabilities by alt allele, probabilities that the variants are real but not somatic ie germline or contamination\n+     * @param vc the variant context the data apply to\n+     */\n+    public void record(int[] tumorADs, final double[] tumorLogOdds, final List<Double> artifactProbabilities, final List<Double> nonSomaticProbabilities, final VariantContext vc) {\n+        // set tumorAD to 0 for symbolic alleles so it won't contribute to overall AD\n+        List<Allele> symbolicAlleles = vc.getAlternateAlleles().stream().filter(allele -> allele.isSymbolic()).collect(Collectors.toList());\n+        // convert allele index to alt allele index\n+        List<Integer> symIndexes = vc.getAlleleIndices(symbolicAlleles).stream().map(i -> i-1).collect(Collectors.toList());", "originalCommit": "717d87fab4fb938aa44737019f62a4ae7fed1172", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "69cca57628a66e36324ca4062b40c98c80e5a728", "url": "https://github.com/broadinstitute/gatk/commit/69cca57628a66e36324ca4062b40c98c80e5a728", "message": "minor refactor", "committedDate": "2020-04-14T14:47:27Z", "type": "commit"}, {"oid": "f51ba1037f969405a808501f567c22c0b8957b9c", "url": "https://github.com/broadinstitute/gatk/commit/f51ba1037f969405a808501f567c22c0b8957b9c", "message": "update example inputs to use broad public bucket for refs", "committedDate": "2020-04-15T13:26:06Z", "type": "commit"}, {"oid": "59f3dc9a2fc244fe4f0e7e64ee69055d79bc0018", "url": "https://github.com/broadinstitute/gatk/commit/59f3dc9a2fc244fe4f0e7e64ee69055d79bc0018", "message": "doc updates", "committedDate": "2020-04-16T19:52:33Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjM5NTE5OA==", "url": "https://github.com/broadinstitute/gatk/pull/6399#discussion_r412395198", "bodyText": "\"a list with one element for each allele\"?", "author": "ldgauthier", "createdAt": "2020-04-21T18:31:53Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/filtering/Mutect2AlleleFilter.java", "diffHunk": "@@ -73,7 +73,7 @@\n \n \n     /**\n-     *\n+     * Returns a list for each alternate allele which is the probability that the allele should be filtered out.", "originalCommit": "59f3dc9a2fc244fe4f0e7e64ee69055d79bc0018", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}