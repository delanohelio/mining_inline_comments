{"pr_number": 6425, "pr_title": "Handle zero-weight Gaussians correctly in VariantRecalibrator", "pr_createdAt": "2020-01-29T21:22:07Z", "pr_url": "https://github.com/broadinstitute/gatk/pull/6425", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjc2Mjg4Nw==", "url": "https://github.com/broadinstitute/gatk/pull/6425#discussion_r372762887", "bodyText": "It's not clear to me why this special case is needed.  If it's negative infinity, then adding the normalDistributionLog10 to it yields negative infinity.  You avoid calculating it, of course, but presumably this is an edge case and the optimization doesn't matter.\nMy other guess is that sigma or mu could be NaNs somehow?  If so, why is this connected to zero weight?", "author": "davidbenjamin", "createdAt": "2020-01-30T05:15:36Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/vqsr/GaussianMixtureModel.java", "diffHunk": "@@ -212,7 +212,11 @@ public Double evaluateDatumInOneDimension( final VariantDatum datum, final int i\n         final double[] pVarInGaussianLog10 = new double[gaussians.size()];\n         int gaussianIndex = 0;\n         for( final MultivariateGaussian gaussian : gaussians ) {\n-            pVarInGaussianLog10[gaussianIndex++] = gaussian.pMixtureLog10 + MathUtils.normalDistributionLog10(gaussian.mu[iii], gaussian.sigma.get(iii, iii), datum.annotations[iii]);\n+            pVarInGaussianLog10[gaussianIndex] = gaussian.pMixtureLog10;\n+            if (gaussian.pMixtureLog10 != Double.NEGATIVE_INFINITY) {\n+                pVarInGaussianLog10[gaussianIndex] += MathUtils.normalDistributionLog10(gaussian.mu[iii], gaussian.sigma.get(iii, iii), datum.annotations[iii]);", "originalCommit": "389d956feb94d9bbcd82e8cf0d2b46667bdf8db7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjk3MTU3OA==", "url": "https://github.com/broadinstitute/gatk/pull/6425#discussion_r372971578", "bodyText": "Yeah, sigma and mu are in fact NaNs.  I'm not entirely sure where that happens in the code, but I can look.", "author": "ldgauthier", "createdAt": "2020-01-30T14:14:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjc2Mjg4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzA0Mjc3NA==", "url": "https://github.com/broadinstitute/gatk/pull/6425#discussion_r373042774", "bodyText": "I think this typically happens when the effective number (i.e., the sum of responsibilities) for the component goes to zero; if so, you can just add an epsilon to the denominators in the updates for the means and covariances to avoid this.  Perhaps see my notebook from an ancient MIA primer for some pointers.\nAny plans to update this implementation?  See a few of my objections in #2062.", "author": "samuelklee", "createdAt": "2020-01-30T16:09:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjc2Mjg4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzA0OTM3Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6425#discussion_r373049373", "bodyText": "I think I understand what's going on (that bus number is on the move now).  When a Gaussian has no data assigned the M step MultivariateGaussian.maximizeGaussian ends up with zero matrices for pVarSigma and wishart, and thus the resulting sigma equals the empiricalSigma, which is just the whole-data (unclustered) covariance matrix.\nSince, one of the annotations is constant and has zero variance, this empiricalSigma is degenerate.  Since the clusters with data assigned have non-zero wishart and pVarSigma, only the empty cluster has a problem.\nI'm pretty sure that this is wrong, because variational Bayes should regularize the tendency of an empty cluster to take on a degenerate value -- that's the role of the prior.  We could fix it by reconciling the code with Chapter 10 of Bishop (at the cost of several days of dragon-slaying), or we could just re-randomly initialize sigma for an empty cluster, but I would like to the fix the problem at its source so that we avoid creating a pathological Gaussian int he first place.", "author": "davidbenjamin", "createdAt": "2020-01-30T16:19:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjc2Mjg4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzA1OTg1OA==", "url": "https://github.com/broadinstitute/gatk/pull/6425#discussion_r373059858", "bodyText": "I understand the basic idea of the Bishop approach of dealing with the singularity (mentioned at the end of 10.2.1), but I don't have any confidence in being able to work out the new closed-form solution.  Maybe I can get some help offline?", "author": "ldgauthier", "createdAt": "2020-01-30T16:37:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjc2Mjg4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzA2MTA5Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6425#discussion_r373061096", "bodyText": "@samuelklee would you prefer a refactored, generalized Java implementation over, for example, a scikit-learn implementation?  You noted your desires at a time when we didn't have any capacity for integrating Python libraries in GATK.", "author": "ldgauthier", "createdAt": "2020-01-30T16:39:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjc2Mjg4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzA2ODI4OA==", "url": "https://github.com/broadinstitute/gatk/pull/6425#discussion_r373068288", "bodyText": "Definitely think it would be worthwhile to ultimately extract a Java method/class that calls out to sklearn or some other well tested implementation.  If done right, this could allow easy comparison against other methods available there.  Downsides are of course introducing the python dependency, etc., which might not be something we want to do for a more venerable tool like VQSR.  An intermediate option would be to see if there's a good implementation available in a Java library now.\nIn any case, I think refactoring to separate out the Bayesian GMM implementation (which I believe I concluded was just standard Bishop, albeit obfuscated by different naming conventions, when I originally wrote those comments---could be wrong, as bus numbers are not only affected by paternity leave but also by time!) from the VQSR-specific machinery is definitely worthwhile.  Not sure how this stacks against other priorities, though.  And I hope I can claim the right to rescind my original offer to be the one to do the work---would be happy to advise, though!", "author": "samuelklee", "createdAt": "2020-01-30T16:51:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjc2Mjg4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzA3ODk3MA==", "url": "https://github.com/broadinstitute/gatk/pull/6425#discussion_r373078970", "bodyText": "I have a number of reasons that I agree it's just standard Bishop.  To fully generalize, I think all that has to happen is to uncouple the actual data from the rest of the VariantDatum object, which I can probably do in a not unreasonable amount of time.  I think I modified that class in the past.  I might also be able to fully uncouple the model training from the inference, which I did in a crufty way in the past.  And improve on the memory requirements...", "author": "ldgauthier", "createdAt": "2020-01-30T17:10:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjc2Mjg4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzE1ODUxMg==", "url": "https://github.com/broadinstitute/gatk/pull/6425#discussion_r373158512", "bodyText": "I'm not yet convinced that our implementation is faithful to the variational Bayes GMM.  Bishop 10.6 factorizes down to a joint distribution on mu and sigma, but it doesn't split any further, for one thing.  And even if it did, our GMM has a point estimate of sigma for each cluster, which is not the same as a VB M step that computes a posterior.  Finally, I couldn't find anything in Bishop that resembles our empiricalSigma, which is the covariance of all the data.\nNote that one might worry that Bishop's joint posterior on mu and sigma -- a Gauss-Wishart distribution -- would be intractable for VQSR because the predictive densities could not be computed, but this is not the case.  As Bishop shows, this is just a Student's t.\nThis is not to say that I think we should go after this dragon, but I think it's worth settling whether our implementation is actually a true VB GMM.", "author": "davidbenjamin", "createdAt": "2020-01-30T19:49:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjc2Mjg4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzE3NzMzMg==", "url": "https://github.com/broadinstitute/gatk/pull/6425#discussion_r373177332", "bodyText": "Don't empiricalMu and empiricalSigma determine the respective priors?  From a glance, it looks like they are initialized to the origin and some magic number times the identity matrix, respectively, and then never updated.  You might be right about the implementation not being exactly faithful, but I think I concluded that it probably attempted to be.\nI think that the difficulty of maintaining or improving this code arises because 1) the intended implementation is not clearly documented and 2) the code is not documented or easily parsed as the corresponding mathematical expressions.  This is further compounded by the need for the relatively trivial extraction and refactoring mentioned previously.", "author": "samuelklee", "createdAt": "2020-01-30T20:29:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjc2Mjg4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzE5Mjg2OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6425#discussion_r373192869", "bodyText": "The point estimate you mention (i.e., the one computed by evaluateFinalModelParameters) seems to be somewhat arbitrarily chosen, but I think that the updates for the variational parameters calculated in maximizeGaussian do seem to follow 10.58 and 10.60-63.", "author": "samuelklee", "createdAt": "2020-01-30T21:05:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjc2Mjg4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODk0MTg4MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6425#discussion_r388941881", "bodyText": "Just noting for future reference that I went digging into the original GATK3 code (the commit history over there seems corrupted...not sure if something went wrong at some point, perhaps during a public/protected split?).  It looks like empiricalMu and empiricalSigma were indeed originally initialized to the mean and covariance of the data (although the code for the latter was commented out), although I'm not sure what the justification for this was.  In broadgsa/gatk@3224bbe these were changed to represent the quantities for the Bishop prior, but the variable names were not changed.", "author": "samuelklee", "createdAt": "2020-03-06T14:44:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjc2Mjg4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjc2NzM3NA==", "url": "https://github.com/broadinstitute/gatk/pull/6425#discussion_r372767374", "bodyText": "Is there anything in the data or arguments from which we expect a priori that these errors will occur, and at the given indices?", "author": "davidbenjamin", "createdAt": "2020-01-30T05:40:08Z", "path": "src/test/java/org/broadinstitute/hellbender/tools/walkers/vqsr/VariantRecalibratorIntegrationTest.java", "diffHunk": "@@ -406,6 +406,8 @@ public void testVariantRecalibratorSNPscattered(final String[] params) throws IO\n         doSNPTest(params, getLargeVQSRTestDataDir() + \"/snpTranches.scattered.txt\", getLargeVQSRTestDataDir() + \"snpRecal.vcf\"); //tranches file isn't in the expected/ directory because it's input to GatherTranchesIntegrationTest\n     }\n \n+    //One of the 8 positive Gaussians (index 4) has weight zero, which is fine, but the one at index 2 has a covariance", "originalCommit": "389d956feb94d9bbcd82e8cf0d2b46667bdf8db7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjk3MzI1Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6425#discussion_r372973252", "bodyText": "In the input data AS_MQrankSum is constant, which I note where the argument string is defined, but I could make that more clear.  Specifying the index of the Gaussian probably isn't informative since anything that touches the random number generator has the potential to change that.", "author": "ldgauthier", "createdAt": "2020-01-30T14:17:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjc2NzM3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzAyNzc5NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6425#discussion_r373027795", "bodyText": "In that case a comment here that the constantness (constancy?) of AS_MQrankSum causes one Gaussian to have a degenerate covariance would be helpful.  I would either not even mention that it's the one with index 2 or be explicit that this is arbitrary (likewise for the zero-weight Gaussian at index 4).", "author": "davidbenjamin", "createdAt": "2020-01-30T15:45:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjc2NzM3NA=="}], "type": "inlineReview"}, {"oid": "f54aafd1b19ddd1d22462f621c371d9607fb06fb", "url": "https://github.com/broadinstitute/gatk/commit/f54aafd1b19ddd1d22462f621c371d9607fb06fb", "message": "We expect the variational Bayes to sometimes reduce the contribution of\none or more Gaussians to zero (hence *MAX* Gaussians), but this causes\nproblems in log10 space", "committedDate": "2020-02-04T19:55:34Z", "type": "commit"}, {"oid": "f54aafd1b19ddd1d22462f621c371d9607fb06fb", "url": "https://github.com/broadinstitute/gatk/commit/f54aafd1b19ddd1d22462f621c371d9607fb06fb", "message": "We expect the variational Bayes to sometimes reduce the contribution of\none or more Gaussians to zero (hence *MAX* Gaussians), but this causes\nproblems in log10 space", "committedDate": "2020-02-04T19:55:34Z", "type": "forcePushed"}]}