{"pr_number": 6554, "pr_title": "(all of) gCNV exome joint calling", "pr_createdAt": "2020-04-17T20:54:42Z", "pr_url": "https://github.com/broadinstitute/gatk/pull/6554", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2NjYzMw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410466633", "bodyText": "This doesn't go so well with just two events that have the minimal overlap.", "author": "ldgauthier", "createdAt": "2020-04-17T20:58:44Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations", "originalCommit": "34b71a1aba60653bf73cebae2bd0cf49b728a701", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMzNTE4NA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422335184", "bodyText": "Fair point, although I think there are downsides to any simple rule. The solution may be to incorporate evidence to make a more intelligent decision. For depth-only calls where the breakpoints are imprecise, it seems reasonable to use a mean value. Other options would be min start / max end or max start / min end. It would be great if all these could be chosen by the user with another commandline arg.", "author": "mwalker174", "createdAt": "2020-05-08T19:39:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2NjYzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDgzMTYzMg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r434831632", "bodyText": "Done.", "author": "ldgauthier", "createdAt": "2020-06-03T20:23:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2NjYzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2Njg1NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410466855", "bodyText": "I'm not sure what role the evidence plays in the WGS pipeline.", "author": "ldgauthier", "createdAt": "2020-04-17T20:59:10Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations\n+        final int medianStart = startPositions.get(startPositions.size() / 2);\n+        final int medianEnd = endPositions.get(endPositions.size() / 2);\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = exampleCall.getContig().equals(exampleCall.getEndContig()) && !exampleCall.getType().equals(StructuralVariantType.INS) ? medianEnd - medianStart + 1: exampleCall.getLength();\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList());\n+        final List<Genotype> clusterSamples = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+\n+        final int newStart;\n+        final int newEnd;\n+        if (exampleCall.getType().equals(StructuralVariantType.INS)) {\n+            // Insertions should be a single locus; also fixes case where end-supporting split reads are to the\n+            // left of start-supporting split reads\n+            final int mean = (medianStart + medianEnd) / 2;\n+            newStart = mean;\n+            newEnd = mean + 1;\n+        } else {\n+            newStart = medianStart;\n+            newEnd = medianEnd;\n+        }\n+        //??? does evidence not need to be merged???", "originalCommit": "34b71a1aba60653bf73cebae2bd0cf49b728a701", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY2NDQzNQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r424664435", "bodyText": "Don't need it for gCNV but I think you can leave it in", "author": "mwalker174", "createdAt": "2020-05-13T19:00:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2Njg1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2NzA0MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410467041", "bodyText": "Same here -- are they identical if the evidence doesn't match?", "author": "ldgauthier", "createdAt": "2020-04-17T20:59:35Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations\n+        final int medianStart = startPositions.get(startPositions.size() / 2);\n+        final int medianEnd = endPositions.get(endPositions.size() / 2);\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = exampleCall.getContig().equals(exampleCall.getEndContig()) && !exampleCall.getType().equals(StructuralVariantType.INS) ? medianEnd - medianStart + 1: exampleCall.getLength();\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList());\n+        final List<Genotype> clusterSamples = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+\n+        final int newStart;\n+        final int newEnd;\n+        if (exampleCall.getType().equals(StructuralVariantType.INS)) {\n+            // Insertions should be a single locus; also fixes case where end-supporting split reads are to the\n+            // left of start-supporting split reads\n+            final int mean = (medianStart + medianEnd) / 2;\n+            newStart = mean;\n+            newEnd = mean + 1;\n+        } else {\n+            newStart = medianStart;\n+            newEnd = medianEnd;\n+        }\n+        //??? does evidence not need to be merged???\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterSamples,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());\n+    }\n+\n+    @Override\n+    protected boolean clusterTogether(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        if (!a.getType().equals(b.getType())) return false;\n+        final boolean depthOnlyA = isDepthOnlyCall(a);\n+        final boolean depthOnlyB = isDepthOnlyCall(b);\n+        if (depthOnlyA && depthOnlyB) {\n+            return clusterTogetherBothDepthOnly(a, b);\n+        } else if (depthOnlyA != depthOnlyB) {\n+            return clusterTogetherMixedEvidence(a, b);\n+        } else {\n+            return clusterTogetherBothWithEvidence(a, b);\n+        }\n+    }\n+\n+    /**\n+     * Determine an overlap interval for clustering using reciprocal overlap or breakend window, as applicable\n+     * Returned interval represents the interval in which the start position of a new event must fall in order to be added to the cluster (including {@param call})\n+     * @param call  new event to be clustered\n+     * @param clusterMinStartInterval    the cluster of interest, may be null\n+     * @return  an interval describing the cluster after {@param call} is added\n+     */\n+    @Override\n+    protected SimpleInterval getClusteringInterval(final SVCallRecordWithEvidence call, final SimpleInterval clusterMinStartInterval) {\n+        final int minStart;\n+        final int maxStart;\n+        if (isDepthOnlyCall(call)) {\n+            minStart = (int) (call.getEnd() - call.getLength() / MIN_RECIPROCAL_OVERLAP_DEPTH); //start of an overlapping event such that call represents (reciprocal overlap) of that event\n+            maxStart = (int) (call.getStart() + (1.0 - MIN_RECIPROCAL_OVERLAP_DEPTH) * call.getLength());\n+        } else {\n+            minStart = call.getStart() - MAX_BREAKEND_CLUSTERING_WINDOW;\n+            maxStart = call.getStart() + MAX_BREAKEND_CLUSTERING_WINDOW;\n+        }\n+        final String currentContig = getCurrentContig();\n+        if (clusterMinStartInterval == null) {\n+            return IntervalUtils.trimIntervalToContig(currentContig, minStart, maxStart, dictionary.getSequence(currentContig).getSequenceLength());\n+        }\n+        //NOTE: this is an approximation -- best method would back calculate cluster bounds, then rederive start and end based on call + cluster\n+        final int newMinStart = Math.min(minStart, clusterMinStartInterval.getStart());\n+        final int newMaxStart = Math.max(maxStart, clusterMinStartInterval.getEnd());\n+        return IntervalUtils.trimIntervalToContig(currentContig, newMinStart, newMaxStart, dictionary.getSequence(currentContig).getSequenceLength());\n+    }\n+\n+    @Override\n+    protected boolean itemsAreIdentical(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {", "originalCommit": "34b71a1aba60653bf73cebae2bd0cf49b728a701", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ1NDQzOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427454438", "bodyText": "Yes I would like to keep the flexibility to be able to compare items without using equals() for the purposes of deduplication. (I'll comment elsewhere on whether we should be using SVCallRecordWithEvidence).", "author": "mwalker174", "createdAt": "2020-05-19T16:55:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2NzA0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2NzU4Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410467583", "bodyText": "I modified the math here to better reflect my idea of how we compute reciprocal overlap, but I'm open to discussion.", "author": "ldgauthier", "createdAt": "2020-04-17T21:00:47Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations\n+        final int medianStart = startPositions.get(startPositions.size() / 2);\n+        final int medianEnd = endPositions.get(endPositions.size() / 2);\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = exampleCall.getContig().equals(exampleCall.getEndContig()) && !exampleCall.getType().equals(StructuralVariantType.INS) ? medianEnd - medianStart + 1: exampleCall.getLength();\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList());\n+        final List<Genotype> clusterSamples = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+\n+        final int newStart;\n+        final int newEnd;\n+        if (exampleCall.getType().equals(StructuralVariantType.INS)) {\n+            // Insertions should be a single locus; also fixes case where end-supporting split reads are to the\n+            // left of start-supporting split reads\n+            final int mean = (medianStart + medianEnd) / 2;\n+            newStart = mean;\n+            newEnd = mean + 1;\n+        } else {\n+            newStart = medianStart;\n+            newEnd = medianEnd;\n+        }\n+        //??? does evidence not need to be merged???\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterSamples,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());\n+    }\n+\n+    @Override\n+    protected boolean clusterTogether(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        if (!a.getType().equals(b.getType())) return false;\n+        final boolean depthOnlyA = isDepthOnlyCall(a);\n+        final boolean depthOnlyB = isDepthOnlyCall(b);\n+        if (depthOnlyA && depthOnlyB) {\n+            return clusterTogetherBothDepthOnly(a, b);\n+        } else if (depthOnlyA != depthOnlyB) {\n+            return clusterTogetherMixedEvidence(a, b);\n+        } else {\n+            return clusterTogetherBothWithEvidence(a, b);\n+        }\n+    }\n+\n+    /**\n+     * Determine an overlap interval for clustering using reciprocal overlap or breakend window, as applicable\n+     * Returned interval represents the interval in which the start position of a new event must fall in order to be added to the cluster (including {@param call})\n+     * @param call  new event to be clustered\n+     * @param clusterMinStartInterval    the cluster of interest, may be null\n+     * @return  an interval describing the cluster after {@param call} is added\n+     */\n+    @Override\n+    protected SimpleInterval getClusteringInterval(final SVCallRecordWithEvidence call, final SimpleInterval clusterMinStartInterval) {\n+        final int minStart;\n+        final int maxStart;\n+        if (isDepthOnlyCall(call)) {\n+            minStart = (int) (call.getEnd() - call.getLength() / MIN_RECIPROCAL_OVERLAP_DEPTH); //start of an overlapping event such that call represents (reciprocal overlap) of that event\n+            maxStart = (int) (call.getStart() + (1.0 - MIN_RECIPROCAL_OVERLAP_DEPTH) * call.getLength());", "originalCommit": "34b71a1aba60653bf73cebae2bd0cf49b728a701", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc0MTQzNA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r424741434", "bodyText": "Hm this looks correct! Not sure what I was doing before...", "author": "mwalker174", "createdAt": "2020-05-13T21:27:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2NzU4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2ODEwOQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410468109", "bodyText": "Should we be checking the copy number here?  I don't think I want to merge a CN1 deletion with a CN0 deletion, for example.", "author": "ldgauthier", "createdAt": "2020-04-17T21:02:05Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVDepthOnlyCallDefragmenter.java", "diffHunk": "@@ -0,0 +1,120 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVDepthOnlyCallDefragmenter extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private final double minSampleOverlap;\n+    private static final double PADDING_FRACTION = 0.2;\n+\n+    public SVDepthOnlyCallDefragmenter(final SAMSequenceDictionary dictionary) {\n+        this(dictionary, 0.9);\n+    }\n+\n+    //for single-sample clustering case\n+    public SVDepthOnlyCallDefragmenter(final SAMSequenceDictionary dictionary, double minSampleOverlap) {\n+        super(dictionary, CLUSTERING_TYPE.SINGLE_LINKAGE);\n+        this.minSampleOverlap = minSampleOverlap;\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call encompassing all the cluster's events and containing all the algorithms and genotypes\n+     */\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final int newStart =  cluster.stream().mapToInt(SVCallRecordWithEvidence::getStart).min().getAsInt();\n+        final int newEnd = cluster.stream().mapToInt(SVCallRecordWithEvidence::getEnd).max().getAsInt();\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = newEnd - newStart + 1;  //+1 because GATK intervals are inclusive\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList()); //should be depth only\n+        final List<Genotype> clusterGenotypes = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterGenotypes,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());\n+    }\n+\n+    /**\n+     * Determine if two calls should cluster based on their padded intervals and genotyped samples\n+     * @param a\n+     * @param b\n+     * @return true if the two calls should be in the same cluster\n+     */\n+    @Override\n+    protected boolean clusterTogether(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        if (!isDepthOnlyCall(a) || !isDepthOnlyCall(b)) return false;\n+        Utils.validate(a.getContig().equals(a.getEndContig()), \"Call A is depth-only but interchromosomal\");\n+        Utils.validate(b.getContig().equals(b.getEndContig()), \"Call B is depth-only but interchromosomal\");\n+        if (!a.getType().equals(b.getType())) return false;\n+        final Set<String> sharedSamples = new LinkedHashSet<>(a.getSamples());\n+        sharedSamples.retainAll(b.getSamples());\n+        final double sampleOverlap = Math.min(sharedSamples.size() / (double) a.getSamples().size(), sharedSamples.size() / (double) b.getSamples().size());\n+        if (sampleOverlap < minSampleOverlap) return false;\n+        return getClusteringInterval(a, null)", "originalCommit": "34b71a1aba60653bf73cebae2bd0cf49b728a701", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU0MTk2Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427541962", "bodyText": "Thanks for pointing that out - we should check genotypes. I think easiest way would be to add a genotype to SVCallRecord.", "author": "mwalker174", "createdAt": "2020-05-19T19:18:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2ODEwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2ODYzMw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410468633", "bodyText": "Is the length inclusive?  That's typically the GATK convention, in which case this needs a +1.  Unless it comes as the above compensatory +1?", "author": "ldgauthier", "createdAt": "2020-04-17T21:03:18Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,217 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+        final List<Genotype> passing = variant.getGenotypes().stream()\n+                .filter(Genotype::isCalled)\n+                .filter(g -> Integer.valueOf((String)g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) >= minQuality)\n+                .collect(Collectors.toList());\n+        if (passing.isEmpty()) return null;\n+        final List<String> algorithms = Collections.singletonList(GATKSVVCFConstants.DEPTH_ALGORITHM);\n+\n+        //TODO : use new vcfs to get actual allele\n+        final int copyNumber = Integer.valueOf((String)variant.getGenotypes().get(0).getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN));\n+        if (copyNumber == 2) return null;\n+        final boolean isDel = copyNumber < 2;\n+        final boolean startStrand = isDel ? true : false;\n+        final boolean endStrand = isDel ? false : true;\n+        final StructuralVariantType type = isDel ? StructuralVariantType.DEL : StructuralVariantType.DUP;\n+\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final int end = variant.getEnd() + 1; // TODO this is a bug with gCNV vcf generation\n+        final int length = end - start;", "originalCommit": "34b71a1aba60653bf73cebae2bd0cf49b728a701", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMyNjE2OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422326169", "bodyText": "Let's go with the GATK convention", "author": "mwalker174", "createdAt": "2020-05-08T19:20:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2ODYzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2OTU2MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410469561", "bodyText": "This is a more strict check compared with itemsAreIdentical -- I use this in the tests", "author": "ldgauthier", "createdAt": "2020-04-17T21:05:27Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecordWithEvidence.java", "diffHunk": "@@ -0,0 +1,82 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.util.*;\n+\n+public class SVCallRecordWithEvidence extends SVCallRecord {\n+\n+    private final List<SplitReadSite> startSplitReadSites;\n+    private final List<SplitReadSite> endSplitReadSites;\n+    private final List<DiscordantPairEvidence> discordantPairs;\n+\n+    public SVCallRecordWithEvidence(final SVCallRecord record) {\n+        super(record.getContig(), record.getStart(), record.getStartStrand(), record.getEndContig(), record.getEnd(),\n+                record.getEndStrand(), record.getType(), record.getLength(), record.getAlgorithms(), record.getGenotypes());\n+        this.startSplitReadSites = Collections.emptyList();\n+        this.endSplitReadSites = Collections.emptyList();\n+        this.discordantPairs = Collections.emptyList();\n+    }\n+\n+    public SVCallRecordWithEvidence(final String startContig,\n+                                    final int start,\n+                                    final boolean startStrand,\n+                                    final String endContig,\n+                                    final int end,\n+                                    final boolean endStrand,\n+                                    final StructuralVariantType type,\n+                                    final int length,\n+                                    final List<String> algorithms,\n+                                    final List<Genotype> genotypes,\n+                                    final List<SplitReadSite> startSplitReadSites,\n+                                    final List<SplitReadSite> endSplitReadSites,\n+                                    final List<DiscordantPairEvidence> discordantPairs) {\n+        super(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, genotypes);\n+        Utils.nonNull(startSplitReadSites);\n+        Utils.nonNull(endSplitReadSites);\n+        Utils.nonNull(discordantPairs);\n+        Utils.containsNoNull(startSplitReadSites, \"Encountered null in start split reads\");\n+        Utils.containsNoNull(endSplitReadSites, \"Encountered null in end split reads\");\n+        Utils.containsNoNull(discordantPairs, \"Encountered null in discordant pairs\");\n+        this.startSplitReadSites = startSplitReadSites;\n+        this.endSplitReadSites = endSplitReadSites;\n+        this.discordantPairs = discordantPairs;\n+    }\n+\n+    public List<DiscordantPairEvidence> getDiscordantPairs() {\n+        return discordantPairs;\n+    }\n+\n+    public List<SplitReadSite> getStartSplitReadSites() {\n+        return startSplitReadSites;\n+    }\n+\n+    public List<SplitReadSite> getEndSplitReadSites() {\n+        return endSplitReadSites;\n+    }\n+\n+    @Override\n+    public boolean equals(final Object obj) {", "originalCommit": "34b71a1aba60653bf73cebae2bd0cf49b728a701", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ2OTc3MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410469770", "bodyText": "I don't have any insertion tests yet.", "author": "ldgauthier", "createdAt": "2020-04-17T21:05:57Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,205 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations\n+        final int medianStart = startPositions.get(startPositions.size() / 2);\n+        final int medianEnd = endPositions.get(endPositions.size() / 2);\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = exampleCall.getContig().equals(exampleCall.getEndContig()) && !exampleCall.getType().equals(StructuralVariantType.INS) ? medianEnd - medianStart + 1: exampleCall.getLength();\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList());\n+        final List<Genotype> clusterSamples = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+\n+        final int newStart;\n+        final int newEnd;\n+        if (exampleCall.getType().equals(StructuralVariantType.INS)) {", "originalCommit": "34b71a1aba60653bf73cebae2bd0cf49b728a701", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ3MDEyOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r410470128", "bodyText": "Would it be better to throw an IllegalStateException if we shouldn't reach here?", "author": "ldgauthier", "createdAt": "2020-04-17T21:06:54Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVDepthOnlyCallDefragmenter.java", "diffHunk": "@@ -0,0 +1,120 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVDepthOnlyCallDefragmenter extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private final double minSampleOverlap;\n+    private static final double PADDING_FRACTION = 0.2;\n+\n+    public SVDepthOnlyCallDefragmenter(final SAMSequenceDictionary dictionary) {\n+        this(dictionary, 0.9);\n+    }\n+\n+    //for single-sample clustering case\n+    public SVDepthOnlyCallDefragmenter(final SAMSequenceDictionary dictionary, double minSampleOverlap) {\n+        super(dictionary, CLUSTERING_TYPE.SINGLE_LINKAGE);\n+        this.minSampleOverlap = minSampleOverlap;\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call encompassing all the cluster's events and containing all the algorithms and genotypes\n+     */\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final int newStart =  cluster.stream().mapToInt(SVCallRecordWithEvidence::getStart).min().getAsInt();\n+        final int newEnd = cluster.stream().mapToInt(SVCallRecordWithEvidence::getEnd).max().getAsInt();\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = newEnd - newStart + 1;  //+1 because GATK intervals are inclusive\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList()); //should be depth only\n+        final List<Genotype> clusterGenotypes = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterGenotypes,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());\n+    }\n+\n+    /**\n+     * Determine if two calls should cluster based on their padded intervals and genotyped samples\n+     * @param a\n+     * @param b\n+     * @return true if the two calls should be in the same cluster\n+     */\n+    @Override\n+    protected boolean clusterTogether(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        if (!isDepthOnlyCall(a) || !isDepthOnlyCall(b)) return false;\n+        Utils.validate(a.getContig().equals(a.getEndContig()), \"Call A is depth-only but interchromosomal\");\n+        Utils.validate(b.getContig().equals(b.getEndContig()), \"Call B is depth-only but interchromosomal\");\n+        if (!a.getType().equals(b.getType())) return false;\n+        final Set<String> sharedSamples = new LinkedHashSet<>(a.getSamples());\n+        sharedSamples.retainAll(b.getSamples());\n+        final double sampleOverlap = Math.min(sharedSamples.size() / (double) a.getSamples().size(), sharedSamples.size() / (double) b.getSamples().size());\n+        if (sampleOverlap < minSampleOverlap) return false;\n+        return getClusteringInterval(a, null)\n+                .overlaps(getClusteringInterval(b, null));\n+    }\n+\n+\n+    /**\n+     * Determine an overlap interval for clustering using {@value #PADDING_FRACTION} padding\n+     * Returned interval represents the interval in which the start position of a new event must fall in order to be added to the cluster (including {@param call})\n+     * @param call  new event to be clustered\n+     * @param currentClusterInterval    the cluster of interest, may be null\n+     * @return  an interval describing the cluster after {@param call} is added\n+     */\n+    @Override\n+    protected SimpleInterval getClusteringInterval(final SVCallRecordWithEvidence call, final SimpleInterval currentClusterInterval) {\n+        Utils.nonNull(call);\n+        final SimpleInterval callInterval = getCallInterval(call);\n+        final int paddedCallStart = (int) (callInterval.getStart() - PADDING_FRACTION * callInterval.getLengthOnReference());\n+        final int paddedCallEnd = (int) (callInterval.getEnd() + PADDING_FRACTION * callInterval.getLengthOnReference());\n+        final String currentContig = getCurrentContig();\n+        final int contigLength = dictionary.getSequence(currentContig).getSequenceLength();\n+        if (currentClusterInterval == null) {\n+            return IntervalUtils.trimIntervalToContig(currentContig, paddedCallStart, paddedCallEnd, contigLength);\n+        }\n+        //NOTE: this is an approximation -- padding should be based on the length of the call plus currentClusterIntervals\n+        final int newMinStart = Math.min(paddedCallStart, currentClusterInterval.getStart());\n+        final int newMaxEnd = Math.max(paddedCallEnd, currentClusterInterval.getEnd());\n+        return IntervalUtils.trimIntervalToContig(currentContig, newMinStart, newMaxEnd, contigLength);\n+    }\n+\n+    // Not used for single-linkage clustering\n+    @Override\n+    protected boolean itemsAreIdentical(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        return false;", "originalCommit": "34b71a1aba60653bf73cebae2bd0cf49b728a701", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDgyMDM1Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r424820353", "bodyText": "Actually, as it's written now, it appears that this code does get called but will always be false for single-linkage clustering. The code path is getOutput() -> deduplicateItems() -> itemsAreIdentical(), so I think the better solution would be to check if the class algorithm type is single-linkage or max-clique and skip deduplicateItems() if it's single-linkage. In that case, we could throw a NeverReachHere exception.", "author": "mwalker174", "createdAt": "2020-05-14T01:27:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDQ3MDEyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjI0OTk5MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422249990", "bodyText": "Unfortunate that it's come to this, but I suppose this is our best option at this point given all of the gCNV vcfs that have been generated already. Alternatively we could implement a task to detect missing sequence dictionaries and fix the headers. @droazen Any concerns?", "author": "mwalker174", "createdAt": "2020-05-08T16:47:32Z", "path": "src/main/java/org/broadinstitute/hellbender/engine/MultiVariantDataSource.java", "diffHunk": "@@ -60,6 +60,8 @@\n     private CloseableIterator<VariantContext> currentIterator;\n     private SortedSet<String> mergedSamples;\n \n+    private boolean skipDictionaryValidation = false;", "originalCommit": "acc7db5effdd87b4828b4182b1255bd8a069128e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE1NjUwMA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r515156500", "bodyText": "Not sure I have all the context here, but if there are a large amount of gCNV VCFs that have already been generated with inconsistent and/or missing dictionaries, can we just run UpdateVCFSequenceDictionary on them once and pretend like it never happened? I think it would be good to enforce correct behavior with this pipeline (and/or with this engine-level class) going forward.", "author": "samuelklee", "createdAt": "2020-10-30T14:53:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjI0OTk5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjI1NTMwOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422255308", "bodyText": "These aren't needed are they?", "author": "mwalker174", "createdAt": "2020-05-08T16:57:13Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/spark/sv/utils/GATKSVVCFConstants.java", "diffHunk": "@@ -74,6 +74,14 @@\n     public static final String DUP_TAN_EXPANSION_INTERNAL_ID_START_STRING = \"INS-DUPLICATION-TANDEM-EXPANSION\";\n     public static final String DUP_INV_INTERNAL_ID_START_STRING = \"INS-DUPLICATION-INVERTED-EXPANSION\";\n \n+    // for breakpoint segmentation\n+    public static final String ALGORITHMS_ATTRIBUTE = \"ALGORITHMS\";\n+    public static final String STRANDS_ATTRIBUTE = \"STRANDS\";\n+    public static final String DEPTH_ALGORITHM = \"depth\";\n+    public static final String END_CONTIG_ATTRIBUTE = \"CHR2\";\n+    public static String START_SPLIT_READ_COUNT_ATTRIBUTE = \"SSR\";\n+    public static String END_SPLIT_READ_COUNT_ATTRIBUTE = \"ESR\";\n+    public static String DISCORDANT_PAIR_COUNT_ATTRIBUTE = \"PE\";", "originalCommit": "acc7db5effdd87b4828b4182b1255bd8a069128e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMxNDYyMw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422314623", "bodyText": "This seems to do a lot of unneeded work if the compared objects are not equal. I think you should short-circuit the checks like this:\nif (!this.getContig().equals(b.getContig())) return false;\nif (this.getStart() != b.getStart()) return false;\n...\n\nIt's a little less readable but IMO the efficiency costs are worth it.", "author": "mwalker174", "createdAt": "2020-05-08T18:56:38Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,217 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+        final List<Genotype> passing = variant.getGenotypes().stream()\n+                .filter(Genotype::isCalled)\n+                .filter(g -> Integer.valueOf((String)g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) >= minQuality)\n+                .collect(Collectors.toList());\n+        if (passing.isEmpty()) return null;\n+        final List<String> algorithms = Collections.singletonList(GATKSVVCFConstants.DEPTH_ALGORITHM);\n+\n+        //TODO : use new vcfs to get actual allele\n+        final int copyNumber = Integer.valueOf((String)variant.getGenotypes().get(0).getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN));\n+        if (copyNumber == 2) return null;\n+        final boolean isDel = copyNumber < 2;\n+        final boolean startStrand = isDel ? true : false;\n+        final boolean endStrand = isDel ? false : true;\n+        final StructuralVariantType type = isDel ? StructuralVariantType.DEL : StructuralVariantType.DUP;\n+\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final int end = variant.getEnd() + 1; // TODO this is a bug with gCNV vcf generation\n+        final int length = end - start;\n+        return new SVCallRecord(startContig, start, startStrand, startContig, end, endStrand, type, length, algorithms, passing);\n+    }\n+\n+    public SVCallRecord(final String startContig,\n+                        final int start,\n+                        final boolean startStrand,\n+                        final String endContig,\n+                        final int end,\n+                        final boolean endStrand,\n+                        final StructuralVariantType type,\n+                        final int length,\n+                        final List<String> algorithms,\n+                        final List<Genotype> genotypes) {\n+        Utils.nonNull(startContig);\n+        Utils.nonNull(endContig);\n+        Utils.nonNull(type);\n+        Utils.nonNull(algorithms);\n+        Utils.nonNull(genotypes);\n+        Utils.nonEmpty(algorithms);\n+        Utils.nonEmpty(genotypes);\n+        Utils.containsNoNull(algorithms, \"Encountered null algorithm\");\n+        Utils.containsNoNull(genotypes, \"Encountered null genotype\");\n+        this.startContig = startContig;\n+        this.start = start;\n+        this.startStrand = startStrand;\n+        this.endContig = endContig;\n+        this.end = end;\n+        this.endStrand = endStrand;\n+        this.type = type;\n+        this.length = length;\n+        this.algorithms = algorithms;\n+        this.genotypes = genotypes;\n+        this.samples = genotypes.stream()\n+                .filter(Genotype::isCalled)\n+                .map(Genotype::getSampleName)\n+                .collect(Collectors.toCollection(LinkedHashSet::new));\n+    }\n+\n+    @Override\n+    public String getContig() {\n+        return startContig;\n+    }\n+\n+    @Override\n+    public int getStart() {\n+        return start;\n+    }\n+\n+    public boolean getStartStrand() {\n+        return startStrand;\n+    }\n+\n+    public String getEndContig() {\n+        return endContig;\n+    }\n+\n+    @Override\n+    public int getEnd() {\n+        return end;\n+    }\n+\n+    public boolean getEndStrand() {\n+        return endStrand;\n+    }\n+\n+    public StructuralVariantType getType() {\n+        return type;\n+    }\n+\n+    public int getLength() {\n+        return length;\n+    }\n+\n+    public List<String> getAlgorithms() {\n+        return algorithms;\n+    }\n+\n+    public Set<String> getSamples() {\n+        return samples;\n+    }\n+\n+    public List<Genotype> getGenotypes() {\n+        return genotypes;\n+    }\n+\n+    public SimpleInterval getStartAsInterval() {\n+        return new SimpleInterval(startContig, start, start + 1);\n+    }\n+\n+    public SimpleInterval getEndAsInterval() {\n+        return new SimpleInterval(endContig, end, end + 1);\n+    }\n+\n+    @Override\n+    public boolean equals(final Object obj) {\n+        if (obj == null) {\n+            return false;\n+        }\n+        if (this.getClass() != obj.getClass()) {\n+            return false;\n+        }\n+        final SVCallRecord b = (SVCallRecord) obj;\n+        boolean areEqual = this.getContig().equals(b.getContig());\n+        areEqual &= this.getStart() == b.getStart();\n+        areEqual &= this.getStartStrand() == b.getStartStrand();\n+\n+        areEqual &= this.getEndContig() == b.getEndContig();\n+        areEqual &= this.getEnd() == b.getEnd();\n+        areEqual &= this.getEndStrand() == b.getEndStrand();\n+\n+        areEqual &= this.getType() == b.getType();\n+        areEqual &= this.getLength() == b.getLength();\n+\n+        areEqual &= this.getAlgorithms().containsAll(b.getAlgorithms());\n+        areEqual &= b.getAlgorithms().containsAll(this.getAlgorithms());\n+\n+        areEqual &= this.getSamples().containsAll(b.getSamples());\n+        areEqual &= b.getSamples().containsAll(this.getSamples());\n+\n+        areEqual &= this.getGenotypes().containsAll(b.getGenotypes());\n+        areEqual &= b.getGenotypes().containsAll(this.getGenotypes());\n+\n+        return areEqual;", "originalCommit": "acc7db5effdd87b4828b4182b1255bd8a069128e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMyODA1OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422328059", "bodyText": "I'm fine with merging this as is, but we should open a ticket to keep track of this", "author": "mwalker174", "createdAt": "2020-05-08T19:24:23Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,217 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+        final List<Genotype> passing = variant.getGenotypes().stream()\n+                .filter(Genotype::isCalled)\n+                .filter(g -> Integer.valueOf((String)g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) >= minQuality)\n+                .collect(Collectors.toList());\n+        if (passing.isEmpty()) return null;\n+        final List<String> algorithms = Collections.singletonList(GATKSVVCFConstants.DEPTH_ALGORITHM);\n+\n+        //TODO : use new vcfs to get actual allele", "originalCommit": "acc7db5effdd87b4828b4182b1255bd8a069128e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY1MTcyOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r424651728", "bodyText": "Have you been excluding allosomes when testing?", "author": "mwalker174", "createdAt": "2020-05-13T18:39:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMyODA1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjMyOTcwOQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r422329709", "bodyText": "Remove + 1's", "author": "mwalker174", "createdAt": "2020-05-08T19:27:49Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,217 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+        final List<Genotype> passing = variant.getGenotypes().stream()\n+                .filter(Genotype::isCalled)\n+                .filter(g -> Integer.valueOf((String)g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) >= minQuality)\n+                .collect(Collectors.toList());\n+        if (passing.isEmpty()) return null;\n+        final List<String> algorithms = Collections.singletonList(GATKSVVCFConstants.DEPTH_ALGORITHM);\n+\n+        //TODO : use new vcfs to get actual allele\n+        final int copyNumber = Integer.valueOf((String)variant.getGenotypes().get(0).getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN));\n+        if (copyNumber == 2) return null;\n+        final boolean isDel = copyNumber < 2;\n+        final boolean startStrand = isDel ? true : false;\n+        final boolean endStrand = isDel ? false : true;\n+        final StructuralVariantType type = isDel ? StructuralVariantType.DEL : StructuralVariantType.DUP;\n+\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final int end = variant.getEnd() + 1; // TODO this is a bug with gCNV vcf generation\n+        final int length = end - start;\n+        return new SVCallRecord(startContig, start, startStrand, startContig, end, endStrand, type, length, algorithms, passing);\n+    }\n+\n+    public SVCallRecord(final String startContig,\n+                        final int start,\n+                        final boolean startStrand,\n+                        final String endContig,\n+                        final int end,\n+                        final boolean endStrand,\n+                        final StructuralVariantType type,\n+                        final int length,\n+                        final List<String> algorithms,\n+                        final List<Genotype> genotypes) {\n+        Utils.nonNull(startContig);\n+        Utils.nonNull(endContig);\n+        Utils.nonNull(type);\n+        Utils.nonNull(algorithms);\n+        Utils.nonNull(genotypes);\n+        Utils.nonEmpty(algorithms);\n+        Utils.nonEmpty(genotypes);\n+        Utils.containsNoNull(algorithms, \"Encountered null algorithm\");\n+        Utils.containsNoNull(genotypes, \"Encountered null genotype\");\n+        this.startContig = startContig;\n+        this.start = start;\n+        this.startStrand = startStrand;\n+        this.endContig = endContig;\n+        this.end = end;\n+        this.endStrand = endStrand;\n+        this.type = type;\n+        this.length = length;\n+        this.algorithms = algorithms;\n+        this.genotypes = genotypes;\n+        this.samples = genotypes.stream()\n+                .filter(Genotype::isCalled)\n+                .map(Genotype::getSampleName)\n+                .collect(Collectors.toCollection(LinkedHashSet::new));\n+    }\n+\n+    @Override\n+    public String getContig() {\n+        return startContig;\n+    }\n+\n+    @Override\n+    public int getStart() {\n+        return start;\n+    }\n+\n+    public boolean getStartStrand() {\n+        return startStrand;\n+    }\n+\n+    public String getEndContig() {\n+        return endContig;\n+    }\n+\n+    @Override\n+    public int getEnd() {\n+        return end;\n+    }\n+\n+    public boolean getEndStrand() {\n+        return endStrand;\n+    }\n+\n+    public StructuralVariantType getType() {\n+        return type;\n+    }\n+\n+    public int getLength() {\n+        return length;\n+    }\n+\n+    public List<String> getAlgorithms() {\n+        return algorithms;\n+    }\n+\n+    public Set<String> getSamples() {\n+        return samples;\n+    }\n+\n+    public List<Genotype> getGenotypes() {\n+        return genotypes;\n+    }\n+\n+    public SimpleInterval getStartAsInterval() {\n+        return new SimpleInterval(startContig, start, start + 1);\n+    }\n+\n+    public SimpleInterval getEndAsInterval() {\n+        return new SimpleInterval(endContig, end, end + 1);\n+    }", "originalCommit": "acc7db5effdd87b4828b4182b1255bd8a069128e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY0OTI1MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r424649251", "bodyText": "By GATK convention, should be just return start;", "author": "mwalker174", "createdAt": "2020-05-13T18:34:43Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/DiscordantPairEvidence.java", "diffHunk": "@@ -0,0 +1,62 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+\n+import htsjdk.tribble.Feature;\n+\n+public final class DiscordantPairEvidence implements Feature {\n+\n+    final String sample;\n+    final String startContig;\n+    final String endContig;\n+    final int start;\n+    final int end;\n+    final boolean startStrand;\n+    final boolean endStrand;\n+\n+    public DiscordantPairEvidence(final String sample, final String startContig, final int start, final boolean startStrand,\n+                                  final String endContig, final int end, final boolean endStrand) {\n+        this.sample = sample;\n+        this.startContig = startContig;\n+        this.start = start;\n+        this.startStrand = startStrand;\n+        this.endContig = endContig;\n+        this.end = end;\n+        this.endStrand = endStrand;\n+    }\n+\n+    public String getSample() {\n+        return sample;\n+    }\n+\n+    // For purposes of indexing, we will return the start position\n+    @Override\n+    public String getContig() {\n+        return startContig;\n+    }\n+\n+    @Override\n+    public int getStart() {\n+        return start;\n+    }\n+\n+    public boolean getStartStrand() {\n+        return startStrand;\n+    }\n+\n+    public String getEndContig() {\n+        return endContig;\n+    }\n+\n+    @Override\n+    public int getEnd() {\n+        if (startContig.equals(endContig)) {\n+            return end;\n+        } else {\n+            return start + 1;", "originalCommit": "acc7db5effdd87b4828b4182b1255bd8a069128e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc0ODI5MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r424748290", "bodyText": "I think it should be 1 + validClusterIds.size()", "author": "mwalker174", "createdAt": "2020-05-13T21:43:02Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/LocatableClusterEngine.java", "diffHunk": "@@ -0,0 +1,294 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.util.Locatable;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import scala.Tuple2;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+public abstract class LocatableClusterEngine<T extends Locatable> {\n+\n+    public enum CLUSTERING_TYPE {\n+        SINGLE_LINKAGE,\n+        MAX_CLIQUE\n+    }\n+\n+    protected final SAMSequenceDictionary dictionary;\n+    private final List<Tuple2<SimpleInterval, List<Long>>> currentClusters; // Pairs of cluster start interval with item IDs\n+    private final Map<Long,T> idToItemMap;\n+    private final List<T> outputBuffer;\n+    private final CLUSTERING_TYPE clusteringType;\n+    private long currentItemId;\n+    private String currentContig;\n+\n+\n+    public LocatableClusterEngine(final SAMSequenceDictionary dictionary, final CLUSTERING_TYPE clusteringType) {\n+        this.dictionary = dictionary;\n+        this.clusteringType = clusteringType;\n+        this.currentClusters = new LinkedList<>();\n+        this.idToItemMap = new HashMap<>();\n+        this.outputBuffer = new ArrayList<>();\n+        currentItemId = 0;\n+        currentContig = null;\n+    }\n+\n+    abstract protected boolean clusterTogether(final T a, final T b);\n+    abstract protected SimpleInterval getClusteringInterval(final T item, final SimpleInterval currentClusterInterval);\n+    abstract protected T deduplicateIdenticalItems(final Collection<T> items);\n+    abstract protected boolean itemsAreIdentical(final T a, final T b);\n+    abstract protected T flattenCluster(final Collection<T> cluster);\n+\n+    public List<T> getOutput() {\n+        flushClusters();\n+        final List<T> output = deduplicateItems(outputBuffer);\n+        outputBuffer.clear();\n+        return output;\n+    }\n+\n+    private void resetItemIds() {\n+        Utils.validate(currentClusters.isEmpty(), \"Current cluster collection not empty\");\n+        currentItemId = 0;\n+        idToItemMap.clear();\n+    }\n+\n+    public boolean isEmpty() {\n+        return currentContig == null;\n+    }\n+\n+    public void add(final T item) {\n+\n+        // Start a new cluster if on a new contig\n+        if (!item.getContig().equals(currentContig)) {\n+            flushClusters();\n+            currentContig = item.getContig();\n+            idToItemMap.put(currentItemId, item);\n+            seedCluster(currentItemId);\n+            currentItemId++;\n+            return;\n+        }\n+\n+        // Keep track of a unique id for each item\n+        idToItemMap.put(currentItemId, item);\n+        final List<Integer> clusterIdsToProcess = cluster(item);\n+        processFinalizedClusters(clusterIdsToProcess);\n+        deleteRedundantClusters();\n+        currentItemId++;\n+    }\n+\n+    public String getCurrentContig() {\n+        return currentContig;\n+    }\n+\n+    public List<T> deduplicateItems(final List<T> items) {\n+        final List<T> sortedItems = IntervalUtils.sortLocatablesBySequenceDictionary(items, dictionary);\n+        final List<T> deduplicatedList = new ArrayList<>();\n+        int i = 0;\n+        while (i < sortedItems.size()) {\n+            final T record = sortedItems.get(i);\n+            int j = i + 1;\n+            final Collection<Integer> identicalItemIndexes = new ArrayList<>();\n+            while (j < sortedItems.size() && record.getStart() == sortedItems.get(j).getStart()) {\n+                final T other = sortedItems.get(j);\n+                if (itemsAreIdentical(record, other)) {\n+                    identicalItemIndexes.add(j);\n+                }\n+                j++;\n+            }\n+            if (identicalItemIndexes.isEmpty()) {\n+                deduplicatedList.add(record);\n+                i++;\n+            } else {\n+                identicalItemIndexes.add(i);\n+                final List<T> identicalItems = identicalItemIndexes.stream().map(sortedItems::get).collect(Collectors.toList());\n+                deduplicatedList.add(deduplicateIdenticalItems(identicalItems));\n+                i = j;\n+            }\n+        }\n+        return deduplicatedList;\n+    }\n+\n+    /**\n+     * Add a new {@param <T>} to the current clusters and determine which are complete\n+     * @param item to be added\n+     * @return the IDs for clusters that are complete and ready for processing\n+     */\n+    private List<Integer> cluster(final T item) {\n+        // Get list of item IDs from active clusters that cluster with this item\n+        final Set<Long> linkedItemIds = idToItemMap.entrySet().stream()\n+                .filter(other -> other.getKey().intValue() != currentItemId && clusterTogether(item, other.getValue()))\n+                .map(Map.Entry::getKey)\n+                .collect(Collectors.toCollection(LinkedHashSet::new));\n+\n+        // Find clusters to which this item belongs, and which active clusters we're definitely done with\n+        int clusterIndex = 0;\n+        final List<Integer> clusterIdsToProcess = new ArrayList<>();\n+        final List<Integer> clustersToAdd = new ArrayList<>();\n+        final List<Integer> clustersToSeedWith = new ArrayList<>();\n+        for (final Tuple2<SimpleInterval, List<Long>> cluster : currentClusters) {\n+            final SimpleInterval clusterInterval = cluster._1;\n+            final List<Long> clusterItemIds = cluster._2;\n+            if (item.getStart() > clusterInterval.getEnd()) {\n+                clusterIdsToProcess.add(clusterIndex);  //this cluster is complete -- process it when we're done\n+            } else {\n+                if (clusteringType.equals(CLUSTERING_TYPE.SINGLE_LINKAGE)) {\n+                    final int n = (int) clusterItemIds.stream().filter(linkedItemIds::contains).count();\n+                    if (n == clusterItemIds.size()) {\n+                        clustersToAdd.add(clusterIndex);\n+                    } else if (n > 0) {\n+                        clustersToSeedWith.add(clusterIndex);\n+                    }\n+                } else if (clusteringType.equals(CLUSTERING_TYPE.MAX_CLIQUE)) {\n+                    final boolean matchesCluster = clusterItemIds.stream().anyMatch(linkedItemIds::contains);\n+                    if (matchesCluster) {\n+                        clustersToAdd.add(clusterIndex);\n+                    }\n+                } else {\n+                    throw new IllegalArgumentException(\"Clustering algorithm for type \" + clusteringType.name() + \" not implemented\");\n+                }\n+            }\n+            clusterIndex++;\n+        }\n+\n+        // Add to item clusters\n+        for (final int index : clustersToAdd) {\n+            addToCluster(index, currentItemId);\n+        }\n+        // Create new clusters/cliques\n+        for (final int index : clustersToSeedWith) {\n+            seedWithExistingCluster(currentItemId, index, linkedItemIds);\n+        }\n+        // If there weren't any matches, create a new singleton cluster\n+        if (clustersToAdd.isEmpty() && clustersToSeedWith.isEmpty()) {\n+            seedCluster(currentItemId);\n+        }\n+        return clusterIdsToProcess;\n+    }\n+\n+    private void processCluster(final int clusterIndex) {\n+        final Tuple2<SimpleInterval, List<Long>> cluster = validateClusterIndex(clusterIndex);\n+        final List<Long> clusterItemIds = cluster._2;\n+        currentClusters.remove(clusterIndex);\n+        final List<T> clusterItems = clusterItemIds.stream().map(idToItemMap::get).collect(Collectors.toList());\n+        outputBuffer.add(flattenCluster(clusterItems));\n+    }\n+\n+    private void processFinalizedClusters(final List<Integer> clusterIdsToProcess) {\n+        final Set<Integer> activeClusterIds = IntStream.range(0, currentClusters.size()).boxed().collect(Collectors.toSet());\n+        activeClusterIds.removeAll(clusterIdsToProcess);\n+        final Set<Long> activeClusterItemIds = activeClusterIds.stream().flatMap(i -> currentClusters.get(i)._2.stream()).collect(Collectors.toSet());\n+        final Set<Long> finalizedItemIds = clusterIdsToProcess.stream()\n+                .flatMap(i -> currentClusters.get(i)._2.stream())\n+                .filter(i -> !activeClusterItemIds.contains(i))\n+                .collect(Collectors.toSet());\n+        for (int i = clusterIdsToProcess.size() - 1; i >= 0; i--) {\n+            processCluster(clusterIdsToProcess.get(i));\n+        }\n+        finalizedItemIds.stream().forEach(idToItemMap::remove);\n+    }\n+\n+    private void deleteRedundantClusters() {\n+        final Set<Integer> redundantClusterSet = new HashSet<>();\n+        for (int i = 0; i < currentClusters.size(); i++) {\n+            final Set<Long> clusterSetA = new HashSet<>(currentClusters.get(i)._2);\n+            for (int j = 0; j < i; j++) {\n+                final Set<Long> clusterSetB = new HashSet<>(currentClusters.get(j)._2);\n+                if (clusterSetA.containsAll(clusterSetB)) {\n+                    redundantClusterSet.add(j);\n+                } else if (clusterSetA.size() != clusterSetB.size() && clusterSetB.containsAll(clusterSetA)) {\n+                    redundantClusterSet.add(i);\n+                }\n+            }\n+        }\n+        final List<Integer> redundantClustersList = new ArrayList<>(redundantClusterSet);\n+        redundantClustersList.sort(Comparator.naturalOrder());\n+        for (int i = redundantClustersList.size() - 1; i >= 0; i--) {\n+            currentClusters.remove((int)redundantClustersList.get(i));\n+        }\n+    }\n+\n+    private void flushClusters() {\n+        while (!currentClusters.isEmpty()) {\n+            processCluster(0);\n+        }\n+        resetItemIds();\n+    }\n+\n+    private void seedCluster(final long seedId) {\n+        final T seed = validateItemIndex(seedId);\n+        final List<Long> newCluster = new ArrayList<>(1);\n+        newCluster.add(seedId);\n+        currentClusters.add(new Tuple2<>(getClusteringInterval(seed, null), newCluster));\n+    }\n+\n+    /**\n+     * Create a new cluster\n+     * @param seedId    itemId\n+     * @param existingClusterIndex\n+     * @param clusteringIds\n+     */\n+    private void seedWithExistingCluster(final Long seedId, final int existingClusterIndex, final Set<Long> clusteringIds) {\n+        final T seed = validateItemIndex(seedId);\n+        final List<Long> existingCluster = currentClusters.get(existingClusterIndex)._2;\n+        final List<Long> validClusterIds = existingCluster.stream().filter(clusteringIds::contains).collect(Collectors.toList());\n+        final List<Long> newCluster = new ArrayList<>(1 + existingCluster.size());", "originalCommit": "acc7db5effdd87b4828b4182b1255bd8a069128e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc5OTg4NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r424799885", "bodyText": "Uh ohhhh, this should be CLUSTERING_TYPE.MAX_CLIQUE and vice versa below. This is my mistake.", "author": "mwalker174", "createdAt": "2020-05-14T00:09:08Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/LocatableClusterEngine.java", "diffHunk": "@@ -0,0 +1,294 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.util.Locatable;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import scala.Tuple2;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+public abstract class LocatableClusterEngine<T extends Locatable> {\n+\n+    public enum CLUSTERING_TYPE {\n+        SINGLE_LINKAGE,\n+        MAX_CLIQUE\n+    }\n+\n+    protected final SAMSequenceDictionary dictionary;\n+    private final List<Tuple2<SimpleInterval, List<Long>>> currentClusters; // Pairs of cluster start interval with item IDs\n+    private final Map<Long,T> idToItemMap;\n+    private final List<T> outputBuffer;\n+    private final CLUSTERING_TYPE clusteringType;\n+    private long currentItemId;\n+    private String currentContig;\n+\n+\n+    public LocatableClusterEngine(final SAMSequenceDictionary dictionary, final CLUSTERING_TYPE clusteringType) {\n+        this.dictionary = dictionary;\n+        this.clusteringType = clusteringType;\n+        this.currentClusters = new LinkedList<>();\n+        this.idToItemMap = new HashMap<>();\n+        this.outputBuffer = new ArrayList<>();\n+        currentItemId = 0;\n+        currentContig = null;\n+    }\n+\n+    abstract protected boolean clusterTogether(final T a, final T b);\n+    abstract protected SimpleInterval getClusteringInterval(final T item, final SimpleInterval currentClusterInterval);\n+    abstract protected T deduplicateIdenticalItems(final Collection<T> items);\n+    abstract protected boolean itemsAreIdentical(final T a, final T b);\n+    abstract protected T flattenCluster(final Collection<T> cluster);\n+\n+    public List<T> getOutput() {\n+        flushClusters();\n+        final List<T> output = deduplicateItems(outputBuffer);\n+        outputBuffer.clear();\n+        return output;\n+    }\n+\n+    private void resetItemIds() {\n+        Utils.validate(currentClusters.isEmpty(), \"Current cluster collection not empty\");\n+        currentItemId = 0;\n+        idToItemMap.clear();\n+    }\n+\n+    public boolean isEmpty() {\n+        return currentContig == null;\n+    }\n+\n+    public void add(final T item) {\n+\n+        // Start a new cluster if on a new contig\n+        if (!item.getContig().equals(currentContig)) {\n+            flushClusters();\n+            currentContig = item.getContig();\n+            idToItemMap.put(currentItemId, item);\n+            seedCluster(currentItemId);\n+            currentItemId++;\n+            return;\n+        }\n+\n+        // Keep track of a unique id for each item\n+        idToItemMap.put(currentItemId, item);\n+        final List<Integer> clusterIdsToProcess = cluster(item);\n+        processFinalizedClusters(clusterIdsToProcess);\n+        deleteRedundantClusters();\n+        currentItemId++;\n+    }\n+\n+    public String getCurrentContig() {\n+        return currentContig;\n+    }\n+\n+    public List<T> deduplicateItems(final List<T> items) {\n+        final List<T> sortedItems = IntervalUtils.sortLocatablesBySequenceDictionary(items, dictionary);\n+        final List<T> deduplicatedList = new ArrayList<>();\n+        int i = 0;\n+        while (i < sortedItems.size()) {\n+            final T record = sortedItems.get(i);\n+            int j = i + 1;\n+            final Collection<Integer> identicalItemIndexes = new ArrayList<>();\n+            while (j < sortedItems.size() && record.getStart() == sortedItems.get(j).getStart()) {\n+                final T other = sortedItems.get(j);\n+                if (itemsAreIdentical(record, other)) {\n+                    identicalItemIndexes.add(j);\n+                }\n+                j++;\n+            }\n+            if (identicalItemIndexes.isEmpty()) {\n+                deduplicatedList.add(record);\n+                i++;\n+            } else {\n+                identicalItemIndexes.add(i);\n+                final List<T> identicalItems = identicalItemIndexes.stream().map(sortedItems::get).collect(Collectors.toList());\n+                deduplicatedList.add(deduplicateIdenticalItems(identicalItems));\n+                i = j;\n+            }\n+        }\n+        return deduplicatedList;\n+    }\n+\n+    /**\n+     * Add a new {@param <T>} to the current clusters and determine which are complete\n+     * @param item to be added\n+     * @return the IDs for clusters that are complete and ready for processing\n+     */\n+    private List<Integer> cluster(final T item) {\n+        // Get list of item IDs from active clusters that cluster with this item\n+        final Set<Long> linkedItemIds = idToItemMap.entrySet().stream()\n+                .filter(other -> other.getKey().intValue() != currentItemId && clusterTogether(item, other.getValue()))\n+                .map(Map.Entry::getKey)\n+                .collect(Collectors.toCollection(LinkedHashSet::new));\n+\n+        // Find clusters to which this item belongs, and which active clusters we're definitely done with\n+        int clusterIndex = 0;\n+        final List<Integer> clusterIdsToProcess = new ArrayList<>();\n+        final List<Integer> clustersToAdd = new ArrayList<>();\n+        final List<Integer> clustersToSeedWith = new ArrayList<>();\n+        for (final Tuple2<SimpleInterval, List<Long>> cluster : currentClusters) {\n+            final SimpleInterval clusterInterval = cluster._1;\n+            final List<Long> clusterItemIds = cluster._2;\n+            if (item.getStart() > clusterInterval.getEnd()) {\n+                clusterIdsToProcess.add(clusterIndex);  //this cluster is complete -- process it when we're done\n+            } else {\n+                if (clusteringType.equals(CLUSTERING_TYPE.SINGLE_LINKAGE)) {", "originalCommit": "acc7db5effdd87b4828b4182b1255bd8a069128e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ0Njc3Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427446773", "bodyText": "Thanks it looks like you fixed this from my version where I had min/max switched, but I'm not sure I follow how this is an approximation. The purpose of the cluster interval is to determine whether a given item is \"past\" the cluster, ie the item's start position guarantees that it cannot be linked to any members of the cluster. To guarantee this, don't we just take the largest end position of all the items in the cluster?", "author": "mwalker174", "createdAt": "2020-05-19T16:43:59Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,209 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary, boolean depthOnly) {\n+        super(dictionary, depthOnly ? CLUSTERING_TYPE.SINGLE_LINKAGE : CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations\n+        final int medianStart = startPositions.get(startPositions.size() / 2);\n+        final int medianEnd = endPositions.get(endPositions.size() / 2);\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = exampleCall.getContig().equals(exampleCall.getEndContig()) && !exampleCall.getType().equals(StructuralVariantType.INS) ? medianEnd - medianStart + 1: exampleCall.getLength();\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList());\n+        final List<Genotype> clusterSamples = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+\n+        final int newStart;\n+        final int newEnd;\n+        if (exampleCall.getType().equals(StructuralVariantType.INS)) {\n+            // Insertions should be a single locus; also fixes case where end-supporting split reads are to the\n+            // left of start-supporting split reads\n+            final int mean = (medianStart + medianEnd) / 2;\n+            newStart = mean;\n+            newEnd = mean + 1;\n+        } else {\n+            newStart = medianStart;\n+            newEnd = medianEnd;\n+        }\n+        //??? does evidence not need to be merged???\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterSamples,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());\n+    }\n+\n+    @Override\n+    protected boolean clusterTogether(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        if (!a.getType().equals(b.getType())) return false;\n+        final boolean depthOnlyA = isDepthOnlyCall(a);\n+        final boolean depthOnlyB = isDepthOnlyCall(b);\n+        if (depthOnlyA && depthOnlyB) {\n+            return clusterTogetherBothDepthOnly(a, b);\n+        } else if (depthOnlyA != depthOnlyB) {\n+            return clusterTogetherMixedEvidence(a, b);\n+        } else {\n+            return clusterTogetherBothWithEvidence(a, b);\n+        }\n+    }\n+\n+    /**\n+     * Determine an overlap interval for clustering using reciprocal overlap or breakend window, as applicable\n+     * Returned interval represents the interval in which the start position of a new event must fall in order to be added to the cluster (including {@param call})\n+     * @param call  new event to be clustered\n+     * @param clusterMinStartInterval    the cluster of interest, may be null\n+     * @return  an interval describing the cluster after {@param call} is added\n+     */\n+    @Override\n+    protected SimpleInterval getClusteringInterval(final SVCallRecordWithEvidence call, final SimpleInterval clusterMinStartInterval) {\n+        final int minStart;\n+        final int maxStart;\n+        if (isDepthOnlyCall(call)) {\n+            minStart = (int) (call.getEnd() - call.getLength() / MIN_RECIPROCAL_OVERLAP_DEPTH); //start of an overlapping event such that call represents (reciprocal overlap) of that event\n+            maxStart = (int) (call.getStart() + (1.0 - MIN_RECIPROCAL_OVERLAP_DEPTH) * call.getLength());\n+        } else {\n+            minStart = call.getStart() - MAX_BREAKEND_CLUSTERING_WINDOW;\n+            maxStart = call.getStart() + MAX_BREAKEND_CLUSTERING_WINDOW;\n+        }\n+        final String currentContig = getCurrentContig();\n+        if (clusterMinStartInterval == null) {\n+            return IntervalUtils.trimIntervalToContig(currentContig, minStart, maxStart, dictionary.getSequence(currentContig).getSequenceLength());\n+        }\n+        //NOTE: this is an approximation -- best method would back calculate cluster bounds, then rederive start and end based on call + cluster\n+        final int newMinStart = Math.min(minStart, clusterMinStartInterval.getStart());\n+        final int newMaxStart = Math.max(maxStart, clusterMinStartInterval.getEnd());", "originalCommit": "505d571eb5c09fdb8b1619f3d0a8bab69067a203", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzQ1Njk5MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427456990", "bodyText": "I'm seeing this again and don't like it. Can you add a TODO here stating that we need to put more thought into this?", "author": "mwalker174", "createdAt": "2020-05-19T16:59:30Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVClusterEngine.java", "diffHunk": "@@ -0,0 +1,209 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class SVClusterEngine extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private static final double MIN_RECIPROCAL_OVERLAP_DEPTH = 0.8;\n+    private final double BREAKEND_CLUSTERING_WINDOW_FRACTION = 0.5;\n+    private final int MIN_BREAKEND_CLUSTERING_WINDOW = 50;\n+    private final int MAX_BREAKEND_CLUSTERING_WINDOW = 300;\n+    private final int MIXED_CLUSTERING_WINDOW = 2000;\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary) {\n+        super(dictionary, CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    public SVClusterEngine(final SAMSequenceDictionary dictionary, boolean depthOnly) {\n+        super(dictionary, depthOnly ? CLUSTERING_TYPE.SINGLE_LINKAGE : CLUSTERING_TYPE.MAX_CLIQUE);\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call approximating the average event for the cluster and containing all the algorithms and genotypes\n+     */\n+\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final List<Integer> startPositions = cluster.stream().map(SVCallRecordWithEvidence::getStart).sorted().collect(Collectors.toList());\n+        final List<Integer> endPositions = cluster.stream().map(SVCallRecordWithEvidence::getEnd).sorted().collect(Collectors.toList());\n+        //use the mid value of the sorted list so the start and end represent real breakpoint observations\n+        final int medianStart = startPositions.get(startPositions.size() / 2);\n+        final int medianEnd = endPositions.get(endPositions.size() / 2);\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = exampleCall.getContig().equals(exampleCall.getEndContig()) && !exampleCall.getType().equals(StructuralVariantType.INS) ? medianEnd - medianStart + 1: exampleCall.getLength();\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList());\n+        final List<Genotype> clusterSamples = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+\n+        final int newStart;\n+        final int newEnd;\n+        if (exampleCall.getType().equals(StructuralVariantType.INS)) {\n+            // Insertions should be a single locus; also fixes case where end-supporting split reads are to the\n+            // left of start-supporting split reads\n+            final int mean = (medianStart + medianEnd) / 2;\n+            newStart = mean;\n+            newEnd = mean + 1;\n+        } else {\n+            newStart = medianStart;\n+            newEnd = medianEnd;\n+        }\n+        //??? does evidence not need to be merged???\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterSamples,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());\n+    }\n+\n+    @Override\n+    protected boolean clusterTogether(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        if (!a.getType().equals(b.getType())) return false;\n+        final boolean depthOnlyA = isDepthOnlyCall(a);\n+        final boolean depthOnlyB = isDepthOnlyCall(b);\n+        if (depthOnlyA && depthOnlyB) {\n+            return clusterTogetherBothDepthOnly(a, b);\n+        } else if (depthOnlyA != depthOnlyB) {\n+            return clusterTogetherMixedEvidence(a, b);\n+        } else {\n+            return clusterTogetherBothWithEvidence(a, b);\n+        }\n+    }\n+\n+    /**\n+     * Determine an overlap interval for clustering using reciprocal overlap or breakend window, as applicable\n+     * Returned interval represents the interval in which the start position of a new event must fall in order to be added to the cluster (including {@param call})\n+     * @param call  new event to be clustered\n+     * @param clusterMinStartInterval    the cluster of interest, may be null\n+     * @return  an interval describing the cluster after {@param call} is added\n+     */\n+    @Override\n+    protected SimpleInterval getClusteringInterval(final SVCallRecordWithEvidence call, final SimpleInterval clusterMinStartInterval) {\n+        final int minStart;\n+        final int maxStart;\n+        if (isDepthOnlyCall(call)) {\n+            minStart = (int) (call.getEnd() - call.getLength() / MIN_RECIPROCAL_OVERLAP_DEPTH); //start of an overlapping event such that call represents (reciprocal overlap) of that event\n+            maxStart = (int) (call.getStart() + (1.0 - MIN_RECIPROCAL_OVERLAP_DEPTH) * call.getLength());\n+        } else {\n+            minStart = call.getStart() - MAX_BREAKEND_CLUSTERING_WINDOW;\n+            maxStart = call.getStart() + MAX_BREAKEND_CLUSTERING_WINDOW;\n+        }\n+        final String currentContig = getCurrentContig();\n+        if (clusterMinStartInterval == null) {\n+            return IntervalUtils.trimIntervalToContig(currentContig, minStart, maxStart, dictionary.getSequence(currentContig).getSequenceLength());\n+        }\n+        //NOTE: this is an approximation -- best method would back calculate cluster bounds, then rederive start and end based on call + cluster\n+        final int newMinStart = Math.min(minStart, clusterMinStartInterval.getStart());\n+        final int newMaxStart = Math.max(maxStart, clusterMinStartInterval.getEnd());\n+        return IntervalUtils.trimIntervalToContig(currentContig, newMinStart, newMaxStart, dictionary.getSequence(currentContig).getSequenceLength());\n+    }\n+\n+    @Override\n+    protected boolean itemsAreIdentical(final SVCallRecordWithEvidence a, final SVCallRecordWithEvidence b) {\n+        return a.getContig().equals(b.getContig())\n+                && a.getStart() == b.getStart()\n+                && a.getEndContig().equals(b.getEndContig())\n+                && a.getEnd() == b.getEnd()\n+                && a.getType().equals(b.getType())\n+                && a.getStartStrand() == b.getStartStrand()\n+                && a.getEndStrand() == b.getEndStrand();\n+    }\n+\n+    /**\n+     *  Merge genotypes and algorithms for multiple calls describing the same event\n+     * @param items all entries are assumed to describe the same event, i.e. satisfy {@link #itemsAreIdentical}\n+     * @return  a single representative call\n+     */\n+    @Override\n+    protected SVCallRecordWithEvidence deduplicateIdenticalItems(final Collection<SVCallRecordWithEvidence> items) {\n+        if (items.isEmpty()) {\n+            return null;\n+        }\n+        final List<Genotype> genotypes = items.stream()\n+                .map(SVCallRecordWithEvidence::getGenotypes)\n+                .flatMap(Collection::stream)\n+                .collect(Collectors.toList());\n+        final List<String> algorithms = items.stream()\n+                .map(SVCallRecordWithEvidence::getAlgorithms)\n+                .flatMap(Collection::stream)\n+                .distinct()\n+                .collect(Collectors.toList());\n+        final SVCallRecordWithEvidence example = items.iterator().next();\n+        return new SVCallRecordWithEvidence(\n+                example.getContig(),\n+                example.getStart(),\n+                example.getStartStrand(),\n+                example.getEndContig(),\n+                example.getEnd(),\n+                example.getEndStrand(),\n+                example.getType(),\n+                example.getLength(),\n+                algorithms,\n+                genotypes,\n+                example.getStartSplitReadSites(),\n+                example.getEndSplitReadSites(),\n+                example.getDiscordantPairs());", "originalCommit": "505d571eb5c09fdb8b1619f3d0a8bab69067a203", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzUzODU3Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427538572", "bodyText": "I'm looking back at my code and I believe I used SVCallRecordWithEvidence rather than SVCallRecord out of convenience for my clustering tool. However, I think it would be better to refactor using the latter so we can avoid issues like this one here.", "author": "mwalker174", "createdAt": "2020-05-19T19:12:35Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVDepthOnlyCallDefragmenter.java", "diffHunk": "@@ -0,0 +1,120 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.Genotype;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVDepthOnlyCallDefragmenter extends LocatableClusterEngine<SVCallRecordWithEvidence> {\n+\n+    private final double minSampleOverlap;\n+    private static final double PADDING_FRACTION = 0.2;\n+\n+    public SVDepthOnlyCallDefragmenter(final SAMSequenceDictionary dictionary) {\n+        this(dictionary, 0.9);\n+    }\n+\n+    //for single-sample clustering case\n+    public SVDepthOnlyCallDefragmenter(final SAMSequenceDictionary dictionary, double minSampleOverlap) {\n+        super(dictionary, CLUSTERING_TYPE.SINGLE_LINKAGE);\n+        this.minSampleOverlap = minSampleOverlap;\n+    }\n+\n+    /**\n+     * Find a single call representative of all the calls in the {@param cluster}\n+     * @param cluster   the events that are clustered together\n+     * @return  a call encompassing all the cluster's events and containing all the algorithms and genotypes\n+     */\n+    @Override\n+    protected SVCallRecordWithEvidence flattenCluster(final Collection<SVCallRecordWithEvidence> cluster) {\n+        final int newStart =  cluster.stream().mapToInt(SVCallRecordWithEvidence::getStart).min().getAsInt();\n+        final int newEnd = cluster.stream().mapToInt(SVCallRecordWithEvidence::getEnd).max().getAsInt();\n+        final SVCallRecordWithEvidence exampleCall = cluster.iterator().next();\n+        final int length = newEnd - newStart + 1;  //+1 because GATK intervals are inclusive\n+        final List<String> algorithms = cluster.stream().flatMap(v -> v.getAlgorithms().stream()).distinct().collect(Collectors.toList()); //should be depth only\n+        final List<Genotype> clusterGenotypes = cluster.stream().flatMap(v -> v.getGenotypes().stream()).collect(Collectors.toList());\n+        return new SVCallRecordWithEvidence(exampleCall.getContig(), newStart, exampleCall.getStartStrand(),\n+                exampleCall.getEndContig(), newEnd, exampleCall.getEndStrand(), exampleCall.getType(), length, algorithms, clusterGenotypes,\n+                exampleCall.getStartSplitReadSites(), exampleCall.getEndSplitReadSites(), exampleCall.getDiscordantPairs());", "originalCommit": "505d571eb5c09fdb8b1619f3d0a8bab69067a203", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU0MjcyMw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427542723", "bodyText": "I think this and DiscordantPairEvidence can be omitted if you stick to SVCallRecord", "author": "mwalker174", "createdAt": "2020-05-19T19:20:07Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SplitReadSite.java", "diffHunk": "@@ -0,0 +1,46 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+final class SplitReadSite {", "originalCommit": "505d571eb5c09fdb8b1619f3d0a8bab69067a203", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU0MzQwNQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427543405", "bodyText": "20 seems low to me. Did you consult with jfu on this?", "author": "mwalker174", "createdAt": "2020-05-19T19:21:15Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,172 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import com.google.common.collect.Lists;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFHeaderLines;\n+import org.broadinstitute.hellbender.utils.variant.HomoSapiensConstants;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+\n+    private String currentContig;\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;", "originalCommit": "505d571eb5c09fdb8b1619f3d0a8bab69067a203", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODM1MzcxOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r518353718", "bodyText": "That's still what Isaac has in the latest version.  Higher would be more efficient (i.e. merging fewer calls), but this is safer in case someone reduces the QS filter threshold downstream.", "author": "ldgauthier", "createdAt": "2020-11-05T20:45:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU0MzQwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU0NTgxNQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427545815", "bodyText": "I feel like this should get built into the engine with a requiresDictionary() method", "author": "mwalker174", "createdAt": "2020-05-19T19:25:31Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,172 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import com.google.common.collect.Lists;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFHeaderLines;\n+import org.broadinstitute.hellbender.utils.variant.HomoSapiensConstants;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+\n+    private String currentContig;\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Override\n+    public void onTraversalStart() {\n+        dictionary = getBestAvailableSequenceDictionary();\n+        if (dictionary == null) {\n+            throw new UserException(\"Reference sequence dictionary required\");\n+        }", "originalCommit": "505d571eb5c09fdb8b1619f3d0a8bab69067a203", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYwNzc2OA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427607768", "bodyText": "I think the end will now be at call.getEnd() + 1", "author": "mwalker174", "createdAt": "2020-05-19T21:22:16Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,172 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import com.google.common.collect.Lists;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.IntervalUtils;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.variant.GATKVCFHeaderLines;\n+import org.broadinstitute.hellbender.utils.variant.HomoSapiensConstants;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+\n+    private String currentContig;\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Override\n+    public void onTraversalStart() {\n+        dictionary = getBestAvailableSequenceDictionary();\n+        if (dictionary == null) {\n+            throw new UserException(\"Reference sequence dictionary required\");\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.0);\n+        clusterEngine = new SVClusterEngine(dictionary, true);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                defragmenter.add(new SVCallRecordWithEvidence(record));\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();  //Don't forget to do the last cluster!!!\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+        defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        //defragmented calls may still be overlapping, so run the clustering engine to combine based on reciprocal overlap\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        calls.stream()\n+                .sorted(Comparator.comparing(c -> c.getStartAsInterval(), IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(this::buildVariantContext)\n+                .forEachOrdered(vcfWriter::add);\n+    }\n+\n+    public VariantContext buildVariantContext(final SVCallRecordWithEvidence call) {\n+        Utils.nonNull(call);\n+        final Allele altAllele = Allele.create(\"<\" + call.getType().name() + \">\", false);\n+        final Allele refAllele = Allele.REF_N;\n+        final VariantContextBuilder builder = new VariantContextBuilder(\"\", call.getContig(), call.getStart(), call.getEnd(),\n+                Lists.newArrayList(refAllele, altAllele));\n+        builder.attribute(VCFConstants.END_KEY, call.getEnd());", "originalCommit": "505d571eb5c09fdb8b1619f3d0a8bab69067a203", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYxMDU3Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427610572", "bodyText": "What was the impetus for these changes?", "author": "mwalker174", "createdAt": "2020-05-19T21:27:39Z", "path": "src/main/java/org/broadinstitute/hellbender/utils/IndexUtils.java", "diffHunk": "@@ -26,16 +31,17 @@ private IndexUtils(){}\n      * Load a Tribble .idx index from disk, checking for out of date indexes and old versions\n      * @return an Index, or null if we're unable to load\n      */\n-    public static Index loadTribbleIndex(final File featureFile) {\n+    public static Index loadTribbleIndex(final Path featureFile) {", "originalCommit": "505d571eb5c09fdb8b1619f3d0a8bab69067a203", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODg0MTQwMQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r428841401", "bodyText": "This is necessary to get the sequence dictionary out of the tabix index over NIO because the results I was working with were from an older version.  I've since added the sequence dictionary to the header, but this bug should be fixed.", "author": "ldgauthier", "createdAt": "2020-05-21T18:41:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYxMDU3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYxMTczMg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r427611732", "bodyText": "This class will probably evolve later, but it would be best to change this one line now. Let's make the suffix \".sv_calls.tsv.gz\" otherwise we could create trouble with feature codec collisions.", "author": "mwalker174", "createdAt": "2020-05-19T21:30:12Z", "path": "src/main/java/org/broadinstitute/hellbender/utils/codecs/SVCallRecordCodec.java", "diffHunk": "@@ -0,0 +1,76 @@\n+package org.broadinstitute.hellbender.utils.codecs;\n+\n+import com.google.common.base.Splitter;\n+import htsjdk.tribble.AsciiFeatureCodec;\n+import htsjdk.tribble.index.tabix.TabixFormat;\n+import htsjdk.tribble.readers.LineIterator;\n+import htsjdk.variant.variantcontext.GenotypeBuilder;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+public class SVCallRecordCodec extends AsciiFeatureCodec<SVCallRecord> {\n+\n+    public static final String FORMAT_SUFFIX = \".tsv.gz\";", "originalCommit": "505d571eb5c09fdb8b1619f3d0a8bab69067a203", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc4OTA5OA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r434789098", "bodyText": "Now that I'm looking at this again, I don't understand this logic.  Inversions, sure, but dupes?", "author": "ldgauthier", "createdAt": "2020-06-03T19:02:52Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,217 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+        final List<Genotype> passing = variant.getGenotypes().stream()\n+                .filter(Genotype::isCalled)\n+                .filter(g -> Integer.valueOf((String)g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) >= minQuality)\n+                .collect(Collectors.toList());\n+        if (passing.isEmpty()) return null;\n+        final List<String> algorithms = Collections.singletonList(GATKSVVCFConstants.DEPTH_ALGORITHM);\n+\n+        //TODO : use new vcfs to get actual allele\n+        final int copyNumber = Integer.valueOf((String)variant.getGenotypes().get(0).getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN));\n+        if (copyNumber == 2) return null;\n+        final boolean isDel = copyNumber < 2;\n+        final boolean startStrand = isDel ? true : false;\n+        final boolean endStrand = isDel ? false : true;", "originalCommit": "acc7db5effdd87b4828b4182b1255bd8a069128e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0MjQxMQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527142411", "bodyText": "DELs are +/- (true/false) and DUPs and -/+. Inversions are -/- and +/+.", "author": "mwalker174", "createdAt": "2020-11-19T19:27:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc4OTA5OA=="}], "type": "inlineReview"}, {"oid": "4f8b4b099475adf375c72df9e7f1096c194a1bed", "url": "https://github.com/broadinstitute/gatk/commit/4f8b4b099475adf375c72df9e7f1096c194a1bed", "message": "More tests to check qual score calculation", "committedDate": "2020-06-15T20:32:52Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI2MzUwMg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r447263502", "bodyText": "Note that use of non-overlapping intervals for all CNV locatable collections was enforced by using the base AbstractLocatableCollection class---see documentation of that class and subclasses. In practice, PreprocessIntervals creates valid non-overlapping bins, which remain valid as they move through the pipelines.\nIt might be kind of a nightmare to go through and change all the relevant documentation---and even more of a nightmare to define behavior for algorithms/procedures (such as segmentation, segment merging, identifying unique site/bin overlaps, etc.), which rely on this rather fundamental property (part of why we enforce this in the first place).\nI haven't looked closely at the code to see why you absolutely need to use the CNV collections classes to represent overlapping intervals, but I'd strongly suggest refactoring to avoid this if possible. Perhaps instead convert to VCF, IntervalList, etc.?", "author": "samuelklee", "createdAt": "2020-06-29T21:25:48Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/arguments/CopyNumberArgumentValidationUtils.java", "diffHunk": "@@ -48,17 +48,24 @@ public static void validateIntervalArgumentCollection(final IntervalArgumentColl\n     }\n \n     /**\n-     * Validate that a list of locatables is valid and sorted according to a sequence dictionary and contains no duplicates or overlaps.\n+     * Validate that a list of locatables is valid and sorted according to a sequence dictionary", "originalCommit": "6cd8471380a722a009cf80fd81446c8000ebd115", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI3NjM2NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r447276365", "bodyText": "Took a quick look---you only need to validate the sample name (which might be a redundant check, haven't looked closely) and read in a list of IntegerCopyNumberSegments from copyNumberSegmentsFile. So unless you need the other machinery that the CNV collection class buys you, you might get away with doing something more one-off for this procedure---perhaps a private method that uses one of the other more generic TSV reading utility methods.\nIn any case, I'd add some comments/documentation that indicates that this business with overlapping intervals is the exception, not the norm.", "author": "samuelklee", "createdAt": "2020-06-29T21:54:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI2MzUwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM3NjM1Nw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r458376357", "bodyText": "I was hoping to be able to use the encoder and decoder to get my data into a format compatible with FuncotateSegments without writing more IO methods, but that didn't work out so well because FuncotateSegments expects fields that the Python segment generation code doesn't write out.  I'm still struggling with an elegant way to turn the VCF into something that can be annotated, so I may end up ripping out the CopyNumberSegmentCollection-related changes anyway.", "author": "ldgauthier", "createdAt": "2020-07-21T20:44:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI2MzUwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM4NzMzOQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r458387339", "bodyText": "I thought FuncotateSegments was only intended to be used on somatic seg files, not the VCFs produced by the gCNV pipeline? Is there any additional functionality in FuncotateSegments besides being able to read seg files---can you not use Funcotator instead?\nI was always in favor of having the somatic pipeline output VCFs rather than seg files (there are probably still a few related issues filed), but not sure what the plan going forward should be.", "author": "samuelklee", "createdAt": "2020-07-21T21:05:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI2MzUwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE0OTYxMA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r515149610", "bodyText": "Was reminded of this while looking at #6924. Not sure if you were planning on going back and addressing it, but I would make sure that all concomitant changes are reverted before merging. Happy to help review or discuss more!", "author": "samuelklee", "createdAt": "2020-10-30T14:45:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI2MzUwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE5MTA0NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r516191045", "bodyText": "Jon says Funcotator doesn't understand the END key.  It's on the list, but it's a long list. He says converting the VCF to a .set and using FuncotateSegments is the best bet for now.", "author": "ldgauthier", "createdAt": "2020-11-02T19:00:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI2MzUwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE5NDAwNQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r516194005", "bodyText": "Sorry, just to be clear, I meant the changes to AbstractLocatableCollection, etc. to allow overlapping intervals should be rolled back, since most of the existing methods and code very strongly depend on the assumption of non-overlapping intervals. I think it will be difficult to update code and documentation if you want to keep those changes. I don\u2019t think I\u2019m up to date on the development plans for Funcotator/FuncotateSegments, so I don\u2019t have strong opinions there.", "author": "samuelklee", "createdAt": "2020-11-02T19:05:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI2MzUwMg=="}], "type": "inlineReview"}, {"oid": "854619b80a1516622859192909557b831afe7fc9", "url": "https://github.com/broadinstitute/gatk/commit/854619b80a1516622859192909557b831afe7fc9", "message": "Use call intervals for bin-space defragmentation\nAdjust copy number for overlapping events (not super efficient)\nDon't fill in ref GTs until we look at upstream calls\nBig refactor to allow overlap in segments for joint calling\nDiploid genotypes and actually get ref base (if reference is supplied)\nQS filtering and AC calculation\nContinue AC annotation.  Add some checks for single-sample VCFs\nWith bcftools fast combine and filtering and annotation\nFilter by raw calls and filtered calls\nUpdate VC alleles if we update genotype\nFail if X contig names don't match up", "committedDate": "2020-10-19T17:15:55Z", "type": "forcePushed"}, {"oid": "0d6bb314543637a02636795e07be1ba7345f023c", "url": "https://github.com/broadinstitute/gatk/commit/0d6bb314543637a02636795e07be1ba7345f023c", "message": "gCNV joint calling improvements:\nUse call intervals for bin-space defragmentation\nAdjust copy number for overlapping events (not super efficient)\nDon't fill in ref GTs until we look at upstream calls\nBig refactor to allow overlap in segments for joint calling\nDiploid genotypes and actually get ref base (if reference is supplied)\nQS filtering and AC calculation\nContinue AC annotation.  Add some checks for single-sample VCFs\nWith bcftools fast combine and filtering and annotation\nFilter by raw calls and filtered calls\nUpdate VC alleles if we update genotype\nFail if X contig names don't match up", "committedDate": "2020-10-19T20:31:54Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2ODU2OA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r515168568", "bodyText": "Looks like this isn't used yet, but if it's something you'll add, perhaps change the argument name to input-denoised-copy-ratios to be consistent with output-denoised-copy-ratios.", "author": "samuelklee", "createdAt": "2020-10-30T15:09:48Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java", "diffHunk": "@@ -128,6 +129,15 @@\n     public static final String OUTPUT_DENOISED_COPY_RATIOS_LONG_NAME = \"output-denoised-copy-ratios\";\n     public static final String AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME = \"autosomal-ref-copy-number\";\n     public static final String ALLOSOMAL_CONTIG_LONG_NAME = \"allosomal-contig\";\n+    public static final String INPUT_INTERVALS_LONG_NAME = \"input-intervals-vcf\";\n+    public static final String INPUT_DENOISED_COPY_RATIO_LONG_NAME = \"input-denoised-copy-ratio\";", "originalCommit": "0d6bb314543637a02636795e07be1ba7345f023c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "874848454bbd012d995a7c879927ff4491174b34", "url": "https://github.com/broadinstitute/gatk/commit/874848454bbd012d995a7c879927ff4491174b34", "message": "Python unit test runner looks good\nAddress overlapping interval collection refactor", "committedDate": "2020-11-17T15:44:45Z", "type": "forcePushed"}, {"oid": "62b655c720bdea7621392a194861c8366a641fae", "url": "https://github.com/broadinstitute/gatk/commit/62b655c720bdea7621392a194861c8366a641fae", "message": "I thought we would need this....", "committedDate": "2020-11-19T17:03:31Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExMjM2Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527112363", "bodyText": "optional=true", "author": "mwalker174", "createdAt": "2020-11-19T18:37:26Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExMjU0Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527112546", "bodyText": "This was a little confusing to me, and I would think that WGS would need this as well. Suggestion: gCNV model intervals created with the FilterIntervals tool.", "author": "mwalker174", "createdAt": "2020-11-19T18:37:47Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExMzcwMw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527113703", "bodyText": "Also if this is truly optional, add optional = true", "author": "mwalker174", "createdAt": "2020-11-19T18:39:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExMjU0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjg2ODQ4OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r532868489", "bodyText": "WGS would only need this if the bin sizes were uneven -- do we ever expect gCNV to be run like that?", "author": "ldgauthier", "createdAt": "2020-11-30T20:01:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExMjU0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjg4NTQ3Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r532885473", "bodyText": "I think the current CNV tools themselves are WGS/WES/etc. agnostic, for the most part if not entirely\u2014they just have enough exposed generic parameters to handle both cases.\nNot sure I have the entire context here, but I would generally expect FilterIntervals to be run regardless of sequencing type, and it\u2019s conceivable that arbitrary bins could be used on WGS data for certain use cases.", "author": "samuelklee", "createdAt": "2020-11-30T20:32:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExMjU0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQxMzU2OA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r534413568", "bodyText": "Yes sometimes the bin sizes are uneven because gaps are always omitted, among other possibilities.", "author": "mwalker174", "createdAt": "2020-12-02T19:07:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExMjU0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExNjg3OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527116879", "bodyText": "optional = true. I didn't realize barclay supported enum type inputs. Should you list the options here or do they pop out somewhere in the docs?", "author": "mwalker174", "createdAt": "2020-11-19T18:45:04Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjg2Nzg4OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r532867889", "bodyText": "Pretty sure they pop out in the docs, like for --emit-reference-confidence:\nhttps://gatk.broadinstitute.org/hc/en-us/articles/360050814612-HaplotypeCaller", "author": "ldgauthier", "createdAt": "2020-11-30T19:59:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExNjg3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQxMzk2NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r534413965", "bodyText": "Very cool. I like this.", "author": "mwalker174", "createdAt": "2020-12-02T19:07:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExNjg3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExNzE4OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527117189", "bodyText": "Clarify that this is a vcf. Also can remove optional=false since that is default.", "author": "mwalker174", "createdAt": "2020-11-19T18:45:31Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExODAxOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527118018", "bodyText": "Can you make them all end in _LONG_NAME or just _NAME?", "author": "mwalker174", "createdAt": "2020-11-19T18:47:00Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExOTAzMw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527119033", "bodyText": "What's the reasoning for this? Maybe we should make disabling this an option.", "author": "mwalker174", "createdAt": "2020-11-19T18:48:41Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ4NDg4MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r534484880", "bodyText": "This is to avoid the inefficient dictionary checks mentioned above.  The tool requires a reference, though, so each input VCF will be checked against the reference, which is just O(N).", "author": "ldgauthier", "createdAt": "2020-12-02T21:13:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzExOTAzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyMTMzMg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527121332", "bodyText": "I've written some machinery to read in gCNV contig ploidy calls that we can use to deal with inferring ploidy. We can put a pin in that for now.\nAlso, I do think it's nice to have the ped file option although my understanding is the sex assignments tend to be unreliable in practice.", "author": "mwalker174", "createdAt": "2020-11-19T18:52:19Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ4NTg0Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r534485842", "bodyText": "In theory one could pass in a pedigree constructed in another way, e.g. one that's higher confidence than the gCNV X calls stapled together.", "author": "ldgauthier", "createdAt": "2020-12-02T21:15:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyMTMzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyMjAwOQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527122009", "bodyText": "For readability, can you put these validation blocks into a separate function?", "author": "mwalker174", "createdAt": "2020-11-19T18:53:29Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyMjEwNQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527122105", "bodyText": "Extra newline here", "author": "mwalker174", "createdAt": "2020-11-19T18:53:38Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyMjMyMA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527122320", "bodyText": "Missing indent", "author": "mwalker174", "createdAt": "2020-11-19T18:53:57Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyMzEwNw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527123107", "bodyText": "Should make 0.8 a tool parameter", "author": "mwalker174", "createdAt": "2020-11-19T18:55:14Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyMzk4Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527123983", "bodyText": "callIntervals is null by default already, can reduce code here:\nif (modelCallIntervalList != null) {\n    final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n    final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n    callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n}", "author": "mwalker174", "createdAt": "2020-11-19T18:56:40Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyNjc0OA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527126748", "bodyText": "There is probably something I don't understand here, but why not just final VCFHeader vcfHeader = new VCFHeader(headerLines, samples);?", "author": "mwalker174", "createdAt": "2020-11-19T19:01:09Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM3ODE2Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r535378166", "bodyText": "I copied this from a tool that copied this from a tool that copied this from CombineGVCFs where it has @jamesemery 's name on it.  It looks like these hijinx are to prevent the writer's samples from being modified?", "author": "ldgauthier", "createdAt": "2020-12-03T16:19:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyNjc0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyNzU2OA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527127568", "bodyText": "Can make all parameters final", "author": "mwalker174", "createdAt": "2020-11-19T19:02:29Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyODIwNw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527128207", "bodyText": "This is very neat", "author": "mwalker174", "createdAt": "2020-11-19T19:03:40Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ4Nzg1OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r534487859", "bodyText": "I believe that was James's invention.  Very handy.", "author": "ldgauthier", "createdAt": "2020-12-02T21:18:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyODIwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyOTQxOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527129418", "bodyText": "Should add some minimal documentation here", "author": "mwalker174", "createdAt": "2020-11-19T19:05:41Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwNTE0NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r538805145", "bodyText": "done", "author": "ldgauthier", "createdAt": "2020-12-08T21:02:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEyOTQxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEzMDc1Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527130753", "bodyText": "vc.getNSamples() would be better", "author": "mwalker174", "createdAt": "2020-11-19T19:08:09Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEzMjE4MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527132181", "bodyText": "Is this behavior documented somewhere?", "author": "mwalker174", "createdAt": "2020-11-19T19:10:30Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEzNTQ3MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527135470", "bodyText": "This variable was a little confusing at first. I think this should be called isMultiSampleInput and assigned using the header in onTraversalStart, rather than checking the number of samples for every variant.", "author": "mwalker174", "createdAt": "2020-11-19T19:16:00Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM4MTc5Nw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r535381797", "bodyText": "That would be too easy, though, wouldn't it?\nOn the plus side, I got to use the \"Invert Boolean\" refactor for the first time!", "author": "ldgauthier", "createdAt": "2020-12-03T16:23:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzEzNTQ3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0Mzc4Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527143786", "bodyText": "Someday we will be able to just check the alt alleles.", "author": "mwalker174", "createdAt": "2020-11-19T19:29:27Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,281 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    /**\n+     *\n+     * @param variant single-sample variant from a gCNV segments VCF\n+     * @param minQuality drop events with quality lower than this\n+     * @return\n+     */\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+\n+        if (variant.getGenotypes().size() == 1) {\n+            //only cluster good variants\n+            final Genotype g = variant.getGenotypes().get(0);\n+            if (g.isHomRef() || (g.isNoCall() && !g.hasExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT))\n+                    || Integer.valueOf((String) g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) < minQuality\n+                    || isNullCall(g)) {\n+                return null;\n+            }\n+        }\n+\n+\n+        final List<String> algorithms = Collections.singletonList(GATKSVVCFConstants.DEPTH_ALGORITHM);\n+\n+        boolean isDel = false;", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0NzMwOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527147308", "bodyText": "Should make this a class variable and initialize this in onTraversalStart()", "author": "mwalker174", "createdAt": "2020-11-19T19:35:10Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0ODE2OA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527148168", "bodyText": "Can be private", "author": "mwalker174", "createdAt": "2020-11-19T19:36:38Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 2;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            } else if (contig.equals(\"Y\") || contig.equals(\"chrY\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 0;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            }\n+        }\n+        return samplePloidy;\n+    }\n+\n+    public VariantContext buildVariantContext(final SVCallRecordWithEvidence call, final ReferenceSequenceFile reference) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE1MDIxMw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527150213", "bodyText": "Since this is a high-use function, we should assign this list its final size here", "author": "mwalker174", "createdAt": "2020-11-19T19:40:13Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 2;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            } else if (contig.equals(\"Y\") || contig.equals(\"chrY\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 0;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            }\n+        }\n+        return samplePloidy;\n+    }\n+\n+    public VariantContext buildVariantContext(final SVCallRecordWithEvidence call, final ReferenceSequenceFile reference) {\n+        Utils.nonNull(call);\n+        Utils.nonNull(reference);\n+        final List<Allele> outputAlleles = new ArrayList<>();", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE1MTc0NA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527151744", "bodyText": "Avoid running this twice and make it a variable, boolean isCnv = call.getType().equals(StructuralVariantType.CNV). This will also help initializing the alleles list size.", "author": "mwalker174", "createdAt": "2020-11-19T19:42:46Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 2;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            } else if (contig.equals(\"Y\") || contig.equals(\"chrY\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 0;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            }\n+        }\n+        return samplePloidy;\n+    }\n+\n+    public VariantContext buildVariantContext(final SVCallRecordWithEvidence call, final ReferenceSequenceFile reference) {\n+        Utils.nonNull(call);\n+        Utils.nonNull(reference);\n+        final List<Allele> outputAlleles = new ArrayList<>();\n+        final Allele refAllele = Allele.create(ReferenceUtils.getRefBaseAtPosition(reference, call.getContig(), call.getStart()), true);\n+        outputAlleles.add(refAllele);\n+        if (!call.getType().equals(StructuralVariantType.CNV)) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM4MzE2MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r535383161", "bodyText": "Will do, but is the compiler not smart enough to do this internally?", "author": "ldgauthier", "createdAt": "2020-12-03T16:25:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE1MTc0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzU0NTQzMg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r543545432", "bodyText": "Possibly, but I think it helps with readability too.", "author": "mwalker174", "createdAt": "2020-12-15T17:33:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE1MTc0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE1MjQ3MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527152471", "bodyText": "We've moved away from MCNV in gatk-sv, is there a specific reason here? This should probably be defined as a static String somewhere.", "author": "mwalker174", "createdAt": "2020-11-19T19:44:06Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 2;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            } else if (contig.equals(\"Y\") || contig.equals(\"chrY\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 0;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            }\n+        }\n+        return samplePloidy;\n+    }\n+\n+    public VariantContext buildVariantContext(final SVCallRecordWithEvidence call, final ReferenceSequenceFile reference) {\n+        Utils.nonNull(call);\n+        Utils.nonNull(reference);\n+        final List<Allele> outputAlleles = new ArrayList<>();\n+        final Allele refAllele = Allele.create(ReferenceUtils.getRefBaseAtPosition(reference, call.getContig(), call.getStart()), true);\n+        outputAlleles.add(refAllele);\n+        if (!call.getType().equals(StructuralVariantType.CNV)) {\n+            outputAlleles.add(Allele.create(\"<\" + call.getType().name() + \">\", false));\n+        } else {\n+            outputAlleles.add(GATKSVVCFConstants.DEL_ALLELE);\n+            outputAlleles.add(GATKSVVCFConstants.DUP_ALLELE);\n+        }\n+\n+        final VariantContextBuilder builder = new VariantContextBuilder(\"\", call.getContig(), call.getStart(), call.getEnd(),\n+                outputAlleles);\n+        builder.attribute(VCFConstants.END_KEY, call.getEnd());\n+        builder.attribute(GATKSVVCFConstants.SVLEN, call.getLength());\n+        if (call.getType().equals(StructuralVariantType.CNV)) {\n+            builder.attribute(VCFConstants.SVTYPE, \"MCNV\");", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ5Mzk1MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r534493950", "bodyText": "This is for compatibility with the svtk annotate tool.  I will add a comment to that extent.", "author": "ldgauthier", "createdAt": "2020-12-02T21:30:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE1MjQ3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE1MzA4Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527153086", "bodyText": "final and initial size g.getAlleles().size()", "author": "mwalker174", "createdAt": "2020-11-19T19:45:05Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 2;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            } else if (contig.equals(\"Y\") || contig.equals(\"chrY\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 0;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            }\n+        }\n+        return samplePloidy;\n+    }\n+\n+    public VariantContext buildVariantContext(final SVCallRecordWithEvidence call, final ReferenceSequenceFile reference) {\n+        Utils.nonNull(call);\n+        Utils.nonNull(reference);\n+        final List<Allele> outputAlleles = new ArrayList<>();\n+        final Allele refAllele = Allele.create(ReferenceUtils.getRefBaseAtPosition(reference, call.getContig(), call.getStart()), true);\n+        outputAlleles.add(refAllele);\n+        if (!call.getType().equals(StructuralVariantType.CNV)) {\n+            outputAlleles.add(Allele.create(\"<\" + call.getType().name() + \">\", false));\n+        } else {\n+            outputAlleles.add(GATKSVVCFConstants.DEL_ALLELE);\n+            outputAlleles.add(GATKSVVCFConstants.DUP_ALLELE);\n+        }\n+\n+        final VariantContextBuilder builder = new VariantContextBuilder(\"\", call.getContig(), call.getStart(), call.getEnd(),\n+                outputAlleles);\n+        builder.attribute(VCFConstants.END_KEY, call.getEnd());\n+        builder.attribute(GATKSVVCFConstants.SVLEN, call.getLength());\n+        if (call.getType().equals(StructuralVariantType.CNV)) {\n+            builder.attribute(VCFConstants.SVTYPE, \"MCNV\");\n+        } else {\n+            builder.attribute(VCFConstants.SVTYPE, call.getType());\n+        }\n+        final List<Genotype> genotypes = new ArrayList<>();\n+        for (final Genotype g : call.getGenotypes()) {\n+            final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(g);\n+            //update reference alleles\n+            List<Allele> newGenotypeAlleles = new ArrayList<>();", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE1OTg0Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527159846", "bodyText": "Initial size call.getGenotypes().size()", "author": "mwalker174", "createdAt": "2020-11-19T19:56:31Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 2;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            } else if (contig.equals(\"Y\") || contig.equals(\"chrY\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 0;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            }\n+        }\n+        return samplePloidy;\n+    }\n+\n+    public VariantContext buildVariantContext(final SVCallRecordWithEvidence call, final ReferenceSequenceFile reference) {\n+        Utils.nonNull(call);\n+        Utils.nonNull(reference);\n+        final List<Allele> outputAlleles = new ArrayList<>();\n+        final Allele refAllele = Allele.create(ReferenceUtils.getRefBaseAtPosition(reference, call.getContig(), call.getStart()), true);\n+        outputAlleles.add(refAllele);\n+        if (!call.getType().equals(StructuralVariantType.CNV)) {\n+            outputAlleles.add(Allele.create(\"<\" + call.getType().name() + \">\", false));\n+        } else {\n+            outputAlleles.add(GATKSVVCFConstants.DEL_ALLELE);\n+            outputAlleles.add(GATKSVVCFConstants.DUP_ALLELE);\n+        }\n+\n+        final VariantContextBuilder builder = new VariantContextBuilder(\"\", call.getContig(), call.getStart(), call.getEnd(),\n+                outputAlleles);\n+        builder.attribute(VCFConstants.END_KEY, call.getEnd());\n+        builder.attribute(GATKSVVCFConstants.SVLEN, call.getLength());\n+        if (call.getType().equals(StructuralVariantType.CNV)) {\n+            builder.attribute(VCFConstants.SVTYPE, \"MCNV\");\n+        } else {\n+            builder.attribute(VCFConstants.SVTYPE, call.getType());\n+        }\n+        final List<Genotype> genotypes = new ArrayList<>();", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE2MjY3Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527162673", "bodyText": "final", "author": "mwalker174", "createdAt": "2020-11-19T20:01:48Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE2NTYxMQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527165611", "bodyText": "Suggestion: resolvedVCs.forEach(vcfWriter::add);", "author": "mwalker174", "createdAt": "2020-11-19T20:07:15Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE2NzgwMA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527167800", "bodyText": "I think you can simplify here by just having this:\nint clusterEnd = 0;\nString clusterContig = null;\n\nand then maybe have a check at the beginning of resolveVariantContexts() for empty input.", "author": "mwalker174", "createdAt": "2020-11-19T20:11:23Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE2OTQyMQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527169421", "bodyText": "Give initial size", "author": "mwalker174", "createdAt": "2020-11-19T20:14:21Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE3MTQwOQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527171409", "bodyText": "Avoid parsing g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT) twice by assigning to a variable, and make sure it exists first.", "author": "mwalker174", "createdAt": "2020-11-19T20:17:49Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE3MzI4Nw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527173287", "bodyText": "Can this be combined with the previous loop to be more efficient?", "author": "mwalker174", "createdAt": "2020-11-19T20:21:08Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE3NDQwOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527174408", "bodyText": "Assign initial size with SVUtils.hashMapCapacity()", "author": "mwalker174", "createdAt": "2020-11-19T20:23:12Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNTEzOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527205138", "bodyText": "Can you wrap this into a nested class? The getLeft()'s and getRights()'s can be hard to follow", "author": "mwalker174", "createdAt": "2020-11-19T21:18:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE3NDQwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE3NDQ3Nw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527174477", "bodyText": "Assign initial size with SVUtils.hashMapCapacity()", "author": "mwalker174", "createdAt": "2020-11-19T20:23:21Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE3NTU5MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527175590", "bodyText": "When is this the case?", "author": "mwalker174", "createdAt": "2020-11-19T20:25:24Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ5NjY4Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r534496682", "bodyText": "As noted in the javadoc, the VC just has variant genotypes.  I added some clarification.", "author": "ldgauthier", "createdAt": "2020-12-02T21:35:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE3NTU5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4NDc3OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527184779", "bodyText": "Should make this into a Set", "author": "mwalker174", "createdAt": "2020-11-19T20:41:40Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4NDkzOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527184938", "bodyText": "Extra newline", "author": "mwalker174", "createdAt": "2020-11-19T20:41:59Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4NTUwMQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527185501", "bodyText": "The if clause is redundant, and can combine with previous line -> } else {", "author": "mwalker174", "createdAt": "2020-11-19T20:43:05Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4NzQ2MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527187460", "bodyText": "Should this be Samples missing from pedigree and without VCF genotypes assumed to have ploidy 1 on allosomes. ?", "author": "mwalker174", "createdAt": "2020-11-19T20:46:36Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTYxNzM3MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r535617370", "bodyText": "I went through a couple different iterations of trying to avoid the ped file, which is now required.  The better solution is just to require all samples to be in the pedigree and check on startup, which I will do.  I don't love that you need a pedigree even if you're just calling over the autosomes, though.  Although maybe we should just go with @samuelklee's philosophy that these inputs are easy if the tool is run in the WDL.", "author": "ldgauthier", "createdAt": "2020-12-03T21:05:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4NzQ2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4ODY2Nw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527188667", "bodyText": "I think you could eliminate the samplePloidy variable and just have return statements wherever it is assigned and at the default-case warning message.", "author": "mwalker174", "createdAt": "2020-11-19T20:49:01Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 2;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            } else if (contig.equals(\"Y\") || contig.equals(\"chrY\")) {\n+                if (sampleSex.equals(Sex.FEMALE)) {\n+                    samplePloidy = 0;\n+                } else if (sampleSex.equals(Sex.MALE)) {\n+                    samplePloidy = 1;\n+                }\n+            }\n+        }\n+        return samplePloidy;", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4OTk2NA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527189964", "bodyText": "It seems confusing to allow users to specify a list of allosomal contigs but hard-code X and Y. Could we instead have separate \"X\" and \"Y\" tool input lists and use them here?", "author": "mwalker174", "createdAt": "2020-11-19T20:51:17Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, alleleCounts)\n+                    .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, alleleFreqs)\n+                    .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            }\n+        }\n+        return builder.make();\n+    }\n+\n+    /**\n+     *\n+     * @param sampleName\n+     * @param contig\n+     * @param g may be null\n+     * @return\n+     */\n+    private int getSamplePloidy(final String sampleName, final String contig, final Genotype g) {\n+        if (!allosomalContigList.contains(contig)) {\n+            return refAutosomalCopyNumber;\n+        }\n+        int samplePloidy = 1;\n+        if (sampleDB == null || sampleDB.getSample(sampleName) == null) {\n+            if (g != null) {\n+                samplePloidy = g.getPloidy();\n+            } else {\n+                oneShotLogger.warn(\"Samples missing from pedigree assumed to have ploidy 1 on allosomes.\");\n+            }\n+\n+        }\n+        else if (sampleDB != null && sampleDB.getSample(sampleName) != null) {\n+            final Sex sampleSex = sampleDB.getSample(sampleName).getSex();\n+            if (contig.equals(\"X\") || contig.equals(\"chrX\")) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY2NzQyOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r535667428", "bodyText": "I just came up out of a deep Wikipedia rabbit hole about Bryophytes and platyfish, but honestly the only non-human species I can imagine this being run on are canine and mouse, which both have XX/XY sex determination.  The issue at hand is that we can't trust the old PostprocessGermlineCNVCalls genotypes because they were all haploid.  The new ones are fixed, so they shouldn't need a pedigree file, but I was still working with old data.  Maybe this is a lot of effort for backward compatibility.  Another option is to output a schema version in the Postprocess VCF that we can parse here and then trust those genotype ploidies if it's the new version.", "author": "ldgauthier", "createdAt": "2020-12-03T21:54:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4OTk2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwNDUyNA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r538804524", "bodyText": "I took out the allosome arg, so the tool only supports XX/XY, and I also put the schema version into the segments VCF header.  The intervals VCFs haven't been updated, but when they are I'll change the header there too.", "author": "ldgauthier", "createdAt": "2020-12-08T21:01:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4OTk2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE5MjU1NA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527192554", "bodyText": "Suggest rewriting as: final int copyNumber = g != null && g.hasAnyAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT) ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()) : samplePloidy;", "author": "mwalker174", "createdAt": "2020-11-19T20:55:59Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTYwMjUyNA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r535602524", "bodyText": "You and David Benjamin can be on Team Totally Ternary, but if its multiple lines I prefer regular ifs.", "author": "ldgauthier", "createdAt": "2020-12-03T20:55:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE5MjU1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE5NDk0MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527194941", "bodyText": "Can you add a doc section for this?", "author": "mwalker174", "createdAt": "2020-11-19T21:00:14Z", "path": "src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java", "diffHunk": "@@ -0,0 +1,63 @@\n+package org.broadinstitute.hellbender.utils.variant;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class GATKSVVariantContextUtils {\n+    public static List<Allele> makeGenotypeAlleles(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE5NjY3MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527196670", "bodyText": "Also maybe rename to make it clear that this is for DEL/DUP/CNV only", "author": "mwalker174", "createdAt": "2020-11-19T21:03:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE5NDk0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE5NTYzNA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527195634", "bodyText": "Can avoid allele variable by replacing assignments with return statements", "author": "mwalker174", "createdAt": "2020-11-19T21:01:32Z", "path": "src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java", "diffHunk": "@@ -0,0 +1,63 @@\n+package org.broadinstitute.hellbender.utils.variant;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class GATKSVVariantContextUtils {\n+    public static List<Allele> makeGenotypeAlleles(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {\n+        final List<Allele> returnAlleles = new ArrayList<>();\n+        final Allele genotypeAllele = getAlleleForCopyNumber(copyNumberCall, refCopyNumber, refAllele);\n+        //some allosomes like Y can have ref copy number zero, in which case we just no-call\n+        if (refCopyNumber == 0) {\n+            return GATKVariantContextUtils.noCallAlleles(1);\n+        }\n+        //for only one haplotype we know which allele it has\n+        if (refCopyNumber == 1) {\n+           return Arrays.asList(getAlleleForCopyNumber(copyNumberCall, refCopyNumber, refAllele));\n+        //can't determine counts per haplotypes if there is a duplication\n+        } else if (genotypeAllele.equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+            return GATKVariantContextUtils.noCallAlleles(refCopyNumber);\n+        //for homDels, hetDels or homRefs\n+        } else if (refCopyNumber == 2) {\n+            if (copyNumberCall == 0) {\n+                returnAlleles.add(genotypeAllele);\n+            } else {\n+                returnAlleles.add(refAllele);\n+            }\n+            returnAlleles.add(genotypeAllele);\n+            return returnAlleles;\n+        //multiploid dels\n+        } else {\n+            for (int i = 0; i < copyNumberCall; i++) {\n+                returnAlleles.add(refAllele);\n+            }\n+            for (int i = copyNumberCall; i < refCopyNumber; i++) {\n+                returnAlleles.add(GATKSVVCFConstants.DEL_ALLELE);\n+            }\n+            return returnAlleles;\n+        }\n+    }\n+\n+    /**\n+     *\n+     * @param copyNumberCall\n+     * @param refCopyNumber\n+     * @param refAllele\n+     * @return variant allele if copyNumberCall != refCopyNumber, else refAllele\n+     */\n+    public static Allele getAlleleForCopyNumber(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {\n+        final Allele allele;\n+        if (copyNumberCall > refCopyNumber) {\n+            allele = GATKSVVCFConstants.DUP_ALLELE;\n+        } else if (copyNumberCall < refCopyNumber) {\n+            allele = GATKSVVCFConstants.DEL_ALLELE;\n+        } else {\n+            allele = refAllele;\n+        }\n+        return allele;", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE5NzQzMg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527197432", "bodyText": "return Collections.singletonList(genotypeAllele);", "author": "mwalker174", "createdAt": "2020-11-19T21:04:50Z", "path": "src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java", "diffHunk": "@@ -0,0 +1,63 @@\n+package org.broadinstitute.hellbender.utils.variant;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class GATKSVVariantContextUtils {\n+    public static List<Allele> makeGenotypeAlleles(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {\n+        final List<Allele> returnAlleles = new ArrayList<>();\n+        final Allele genotypeAllele = getAlleleForCopyNumber(copyNumberCall, refCopyNumber, refAllele);\n+        //some allosomes like Y can have ref copy number zero, in which case we just no-call\n+        if (refCopyNumber == 0) {\n+            return GATKVariantContextUtils.noCallAlleles(1);\n+        }\n+        //for only one haplotype we know which allele it has\n+        if (refCopyNumber == 1) {\n+           return Arrays.asList(getAlleleForCopyNumber(copyNumberCall, refCopyNumber, refAllele));", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE5Nzk0NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527197945", "bodyText": "All of these else's are unnecessary (only need if)", "author": "mwalker174", "createdAt": "2020-11-19T21:05:41Z", "path": "src/main/java/org/broadinstitute/hellbender/utils/variant/GATKSVVariantContextUtils.java", "diffHunk": "@@ -0,0 +1,63 @@\n+package org.broadinstitute.hellbender.utils.variant;\n+\n+import htsjdk.variant.variantcontext.Allele;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+public class GATKSVVariantContextUtils {\n+    public static List<Allele> makeGenotypeAlleles(final int copyNumberCall, final int refCopyNumber, final Allele refAllele) {\n+        final List<Allele> returnAlleles = new ArrayList<>();\n+        final Allele genotypeAllele = getAlleleForCopyNumber(copyNumberCall, refCopyNumber, refAllele);\n+        //some allosomes like Y can have ref copy number zero, in which case we just no-call\n+        if (refCopyNumber == 0) {\n+            return GATKVariantContextUtils.noCallAlleles(1);\n+        }\n+        //for only one haplotype we know which allele it has\n+        if (refCopyNumber == 1) {\n+           return Arrays.asList(getAlleleForCopyNumber(copyNumberCall, refCopyNumber, refAllele));\n+        //can't determine counts per haplotypes if there is a duplication\n+        } else if (genotypeAllele.equals(GATKSVVCFConstants.DUP_ALLELE)) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMTMzNA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527201334", "bodyText": "Could avoid potentially computing vc.hasGenotype(sample) twice using nested if's", "author": "mwalker174", "createdAt": "2020-11-19T21:11:48Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzQ4OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527203489", "bodyText": "(copyNumber > samplePloidy) -> copyNumber > samplePloidy", "author": "mwalker174", "createdAt": "2020-11-19T21:15:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMTMzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzE1Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527203152", "bodyText": "temp -> count (and below)", "author": "mwalker174", "createdAt": "2020-11-19T21:15:11Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNjkyMg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527206922", "bodyText": "Seems like this should be built into the initialization of copyNumber above. It's a bit confusing this way.", "author": "mwalker174", "createdAt": "2020-11-19T21:21:58Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIxNDU1OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527214559", "bodyText": "I think this could be consolidated into a one-liner: final long AC = alleleCountMap.get(vc.getAlternateAllele(0));", "author": "mwalker174", "createdAt": "2020-11-19T21:35:15Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIxNTI0MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527215241", "bodyText": "Give initial sizes", "author": "mwalker174", "createdAt": "2020-11-19T21:35:59Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUwNjMxMg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r534506312", "bodyText": "I promise I'll do better next time:", "author": "ldgauthier", "createdAt": "2020-12-02T21:53:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIxNTI0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIxNzE2MA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527217160", "bodyText": "When would we expect a non-ref allele to not be in the count map? We should probably throw an error", "author": "mwalker174", "createdAt": "2020-11-19T21:38:05Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());\n+                //}\n+                for (final Allele a : builder.getAlleles()) {\n+                    if (a.isReference()) {\n+                        continue;\n+                    }\n+                    alleleCounts.add(alleleCountMap.containsKey(a) ? alleleCountMap.get(a) : 0L);\n+                    alleleFreqs.add(alleleCountMap.containsKey(a) ? Double.valueOf(alleleCountMap.get(a)) / alleleNumber : 0L);", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIyMTA5OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527221099", "bodyText": "What happened here? Are the proper alleles set elsewhere?", "author": "mwalker174", "createdAt": "2020-11-19T21:42:20Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));\n+                    sampleCopyNumbers.get(s).setLeft(curr.getEnd());\n+                }\n+            }\n+            resolvedVCs.add(updateGenotypes(curr, sampleCopyNumbers));\n+            //update copy number table for subsequent VCs using variants genotypes from input VCs\n+            for (final Genotype g : curr.getGenotypes()) {\n+                if (g.hasAnyAttribute(GermlineCNVSegmentVariantComposer.CN)) {\n+                    sampleCopyNumbers.put(g.getSampleName(), new MutablePair<>(Integer.parseInt(g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.CN).toString()), curr.getAttributeAsInt(VCFConstants.END_KEY, curr.getStart())));\n+                }\n+            }\n+        }\n+        return resolvedVCs;\n+    }\n+\n+    /**\n+     *\n+     * @param vc VariantContext with just variant samples\n+     * @param sampleCopyNumbers may be modified to remove terminated variants\n+     * @return new VariantContext with AC and AF\n+     */\n+    private VariantContext updateGenotypes(final VariantContext vc, final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers) {\n+        final VariantContextBuilder builder = new VariantContextBuilder(vc);\n+        final List<Genotype> newGenotypes = new ArrayList<>();\n+        final Allele vcRefAllele = vc.getReference();\n+        final Map<Allele, Long> alleleCountMap = new HashMap<>();\n+        alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, 0L);\n+        alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, 0L);\n+        int alleleNumber = 0;\n+        for (final String sample : samples) {\n+            final int samplePloidy;\n+            //\"square off\" the genotype matrix by adding homRef calls\n+            if (!sampleCopyNumbers.containsKey(sample) && !vc.hasGenotype(sample)) {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), null);\n+                genotypeBuilder.alleles(GATKVariantContextUtils.makePloidyLengthAlleleList(samplePloidy, vcRefAllele));\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, samplePloidy);\n+                newGenotypes.add(genotypeBuilder.make());\n+            } else {\n+                final GenotypeBuilder genotypeBuilder = new GenotypeBuilder(sample);\n+                final Genotype g = vc.getGenotype(sample); //may be null\n+                samplePloidy = getSamplePloidy(sample, vc.getContig(), g);\n+                final int copyNumber = g != null ? Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT, samplePloidy).toString()) :\n+                        samplePloidy;\n+                final List<Allele> alleles;\n+                if (doDefragmentation || g == null) {  //if it's a multi-sample VCF we can trust the input genotypes\n+                    alleles = GATKSVVariantContextUtils.makeGenotypeAlleles(copyNumber, samplePloidy, vcRefAllele);\n+                } else {\n+                    alleles = g.getAlleles();\n+                }\n+                genotypeBuilder.alleles(alleles);\n+                //check for genotype in VC because we don't want to count overlapping events (in sampleCopyNumbers map) towards AC\n+                if (vc.hasGenotype(sample) && alleles.contains(GATKSVVCFConstants.DEL_ALLELE)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DEL_ALLELE, temp + alleles.stream().filter(Allele::isNonReference).count());\n+                } else if (vc.hasGenotype(sample) && (copyNumber > samplePloidy)) {\n+                    final Long temp = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                    alleleCountMap.put(GATKSVVCFConstants.DUP_ALLELE, temp + 1); //best we can do for dupes is carrier frequency\n+                }\n+                genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, copyNumber);\n+                if (sampleCopyNumbers.containsKey(sample)) {\n+                    if (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {\n+                        genotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n+                    }\n+                }\n+                newGenotypes.add(genotypeBuilder.make());\n+            }\n+            alleleNumber += samplePloidy;\n+        }\n+        builder.genotypes(newGenotypes);\n+        if (alleleNumber > 0) {\n+            if (vc.getAlternateAlleles().size() == 1) {\n+                final long AC;\n+                if (vc.getAlternateAllele(0).equals(GATKSVVCFConstants.DUP_ALLELE)) {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE);\n+                } else {\n+                    AC = alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE);\n+                }\n+                builder.attribute(VCFConstants.ALLELE_COUNT_KEY, AC)\n+                        .attribute(VCFConstants.ALLELE_FREQUENCY_KEY, Double.valueOf(AC) / alleleNumber)\n+                        .attribute(VCFConstants.ALLELE_NUMBER_KEY, alleleNumber);\n+            } else {\n+                final List<Long> alleleCounts = new ArrayList<>();\n+                final List<Double> alleleFreqs = new ArrayList<>();\n+                //if we merged and del and a dupe from different callsets, then make sure the VC has both alleles\n+                //if (alleleCountMap.get(GATKSVVCFConstants.DEL_ALLELE) > 0 && alleleCountMap.get(GATKSVVCFConstants.DUP_ALLELE) > 0) {\n+                //    builder.alleles(vc.getReference().getDisplayString(), GATKSVVCFConstants.DUP_ALLELE.getDisplayString(), GATKSVVCFConstants.DEL_ALLELE.getDisplayString());", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUxMzExOA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r534513118", "bodyText": "buildVariantContext takes care of this now.  I'll remove the comments.", "author": "ldgauthier", "createdAt": "2020-12-02T22:05:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIyMTA5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIzMzI3MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527233271", "bodyText": "Mention this can be generated with the JointGermlineCNVSegmentation tool", "author": "mwalker174", "createdAt": "2020-11-19T22:05:11Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java", "diffHunk": "@@ -171,6 +179,20 @@\n     )\n     private List<String> allosomalContigList;\n \n+    @Argument(\n+            doc = \"Input VCF with combined intervals for all samples\",\n+            fullName = INPUT_INTERVALS_LONG_NAME,\n+            optional = true\n+    )\n+    private File combinedIntervalsVCFFile = null;\n+\n+    @Argument(\n+            doc = \"VCF with clustered breakpoints and copy number calls for all samples\",", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIzMzc0Nw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527233747", "bodyText": "Also the functionality you've added should be added to the tool documentation", "author": "mwalker174", "createdAt": "2020-11-19T22:06:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIzMzI3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI0MTg1NA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527241854", "bodyText": "What differentiates GATKSVVCFConstants.COPY_NUMBER_FORMAT and GermlineCNVSegmentVariantComposer.CN? Maybe we should only have one", "author": "mwalker174", "createdAt": "2020-11-19T22:18:55Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,281 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    /**\n+     *\n+     * @param variant single-sample variant from a gCNV segments VCF\n+     * @param minQuality drop events with quality lower than this\n+     * @return\n+     */\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+\n+        if (variant.getGenotypes().size() == 1) {\n+            //only cluster good variants\n+            final Genotype g = variant.getGenotypes().get(0);\n+            if (g.isHomRef() || (g.isNoCall() && !g.hasExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT))", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM5MzU3OA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r535393578", "bodyText": "Will do. There needs to be a serious constant roundup and refactor in the GCNV tools, but that's a task for another day.", "author": "ldgauthier", "createdAt": "2020-12-03T16:38:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI0MTg1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI0MzI3NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527243275", "bodyText": "Switch to using the VariantContextGetters utils throughout for getting genotype and variant attributes.", "author": "mwalker174", "createdAt": "2020-11-19T22:21:45Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/sv/SVCallRecord.java", "diffHunk": "@@ -0,0 +1,281 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.Allele;\n+import htsjdk.variant.variantcontext.Genotype;\n+import htsjdk.variant.variantcontext.StructuralVariantType;\n+import htsjdk.variant.variantcontext.VariantContext;\n+import htsjdk.variant.vcf.VCFConstants;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.utils.SimpleInterval;\n+import org.broadinstitute.hellbender.utils.Utils;\n+import org.broadinstitute.hellbender.utils.codecs.SVCallRecordCodec;\n+\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+public class SVCallRecord implements Feature {\n+\n+    private final String startContig;\n+    private final int start;\n+    private final boolean startStrand;\n+    private final String endContig;\n+    private final int end;\n+    private final boolean endStrand;\n+    private final StructuralVariantType type;\n+    private int length;\n+    private final List<String> algorithms;\n+    private final List<Genotype> genotypes;\n+    private LinkedHashSet<String> samples;\n+\n+    private final static List<String> nonDepthCallerAttributes = Arrays.asList(\n+            VCFConstants.END_KEY,\n+            GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE,\n+            GATKSVVCFConstants.STRANDS_ATTRIBUTE,\n+            GATKSVVCFConstants.SVLEN,\n+            GATKSVVCFConstants.SVTYPE\n+    );\n+\n+    public static SVCallRecord create(final VariantContext variant) {\n+        Utils.nonNull(variant);\n+        Utils.validate(variant.getAttributes().keySet().containsAll(nonDepthCallerAttributes), \"Call is missing attributes\");\n+        final String startContig = variant.getContig();\n+        final int start = variant.getStart();\n+        final String endContig = variant.getAttributeAsString(GATKSVVCFConstants.END_CONTIG_ATTRIBUTE, \"NA\");\n+        final int end = variant.getAttributeAsInt(VCFConstants.END_KEY, variant.getStart());\n+        final StructuralVariantType type = variant.getStructuralVariantType();\n+        final List<String> algorithms = variant.getAttributeAsStringList(GATKSVVCFConstants.ALGORITHMS_ATTRIBUTE, GATKSVVCFConstants.DEPTH_ALGORITHM);\n+        final String strands = variant.getAttributeAsString(GATKSVVCFConstants.STRANDS_ATTRIBUTE, \"0\");\n+        if (strands.length() != 2) {\n+            throw new IllegalArgumentException(\"Strands field is not 2 characters long\");\n+        }\n+        final String startStrandChar = strands.substring(0, 1);\n+        if (!startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !startStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid start strand not found\");\n+        }\n+        final String endStrandChar = strands.substring(1, 2);\n+        if (!endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS) && !endStrandChar.equals(SVCallRecordCodec.STRAND_MINUS)) {\n+            throw new IllegalArgumentException(\"Valid end strand not found\");\n+        }\n+        final boolean startStrand = startStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final boolean endStrand = endStrandChar.equals(SVCallRecordCodec.STRAND_PLUS);\n+        final int length = variant.getAttributeAsInt(GATKSVVCFConstants.SVLEN, 0);\n+        return new SVCallRecord(startContig, start, startStrand, endContig, end, endStrand, type, length, algorithms, variant.getGenotypes());\n+    }\n+\n+    /**\n+     *\n+     * @param variant single-sample variant from a gCNV segments VCF\n+     * @param minQuality drop events with quality lower than this\n+     * @return\n+     */\n+    public static SVCallRecord createDepthOnlyFromGCNV(final VariantContext variant, final double minQuality) {\n+        Utils.nonNull(variant);\n+\n+        if (variant.getGenotypes().size() == 1) {\n+            //only cluster good variants\n+            final Genotype g = variant.getGenotypes().get(0);\n+            if (g.isHomRef() || (g.isNoCall() && !g.hasExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT))\n+                    || Integer.valueOf((String) g.getExtendedAttribute(GermlineCNVSegmentVariantComposer.QS)) < minQuality", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI0Njg4MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527246881", "bodyText": "It seems like setRight() and setLeft() are switched with respect to the use of getLeft() and getRight():\ngenotypeBuilder.attribute(GermlineCNVSegmentVariantComposer.CN, sampleCopyNumbers.get(sample).getLeft());\n\nif (sampleCopyNumbers.get(sample).getRight() > vc.getStart()) {", "author": "mwalker174", "createdAt": "2020-11-19T22:29:02Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java", "diffHunk": "@@ -0,0 +1,483 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.reference.ReferenceSequenceFile;\n+import htsjdk.variant.variantcontext.*;\n+import htsjdk.variant.variantcontext.writer.VariantContextWriter;\n+import htsjdk.variant.vcf.VCFConstants;\n+import htsjdk.variant.vcf.VCFHeader;\n+import htsjdk.variant.vcf.VCFHeaderLine;\n+import htsjdk.variant.vcf.VCFStandardHeaderLines;\n+import org.apache.commons.lang3.tuple.MutablePair;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.StandardArgumentDefinitions;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.MultiVariantWalkerGroupedOnStart;\n+import org.broadinstitute.hellbender.engine.ReadsContext;\n+import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls;\n+import org.broadinstitute.hellbender.tools.copynumber.gcnv.GermlineCNVSegmentVariantComposer;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFConstants;\n+import org.broadinstitute.hellbender.tools.spark.sv.utils.GATKSVVCFHeaderLines;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecord;\n+import org.broadinstitute.hellbender.tools.sv.SVClusterEngine;\n+import org.broadinstitute.hellbender.tools.sv.SVCallRecordWithEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SVDepthOnlyCallDefragmenter;\n+import org.broadinstitute.hellbender.utils.*;\n+import org.broadinstitute.hellbender.utils.genotyper.IndexedSampleList;\n+import org.broadinstitute.hellbender.utils.logging.OneShotLogger;\n+import org.broadinstitute.hellbender.utils.reference.ReferenceUtils;\n+import org.broadinstitute.hellbender.utils.samples.PedigreeValidationType;\n+import org.broadinstitute.hellbender.utils.samples.SampleDB;\n+import org.broadinstitute.hellbender.utils.samples.SampleDBBuilder;\n+import org.broadinstitute.hellbender.utils.samples.Sex;\n+import org.broadinstitute.hellbender.utils.variant.*;\n+\n+import java.io.File;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Gathers single-sample segmented gCNV VCFs, harmonizes breakpoints, and outputs a cohort VCF with genotypes.\",\n+        oneLineSummary = \"Combined single-sample segmented gCNV VCFs.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class JointGermlineCNVSegmentation extends MultiVariantWalkerGroupedOnStart {\n+\n+    private SortedSet<String> samples;\n+    private VariantContextWriter vcfWriter;\n+    private SAMSequenceDictionary dictionary;\n+    private SVDepthOnlyCallDefragmenter defragmenter;\n+    private SVClusterEngine clusterEngine;\n+    private List<GenomeLoc> callIntervals;\n+    private String currentContig;\n+    private SampleDB sampleDB;\n+    private boolean doDefragmentation;\n+\n+    protected final OneShotLogger oneShotLogger = new OneShotLogger(logger);\n+\n+    public static final String MIN_QUALITY_LONG_NAME = \"minimum-qs-score\";\n+    public static final String MODEL_CALL_INTERVALS = \"model-call-intervals\";\n+    public static final String BREAKPOINT_SUMMARY_STRATEGY = \"breakpoint-summary-strategy\";\n+\n+    @Argument(fullName = MIN_QUALITY_LONG_NAME, doc = \"Minimum QS score to combine a variant segment\")\n+    private int minQS = 20;\n+\n+    @Argument(fullName = MODEL_CALL_INTERVALS, doc = \"Intervals used for gCNV calls.  Should be preprocessed and filtered to line up with model calls. Required for exomes.\")\n+    private File modelCallIntervalList;\n+\n+    @Argument(fullName = BREAKPOINT_SUMMARY_STRATEGY, doc = \"Strategy to use for choosing a representative value for a breakpoint cluster.\")\n+    private SVClusterEngine.BreakpointSummaryStrategy breakpointSummaryStrategy = SVClusterEngine.BreakpointSummaryStrategy.MEDIAN_START_MEDIAN_END;\n+\n+    @Argument(fullName= StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n+            shortName=StandardArgumentDefinitions.OUTPUT_SHORT_NAME,\n+            doc=\"The combined output file\", optional=false)\n+    private File outputFile;\n+\n+    @Argument(\n+            doc = \"Reference copy-number on autosomal intervals.\",\n+            fullName = PostprocessGermlineCNVCalls.AUTOSOMAL_REF_COPY_NUMBER_LONG_NAME,\n+            minValue = 0,\n+            optional = true\n+    )\n+    private int refAutosomalCopyNumber = 2;\n+\n+    @Argument(\n+            doc = \"Contigs to treat as allosomal (i.e. choose their reference copy-number allele according to \" +\n+                    \"the sample karyotype).\",\n+            fullName = PostprocessGermlineCNVCalls.ALLOSOMAL_CONTIG_LONG_NAME,\n+            optional = true\n+    )\n+    private List<String> allosomalContigList = Arrays.asList(\"X\",\"Y\",\"chrX\",\"chrY\");\n+\n+    /**\n+     * See https://software.broadinstitute.org/gatk/documentation/article.php?id=7696 for more details on the PED\n+     * format. Note that each -ped argument can be tagged with NO_FAMILY_ID, NO_PARENTS, NO_SEX, NO_PHENOTYPE to\n+     * tell the GATK PED parser that the corresponding fields are missing from the ped file.\n+     *\n+     */\n+    @Argument(fullName=StandardArgumentDefinitions.PEDIGREE_FILE_LONG_NAME, shortName=StandardArgumentDefinitions.PEDIGREE_FILE_SHORT_NAME, doc=\"Pedigree file for samples\", optional=false)\n+    private File pedigreeFile = null;\n+\n+    @Override\n+    public boolean doDictionaryCrossValidation() {\n+        return false;\n+    }\n+\n+    //require a reference to do dictionary validation since there may be too many samples for cross-validating\n+    @Override\n+    public boolean requiresReference() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void onTraversalStart() {\n+        sampleDB = initializeSampleDB();\n+\n+        if (sampleDB == null) {\n+            logger.warn(\"No pedigree file supplied for sex genotype calls. Will attempt to infer ploidy from segments VCF genotype ploidy.\");  //note that this only works if there are called variants on the sex chromosomes\n+        }\n+\n+        final Collection<String> samples = getSamplesForVariants();\n+        if (samples != null) {\n+            for (String sample : samples) {\n+                if (sampleDB.getSample(sample) == null) {\n+                    logger.warn(\"Sample \" + sample + \" is missing from the supplied pedigree file. Will attempt to infer ploidy from segments VCF genotype ploidy.\"); //note that this only works if there are called variants on the sex chromosomes\n+                }\n+            }\n+        }\n+\n+\n+        dictionary = getBestAvailableSequenceDictionary();\n+        //dictionary will not be null because this tool requiresReference()\n+\n+        final GenomeLocParser parser = new GenomeLocParser(this.dictionary);\n+\n+        if (modelCallIntervalList == null) {\n+            callIntervals = null;\n+        } else {\n+        final List<GenomeLoc> inputCoverageIntervals = IntervalUtils.featureFileToIntervals(parser, modelCallIntervalList.getAbsolutePath());\n+        final List<GenomeLoc> inputTraversalIntervals = IntervalUtils.genomeLocsFromLocatables(parser,getTraversalIntervals());\n+            callIntervals = IntervalUtils.mergeListsBySetOperator(inputCoverageIntervals, inputTraversalIntervals, IntervalSetRule.INTERSECTION);\n+        }\n+\n+        defragmenter = new SVDepthOnlyCallDefragmenter(dictionary, 0.8, callIntervals);\n+        clusterEngine = new SVClusterEngine(dictionary, true, breakpointSummaryStrategy);\n+\n+        vcfWriter = getVCFWriter();\n+    }\n+\n+    //TODO: this is the third copy of this method\n+    /**\n+     * Entry-point function to initialize the samples database from input data\n+     */\n+    private SampleDB initializeSampleDB() {\n+        final SampleDBBuilder sampleDBBuilder = new SampleDBBuilder(PedigreeValidationType.STRICT);  //strict will warn about missing samples\n+        if (pedigreeFile != null) {\n+            sampleDBBuilder.addSamplesFromPedigreeFiles(Collections.singletonList(pedigreeFile));\n+        }\n+        return sampleDBBuilder.getFinalSampleDB();\n+    }\n+\n+    private VariantContextWriter getVCFWriter() {\n+        samples = getSamplesForVariants();\n+\n+        final VCFHeader inputVCFHeader = new VCFHeader(getHeaderForVariants().getMetaDataInInputOrder(), samples);\n+\n+        final Set<VCFHeaderLine> headerLines = new LinkedHashSet<>(inputVCFHeader.getMetaDataInInputOrder());\n+        headerLines.addAll(getDefaultToolVCFHeaderLines());\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVLEN));\n+        headerLines.add(GATKSVVCFHeaderLines.getInfoLine(GATKSVVCFConstants.SVTYPE));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_FREQUENCY_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_COUNT_KEY));\n+        headerLines.add(VCFStandardHeaderLines.getInfoLine(VCFConstants.ALLELE_NUMBER_KEY));\n+\n+        VariantContextWriter writer = createVCFWriter(outputFile);\n+\n+        final Set<String> sampleNameSet = new IndexedSampleList(samples).asSetOfSamples();\n+        final VCFHeader vcfHeader = new VCFHeader(headerLines, new TreeSet<>(sampleNameSet));\n+        writer.writeHeader(vcfHeader);\n+\n+        return writer;\n+    }\n+\n+    /**\n+     * @param variantContexts  VariantContexts from driving variants with matching start position\n+     *                         NOTE: This will never be empty\n+     * @param referenceContext ReferenceContext object covering the reference of the longest spanning VariantContext\n+     * @param readsContexts\n+     */\n+    @Override\n+    public void apply(List<VariantContext> variantContexts, ReferenceContext referenceContext, List<ReadsContext> readsContexts) {\n+        if (currentContig == null) {\n+            currentContig = variantContexts.get(0).getContig(); //variantContexts should have identical start, so choose 0th arbitrarily\n+        } else if (!variantContexts.get(0).getContig().equals(currentContig)) {\n+            processClusters();\n+            currentContig = variantContexts.get(0).getContig();\n+        }\n+        for (final VariantContext vc : variantContexts) {\n+            if (vc.getGenotypes().size() != 1) {\n+                oneShotLogger.warn(\"Multi-sample VCFs found, which are assumed to be pre-clustered. Skipping defragmentation.\");\n+                doDefragmentation = false;\n+            } else {\n+                doDefragmentation = true;\n+            }\n+            final SVCallRecord record = SVCallRecord.createDepthOnlyFromGCNV(vc, minQS);\n+            if (record != null) {\n+                if (doDefragmentation) {\n+                    defragmenter.add(new SVCallRecordWithEvidence(record));\n+                } else {\n+                    clusterEngine.add(new SVCallRecordWithEvidence(record));\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public Object onTraversalSuccess() {\n+        processClusters();\n+        return null;\n+    }\n+\n+    private void processClusters() {\n+        if (!defragmenter.isEmpty()) {\n+            final List<SVCallRecordWithEvidence> defragmentedCalls = defragmenter.getOutput();\n+            defragmentedCalls.stream().forEachOrdered(clusterEngine::add);\n+        }\n+        //Jack and Isaac cluster first and then defragment\n+        final List<SVCallRecordWithEvidence> clusteredCalls = clusterEngine.getOutput();\n+        write(clusteredCalls);\n+    }\n+\n+    private void write(final List<SVCallRecordWithEvidence> calls) {\n+        final ReferenceSequenceFile reference = ReferenceUtils.createReferenceReader(referenceArguments.getReferenceSpecifier());\n+        final List<VariantContext> sortedCalls = calls.stream()\n+                .sorted(Comparator.comparing(c -> new SimpleInterval(c.getContig(), c.getStart(), c.getEnd()), //VCs have to be sorted by end as well\n+                        IntervalUtils.getDictionaryOrderComparator(dictionary)))\n+                .map(record -> buildVariantContext(record, reference))\n+                .collect(Collectors.toList());\n+        Iterator<VariantContext> it = sortedCalls.iterator();\n+        ArrayList<VariantContext> overlappingVCs = new ArrayList<>();\n+        if (!it.hasNext()) {\n+            return;\n+        }\n+        VariantContext prev = it.next();\n+        overlappingVCs.add(prev);\n+        int clusterEnd = prev.getEnd();\n+        String clusterContig = prev.getContig();\n+        //gather groups of overlapping VCs and update the genotype copy numbers appropriately\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            if (curr.getStart() < clusterEnd && curr.getContig().equals(clusterContig)) {\n+                overlappingVCs.add(curr);\n+                if (curr.getEnd() > clusterEnd) {\n+                    clusterEnd = curr.getEnd();\n+                }\n+            } else {\n+                final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+                for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+                overlappingVCs = new ArrayList<>();\n+                overlappingVCs.add(curr);\n+                clusterEnd = curr.getEnd();\n+                clusterContig = curr.getContig();\n+            }\n+        }\n+        //write out the last set of overlapping VCs\n+        final List<VariantContext> resolvedVCs = resolveVariantContexts(overlappingVCs);\n+        for (final VariantContext vc : resolvedVCs) { vcfWriter.add(vc); }\n+    }\n+\n+    /**\n+     * Ensure genotype calls are consistent for overlapping variant contexts\n+     * Note that we assume that a sample will not occur twice with the same copy number because it should have been defragmented\n+     * @param overlappingVCs\n+     * @return\n+     */\n+    private List<VariantContext> resolveVariantContexts(final List<VariantContext> overlappingVCs) {\n+        Utils.nonNull(overlappingVCs);\n+        final List<VariantContext> resolvedVCs = new ArrayList<>();\n+        final Iterator<VariantContext> it = overlappingVCs.iterator();\n+\n+        final Map<String, MutablePair<Integer, Integer>> sampleCopyNumbers = new LinkedHashMap<>();  //sampleName, copyNumber, endPos -- it's safe to just use position because if the VCs overlap then they must be on the same contig\n+        while (it.hasNext()) {\n+            final VariantContext curr = it.next();\n+            for (final Genotype g : curr.getGenotypes()) {\n+                //if this sample is in the table and we have a new variant for this sample, update the table\n+                final String s = g.getSampleName();\n+                if (sampleCopyNumbers.containsKey(s) && sampleCopyNumbers.get(s).getLeft() != Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString())) {\n+                    sampleCopyNumbers.get(s).setRight(Integer.parseInt(g.getExtendedAttribute(GATKSVVCFConstants.COPY_NUMBER_FORMAT).toString()));", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ4NDA3NA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r534484074", "bodyText": "You're totally right, but that map doesn't get used until these values are overwritten by some almost redundant but correct code.  I took this block out.", "author": "ldgauthier", "createdAt": "2020-12-02T21:11:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI0Njg4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI1MjI3OA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527252278", "bodyText": "Suggested edit:\n        //if we supply a breakpoints file, then allow overlapping segments\n        final AbstractRecordCollection integerCopyNumberSegmentCollection;\n        final String sampleNameFromSegmentCollection;\n        if (clusteredBreakpointsVCFFile == null) {\n            final IntegerCopyNumberSegmentCollection collection = new IntegerCopyNumberSegmentCollection(copyNumberSegmentsFile);\n            sampleNameFromSegmentCollection = collection.getMetadata().getSampleName();\n            integerCopyNumberSegmentCollection = collection;\n        } else {\n            final OverlappingIntegerCopyNumberSegmentCollection collection = new OverlappingIntegerCopyNumberSegmentCollection(copyNumberSegmentsFile);\n            sampleNameFromSegmentCollection = collection.getMetadata().getSampleName();\n            integerCopyNumberSegmentCollection = collection;\n        }\n        Utils.validate(sampleNameFromSegmentCollection.equals(sampleName),\n                String.format(\"Sample name found in the header of copy-number segments file is \" +\n                                \"different from the expected sample name (found: %s, expected: %s).\",\n                        sampleNameFromSegmentCollection, sampleName));\n...\n        germlineCNVSegmentVariantComposer.writeAll(integerCopyNumberSegmentCollection.getRecords());", "author": "mwalker174", "createdAt": "2020-11-19T22:40:43Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java", "diffHunk": "@@ -387,36 +442,56 @@ private void generateIntervalsVCFFileFromAllShards() {\n     }\n \n     private void generateSegmentsVCFFileFromAllShards() {\n-        logger.info(\"Generating segments VCF file...\");\n+        logger.info(\"Generating segments...\");\n \n         /* perform segmentation */\n         final File pythonScriptOutputPath = IOUtils.createTempDir(\"gcnv-segmented-calls\");\n         final boolean pythonScriptSucceeded = executeSegmentGermlineCNVCallsPythonScript(\n                 sampleIndex, inputContigPloidyCallsPath, sortedCallsShardPaths, sortedModelShardPaths,\n-                pythonScriptOutputPath);\n+                combinedIntervalsVCFFile, clusteredBreakpointsVCFFile, pythonScriptOutputPath);\n         if (!pythonScriptSucceeded) {\n             throw new UserException(\"Python return code was non-zero.\");\n         }\n \n         /* parse segments */\n+        logger.info(\"Parsing Python output...\");\n         final File copyNumberSegmentsFile = getCopyNumberSegmentsFile(pythonScriptOutputPath, sampleIndex);\n-        final IntegerCopyNumberSegmentCollection integerCopyNumberSegmentCollection =\n-                new IntegerCopyNumberSegmentCollection(copyNumberSegmentsFile);\n-        final String sampleNameFromSegmentCollection = integerCopyNumberSegmentCollection\n-                .getMetadata().getSampleName();\n-        Utils.validate(sampleNameFromSegmentCollection.equals(sampleName),\n-                String.format(\"Sample name found in the header of copy-number segments file is \" +\n-                                \"different from the expected sample name (found: %s, expected: %s).\",\n-                        sampleNameFromSegmentCollection, sampleName));\n+\n+        final List<IntegerCopyNumberSegment> records;\n+        //if we supply a breakpoints file, then allow overlapping segments\n+        if (clusteredBreakpointsVCFFile == null) {\n+            final IntegerCopyNumberSegmentCollection integerCopyNumberSegmentCollection\n+                    = new IntegerCopyNumberSegmentCollection(copyNumberSegmentsFile);\n+            final String sampleNameFromSegmentCollection = integerCopyNumberSegmentCollection\n+                    .getMetadata().getSampleName();\n+            Utils.validate(sampleNameFromSegmentCollection.equals(sampleName),\n+                    String.format(\"Sample name found in the header of copy-number segments file is \" +\n+                                    \"different from the expected sample name (found: %s, expected: %s).\",\n+                            sampleNameFromSegmentCollection, sampleName));\n+            records = integerCopyNumberSegmentCollection.getRecords();\n+        } else {\n+            final OverlappingIntegerCopyNumberSegmentCollection integerCopyNumberSegmentCollection\n+                    = new OverlappingIntegerCopyNumberSegmentCollection(copyNumberSegmentsFile);\n+            final String sampleNameFromSegmentCollection = integerCopyNumberSegmentCollection\n+                    .getMetadata().getSampleName();\n+            Utils.validate(sampleNameFromSegmentCollection.equals(sampleName),\n+                    String.format(\"Sample name found in the header of copy-number segments file is \" +\n+                                    \"different from the expected sample name (found: %s, expected: %s).\",", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI1NDU4OA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527254588", "bodyText": "Can you break both lambdas out into their own functions?", "author": "mwalker174", "createdAt": "2020-11-19T22:45:37Z", "path": "src/main/java/org/broadinstitute/hellbender/engine/MultiVariantDataSource.java", "diffHunk": "@@ -284,23 +288,24 @@ private void validateAllSequenceDictionaries() {\n                 if (dictionary == null) {\n                     logger.warn(\n                             \"A sequence dictionary is required for each input when using multiple inputs, and one could\" +\n-                            \" not be obtained for feature input: \" + ds.getName() +\n-                            \". The input may not exist or may not have a valid header\");\n+                                    \" not be obtained for feature input: \" + ds.getName() +\n+                                    \". The input may not exist or may not have a valid header\");\n                 } else {\n+                    //This is HORRIFICALLY inefficient and is going to bite me when we do large cohorts\n                     dictionary.getSequences().forEach(\n-                        sourceSequence -> {\n-                            final String sourceSequenceName = sourceSequence.getSequenceName();\n-                            final FeatureDataSource<VariantContext> previousDataSource = contigMap.getOrDefault(sourceSequenceName, null);\n-                            if (previousDataSource != null) {\n-                                final SAMSequenceDictionary previousDictionary = previousDataSource.getSequenceDictionary();\n-                                final SAMSequenceRecord previousSequence = previousDictionary.getSequence(sourceSequenceName);\n-                                validateSequenceDictionaryRecords(\n-                                        ds.getName(), dictionary, sourceSequence,\n-                                        previousDataSource.getName(), previousDictionary, previousSequence);\n-                            } else {\n-                                contigMap.put(sourceSequenceName, ds);\n+                            sourceSequence -> {", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI2MDM5Mg==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527260392", "bodyText": "It does seem complicated. Do you know how it affects runtime now?", "author": "mwalker174", "createdAt": "2020-11-19T22:58:14Z", "path": "src/main/java/org/broadinstitute/hellbender/engine/MultiVariantDataSource.java", "diffHunk": "@@ -284,23 +288,24 @@ private void validateAllSequenceDictionaries() {\n                 if (dictionary == null) {\n                     logger.warn(\n                             \"A sequence dictionary is required for each input when using multiple inputs, and one could\" +\n-                            \" not be obtained for feature input: \" + ds.getName() +\n-                            \". The input may not exist or may not have a valid header\");\n+                                    \" not be obtained for feature input: \" + ds.getName() +\n+                                    \". The input may not exist or may not have a valid header\");\n                 } else {\n+                    //This is HORRIFICALLY inefficient and is going to bite me when we do large cohorts", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjg3MTA4NA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r532871084", "bodyText": "200 samples takes like 20 minutes to validate the sequence dictionaries.  If we require a reference and skip cross-validation (i.e. just validate each against the reference), then it's negligible.", "author": "ldgauthier", "createdAt": "2020-11-30T20:05:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI2MDM5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQxNTA0Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r534415043", "bodyText": "Yikes! This is okay as long as the ref is provided in our WDLs.", "author": "mwalker174", "createdAt": "2020-12-02T19:09:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI2MDM5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI2MzkwMw==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r527263903", "bodyText": "File inputs should be GATKPaths (also in JointGermlineCNVSegmentation)", "author": "mwalker174", "createdAt": "2020-11-19T23:05:59Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/copynumber/PostprocessGermlineCNVCalls.java", "diffHunk": "@@ -171,6 +179,20 @@\n     )\n     private List<String> allosomalContigList;\n \n+    @Argument(\n+            doc = \"Input VCF with combined intervals for all samples\",\n+            fullName = INPUT_INTERVALS_LONG_NAME,\n+            optional = true\n+    )\n+    private File combinedIntervalsVCFFile = null;", "originalCommit": "62b655c720bdea7621392a194861c8366a641fae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU5MDQ2OA==", "url": "https://github.com/broadinstitute/gatk/pull/6554#discussion_r535590468", "bodyText": "Since this is going to go to the underlying Python script as-is, I think it has to stay a File right?  I suspect it will crash and burn if it gets a GCS path.", "author": "ldgauthier", "createdAt": "2020-12-03T20:45:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI2MzkwMw=="}], "type": "inlineReview"}, {"oid": "61759e542a67b74ae02f728f6ca1cbac3ba0a857", "url": "https://github.com/broadinstitute/gatk/commit/61759e542a67b74ae02f728f6ca1cbac3ba0a857", "message": "More integration tests and other review responses", "committedDate": "2020-12-08T21:13:29Z", "type": "forcePushed"}, {"oid": "c5dc7c5e0cad8aa80ac14734eb7f60ead3b9cfb8", "url": "https://github.com/broadinstitute/gatk/commit/c5dc7c5e0cad8aa80ac14734eb7f60ead3b9cfb8", "message": "Add clustering code (from mw_gatk_sv_v2) and modify for gCNV exome joint calling\ngCNV joint calling improvements:\nUse call intervals for bin-space defragmentation\nAdjust copy number for overlapping events (not super efficient)\nDiploid genotypes and actually get ref base (if reference is supplied)\nQS filtering and AC calculation\nWith bcftools fast combine and filtering and annotation\nFilter by raw calls and filtered calls\nUpdate VC alleles if we update genotype\nFail if X contig names don't match up\nNew Python unit test runner", "committedDate": "2020-12-22T18:58:48Z", "type": "forcePushed"}, {"oid": "b7175045562c85450dedbc8ecb909edf4bb1b4c6", "url": "https://github.com/broadinstitute/gatk/commit/b7175045562c85450dedbc8ecb909edf4bb1b4c6", "message": "Add clustering code (from mw_gatk_sv_v2) and modify for gCNV exome joint calling", "committedDate": "2020-12-22T20:54:33Z", "type": "commit"}, {"oid": "c41239987b118546daa78533256216fcee3378c0", "url": "https://github.com/broadinstitute/gatk/commit/c41239987b118546daa78533256216fcee3378c0", "message": "gCNV joint calling improvements:\nUse call intervals for bin-space defragmentation\nAdjust copy number for overlapping events (not super efficient)\nDiploid genotypes and actually get ref base (if reference is supplied)\nQS filtering and AC calculation\nFilter by raw calls and filtered calls\nNew Python unit test runner", "committedDate": "2020-12-22T20:54:36Z", "type": "commit"}, {"oid": "c41239987b118546daa78533256216fcee3378c0", "url": "https://github.com/broadinstitute/gatk/commit/c41239987b118546daa78533256216fcee3378c0", "message": "gCNV joint calling improvements:\nUse call intervals for bin-space defragmentation\nAdjust copy number for overlapping events (not super efficient)\nDiploid genotypes and actually get ref base (if reference is supplied)\nQS filtering and AC calculation\nFilter by raw calls and filtered calls\nNew Python unit test runner", "committedDate": "2020-12-22T20:54:36Z", "type": "forcePushed"}]}