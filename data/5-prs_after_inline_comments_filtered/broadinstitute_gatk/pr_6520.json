{"pr_number": 6520, "pr_title": "Make adaptive pruner smarter in complex graphs", "pr_createdAt": "2020-03-23T18:01:04Z", "pr_url": "https://github.com/broadinstitute/gatk/pull/6520", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc0MTg5OA==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r396741898", "bodyText": "So my understanding, now we are saving all chains with a MAX edge weight of (graph max / 10) and then growing outwards from there based on the same log odds ratio we had before correct? Except now we are demanding that at least one edge stems from both an already blessed \"good\" edge instead of both edges needing to satisfy the log  odds test. This seems reasonable I think but I want to think a little more on it. Have some comments I noticed. I will give this a try on haplotype caller.", "author": "jamesemery", "createdAt": "2020-03-23T20:38:26Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +30,73 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+\n+        // start from set of really obvious good chains, defined as those with an edge with multiplicity above some fraction of the", "originalCommit": "b94426887d6d2c0b7ab8fbb20f78b8cb1bdcc035", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc0MzIzOQ==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r396743239", "bodyText": "Parameterize the \"10\". Also I would make the comment that the maxEdgeWeight / 10 will probably falter somewhat around homopolymers in the graph, as a 40 base homopolomyer at 25mer weight without any issues in coverage should have (40 - 25) or 15x the coverage of any of the sequence around it. This doesn't matter for the old graph code but it might for the new graph code and I suspect this will call everything \"bad\" and either be very slow or possibly throw almost everything away.", "author": "jamesemery", "createdAt": "2020-03-23T20:40:59Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +30,73 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+\n+        // start from set of really obvious good chains, defined as those with an edge with multiplicity above some fraction of the\n+        // maximum multiplicity edge.  Then grow the set of good chains by looking at the log odds of junctions between\n+        // good chains and unknown chains.\n+        final int maxEdgeWeight = graph.edgeSet().stream().mapToInt(BaseEdge::getMultiplicity).max().orElse(0);\n+        final int thresholdEdgeWeight = maxEdgeWeight / 10;", "originalCommit": "b94426887d6d2c0b7ab8fbb20f78b8cb1bdcc035", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc0MzM0Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r396743346", "bodyText": "Why not recover reference chains here?", "author": "jamesemery", "createdAt": "2020-03-23T20:41:12Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +30,73 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+\n+        // start from set of really obvious good chains, defined as those with an edge with multiplicity above some fraction of the\n+        // maximum multiplicity edge.  Then grow the set of good chains by looking at the log odds of junctions between\n+        // good chains and unknown chains.\n+        final int maxEdgeWeight = graph.edgeSet().stream().mapToInt(BaseEdge::getMultiplicity).max().orElse(0);\n+        final int thresholdEdgeWeight = maxEdgeWeight / 10;\n+        final Set<Path<V, E>> definiteGoodChains = chains.stream()\n+                .filter(chain -> chain.getEdges().stream().mapToInt(BaseEdge::getMultiplicity).max().orElse(0) > thresholdEdgeWeight)", "originalCommit": "b94426887d6d2c0b7ab8fbb20f78b8cb1bdcc035", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc0NDIwNQ==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r396744205", "bodyText": "Put a cap on the number of iterations here. I'm worried this might loop functionally forever on very pathological/crazy high coverage sites with lots of chains and a high disparity of depths.", "author": "jamesemery", "createdAt": "2020-03-23T20:42:56Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +30,73 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+\n+        // start from set of really obvious good chains, defined as those with an edge with multiplicity above some fraction of the\n+        // maximum multiplicity edge.  Then grow the set of good chains by looking at the log odds of junctions between\n+        // good chains and unknown chains.\n+        final int maxEdgeWeight = graph.edgeSet().stream().mapToInt(BaseEdge::getMultiplicity).max().orElse(0);\n+        final int thresholdEdgeWeight = maxEdgeWeight / 10;\n+        final Set<Path<V, E>> definiteGoodChains = chains.stream()\n+                .filter(chain -> chain.getEdges().stream().mapToInt(BaseEdge::getMultiplicity).max().orElse(0) > thresholdEdgeWeight)\n+                .collect(Collectors.toSet());\n+\n+        final Set<V> terminalVerticesOfGoodChains = definiteGoodChains.stream()\n+                .flatMap(c -> Stream.of(c.getFirstVertex(), c.getLastVertex())).collect(Collectors.toSet());\n+\n+        // initialize result to all chains that aren't definitely good and iteratively reduce this set as chains in\n+        // question are found to have good log odds at junctions with good chains\n+        final Set<Path<V,E>> errorChains = chains.stream().filter(c -> !definiteGoodChains.contains(c)).collect(Collectors.toSet());\n+\n+        // we only need to calculate log odds for questionable chains\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = errorChains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        while (true) {", "originalCommit": "b94426887d6d2c0b7ab8fbb20f78b8cb1bdcc035", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc1MTkxNg==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r396751916", "bodyText": "On second thought, i think there might be a more efficient way to do this. If you preprocess every chain into a map of end vertex-> list of adjacent chains and then just poll them in a queue looking at each vertex exactly once (or putting it into the queue if we discover a new vertex from our processing) i think should avoid the problem of iterating over the same gazillion error chains forever and only ever making progress on one or two vertices at a time. Indeed it seems to me that once a vertex is \"good\" we need only check its incoming and outgoing neighbors exactly once and then we can skip ever looking at its neighboring vertexes ever again.", "author": "jamesemery", "createdAt": "2020-03-23T20:57:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc0NDIwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc0NTk0NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r396745945", "bodyText": "I'm worried about this sort, it seems to me that the LogOdds would have discrete values (since they are based on comparing counts that are descrete) so it seems likely there would be a tie here. Without a tiebreaker this seems prone to non-deterministic behavior. I would recommend either putting a relatively bulletproof set of tiebreakers in here or making all of the underlying sets Linked.", "author": "jamesemery", "createdAt": "2020-03-23T20:46:23Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +30,73 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+\n+        // start from set of really obvious good chains, defined as those with an edge with multiplicity above some fraction of the\n+        // maximum multiplicity edge.  Then grow the set of good chains by looking at the log odds of junctions between\n+        // good chains and unknown chains.\n+        final int maxEdgeWeight = graph.edgeSet().stream().mapToInt(BaseEdge::getMultiplicity).max().orElse(0);\n+        final int thresholdEdgeWeight = maxEdgeWeight / 10;\n+        final Set<Path<V, E>> definiteGoodChains = chains.stream()\n+                .filter(chain -> chain.getEdges().stream().mapToInt(BaseEdge::getMultiplicity).max().orElse(0) > thresholdEdgeWeight)\n+                .collect(Collectors.toSet());\n+\n+        final Set<V> terminalVerticesOfGoodChains = definiteGoodChains.stream()\n+                .flatMap(c -> Stream.of(c.getFirstVertex(), c.getLastVertex())).collect(Collectors.toSet());\n+\n+        // initialize result to all chains that aren't definitely good and iteratively reduce this set as chains in\n+        // question are found to have good log odds at junctions with good chains\n+        final Set<Path<V,E>> errorChains = chains.stream().filter(c -> !definiteGoodChains.contains(c)).collect(Collectors.toSet());\n+\n+        // we only need to calculate log odds for questionable chains\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = errorChains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        while (true) {\n+            final Set<Path<V, E>> moreGoodChains = new HashSet<>();\n+\n+            for (final Path<V, E> chain : errorChains) {\n+                if (terminalVerticesOfGoodChains.contains(chain.getFirstVertex()) && chainLogOdds.get(chain).getLeft() > logOddsThreshold) {\n+                    moreGoodChains.add(chain);\n+                    terminalVerticesOfGoodChains.add(chain.getLastVertex());\n+                } else if (terminalVerticesOfGoodChains.contains(chain.getLastVertex()) && chainLogOdds.get(chain).getRight() > logOddsThreshold) {\n+                    moreGoodChains.add(chain);\n+                    terminalVerticesOfGoodChains.add(chain.getFirstVertex());\n+                }\n+            }\n \n-        chainLogOdds.forEach((chain, lod) -> {\n-            if (lod < logOddsThreshold) {\n-                result.add(chain);\n+            if (moreGoodChains.isEmpty()) {\n+                break;\n+            } else {\n+                errorChains.removeAll(moreGoodChains);\n             }\n-        });\n+        }\n \n-        chains.stream().filter(c -> isChainPossibleVariant(c, graph))\n-                .sorted(Comparator.comparingDouble((ToDoubleFunction<Path<V, E>>) chainLogOdds::get)\n-                        .reversed().thenComparingInt(Path::length))\n+        // add non-error chains to error chains if maximum number of variants has been exceeded\n+        chains.stream()", "originalCommit": "b94426887d6d2c0b7ab8fbb20f78b8cb1bdcc035", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTgzMjI2Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r449832266", "bodyText": "done using your vertex lexicographical ordering suggestion as below", "author": "davidbenjamin", "createdAt": "2020-07-05T04:16:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc0NTk0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc0Nzg3Nw==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r396747877", "bodyText": "I would recommend including a test where the reference has \"badMultiplicity\" and making sure that we aren't accidentally pruning that and causing potential problems.", "author": "jamesemery", "createdAt": "2020-03-23T20:50:13Z", "path": "src/test/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/ChainPrunerUnitTest.java", "diffHunk": "@@ -175,6 +176,47 @@ public void testAdaptivePruning(final int kmerSize, final byte[] ref, final byte\n         Assert.assertTrue(bestPaths.size() < 15);\n     }\n \n+    // test that in graph with good path A -> B -> C and bad edges A -> D -> C and D -> B that the adjacency of bad edges --\n+    // such that when bad edges meet the multiplicities do not indicate an error - does not harm pruning.\n+    // we test with and without a true variant path A -> E -> C\n+    @Test\n+    public void testAdaptivePruningWithAdjacentBadEdges() {", "originalCommit": "b94426887d6d2c0b7ab8fbb20f78b8cb1bdcc035", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk3NTgxOA==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r449975818", "bodyText": "Done by modifying testAdaptivePruning and its data provider.", "author": "davidbenjamin", "createdAt": "2020-07-06T04:19:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc0Nzg3Nw=="}], "type": "inlineReview"}, {"oid": "abcd704a9d283548eba0d1186754fbfce636ad25", "url": "https://github.com/broadinstitute/gatk/commit/abcd704a9d283548eba0d1186754fbfce636ad25", "message": "Improve adaptive pruner to be smarter about adjacent chains with errors", "committedDate": "2020-04-30T05:29:05Z", "type": "forcePushed"}, {"oid": "908b7f35f3080265c583b72ef0fa23c0e6925347", "url": "https://github.com/broadinstitute/gatk/commit/908b7f35f3080265c583b72ef0fa23c0e6925347", "message": "Improve adaptive pruner to be smarter about adjacent chains with errors", "committedDate": "2020-04-30T05:33:35Z", "type": "forcePushed"}, {"oid": "5806cefcc10c6073fc770c5dfebbde0ba787e28b", "url": "https://github.com/broadinstitute/gatk/commit/5806cefcc10c6073fc770c5dfebbde0ba787e28b", "message": "Improve adaptive pruner to be smarter about adjacent chains with errors", "committedDate": "2020-04-30T05:39:18Z", "type": "forcePushed"}, {"oid": "5f399678db389bd479d253bc5abf19514941330f", "url": "https://github.com/broadinstitute/gatk/commit/5f399678db389bd479d253bc5abf19514941330f", "message": "Improve adaptive pruner to be smarter about adjacent chains with errors", "committedDate": "2020-04-30T05:53:04Z", "type": "forcePushed"}, {"oid": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3", "url": "https://github.com/broadinstitute/gatk/commit/3e3838fad1a76dbe18830d2e4166c6f76d6453c3", "message": "Improve adaptive pruner to be smarter about adjacent chains with errors", "committedDate": "2020-05-04T20:39:19Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc0NDcyNQ==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r434744725", "bodyText": "This is good, I think this resolves my previous concern about potentially pruning ref edges. I still think it would be prudent to adapt the test you have below to include a reference path that could/should be pruned otherwise just to be sure.", "author": "jamesemery", "createdAt": "2020-06-03T17:44:04Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +33,105 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());", "originalCommit": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk3NTU1Mw==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r449975553", "bodyText": "Done by checking that ref path is assembled in testAdaptivePruning and by giving it's data provider a case where altFraction is 1.0", "author": "davidbenjamin", "createdAt": "2020-07-06T04:17:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc0NDcyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc1NTA3NA==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r434755074", "bodyText": "Interesting, I do think you are right that this solves the problem of drastically differing edge weights being commonplace.  I am a little concerned though, it seems that having no ties whatsoever to the depth of good/reference will run the risk of graphs that look something like this rescuing edges:\n........................../ (1x coverage) B ---\n........................./ (1xcoverage)  A -----\\\n......................../ (1x coverage)...............\nREF SOURCE / ---- (100 coverage) C ------REFSink\nUnder your last proposal we would only have selected C or ref sinks as seeds and then we would have thrown away every other path, whereas now its possible (certain?) that both A and B would initially be marked as seed vertexes and consequently we would still end up recovering those paths. That seems problematic, as i'm sure there are other strange edge cases where low weight error kmers \"rescue\" each-other.  Still though I think this fixes a bunch of the cases.", "author": "jamesemery", "createdAt": "2020-06-03T18:01:36Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +33,105 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+        /////// NEW\n+\n+        // pre-compute the left and right log odds of each chain\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = chains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        // compute correspondence of vertices to incident chains with log odds above the seeding and extending thresholds\n+        final Multimap<V, Path<V,E>> vertexToSeedableChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodIncomingChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodOutgoingChains = ArrayListMultimap.create();\n+\n+        for (final Path<V,E> chain : chains) {\n+            if (chainLogOdds.get(chain).getRight() >= logOddsThreshold) {\n+                vertexToGoodIncomingChains.put(chain.getLastVertex(), chain);\n+            }\n+\n+            if (chainLogOdds.get(chain).getLeft() >= logOddsThreshold) {\n+                vertexToGoodOutgoingChains.put(chain.getFirstVertex(), chain);\n+            }\n+\n+            // seed-worthy chains must pass the more stringent seeding log odds threshold on both sides", "originalCommit": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2MDQ2Nw==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r434760467", "bodyText": "I guess part of the question is about the LOD score and how its computed, perhaps we could harden ourselves to some of these edge cases if it weren't the case that a bubble where both ends are anchored by 1 base verses an alternative path that is also anchored by 1 base. Perhaps by weighting against edges with below some minimum of coverage anyway.", "author": "jamesemery", "createdAt": "2020-06-03T18:11:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc1NTA3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA3MDg3NQ==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r449070875", "bodyText": "This danger is averted by the seedingLogOddsThreshold.  If your incoming and outgoing weights are both very low the log odds will be close to zero.  That is, not enough evidence to decisively reject the edge with the regular chain threshold, but also not enough evidence to count it as a seed.", "author": "davidbenjamin", "createdAt": "2020-07-02T15:09:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc1NTA3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2NDA5NA==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r434764094", "bodyText": "I wouldn't use Path::hashCode to break ties here as the underlying Path objects are LinkedLists with the java object hash function which is based on the memory address for the object and consequently could still vary from run to run. If you want to do something quick and dirty you could use the lexicographic ordering for the first kmer in the chain (which should never match (well maybe they could in the edge case where we duplicate repeated kmers...)).", "author": "jamesemery", "createdAt": "2020-06-03T18:17:41Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +33,105 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+        /////// NEW\n+\n+        // pre-compute the left and right log odds of each chain\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = chains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        // compute correspondence of vertices to incident chains with log odds above the seeding and extending thresholds\n+        final Multimap<V, Path<V,E>> vertexToSeedableChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodIncomingChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodOutgoingChains = ArrayListMultimap.create();\n+\n+        for (final Path<V,E> chain : chains) {\n+            if (chainLogOdds.get(chain).getRight() >= logOddsThreshold) {\n+                vertexToGoodIncomingChains.put(chain.getLastVertex(), chain);\n+            }\n+\n+            if (chainLogOdds.get(chain).getLeft() >= logOddsThreshold) {\n+                vertexToGoodOutgoingChains.put(chain.getFirstVertex(), chain);\n+            }\n+\n+            // seed-worthy chains must pass the more stringent seeding log odds threshold on both sides\n+            // in addition to that, we only seed from vertices with multiple such chains incoming or outgoing (see below)\n+            if (chainLogOdds.get(chain).getRight() >= seedingLogOddsThreshold && chainLogOdds.get(chain).getLeft() >= seedingLogOddsThreshold) {\n+                vertexToSeedableChains.put(chain.getFirstVertex(), chain);\n+                vertexToSeedableChains.put(chain.getLastVertex(), chain);\n+            }\n+        }\n+\n+        // find a subset of good vertices from which to grow the subgraph of good chains.\n+        final Queue<V> verticesToProcess = new ArrayDeque<>();\n+        final Path<V,E> maxWeightChain = getMaxWeightChain(chains);\n+        verticesToProcess.add(maxWeightChain.getFirstVertex());\n+        verticesToProcess.add(maxWeightChain.getLastVertex());\n+\n+        // look for vertices with two incoming or two outgoing chains (plus one outgoing or incoming for a total of 3 or more) with good log odds to seed the subgraph of good vertices\n+        // the logic here is that a high-multiplicity error chain A that branches into a second error chain B and a continuation-of-the-original-error chain A'\n+        // may have a high log odds for A'.  However, only in the case of true variation will  multiple branches leaving the same vertex have good log odds.\n+        vertexToSeedableChains.keySet().stream()\n+                .filter(v -> vertexToSeedableChains.get(v).size() > 2)\n+                .forEach(verticesToProcess::add);\n+\n+        final Set<V> processedVertices = new HashSet<>();\n+        final Set<Path<V,E>> goodChains = new HashSet<>();\n+\n+        // starting from the high-confidence seed vertices, grow the \"good\" subgraph along chains with above-threshold log odds,\n+        // discovering good chains as we go.\n+        while (!verticesToProcess.isEmpty()) {\n+            final V vertex = verticesToProcess.poll();\n+            processedVertices.add(vertex);\n+            for (final Path<V,E> outgoingChain : vertexToGoodOutgoingChains.get(vertex)) {\n+                goodChains.add(outgoingChain);\n+                if(!processedVertices.contains(outgoingChain.getLastVertex())) {\n+                    verticesToProcess.add(outgoingChain.getLastVertex());\n+                }\n+            }\n \n-        chainLogOdds.forEach((chain, lod) -> {\n-            if (lod < logOddsThreshold) {\n-                result.add(chain);\n+            for (final Path<V,E> incomingChain : vertexToGoodIncomingChains.get(vertex)) {\n+                goodChains.add(incomingChain);\n+                if(!processedVertices.contains(incomingChain.getFirstVertex())) {\n+                    verticesToProcess.add(incomingChain.getFirstVertex());\n+                }\n             }\n-        });\n+        }\n+\n+        final Set<Path<V,E>> errorChains = chains.stream().filter(c -> !goodChains.contains(c)).collect(Collectors.toSet());\n \n-        chains.stream().filter(c -> isChainPossibleVariant(c, graph))\n-                .sorted(Comparator.comparingDouble((ToDoubleFunction<Path<V, E>>) chainLogOdds::get)\n-                        .reversed().thenComparingInt(Path::length))\n+        // add non-error chains to error chains if maximum number of variants has been exceeded\n+        chains.stream()\n+                .filter(c -> !errorChains.contains(c))\n+                .filter(c -> isChainPossibleVariant(c, graph))\n+                .sorted(Comparator.comparingDouble(c -> Math.min(chainLogOdds.get(c).getLeft(), chainLogOdds.get(c).getLeft())).reversed())\n                 .skip(maxUnprunedVariants)\n-                .forEach(result::add);\n+                .forEach(errorChains::add);\n \n-        return result;\n+        return errorChains;\n \n     }\n \n-    private double chainLogOdds(final Path<V,E> chain, final BaseGraph<V,E> graph, final double errorRate) {\n-        if (chain.getEdges().stream().anyMatch(E::isRef)) {\n-            return Double.POSITIVE_INFINITY;\n-        }\n+    // find the chain containing the edge of greatest weight, taking care to break ties deterministically\n+    private Path<V, E> getMaxWeightChain(final Collection<Path<V, E>> chains) {\n+        return chains.stream()\n+                .max(Comparator.comparingInt((Path<V, E> chain) -> chain.getEdges().stream().mapToInt(BaseEdge::getMultiplicity).max().orElse(0))\n+                        .thenComparingInt(Path::length)\n+                        .thenComparingInt(Path::hashCode)).get();", "originalCommit": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2OTU4MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r434769581", "bodyText": "Or some other such unique feature of the chains (perhaps compare more edge weights?)", "author": "jamesemery", "createdAt": "2020-06-03T18:27:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2NDA5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTgzMjA5MA==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r449832090", "bodyText": "done (the lexicographic version)", "author": "davidbenjamin", "createdAt": "2020-07-05T04:13:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2NDA5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2ODE0Nw==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r434768147", "bodyText": "Is there a reason we are using a flat chain/seed pruning log-odds here? Why not test with the defaults laid out in the mutect2 argument collection?", "author": "jamesemery", "createdAt": "2020-06-03T18:24:38Z", "path": "src/test/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/ChainPrunerUnitTest.java", "diffHunk": "@@ -175,6 +175,47 @@ public void testAdaptivePruning(final int kmerSize, final byte[] ref, final byte\n         Assert.assertTrue(bestPaths.size() < 15);\n     }\n \n+    // test that in graph with good path A -> B -> C and bad edges A -> D -> C and D -> B that the adjacency of bad edges --\n+    // such that when bad edges meet the multiplicities do not indicate an error - does not harm pruning.\n+    // we test with and without a true variant path A -> E -> C\n+    @Test\n+    public void testAdaptivePruningWithAdjacentBadEdges() {\n+        final int goodMultiplicity = 1000;\n+        final int variantMultiplicity = 50;\n+        final int badMultiplicity = 5;\n+\n+        final SeqVertex source = new SeqVertex(\"source\");\n+        final SeqVertex sink = new SeqVertex(\"sink\");\n+        final SeqVertex A = new SeqVertex(\"A\");\n+        final SeqVertex B = new SeqVertex(\"B\");\n+        final SeqVertex C = new SeqVertex(\"C\");\n+        final SeqVertex D = new SeqVertex(\"D\");\n+        final SeqVertex E = new SeqVertex(\"E\");\n+\n+\n+        for (boolean variantPresent : new boolean[] {false, true}) {\n+            final SeqGraph graph = new SeqGraph(20);\n+\n+            graph.addVertices(source, A, B, C, D, sink);\n+            graph.addEdges(() -> new BaseEdge(true, goodMultiplicity), source, A, B, C, sink);\n+            graph.addEdges(() -> new BaseEdge(false, badMultiplicity), A, D, C);\n+            graph.addEdges(() -> new BaseEdge(false, badMultiplicity), D, B);\n+\n+            if (variantPresent) {\n+                graph.addVertices(E);\n+                graph.addEdges(() -> new BaseEdge(false, variantMultiplicity), A, E, C);\n+            }\n+\n+            final ChainPruner<SeqVertex, BaseEdge> pruner = new AdaptiveChainPruner<>(0.01, 2.0, 2.0, 50);", "originalCommit": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDMwNzUwNQ==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r450307505", "bodyText": "nope.  Changed it.", "author": "davidbenjamin", "createdAt": "2020-07-06T15:36:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2ODE0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2OTI5MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r434769291", "bodyText": "You should add a test along the lines of the example I laid out above (where there is a high confidence reference path and a bubble with bad connections to the root of the tree and demonstrate that a strict seed LOD ratio will prevent that case from being recovered.", "author": "jamesemery", "createdAt": "2020-06-03T18:26:40Z", "path": "src/test/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/ChainPrunerUnitTest.java", "diffHunk": "@@ -175,6 +175,47 @@ public void testAdaptivePruning(final int kmerSize, final byte[] ref, final byte\n         Assert.assertTrue(bestPaths.size() < 15);\n     }\n \n+    // test that in graph with good path A -> B -> C and bad edges A -> D -> C and D -> B that the adjacency of bad edges --\n+    // such that when bad edges meet the multiplicities do not indicate an error - does not harm pruning.\n+    // we test with and without a true variant path A -> E -> C\n+    @Test\n+    public void testAdaptivePruningWithAdjacentBadEdges() {\n+        final int goodMultiplicity = 1000;\n+        final int variantMultiplicity = 50;\n+        final int badMultiplicity = 5;\n+\n+        final SeqVertex source = new SeqVertex(\"source\");\n+        final SeqVertex sink = new SeqVertex(\"sink\");\n+        final SeqVertex A = new SeqVertex(\"A\");\n+        final SeqVertex B = new SeqVertex(\"B\");\n+        final SeqVertex C = new SeqVertex(\"C\");\n+        final SeqVertex D = new SeqVertex(\"D\");\n+        final SeqVertex E = new SeqVertex(\"E\");\n+\n+\n+        for (boolean variantPresent : new boolean[] {false, true}) {\n+            final SeqGraph graph = new SeqGraph(20);\n+\n+            graph.addVertices(source, A, B, C, D, sink);\n+            graph.addEdges(() -> new BaseEdge(true, goodMultiplicity), source, A, B, C, sink);\n+            graph.addEdges(() -> new BaseEdge(false, badMultiplicity), A, D, C);\n+            graph.addEdges(() -> new BaseEdge(false, badMultiplicity), D, B);\n+\n+            if (variantPresent) {\n+                graph.addVertices(E);\n+                graph.addEdges(() -> new BaseEdge(false, variantMultiplicity), A, E, C);\n+            }\n+\n+            final ChainPruner<SeqVertex, BaseEdge> pruner = new AdaptiveChainPruner<>(0.01, 2.0, 2.0, 50);\n+            pruner.pruneLowWeightChains(graph);\n+\n+            Assert.assertFalse(graph.containsVertex(D));\n+            if (variantPresent) {\n+                Assert.assertTrue(graph.containsVertex(E));\n+            }\n+        }\n+    }\n+", "originalCommit": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTgzMjMwMw==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r449832303", "bodyText": "done", "author": "davidbenjamin", "createdAt": "2020-07-05T04:17:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2OTI5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc3NDgzMw==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r434774833", "bodyText": "Interesting, I still think this can be fooled if there is a bubble of low weight in an error branch for which two low weight edges of equal weight split and merge and ultimately are connected to the reference only by bogus chains.", "author": "jamesemery", "createdAt": "2020-06-03T18:36:20Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +33,105 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+        /////// NEW\n+\n+        // pre-compute the left and right log odds of each chain\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = chains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        // compute correspondence of vertices to incident chains with log odds above the seeding and extending thresholds\n+        final Multimap<V, Path<V,E>> vertexToSeedableChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodIncomingChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodOutgoingChains = ArrayListMultimap.create();\n+\n+        for (final Path<V,E> chain : chains) {\n+            if (chainLogOdds.get(chain).getRight() >= logOddsThreshold) {\n+                vertexToGoodIncomingChains.put(chain.getLastVertex(), chain);\n+            }\n+\n+            if (chainLogOdds.get(chain).getLeft() >= logOddsThreshold) {\n+                vertexToGoodOutgoingChains.put(chain.getFirstVertex(), chain);\n+            }\n+\n+            // seed-worthy chains must pass the more stringent seeding log odds threshold on both sides\n+            // in addition to that, we only seed from vertices with multiple such chains incoming or outgoing (see below)\n+            if (chainLogOdds.get(chain).getRight() >= seedingLogOddsThreshold && chainLogOdds.get(chain).getLeft() >= seedingLogOddsThreshold) {\n+                vertexToSeedableChains.put(chain.getFirstVertex(), chain);\n+                vertexToSeedableChains.put(chain.getLastVertex(), chain);\n+            }\n+        }\n+\n+        // find a subset of good vertices from which to grow the subgraph of good chains.\n+        final Queue<V> verticesToProcess = new ArrayDeque<>();\n+        final Path<V,E> maxWeightChain = getMaxWeightChain(chains);\n+        verticesToProcess.add(maxWeightChain.getFirstVertex());\n+        verticesToProcess.add(maxWeightChain.getLastVertex());\n+\n+        // look for vertices with two incoming or two outgoing chains (plus one outgoing or incoming for a total of 3 or more) with good log odds to seed the subgraph of good vertices\n+        // the logic here is that a high-multiplicity error chain A that branches into a second error chain B and a continuation-of-the-original-error chain A'\n+        // may have a high log odds for A'.  However, only in the case of true variation will  multiple branches leaving the same vertex have good log odds.", "originalCommit": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTEyNzkwMw==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r449127903", "bodyText": "This is also averted by the stricter seeding log odds threshold.  I wrote a unit test for it.", "author": "davidbenjamin", "createdAt": "2020-07-02T16:16:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc3NDgzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc3NTk3NA==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r434775974", "bodyText": "I would advocate this threshold be made very strict and that we do some reasoning about how probable it is for this to come up. It should be fairly unlikely for the seeding vertexes to ever fall off of good variation by accident due to low coverage I think.", "author": "jamesemery", "createdAt": "2020-06-03T18:38:37Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/ReadThreadingAssemblerArgumentCollection.java", "diffHunk": "@@ -18,6 +18,7 @@\n     private static final long serialVersionUID = 1L;\n \n     public static final double DEFAULT_PRUNING_LOG_ODDS_THRESHOLD = MathUtils.log10ToLog(1.0);\n+    public static final double DEFAULT_PRUNING_SEEDING_LOG_ODDS_THRESHOLD = MathUtils.log10ToLog(2.0);", "originalCommit": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDMzNzkxNg==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r450337916", "bodyText": "You're right, and I'll reason it out right now.  I don't know what conclusions this exercise will lead to. . .\nThe seeding method is fooled when an error chain A splits into two chains B and C, where B is the continuation of the original error and C is an additional error on top of the first, such that the likelihood of outgoing coverage N_C given incoming coverage N_A exceeds the threshold.\nSo let's first tabulate some values of N_A and N_C along with their log odds (from Mutect2Engine.logLikelihoodRatio(N_B, N_C, errorRate) as in the pruning code).  We'll use an error rate of 0.001.  Note that the current threshold is 4.605.\nN_A  N_C  LOD\n30     2      4.3\n30     3      8.9\n15     2      6.4\n15     3      11.8\n10     1       2.2\n10     2       7.6\n10     3       13.5\n5       1       3.5\n5       2       9.7\n3       1        4.4\n2        1        5.5\nWithout further calculation this already looks like the current threshold is too lenient and that raising the threshold significantly would not cause many real variants to fail.  Furthermore, even if a real variant with say 15 reads coming in and 2 coming out doesn't get counted as a seeding chain, it is overwhelmingly likely that the chain from which that 15-read chain is outgoing will be a seeding chain.  Thus I think this threshold can be raised to at least 10 with no harm to sensitivity whatsoever.  [To be continued. . .]", "author": "davidbenjamin", "createdAt": "2020-07-06T16:24:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc3NTk3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDQzNjQwNQ==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r450436405", "bodyText": "So now let's think about false positive rates \u2014 that is, the probability of an error chain being used for seeding.  Note also that this isn't the end of the world.  It is not true that an error chain used for seeding starts a chain reaction of every error chain remaining unpruned, since chains still must pass the non-seeding log odds threshold.  Nonetheless, it's undesirable.\nSuppose we have a coverage of N at some correct chain representing a real haplotype.  To get incorrect seeding we must have the same two errors sharing a kmer.  Otherwise the first error's chain will have returned to the non-error chain before the second error's chain begins.\nIf we have a seeding threshold of 10, the table above shows that we need a coincidence of errors somewhere in the ballpark of 5 for the first and two for the second.  At a coverage of 100 and an error rate of 0.001, the chance of this is 1 in ten million per locus.  The chance of two or more errors at any given locus is about 5 in 1000, so the chance of the second error site occurring within k = 25 bases is about one in ten.  If we have a 500-base window we would then expect a false seeding event one out of every 200 thousand assemblies.  Add to that the fairly small chance that the false seeding actually does any harm and I think we can safely say a seeding log odds threshold of 10 (this is all in log space, by the way) incurs no risk of under-pruning.", "author": "davidbenjamin", "createdAt": "2020-07-06T19:32:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc3NTk3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc3NjkzMQ==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r434776931", "bodyText": "I actually do not think this matters based on how you use these but for extra safety and defense against future alterations to this code I would make these linkedHashSets, especially since you are still iterating over the first one.", "author": "jamesemery", "createdAt": "2020-06-03T18:40:17Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +33,105 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+        /////// NEW\n+\n+        // pre-compute the left and right log odds of each chain\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = chains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        // compute correspondence of vertices to incident chains with log odds above the seeding and extending thresholds\n+        final Multimap<V, Path<V,E>> vertexToSeedableChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodIncomingChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodOutgoingChains = ArrayListMultimap.create();\n+\n+        for (final Path<V,E> chain : chains) {\n+            if (chainLogOdds.get(chain).getRight() >= logOddsThreshold) {\n+                vertexToGoodIncomingChains.put(chain.getLastVertex(), chain);\n+            }\n+\n+            if (chainLogOdds.get(chain).getLeft() >= logOddsThreshold) {\n+                vertexToGoodOutgoingChains.put(chain.getFirstVertex(), chain);\n+            }\n+\n+            // seed-worthy chains must pass the more stringent seeding log odds threshold on both sides\n+            // in addition to that, we only seed from vertices with multiple such chains incoming or outgoing (see below)\n+            if (chainLogOdds.get(chain).getRight() >= seedingLogOddsThreshold && chainLogOdds.get(chain).getLeft() >= seedingLogOddsThreshold) {\n+                vertexToSeedableChains.put(chain.getFirstVertex(), chain);\n+                vertexToSeedableChains.put(chain.getLastVertex(), chain);\n+            }\n+        }\n+\n+        // find a subset of good vertices from which to grow the subgraph of good chains.\n+        final Queue<V> verticesToProcess = new ArrayDeque<>();\n+        final Path<V,E> maxWeightChain = getMaxWeightChain(chains);\n+        verticesToProcess.add(maxWeightChain.getFirstVertex());\n+        verticesToProcess.add(maxWeightChain.getLastVertex());\n+\n+        // look for vertices with two incoming or two outgoing chains (plus one outgoing or incoming for a total of 3 or more) with good log odds to seed the subgraph of good vertices\n+        // the logic here is that a high-multiplicity error chain A that branches into a second error chain B and a continuation-of-the-original-error chain A'\n+        // may have a high log odds for A'.  However, only in the case of true variation will  multiple branches leaving the same vertex have good log odds.\n+        vertexToSeedableChains.keySet().stream()\n+                .filter(v -> vertexToSeedableChains.get(v).size() > 2)\n+                .forEach(verticesToProcess::add);\n+\n+        final Set<V> processedVertices = new HashSet<>();", "originalCommit": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTEyOTA5OQ==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r449129099", "bodyText": "done", "author": "davidbenjamin", "createdAt": "2020-07-02T16:18:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc3NjkzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc4Mzg1NA==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r434783854", "bodyText": "You know, it occurs to me that the maxUnprunedVariants limit here probably doesn't make sense. If you think about it there is not a 1:1 mapping of chains to variants after pruning. Its very possible for a \"variant\" to be a single chain in the simple case or possible correspond to a few distinct \"chains\" because there could be forked paths with low LOD that get pruned that end up splitting that one good \"chain\" into a few smaller ones. Furthermore it seems possible that a perfectly good variant has part a part of its chain removed at this stage as a result resulting in 1 or potentially 2 dangling ends after pruning and deleting the connection. I don't suspect this happens often enough to worry about here though. Worth keeping in mind for future changes though.", "author": "jamesemery", "createdAt": "2020-06-03T18:53:00Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +33,105 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+        /////// NEW\n+\n+        // pre-compute the left and right log odds of each chain\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = chains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        // compute correspondence of vertices to incident chains with log odds above the seeding and extending thresholds\n+        final Multimap<V, Path<V,E>> vertexToSeedableChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodIncomingChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodOutgoingChains = ArrayListMultimap.create();\n+\n+        for (final Path<V,E> chain : chains) {\n+            if (chainLogOdds.get(chain).getRight() >= logOddsThreshold) {\n+                vertexToGoodIncomingChains.put(chain.getLastVertex(), chain);\n+            }\n+\n+            if (chainLogOdds.get(chain).getLeft() >= logOddsThreshold) {\n+                vertexToGoodOutgoingChains.put(chain.getFirstVertex(), chain);\n+            }\n+\n+            // seed-worthy chains must pass the more stringent seeding log odds threshold on both sides\n+            // in addition to that, we only seed from vertices with multiple such chains incoming or outgoing (see below)\n+            if (chainLogOdds.get(chain).getRight() >= seedingLogOddsThreshold && chainLogOdds.get(chain).getLeft() >= seedingLogOddsThreshold) {\n+                vertexToSeedableChains.put(chain.getFirstVertex(), chain);\n+                vertexToSeedableChains.put(chain.getLastVertex(), chain);\n+            }\n+        }\n+\n+        // find a subset of good vertices from which to grow the subgraph of good chains.\n+        final Queue<V> verticesToProcess = new ArrayDeque<>();\n+        final Path<V,E> maxWeightChain = getMaxWeightChain(chains);\n+        verticesToProcess.add(maxWeightChain.getFirstVertex());\n+        verticesToProcess.add(maxWeightChain.getLastVertex());\n+\n+        // look for vertices with two incoming or two outgoing chains (plus one outgoing or incoming for a total of 3 or more) with good log odds to seed the subgraph of good vertices\n+        // the logic here is that a high-multiplicity error chain A that branches into a second error chain B and a continuation-of-the-original-error chain A'\n+        // may have a high log odds for A'.  However, only in the case of true variation will  multiple branches leaving the same vertex have good log odds.\n+        vertexToSeedableChains.keySet().stream()\n+                .filter(v -> vertexToSeedableChains.get(v).size() > 2)\n+                .forEach(verticesToProcess::add);\n+\n+        final Set<V> processedVertices = new HashSet<>();\n+        final Set<Path<V,E>> goodChains = new HashSet<>();\n+\n+        // starting from the high-confidence seed vertices, grow the \"good\" subgraph along chains with above-threshold log odds,\n+        // discovering good chains as we go.\n+        while (!verticesToProcess.isEmpty()) {\n+            final V vertex = verticesToProcess.poll();\n+            processedVertices.add(vertex);\n+            for (final Path<V,E> outgoingChain : vertexToGoodOutgoingChains.get(vertex)) {\n+                goodChains.add(outgoingChain);\n+                if(!processedVertices.contains(outgoingChain.getLastVertex())) {\n+                    verticesToProcess.add(outgoingChain.getLastVertex());\n+                }\n+            }\n \n-        chainLogOdds.forEach((chain, lod) -> {\n-            if (lod < logOddsThreshold) {\n-                result.add(chain);\n+            for (final Path<V,E> incomingChain : vertexToGoodIncomingChains.get(vertex)) {\n+                goodChains.add(incomingChain);\n+                if(!processedVertices.contains(incomingChain.getFirstVertex())) {\n+                    verticesToProcess.add(incomingChain.getFirstVertex());\n+                }\n             }\n-        });\n+        }\n+\n+        final Set<Path<V,E>> errorChains = chains.stream().filter(c -> !goodChains.contains(c)).collect(Collectors.toSet());\n \n-        chains.stream().filter(c -> isChainPossibleVariant(c, graph))\n-                .sorted(Comparator.comparingDouble((ToDoubleFunction<Path<V, E>>) chainLogOdds::get)\n-                        .reversed().thenComparingInt(Path::length))\n+        // add non-error chains to error chains if maximum number of variants has been exceeded\n+        chains.stream()\n+                .filter(c -> !errorChains.contains(c))\n+                .filter(c -> isChainPossibleVariant(c, graph))\n+                .sorted(Comparator.comparingDouble(c -> Math.min(chainLogOdds.get(c).getLeft(), chainLogOdds.get(c).getLeft())).reversed())\n                 .skip(maxUnprunedVariants)\n-                .forEach(result::add);\n+                .forEach(errorChains::add);", "originalCommit": "3e3838fad1a76dbe18830d2e4166c6f76d6453c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTYzNTEwOQ==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r449635109", "bodyText": "You have a good point, and it's worth fixing now. The number of variants associated with a vertex is the number of good outgoing chains minus one.  This works even where there is one good incoming chain, one good outgoing chain, and one bad outgoing chain.  Thus we have\nnumber of variants in graph = sum_vertices (max(number of outgoing chains - 1, 0))\nAfter computing this number of variants we could prune good chains, starting from those with the worst log odds, until the number of variants is low enough.  Every time we prune a good chain whose first vertex has at least one other good outgoing chain we have eliminated one variant from the graph.", "author": "davidbenjamin", "createdAt": "2020-07-03T15:29:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc4Mzg1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTY0NTYyOA==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r449645628", "bodyText": "Done.", "author": "davidbenjamin", "createdAt": "2020-07-03T16:01:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc4Mzg1NA=="}], "type": "inlineReview"}, {"oid": "cfa1b7b7f60a17deaaf4b8663ac0c91e60ad1d6a", "url": "https://github.com/broadinstitute/gatk/commit/cfa1b7b7f60a17deaaf4b8663ac0c91e60ad1d6a", "message": "review edits", "committedDate": "2020-07-02T16:06:13Z", "type": "forcePushed"}, {"oid": "2f0079dc7ef0588dbcb9c0143ee4a30319bf6c91", "url": "https://github.com/broadinstitute/gatk/commit/2f0079dc7ef0588dbcb9c0143ee4a30319bf6c91", "message": "review edits", "committedDate": "2020-07-06T19:58:06Z", "type": "forcePushed"}, {"oid": "e10dada478d42f1a28ad84878c7ec036a09706ed", "url": "https://github.com/broadinstitute/gatk/commit/e10dada478d42f1a28ad84878c7ec036a09706ed", "message": "review edits", "committedDate": "2020-07-08T15:11:07Z", "type": "forcePushed"}, {"oid": "e0113bb902c67e48dbf713d277f4bb3cb2ec2ab7", "url": "https://github.com/broadinstitute/gatk/commit/e0113bb902c67e48dbf713d277f4bb3cb2ec2ab7", "message": "review edits", "committedDate": "2020-07-08T15:20:25Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk1OTA5Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r455959096", "bodyText": "Commented code", "author": "jamesemery", "createdAt": "2020-07-16T17:40:01Z", "path": "src/test/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/ChainPrunerUnitTest.java", "diffHunk": "@@ -167,12 +170,19 @@ public void testAdaptivePruning(final int kmerSize, final byte[] ref, final byte\n         final List<KBestHaplotype<SeqVertex, BaseEdge>> bestPaths = new GraphBasedKBestHaplotypeFinder<>(seqGraph).findBestHaplotypes(10);\n \n         final OptionalInt altIndex = IntStream.range(0, bestPaths.size()).filter(n -> bestPaths.get(n).haplotype().basesMatch(alt)).findFirst();\n-        Assert.assertTrue(altIndex.isPresent());\n+        //Assert.assertTrue(altIndex.isPresent());\n+        if (!altIndex.isPresent()) {\n+            int g = 90;\n+        }\n+\n+        // ref path should not be pruned even if all reads are alt\n+        final OptionalInt refIndex = IntStream.range(0, bestPaths.size()).filter(n -> bestPaths.get(n).haplotype().basesMatch(ref)).findFirst();\n+        Assert.assertTrue(refIndex.isPresent());\n \n         // the haplotype score is the sum of the log-10 of all branching fractions, so the alt haplotype score should come out to\n-        // around the log-10 of the allele fraction up to some fudge factor, assumign we didn't do any dumb pruning\n-        Assert.assertEquals(bestPaths.get(altIndex.getAsInt()).score(), Math.log10(altFraction), 0.5);\n-        Assert.assertTrue(bestPaths.size() < 15);\n+        // around the log-10 of the allele fraction up to some fudge factor, assuming we didn't do any dumb pruning\n+        //Assert.assertEquals(bestPaths.get(altIndex.getAsInt()).score(), Math.log10(altFraction), 0.5);", "originalCommit": "e0113bb902c67e48dbf713d277f4bb3cb2ec2ab7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzA2MTEzNA==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r483061134", "bodyText": "fixed.", "author": "davidbenjamin", "createdAt": "2020-09-03T15:21:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk1OTA5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk2Njc2Nw==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r455966767", "bodyText": "This G is unused.", "author": "jamesemery", "createdAt": "2020-07-16T17:52:32Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +34,136 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+        // pre-compute the left and right log odds of each chain\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = chains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        // compute correspondence of vertices to incident chains with log odds above the seeding and extending thresholds\n+        final Multimap<V, Path<V,E>> vertexToSeedableChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodIncomingChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodOutgoingChains = ArrayListMultimap.create();\n+\n+        for (final Path<V,E> chain : chains) {\n+            if (chainLogOdds.get(chain).getRight() >= logOddsThreshold) {\n+                vertexToGoodIncomingChains.put(chain.getLastVertex(), chain);\n+            }\n \n-        chainLogOdds.forEach((chain, lod) -> {\n-            if (lod < logOddsThreshold) {\n-                result.add(chain);\n+            if (chainLogOdds.get(chain).getLeft() >= logOddsThreshold) {\n+                vertexToGoodOutgoingChains.put(chain.getFirstVertex(), chain);\n             }\n-        });\n \n-        chains.stream().filter(c -> isChainPossibleVariant(c, graph))\n-                .sorted(Comparator.comparingDouble((ToDoubleFunction<Path<V, E>>) chainLogOdds::get)\n-                        .reversed().thenComparingInt(Path::length))\n-                .skip(maxUnprunedVariants)\n-                .forEach(result::add);\n+            // seed-worthy chains must pass the more stringent seeding log odds threshold on both sides\n+            // in addition to that, we only seed from vertices with multiple such chains incoming or outgoing (see below)\n+            if (chainLogOdds.get(chain).getRight() >= seedingLogOddsThreshold && chainLogOdds.get(chain).getLeft() >= seedingLogOddsThreshold) {\n+                vertexToSeedableChains.put(chain.getFirstVertex(), chain);\n+                vertexToSeedableChains.put(chain.getLastVertex(), chain);\n+            }\n+        }\n \n-        return result;\n+        // find a subset of good vertices from which to grow the subgraph of good chains.\n+        final Queue<V> verticesToProcess = new ArrayDeque<>();\n+        final Path<V,E> maxWeightChain = getMaxWeightChain(chains);\n+        verticesToProcess.add(maxWeightChain.getFirstVertex());\n+        verticesToProcess.add(maxWeightChain.getLastVertex());\n+\n+        // look for vertices with two incoming or two outgoing chains (plus one outgoing or incoming for a total of 3 or more) with good log odds to seed the subgraph of good vertices\n+        // the logic here is that a high-multiplicity error chain A that branches into a second error chain B and a continuation-of-the-original-error chain A'\n+        // may have a high log odds for A'.  However, only in the case of true variation will  multiple branches leaving the same vertex have good log odds.\n+        vertexToSeedableChains.keySet().stream()\n+                .filter(v -> vertexToSeedableChains.get(v).size() > 2)\n+                .forEach(verticesToProcess::add);\n+\n+        final Set<V> processedVertices = new LinkedHashSet<>();\n+        final Set<Path<V,E>> goodChains = new LinkedHashSet<>();\n+\n+        // starting from the high-confidence seed vertices, grow the \"good\" subgraph along chains with above-threshold log odds,\n+        // discovering good chains as we go.\n+        while (!verticesToProcess.isEmpty()) {\n+            final V vertex = verticesToProcess.poll();\n+            processedVertices.add(vertex);\n+            for (final Path<V,E> outgoingChain : vertexToGoodOutgoingChains.get(vertex)) {\n+                goodChains.add(outgoingChain);\n+                if(!processedVertices.contains(outgoingChain.getLastVertex())) {\n+                    verticesToProcess.add(outgoingChain.getLastVertex());\n+                }\n+            }\n \n-    }\n+            for (final Path<V,E> incomingChain : vertexToGoodIncomingChains.get(vertex)) {\n+                goodChains.add(incomingChain);\n+                if(!processedVertices.contains(incomingChain.getFirstVertex())) {\n+                    verticesToProcess.add(incomingChain.getFirstVertex());\n+                }\n+            }\n+        }\n+\n+        final Set<Path<V,E>> errorChains = chains.stream().filter(c -> !goodChains.contains(c)).collect(Collectors.toSet());\n+\n+        // A vertex with N > 0 outgoing good chains corresponds to N - 1 variants\n+        int numberOfVariantsInGraph = vertexToGoodOutgoingChains.keySet().stream()\n+                .mapToInt(v -> Math.max(vertexToGoodOutgoingChains.get(v).size() - 1, 0)).sum();\n+\n+        if (numberOfVariantsInGraph > maxUnprunedVariants) {\n+            // the vertex-to-good-incoming/outgoing chain maps contain all chains with log odds passing the threshold, even if\n+            // the seeding and extending process revealed them to be errors.  We need to cull such chains for the following steps\n+            for (final Path<V,E> chain : errorChains) {\n+                vertexToGoodOutgoingChains.remove(chain.getFirstVertex(), chain);\n+                vertexToGoodIncomingChains.remove(chain.getLastVertex(), chain);\n+            }\n+\n+            // recalculate now that we have more accurate vertex-to-good-chains maps\n+            numberOfVariantsInGraph = vertexToGoodOutgoingChains.keySet().stream()\n+                    .mapToInt(v -> Math.max(vertexToGoodOutgoingChains.get(v).size() - 1, 0)).sum();\n \n-    private double chainLogOdds(final Path<V,E> chain, final BaseGraph<V,E> graph, final double errorRate) {\n-        if (chain.getEdges().stream().anyMatch(E::isRef)) {\n-            return Double.POSITIVE_INFINITY;\n+            // start with the worst good variants\n+            final PriorityQueue<Path<V,E>> excessGoodChainsToPrune = new PriorityQueue<>(\n+                    Comparator.comparingDouble((Path<V,E> c) -> Math.min(chainLogOdds.get(c).getLeft(), chainLogOdds.get(c).getLeft()))\n+                            .thenComparing((Path<V,E> c) -> c.getFirstVertex().getSequence(), BaseUtils.BASES_COMPARATOR));\n+\n+            excessGoodChainsToPrune.addAll(goodChains);\n+\n+            while (numberOfVariantsInGraph > maxUnprunedVariants) {\n+                final Path<V,E> worstGoodChain = excessGoodChainsToPrune.poll();\n+                errorChains.add(worstGoodChain);\n+                //if removing this chain pops a bubble, we have pruned a variant\n+                if (vertexToGoodOutgoingChains.get(worstGoodChain.getFirstVertex()).size() > 1) {\n+                    numberOfVariantsInGraph--;\n+                }\n+                // remove the chain\n+                vertexToGoodOutgoingChains.remove(worstGoodChain.getFirstVertex(), worstGoodChain);\n+                vertexToGoodIncomingChains.remove(worstGoodChain.getLastVertex(), worstGoodChain);\n+\n+                int numberOfVariantsInGraphRecalculated = vertexToGoodOutgoingChains.keySet().stream()\n+                        .mapToInt(v -> Math.max(vertexToGoodOutgoingChains.get(v).size() - 1, 0)).sum();\n+\n+                int g = 0;", "originalCommit": "e0113bb902c67e48dbf713d277f4bb3cb2ec2ab7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzA2MTU4Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r483061586", "bodyText": "fixed.", "author": "davidbenjamin", "createdAt": "2020-09-03T15:21:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk2Njc2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk2NzExMg==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r455967112", "bodyText": "This value is not actually used anywhere.", "author": "jamesemery", "createdAt": "2020-07-16T17:53:04Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +34,136 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+        // pre-compute the left and right log odds of each chain\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = chains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        // compute correspondence of vertices to incident chains with log odds above the seeding and extending thresholds\n+        final Multimap<V, Path<V,E>> vertexToSeedableChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodIncomingChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodOutgoingChains = ArrayListMultimap.create();\n+\n+        for (final Path<V,E> chain : chains) {\n+            if (chainLogOdds.get(chain).getRight() >= logOddsThreshold) {\n+                vertexToGoodIncomingChains.put(chain.getLastVertex(), chain);\n+            }\n \n-        chainLogOdds.forEach((chain, lod) -> {\n-            if (lod < logOddsThreshold) {\n-                result.add(chain);\n+            if (chainLogOdds.get(chain).getLeft() >= logOddsThreshold) {\n+                vertexToGoodOutgoingChains.put(chain.getFirstVertex(), chain);\n             }\n-        });\n \n-        chains.stream().filter(c -> isChainPossibleVariant(c, graph))\n-                .sorted(Comparator.comparingDouble((ToDoubleFunction<Path<V, E>>) chainLogOdds::get)\n-                        .reversed().thenComparingInt(Path::length))\n-                .skip(maxUnprunedVariants)\n-                .forEach(result::add);\n+            // seed-worthy chains must pass the more stringent seeding log odds threshold on both sides\n+            // in addition to that, we only seed from vertices with multiple such chains incoming or outgoing (see below)\n+            if (chainLogOdds.get(chain).getRight() >= seedingLogOddsThreshold && chainLogOdds.get(chain).getLeft() >= seedingLogOddsThreshold) {\n+                vertexToSeedableChains.put(chain.getFirstVertex(), chain);\n+                vertexToSeedableChains.put(chain.getLastVertex(), chain);\n+            }\n+        }\n \n-        return result;\n+        // find a subset of good vertices from which to grow the subgraph of good chains.\n+        final Queue<V> verticesToProcess = new ArrayDeque<>();\n+        final Path<V,E> maxWeightChain = getMaxWeightChain(chains);\n+        verticesToProcess.add(maxWeightChain.getFirstVertex());\n+        verticesToProcess.add(maxWeightChain.getLastVertex());\n+\n+        // look for vertices with two incoming or two outgoing chains (plus one outgoing or incoming for a total of 3 or more) with good log odds to seed the subgraph of good vertices\n+        // the logic here is that a high-multiplicity error chain A that branches into a second error chain B and a continuation-of-the-original-error chain A'\n+        // may have a high log odds for A'.  However, only in the case of true variation will  multiple branches leaving the same vertex have good log odds.\n+        vertexToSeedableChains.keySet().stream()\n+                .filter(v -> vertexToSeedableChains.get(v).size() > 2)\n+                .forEach(verticesToProcess::add);\n+\n+        final Set<V> processedVertices = new LinkedHashSet<>();\n+        final Set<Path<V,E>> goodChains = new LinkedHashSet<>();\n+\n+        // starting from the high-confidence seed vertices, grow the \"good\" subgraph along chains with above-threshold log odds,\n+        // discovering good chains as we go.\n+        while (!verticesToProcess.isEmpty()) {\n+            final V vertex = verticesToProcess.poll();\n+            processedVertices.add(vertex);\n+            for (final Path<V,E> outgoingChain : vertexToGoodOutgoingChains.get(vertex)) {\n+                goodChains.add(outgoingChain);\n+                if(!processedVertices.contains(outgoingChain.getLastVertex())) {\n+                    verticesToProcess.add(outgoingChain.getLastVertex());\n+                }\n+            }\n \n-    }\n+            for (final Path<V,E> incomingChain : vertexToGoodIncomingChains.get(vertex)) {\n+                goodChains.add(incomingChain);\n+                if(!processedVertices.contains(incomingChain.getFirstVertex())) {\n+                    verticesToProcess.add(incomingChain.getFirstVertex());\n+                }\n+            }\n+        }\n+\n+        final Set<Path<V,E>> errorChains = chains.stream().filter(c -> !goodChains.contains(c)).collect(Collectors.toSet());\n+\n+        // A vertex with N > 0 outgoing good chains corresponds to N - 1 variants\n+        int numberOfVariantsInGraph = vertexToGoodOutgoingChains.keySet().stream()\n+                .mapToInt(v -> Math.max(vertexToGoodOutgoingChains.get(v).size() - 1, 0)).sum();\n+\n+        if (numberOfVariantsInGraph > maxUnprunedVariants) {\n+            // the vertex-to-good-incoming/outgoing chain maps contain all chains with log odds passing the threshold, even if\n+            // the seeding and extending process revealed them to be errors.  We need to cull such chains for the following steps\n+            for (final Path<V,E> chain : errorChains) {\n+                vertexToGoodOutgoingChains.remove(chain.getFirstVertex(), chain);\n+                vertexToGoodIncomingChains.remove(chain.getLastVertex(), chain);\n+            }\n+\n+            // recalculate now that we have more accurate vertex-to-good-chains maps\n+            numberOfVariantsInGraph = vertexToGoodOutgoingChains.keySet().stream()\n+                    .mapToInt(v -> Math.max(vertexToGoodOutgoingChains.get(v).size() - 1, 0)).sum();\n \n-    private double chainLogOdds(final Path<V,E> chain, final BaseGraph<V,E> graph, final double errorRate) {\n-        if (chain.getEdges().stream().anyMatch(E::isRef)) {\n-            return Double.POSITIVE_INFINITY;\n+            // start with the worst good variants\n+            final PriorityQueue<Path<V,E>> excessGoodChainsToPrune = new PriorityQueue<>(\n+                    Comparator.comparingDouble((Path<V,E> c) -> Math.min(chainLogOdds.get(c).getLeft(), chainLogOdds.get(c).getLeft()))\n+                            .thenComparing((Path<V,E> c) -> c.getFirstVertex().getSequence(), BaseUtils.BASES_COMPARATOR));\n+\n+            excessGoodChainsToPrune.addAll(goodChains);\n+\n+            while (numberOfVariantsInGraph > maxUnprunedVariants) {\n+                final Path<V,E> worstGoodChain = excessGoodChainsToPrune.poll();\n+                errorChains.add(worstGoodChain);\n+                //if removing this chain pops a bubble, we have pruned a variant\n+                if (vertexToGoodOutgoingChains.get(worstGoodChain.getFirstVertex()).size() > 1) {\n+                    numberOfVariantsInGraph--;\n+                }\n+                // remove the chain\n+                vertexToGoodOutgoingChains.remove(worstGoodChain.getFirstVertex(), worstGoodChain);\n+                vertexToGoodIncomingChains.remove(worstGoodChain.getLastVertex(), worstGoodChain);\n+\n+                int numberOfVariantsInGraphRecalculated = vertexToGoodOutgoingChains.keySet().stream()", "originalCommit": "e0113bb902c67e48dbf713d277f4bb3cb2ec2ab7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzA2MTcwOQ==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r483061709", "bodyText": "fixed.", "author": "davidbenjamin", "createdAt": "2020-09-03T15:21:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk2NzExMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk2ODE1MQ==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r455968151", "bodyText": "Again what does this g mean?", "author": "jamesemery", "createdAt": "2020-07-16T17:54:52Z", "path": "src/test/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/ChainPrunerUnitTest.java", "diffHunk": "@@ -167,12 +170,108 @@ public void testAdaptivePruning(final int kmerSize, final byte[] ref, final byte\n         final List<KBestHaplotype<SeqVertex, BaseEdge>> bestPaths = new GraphBasedKBestHaplotypeFinder<>(seqGraph).findBestHaplotypes(10);\n \n         final OptionalInt altIndex = IntStream.range(0, bestPaths.size()).filter(n -> bestPaths.get(n).haplotype().basesMatch(alt)).findFirst();\n-        Assert.assertTrue(altIndex.isPresent());\n+        //Assert.assertTrue(altIndex.isPresent());\n+        if (!altIndex.isPresent()) {\n+            int g = 90;", "originalCommit": "e0113bb902c67e48dbf713d277f4bb3cb2ec2ab7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzA2MjUzNA==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r483062534", "bodyText": "So sloppy of me.  I put in dummy code to insert breakpoints when I don't feel like using conditional breakpoints.", "author": "davidbenjamin", "createdAt": "2020-09-03T15:23:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk2ODE1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk3MzcxOQ==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r455973719", "bodyText": "I would decompose this part of the method into a new method \"markWorstGoodChainsToSatisfyMaxVariants\" just to make it clearer that this is doing something demonstrably different from the rest of the method and to clean things up a little.", "author": "jamesemery", "createdAt": "2020-07-16T18:04:07Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +34,136 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+        // pre-compute the left and right log odds of each chain\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = chains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        // compute correspondence of vertices to incident chains with log odds above the seeding and extending thresholds\n+        final Multimap<V, Path<V,E>> vertexToSeedableChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodIncomingChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodOutgoingChains = ArrayListMultimap.create();\n+\n+        for (final Path<V,E> chain : chains) {\n+            if (chainLogOdds.get(chain).getRight() >= logOddsThreshold) {\n+                vertexToGoodIncomingChains.put(chain.getLastVertex(), chain);\n+            }\n \n-        chainLogOdds.forEach((chain, lod) -> {\n-            if (lod < logOddsThreshold) {\n-                result.add(chain);\n+            if (chainLogOdds.get(chain).getLeft() >= logOddsThreshold) {\n+                vertexToGoodOutgoingChains.put(chain.getFirstVertex(), chain);\n             }\n-        });\n \n-        chains.stream().filter(c -> isChainPossibleVariant(c, graph))\n-                .sorted(Comparator.comparingDouble((ToDoubleFunction<Path<V, E>>) chainLogOdds::get)\n-                        .reversed().thenComparingInt(Path::length))\n-                .skip(maxUnprunedVariants)\n-                .forEach(result::add);\n+            // seed-worthy chains must pass the more stringent seeding log odds threshold on both sides\n+            // in addition to that, we only seed from vertices with multiple such chains incoming or outgoing (see below)\n+            if (chainLogOdds.get(chain).getRight() >= seedingLogOddsThreshold && chainLogOdds.get(chain).getLeft() >= seedingLogOddsThreshold) {\n+                vertexToSeedableChains.put(chain.getFirstVertex(), chain);\n+                vertexToSeedableChains.put(chain.getLastVertex(), chain);\n+            }\n+        }\n \n-        return result;\n+        // find a subset of good vertices from which to grow the subgraph of good chains.\n+        final Queue<V> verticesToProcess = new ArrayDeque<>();\n+        final Path<V,E> maxWeightChain = getMaxWeightChain(chains);\n+        verticesToProcess.add(maxWeightChain.getFirstVertex());\n+        verticesToProcess.add(maxWeightChain.getLastVertex());\n+\n+        // look for vertices with two incoming or two outgoing chains (plus one outgoing or incoming for a total of 3 or more) with good log odds to seed the subgraph of good vertices\n+        // the logic here is that a high-multiplicity error chain A that branches into a second error chain B and a continuation-of-the-original-error chain A'\n+        // may have a high log odds for A'.  However, only in the case of true variation will  multiple branches leaving the same vertex have good log odds.\n+        vertexToSeedableChains.keySet().stream()\n+                .filter(v -> vertexToSeedableChains.get(v).size() > 2)\n+                .forEach(verticesToProcess::add);\n+\n+        final Set<V> processedVertices = new LinkedHashSet<>();\n+        final Set<Path<V,E>> goodChains = new LinkedHashSet<>();\n+\n+        // starting from the high-confidence seed vertices, grow the \"good\" subgraph along chains with above-threshold log odds,\n+        // discovering good chains as we go.\n+        while (!verticesToProcess.isEmpty()) {\n+            final V vertex = verticesToProcess.poll();\n+            processedVertices.add(vertex);\n+            for (final Path<V,E> outgoingChain : vertexToGoodOutgoingChains.get(vertex)) {\n+                goodChains.add(outgoingChain);\n+                if(!processedVertices.contains(outgoingChain.getLastVertex())) {\n+                    verticesToProcess.add(outgoingChain.getLastVertex());\n+                }\n+            }\n \n-    }\n+            for (final Path<V,E> incomingChain : vertexToGoodIncomingChains.get(vertex)) {\n+                goodChains.add(incomingChain);\n+                if(!processedVertices.contains(incomingChain.getFirstVertex())) {\n+                    verticesToProcess.add(incomingChain.getFirstVertex());\n+                }\n+            }\n+        }\n+\n+        final Set<Path<V,E>> errorChains = chains.stream().filter(c -> !goodChains.contains(c)).collect(Collectors.toSet());\n+\n+        // A vertex with N > 0 outgoing good chains corresponds to N - 1 variants\n+        int numberOfVariantsInGraph = vertexToGoodOutgoingChains.keySet().stream()\n+                .mapToInt(v -> Math.max(vertexToGoodOutgoingChains.get(v).size() - 1, 0)).sum();\n+\n+        if (numberOfVariantsInGraph > maxUnprunedVariants) {", "originalCommit": "e0113bb902c67e48dbf713d277f4bb3cb2ec2ab7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzA2OTcwNg==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r483069706", "bodyText": "done", "author": "davidbenjamin", "createdAt": "2020-09-03T15:32:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk3MzcxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk4ODAyMw==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r455988023", "bodyText": "Hmm... I still think this code carries a risk, it seems possible to me that at this stage you could end up leaving a damaged graph by pruning good chains as part of a bubble that were not zipped together but appear in different orders.\nImagine you have two variants A and B and they are both \"good\" that means 2 bubbles in the graph after puning but both variants had a second bubble that was pruned and thus they are sepearted by chaings A1, A2, B1, and B2. Now say that the order in which the lod scores shakes out the best LOD scores are A1, B1, B2, A2 and we only allow 1 maxUnprunedVariant. In this circumstance it seems possible to me that we might over prune the graph in a destructive manner. The graph will look at A2 first, remove it (wihtout decrementing numberOfVariantsInGraph), then B2 (again without decrementing numberOfVariantsInGraph) then hit B1 and remove it and arrive on the \"corect\" number of variants leftover in the graph (1) with the only chain left being that A1 chain. In this situation the pruning code will delete the chains A2, B1, and B2 which could have left the A1 bubble as a dangling end potentially without enough information to accurately recover it. It seems the absolute safest thing to do in this circumstance would be to \"zip\" the chains before calling this good chain bubble pruning in some capacity. This could either be done by reprocessing everything before pruning good chains so it will never be split across multiple chains that could be zipped or by fixing the LOD scores when you prune down to a single path.", "author": "jamesemery", "createdAt": "2020-07-16T18:28:03Z", "path": "src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/graphs/AdaptiveChainPruner.java", "diffHunk": "@@ -29,41 +34,136 @@ public AdaptiveChainPruner(final double initialErrorProbability, final double lo\n \n         final BaseGraph<V,E> graph = chains.get(0).getGraph();\n \n+\n+\n         Collection<Path<V,E>> probableErrorChains = likelyErrorChains(chains, graph, initialErrorProbability);\n         final int errorCount = probableErrorChains.stream().mapToInt(c -> c.getLastEdge().getMultiplicity()).sum();\n         final int totalBases = chains.stream().mapToInt(c -> c.getEdges().stream().mapToInt(E::getMultiplicity).sum()).sum();\n         final double errorRate = (double) errorCount / totalBases;\n \n-        return likelyErrorChains(chains, graph, errorRate);\n+        return likelyErrorChains(chains, graph, errorRate).stream().filter(c -> !c.getEdges().stream().anyMatch(BaseEdge::isRef)).collect(Collectors.toList());\n     }\n \n     private Collection<Path<V,E>> likelyErrorChains(final List<Path<V, E>> chains, final BaseGraph<V,E> graph, final double errorRate) {\n-        final Map<Path<V,E>, Double> chainLogOdds = chains.stream()\n+        // pre-compute the left and right log odds of each chain\n+        final Map<Path<V,E>, Pair<Double, Double>> chainLogOdds = chains.stream()\n                 .collect(Collectors.toMap(c -> c, c-> chainLogOdds(c, graph, errorRate)));\n \n-        final Set<Path<V,E>> result = new HashSet<>(chains.size());\n+        // compute correspondence of vertices to incident chains with log odds above the seeding and extending thresholds\n+        final Multimap<V, Path<V,E>> vertexToSeedableChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodIncomingChains = ArrayListMultimap.create();\n+        final Multimap<V, Path<V,E>> vertexToGoodOutgoingChains = ArrayListMultimap.create();\n+\n+        for (final Path<V,E> chain : chains) {\n+            if (chainLogOdds.get(chain).getRight() >= logOddsThreshold) {\n+                vertexToGoodIncomingChains.put(chain.getLastVertex(), chain);\n+            }\n \n-        chainLogOdds.forEach((chain, lod) -> {\n-            if (lod < logOddsThreshold) {\n-                result.add(chain);\n+            if (chainLogOdds.get(chain).getLeft() >= logOddsThreshold) {\n+                vertexToGoodOutgoingChains.put(chain.getFirstVertex(), chain);\n             }\n-        });\n \n-        chains.stream().filter(c -> isChainPossibleVariant(c, graph))\n-                .sorted(Comparator.comparingDouble((ToDoubleFunction<Path<V, E>>) chainLogOdds::get)\n-                        .reversed().thenComparingInt(Path::length))\n-                .skip(maxUnprunedVariants)\n-                .forEach(result::add);\n+            // seed-worthy chains must pass the more stringent seeding log odds threshold on both sides\n+            // in addition to that, we only seed from vertices with multiple such chains incoming or outgoing (see below)\n+            if (chainLogOdds.get(chain).getRight() >= seedingLogOddsThreshold && chainLogOdds.get(chain).getLeft() >= seedingLogOddsThreshold) {\n+                vertexToSeedableChains.put(chain.getFirstVertex(), chain);\n+                vertexToSeedableChains.put(chain.getLastVertex(), chain);\n+            }\n+        }\n \n-        return result;\n+        // find a subset of good vertices from which to grow the subgraph of good chains.\n+        final Queue<V> verticesToProcess = new ArrayDeque<>();\n+        final Path<V,E> maxWeightChain = getMaxWeightChain(chains);\n+        verticesToProcess.add(maxWeightChain.getFirstVertex());\n+        verticesToProcess.add(maxWeightChain.getLastVertex());\n+\n+        // look for vertices with two incoming or two outgoing chains (plus one outgoing or incoming for a total of 3 or more) with good log odds to seed the subgraph of good vertices\n+        // the logic here is that a high-multiplicity error chain A that branches into a second error chain B and a continuation-of-the-original-error chain A'\n+        // may have a high log odds for A'.  However, only in the case of true variation will  multiple branches leaving the same vertex have good log odds.\n+        vertexToSeedableChains.keySet().stream()\n+                .filter(v -> vertexToSeedableChains.get(v).size() > 2)\n+                .forEach(verticesToProcess::add);\n+\n+        final Set<V> processedVertices = new LinkedHashSet<>();\n+        final Set<Path<V,E>> goodChains = new LinkedHashSet<>();\n+\n+        // starting from the high-confidence seed vertices, grow the \"good\" subgraph along chains with above-threshold log odds,\n+        // discovering good chains as we go.\n+        while (!verticesToProcess.isEmpty()) {\n+            final V vertex = verticesToProcess.poll();\n+            processedVertices.add(vertex);\n+            for (final Path<V,E> outgoingChain : vertexToGoodOutgoingChains.get(vertex)) {\n+                goodChains.add(outgoingChain);\n+                if(!processedVertices.contains(outgoingChain.getLastVertex())) {\n+                    verticesToProcess.add(outgoingChain.getLastVertex());\n+                }\n+            }\n \n-    }\n+            for (final Path<V,E> incomingChain : vertexToGoodIncomingChains.get(vertex)) {\n+                goodChains.add(incomingChain);\n+                if(!processedVertices.contains(incomingChain.getFirstVertex())) {\n+                    verticesToProcess.add(incomingChain.getFirstVertex());\n+                }\n+            }\n+        }\n+\n+        final Set<Path<V,E>> errorChains = chains.stream().filter(c -> !goodChains.contains(c)).collect(Collectors.toSet());\n+\n+        // A vertex with N > 0 outgoing good chains corresponds to N - 1 variants\n+        int numberOfVariantsInGraph = vertexToGoodOutgoingChains.keySet().stream()\n+                .mapToInt(v -> Math.max(vertexToGoodOutgoingChains.get(v).size() - 1, 0)).sum();\n+\n+        if (numberOfVariantsInGraph > maxUnprunedVariants) {\n+            // the vertex-to-good-incoming/outgoing chain maps contain all chains with log odds passing the threshold, even if\n+            // the seeding and extending process revealed them to be errors.  We need to cull such chains for the following steps\n+            for (final Path<V,E> chain : errorChains) {\n+                vertexToGoodOutgoingChains.remove(chain.getFirstVertex(), chain);\n+                vertexToGoodIncomingChains.remove(chain.getLastVertex(), chain);\n+            }\n+\n+            // recalculate now that we have more accurate vertex-to-good-chains maps\n+            numberOfVariantsInGraph = vertexToGoodOutgoingChains.keySet().stream()\n+                    .mapToInt(v -> Math.max(vertexToGoodOutgoingChains.get(v).size() - 1, 0)).sum();\n \n-    private double chainLogOdds(final Path<V,E> chain, final BaseGraph<V,E> graph, final double errorRate) {\n-        if (chain.getEdges().stream().anyMatch(E::isRef)) {\n-            return Double.POSITIVE_INFINITY;\n+            // start with the worst good variants\n+            final PriorityQueue<Path<V,E>> excessGoodChainsToPrune = new PriorityQueue<>(\n+                    Comparator.comparingDouble((Path<V,E> c) -> Math.min(chainLogOdds.get(c).getLeft(), chainLogOdds.get(c).getLeft()))\n+                            .thenComparing((Path<V,E> c) -> c.getFirstVertex().getSequence(), BaseUtils.BASES_COMPARATOR));\n+\n+            excessGoodChainsToPrune.addAll(goodChains);\n+\n+            while (numberOfVariantsInGraph > maxUnprunedVariants) {\n+                final Path<V,E> worstGoodChain = excessGoodChainsToPrune.poll();\n+                errorChains.add(worstGoodChain);\n+                //if removing this chain pops a bubble, we have pruned a variant\n+                if (vertexToGoodOutgoingChains.get(worstGoodChain.getFirstVertex()).size() > 1) {", "originalCommit": "e0113bb902c67e48dbf713d277f4bb3cb2ec2ab7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzIxNTE4Ng==", "url": "https://github.com/broadinstitute/gatk/pull/6520#discussion_r483215186", "bodyText": "You're right; this is possible.  I changed the logic so that we remove all chains within a broken bubble at once.  That is, when we delete A2 we also delete A1.  Essentially we are zipping on the fly within the subgraph of chains that have not already been pruned.", "author": "davidbenjamin", "createdAt": "2020-09-03T19:46:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTk4ODAyMw=="}], "type": "inlineReview"}, {"oid": "5f05b956315863eeb37356e7122ce8c5ac25927a", "url": "https://github.com/broadinstitute/gatk/commit/5f05b956315863eeb37356e7122ce8c5ac25927a", "message": "Improve adaptive pruner to be smarter about adjacent chains with errors", "committedDate": "2020-09-10T02:55:02Z", "type": "commit"}, {"oid": "0c0e99ae3e0c38845748a109ba3cad88ab300ac0", "url": "https://github.com/broadinstitute/gatk/commit/0c0e99ae3e0c38845748a109ba3cad88ab300ac0", "message": "review edits", "committedDate": "2020-09-10T02:55:02Z", "type": "commit"}, {"oid": "de7cdf669fd42bf60a1995eee3427ce8ce5c6496", "url": "https://github.com/broadinstitute/gatk/commit/de7cdf669fd42bf60a1995eee3427ce8ce5c6496", "message": "handled James' concern", "committedDate": "2020-09-10T02:55:02Z", "type": "commit"}, {"oid": "672cf461ec3faa201716a271cf9f4d0751cf0dbd", "url": "https://github.com/broadinstitute/gatk/commit/672cf461ec3faa201716a271cf9f4d0751cf0dbd", "message": "in the thick of things", "committedDate": "2020-09-10T02:55:03Z", "type": "commit"}, {"oid": "0e35a0311ccada14f1e078f6538b5a264df444be", "url": "https://github.com/broadinstitute/gatk/commit/0e35a0311ccada14f1e078f6538b5a264df444be", "message": "oh boy", "committedDate": "2020-09-10T02:55:03Z", "type": "commit"}, {"oid": "4067d9425c5705aeb9e615896118c952f6d1da5b", "url": "https://github.com/broadinstitute/gatk/commit/4067d9425c5705aeb9e615896118c952f6d1da5b", "message": "commit", "committedDate": "2020-09-10T02:55:03Z", "type": "commit"}, {"oid": "507e23d94efa185384e44f39cc0e0047dda4d9e9", "url": "https://github.com/broadinstitute/gatk/commit/507e23d94efa185384e44f39cc0e0047dda4d9e9", "message": "seems like it works", "committedDate": "2020-09-10T03:00:55Z", "type": "commit"}, {"oid": "507e23d94efa185384e44f39cc0e0047dda4d9e9", "url": "https://github.com/broadinstitute/gatk/commit/507e23d94efa185384e44f39cc0e0047dda4d9e9", "message": "seems like it works", "committedDate": "2020-09-10T03:00:55Z", "type": "forcePushed"}, {"oid": "877ec3e9ad429e623be5df880f9a2e2bf9190ca9", "url": "https://github.com/broadinstitute/gatk/commit/877ec3e9ad429e623be5df880f9a2e2bf9190ca9", "message": "post-rebase crud", "committedDate": "2020-09-10T18:35:48Z", "type": "commit"}]}