{"pr_number": 1043, "pr_title": "CDAP-16324: [CDAP-16324] Improve macro support for analytics plugins", "pr_createdAt": "2020-03-20T23:23:07Z", "pr_url": "https://github.com/cdapio/hydrator-plugins/pull/1043", "timeline": [{"oid": "03e6373d3fe1791202f46df6f827908ac350400a", "url": "https://github.com/cdapio/hydrator-plugins/commit/03e6373d3fe1791202f46df6f827908ac350400a", "message": "[CDAP-16324] Improve macro support for analytics plugins.\n\n- Added macro support for row denormalizer.\n- numPartitions is macro enabled.\n- Handle cases where output schema is not known.", "committedDate": "2020-03-21T03:41:08Z", "type": "forcePushed"}, {"oid": "8e4523d87b6f75f31bddaf5a98a8908f2ecb2908", "url": "https://github.com/cdapio/hydrator-plugins/commit/8e4523d87b6f75f31bddaf5a98a8908f2ecb2908", "message": "[CDAP-16324] Improve macro support for analytics plugins.\n\n- Added macro support for row denormalizer.\n- numPartitions is macro enabled.\n- Handle cases where output schema is not known.", "committedDate": "2020-03-22T20:21:06Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU3NjU5Ng==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r397576596", "bodyText": "better not to remove this in case in the future some common class is constructed for the prepareRun() method", "author": "yaojiefeng", "createdAt": "2020-03-25T02:36:52Z", "path": "core-plugins/src/main/java/io/cdap/plugin/batch/aggregator/DedupAggregator.java", "diffHunk": "@@ -49,17 +50,19 @@\n   private DedupConfig.DedupFunctionInfo filterFunction;\n \n   public DedupAggregator(DedupConfig dedupConfig) {\n-    super(dedupConfig.numPartitions);\n     this.dedupConfig = dedupConfig;\n   }\n \n   @Override\n   public void prepareRun(BatchAggregatorContext context) throws Exception {\n-    super.prepareRun(context);", "originalCommit": "8e4523d87b6f75f31bddaf5a98a8908f2ecb2908", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODE3Njk4Ng==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r398176986", "bodyText": "fixed.", "author": "rmstar", "createdAt": "2020-03-25T21:20:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU3NjU5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU3NjkzMw==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r397576933", "bodyText": "what does this \"output\" mean? The validator seems to use this in the log message if field lineage cannot be recorded, better to use context.getStageName() for better identification", "author": "yaojiefeng", "createdAt": "2020-03-25T02:38:18Z", "path": "core-plugins/src/main/java/io/cdap/plugin/batch/aggregator/DedupAggregator.java", "diffHunk": "@@ -49,17 +50,19 @@\n   private DedupConfig.DedupFunctionInfo filterFunction;\n \n   public DedupAggregator(DedupConfig dedupConfig) {\n-    super(dedupConfig.numPartitions);\n     this.dedupConfig = dedupConfig;\n   }\n \n   @Override\n   public void prepareRun(BatchAggregatorContext context) throws Exception {\n-    super.prepareRun(context);\n+    setNumPartitions(context, dedupConfig.numPartitions);\n \n-    TransformLineageRecorderUtils.generateOneToOnes(\n-      TransformLineageRecorderUtils.getFields(context.getInputSchema()), \"dedup\",\n-    \"Removed duplicate records based on unique fields.\");\n+    // in configurePipeline all the necessary checks have been performed already to set output schema\n+    if (SchemaValidator.canRecordLineage(context.getOutputSchema(), \"output\")) {", "originalCommit": "8e4523d87b6f75f31bddaf5a98a8908f2ecb2908", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODE3NzIyMA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r398177220", "bodyText": "fixed.", "author": "rmstar", "createdAt": "2020-03-25T21:20:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU3NjkzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU3NzAxOQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r397577019", "bodyText": "nit - indentation is off", "author": "yaojiefeng", "createdAt": "2020-03-25T02:38:36Z", "path": "core-plugins/src/main/java/io/cdap/plugin/batch/aggregator/DedupAggregator.java", "diffHunk": "@@ -49,17 +50,19 @@\n   private DedupConfig.DedupFunctionInfo filterFunction;\n \n   public DedupAggregator(DedupConfig dedupConfig) {\n-    super(dedupConfig.numPartitions);\n     this.dedupConfig = dedupConfig;\n   }\n \n   @Override\n   public void prepareRun(BatchAggregatorContext context) throws Exception {\n-    super.prepareRun(context);\n+    setNumPartitions(context, dedupConfig.numPartitions);\n \n-    TransformLineageRecorderUtils.generateOneToOnes(\n-      TransformLineageRecorderUtils.getFields(context.getInputSchema()), \"dedup\",\n-    \"Removed duplicate records based on unique fields.\");\n+    // in configurePipeline all the necessary checks have been performed already to set output schema\n+    if (SchemaValidator.canRecordLineage(context.getOutputSchema(), \"output\")) {\n+      TransformLineageRecorderUtils.generateOneToOnes(\n+              TransformLineageRecorderUtils.getFields(context.getInputSchema()), \"dedup\",", "originalCommit": "8e4523d87b6f75f31bddaf5a98a8908f2ecb2908", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODE4MzUyMw==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r398183523", "bodyText": "fixed.", "author": "rmstar", "createdAt": "2020-03-25T21:33:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU3NzAxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU3ODc5MQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r397578791", "bodyText": "numPartitions should be @nullable", "author": "yaojiefeng", "createdAt": "2020-03-25T02:45:33Z", "path": "core-plugins/src/main/java/io/cdap/plugin/batch/aggregator/RecordAggregator.java", "diffHunk": "@@ -26,15 +26,7 @@\n  * Base class for StructuredRecord based aggregators.\n  */\n public abstract class RecordAggregator extends BatchAggregator<StructuredRecord, StructuredRecord, StructuredRecord> {\n-  @Nullable\n-  private final Integer numPartitions;\n-\n-  protected RecordAggregator(@Nullable Integer numPartitions) {\n-    this.numPartitions = numPartitions;\n-  }\n-\n-  @Override\n-  public void prepareRun(BatchAggregatorContext context) throws Exception {\n+  public void setNumPartitions(BatchAggregatorContext context, Integer numPartitions) throws Exception {", "originalCommit": "8e4523d87b6f75f31bddaf5a98a8908f2ecb2908", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODE4Mzg5MQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r398183891", "bodyText": "reverted changed in this file.", "author": "rmstar", "createdAt": "2020-03-25T21:33:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU3ODc5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU3OTYwNA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r397579604", "bodyText": "Do we need to change this class? I feel it will still work since all the macros will be resolved in prepareRun() stage. Look at AbstractFileSource, it uses a similar structure.", "author": "yaojiefeng", "createdAt": "2020-03-25T02:48:29Z", "path": "core-plugins/src/main/java/io/cdap/plugin/batch/aggregator/RecordAggregator.java", "diffHunk": "@@ -26,15 +26,7 @@\n  * Base class for StructuredRecord based aggregators.\n  */\n public abstract class RecordAggregator extends BatchAggregator<StructuredRecord, StructuredRecord, StructuredRecord> {\n-  @Nullable\n-  private final Integer numPartitions;\n-\n-  protected RecordAggregator(@Nullable Integer numPartitions) {", "originalCommit": "8e4523d87b6f75f31bddaf5a98a8908f2ecb2908", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODE4NDAzNg==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r398184036", "bodyText": "Reverted changes in this file.", "author": "rmstar", "createdAt": "2020-03-25T21:34:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU3OTYwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU4MDA1Mg==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r397580052", "bodyText": "Actually this method shouldn't be needed, you should modify the validateInputFields to handle the macro case, can return a boolean if an output schema can be initialized", "author": "yaojiefeng", "createdAt": "2020-03-25T02:50:08Z", "path": "core-plugins/src/main/java/io/cdap/plugin/batch/aggregator/RowDenormalizerAggregator.java", "diffHunk": "@@ -196,4 +200,16 @@ private void validateInputFields(Schema inputSchema, FailureCollector collector)\n     }\n \n   }\n+\n+  /**\n+   * @returns true if input schema and field values are known.\n+   */\n+\n+  private boolean inputFieldsAvailable(Schema inputSchema) {", "originalCommit": "8e4523d87b6f75f31bddaf5a98a8908f2ecb2908", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODE4NDEyOA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r398184128", "bodyText": "fixed.", "author": "rmstar", "createdAt": "2020-03-25T21:34:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU4MDA1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU4MDk1NQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r397580955", "bodyText": "nit - identation", "author": "yaojiefeng", "createdAt": "2020-03-25T02:53:14Z", "path": "core-plugins/src/test/java/io/cdap/plugin/batch/aggregator/DedupTestRun.java", "diffHunk": "@@ -16,121 +16,134 @@\n \n package io.cdap.plugin.batch.aggregator;\n \n+import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Sets;\n+import io.cdap.cdap.api.data.format.StructuredRecord;\n import io.cdap.cdap.api.data.schema.Schema;\n-import io.cdap.cdap.api.dataset.table.Put;\n-import io.cdap.cdap.api.dataset.table.Row;\n import io.cdap.cdap.api.dataset.table.Table;\n import io.cdap.cdap.etl.api.batch.BatchAggregator;\n-import io.cdap.cdap.etl.api.batch.BatchSink;\n-import io.cdap.cdap.etl.api.batch.BatchSource;\n+import io.cdap.cdap.etl.mock.batch.MockSink;\n+import io.cdap.cdap.etl.mock.batch.MockSource;\n import io.cdap.cdap.etl.proto.v2.ETLBatchConfig;\n import io.cdap.cdap.etl.proto.v2.ETLPlugin;\n import io.cdap.cdap.etl.proto.v2.ETLStage;\n import io.cdap.cdap.test.ApplicationManager;\n import io.cdap.cdap.test.DataSetManager;\n import io.cdap.plugin.batch.ETLBatchTestBase;\n-import io.cdap.plugin.common.Properties;\n-import org.apache.hadoop.hbase.util.Bytes;\n import org.junit.Assert;\n import org.junit.Test;\n \n+import java.util.List;\n+\n /**\n  * Test for Dedup Aggregator.\n  */\n public class DedupTestRun extends ETLBatchTestBase {\n-\n+  private static final Schema PURCHASE_SCHEMA = Schema.recordOf(\n+          \"purchase\",\n+          Schema.Field.of(\"fname\", Schema.of(Schema.Type.STRING)),\n+          Schema.Field.of(\"lname\", Schema.of(Schema.Type.STRING)),\n+          Schema.Field.of(\"ts\", Schema.of(Schema.Type.INT)),\n+          Schema.Field.of(\"price\", Schema.of(Schema.Type.DOUBLE)));\n+  private static final Schema SINK_SCHEMA = Schema.recordOf(\"sinkSchema\",\n+          Schema.Field.of(\"fname\", Schema.of(Schema.Type.STRING)),\n+          Schema.Field.of(\"lname\", Schema.of(Schema.Type.STRING)),\n+          Schema.Field.of(\"ts\", Schema.of(Schema.Type.INT)),\n+          Schema.Field.of(\"price\", Schema.of(Schema.Type.DOUBLE)));\n+\n+private void testHelper(String appName, String purchasesDatasetName, ETLStage purchaseStage,", "originalCommit": "8e4523d87b6f75f31bddaf5a98a8908f2ecb2908", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODE4NDUwNQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r398184505", "bodyText": "fixed.", "author": "rmstar", "createdAt": "2020-03-25T21:34:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU4MDk1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU4MjMyMw==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r397582323", "bodyText": "It seems the only difference between two tests is the source knows the output schema or not but your changes is about macro, can you also add a test about that?", "author": "yaojiefeng", "createdAt": "2020-03-25T02:58:25Z", "path": "core-plugins/src/test/java/io/cdap/plugin/batch/aggregator/DedupTestRun.java", "diffHunk": "@@ -16,121 +16,134 @@\n \n package io.cdap.plugin.batch.aggregator;\n \n+import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Sets;\n+import io.cdap.cdap.api.data.format.StructuredRecord;\n import io.cdap.cdap.api.data.schema.Schema;\n-import io.cdap.cdap.api.dataset.table.Put;\n-import io.cdap.cdap.api.dataset.table.Row;\n import io.cdap.cdap.api.dataset.table.Table;\n import io.cdap.cdap.etl.api.batch.BatchAggregator;\n-import io.cdap.cdap.etl.api.batch.BatchSink;\n-import io.cdap.cdap.etl.api.batch.BatchSource;\n+import io.cdap.cdap.etl.mock.batch.MockSink;\n+import io.cdap.cdap.etl.mock.batch.MockSource;\n import io.cdap.cdap.etl.proto.v2.ETLBatchConfig;\n import io.cdap.cdap.etl.proto.v2.ETLPlugin;\n import io.cdap.cdap.etl.proto.v2.ETLStage;\n import io.cdap.cdap.test.ApplicationManager;\n import io.cdap.cdap.test.DataSetManager;\n import io.cdap.plugin.batch.ETLBatchTestBase;\n-import io.cdap.plugin.common.Properties;\n-import org.apache.hadoop.hbase.util.Bytes;\n import org.junit.Assert;\n import org.junit.Test;\n \n+import java.util.List;\n+\n /**\n  * Test for Dedup Aggregator.\n  */\n public class DedupTestRun extends ETLBatchTestBase {\n-\n+  private static final Schema PURCHASE_SCHEMA = Schema.recordOf(\n+          \"purchase\",\n+          Schema.Field.of(\"fname\", Schema.of(Schema.Type.STRING)),\n+          Schema.Field.of(\"lname\", Schema.of(Schema.Type.STRING)),\n+          Schema.Field.of(\"ts\", Schema.of(Schema.Type.INT)),\n+          Schema.Field.of(\"price\", Schema.of(Schema.Type.DOUBLE)));\n+  private static final Schema SINK_SCHEMA = Schema.recordOf(\"sinkSchema\",\n+          Schema.Field.of(\"fname\", Schema.of(Schema.Type.STRING)),\n+          Schema.Field.of(\"lname\", Schema.of(Schema.Type.STRING)),\n+          Schema.Field.of(\"ts\", Schema.of(Schema.Type.INT)),\n+          Schema.Field.of(\"price\", Schema.of(Schema.Type.DOUBLE)));\n+\n+private void testHelper(String appName, String purchasesDatasetName, ETLStage purchaseStage,\n+                        ETLStage dedupStage, String sinkDatasetName, ETLStage sinkStage) throws Exception {\n+  ETLBatchConfig config = ETLBatchConfig.builder()\n+          .addStage(purchaseStage)\n+          .addStage(dedupStage)\n+          .addStage(sinkStage)\n+          .addConnection(purchaseStage.getName(), dedupStage.getName())\n+          .addConnection(dedupStage.getName(), sinkStage.getName())\n+          .build();\n+\n+  ApplicationManager appManager = deployETL(config, appName);\n+\n+  // write input data\n+  // 1: samuel, goel, 10, 100.31\n+  // 2: samuel, goel, 11, 200.43\n+  // 3: john, desai, 5, 300.45\n+  // 4: john, desai, 1, 400.12\n+\n+  DataSetManager<Table> purchaseManager = getDataset(purchasesDatasetName);\n+  //Table purchaseTable = purchaseManager.get();\n+  List<StructuredRecord> input = ImmutableList.of(\n+          StructuredRecord.builder(PURCHASE_SCHEMA)\n+                  .set(\"fname\", \"samuel\")\n+                  .set(\"lname\", \"goel\")\n+                  .set(\"ts\", 10)\n+                  .set(\"price\", 100.31)\n+                  .build(),\n+          StructuredRecord.builder(PURCHASE_SCHEMA)\n+                  .set(\"fname\", \"samuel\")\n+                  .set(\"lname\", \"goel\")\n+                  .set(\"ts\", 11)\n+                  .set(\"price\", 200.43)\n+                  .build(),\n+          StructuredRecord.builder(PURCHASE_SCHEMA)\n+                  .set(\"fname\", \"john\")\n+                  .set(\"lname\", \"desai\")\n+                  .set(\"ts\", 5)\n+                  .set(\"price\", 300.45)\n+                  .build(),\n+          StructuredRecord.builder(PURCHASE_SCHEMA)\n+                  .set(\"fname\", \"john\")\n+                  .set(\"lname\", \"desai\")\n+                  .set(\"ts\", 1)\n+                  .set(\"price\", 400.12)\n+                  .build());\n+  MockSource.writeInput(purchaseManager, input);\n+\n+  runETLOnce(appManager);\n+\n+  DataSetManager<Table> sinkManager = getDataset(sinkDatasetName);\n+  List<StructuredRecord> output = MockSink.readOutput(sinkManager);\n+  Assert.assertEquals(\"Expected records\", 2, output.size());\n+  List<StructuredRecord> expectedOutput = ImmutableList.of(\n+          StructuredRecord.builder(PURCHASE_SCHEMA)\n+                  .set(\"fname\", \"samuel\")\n+                  .set(\"lname\", \"goel\")\n+                  .set(\"ts\", 11)\n+                  .set(\"price\", 200.43)\n+                  .build(),\n+          StructuredRecord.builder(PURCHASE_SCHEMA)\n+                  .set(\"fname\", \"john\")\n+                  .set(\"lname\", \"desai\")\n+                  .set(\"ts\", 5)\n+                  .set(\"price\", 300.45)\n+                  .build());\n+  Assert.assertEquals(Sets.newHashSet(output), Sets.newHashSet(expectedOutput));\n+}\n   @Test\n   public void testDedup() throws Exception {\n     String purchasesDatasetName = \"purchases\";\n     String sinkDatasetName = \"sinkDataset\";\n-\n-    Schema purchaseSchema = Schema.recordOf(\n-      \"purchase\",\n-      Schema.Field.of(\"fname\", Schema.of(Schema.Type.STRING)),\n-      Schema.Field.of(\"lname\", Schema.of(Schema.Type.STRING)),\n-      Schema.Field.of(\"ts\", Schema.of(Schema.Type.INT)),\n-      Schema.Field.of(\"price\", Schema.of(Schema.Type.DOUBLE)));\n-\n-    ETLStage purchaseStage = new ETLStage(\"purchases\", new ETLPlugin(\n-      \"Table\", BatchSource.PLUGIN_TYPE, ImmutableMap.of(Properties.BatchReadableWritable.NAME, purchasesDatasetName,\n-                                                        Properties.Table.PROPERTY_SCHEMA, purchaseSchema.toString()),\n-      null));\n+    String appName = \"dedup-test\";\n+    ETLStage purchaseStage = new ETLStage(\"purchases\", MockSource.getPlugin(purchasesDatasetName, PURCHASE_SCHEMA));\n     ETLStage dedupStage = new ETLStage(\"dedupStage\", new ETLPlugin(\n       \"Deduplicate\", BatchAggregator.PLUGIN_TYPE, ImmutableMap.of(\"uniqueFields\", \"fname,lname\",\n                                                                   \"filterOperation\", \"ts:max\"), null));\n+    ETLStage sinkStage = new ETLStage(\"tableSink\", MockSink.getPlugin(sinkDatasetName));\n \n-    Schema sinkSchema = Schema.recordOf(\"sinkSchema\", Schema.Field.of(\"fname\", Schema.of(Schema.Type.STRING)),\n-                                        Schema.Field.of(\"lname\", Schema.of(Schema.Type.STRING)),\n-                                        Schema.Field.of(\"ts\", Schema.of(Schema.Type.INT)),\n-                                        Schema.Field.of(\"price\", Schema.of(Schema.Type.DOUBLE)));\n-    ETLStage sinkStage = new ETLStage(\"tableSink\", new ETLPlugin(\n-      \"Table\", BatchSink.PLUGIN_TYPE, ImmutableMap.of(Properties.BatchReadableWritable.NAME, sinkDatasetName,\n-                                                      Properties.Table.PROPERTY_SCHEMA, sinkSchema.toString(),\n-                                                      Properties.Table.PROPERTY_SCHEMA_ROW_FIELD, \"ts\"), null));\n-\n-    ETLBatchConfig config = ETLBatchConfig.builder(\"* * * * *\")\n-      .addStage(purchaseStage)\n-      .addStage(dedupStage)\n-      .addStage(sinkStage)\n-      .addConnection(purchaseStage.getName(), dedupStage.getName())\n-      .addConnection(dedupStage.getName(), sinkStage.getName())\n-      .build();\n-\n-    ApplicationManager appManager = deployETL(config, \"dedup-test\");\n-\n-    // write input data\n-    // 1: samuel, goel, 10, 100.31\n-    // 2: samuel, goel, 11, 200.43\n-    // 3: john, desai, 5, 300.45\n-    // 4: john, desai, 1, 400.12\n-\n-    DataSetManager<Table> purchaseManager = getDataset(purchasesDatasetName);\n-    Table purchaseTable = purchaseManager.get();\n-\n-    Put put = new Put(Bytes.toBytes(1));\n-    put.add(\"fname\", \"samuel\");\n-    put.add(\"lname\", \"goel\");\n-    put.add(\"ts\", 10);\n-    put.add(\"price\", 100.31);\n-    purchaseTable.put(put);\n-    put = new Put(Bytes.toBytes(2));\n-    put.add(\"fname\", \"samuel\");\n-    put.add(\"lname\", \"goel\");\n-    put.add(\"ts\", 11);\n-    put.add(\"price\", 200.43);\n-    purchaseTable.put(put);\n-    put = new Put(Bytes.toBytes(3));\n-    put.add(\"fname\", \"john\");\n-    put.add(\"lname\", \"desai\");\n-    put.add(\"ts\", 5);\n-    put.add(\"price\", 300.45);\n-    purchaseTable.put(put);\n-    put = new Put(Bytes.toBytes(4));\n-    put.add(\"fname\", \"john\");\n-    put.add(\"lname\", \"desai\");\n-    put.add(\"ts\", 1);\n-    put.add(\"price\", 400.12);\n-    purchaseTable.put(put);\n-    purchaseManager.flush();\n-\n-    runETLOnce(appManager);\n-\n-    DataSetManager<Table> sinkManager = getDataset(sinkDatasetName);\n-    try (Table sinkTable = sinkManager.get()) {\n+    testHelper(appName, purchasesDatasetName, purchaseStage, dedupStage, sinkDatasetName, sinkStage);\n+  }\n \n-      // table should have:\n-      // 11 : samuel, goel, 200.43\n-      // 5 : john, desai, 300.45\n-      Row row = sinkTable.get(Bytes.toBytes(11));\n-      Assert.assertEquals(\"samuel\", row.getString(\"fname\"));\n-      Assert.assertEquals(\"goel\", row.getString(\"lname\"));\n-      Assert.assertEquals(200.43, row.getDouble(\"price\"), 0.0001);\n+  @Test\n+  public void testDedupWithUnknownInputSchema() throws Exception {\n+    String purchasesDatasetName = \"purchases-null-inputschema\";\n+    String sinkDatasetName = \"sinkDataset-null-inputschema\";\n+    String appName = \"dedup-test-null-inputschema\";\n+    ETLStage purchaseStage = new ETLStage(\"purchases\", MockSource.getPlugin(purchasesDatasetName));", "originalCommit": "8e4523d87b6f75f31bddaf5a98a8908f2ecb2908", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODE4NTA4NQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r398185085", "bodyText": "Modified this testcase to also test other macro fields.", "author": "rmstar", "createdAt": "2020-03-25T21:36:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU4MjMyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU4MjU4Ng==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r397582586", "bodyText": "nit - identation", "author": "yaojiefeng", "createdAt": "2020-03-25T02:59:23Z", "path": "core-plugins/src/test/java/io/cdap/plugin/batch/aggregator/DistinctTestRun.java", "diffHunk": "@@ -76,68 +136,32 @@ public void testDistinct() throws Exception {\n \n     ETLStage sinkStage = new ETLStage(\n       \"sink\", new ETLPlugin(\"TPFSAvro\", BatchSink.PLUGIN_TYPE,\n-                            ImmutableMap.of(Properties.TimePartitionedFileSetDataset.SCHEMA, outputSchema.toString(),\n+                            ImmutableMap.of(Properties.TimePartitionedFileSetDataset.SCHEMA, OUTPUT_SCHEMA.toString(),\n                                             Properties.TimePartitionedFileSetDataset.TPFS_NAME, outputDatasetName),\n                             null));\n+    testHelper(appName, inputDatasetName, sourceStage, distinctStage, outputDatasetName, sinkStage);\n+  }\n \n-    ETLBatchConfig config = ETLBatchConfig.builder(\"* * * * *\")\n-      .addStage(sourceStage)\n-      .addStage(distinctStage)\n-      .addStage(sinkStage)\n-      .addConnection(sourceStage.getName(), distinctStage.getName())\n-      .addConnection(distinctStage.getName(), sinkStage.getName())\n-      .build();\n-    ApplicationManager appManager = deployETL(config, \"distinct-test\");\n+  @Test\n+  public void testDistinctWithUnknownInputSchema() throws Exception {\n+    String inputDatasetName = \"distinct-input-unknown-inputschema\";\n+    String outputDatasetName = \"distinct-output-unknown-inputschema\";\n+    String appName = \"distinct-test-unknown-inputschema\";\n \n-    // write input data\n-    DataSetManager<Table> purchaseManager = getDataset(inputDatasetName);\n-    Table purchaseTable = purchaseManager.get();\n-    Put put = new Put(Bytes.toBytes(1));\n-    put.add(\"ts\", 1234567890000L);\n-    put.add(\"user_name\", \"samuel\");\n-    put.add(\"item\", \"shirt\");\n-    put.add(\"price\", 10d);\n-    purchaseTable.put(put);\n-    put = new Put(Bytes.toBytes(2));\n-    put.add(\"ts\", 1234567890001L);\n-    put.add(\"user_name\", \"samuel\");\n-    put.add(\"item\", \"shirt\");\n-    put.add(\"price\", 15.34d);\n-    purchaseTable.put(put);\n-    put = new Put(Bytes.toBytes(3));\n-    put.add(\"ts\", 1234567890001L);\n-    put.add(\"user_name\", \"samuel\");\n-    put.add(\"item\", \"pie\");\n-    put.add(\"price\", 3.14d);\n-    purchaseTable.put(put);\n-    put = new Put(Bytes.toBytes(4));\n-    put.add(\"ts\", 1234567890002L);\n-    put.add(\"user_name\", \"samuel\");\n-    put.add(\"item\", \"pie\");\n-    put.add(\"price\", 3.14d);\n-    purchaseTable.put(put);\n-    put = new Put(Bytes.toBytes(5));\n-    put.add(\"ts\", 1234567890003L);\n-    put.add(\"user_name\", \"samuel\");\n-    put.add(\"item\", \"shirt\");\n-    put.add(\"price\", 20.53d);\n-    purchaseTable.put(put);\n-    purchaseManager.flush();\n+    ETLStage sourceStage = new ETLStage(\"purchases\", MockSource.getPlugin(inputDatasetName));\n \n-    // run the pipeline\n-    runETLOnce(appManager);\n \n-    DataSetManager<TimePartitionedFileSet> outputManager = getDataset(outputDatasetName);\n-    TimePartitionedFileSet fileSet = outputManager.get();\n-    List<GenericRecord> records = readOutput(fileSet, outputSchema);\n-    Assert.assertEquals(2, records.size());\n-    Set<String> items = new HashSet<>();\n-    Set<String> users = new HashSet<>();\n-    for (GenericRecord record : records) {\n-      items.add(record.get(\"item\").toString());\n-      users.add(record.get(\"user_name\").toString());\n-    }\n-    Assert.assertEquals(ImmutableSet.of(\"samuel\"), users);\n-    Assert.assertEquals(ImmutableSet.of(\"shirt\", \"pie\"), items);\n+    ETLStage distinctStage = new ETLStage(\n+            \"distinct\", new ETLPlugin(\"Distinct\", BatchAggregator.PLUGIN_TYPE,", "originalCommit": "8e4523d87b6f75f31bddaf5a98a8908f2ecb2908", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODE4NzQ2Nw==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r398187467", "bodyText": "fixed.", "author": "rmstar", "createdAt": "2020-03-25T21:41:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU4MjU4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODE5MTMyNA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r398191324", "bodyText": "use context.getStageName()", "author": "yaojiefeng", "createdAt": "2020-03-25T21:48:59Z", "path": "core-plugins/src/main/java/io/cdap/plugin/batch/aggregator/DistinctAggregator.java", "diffHunk": "@@ -104,14 +108,16 @@ public void validate(Schema inputSchema, Iterable<String> fields, FailureCollect\n   @Override\n   public void prepareRun(BatchAggregatorContext context) throws Exception {\n     super.prepareRun(context);\n-\n     validate(context.getInputSchema(), conf.getFields(), context.getFailureCollector());\n     context.getFailureCollector().getOrThrowException();\n \n-    List<String> fields = conf.getFields() == null ?\n-      TransformLineageRecorderUtils.getFields(context.getInputSchema()) : Lists.newArrayList(conf.getFields());\n-    context.record(TransformLineageRecorderUtils.generateOneToOnes(fields, \"distinctAggregator\",\n-      \"Removed duplicates in input records.\"));\n+    // in configurePipeline all the necessary checks have been performed already to set output schema\n+    if (SchemaValidator.canRecordLineage(context.getOutputSchema(), \"output\")) {", "originalCommit": "4b573c34086000613b59e99ad9fb2b79d2bbe0d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODE5MTQzMw==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r398191433", "bodyText": "mark inputSchema as Nullable", "author": "yaojiefeng", "createdAt": "2020-03-25T21:49:13Z", "path": "core-plugins/src/main/java/io/cdap/plugin/batch/aggregator/DistinctAggregator.java", "diffHunk": "@@ -88,7 +92,7 @@ public void configurePipeline(PipelineConfigurer pipelineConfigurer) {\n   }\n \n   public void validate(Schema inputSchema, Iterable<String> fields, FailureCollector collector) {", "originalCommit": "4b573c34086000613b59e99ad9fb2b79d2bbe0d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODE5MjQzOQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r398192439", "bodyText": "this is not what I mean, if one field is macro enabled, the other logic should still be validated. You should incorporate the this logic in the validation", "author": "yaojiefeng", "createdAt": "2020-03-25T21:51:07Z", "path": "core-plugins/src/main/java/io/cdap/plugin/batch/aggregator/RowDenormalizerAggregator.java", "diffHunk": "@@ -144,9 +147,19 @@ private Schema initializeOutputSchema() {\n    * @param inputSchema Validates whether the keyfield, fieldname, and fieldvalue entered by the user is of type\n    *                    String or Nullable String and present in the input schema or not.\n    * @param collector FailureCollector used to log all errors and return them to the user.\n-   * @return true\n+   * @returns true if input fields have been validated.\n    */\n-  private void validateInputFields(Schema inputSchema, FailureCollector collector) {\n+  private boolean validateInputFields(Schema inputSchema, FailureCollector collector) {\n+\n+    // Bail out if input schema or field values are unknown.\n+    if (inputSchema == null ||", "originalCommit": "4b573c34086000613b59e99ad9fb2b79d2bbe0d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODE5MjU5MA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r398192590", "bodyText": "should this return true, since it passes all validation?", "author": "yaojiefeng", "createdAt": "2020-03-25T21:51:28Z", "path": "core-plugins/src/main/java/io/cdap/plugin/batch/aggregator/RowDenormalizerAggregator.java", "diffHunk": "@@ -194,6 +207,6 @@ private void validateInputFields(Schema inputSchema, FailureCollector collector)\n           .withConfigProperty(VALUE_FIELD);\n       }\n     }\n-\n+    return false;", "originalCommit": "4b573c34086000613b59e99ad9fb2b79d2bbe0d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIxNDEzNQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r398214135", "bodyText": "nit - this should apply with String.format", "author": "yaojiefeng", "createdAt": "2020-03-25T22:39:55Z", "path": "core-plugins/src/main/java/io/cdap/plugin/batch/aggregator/RowDenormalizerAggregator.java", "diffHunk": "@@ -161,52 +163,73 @@ private boolean validateInputFields(Schema inputSchema, FailureCollector collect\n       return false;\n     }\n \n-    if (inputSchema.getField(conf.getKeyField()) == null) {\n-      collector.addFailure(String.format(\"Key field '%s' does not exist in input schema\", conf.getKeyField()), null)\n-        .withConfigProperty(KEY_FIELD);\n-    } else {\n-      Schema keyFieldSchema = inputSchema.getField(conf.getKeyField()).getSchema();\n-\n-      final Schema.Type schemaType = keyFieldSchema.isNullable() ?\n-                                    keyFieldSchema.getNonNullable().getType() :\n-                                    keyFieldSchema.getType();\n+    if (inputSchema == null) {\n+      return false;\n+    }\n \n-      if (!schemaType.equals(Schema.Type.STRING)) {\n-        collector.addFailure(String.format(\"Key field '%s' in the input record must be a String\", conf.getKeyField()),\n-                             null)\n+    if (conf.containsMacro(RowDenormalizerConfig.KEY_FIELD)) {\n+      canSetOutputSchema = false;\n+    } else {\n+      if (inputSchema.getField(conf.getKeyField()) == null) {\n+        collector.addFailure(String.format(\"Key field '%s' does not exist in input schema\", conf.getKeyField()), null)\n           .withConfigProperty(KEY_FIELD);\n+      } else {\n+        Schema keyFieldSchema = inputSchema.getField(conf.getKeyField()).getSchema();\n+\n+        final Schema.Type schemaType = keyFieldSchema.isNullable() ?\n+          keyFieldSchema.getNonNullable().getType() :\n+          keyFieldSchema.getType();\n+\n+        if (!schemaType.equals(Schema.Type.STRING)) {\n+          collector.addFailure(String.format(\"Key field '%s' in the input record must be a String\", conf.getKeyField()),\n+            null)\n+            .withConfigProperty(KEY_FIELD);\n+        }\n       }\n     }\n \n-    if (inputSchema.getField(conf.getNameField()) == null) {\n-      collector.addFailure(String.format(\"Name field '%s' does not exist in input schema\", conf.getNameField()),\n-                           null)\n-        .withConfigProperty(NAME_FIELD);\n+    if (conf.containsMacro(RowDenormalizerConfig.NAME_FIELD)) {\n+      canSetOutputSchema = false;\n     } else {\n-      Schema nameFieldSchema = inputSchema.getField(conf.getNameField()).getSchema();\n-\n-      if (!((nameFieldSchema.isNullable() ? nameFieldSchema.getNonNullable().getType() : nameFieldSchema\n-              .getType()).equals(Schema.Type.STRING))) {\n-        collector.addFailure(String.format(\"Name field '%s' in the input record must be a String\", conf.getNameField()),\n-                             null)\n+      if (inputSchema.getField(conf.getNameField()) == null) {\n+        collector.addFailure(String.format(\"Name field '%s' does not exist in input schema\", conf.getNameField()),\n+          null)", "originalCommit": "0f0cc005437ec9f9b6f82dc22be5ab8359e563d8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIxNTAwMA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r398215000", "bodyText": "nit - don't indent like this, you can separate the line after :", "author": "yaojiefeng", "createdAt": "2020-03-25T22:42:05Z", "path": "core-plugins/src/main/java/io/cdap/plugin/batch/aggregator/RowDenormalizerAggregator.java", "diffHunk": "@@ -161,52 +163,73 @@ private boolean validateInputFields(Schema inputSchema, FailureCollector collect\n       return false;\n     }\n \n-    if (inputSchema.getField(conf.getKeyField()) == null) {\n-      collector.addFailure(String.format(\"Key field '%s' does not exist in input schema\", conf.getKeyField()), null)\n-        .withConfigProperty(KEY_FIELD);\n-    } else {\n-      Schema keyFieldSchema = inputSchema.getField(conf.getKeyField()).getSchema();\n-\n-      final Schema.Type schemaType = keyFieldSchema.isNullable() ?\n-                                    keyFieldSchema.getNonNullable().getType() :\n-                                    keyFieldSchema.getType();\n+    if (inputSchema == null) {\n+      return false;\n+    }\n \n-      if (!schemaType.equals(Schema.Type.STRING)) {\n-        collector.addFailure(String.format(\"Key field '%s' in the input record must be a String\", conf.getKeyField()),\n-                             null)\n+    if (conf.containsMacro(RowDenormalizerConfig.KEY_FIELD)) {\n+      canSetOutputSchema = false;\n+    } else {\n+      if (inputSchema.getField(conf.getKeyField()) == null) {\n+        collector.addFailure(String.format(\"Key field '%s' does not exist in input schema\", conf.getKeyField()), null)\n           .withConfigProperty(KEY_FIELD);\n+      } else {\n+        Schema keyFieldSchema = inputSchema.getField(conf.getKeyField()).getSchema();\n+\n+        final Schema.Type schemaType = keyFieldSchema.isNullable() ?\n+          keyFieldSchema.getNonNullable().getType() :\n+          keyFieldSchema.getType();\n+\n+        if (!schemaType.equals(Schema.Type.STRING)) {\n+          collector.addFailure(String.format(\"Key field '%s' in the input record must be a String\", conf.getKeyField()),\n+            null)\n+            .withConfigProperty(KEY_FIELD);\n+        }\n       }\n     }\n \n-    if (inputSchema.getField(conf.getNameField()) == null) {\n-      collector.addFailure(String.format(\"Name field '%s' does not exist in input schema\", conf.getNameField()),\n-                           null)\n-        .withConfigProperty(NAME_FIELD);\n+    if (conf.containsMacro(RowDenormalizerConfig.NAME_FIELD)) {\n+      canSetOutputSchema = false;\n     } else {\n-      Schema nameFieldSchema = inputSchema.getField(conf.getNameField()).getSchema();\n-\n-      if (!((nameFieldSchema.isNullable() ? nameFieldSchema.getNonNullable().getType() : nameFieldSchema\n-              .getType()).equals(Schema.Type.STRING))) {\n-        collector.addFailure(String.format(\"Name field '%s' in the input record must be a String\", conf.getNameField()),\n-                             null)\n+      if (inputSchema.getField(conf.getNameField()) == null) {\n+        collector.addFailure(String.format(\"Name field '%s' does not exist in input schema\", conf.getNameField()),\n+          null)\n           .withConfigProperty(NAME_FIELD);\n+      } else {\n+        Schema nameFieldSchema = inputSchema.getField(conf.getNameField()).getSchema();\n+\n+        if (!((nameFieldSchema.isNullable() ? nameFieldSchema.getNonNullable().getType() : nameFieldSchema\n+          .getType()).equals(Schema.Type.STRING))) {", "originalCommit": "0f0cc005437ec9f9b6f82dc22be5ab8359e563d8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIxNTEzNA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1043#discussion_r398215134", "bodyText": "nit - should apply with String.format", "author": "yaojiefeng", "createdAt": "2020-03-25T22:42:29Z", "path": "core-plugins/src/main/java/io/cdap/plugin/batch/aggregator/RowDenormalizerAggregator.java", "diffHunk": "@@ -161,52 +163,73 @@ private boolean validateInputFields(Schema inputSchema, FailureCollector collect\n       return false;\n     }\n \n-    if (inputSchema.getField(conf.getKeyField()) == null) {\n-      collector.addFailure(String.format(\"Key field '%s' does not exist in input schema\", conf.getKeyField()), null)\n-        .withConfigProperty(KEY_FIELD);\n-    } else {\n-      Schema keyFieldSchema = inputSchema.getField(conf.getKeyField()).getSchema();\n-\n-      final Schema.Type schemaType = keyFieldSchema.isNullable() ?\n-                                    keyFieldSchema.getNonNullable().getType() :\n-                                    keyFieldSchema.getType();\n+    if (inputSchema == null) {\n+      return false;\n+    }\n \n-      if (!schemaType.equals(Schema.Type.STRING)) {\n-        collector.addFailure(String.format(\"Key field '%s' in the input record must be a String\", conf.getKeyField()),\n-                             null)\n+    if (conf.containsMacro(RowDenormalizerConfig.KEY_FIELD)) {\n+      canSetOutputSchema = false;\n+    } else {\n+      if (inputSchema.getField(conf.getKeyField()) == null) {\n+        collector.addFailure(String.format(\"Key field '%s' does not exist in input schema\", conf.getKeyField()), null)\n           .withConfigProperty(KEY_FIELD);\n+      } else {\n+        Schema keyFieldSchema = inputSchema.getField(conf.getKeyField()).getSchema();\n+\n+        final Schema.Type schemaType = keyFieldSchema.isNullable() ?\n+          keyFieldSchema.getNonNullable().getType() :\n+          keyFieldSchema.getType();\n+\n+        if (!schemaType.equals(Schema.Type.STRING)) {\n+          collector.addFailure(String.format(\"Key field '%s' in the input record must be a String\", conf.getKeyField()),\n+            null)\n+            .withConfigProperty(KEY_FIELD);\n+        }\n       }\n     }\n \n-    if (inputSchema.getField(conf.getNameField()) == null) {\n-      collector.addFailure(String.format(\"Name field '%s' does not exist in input schema\", conf.getNameField()),\n-                           null)\n-        .withConfigProperty(NAME_FIELD);\n+    if (conf.containsMacro(RowDenormalizerConfig.NAME_FIELD)) {\n+      canSetOutputSchema = false;\n     } else {\n-      Schema nameFieldSchema = inputSchema.getField(conf.getNameField()).getSchema();\n-\n-      if (!((nameFieldSchema.isNullable() ? nameFieldSchema.getNonNullable().getType() : nameFieldSchema\n-              .getType()).equals(Schema.Type.STRING))) {\n-        collector.addFailure(String.format(\"Name field '%s' in the input record must be a String\", conf.getNameField()),\n-                             null)\n+      if (inputSchema.getField(conf.getNameField()) == null) {\n+        collector.addFailure(String.format(\"Name field '%s' does not exist in input schema\", conf.getNameField()),\n+          null)\n           .withConfigProperty(NAME_FIELD);\n+      } else {\n+        Schema nameFieldSchema = inputSchema.getField(conf.getNameField()).getSchema();\n+\n+        if (!((nameFieldSchema.isNullable() ? nameFieldSchema.getNonNullable().getType() : nameFieldSchema\n+          .getType()).equals(Schema.Type.STRING))) {\n+          collector.addFailure(String.format(\"Name field '%s' in the input record must be a String\", conf.getNameField()),\n+            null)", "originalCommit": "0f0cc005437ec9f9b6f82dc22be5ab8359e563d8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "a739a88097971ae82b029ef9723ed00e70d9bd4b", "url": "https://github.com/cdapio/hydrator-plugins/commit/a739a88097971ae82b029ef9723ed00e70d9bd4b", "message": "[CDAP-16324] Improve macro support for analytics plugins.\n\n- Added macro support for row denormalizer.\n- numPartitions is macro enabled.\n- Handle cases where output schema is not known.", "committedDate": "2020-03-25T23:12:24Z", "type": "commit"}, {"oid": "a739a88097971ae82b029ef9723ed00e70d9bd4b", "url": "https://github.com/cdapio/hydrator-plugins/commit/a739a88097971ae82b029ef9723ed00e70d9bd4b", "message": "[CDAP-16324] Improve macro support for analytics plugins.\n\n- Added macro support for row denormalizer.\n- numPartitions is macro enabled.\n- Handle cases where output schema is not known.", "committedDate": "2020-03-25T23:12:24Z", "type": "forcePushed"}]}