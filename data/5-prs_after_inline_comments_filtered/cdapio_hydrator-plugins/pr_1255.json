{"pr_number": 1255, "pr_title": "[CDAP-17248] Added support for decoding files encoded in fixed-length charsets when reading from a file source.", "pr_createdAt": "2020-12-03T23:44:57Z", "pr_url": "https://github.com/cdapio/hydrator-plugins/pull/1255", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTcyOTExNQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r535729115", "bodyText": "Ignore these log statements for now. I've been using them for debugging.", "author": "fernst", "createdAt": "2020-12-03T23:45:34Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingCodec.java", "diffHunk": "@@ -0,0 +1,167 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.io.compress.CompressionInputStream;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.io.compress.Compressor;\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.apache.hadoop.io.compress.DefaultCodec;\n+import org.apache.hadoop.io.compress.DirectDecompressor;\n+import org.apache.hadoop.io.compress.SplitCompressionInputStream;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+\n+/**\n+ * Codec implementation that returns a decompressor for Fixed Length character encodings.\n+ */\n+public class FixedLengthCharsetTransformingCodec extends DefaultCodec\n+  implements Configurable, SplittableCompressionCodec {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingCodec.class);\n+\n+  FixedLengthCharset fixedLengthCharset;\n+\n+  public FixedLengthCharsetTransformingCodec(FixedLengthCharset fixedLengthCharset) {\n+    this.fixedLengthCharset = fixedLengthCharset;\n+  }\n+\n+  @Override\n+  public CompressionOutputStream createOutputStream(OutputStream out) throws IOException {\n+    throw new RuntimeException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public CompressionOutputStream createOutputStream(OutputStream out, Compressor compressor) throws IOException {\n+    throw new RuntimeException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public Class<? extends Compressor> getCompressorType() {\n+    throw new RuntimeException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public Compressor createCompressor() {\n+    throw new RuntimeException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public CompressionInputStream createInputStream(InputStream in) throws IOException {\n+    return super.createInputStream(in, new FixedLengthCharsetTransformingDecompressor(this.fixedLengthCharset));\n+  }\n+\n+  @Override\n+  public CompressionInputStream createInputStream(InputStream in, Decompressor decompressor) throws IOException {\n+    return super.createInputStream(in, decompressor);\n+  }\n+\n+  @Override\n+  public Class<? extends Decompressor> getDecompressorType() {\n+    return FixedLengthCharsetTransformingDecompressor.class;\n+  }\n+\n+  @Override\n+  public DirectDecompressor createDirectDecompressor() {\n+    throw new RuntimeException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public String getDefaultExtension() {\n+    return \".*\";\n+  }\n+\n+  @Override\n+  public Decompressor createDecompressor() {\n+    return new FixedLengthCharsetTransformingDecompressor(fixedLengthCharset);\n+  }\n+\n+  @Override\n+  public SplitCompressionInputStream createInputStream(InputStream seekableIn,\n+                                                       Decompressor decompressor,\n+                                                       long start,\n+                                                       long end,\n+                                                       READ_MODE readMode) throws IOException {\n+    if (!(seekableIn instanceof Seekable)) {\n+      throw new IOException(\"seekableIn must be an instance of \" +\n+                              Seekable.class.getName());\n+    }\n+\n+    //Adjust start to align to the next character boundary.\n+    if (start % fixedLengthCharset.getCharLength() != 0) {\n+      LOG.info(\"Adjusted Start from {} to {} by {} bytes\",", "originalCommit": "e11babb0b1c4c9cff529037debb1fae3169d6c3b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTg1Nzk0Mw==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r535857943", "bodyText": "Can you add comments about which lines are different than the copy", "author": "albertshau", "createdAt": "2020-12-04T06:05:08Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/CharsetTransformingLineRecordReader.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset;\n+\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharset;\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharsetTransformingCodec;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.apache.hadoop.io.compress.SplitCompressionInputStream;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.hadoop.mapreduce.lib.input.LineRecordReader;\n+import org.apache.hadoop.mapreduce.lib.input.SplitLineReader;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Copy of Hadoop's Line Record Reader (Hadoop Version 2.3.0)", "originalCommit": "e11babb0b1c4c9cff529037debb1fae3169d6c3b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjE3NjU1NA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r536176554", "bodyText": "I will.", "author": "fernst", "createdAt": "2020-12-04T15:22:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTg1Nzk0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTg1OTQ5Mg==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r535859492", "bodyText": "nit: try to avoid throwing Exception or RuntimeException. This method is similar to Enum's valueOf() method, which will throw IllegalArgumentException in a similar situation.", "author": "albertshau", "createdAt": "2020-12-04T06:09:31Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharset.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Enumeration containing all supported Fixed Length charsets.\n+ */\n+public enum FixedLengthCharset {\n+  UTF_32(\"UTF-32\", Charset.forName(\"UTF-32\"), 4),\n+  ISO_8859_1(\"ISO-8859-1\", StandardCharsets.ISO_8859_1, 1),\n+  WINDOWS_1252(\"Windows-1252\", Charset.forName(\"windows-1252\"), 1);\n+\n+  private final String name;\n+  private final Charset charset;\n+  private final int charLength;\n+\n+  FixedLengthCharset(String name, Charset charset, int charLength) {\n+    this.name = name;\n+    this.charset = charset;\n+    this.charLength = charLength;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Charset getCharset() {\n+    return charset;\n+  }\n+\n+  public int getCharLength() {\n+    return charLength;\n+  }\n+\n+  /**\n+   * Find a FixedLengthCharset for a given encoding name. Throws a runtime exception if not found.\n+   * @param name Charset name\n+   * @return FixedLengthCharset for the desired charset.\n+   */\n+  public static FixedLengthCharset forName(String name) {\n+    for (FixedLengthCharset c : FixedLengthCharset.values()) {\n+      if (name.equalsIgnoreCase(c.getName())) {\n+        return c;\n+      }\n+    }\n+\n+    throw new RuntimeException(\"Cannot find FixedLengthCharset with name: \" + name);", "originalCommit": "e11babb0b1c4c9cff529037debb1fae3169d6c3b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTg2MDEyOA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r535860128", "bodyText": "privatre final", "author": "albertshau", "createdAt": "2020-12-04T06:11:21Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingCodec.java", "diffHunk": "@@ -0,0 +1,167 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.io.compress.CompressionInputStream;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.io.compress.Compressor;\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.apache.hadoop.io.compress.DefaultCodec;\n+import org.apache.hadoop.io.compress.DirectDecompressor;\n+import org.apache.hadoop.io.compress.SplitCompressionInputStream;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+\n+/**\n+ * Codec implementation that returns a decompressor for Fixed Length character encodings.\n+ */\n+public class FixedLengthCharsetTransformingCodec extends DefaultCodec\n+  implements Configurable, SplittableCompressionCodec {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingCodec.class);\n+\n+  FixedLengthCharset fixedLengthCharset;", "originalCommit": "e11babb0b1c4c9cff529037debb1fae3169d6c3b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTg2MDI5Mg==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r535860292", "bodyText": "UnsupportedOperationException (same with others)", "author": "albertshau", "createdAt": "2020-12-04T06:11:45Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingCodec.java", "diffHunk": "@@ -0,0 +1,167 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.io.compress.CompressionInputStream;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.io.compress.Compressor;\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.apache.hadoop.io.compress.DefaultCodec;\n+import org.apache.hadoop.io.compress.DirectDecompressor;\n+import org.apache.hadoop.io.compress.SplitCompressionInputStream;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+\n+/**\n+ * Codec implementation that returns a decompressor for Fixed Length character encodings.\n+ */\n+public class FixedLengthCharsetTransformingCodec extends DefaultCodec\n+  implements Configurable, SplittableCompressionCodec {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingCodec.class);\n+\n+  FixedLengthCharset fixedLengthCharset;\n+\n+  public FixedLengthCharsetTransformingCodec(FixedLengthCharset fixedLengthCharset) {\n+    this.fixedLengthCharset = fixedLengthCharset;\n+  }\n+\n+  @Override\n+  public CompressionOutputStream createOutputStream(OutputStream out) throws IOException {\n+    throw new RuntimeException(\"Not supported\");", "originalCommit": "e11babb0b1c4c9cff529037debb1fae3169d6c3b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTg2MTExOQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r535861119", "bodyText": "private final", "author": "albertshau", "createdAt": "2020-12-04T06:14:16Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressor.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.io.compress.Decompressor;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+\n+/**\n+ * Decompressor that can be used to convert byte streams in fixed-length character encodings to a stream of UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressor implements Decompressor {\n+\n+  final FixedLengthCharset origin;", "originalCommit": "e11babb0b1c4c9cff529037debb1fae3169d6c3b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTg2MzIwNw==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r535863207", "bodyText": "unused?", "author": "albertshau", "createdAt": "2020-12-04T06:20:15Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressorStream.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.io.compress.DecompressorStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * DecompressorStream implementation for the FixedLengthCharsetTransformingDecompressor.\n+ * <p>\n+ * This can be used to convert input streams containing bytes for fixed length charsets into UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressorStream extends DecompressorStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingDecompressorStream.class);\n+\n+  //Starting and ending position in the file.\n+  long start;\n+  long end;\n+\n+  protected FixedLengthCharsetTransformingDecompressorStream(InputStream in,\n+                                                             FixedLengthCharset fixedLengthCharset,\n+                                                             long start,\n+                                                             long end)\n+    throws IOException {\n+    super(in, new FixedLengthCharsetTransformingDecompressor(fixedLengthCharset));\n+    long skippedBytes = in.skip(start);", "originalCommit": "e11babb0b1c4c9cff529037debb1fae3169d6c3b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzYzNzU0OA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r537637548", "bodyText": "Indeed, we're not really using this value. Maybe I should throw an exception if it's not possible to follow this instruction.", "author": "fernst", "createdAt": "2020-12-07T16:20:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTg2MzIwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzY4MzM5Ng==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r537683396", "bodyText": "I believe getting less than asked means we've got EOF earlier.", "author": "tivv", "createdAt": "2020-12-07T17:19:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTg2MzIwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTg2Mzk0NA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r535863944", "bodyText": "seems like this would never happen?", "author": "albertshau", "createdAt": "2020-12-04T06:22:04Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressorStream.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.io.compress.DecompressorStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * DecompressorStream implementation for the FixedLengthCharsetTransformingDecompressor.\n+ * <p>\n+ * This can be used to convert input streams containing bytes for fixed length charsets into UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressorStream extends DecompressorStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingDecompressorStream.class);\n+\n+  //Starting and ending position in the file.\n+  long start;\n+  long end;\n+\n+  protected FixedLengthCharsetTransformingDecompressorStream(InputStream in,\n+                                                             FixedLengthCharset fixedLengthCharset,\n+                                                             long start,\n+                                                             long end)\n+    throws IOException {\n+    super(in, new FixedLengthCharsetTransformingDecompressor(fixedLengthCharset));\n+    long skippedBytes = in.skip(start);\n+    this.start = start;\n+    this.end = end;\n+  }\n+\n+  @Override\n+  protected int decompress(byte[] b, int off, int len) throws IOException {\n+    //Set input for decompression if it's needed for execution.\n+    if (this.decompressor.needsInput()) {\n+      int l = getCompressedData();\n+      if (l > 0) {\n+        this.decompressor.setInput(buffer, 0, l);\n+      }\n+    }\n+\n+    //Proceed with super method.\n+    return super.decompress(b, off, len);\n+  }\n+\n+  @Override\n+  public long getPos() throws IOException {\n+    //If we're working with a Charset Transforming decompressor, we can calculate the current position on the input file\n+    // By adding the starting position in the file with the number of bytes we have read so far.\n+    if (this.decompressor instanceof FixedLengthCharsetTransformingDecompressor) {\n+      FixedLengthCharsetTransformingDecompressor flcDecompressor =\n+        (FixedLengthCharsetTransformingDecompressor) this.decompressor;\n+\n+      //Actual position is starting possition + the number of bytes we have consumed - the remaining bytes\n+      return start + flcDecompressor.getNumConsumedBytes();\n+    } else {\n+      return super.getPos();", "originalCommit": "e11babb0b1c4c9cff529037debb1fae3169d6c3b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzYzNjQ4NA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r537636484", "bodyText": "Good point. Fixed it.", "author": "fernst", "createdAt": "2020-12-07T16:18:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTg2Mzk0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjM5MDUwMQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r536390501", "bodyText": "I don't think you need this override", "author": "tivv", "createdAt": "2020-12-04T21:29:37Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingCodec.java", "diffHunk": "@@ -0,0 +1,167 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.io.compress.CompressionInputStream;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.io.compress.Compressor;\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.apache.hadoop.io.compress.DefaultCodec;\n+import org.apache.hadoop.io.compress.DirectDecompressor;\n+import org.apache.hadoop.io.compress.SplitCompressionInputStream;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+\n+/**\n+ * Codec implementation that returns a decompressor for Fixed Length character encodings.\n+ */\n+public class FixedLengthCharsetTransformingCodec extends DefaultCodec\n+  implements Configurable, SplittableCompressionCodec {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingCodec.class);\n+\n+  FixedLengthCharset fixedLengthCharset;\n+\n+  public FixedLengthCharsetTransformingCodec(FixedLengthCharset fixedLengthCharset) {\n+    this.fixedLengthCharset = fixedLengthCharset;\n+  }\n+\n+  @Override\n+  public CompressionOutputStream createOutputStream(OutputStream out) throws IOException {\n+    throw new RuntimeException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public CompressionOutputStream createOutputStream(OutputStream out, Compressor compressor) throws IOException {\n+    throw new RuntimeException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public Class<? extends Compressor> getCompressorType() {\n+    throw new RuntimeException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public Compressor createCompressor() {\n+    throw new RuntimeException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public CompressionInputStream createInputStream(InputStream in) throws IOException {\n+    return super.createInputStream(in, new FixedLengthCharsetTransformingDecompressor(this.fixedLengthCharset));\n+  }\n+\n+  @Override\n+  public CompressionInputStream createInputStream(InputStream in, Decompressor decompressor) throws IOException {", "originalCommit": "e11babb0b1c4c9cff529037debb1fae3169d6c3b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjM5NjkzMA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r536396930", "bodyText": "You are doing a lot of copying here. Please consider using java.nio.charset.Charset along with ByteBuffer/CharBuffer.", "author": "tivv", "createdAt": "2020-12-04T21:43:51Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressor.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.io.compress.Decompressor;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+\n+/**\n+ * Decompressor that can be used to convert byte streams in fixed-length character encodings to a stream of UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressor implements Decompressor {\n+\n+  final FixedLengthCharset origin;\n+  final Charset destination = StandardCharsets.UTF_8;\n+  long numConsumedBytes = 0;\n+\n+  ByteArrayOutputStream incomingBuffer = new ByteArrayOutputStream();\n+  ByteArrayInputStream outgoingBuffer = new ByteArrayInputStream(new byte[]{});\n+\n+  public FixedLengthCharsetTransformingDecompressor(FixedLengthCharset origin) {\n+    this.origin = origin;\n+  }\n+\n+  @Override\n+  public void setInput(byte[] b, int off, int len) {", "originalCommit": "e11babb0b1c4c9cff529037debb1fae3169d6c3b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQyOTcwNQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r536429705", "bodyText": "Hmm, good point. I can probably get away with using a byte buffer for input and transform as needed when decompression is needed.", "author": "fernst", "createdAt": "2020-12-04T22:49:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjM5NjkzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQwMTc3NQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r536401775", "bodyText": "Can we have EBCDIC here. Also I am not sure if it would make sense to have some kind of override to be able to use it for arbitrary charset.", "author": "tivv", "createdAt": "2020-12-04T21:51:30Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharset.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Enumeration containing all currently supported Fixed Length charsets.\n+ *\n+ * This currently includes:\n+ * - UTF-32,\n+ * - ISO-8859 variants supported by Java\n+ * - Windows single-byte code pages supported by Java.\n+ */\n+public enum FixedLengthCharset {\n+  UTF_32(\"UTF-32\", Charset.forName(\"UTF-32\"), 4),\n+  ISO_8859_1(\"ISO-8859-1\", StandardCharsets.ISO_8859_1, 1),\n+  ISO_8859_2(\"ISO-8859-2\", Charset.forName(\"ISO-8859-2\"), 1),\n+  ISO_8859_3(\"ISO-8859-3\", Charset.forName(\"ISO-8859-3\"), 1),\n+  ISO_8859_4(\"ISO-8859-4\", Charset.forName(\"ISO-8859-4\"), 1),\n+  ISO_8859_5(\"ISO-8859-5\", Charset.forName(\"ISO-8859-5\"), 1),\n+  ISO_8859_6(\"ISO-8859-6\", Charset.forName(\"ISO-8859-6\"), 1),\n+  ISO_8859_7(\"ISO-8859-7\", Charset.forName(\"ISO-8859-7\"), 1),\n+  ISO_8859_8(\"ISO-8859-8\", Charset.forName(\"ISO-8859-8\"), 1),\n+  ISO_8859_9(\"ISO-8859-9\", Charset.forName(\"ISO-8859-9\"), 1),\n+  ISO_8859_11(\"ISO-8859-11\", Charset.forName(\"ISO-8859-11\"), 1),\n+  ISO_8859_13(\"ISO-8859-13\", Charset.forName(\"ISO-8859-13\"), 1),\n+  ISO_8859_15(\"ISO-8859-15\", Charset.forName(\"ISO-8859-15\"), 1),\n+  WINDOWS_1250(\"Windows-1250\", Charset.forName(\"windows-1250\"), 1),\n+  WINDOWS_1251(\"Windows-1251\", Charset.forName(\"windows-1251\"), 1),\n+  WINDOWS_1252(\"Windows-1252\", Charset.forName(\"windows-1252\"), 1),\n+  WINDOWS_1253(\"Windows-1253\", Charset.forName(\"windows-1253\"), 1),\n+  WINDOWS_1254(\"Windows-1254\", Charset.forName(\"windows-1254\"), 1),\n+  WINDOWS_1255(\"Windows-1255\", Charset.forName(\"windows-1255\"), 1),\n+  WINDOWS_1256(\"Windows-1256\", Charset.forName(\"windows-1256\"), 1),\n+  WINDOWS_1257(\"Windows-1257\", Charset.forName(\"windows-1257\"), 1),\n+  WINDOWS_1258(\"Windows-1258\", Charset.forName(\"windows-1258\"), 1);", "originalCommit": "83427f7002b016044f5f4606d998b92511c92ceb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQwNDUxMA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r536404510", "bodyText": "You can also try to use java.nio.charset.CharsetDecoder maxCharsPerByte/averageCharsPerByte and skip this table altogether", "author": "tivv", "createdAt": "2020-12-04T21:54:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQwMTc3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTYyMTMxOA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r539621318", "bodyText": "So, can we automatically allow any single-byte encoding here?", "author": "tivv", "createdAt": "2020-12-09T20:25:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQwMTc3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ2Mzk0Mw==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r536463943", "bodyText": "What is modified and why is needed?", "author": "chtyim", "createdAt": "2020-12-05T00:44:32Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/CharsetTransformingLineRecordReader.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset;\n+\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharset;\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharsetTransformingCodec;\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharsetTransformingDecompressorStream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.compress.CodecPool;\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.apache.hadoop.io.compress.SplitCompressionInputStream;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.hadoop.mapreduce.lib.input.LineRecordReader;\n+import org.apache.hadoop.mapreduce.lib.input.SplitLineReader;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Copy of Hadoop's LineRecordReader (Hadoop Version 2.3.0). The reason we copy this class is to modify some behaviors\n+ * related to the creation of the decompressor. This also allows us to implement", "originalCommit": "8aa2eead624ea2e0b5e6dc4fd61859f844554160", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzcxODgzOQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r537718839", "bodyText": "@chtyim The original implementation of Hadoop's LineRecordReader uses the file name to determine which codec to implement:\nhttps://github.com/facebookarchive/hadoop-20/blob/master/src/mapred/org/apache/hadoop/mapred/LineRecordReader.java#L92\nhttps://github.com/facebookarchive/hadoop-20/blob/master/src/core/org/apache/hadoop/io/compress/CompressionCodecFactory.java#L160-L175\nThis is not particularly useful in our use case as the file extension could be anything.\nIn order to go around this limitation, I copied this class and simplified the code as we can assume the input is \"compressed\" and we need to \"decompress\" (apply the charset conversion) as we read the file.", "author": "fernst", "createdAt": "2020-12-07T18:09:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ2Mzk0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgxMzAzMg==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r537813032", "bodyText": "It is controlled by config\nhttps://github.com/apache/hadoop/blob/branch-2.9.2/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java#L124", "author": "chtyim", "createdAt": "2020-12-07T20:33:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ2Mzk0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODk1NTExOQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r538955119", "bodyText": "Can we do it without copying this class?", "author": "chtyim", "createdAt": "2020-12-09T02:23:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ2Mzk0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU2Mjc4MQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r539562781", "bodyText": "I'll work on extending the LineRecordReader class and override this functionality instead of copying the whole class.", "author": "fernst", "createdAt": "2020-12-09T18:54:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ2Mzk0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU2NzcwMA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r539567700", "bodyText": "@chtyim I don't think the codec-by-configuration approach will work for us because the default implementation of the LineRecordReader class always invokes this function:\nhttps://github.com/apache/hadoop/blob/branch-2.9.2/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java#L197-L213\nThis means that the codec is matched based on file extension, which is not a desirable behavior in my case.", "author": "fernst", "createdAt": "2020-12-09T19:01:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ2Mzk0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTYxNzM3Ng==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r539617376", "bodyText": "Well, I believe if we have a single codec specified in io.compression.codecs configuration and it returns an empty   string as a suffix it would be picked up, is not it?", "author": "tivv", "createdAt": "2020-12-09T20:18:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ2Mzk0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzY5NDM1Nw==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r543694357", "bodyText": "@tivv You might be right. Let me give it a try. I had not considered that headMap using an empty string as key will just match every record in the map.", "author": "fernst", "createdAt": "2020-12-15T21:20:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ2Mzk0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQzNzYwMQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r544437601", "bodyText": "@tivv @chtyim After going through this attempt, I discovered a potential issue with using the default configuration:\nIf you look at the setCodecClasses method, the CompressionCodecFactory will also load CompressionCodecs in the classpath using the ServiceLoader: https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/io/compress/CompressionCodecFactory.html#setCodecClasses(org.apache.hadoop.conf.Configuration,%20java.util.List).\nThis means the headMap method call will always return multiple potential candidates. The default implementation then, picks the last result in this new map:\n\nI don't think we can get around this limitation (unless we override every single possible file extension with our codec) but this does not seem like a good idea.", "author": "fernst", "createdAt": "2020-12-16T16:24:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ2Mzk0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQ3MzQ3Ng==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r544473476", "bodyText": "Yep, it would be very hacky to work around ServiceLoader. We would need to change the context classloader. Not worth it in my opinion.", "author": "tivv", "createdAt": "2020-12-16T17:11:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ2Mzk0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg0NDQ4OA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r537844488", "bodyText": "@chtyim @albertshau @tivv I want to bring your attention to this line function:\nThe problem I'm facing at this time is that, in order to ensure that we respect the partition boundaries, and process each record exactly once, I need to make sure that I only read the last line that begins in a partition. This means that if a line is \"split\" by a partition, the worker that was reading the previous partition is the one that reads this line.\nThe way Hadoop does this for normal text files is to check against the file position in order to make sure the last line has not been read. However, this poses a problem for me, because the compressor reads part of the file in chunks, and the file position does not necessarily align with the position of the last line that was read and decoded.\nThe block of code I'm describing can be found here: https://github.com/cdapio/hydrator-plugins/blob/834s7f7002b016044f5f4606d998b92511c92ceb/format-common/src/main/java/io/cdap/plugin/format/charset/CharsetTransformingLineRecordReader.java#L165-L171\nThis poses a challenge for me. I see 2 ways to fix this:\n\n\nAs you can see in my implementation above, once I approach the partition boundary, I make sure to only read one character at a time, until the function that reads a line is able to complete successfully, and break out of the loop. This means that, as I approach the partition boundary, the execution speed slows down.\n\n\nThe other solution I saw is to calculate the file position based on reading character-by-character from the input in the decompressor. The problem I see with that is that, for every character I read, I would need to create a String instance, convert to bytes, and fill the output buffer in this way. This would give me the ability to accurately determine my position in the underlying file, but doing the Bytes -> 1 Character long String -> Bytes in UTF-8 charset conversion for every character in my input file is definitely not a performant solution, and this will impact performance everywhere in my partition, not just the end of the partition.\n\n\nLet me know if you can think of a better way to approach this partition boundary issue. The hybrid approach would be to add this \"character by character\" logic on the decompressor, but this does not strike me as the place where this logic should live.", "author": "fernst", "createdAt": "2020-12-07T21:26:19Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressorStream.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import io.cdap.plugin.format.charset.CharsetTransformingLineRecordReader;\n+import org.apache.hadoop.io.compress.DecompressorStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * DecompressorStream implementation for the FixedLengthCharsetTransformingDecompressor.\n+ * <p>\n+ * This can be used to convert input streams containing bytes for fixed length charsets into UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressorStream extends DecompressorStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingDecompressorStream.class);\n+\n+  //Starting and ending position in the file.\n+  protected final long start;\n+  protected final long end;\n+  protected final FixedLengthCharset fixedLengthCharset;\n+\n+  protected FixedLengthCharsetTransformingDecompressorStream(InputStream in,\n+                                                             FixedLengthCharset fixedLengthCharset,\n+                                                             long start,\n+                                                             long end)\n+    throws IOException {\n+    super(in, new FixedLengthCharsetTransformingDecompressor(fixedLengthCharset));\n+    long skippedBytes = in.skip(start);\n+    this.fixedLengthCharset = fixedLengthCharset;\n+    this.start = start;\n+    this.end = end;\n+  }\n+\n+  @Override\n+  protected int decompress(byte[] b, int off, int len) throws IOException {\n+    //Set input for decompression if it's needed for execution.\n+    if (this.decompressor.needsInput()) {\n+      int l = getCompressedData();\n+      if (l > 0) {\n+        this.decompressor.setInput(buffer, 0, l);\n+      }\n+    }\n+\n+    //Proceed with super method.\n+    return super.decompress(b, off, len);\n+  }\n+\n+  @Override\n+  public long getPos() throws IOException {\n+    // Since we're working with a Charset Transforming decompressor, we can calculate the current position on the\n+    // input file.\n+    // By adding the starting position in the file with the number of bytes we have read so far.\n+    FixedLengthCharsetTransformingDecompressor flcDecompressor =\n+      (FixedLengthCharsetTransformingDecompressor) this.decompressor;\n+\n+    //Actual position is starting possition + the number of bytes we have consumed.\n+    return start + flcDecompressor.getNumConsumedBytes();\n+  }\n+\n+  /**", "originalCommit": "9040a4823a7aac5b96f42cb3f759b543c2074309", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg1NDM1Nw==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r537854357", "bodyText": "Did you try switching to ByteBuffer/CharBuffer? I believe those should remove most of the overhead", "author": "tivv", "createdAt": "2020-12-07T21:43:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg0NDQ4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg1NzE5OA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r537857198", "bodyText": "Also, can't you just scan without decoding until you see a line delimiter byte sequence or EOF ? This way you would know exact length of the last line and decode it fast.", "author": "tivv", "createdAt": "2020-12-07T21:47:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg0NDQ4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg1ODQzMQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r537858431", "bodyText": "The latter may have memory problems if lines are very long, so using buffers instead of strings can be a better solution.", "author": "tivv", "createdAt": "2020-12-07T21:49:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg0NDQ4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg1OTAwMQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r537859001", "bodyText": "The way I see it is, you read into ByteBuffer, using a CharsetDecoder to decode the bytes into the source charset, and then encode it to UTF-8 ByteBuffer. Then you can feed the UTF-8 bytes to the regular Hadoop reader.", "author": "chtyim", "createdAt": "2020-12-07T21:50:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg0NDQ4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg3MDM3OA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r537870378", "bodyText": "Thanks @tivv. I was still tying up some things. I'll take a look at both of your suggestions and commit my changes \ud83d\udc40", "author": "fernst", "createdAt": "2020-12-07T22:09:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg0NDQ4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU2NTQ1NQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r539565455", "bodyText": "@tivv @chtyim I've reimplemented the logic using ByteBuffer and CharBuffer and it works significantly faster than before.", "author": "fernst", "createdAt": "2020-12-09T18:57:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg0NDQ4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTYxNDM1Ng==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r539614356", "bodyText": "How does UTF-8 become null here? I am missing it.", "author": "tivv", "createdAt": "2020-12-09T20:14:08Z", "path": "core-plugins/src/main/java/io/cdap/plugin/batch/source/FileBatchSource.java", "diffHunk": "@@ -48,6 +48,9 @@ public FileBatchSource(FileSourceConfig config) {\n     if (config.shouldCopyHeader()) {\n       properties.put(PathTrackingInputFormat.COPY_HEADER, \"true\");\n     }\n+    if (config.getFileEncoding() != null) {", "originalCommit": "758257c745606bbe61d48d819215a189a4c7c286", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDUxNjY5NA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r544516694", "bodyText": "Fixed it.", "author": "fernst", "createdAt": "2020-12-16T18:13:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTYxNDM1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTYzNTE0Mg==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r539635142", "bodyText": "Why do we need this? I see that super already handles similar case", "author": "tivv", "createdAt": "2020-12-09T20:46:18Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressorStream.java", "diffHunk": "@@ -0,0 +1,78 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import io.cdap.plugin.format.charset.CharsetTransformingLineRecordReader;\n+import org.apache.hadoop.io.compress.DecompressorStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * DecompressorStream implementation for the FixedLengthCharsetTransformingDecompressor.\n+ * <p>\n+ * This can be used to convert input streams containing bytes for fixed length charsets into UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressorStream extends DecompressorStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingDecompressorStream.class);\n+\n+  //Starting and ending position in the file.\n+  protected final long start;\n+  protected final long end;\n+  protected final FixedLengthCharset fixedLengthCharset;\n+\n+  protected FixedLengthCharsetTransformingDecompressorStream(InputStream in,\n+                                                             FixedLengthCharset fixedLengthCharset,\n+                                                             long start,\n+                                                             long end)\n+    throws IOException {\n+    super(in, new FixedLengthCharsetTransformingDecompressor(fixedLengthCharset));\n+    in.skip(start);\n+    this.fixedLengthCharset = fixedLengthCharset;\n+    this.start = start;\n+    this.end = end;\n+  }\n+\n+  @Override\n+  protected int decompress(byte[] b, int off, int len) throws IOException {\n+    //Set input for decompression if it's needed for execution.\n+    if (this.decompressor.needsInput()) {", "originalCommit": "758257c745606bbe61d48d819215a189a4c7c286", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDYzNDkzNg==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r544634936", "bodyText": "You are right. I'm removing this method and just let the call to the super method happen as usual.", "author": "fernst", "createdAt": "2020-12-16T21:27:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTYzNTE0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTYzOTI4NA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r539639284", "bodyText": "I believe record delimiting is done after we recoded everything to UTF-8, so  this value should be in UTF-8. Could you please add a test with record delimiting?", "author": "tivv", "createdAt": "2020-12-09T20:52:56Z", "path": "format-common/src/main/java/io/cdap/plugin/format/input/CharsetTransformingPathTrackingInputFormat.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.input;\n+\n+import io.cdap.plugin.format.charset.CharsetTransformingLineRecordReader;\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharset;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\n+\n+/**\n+ * An input format that tracks which the file path each record was read from. This InputFormat is a wrapper around\n+ * underlying input formats. The responsibility of this class is to keep track of which file each record is reading\n+ * from, and to add the file URI to each record. In addition, for text files, it can be configured to keep track\n+ * of the header for the file, which underlying record readers can use.\n+ */\n+public class CharsetTransformingPathTrackingInputFormat extends TextInputFormat {\n+\n+  protected final FixedLengthCharset fixedLengthCharset;\n+\n+  public CharsetTransformingPathTrackingInputFormat(String charsetName) {\n+    this.fixedLengthCharset = FixedLengthCharset.forName(charsetName);\n+  }\n+\n+  @Override\n+  public RecordReader<LongWritable, Text> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    String delimiter = context.getConfiguration().get(\"textinputformat.record.delimiter\");\n+    byte[] recordDelimiterBytes = null;\n+    if (null != delimiter) {\n+      recordDelimiterBytes = delimiter.getBytes(fixedLengthCharset.getCharset());", "originalCommit": "758257c745606bbe61d48d819215a189a4c7c286", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDU0NDA3Nw==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r544544077", "bodyText": "You are indeed correct. Let me fix this.", "author": "fernst", "createdAt": "2020-12-16T18:55:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTYzOTI4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTY1NzAyMQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r539657021", "bodyText": "Can we just do ByteBuffer.wrap(b, off, len) and save one more copy?", "author": "tivv", "createdAt": "2020-12-09T21:22:05Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressor.java", "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.CharBuffer;\n+import java.nio.charset.Charset;\n+import java.nio.charset.CharsetDecoder;\n+import java.nio.charset.CharsetEncoder;\n+import java.nio.charset.CoderResult;\n+import java.nio.charset.StandardCharsets;\n+\n+/**\n+ * Decompressor that can be used to convert byte streams in fixed-length character encodings to a stream of UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressor implements Decompressor {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingDecompressor.class);\n+\n+  protected final FixedLengthCharset sourceEncoding;\n+  protected final CharsetDecoder decoder;\n+  protected final CharsetEncoder encoder;\n+  protected final Charset targetCharset = StandardCharsets.UTF_8;\n+  protected long numDecodedCharacters = 0;\n+  protected long numEncodedCharacters = 0;\n+\n+  //Initializing all buffers.\n+  protected ByteBuffer inputByteBuffer = ByteBuffer.allocate(0);\n+  protected CharBuffer decodedCharBuffer = CharBuffer.allocate(0);\n+  protected ByteBuffer partialOutputByteBuffer = ByteBuffer.allocate(0);\n+\n+  public FixedLengthCharsetTransformingDecompressor(FixedLengthCharset sourceEncoding) {\n+    this.sourceEncoding = sourceEncoding;\n+    this.decoder = sourceEncoding.getCharset().newDecoder();\n+    this.encoder = targetCharset.newEncoder();\n+  }\n+\n+  @Override\n+  public void setInput(byte[] b, int off, int len) {\n+    //Expand incoming buffer if needed.\n+    if (inputByteBuffer.remaining() < len) {\n+      //Allocate new buffer that can fill the existing input + newly received bytes\n+      ByteBuffer newIncomingBuffer = ByteBuffer.allocate(len + inputByteBuffer.capacity());\n+\n+      //Set up incoming buffer for reads and copy contents into new buffer.\n+      inputByteBuffer.flip();\n+      newIncomingBuffer.put(inputByteBuffer);\n+\n+      inputByteBuffer = newIncomingBuffer;\n+    }\n+\n+    //Copy incoming payload into Input Byte Buffer\n+    inputByteBuffer.put(b, off, len);\n+    inputByteBuffer.flip();\n+\n+    //Set up char buffer for writes\n+    decodedCharBuffer.compact();\n+\n+    //Expand the char buffer if needed.\n+    if (decodedCharBuffer.capacity() < inputByteBuffer.limit() / sourceEncoding.getNumBytesPerCharacter()) {\n+      decodedCharBuffer = CharBuffer.allocate(inputByteBuffer.limit() / sourceEncoding.getNumBytesPerCharacter());\n+    }\n+\n+    //Decode bytes from the input buffer into the Decoded Char Buffer\n+    decodeByteBufferIntoCharBuffer(inputByteBuffer);\n+\n+    //Set up decoded char buffer for reads.\n+    decodedCharBuffer.flip();\n+\n+    //Set up incoming buffer for writes.\n+    inputByteBuffer.compact();\n+\n+  }\n+\n+  /**\n+   * Note that we only ask for additional input once we have completely depleted out outgoing buffer.\n+   */\n+  @Override\n+  public boolean needsInput() {\n+    return decodedCharBuffer.remaining() == 0;\n+  }\n+\n+  @Override\n+  public void setDictionary(byte[] b, int off, int len) {\n+    //no-op\n+  }\n+\n+  @Override\n+  public boolean needsDictionary() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean finished() {\n+    return decodedCharBuffer.remaining() == 0 && partialOutputByteBuffer.remaining() == 0;\n+  }\n+\n+  @Override\n+  public int decompress(byte[] b, int off, int len) throws IOException {\n+\n+    //Allocate new outgoing buffer\n+    ByteBuffer encodedBuffer = ByteBuffer.allocate(len - off);", "originalCommit": "758257c745606bbe61d48d819215a189a4c7c286", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDU0MzQ3Ng==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r544543476", "bodyText": "Good catch. I'll fix this.", "author": "fernst", "createdAt": "2020-12-16T18:54:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTY1NzAyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTY1OTcxNg==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r539659716", "bodyText": "This is a pretty heavy operation. I think we need it only when we could not encode anything. It's totally fine to fil less than len as soon as we fill at least something. We are not required to fill each block exactly to the length.", "author": "tivv", "createdAt": "2020-12-09T21:26:09Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressor.java", "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.CharBuffer;\n+import java.nio.charset.Charset;\n+import java.nio.charset.CharsetDecoder;\n+import java.nio.charset.CharsetEncoder;\n+import java.nio.charset.CoderResult;\n+import java.nio.charset.StandardCharsets;\n+\n+/**\n+ * Decompressor that can be used to convert byte streams in fixed-length character encodings to a stream of UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressor implements Decompressor {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingDecompressor.class);\n+\n+  protected final FixedLengthCharset sourceEncoding;\n+  protected final CharsetDecoder decoder;\n+  protected final CharsetEncoder encoder;\n+  protected final Charset targetCharset = StandardCharsets.UTF_8;\n+  protected long numDecodedCharacters = 0;\n+  protected long numEncodedCharacters = 0;\n+\n+  //Initializing all buffers.\n+  protected ByteBuffer inputByteBuffer = ByteBuffer.allocate(0);\n+  protected CharBuffer decodedCharBuffer = CharBuffer.allocate(0);\n+  protected ByteBuffer partialOutputByteBuffer = ByteBuffer.allocate(0);\n+\n+  public FixedLengthCharsetTransformingDecompressor(FixedLengthCharset sourceEncoding) {\n+    this.sourceEncoding = sourceEncoding;\n+    this.decoder = sourceEncoding.getCharset().newDecoder();\n+    this.encoder = targetCharset.newEncoder();\n+  }\n+\n+  @Override\n+  public void setInput(byte[] b, int off, int len) {\n+    //Expand incoming buffer if needed.\n+    if (inputByteBuffer.remaining() < len) {\n+      //Allocate new buffer that can fill the existing input + newly received bytes\n+      ByteBuffer newIncomingBuffer = ByteBuffer.allocate(len + inputByteBuffer.capacity());\n+\n+      //Set up incoming buffer for reads and copy contents into new buffer.\n+      inputByteBuffer.flip();\n+      newIncomingBuffer.put(inputByteBuffer);\n+\n+      inputByteBuffer = newIncomingBuffer;\n+    }\n+\n+    //Copy incoming payload into Input Byte Buffer\n+    inputByteBuffer.put(b, off, len);\n+    inputByteBuffer.flip();\n+\n+    //Set up char buffer for writes\n+    decodedCharBuffer.compact();\n+\n+    //Expand the char buffer if needed.\n+    if (decodedCharBuffer.capacity() < inputByteBuffer.limit() / sourceEncoding.getNumBytesPerCharacter()) {\n+      decodedCharBuffer = CharBuffer.allocate(inputByteBuffer.limit() / sourceEncoding.getNumBytesPerCharacter());\n+    }\n+\n+    //Decode bytes from the input buffer into the Decoded Char Buffer\n+    decodeByteBufferIntoCharBuffer(inputByteBuffer);\n+\n+    //Set up decoded char buffer for reads.\n+    decodedCharBuffer.flip();\n+\n+    //Set up incoming buffer for writes.\n+    inputByteBuffer.compact();\n+\n+  }\n+\n+  /**\n+   * Note that we only ask for additional input once we have completely depleted out outgoing buffer.\n+   */\n+  @Override\n+  public boolean needsInput() {\n+    return decodedCharBuffer.remaining() == 0;\n+  }\n+\n+  @Override\n+  public void setDictionary(byte[] b, int off, int len) {\n+    //no-op\n+  }\n+\n+  @Override\n+  public boolean needsDictionary() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean finished() {\n+    return decodedCharBuffer.remaining() == 0 && partialOutputByteBuffer.remaining() == 0;\n+  }\n+\n+  @Override\n+  public int decompress(byte[] b, int off, int len) throws IOException {\n+\n+    //Allocate new outgoing buffer\n+    ByteBuffer encodedBuffer = ByteBuffer.allocate(len - off);\n+\n+    //Consume any remaining bytes from a previous decompress invocation.\n+    while (partialOutputByteBuffer != null && partialOutputByteBuffer.hasRemaining() && encodedBuffer.hasRemaining()) {\n+      encodedBuffer.put(partialOutputByteBuffer.get());\n+    }\n+\n+    //Encode as many characters as possible into the Encoded Buffer.\n+    encodeCharBufferIntoByteBuffer(encodedBuffer);\n+\n+    // Handle the case where the outgoing buffer can still fit additional space.\n+    // This means we need to encode one extra character and add as many bytes as possible into the output buffer.\n+    if (decodedCharBuffer.remaining() > 0 && encodedBuffer.remaining() > 0) {\n+      encodePartialCharacter(encodedBuffer);", "originalCommit": "758257c745606bbe61d48d819215a189a4c7c286", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDU0OTQ5OA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r544549498", "bodyText": "Good catch. I've added additional logic to only execute this operation when there is no other option (Meaning, we were not able to add a single byte to the output.", "author": "fernst", "createdAt": "2020-12-16T19:03:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTY1OTcxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDU2Njc0Mg==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r544566742", "bodyText": "I actually think this can be our problem and not partition alignment. If we read something from input and then don't return it to user we can't give proper position information and records are lost", "author": "tivv", "createdAt": "2020-12-16T19:31:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTY1OTcxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzc4MTMzMw==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r543781333", "bodyText": "I see you committed this for test support, but I can't seem to find the test that uses it.", "author": "tivv", "createdAt": "2020-12-16T00:14:48Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressorStream.java", "diffHunk": "@@ -104,4 +119,17 @@ protected int getCompressedData() throws IOException {\n \n     return numReadBytes;\n   }\n+\n+  /**\n+   * Method that invokes the parent method to get decompressed data, without partition boundary awareness.\n+   * <p>\n+   * This method is visible for the purposes of testing the partition boundaries.\n+   *\n+   * @return number of bytes read from source input stream\n+   * @throws IOException If there was a problem reading from the underlying input stream.\n+   */\n+  @VisibleForTesting\n+  protected int getCompressedDataSuper() throws IOException {", "originalCommit": "125413aeb91761443be156c954426dc91869793a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQ1NTg3NQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r544455875", "bodyText": "I've added the missing class. I forgot to stage it for commit.", "author": "fernst", "createdAt": "2020-12-16T16:47:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzc4MTMzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzc5MTA1Mw==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r543791053", "bodyText": "What does this inheritance give you? You are only using it to wrap into TransformingCompressionInputStream. Would it make sense to move logic from here to TransformingCompressionInputStream and use DecompressorStream from TransformingCompressionInputStream. Having one less layer can make position handling easier.", "author": "tivv", "createdAt": "2020-12-16T00:39:44Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressorStream.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.io.compress.DecompressorStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * DecompressorStream implementation for the FixedLengthCharsetTransformingDecompressor.\n+ * <p>\n+ * This can be used to convert input streams containing bytes for fixed length charsets into UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressorStream extends DecompressorStream {", "originalCommit": "125413aeb91761443be156c954426dc91869793a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzc5ODY5NA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r543798694", "bodyText": "Generally I am not sure if using DecompressorStream simplifies things or makes it harder. BZip2CompressionInputStream does not use it and it looks very clean and small.", "author": "tivv", "createdAt": "2020-12-16T00:59:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzc5MTA1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQ0MzYwMQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r544443601", "bodyText": "The DecompressorStream gives me a place where I can check for partition boundaries and align the reads with the partition boundary. This ensures the Line Record Reader is able to read all the lines in a partition: \n  \n    \n      hydrator-plugins/format-common/src/main/java/io/cdap/plugin/format/charset/CharsetTransformingLineRecordReader.java\n    \n    \n         Line 159\n      in\n      125413a\n    \n    \n    \n    \n\n        \n          \n           while (getFilePosition() <= end || in.needAdditionalRecordAfterSplit()) {", "author": "fernst", "createdAt": "2020-12-16T16:32:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzc5MTA1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQ4MTMyNw==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r544481327", "bodyText": "You are using SplitCompressionInputStream in there. In it you are building upon FixedLengthCharsetTransformingDecompressorStream that extends DecompressorStream My question is if it simpler to implement SplitCompressionInputStream directly without additional layers, especially extending DecompressorStream in a non-trivial fragile way. The one example implementation of SplitCompressionInputStream i hadoop is BZip2CompressionInputStream that do not use DecompressorStream underneath, but works with BZIP algorithm directly.", "author": "tivv", "createdAt": "2020-12-16T17:21:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzc5MTA1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDU4MTcwMA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r544581700", "bodyText": "The DecompressorStream gives us an avenue to test the partition boundaries in https://github.com/cdapio/hydrator-plugins/blob/29aaaf55010acbc6fb19c775a3f64fbfb2c7e3df/format-common/src/test/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressorStreamTest.java.\nI do not think the encapsulation/additional layers really pose a problem for us. I tried to keep the implementation as similar as possible to the original LineRecordReader class I copied for simplicity/maintainability in the future.", "author": "fernst", "createdAt": "2020-12-16T19:55:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzc5MTA1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDU4NTMxOA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r544585318", "bodyText": "Changes for that test is actually one of the biggest indicators of structure issues. Having a method like getCompressedDataSuper is a big smell of structural problems. Let's sync up on this partition boundary synchronization, with a good integration test that shows the problem we are trying to solve.", "author": "tivv", "createdAt": "2020-12-16T20:01:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzc5MTA1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDYzNjg4Mw==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r544636883", "bodyText": "The purpose of the test is to demonstrate that the super method exceeds the partition boundary, which poses a problem when reading lines.\nI will come up with a scenario to demonstrate this issue tomorrow.", "author": "fernst", "createdAt": "2020-12-16T21:30:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzc5MTA1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzc5MTQxNA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r543791414", "bodyText": "I believe this is not needed. Same logic would be done in DefaultCodec", "author": "tivv", "createdAt": "2020-12-16T00:40:37Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingCodec.java", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.io.compress.CompressionInputStream;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.io.compress.Compressor;\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.apache.hadoop.io.compress.DefaultCodec;\n+import org.apache.hadoop.io.compress.DirectDecompressor;\n+import org.apache.hadoop.io.compress.SplitCompressionInputStream;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+\n+/**\n+ * Codec implementation that returns a decompressor for Fixed Length character encodings.\n+ */\n+public class FixedLengthCharsetTransformingCodec extends DefaultCodec\n+  implements Configurable, SplittableCompressionCodec {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingCodec.class);\n+\n+  private final FixedLengthCharset sourceEncoding;\n+\n+  public FixedLengthCharsetTransformingCodec(FixedLengthCharset sourceEncoding) {\n+    this.sourceEncoding = sourceEncoding;\n+  }\n+\n+  @Override\n+  public CompressionOutputStream createOutputStream(OutputStream out) throws IOException {", "originalCommit": "125413aeb91761443be156c954426dc91869793a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQ1MzA1NA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r544453054", "bodyText": "I wanted to make it clear that this codec is supposed to be used only for decoding into UTF-8, not viceversa.", "author": "fernst", "createdAt": "2020-12-16T16:44:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzc5MTQxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQ3NjgwMA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r544476800", "bodyText": "I am worrying that DefaultCodec is still marked as @InterfaceStability.Evolving, so it's subject to change and less we override less problems we would have. Also we would still have two methods throwing unsupported exception that should explain the intent", "author": "tivv", "createdAt": "2020-12-16T17:15:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzc5MTQxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzc5MTU0Mg==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r543791542", "bodyText": "I believe this is not needed. Given compressor is passed it's fine to allow it.", "author": "tivv", "createdAt": "2020-12-16T00:41:03Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingCodec.java", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.io.compress.CompressionInputStream;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.io.compress.Compressor;\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.apache.hadoop.io.compress.DefaultCodec;\n+import org.apache.hadoop.io.compress.DirectDecompressor;\n+import org.apache.hadoop.io.compress.SplitCompressionInputStream;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+\n+/**\n+ * Codec implementation that returns a decompressor for Fixed Length character encodings.\n+ */\n+public class FixedLengthCharsetTransformingCodec extends DefaultCodec\n+  implements Configurable, SplittableCompressionCodec {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingCodec.class);\n+\n+  private final FixedLengthCharset sourceEncoding;\n+\n+  public FixedLengthCharsetTransformingCodec(FixedLengthCharset sourceEncoding) {\n+    this.sourceEncoding = sourceEncoding;\n+  }\n+\n+  @Override\n+  public CompressionOutputStream createOutputStream(OutputStream out) throws IOException {\n+    throw new UnsupportedOperationException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public CompressionOutputStream createOutputStream(OutputStream out, Compressor compressor) throws IOException {", "originalCommit": "125413aeb91761443be156c954426dc91869793a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDU1ODU1Mg==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r544558552", "bodyText": "Good point. I've removed this method so the superclass method gets called instead.", "author": "fernst", "createdAt": "2020-12-16T19:18:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzc5MTU0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzc5MTY5NQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r543791695", "bodyText": "I believe this is not needed. Same logic would be done in DefaultCodec", "author": "tivv", "createdAt": "2020-12-16T00:41:31Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingCodec.java", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.io.compress.CompressionInputStream;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.io.compress.Compressor;\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.apache.hadoop.io.compress.DefaultCodec;\n+import org.apache.hadoop.io.compress.DirectDecompressor;\n+import org.apache.hadoop.io.compress.SplitCompressionInputStream;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+\n+/**\n+ * Codec implementation that returns a decompressor for Fixed Length character encodings.\n+ */\n+public class FixedLengthCharsetTransformingCodec extends DefaultCodec\n+  implements Configurable, SplittableCompressionCodec {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingCodec.class);\n+\n+  private final FixedLengthCharset sourceEncoding;\n+\n+  public FixedLengthCharsetTransformingCodec(FixedLengthCharset sourceEncoding) {\n+    this.sourceEncoding = sourceEncoding;\n+  }\n+\n+  @Override\n+  public CompressionOutputStream createOutputStream(OutputStream out) throws IOException {\n+    throw new UnsupportedOperationException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public CompressionOutputStream createOutputStream(OutputStream out, Compressor compressor) throws IOException {\n+    throw new UnsupportedOperationException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public Class<? extends Compressor> getCompressorType() {\n+    throw new UnsupportedOperationException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public Compressor createCompressor() {\n+    throw new UnsupportedOperationException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public CompressionInputStream createInputStream(InputStream in) throws IOException {", "originalCommit": "125413aeb91761443be156c954426dc91869793a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQ1NTQ2Ng==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r544455466", "bodyText": "Good point. Removed.", "author": "fernst", "createdAt": "2020-12-16T16:47:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzc5MTY5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI2ODEyMg==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545268122", "bodyText": "Some imports are not used. Please optimize imports across files.", "author": "tivv", "createdAt": "2020-12-17T17:27:43Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharset.java", "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;", "originalCommit": "40bd8dc118818fd5afcbfae8edcec9841ee001cb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTM4Nzc3OA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545387778", "bodyText": "Done.", "author": "fernst", "createdAt": "2020-12-17T20:36:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI2ODEyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3MTQ4Nw==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545271487", "bodyText": "I don't think this one is used", "author": "tivv", "createdAt": "2020-12-17T17:32:32Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressorStream.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.io.compress.DecompressorStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * DecompressorStream implementation for the FixedLengthCharsetTransformingDecompressor.\n+ * <p>\n+ * This can be used to convert input streams containing bytes for fixed length charsets into UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressorStream extends DecompressorStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingDecompressorStream.class);", "originalCommit": "40bd8dc118818fd5afcbfae8edcec9841ee001cb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQwNjg2OQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545406869", "bodyText": "You're right. Removed.", "author": "fernst", "createdAt": "2020-12-17T21:12:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3MTQ4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3MTcwMQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545271701", "bodyText": "I don't think you use this value outside of constructor, no need for a field", "author": "tivv", "createdAt": "2020-12-17T17:32:53Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressorStream.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.io.compress.DecompressorStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * DecompressorStream implementation for the FixedLengthCharsetTransformingDecompressor.\n+ * <p>\n+ * This can be used to convert input streams containing bytes for fixed length charsets into UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressorStream extends DecompressorStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingDecompressorStream.class);\n+\n+  //Starting and ending position in the file.\n+  private final long start;\n+  private final long end;\n+  private final FixedLengthCharset fixedLengthCharset;", "originalCommit": "40bd8dc118818fd5afcbfae8edcec9841ee001cb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQwNjc5NA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545406794", "bodyText": "Good point. Removed.", "author": "fernst", "createdAt": "2020-12-17T21:12:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3MTcwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3MzA3Mg==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545273072", "bodyText": "nit: pos is not used. It may make sense even to remove the method", "author": "tivv", "createdAt": "2020-12-17T17:34:31Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/CharsetTransformingLineRecordReader.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharset;\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharsetTransformingCodec;\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharsetTransformingDecompressorStream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.apache.hadoop.io.compress.SplitCompressionInputStream;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.hadoop.mapreduce.lib.input.SplitLineReader;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Copy of Hadoop's LineRecordReader (Hadoop Version 2.3.0). The reason we copy this class is to modify some behaviors\n+ * related to the creation of the decompressor. This also allows us to implement\n+ * <p>\n+ * This class uses a fixed Codec and Decompressor to parse records.\n+ */\n+public class CharsetTransformingLineRecordReader extends RecordReader<LongWritable, Text> {\n+  private static final Logger LOG = LoggerFactory.getLogger(CharsetTransformingLineRecordReader.class);\n+  public static final String MAX_LINE_LENGTH =\n+    \"mapreduce.input.linerecordreader.line.maxlength\";\n+\n+  private final FixedLengthCharset fixedLengthCharset;\n+  private final byte[] recordDelimiterBytes;\n+  private long start;\n+  private long pos;\n+  private long end;\n+  private SplitLineReader in;\n+  private Seekable filePosition;\n+  private int maxLineLength;\n+  private LongWritable key;\n+  private Text value;\n+  private Decompressor decompressor;\n+\n+  public CharsetTransformingLineRecordReader(FixedLengthCharset fixedLengthCharset, byte[] recordDelimiter) {\n+    this.fixedLengthCharset = fixedLengthCharset;\n+    this.recordDelimiterBytes = recordDelimiter;\n+  }\n+\n+  @VisibleForTesting\n+  protected CharsetTransformingLineRecordReader(FixedLengthCharset fixedLengthCharset,\n+                                                byte[] recordDelimiter,\n+                                                SplitLineReader in,\n+                                                long start,\n+                                                long pos,\n+                                                long end,\n+                                                int maxLineLength) {\n+    this(fixedLengthCharset, recordDelimiter);\n+    this.in = in;\n+    this.start = start;\n+    this.pos = pos;\n+    this.end = end;\n+    this.maxLineLength = maxLineLength;\n+  }\n+\n+  /**\n+   * Initialize method from parent class, simplified for this our use case from the base class.\n+   *\n+   * @param genericSplit File Split\n+   * @param context      Execution context\n+   * @throws IOException if the underlying file or decompression operations fail.\n+   */\n+  public void initialize(InputSplit genericSplit,\n+                         TaskAttemptContext context) throws IOException {\n+    FileSplit split = (FileSplit) genericSplit;\n+    Configuration job = context.getConfiguration();\n+    this.maxLineLength = job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n+    start = split.getStart();\n+    end = start + split.getLength();\n+    final Path file = split.getPath();\n+\n+    // open the file and seek to the start of the split\n+    final FileSystem fs = file.getFileSystem(job);\n+    FSDataInputStream fileIn = fs.open(file);\n+\n+    SplittableCompressionCodec codec = new FixedLengthCharsetTransformingCodec(fixedLengthCharset);\n+    decompressor = codec.createDecompressor();\n+\n+    final SplitCompressionInputStream cIn =\n+      codec.createInputStream(\n+        fileIn, decompressor, start, end,\n+        SplittableCompressionCodec.READ_MODE.CONTINUOUS);\n+    in = new CompressedSplitLineReader(cIn, job,\n+                                       this.recordDelimiterBytes);\n+    start = cIn.getAdjustedStart();\n+    end = cIn.getAdjustedEnd();\n+    filePosition = cIn;\n+\n+    // If this is not the first split, we always throw away first record\n+    // because we always (except the last split) read one extra line in\n+    // next() method.\n+    if (start != 0) {\n+      Text t = new Text();\n+      start += in.readLine(t, 4096, maxBytesToConsume(start));\n+      LOG.info(\"Discarded line: \" + t.toString());\n+    }\n+    this.pos = start;\n+  }\n+\n+  /**\n+   * Returns the maximum of bytes to consume from the input stream\n+   * Since the input is compressed, there is no way to accurately determine how many bytes we need to consume.\n+   *\n+   * @param pos Current file position\n+   * @return Number of bytes to consume.\n+   */\n+  private int maxBytesToConsume(long pos) {", "originalCommit": "40bd8dc118818fd5afcbfae8edcec9841ee001cb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQwNjY5MA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545406690", "bodyText": "Removed.", "author": "fernst", "createdAt": "2020-12-17T21:12:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3MzA3Mg=="}], "type": "inlineReview"}, {"oid": "7bf14e395cb5ecd3c3ac71a4fb097469d23ddad9", "url": "https://github.com/cdapio/hydrator-plugins/commit/7bf14e395cb5ecd3c3ac71a4fb097469d23ddad9", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nI've identified an edge case when some lines get skipped in the partition edges. I'm close to isolating the cause of the issue.\n\nFixed issue with the final line (or lines) or a partition being skipped because of boundary calculation issues.\n\nAdded additional fixed-length encodings and tests.\n\nWill continue to add additonal tests and tie all configurations together.\n\nAdded support for configuration parameter supplied by the UI.\n\nAdditional cleanup.\n\nAddressed some of the PR comments.\n\nAdded additional comments.\n\nRewrote decompressor to use ByteBuffer and CharBuffer.\n\nAdded UI elements for the new File Encoding option for the File Batch Source.\n\nSmall tweaks based on comments on the PR.\n\nAdded logic to deal with partition boundaries. This ensure records are read exactly once.\n\nTested with UTF-32 and ISO-8859-1.\n\nSmall tweak based on comments.\n\nImproved Decompressor Stream partition boundary handling.\n\nFixed issue with Abstract File Source configuration. When the default encoding (UTF-8) is selected, the standard Hadoop Line Record Reader is used.\n\nAdded additional tests to validate partition boundary behavior.\n\nAdded missing test class.\n\nRemoved some redundant functions.\n\nAddressed some performance concerns on the decompressor.\n\nSimplified edge case handling for the decompressor.\n\nFurther cleanup based on review.\n\nSet up test which highlights the partition boundary issues we have to solve in the DecompressorStream.\n\nCleaning up.\n\nAdditional cleaning up for DecompressorStream class.\n\nUpdated tests to use base instances to highlight implementation details in our solution\n\nFurther cleanup.\n\nAdded support for all IBM code pages supported by Java, including the EBCIDC encodings..", "committedDate": "2020-12-17T20:52:11Z", "type": "forcePushed"}, {"oid": "8ac4d372de73284b8c29febc16f041675dc3b287", "url": "https://github.com/cdapio/hydrator-plugins/commit/8ac4d372de73284b8c29febc16f041675dc3b287", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nI've identified an edge case when some lines get skipped in the partition edges. I'm close to isolating the cause of the issue.\n\nFixed issue with the final line (or lines) or a partition being skipped because of boundary calculation issues.\n\nAdded additional fixed-length encodings and tests.\n\nWill continue to add additonal tests and tie all configurations together.\n\nAdded support for configuration parameter supplied by the UI.\n\nAdditional cleanup.\n\nAddressed some of the PR comments.\n\nAdded additional comments.\n\nRewrote decompressor to use ByteBuffer and CharBuffer.\n\nAdded UI elements for the new File Encoding option for the File Batch Source.\n\nSmall tweaks based on comments on the PR.\n\nAdded logic to deal with partition boundaries. This ensure records are read exactly once.\n\nTested with UTF-32 and ISO-8859-1.\n\nSmall tweak based on comments.\n\nImproved Decompressor Stream partition boundary handling.\n\nFixed issue with Abstract File Source configuration. When the default encoding (UTF-8) is selected, the standard Hadoop Line Record Reader is used.\n\nAdded additional tests to validate partition boundary behavior.\n\nAdded missing test class.\n\nRemoved some redundant functions.\n\nAddressed some performance concerns on the decompressor.\n\nSimplified edge case handling for the decompressor.\n\nFurther cleanup based on review.\n\nSet up test which highlights the partition boundary issues we have to solve in the DecompressorStream.\n\nCleaning up.\n\nAdditional cleaning up for DecompressorStream class.\n\nUpdated tests to use base instances to highlight implementation details in our solution\n\nFurther cleanup.\n\nAdded support for all IBM code pages supported by Java, including the EBCIDC encodings..", "committedDate": "2020-12-17T21:08:35Z", "type": "forcePushed"}, {"oid": "309086086aadd7941a7339d1abbfe9917dba8b48", "url": "https://github.com/cdapio/hydrator-plugins/commit/309086086aadd7941a7339d1abbfe9917dba8b48", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nI've identified an edge case when some lines get skipped in the partition edges. I'm close to isolating the cause of the issue.\n\nFixed issue with the final line (or lines) or a partition being skipped because of boundary calculation issues.\n\nAdded additional fixed-length encodings and tests.\n\nWill continue to add additonal tests and tie all configurations together.\n\nAdded support for configuration parameter supplied by the UI.\n\nAdditional cleanup.\n\nAddressed some of the PR comments.\n\nAdded additional comments.\n\nRewrote decompressor to use ByteBuffer and CharBuffer.\n\nAdded UI elements for the new File Encoding option for the File Batch Source.\n\nSmall tweaks based on comments on the PR.\n\nAdded logic to deal with partition boundaries. This ensure records are read exactly once.\n\nTested with UTF-32 and ISO-8859-1.\n\nSmall tweak based on comments.\n\nImproved Decompressor Stream partition boundary handling.\n\nFixed issue with Abstract File Source configuration. When the default encoding (UTF-8) is selected, the standard Hadoop Line Record Reader is used.\n\nAdded additional tests to validate partition boundary behavior.\n\nAdded missing test class.\n\nRemoved some redundant functions.\n\nAddressed some performance concerns on the decompressor.\n\nSimplified edge case handling for the decompressor.\n\nFurther cleanup based on review.\n\nSet up test which highlights the partition boundary issues we have to solve in the DecompressorStream.\n\nCleaning up.\n\nAdditional cleaning up for DecompressorStream class.\n\nUpdated tests to use base instances to highlight implementation details in our solution\n\nFurther cleanup.\n\nAdded support for all IBM code pages supported by Java, including the EBCIDC encodings..", "committedDate": "2020-12-17T22:28:00Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ4MTA4Nw==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545481087", "bodyText": "there was another change that was recently merged adding a shouldGetSchema() method (see #1288). That new method also needs to be updated to also check that the fileEncoding is not macro enabled.", "author": "albertshau", "createdAt": "2020-12-17T23:51:08Z", "path": "format-common/src/main/java/io/cdap/plugin/format/plugin/AbstractFileSourceConfig.java", "diffHunk": "@@ -133,6 +140,17 @@ public void validate(FailureCollector collector) {\n       collector.addFailure(e.getMessage(), null).withConfigProperty(NAME_SCHEMA).withStacktrace(e.getStackTrace());\n     }\n \n+    if (fileEncoding != null && !fileEncoding.isEmpty()) {", "originalCommit": "309086086aadd7941a7339d1abbfe9917dba8b48", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ4NDE3Mg==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545484172", "bodyText": "is this needed? Seems simpler to just have getFileEncoding() with return fileEncoding == null || fileEncoding.isEmpty() ? DEFAULT_FILE_ENCODING : fileEncoding", "author": "albertshau", "createdAt": "2020-12-17T23:58:53Z", "path": "format-common/src/main/java/io/cdap/plugin/format/plugin/AbstractFileSourceConfig.java", "diffHunk": "@@ -198,6 +216,15 @@ public boolean skipHeader() {\n     return skipHeader == null ? false : skipHeader;\n   }\n \n+  @Nullable\n+  public String getFileEncoding() {\n+    return fileEncoding;\n+  }\n+\n+  public String getDefaultFileEncoding() {", "originalCommit": "309086086aadd7941a7339d1abbfe9917dba8b48", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg3OTI4MQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r546879281", "bodyText": "We still need this to check wether the configured encoding is not UTF-8 (which is the default), as in this case, there is no need for us to enable this decompressor.", "author": "fernst", "createdAt": "2020-12-21T19:08:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ4NDE3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ4NTU4MQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545485581", "bodyText": "style: we only import the class, not the static method.", "author": "albertshau", "createdAt": "2020-12-18T00:03:06Z", "path": "format-common/src/test/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTest.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static io.cdap.plugin.format.plugin.AbstractFileSourceConfig.cleanFileEncodingName;", "originalCommit": "309086086aadd7941a7339d1abbfe9917dba8b48", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ4NjIxNg==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545486216", "bodyText": "it's better not to mutate any fields, as it could lead to weird bugs if getFileEncoding() is called before validate().\nInstead, it seems like we should change the getter to clean the encoding name before returning it. This incurs the extra cost of cleaning per get call, but it shouldn't be a big deal.", "author": "albertshau", "createdAt": "2020-12-18T00:04:59Z", "path": "format-common/src/main/java/io/cdap/plugin/format/plugin/AbstractFileSourceConfig.java", "diffHunk": "@@ -133,6 +140,17 @@ public void validate(FailureCollector collector) {\n       collector.addFailure(e.getMessage(), null).withConfigProperty(NAME_SCHEMA).withStacktrace(e.getStackTrace());\n     }\n \n+    if (fileEncoding != null && !fileEncoding.isEmpty()) {\n+      fileEncoding = cleanFileEncodingName(fileEncoding);", "originalCommit": "309086086aadd7941a7339d1abbfe9917dba8b48", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ4Njk3Mg==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545486972", "bodyText": "ideally we would just configure the UI to use a different display name than the actual value instead of performing some cleanup in the backend. However, I don't think there's a way to do this for the select widget.  @elfenheart can you confirm?", "author": "albertshau", "createdAt": "2020-12-18T00:07:07Z", "path": "format-common/src/main/java/io/cdap/plugin/format/plugin/AbstractFileSourceConfig.java", "diffHunk": "@@ -210,4 +237,16 @@ public Schema getSchema() {\n   public boolean shouldCopyHeader() {\n     return copyHeader;\n   }\n+\n+  /**\n+   * Takes the first word of the file encoding string, as any further words are just charset descriptions.\n+   *\n+   * @param fileEncoding The file encoding parameter supplied by the user\n+   * @return the cleaned up file encoding name\n+   */\n+  public static String cleanFileEncodingName(String fileEncoding) {", "originalCommit": "309086086aadd7941a7339d1abbfe9917dba8b48", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ5MTc2Ng==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545491766", "bodyText": "Looks like there is a way, using:\noptions: [ { label: 'this is the label', value: 'value' }  ]", "author": "albertshau", "createdAt": "2020-12-18T00:21:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ4Njk3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg0MTAwNQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r546841005", "bodyText": "I didn't know the UI supported this configuration format. I'll fix it.", "author": "fernst", "createdAt": "2020-12-21T17:42:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ4Njk3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUyMjE0MA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545522140", "bodyText": "Please update the javadoc", "author": "tivv", "createdAt": "2020-12-18T01:54:33Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharset.java", "diffHunk": "@@ -0,0 +1,97 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import com.google.common.collect.ImmutableMap;\n+\n+import java.nio.charset.Charset;\n+import java.nio.charset.CharsetEncoder;\n+import java.util.Map;\n+\n+/**\n+ * Enumeration containing all currently supported Fixed Length charsets.\n+ * <p>\n+ * This currently includes:\n+ * - UTF-32,\n+ * - ISO-8859 variants supported by Java\n+ * - Windows single-byte code pages supported by Java.", "originalCommit": "309086086aadd7941a7339d1abbfe9917dba8b48", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzAzMTQyOA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r547031428", "bodyText": "Updated.", "author": "fernst", "createdAt": "2020-12-22T02:27:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUyMjE0MA=="}], "type": "inlineReview"}, {"oid": "7b6eee761cf030794bbafa222a42ecc221a2bfd5", "url": "https://github.com/cdapio/hydrator-plugins/commit/7b6eee761cf030794bbafa222a42ecc221a2bfd5", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nI've identified an edge case when some lines get skipped in the partition edges. I'm close to isolating the cause of the issue.\n\nFixed issue with the final line (or lines) or a partition being skipped because of boundary calculation issues.\n\nAdded additional fixed-length encodings and tests.\n\nWill continue to add additonal tests and tie all configurations together.\n\nAdded support for configuration parameter supplied by the UI.\n\nAdditional cleanup.\n\nAddressed some of the PR comments.\n\nAdded additional comments.\n\nRewrote decompressor to use ByteBuffer and CharBuffer.\n\nAdded UI elements for the new File Encoding option for the File Batch Source.\n\nSmall tweaks based on comments on the PR.\n\nAdded logic to deal with partition boundaries. This ensure records are read exactly once.\n\nTested with UTF-32 and ISO-8859-1.\n\nSmall tweak based on comments.\n\nImproved Decompressor Stream partition boundary handling.\n\nFixed issue with Abstract File Source configuration. When the default encoding (UTF-8) is selected, the standard Hadoop Line Record Reader is used.\n\nAdded additional tests to validate partition boundary behavior.\n\nAdded missing test class.\n\nRemoved some redundant functions.\n\nAddressed some performance concerns on the decompressor.\n\nSimplified edge case handling for the decompressor.\n\nFurther cleanup based on review.\n\nSet up test which highlights the partition boundary issues we have to solve in the DecompressorStream.\n\nCleaning up.\n\nAdditional cleaning up for DecompressorStream class.\n\nUpdated tests to use base instances to highlight implementation details in our solution\n\nFurther cleanup.\n\nAdded support for all IBM code pages supported by Java, including the EBCIDC encodings..", "committedDate": "2020-12-21T22:12:42Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzAwMzA3OQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r547003079", "bodyText": "Almost forgot. In this version of JUnit you do assertEquals(expected, actual). Please fix, otherwise errors are looking weird.", "author": "tivv", "createdAt": "2020-12-22T00:40:37Z", "path": "format-common/src/test/java/io/cdap/plugin/format/charset/CharsetTransformingLineRecordReaderTest.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset;\n+\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharset;\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharsetTransformingCodec;\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharsetTransformingDecompressorStream;\n+import io.cdap.plugin.format.charset.fixedlength.TransformingCompressionInputStream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.compress.CompressionInputStream;\n+import org.apache.hadoop.io.compress.SplitCompressionInputStream;\n+import org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.mockito.junit.MockitoJUnitRunner;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+\n+import static org.powermock.api.mockito.PowerMockito.doAnswer;\n+import static org.powermock.api.mockito.PowerMockito.spy;\n+\n+@RunWith(MockitoJUnitRunner.class)\n+public class CharsetTransformingLineRecordReaderTest {\n+\n+  Configuration conf;\n+  FixedLengthCharset fixedLengthCharset;\n+  CharsetTransformingLineRecordReader recordReader;\n+  FixedLengthCharsetTransformingCodec codec;\n+  ByteArrayInputStream inputStream;\n+  int availableBytes;\n+  FixedLengthCharsetTransformingDecompressorStream decompressorStream;\n+  SplitCompressionInputStream compressionInputStream;\n+\n+  //We set up the input so each line is 5 characters long, which is 20 bytes in UTF-32.\n+  final String input = \"abcd\\nedfg\\nijkl\\n\";\n+\n+  @Before\n+  public void before() throws IOException {\n+    //Set up the Compressed Split Line Reader with a buffer size of 4096 bytes.\n+    //This ensures the buffer will consume all characters in the input stream if we allow it to.\n+    conf = new Configuration();\n+    conf.setInt(\"io.file.buffer.size\", 4096);\n+\n+    fixedLengthCharset = FixedLengthCharset.UTF_32;\n+\n+    codec = new FixedLengthCharsetTransformingCodec(fixedLengthCharset);\n+    codec.setConf(conf);\n+\n+    inputStream = new ByteArrayInputStream(input.getBytes(fixedLengthCharset.getCharset()));\n+    availableBytes = inputStream.available();\n+  }\n+\n+  public void setUpRecordReaderForTest(SplitCompressionInputStream splitCompressionInputStream) throws IOException {\n+\n+    // Set up record reader to assume we'll read the file from the beggining, and the partition size is 32 bytes\n+    // which is 8 characters in UTF-32, meaning we expect to read the first 2 lines for this partition.\n+    recordReader = spy(new CharsetTransformingLineRecordReader(\n+      fixedLengthCharset,\n+      null,\n+      new CompressedSplitLineReader(splitCompressionInputStream, conf, null),\n+      0,\n+      0,\n+      32,\n+      4096\n+    ));\n+\n+    //We will calculate position based on the number of bytes consumed from the input stream.\n+    doAnswer(a -> (long) availableBytes - inputStream.available()).when(recordReader).getFilePosition();\n+  }\n+\n+  @Test\n+  public void testGetNextLine() throws IOException {\n+    decompressorStream =\n+      new FixedLengthCharsetTransformingDecompressorStream(inputStream, FixedLengthCharset.UTF_32, 0, 32);\n+    compressionInputStream = new TransformingCompressionInputStream(decompressorStream, 0, 32);\n+\n+    //Set up test\n+    setUpRecordReaderForTest(compressionInputStream);\n+\n+    //Ensure the first line is read\n+    Assert.assertTrue(recordReader.nextKeyValue());\n+    Assert.assertEquals(recordReader.getCurrentKey().get(), 0);", "originalCommit": "7b6eee761cf030794bbafa222a42ecc221a2bfd5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzAxMDE2NQ==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r547010165", "bodyText": "Good catch. I'll fix that.", "author": "fernst", "createdAt": "2020-12-22T01:07:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzAwMzA3OQ=="}], "type": "inlineReview"}, {"oid": "95b37bf97bdfad5e7d43ff4af2cfd16e49da7cc8", "url": "https://github.com/cdapio/hydrator-plugins/commit/95b37bf97bdfad5e7d43ff4af2cfd16e49da7cc8", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nI've identified an edge case when some lines get skipped in the partition edges. I'm close to isolating the cause of the issue.\n\nFixed issue with the final line (or lines) or a partition being skipped because of boundary calculation issues.\n\nAdded additional fixed-length encodings and tests.\n\nWill continue to add additonal tests and tie all configurations together.\n\nAdded support for configuration parameter supplied by the UI.\n\nAdditional cleanup.\n\nAddressed some of the PR comments.\n\nAdded additional comments.\n\nRewrote decompressor to use ByteBuffer and CharBuffer.\n\nAdded UI elements for the new File Encoding option for the File Batch Source.\n\nSmall tweaks based on comments on the PR.\n\nAdded logic to deal with partition boundaries. This ensure records are read exactly once.\n\nTested with UTF-32 and ISO-8859-1.\n\nSmall tweak based on comments.\n\nImproved Decompressor Stream partition boundary handling.\n\nFixed issue with Abstract File Source configuration. When the default encoding (UTF-8) is selected, the standard Hadoop Line Record Reader is used.\n\nAdded additional tests to validate partition boundary behavior.\n\nAdded missing test class.\n\nRemoved some redundant functions.\n\nAddressed some performance concerns on the decompressor.\n\nSimplified edge case handling for the decompressor.\n\nFurther cleanup based on review.\n\nSet up test which highlights the partition boundary issues we have to solve in the DecompressorStream.\n\nCleaning up.\n\nAdditional cleaning up for DecompressorStream class.\n\nUpdated tests to use base instances to highlight implementation details in our solution\n\nFurther cleanup.\n\nAdded support for all IBM code pages supported by Java, including the EBCIDC encodings..", "committedDate": "2020-12-22T02:16:57Z", "type": "forcePushed"}, {"oid": "5d36de5ca33a0e277fec9e7e9362ed24934451e6", "url": "https://github.com/cdapio/hydrator-plugins/commit/5d36de5ca33a0e277fec9e7e9362ed24934451e6", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nI've identified an edge case when some lines get skipped in the partition edges. I'm close to isolating the cause of the issue.\n\nFixed issue with the final line (or lines) or a partition being skipped because of boundary calculation issues.\n\nAdded additional fixed-length encodings and tests.\n\nWill continue to add additonal tests and tie all configurations together.\n\nAdded support for configuration parameter supplied by the UI.\n\nAdditional cleanup.\n\nAddressed some of the PR comments.\n\nAdded additional comments.\n\nRewrote decompressor to use ByteBuffer and CharBuffer.\n\nAdded UI elements for the new File Encoding option for the File Batch Source.\n\nSmall tweaks based on comments on the PR.\n\nAdded logic to deal with partition boundaries. This ensure records are read exactly once.\n\nTested with UTF-32 and ISO-8859-1.\n\nSmall tweak based on comments.\n\nImproved Decompressor Stream partition boundary handling.\n\nFixed issue with Abstract File Source configuration. When the default encoding (UTF-8) is selected, the standard Hadoop Line Record Reader is used.\n\nAdded additional tests to validate partition boundary behavior.\n\nAdded missing test class.\n\nRemoved some redundant functions.\n\nAddressed some performance concerns on the decompressor.\n\nSimplified edge case handling for the decompressor.\n\nFurther cleanup based on review.\n\nSet up test which highlights the partition boundary issues we have to solve in the DecompressorStream.\n\nCleaning up.\n\nAdditional cleaning up for DecompressorStream class.\n\nUpdated tests to use base instances to highlight implementation details in our solution\n\nFurther cleanup.\n\nAdded support for all IBM code pages supported by Java, including the EBCIDC encodings..", "committedDate": "2020-12-22T15:54:16Z", "type": "forcePushed"}, {"oid": "6200875de859e968d129712bddb6feb816ff8f05", "url": "https://github.com/cdapio/hydrator-plugins/commit/6200875de859e968d129712bddb6feb816ff8f05", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nI've identified an edge case when some lines get skipped in the partition edges. I'm close to isolating the cause of the issue.\n\nFixed issue with the final line (or lines) or a partition being skipped because of boundary calculation issues.\n\nAdded additional fixed-length encodings and tests.\n\nWill continue to add additonal tests and tie all configurations together.\n\nAdded support for configuration parameter supplied by the UI.\n\nAdditional cleanup.\n\nAddressed some of the PR comments.\n\nAdded additional comments.\n\nRewrote decompressor to use ByteBuffer and CharBuffer.\n\nAdded UI elements for the new File Encoding option for the File Batch Source.\n\nSmall tweaks based on comments on the PR.\n\nAdded logic to deal with partition boundaries. This ensure records are read exactly once.\n\nTested with UTF-32 and ISO-8859-1.\n\nSmall tweak based on comments.\n\nImproved Decompressor Stream partition boundary handling.\n\nFixed issue with Abstract File Source configuration. When the default encoding (UTF-8) is selected, the standard Hadoop Line Record Reader is used.\n\nAdded additional tests to validate partition boundary behavior.\n\nAdded missing test class.\n\nRemoved some redundant functions.\n\nAddressed some performance concerns on the decompressor.\n\nSimplified edge case handling for the decompressor.\n\nFurther cleanup based on review.\n\nSet up test which highlights the partition boundary issues we have to solve in the DecompressorStream.\n\nCleaning up.\n\nAdditional cleaning up for DecompressorStream class.\n\nUpdated tests to use base instances to highlight implementation details in our solution\n\nFurther cleanup.\n\nAdded support for all IBM code pages supported by Java, including the EBCIDC encodings..\nMerge pull request #462 from data-integrations/feature/PLUGIN-464-flatten-fix\n\nPLUGIN-464 fix flatten to be a no-op on empty lists", "committedDate": "2020-12-23T16:58:16Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODMwOTExMA==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r548309110", "bodyText": "Please cast to seekable and use seek instead. E.g. on S3 filesystem FSDataInputStream is constructed on top of S3InputStream that do not override default skip, so skip is done in old way - by reading and skipping all the data.", "author": "tivv", "createdAt": "2020-12-23T22:59:40Z", "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressorStream.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.io.compress.DecompressorStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * DecompressorStream implementation for the FixedLengthCharsetTransformingDecompressor.\n+ * <p>\n+ * This can be used to convert input streams containing bytes for fixed length charsets into UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressorStream extends DecompressorStream {\n+\n+  private final long start;\n+  private final long end;\n+  private long totalReadBytes = 0;\n+\n+  public FixedLengthCharsetTransformingDecompressorStream(InputStream in,\n+                                                          FixedLengthCharset fixedLengthCharset,\n+                                                          long start,\n+                                                          long end)\n+    throws IOException {\n+    super(in, new FixedLengthCharsetTransformingDecompressor(fixedLengthCharset), 4096);\n+    in.skip(start);", "originalCommit": "6200875de859e968d129712bddb6feb816ff8f05", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODM2Mzc2Mw==", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r548363763", "bodyText": "@tivv I've amended my commit with your suggestion: https://github.com/cdapio/hydrator-plugins/pull/1255/files#diff-805003eb518a2ad27b1fb8bec65c7fffdbb0bbc9cce75df9edd1d7b2edd161d1R56", "author": "fernst", "createdAt": "2020-12-24T03:16:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODMwOTExMA=="}], "type": "inlineReview"}, {"oid": "9ba9dc7558b3f545c97e68fad49998ab434a5b0e", "url": "https://github.com/cdapio/hydrator-plugins/commit/9ba9dc7558b3f545c97e68fad49998ab434a5b0e", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nUpdated Decompressor Stream to use the Seek method from Seekable implementations.", "committedDate": "2020-12-24T03:13:38Z", "type": "commit"}, {"oid": "9ba9dc7558b3f545c97e68fad49998ab434a5b0e", "url": "https://github.com/cdapio/hydrator-plugins/commit/9ba9dc7558b3f545c97e68fad49998ab434a5b0e", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nUpdated Decompressor Stream to use the Seek method from Seekable implementations.", "committedDate": "2020-12-24T03:13:38Z", "type": "forcePushed"}]}