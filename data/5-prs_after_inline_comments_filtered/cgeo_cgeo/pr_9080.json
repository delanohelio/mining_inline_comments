{"pr_number": 9080, "pr_title": "rel to #9015: impr in-memory-filter perf, adjusts datastore-logs, logs logcat command", "pr_createdAt": "2020-09-27T10:01:00Z", "pr_url": "https://github.com/cgeo/cgeo/pull/9080", "timeline": [{"oid": "5466451ec23659876bbc29c87775c318bd682b34", "url": "https://github.com/cgeo/cgeo/commit/5466451ec23659876bbc29c87775c318bd682b34", "message": "rel to #9015: impr in-memory-filter perf, adjusts datastore-logs, logs logcat command", "committedDate": "2020-09-27T09:58:47Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTg3ODIzMQ==", "url": "https://github.com/cgeo/cgeo/pull/9080#discussion_r495878231", "bodyText": "Sorry for random comment, but shouldn't be this method performant in all the cases, including opposite one - filtering very large lists (50k) into a very large list (49k)?", "author": "okainov", "createdAt": "2020-09-28T11:46:09Z", "path": "main/src/cgeo/geocaching/filter/AbstractFilter.java", "diffHunk": "@@ -29,13 +29,19 @@ protected AbstractFilter(final Parcel in) {\n \n     @Override\n     public void filter(@NonNull final List<Geocache> list) {\n-        final List<Geocache> itemsToRemove = new ArrayList<>();\n+\n+        //method must be performant when used with very large lists (e.g. 50000 elements) filtered into very short lists (e.g. 30 elements)", "originalCommit": "5466451ec23659876bbc29c87775c318bd682b34", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTkzMjkzNQ==", "url": "https://github.com/cgeo/cgeo/pull/9080#discussion_r495932935", "bodyText": "It is performant in all the cases \ud83d\ude09", "author": "fm-sys", "createdAt": "2020-09-28T13:20:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTg3ODIzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTk2MTEwNg==", "url": "https://github.com/cgeo/cgeo/pull/9080#discussion_r495961106", "bodyText": "Just from the code, existing version must have been faster for my case. Current code is creating the copy-list with almost the same caches leading to some potential memory overhead and then clears and re-adds items again (so feels like ~3x overhead on the initial size of the list).\nPrevious version would have very short itemsToRemove (i.e. 1 element) and then iterate over the existing list once (for 1 element), shrinking it once... I'm not convinced new approach is faster in this case :)", "author": "okainov", "createdAt": "2020-09-28T13:59:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTg3ODIzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTk3MzU4MQ==", "url": "https://github.com/cgeo/cgeo/pull/9080#discussion_r495973581", "bodyText": "@okainov you are right, at least I should have tested this case too in unit tests, and also the comment is misleading.\nIn general I assume filtering large lists into large results should work fine performance-wise since addAll from arraylist to arraylist should be a single arraycopy underneath (performance-wise same thing happening with every single remove-call on such a list). Memory-wise I agree that temporary a second listsl of size 50000 has to be kept in memory, but this only adds 50000 pointers (not deep copies of the cache data) to existing 50000 complete caches stored in memory - so in relation it should not add much to the (huge!) Load of holding so many caches in memory anyway.", "author": "eddiemuc", "createdAt": "2020-09-28T14:16:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTg3ODIzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjE1Nzk5MA==", "url": "https://github.com/cgeo/cgeo/pull/9080#discussion_r496157990", "bodyText": "@okainov I created PR #9090 for this", "author": "eddiemuc", "createdAt": "2020-09-28T18:41:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTg3ODIzMQ=="}], "type": "inlineReview"}]}