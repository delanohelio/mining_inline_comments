{"pr_number": 8728, "pr_title": "CB-8278: Backup using hdfs cli", "pr_createdAt": "2020-08-05T20:53:02Z", "pr_url": "https://github.com/hortonworks/cloudbreak/pull/8728", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE5Mjk5MQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r466192991", "bodyText": "Why is the region removed?", "author": "keyki", "createdAt": "2020-08-06T07:13:38Z", "path": "core/src/main/java/com/sequenceiq/cloudbreak/reactor/handler/cluster/dr/BackupRestoreSaltConfigGenerator.java", "diffHunk": "@@ -34,7 +34,6 @@ public SaltConfig createSaltConfig(String location, String backupId, Stack stack\n \n         Map<String, String> disasterRecoveryValues = new HashMap<>();\n         disasterRecoveryValues.put(OBJECT_STORAGE_URL_KEY, fullLocation);\n-        disasterRecoveryValues.put(AWS_REGION_KEY, stack.getRegion());", "originalCommit": "6e43ad33e768122070f5c6c40dc0950b02664db3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUyNTAxOA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r466525018", "bodyText": "We got the suggestion to use the hdfs cli instead of s3 to upload/download.\nIt was my understanding that we don't have to specify the region in that case, the hdfs configurations should take care of things?", "author": "brycederriso", "createdAt": "2020-08-06T16:10:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE5Mjk5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI4MTQxMQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r467281411", "bodyText": "@keyki  AWS CLI would depend on permissions granted to the instance profile of the datalake node which is currently LOG_ROLE. Grating more permissions to LOG_ROLE was not a good idea.\nWith this change, we will be using hdfs client to write to S3. This uses S3a. All the services which to S3, internally use S3a. With the configuration we are using, S3a fetches the region information of the bucket and then constructs the request accordingly.", "author": "kkalvagadda1", "createdAt": "2020-08-07T21:31:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE5Mjk5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgyNjczOQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r468826739", "bodyText": "I'm resolving this conversation, given that we have a reason and there's been no further concern.", "author": "brycederriso", "createdAt": "2020-08-11T19:52:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE5Mjk5MQ=="}], "type": "inlineReview"}, {"oid": "c33ebfc599ac204d10ff6b28d621d12691197804", "url": "https://github.com/hortonworks/cloudbreak/commit/c33ebfc599ac204d10ff6b28d621d12691197804", "message": "Backup and restore using HDFS cli agains object store.\n\n* Restore db using HDFS cli to transfer backups.\n* Hack together a fix for embedded database backups.\n* Adjust url construction and region passing to account for HDFS cli usage.\n* Use a new kerberos credential cache when kiniting.\n* Remove unused `region` pillar value.", "committedDate": "2020-08-10T20:53:37Z", "type": "forcePushed"}, {"oid": "1bd741d2e5e079087950a42dc2d977b15b3cf04a", "url": "https://github.com/hortonworks/cloudbreak/commit/1bd741d2e5e079087950a42dc2d977b15b3cf04a", "message": "Backup and restore using HDFS cli agains object store.\n\n* Restore db using HDFS cli to transfer backups.\n* Hack together a fix for embedded database backups.\n* Adjust url construction and region passing to account for HDFS cli usage.\n* Use a new kerberos credential cache when kiniting.\n* Remove unused `region` pillar value.", "committedDate": "2020-08-11T14:57:02Z", "type": "forcePushed"}, {"oid": "57f5ee7564cbcfacb75db6210665f0d561e05dd9", "url": "https://github.com/hortonworks/cloudbreak/commit/57f5ee7564cbcfacb75db6210665f0d561e05dd9", "message": "Backup and restore using HDFS cli agains object store.\n\n* Restore db using HDFS cli to transfer backups.\n* Hack together a fix for embedded database backups.\n* Adjust url construction and region passing to account for HDFS cli usage.\n* Use a new kerberos credential cache when kiniting.\n* Remove unused `region` pillar value.\n* Clean up after successful Ranger admin group substitution.", "committedDate": "2020-08-11T19:49:14Z", "type": "forcePushed"}, {"oid": "628aa1910dd13125e40108de5852e23a355f8a5b", "url": "https://github.com/hortonworks/cloudbreak/commit/628aa1910dd13125e40108de5852e23a355f8a5b", "message": "Backup and restore using HDFS cli agains object store.\n\n* Restore db using HDFS cli to transfer backups.\n* Hack together a fix for embedded database backups.\n* Adjust url construction and region passing to account for HDFS cli usage.\n* Use a new kerberos credential cache when kiniting.\n* Remove unused `region` pillar value.\n* Clean up after successful Ranger admin group substitution.", "committedDate": "2020-08-12T21:27:32Z", "type": "commit"}, {"oid": "628aa1910dd13125e40108de5852e23a355f8a5b", "url": "https://github.com/hortonworks/cloudbreak/commit/628aa1910dd13125e40108de5852e23a355f8a5b", "message": "Backup and restore using HDFS cli agains object store.\n\n* Restore db using HDFS cli to transfer backups.\n* Hack together a fix for embedded database backups.\n* Adjust url construction and region passing to account for HDFS cli usage.\n* Use a new kerberos credential cache when kiniting.\n* Remove unused `region` pillar value.\n* Clean up after successful Ranger admin group substitution.", "committedDate": "2020-08-12T21:27:32Z", "type": "forcePushed"}]}