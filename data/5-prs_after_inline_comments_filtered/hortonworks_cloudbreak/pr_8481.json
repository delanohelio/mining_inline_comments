{"pr_number": 8481, "pr_title": "CB-7798 tests has failed with a generic \"Cloudera Manager [Template i\u2026", "pr_createdAt": "2020-07-02T14:49:57Z", "pr_url": "https://github.com/hortonworks/cloudbreak/pull/8481", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTEyNTkzOA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8481#discussion_r449125938", "bodyText": "this is used in one place, maybe we should consider passing it as parameter", "author": "pdarvasi", "createdAt": "2020-07-02T16:12:53Z", "path": "cluster-cm/src/main/java/com/sequenceiq/cloudbreak/cm/polling/ClouderaManagerPollingServiceProvider.java", "diffHunk": "@@ -131,60 +135,61 @@ public PollingResult startPollingCmHostDecommissioning(Stack stack, ApiClient ap\n \n     public PollingResult startPollingCmManagementServiceStartup(Stack stack, ApiClient apiClient, BigDecimal commandId) {\n         LOGGER.debug(\"Waiting for Cloudera Manager to start management service. [Server address: {}]\", stack.getClusterManagerIp());\n-        return pollCommandWithTimeListener(stack, apiClient, commandId, POLL_ATTEMPTS_FOUR_HOURS,\n+        return pollCommandWithTimeListener(stack, apiClient, commandId, POLL_FOR_ONE_HOUR,\n                 new ClouderaManagerStartManagementServiceListenerTask(clouderaManagerApiPojoFactory, cloudbreakEventService));\n     }\n \n     public PollingResult startPollingCmManagementServiceShutdown(Stack stack, ApiClient apiClient, BigDecimal commandId) {\n         LOGGER.debug(\"Waiting for Cloudera Manager to stop management service. [Server address: {}]\", stack.getClusterManagerIp());\n-        return pollCommandWithTimeListener(stack, apiClient, commandId, POLL_ATTEMPTS_FOUR_HOURS,\n+        return pollCommandWithTimeListener(stack, apiClient, commandId, POLL_FOR_ONE_HOUR,\n                 new ClouderaManagerStopManagementServiceListenerTask(clouderaManagerApiPojoFactory, cloudbreakEventService));\n     }\n \n     public PollingResult startPollingCmServicesRestart(Stack stack, ApiClient apiClient, BigDecimal commandId) {\n         LOGGER.debug(\"Waiting for Cloudera Manager to restart services. [Server address: {}]\", stack.getClusterManagerIp());\n-        return pollCommandWithTimeListener(stack, apiClient, commandId, POLL_ATTEMPTS_FOUR_HOURS,\n+        return pollCommandWithTimeListener(stack, apiClient, commandId, POLL_FOR_ONE_HOUR,\n                 new ClouderaManagerRestartServicesListenerTask(clouderaManagerApiPojoFactory, cloudbreakEventService));\n     }\n \n     public PollingResult startPollingParcelsApiAvailable(Stack stack, ApiClient apiClient) {\n         LOGGER.debug(\"Waiting for Parcels API to become available. [Server address: {}]\", stack.getClusterManagerIp());\n-        return pollCommandWithTimeListener(stack, apiClient, null, POLL_ATTEMPTS_FOUR_HOURS,\n+        return pollCommandWithTimeListener(stack, apiClient, null, POLL_FOR_ONE_HOUR,\n                 new ClouderaManagerParcelsApiListenerTask(clouderaManagerApiPojoFactory, cloudbreakEventService));\n     }\n \n     public PollingResult startPollingCdpRuntimeUpgrade(Stack stack, ApiClient apiClient, BigDecimal commandId) {\n         LOGGER.debug(\"Waiting for Cloudera Manager to upgrade CDP Runtime services. [Server address: {}]\", stack.getClusterManagerIp());\n-        return pollCommandWithTimeListener(stack, apiClient, commandId, POLL_ATTEMPTS_FOUR_HOURS,\n+        return pollCommandWithTimeListener(stack, apiClient, commandId, POLL_FOR_ONE_HOUR,\n                 new ClouderaManagerUpgradeRuntimeListenerTask(clouderaManagerApiPojoFactory, cloudbreakEventService));\n     }\n \n     public PollingResult startPollingCdpRuntimeParcelDownload(Stack stack, ApiClient apiClient, BigDecimal commandId, ParcelResource parcelResource) {\n         LOGGER.debug(\"Waiting for Cloudera Manager to download CDP Runtime Parcel. [Server address: {}]\", stack.getClusterManagerIp());\n-        return pollCommandWithTimeListener(stack, apiClient, commandId, POLL_ATTEMPTS_FOUR_HOURS,\n+        return pollCommandWithTimeListener(stack, apiClient, commandId, POLL_FOR_ONE_HOUR,\n                 new ClouderaManagerUpgradeParcelDownloadListenerTask(clouderaManagerApiPojoFactory, cloudbreakEventService, parcelResource));\n     }\n \n     public PollingResult startPollingCdpRuntimeParcelDistribute(Stack stack, ApiClient apiClient, BigDecimal commandId, ParcelResource parcelResource) {\n         LOGGER.debug(\"Waiting for Cloudera Manager to distribute CDP Runtime Parcel. [Server address: {}]\", stack.getClusterManagerIp());\n-        return pollCommandWithTimeListener(stack, apiClient, commandId, POLL_ATTEMPTS_FOUR_HOURS,\n+        return pollCommandWithTimeListener(stack, apiClient, commandId, POLL_FOR_ONE_HOUR,\n                 new ClouderaManagerUpgradeParcelDistributeListenerTask(clouderaManagerApiPojoFactory, cloudbreakEventService, parcelResource));\n     }\n \n     public PollingResult startPollingCmGenerateCredentials(Stack stack, ApiClient apiClient, BigDecimal commandId) {\n         LOGGER.debug(\"Waiting for Cloudera Manager to finish generate credentials. [Server address: {}]\", stack.getClusterManagerIp());\n-        return pollCommandWithTimeListener(stack, apiClient, commandId, POLL_ATTEMPTS_FOUR_HOURS,\n+        return pollCommandWithTimeListener(stack, apiClient, commandId, POLL_FOR_ONE_HOUR,\n                 new ClouderaManagerGenerateCredentialsListenerTask(clouderaManagerApiPojoFactory, cloudbreakEventService));\n     }\n \n-    private PollingResult pollCommandWithTimeListener(Stack stack, ApiClient apiClient, BigDecimal commandId, int numAttempts,\n+    private PollingResult pollCommandWithTimeListener(Stack stack, ApiClient apiClient, BigDecimal commandId, long maximumWaitTimeInSeconds,\n             AbstractClouderaManagerCommandCheckerTask<ClouderaManagerPollerObject> listenerTask) {\n         ClouderaManagerPollerObject clouderaManagerPollerObject = new ClouderaManagerPollerObject(stack, apiClient, commandId);\n-        return clouderaManagerCommandPollerObjectPollingService.pollWithTimeoutSingleFailure(\n+        return clouderaManagerCommandPollerObjectPollingService.pollWithAbsoluteTimeout(\n                 listenerTask,\n                 clouderaManagerPollerObject,\n                 POLL_INTERVAL,\n-                numAttempts);\n+                maximumWaitTimeInSeconds,\n+                TOLERATE_FIVE_CONSECUTIVE_ERRORS).getLeft();", "originalCommit": "0990989898d838a1143a638c4ee93c08dd81d95d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQ5MTQ0Mg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8481#discussion_r449491442", "bodyText": "Thanks", "author": "akanto", "createdAt": "2020-07-03T09:48:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTEyNTkzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTEyNjM0NQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8481#discussion_r449126345", "bodyText": "typo: resived ->  received", "author": "pdarvasi", "createdAt": "2020-07-02T16:13:33Z", "path": "cloud-common/src/main/java/com/sequenceiq/cloudbreak/polling/PollingService.java", "diffHunk": "@@ -21,37 +21,43 @@\n      * @param interval    sleeps this many milliseconds between status checking attempts\n      * @param maxAttempts signals how many times will the status check be executed before timeout\n      */\n-    public Pair<PollingResult, Exception> pollWithTimeout(StatusCheckerTask<T> statusCheckerTask, T t, long interval, int maxAttempts, int maxFailure) {\n-        return pollWithTimeout(statusCheckerTask, t, interval, new AttemptBasedTimeoutChecker(maxAttempts), maxFailure);\n+    public Pair<PollingResult, Exception> pollWithTimeout(StatusCheckerTask<T> statusCheckerTask, T t, long interval,\n+            int maxAttempts, int maxConsecutiveFailures) {\n+        return pollWithTimeout(statusCheckerTask, t, interval, new AttemptBasedTimeoutChecker(maxAttempts), maxConsecutiveFailures);\n     }\n \n-    public Pair<PollingResult, Exception> pollWithAbsoluteTimeout(StatusCheckerTask<T> statusCheckerTask, T t, long interval, long waitSec, int maxFailure) {\n-        return pollWithTimeout(statusCheckerTask, t, interval, new AbsolutTimeBasedTimeoutChecker(waitSec), maxFailure);\n+    public Pair<PollingResult, Exception> pollWithAbsoluteTimeout(StatusCheckerTask<T> statusCheckerTask, T t, long interval,\n+            long maximumWaitTimeInSeconds, int maxConsecutiveFailures) {\n+        return pollWithTimeout(statusCheckerTask, t, interval, new AbsolutTimeBasedTimeoutChecker(maximumWaitTimeInSeconds), maxConsecutiveFailures);\n     }\n \n     public Pair<PollingResult, Exception> pollWithTimeout(StatusCheckerTask<T> statusCheckerTask, T t, long interval, TimeoutChecker timeoutChecker,\n-            int maxFailure) {\n+            int maxConsecutiveFailures) {\n         boolean success = false;\n         boolean timeout = false;\n         int attempts = 0;\n-        int failures = 0;\n+        int consecutiveFailures = 0;\n         Exception actual = null;\n         boolean exit = statusCheckerTask.exitPolling(t);\n         while (!timeout && !exit) {\n             LOGGER.debug(\"Polling attempt {}.\", attempts);\n             try {\n                 success = statusCheckerTask.checkStatus(t);\n             } catch (Exception ex) {\n-                LOGGER.debug(\"Exception occurred in the polling: {}\", ex.getMessage(), ex);\n-                failures++;\n+                consecutiveFailures++;\n                 actual = ex;\n+                LOGGER.debug(\"Exception occurred in the polling: {}. Number of consecutive failures: [{}/{}]\",\n+                        ex.getMessage(), consecutiveFailures, maxConsecutiveFailures, ex);\n             }\n-            if (failures >= maxFailure) {\n-                LOGGER.debug(\"Polling failure reached the limit which was {}, poller will drop the last exception.\", maxFailure);\n+            if (consecutiveFailures >= maxConsecutiveFailures) {\n+                LOGGER.debug(\"Polling failure reached the limit which was {}, poller will drop the last exception.\", maxConsecutiveFailures);\n                 statusCheckerTask.handleException(actual);\n                 return new ImmutablePair<>(PollingResult.FAILURE, actual);\n             } else if (success) {\n                 LOGGER.debug(statusCheckerTask.successMessage(t));\n+                LOGGER.debug(\"Set the number of consecutive failures to 0, since we resived a positve answer. Original number of consecutiveFailures: {}\",", "originalCommit": "0990989898d838a1143a638c4ee93c08dd81d95d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQ5MTUyNA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8481#discussion_r449491524", "bodyText": "Thanks.", "author": "akanto", "createdAt": "2020-07-03T09:48:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTEyNjM0NQ=="}], "type": "inlineReview"}, {"oid": "334ff862bc5b0f80c140b3b064c42fee11a36e41", "url": "https://github.com/hortonworks/cloudbreak/commit/334ff862bc5b0f80c140b3b064c42fee11a36e41", "message": "CB-7798 tests has failed with a generic \"Cloudera Manager [Template install] operation failed\" error message. The root cause of this problem is that CM API responded with Bad Request while we have polled the progress of the CDP cluster install. This is a CM issue, since the request was not \"bad\", but it was an internal server error at CM side. The poller was configured in that way, that even a single failure during the poll is fatal and we abort the polling and say that cluster install has failed. Of course in the background CM has continued the cluster install and eventually it was successful. The glitch has caused by a network issue between CM and Azure DB.\n\nWhat I have changed:\n- I have realized that the absolute polling time was calculated incorrectly, the constant said that it poller will poll for 4 hours, but in reality it polled for 2880 seconds which is slightly less than a hour. I have changed the polling interval to 1 hour\n- I have introduced a consecutive error counter into the poller, which means that if something fails with an error multiple times consecutivly then we give up, but if it works then we will reset the error counter, and consider that problem as a temporary problem\n- As a workaround I have introduced a retry for bad request as well. it is not ideal, beacuse if we send a bad request then we will resend it multiple times, but there is no way to distingush between the issues when CM has a probem or when we just send a \"bad\" request, so I think this might be an acceptable workaround.", "committedDate": "2020-07-03T08:57:42Z", "type": "commit"}, {"oid": "6b9e92da030aa98590212cce7aeeade48c8d3a91", "url": "https://github.com/hortonworks/cloudbreak/commit/6b9e92da030aa98590212cce7aeeade48c8d3a91", "message": "Commit messages are often short, and does not explain why that commit was done.", "committedDate": "2020-07-03T09:06:55Z", "type": "commit"}, {"oid": "6b9e92da030aa98590212cce7aeeade48c8d3a91", "url": "https://github.com/hortonworks/cloudbreak/commit/6b9e92da030aa98590212cce7aeeade48c8d3a91", "message": "Commit messages are often short, and does not explain why that commit was done.", "committedDate": "2020-07-03T09:06:55Z", "type": "forcePushed"}]}