{"pr_number": 4303, "pr_title": "DB-9924 NSDS v2 Streaming Performance", "pr_createdAt": "2020-10-15T15:23:48Z", "pr_url": "https://github.com/splicemachine/spliceengine/pull/4303", "timeline": [{"oid": "503b07c72096ce9da2b2b199b60f20d7ccf67f2d", "url": "https://github.com/splicemachine/spliceengine/commit/503b07c72096ce9da2b2b199b60f20d7ccf67f2d", "message": "DB-9924 Performance tuning.", "committedDate": "2020-08-07T00:44:31Z", "type": "commit"}, {"oid": "a32d004fe21667a20bc7feaa45c597626807cd95", "url": "https://github.com/splicemachine/spliceengine/commit/a32d004fe21667a20bc7feaa45c597626807cd95", "message": "DB-9924 Shutting down Zookeeper last helps Kafka restart reliably more often.", "committedDate": "2020-08-07T00:46:47Z", "type": "commit"}, {"oid": "d4954ab5e846469a054bca0b02d18634f5f1687c", "url": "https://github.com/splicemachine/spliceengine/commit/d4954ab5e846469a054bca0b02d18634f5f1687c", "message": "DB-9924 Enable kafka topic creation with num partitions and replication factor. Random partition assignment when sending data to Kafka.", "committedDate": "2020-08-07T00:52:21Z", "type": "commit"}, {"oid": "e67ce530cd65e189a224ab5693e2ef172c74d1d8", "url": "https://github.com/splicemachine/spliceengine/commit/e67ce530cd65e189a224ab5693e2ef172c74d1d8", "message": "DB-9924 Added sleep to ensure topics are created and ready.", "committedDate": "2020-08-15T00:31:26Z", "type": "commit"}, {"oid": "3bad6f2d865739646547280930e769bd60983b52", "url": "https://github.com/splicemachine/spliceengine/commit/3bad6f2d865739646547280930e769bd60983b52", "message": "DB-9924 Changed random partition assignment to default in kafka producer.", "committedDate": "2020-08-15T00:34:10Z", "type": "commit"}, {"oid": "9b559ee23f37fb504fb37af8733497b312332ae7", "url": "https://github.com/splicemachine/spliceengine/commit/9b559ee23f37fb504fb37af8733497b312332ae7", "message": "DB-9924 Latest NSDS updates.", "committedDate": "2020-09-09T19:14:58Z", "type": "commit"}, {"oid": "09d48fbc73d8073239ed81a381c1c462980f52c9", "url": "https://github.com/splicemachine/spliceengine/commit/09d48fbc73d8073239ed81a381c1c462980f52c9", "message": "DB-9924 Latest NSDS updates.", "committedDate": "2020-09-16T16:40:02Z", "type": "commit"}, {"oid": "1b7781042b1eac3978e035a3ff61e7ccd522f2bb", "url": "https://github.com/splicemachine/spliceengine/commit/1b7781042b1eac3978e035a3ff61e7ccd522f2bb", "message": "DB-9924 Added end of batch marker when passing data from nsds to the DB.", "committedDate": "2020-09-17T16:29:09Z", "type": "commit"}, {"oid": "29c861da0263fdf28b8efba4f5b26b51ca22bf59", "url": "https://github.com/splicemachine/spliceengine/commit/29c861da0263fdf28b8efba4f5b26b51ca22bf59", "message": "DB-9924 Latest update for NSDS.", "committedDate": "2020-09-18T14:37:03Z", "type": "commit"}, {"oid": "6460c6705de54874f07b4923f04e39bb58d91f7f", "url": "https://github.com/splicemachine/spliceengine/commit/6460c6705de54874f07b4923f04e39bb58d91f7f", "message": "DB-9924 NSDS v2 latest updates.", "committedDate": "2020-09-24T17:39:57Z", "type": "commit"}, {"oid": "87597157af221ac7613a1472913352239fecaf10", "url": "https://github.com/splicemachine/spliceengine/commit/87597157af221ac7613a1472913352239fecaf10", "message": "DB-9924 NSDS v2 Added kryo serialization.", "committedDate": "2020-10-15T08:40:41Z", "type": "commit"}, {"oid": "e8304c61d5723be2181b8b20a48b9a393032cc40", "url": "https://github.com/splicemachine/spliceengine/commit/e8304c61d5723be2181b8b20a48b9a393032cc40", "message": "DB-9924 NSDS v2 Added kryo serialization.", "committedDate": "2020-10-15T09:10:02Z", "type": "commit"}, {"oid": "e668872b8a18b12903a04f001db83cd3e75898ef", "url": "https://github.com/splicemachine/spliceengine/commit/e668872b8a18b12903a04f001db83cd3e75898ef", "message": "Merge branch 'master' into DB-9924", "committedDate": "2020-10-15T10:41:54Z", "type": "commit"}, {"oid": "08e8c53eea34e7b7116af18293f24fe0f59d75db", "url": "https://github.com/splicemachine/spliceengine/commit/08e8c53eea34e7b7116af18293f24fe0f59d75db", "message": "DB-9924 NSDS v2 copy latest updates to spark3.0", "committedDate": "2020-10-15T11:02:08Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc2ODMzOA==", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r506768338", "bodyText": "Is this our naming convention for kafka partition?", "author": "jyuanca", "createdAt": "2020-10-17T00:07:23Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -1268,13 +1268,29 @@ public void decrementOpDepth() {\n         props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());\n         props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ExternalizableDeserializer.class.getName());\n \n-        KafkaConsumer<Integer, Externalizable> consumer = new KafkaConsumer<Integer, Externalizable>(props);\n-        List ps = consumer.partitionsFor(topicName);\n-        List<Integer> partitions = new ArrayList<>(ps.size());\n-        for (int i = 0; i < ps.size(); ++i) {\n-            partitions.add(i);\n+        List<Integer> partitions;\n+        String topicName;", "originalCommit": "08e8c53eea34e7b7116af18293f24fe0f59d75db", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTM4OTMxOQ==", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r509389319", "bodyText": "Often the first batch received by SSDS at the beginning of a data run does not have data in every partition.  In this case, to the topic name SSDS appends :: followed by a comma-separated list of the partition numbers that have data, so that the consumer in the DB will not poll empty partitions, which results in long timeouts.\nKafka's convention for a partition name is topic name hyphen partition number.", "author": "jpanko1", "createdAt": "2020-10-21T15:32:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc2ODMzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM0MTA0OQ==", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r508341049", "bodyText": "What happens when we reach 10 retries? It looks like we silently drop data.", "author": "dgomezferro", "createdAt": "2020-10-20T09:16:08Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/KafkaReadFunction.java", "diffHunk": "@@ -67,59 +69,99 @@ public KafkaReadFunction(OperationContext context, String topicName, String boot\n         props.put(ConsumerConfig.CLIENT_ID_CONFIG, consumer_id);\n \n         props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());\n-        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ExternalizableDeserializer.class.getName());\n+        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n         props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+        \n+        // MAX_POLL_RECORDS_CONFIG helped performance in standalone with lower values (default == 500).\n+        //  With high values, it spent too much time retrieving records from Kafka.\n+        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, \"100\");\n+//        props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, \"10485760\"); // 10% of max == 5242880, default == 1048576\n \n-        KafkaConsumer<Integer, Externalizable> consumer = new KafkaConsumer<Integer, Externalizable>(props);\n+        KafkaConsumer<Integer, byte[]> consumer = new KafkaConsumer<Integer, byte[]>(props);\n         consumer.assign(Arrays.asList(new TopicPartition(topicName, partition)));\n \n+        KryoSerialization kryo = new KryoSerialization();\n+        kryo.init();\n+\n         return new Iterator<ExecRow>() {\n-            Iterator<ConsumerRecord<Integer, Externalizable>> it = null;\n-            \n-            Predicate<ConsumerRecords<Integer, Externalizable>> noRecords = records ->\n+            Iterator<ConsumerRecord<Integer, byte[]>> it = null;\n+            Message prevMessage = new Message();\n+\n+            Predicate<ConsumerRecords<Integer, byte[]>> noRecords = records ->\n                 records == null || records.isEmpty();\n             \n-            private ConsumerRecords<Integer, Externalizable> kafkaRecords(int maxAttempts) throws TaskKilledException {\n+            long totalCount = 0L;\n+            int maxRetries = 10;", "originalCommit": "08e8c53eea34e7b7116af18293f24fe0f59d75db", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQxMDkyMg==", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r509410922", "bodyText": "Each retry has a 1 minute timeout, so after 10 minutes of retrying, if it hasn't received more data within a batch, it stops polling for data and will LOG.error(id + \" KRF.call Didn't get full batch after \" + retries + \" retries, got \" + totalCount + \" records\");.  I've seen data within a batch delayed for several seconds but not as long as minutes, so normally 10 minutes will allow picking up the rest of the data.  In a failure scenario in which the upstream code is unable to send the end of a batch, without the eventual timeout this code will be in an infinite loop tying up a thread / spark task.", "author": "jpanko1", "createdAt": "2020-10-21T16:01:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM0MTA0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzYwMzYwMA==", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r513603600", "bodyText": "The problem is we are not failing the operation (as far as I can see). So after 10 minutes we complete the write without error. I think we should have a shorter timeout (1 minute?) and raise an exception.", "author": "dgomezferro", "createdAt": "2020-10-28T16:50:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM0MTA0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDMwNDYyNA==", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r514304624", "bodyText": "Added exception and shortened timeout in commit 063e869", "author": "jpanko1", "createdAt": "2020-10-29T14:31:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM0MTA0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM0NTQzMw==", "url": "https://github.com/splicemachine/spliceengine/pull/4303#discussion_r508345433", "bodyText": "Serializing ValueRows directly with Kryo still has quite a bit of overhead (we are serializing the schema with each row, for instance). I opened https://splicemachine.atlassian.net/browse/DB-10543 to track this improvement", "author": "dgomezferro", "createdAt": "2020-10-20T09:22:32Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/KafkaReadFunction.java", "diffHunk": "@@ -67,59 +69,99 @@ public KafkaReadFunction(OperationContext context, String topicName, String boot\n         props.put(ConsumerConfig.CLIENT_ID_CONFIG, consumer_id);\n \n         props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());\n-        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ExternalizableDeserializer.class.getName());\n+        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n         props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+        \n+        // MAX_POLL_RECORDS_CONFIG helped performance in standalone with lower values (default == 500).\n+        //  With high values, it spent too much time retrieving records from Kafka.\n+        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, \"100\");\n+//        props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, \"10485760\"); // 10% of max == 5242880, default == 1048576\n \n-        KafkaConsumer<Integer, Externalizable> consumer = new KafkaConsumer<Integer, Externalizable>(props);\n+        KafkaConsumer<Integer, byte[]> consumer = new KafkaConsumer<Integer, byte[]>(props);\n         consumer.assign(Arrays.asList(new TopicPartition(topicName, partition)));\n \n+        KryoSerialization kryo = new KryoSerialization();\n+        kryo.init();\n+\n         return new Iterator<ExecRow>() {\n-            Iterator<ConsumerRecord<Integer, Externalizable>> it = null;\n-            \n-            Predicate<ConsumerRecords<Integer, Externalizable>> noRecords = records ->\n+            Iterator<ConsumerRecord<Integer, byte[]>> it = null;\n+            Message prevMessage = new Message();\n+\n+            Predicate<ConsumerRecords<Integer, byte[]>> noRecords = records ->\n                 records == null || records.isEmpty();\n             \n-            private ConsumerRecords<Integer, Externalizable> kafkaRecords(int maxAttempts) throws TaskKilledException {\n+            long totalCount = 0L;\n+            int maxRetries = 10;\n+            int retries = 0;\n+            final Duration shortTimeout = java.time.Duration.ofMillis(500L);\n+            final Duration longTimeout = java.time.Duration.ofMinutes(1L);\n+            \n+            private ConsumerRecords<Integer, byte[]> kafkaRecords(int maxAttempts, Duration timeout) throws TaskKilledException {\n                 int attempt = 1;\n-                ConsumerRecords<Integer, Externalizable> records = null;\n+                ConsumerRecords<Integer, byte[]> records = null;\n                 do {\n-                    records = consumer.poll(java.time.Duration.ofMillis(1000));\n+                    records = consumer.poll(timeout);\n                     if (TaskContext.get().isInterrupted()) {\n-                        consumer.close();\n+                        LOG.warn( id+\" KRF.call kafkaRecords Spark TaskContext Interrupted\");\n+                        //consumer.close();\n                         throw new TaskKilledException();\n                     }\n                 } while( noRecords.test(records) && attempt++ < maxAttempts );\n                 \n                 return records;\n             }\n             \n-            private boolean hasMoreRecords(int maxAttempts) throws TaskKilledException {\n-                ConsumerRecords<Integer, Externalizable> records = kafkaRecords(maxAttempts);\n+            private boolean hasMoreRecords(int maxAttempts, Duration timeout) throws TaskKilledException {\n+                ConsumerRecords<Integer, byte[]> records = kafkaRecords(maxAttempts, timeout);\n                 if( noRecords.test(records) ) {\n-                    consumer.close();\n+                    if( !prevMessage.last() && retries < maxRetries ) {\n+                        retries++;\n+                        Duration retryTimeout = longTimeout;\n+                        LOG.warn( id+\" KRF.call Missed rcds, got \"+totalCount+\" retry \"+retries+\" for up to \"+retryTimeout );\n+                        return hasMoreRecords(\n+                            maxAttempts,\n+                            retryTimeout\n+                        );\n+                    }\n+                    //consumer.close();\n+                    if( !prevMessage.last() ) {\n+                        LOG.error(id + \" KRF.call Didn't get full batch after \" + retries + \" retries, got \" + totalCount + \" records\");\n+                    }\n                     return false;\n                 } else {\n+                    int ct = records.count();\n+                    totalCount += ct;\n+                    LOG.trace( id+\" KRF.call p \"+partition+\" t \"+topicName+\" records \"+ct );\n+                    retries = 0;\n+                    \n                     it = records.iterator();\n                     return it.hasNext();\n                 }\n             }\n-\n+            \n             @Override\n             public boolean hasNext() {\n-                if (it == null) {\n-                    return hasMoreRecords(60);\n-                }\n-                if (it.hasNext()) {\n-                    return true;\n+                boolean more = false;\n+                \n+                if (it != null && it.hasNext()) {\n+                    more = true;\n+                } else if (!prevMessage.last()) {\n+                    more = hasMoreRecords(1, shortTimeout);\n                 }\n-                else {\n-                    return hasMoreRecords(1);\n+\n+                if (!more) {\n+                    consumer.close();\n+                    kryo.close();\n                 }\n+\n+                return more;\n             }\n \n             @Override\n             public ExecRow next() {\n-                return (ExecRow)it.next().value();\n+                Message m = (Message)kryo.deserialize( it.next().value() );", "originalCommit": "08e8c53eea34e7b7116af18293f24fe0f59d75db", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "bf69fd0c65e44b224c6777a872c38cde5c19b3c4", "url": "https://github.com/splicemachine/spliceengine/commit/bf69fd0c65e44b224c6777a872c38cde5c19b3c4", "message": "Merge branch 'master' into DB-9924", "committedDate": "2020-10-20T22:19:13Z", "type": "commit"}, {"oid": "94f13b7a9ad85ce06d28fe8e783a87ef3ab9f5a6", "url": "https://github.com/splicemachine/spliceengine/commit/94f13b7a9ad85ce06d28fe8e783a87ef3ab9f5a6", "message": "DB-9924 Remove unnecessary comment.", "committedDate": "2020-10-21T19:13:51Z", "type": "commit"}, {"oid": "0263ed86b9c614882b6cde88a47c3caf87ca7027", "url": "https://github.com/splicemachine/spliceengine/commit/0263ed86b9c614882b6cde88a47c3caf87ca7027", "message": "DB-9924 Change printlns to logs. Align kafka partition count to that of the input rdd. Limit last row accumulation in sendData to streaming.", "committedDate": "2020-10-21T19:20:19Z", "type": "commit"}, {"oid": "4997a486eb3fabc2872718be5cf427bd5af784e3", "url": "https://github.com/splicemachine/spliceengine/commit/4997a486eb3fabc2872718be5cf427bd5af784e3", "message": "DB-9924 NSDS v2 Added comments in sendData retry logic.", "committedDate": "2020-10-21T22:49:13Z", "type": "commit"}, {"oid": "7885afdc7924c3ef628140113ea0199336f450d1", "url": "https://github.com/splicemachine/spliceengine/commit/7885afdc7924c3ef628140113ea0199336f450d1", "message": "DB-9924 Fix Spotbugs.", "committedDate": "2020-10-22T04:11:06Z", "type": "commit"}, {"oid": "01f1e76fc165e188fafad4fac71d4a128ff8f46b", "url": "https://github.com/splicemachine/spliceengine/commit/01f1e76fc165e188fafad4fac71d4a128ff8f46b", "message": "DB-9924 Moved creation of AdminClient.", "committedDate": "2020-10-23T04:16:54Z", "type": "commit"}, {"oid": "da8552190acc7bc107ef2b7f9de49c4430c948ad", "url": "https://github.com/splicemachine/spliceengine/commit/da8552190acc7bc107ef2b7f9de49c4430c948ad", "message": "DB-9924 NSDS v2 Added logic to handle empty partitions and RDDs with no partitions.", "committedDate": "2020-10-23T04:24:02Z", "type": "commit"}, {"oid": "c47077b8db5b743454ad520f9291a57e4688403a", "url": "https://github.com/splicemachine/spliceengine/commit/c47077b8db5b743454ad520f9291a57e4688403a", "message": "DB-9924 NSDS v2 Returning partition info from sendData_streaming. Made two utility functions public. Migrated other recent changes from spark2.4 code to the code for other spark versions.", "committedDate": "2020-10-28T07:06:23Z", "type": "commit"}, {"oid": "4815d2c9140af470edcea230e8c0cd8bbf5d3a8d", "url": "https://github.com/splicemachine/spliceengine/commit/4815d2c9140af470edcea230e8c0cd8bbf5d3a8d", "message": "Merge branch 'master' into DB-9924", "committedDate": "2020-10-28T15:24:09Z", "type": "commit"}, {"oid": "063e869b22c07e62976c7e10a8b969397010b898", "url": "https://github.com/splicemachine/spliceengine/commit/063e869b22c07e62976c7e10a8b969397010b898", "message": "DB-9924 Throwing exception when timing out during record retrieval from Kafka. Decreased long timeout.", "committedDate": "2020-10-29T03:40:05Z", "type": "commit"}, {"oid": "776fe353b07dc2d50d8381ec3e2fde2ff350510a", "url": "https://github.com/splicemachine/spliceengine/commit/776fe353b07dc2d50d8381ec3e2fde2ff350510a", "message": "Merge branch 'master' into DB-9924", "committedDate": "2020-10-29T17:06:16Z", "type": "commit"}]}