{"pr_number": 3391, "pr_title": "DB-8627 eNSDS", "pr_createdAt": "2020-04-07T03:41:14Z", "pr_url": "https://github.com/splicemachine/spliceengine/pull/3391", "timeline": [{"oid": "595756a5e3d7d2d72c5f4db3d2a72cdb1caa28f6", "url": "https://github.com/splicemachine/spliceengine/commit/595756a5e3d7d2d72c5f4db3d2a72cdb1caa28f6", "message": "eNSDS implemented Export_Kafka and KafkaVTI", "committedDate": "2020-02-27T14:57:11Z", "type": "commit"}, {"oid": "c5d0f45dd67b784b4edc0596c8ea8509075693f0", "url": "https://github.com/splicemachine/spliceengine/commit/c5d0f45dd67b784b4edc0596c8ea8509075693f0", "message": "Added Kafka path to HBase classpath prepend.", "committedDate": "2020-03-13T20:35:00Z", "type": "commit"}, {"oid": "cea941b2585a2d8b5150aab4925ff667534fd4b4", "url": "https://github.com/splicemachine/spliceengine/commit/cea941b2585a2d8b5150aab4925ff667534fd4b4", "message": "Updated Kafka props.", "committedDate": "2020-03-13T20:39:00Z", "type": "commit"}, {"oid": "dd57567fc3a5d526b0286f7e9797e569210a32e8", "url": "https://github.com/splicemachine/spliceengine/commit/dd57567fc3a5d526b0286f7e9797e569210a32e8", "message": "Added spark modules in profile cdh5.13.3.", "committedDate": "2020-03-13T20:40:04Z", "type": "commit"}, {"oid": "3d88a5c322f60a36ad643bf43b74044d7c17400d", "url": "https://github.com/splicemachine/spliceengine/commit/3d88a5c322f60a36ad643bf43b74044d7c17400d", "message": "Updates for NSDS to use Kafka.", "committedDate": "2020-03-13T20:42:24Z", "type": "commit"}, {"oid": "31806e485b3616f21e134957d4c7d930045872f1", "url": "https://github.com/splicemachine/spliceengine/commit/31806e485b3616f21e134957d4c7d930045872f1", "message": "Added spark_sql module in profile cdh5.14.0", "committedDate": "2020-03-13T22:26:08Z", "type": "commit"}, {"oid": "c0e9cb888381ef348bb22c71f4df88c5cd10b014", "url": "https://github.com/splicemachine/spliceengine/commit/c0e9cb888381ef348bb22c71f4df88c5cd10b014", "message": "Handling timeouts and queries that return no rows.", "committedDate": "2020-03-14T20:52:29Z", "type": "commit"}, {"oid": "30100d56e5feb796bdcc6af1a5663611d29b66e6", "url": "https://github.com/splicemachine/spliceengine/commit/30100d56e5feb796bdcc6af1a5663611d29b66e6", "message": "DB-8627 Added internalRdd. In df, added support for where clause.", "committedDate": "2020-03-14T20:55:46Z", "type": "commit"}, {"oid": "9f2cb98fb96d19cadb8166503f95f0ce123bc686", "url": "https://github.com/splicemachine/spliceengine/commit/9f2cb98fb96d19cadb8166503f95f0ce123bc686", "message": "Added functions bulkImportHFile and splitAndInsert. Changed hard-coded values to parameters.", "committedDate": "2020-03-16T00:36:58Z", "type": "commit"}, {"oid": "599c1b1955c03ff6c9e2578bdfee69a2d640e245", "url": "https://github.com/splicemachine/spliceengine/commit/599c1b1955c03ff6c9e2578bdfee69a2d640e245", "message": "Restored constructor that accept only the JDBC url.", "committedDate": "2020-03-16T23:57:43Z", "type": "commit"}, {"oid": "b3651d51385c8c06bac1583adef7d9920e09b426", "url": "https://github.com/splicemachine/spliceengine/commit/b3651d51385c8c06bac1583adef7d9920e09b426", "message": "Added config param for Kafka Bootstrap Servers.", "committedDate": "2020-03-17T00:11:15Z", "type": "commit"}, {"oid": "f89a72878e00689d031061d975984368dcf042cc", "url": "https://github.com/splicemachine/spliceengine/commit/f89a72878e00689d031061d975984368dcf042cc", "message": "Merge eNSDS with 3.0", "committedDate": "2020-03-18T02:16:08Z", "type": "commit"}, {"oid": "19bb290440b743930f5ed83bcf37f34c877bef9e", "url": "https://github.com/splicemachine/spliceengine/commit/19bb290440b743930f5ed83bcf37f34c877bef9e", "message": "Merge eNSDS with 3.0 continued", "committedDate": "2020-03-18T22:50:51Z", "type": "commit"}, {"oid": "58fd1bae09927ef36a902e2ffea77312b30cec4b", "url": "https://github.com/splicemachine/spliceengine/commit/58fd1bae09927ef36a902e2ffea77312b30cec4b", "message": "Removed a duplicate property.", "committedDate": "2020-03-18T22:51:41Z", "type": "commit"}, {"oid": "2701474f796bb0627d0033d879db53227cd17615", "url": "https://github.com/splicemachine/spliceengine/commit/2701474f796bb0627d0033d879db53227cd17615", "message": "Merge eNSDS with 3.0 continued 2", "committedDate": "2020-03-18T23:15:08Z", "type": "commit"}, {"oid": "71510db7548933f520a1f0e7f3a36629138f28f3", "url": "https://github.com/splicemachine/spliceengine/commit/71510db7548933f520a1f0e7f3a36629138f28f3", "message": "Adding eNSDS code for spark2.4", "committedDate": "2020-03-18T23:20:15Z", "type": "commit"}, {"oid": "47fabafd6e70b23befb1526066f1b1102cbab385", "url": "https://github.com/splicemachine/spliceengine/commit/47fabafd6e70b23befb1526066f1b1102cbab385", "message": "Organizing eNSDS code.", "committedDate": "2020-03-18T23:22:22Z", "type": "commit"}, {"oid": "90e9d03884e669414511c70899ddda0bff32cd2d", "url": "https://github.com/splicemachine/spliceengine/commit/90e9d03884e669414511c70899ddda0bff32cd2d", "message": "Updates to support 3.0 upgrade to Kafka 2.2", "committedDate": "2020-03-19T14:30:22Z", "type": "commit"}, {"oid": "aec391953ca5aac86497750b4ac506e75d15de30", "url": "https://github.com/splicemachine/spliceengine/commit/aec391953ca5aac86497750b4ac506e75d15de30", "message": "Added upsert functions.", "committedDate": "2020-03-19T23:08:42Z", "type": "commit"}, {"oid": "e6d22d4ae5d5bf6c87063f78498151d0f2abcf58", "url": "https://github.com/splicemachine/spliceengine/commit/e6d22d4ae5d5bf6c87063f78498151d0f2abcf58", "message": "Added dependency on slf4j needed by kafka 2.0 used by hdp3.1.0", "committedDate": "2020-03-20T02:52:05Z", "type": "commit"}, {"oid": "c34acdaaa5469399ce6c50a23f0a2923f7a79959", "url": "https://github.com/splicemachine/spliceengine/commit/c34acdaaa5469399ce6c50a23f0a2923f7a79959", "message": "Uncommented splice_spark module.", "committedDate": "2020-03-20T02:54:43Z", "type": "commit"}, {"oid": "9456318d303763243123c6c0ca2ac54f2fd303ce", "url": "https://github.com/splicemachine/spliceengine/commit/9456318d303763243123c6c0ca2ac54f2fd303ce", "message": "Merge branch 'master' into eNSDS-3.1.0.1951\nTo get changes from DB-9200", "committedDate": "2020-03-20T03:00:58Z", "type": "commit"}, {"oid": "3042ef4a091e76fe63609a99607c1be35e91e94a", "url": "https://github.com/splicemachine/spliceengine/commit/3042ef4a091e76fe63609a99607c1be35e91e94a", "message": "Updated parent pom version.", "committedDate": "2020-03-20T21:48:53Z", "type": "commit"}, {"oid": "f22b670a6ab9ed649db7db8314386efd893e061a", "url": "https://github.com/splicemachine/spliceengine/commit/f22b670a6ab9ed649db7db8314386efd893e061a", "message": "Sync with splice_spark pom.", "committedDate": "2020-03-21T02:49:28Z", "type": "commit"}, {"oid": "1d4f6767c7a83f1ec7db9082564a1be3add5bbfb", "url": "https://github.com/splicemachine/spliceengine/commit/1d4f6767c7a83f1ec7db9082564a1be3add5bbfb", "message": "Added getConnection, and did some cleanup.", "committedDate": "2020-03-21T16:59:12Z", "type": "commit"}, {"oid": "53856e1b2830e674c14d42327e1a6516c3afc511", "url": "https://github.com/splicemachine/spliceengine/commit/53856e1b2830e674c14d42327e1a6516c3afc511", "message": "Added export and exportBinary functions.", "committedDate": "2020-03-22T01:09:57Z", "type": "commit"}, {"oid": "79bb5a78d01e607fe7109ced6717887a4fbd32e0", "url": "https://github.com/splicemachine/spliceengine/commit/79bb5a78d01e607fe7109ced6717887a4fbd32e0", "message": "DB-9135 analyzeTable samplePercent param bug fix.", "committedDate": "2020-03-23T00:21:10Z", "type": "commit"}, {"oid": "5ee475df62447e9a65acfa7aa73a5d4503faae45", "url": "https://github.com/splicemachine/spliceengine/commit/5ee475df62447e9a65acfa7aa73a5d4503faae45", "message": "Added update and delete functions.  Brought in analyzeTable bug fix.  Cleaned out commented code and unused imports.", "committedDate": "2020-03-23T00:22:51Z", "type": "commit"}, {"oid": "240a78194398fe5648576e409ac8fd64870924e9", "url": "https://github.com/splicemachine/spliceengine/commit/240a78194398fe5648576e409ac8fd64870924e9", "message": "Cleaned out commented code and unused imports.", "committedDate": "2020-03-23T00:23:20Z", "type": "commit"}, {"oid": "407c2cf6c654c9cc0bd69899a17a74a7fff32aae", "url": "https://github.com/splicemachine/spliceengine/commit/407c2cf6c654c9cc0bd69899a17a74a7fff32aae", "message": "Merge eNSDS with 3.0 follow up.", "committedDate": "2020-03-24T01:03:19Z", "type": "commit"}, {"oid": "3726231b84ea1eedde96bc80ba3dfd8b963558c2", "url": "https://github.com/splicemachine/spliceengine/commit/3726231b84ea1eedde96bc80ba3dfd8b963558c2", "message": "Added kafka to hdp classpaths.", "committedDate": "2020-03-24T01:04:52Z", "type": "commit"}, {"oid": "9fab9d32f8dd6f7f4a2b576378f1630f3b590797", "url": "https://github.com/splicemachine/spliceengine/commit/9fab9d32f8dd6f7f4a2b576378f1630f3b590797", "message": "Using JDBCOptions in spark 2.3 and 2.2 code.", "committedDate": "2020-03-24T01:09:25Z", "type": "commit"}, {"oid": "3e3c29a380eff7a50344f7eacc22af4f29f76bc3", "url": "https://github.com/splicemachine/spliceengine/commit/3e3c29a380eff7a50344f7eacc22af4f29f76bc3", "message": "Maintaining case in select statements.", "committedDate": "2020-03-24T02:40:15Z", "type": "commit"}, {"oid": "8683d4245d4474768d145cb432188969347fc0e1", "url": "https://github.com/splicemachine/spliceengine/commit/8683d4245d4474768d145cb432188969347fc0e1", "message": "Added a bulkImportHFile signature that can be called from pysplice.", "committedDate": "2020-03-25T16:59:26Z", "type": "commit"}, {"oid": "e66d49159afcb062455f1265c08b42f3d9131479", "url": "https://github.com/splicemachine/spliceengine/commit/e66d49159afcb062455f1265c08b42f3d9131479", "message": "Updated getSchema.", "committedDate": "2020-03-26T15:07:54Z", "type": "commit"}, {"oid": "d88ccf547b196f2db7f1472c4cc3b55044d7875d", "url": "https://github.com/splicemachine/spliceengine/commit/d88ccf547b196f2db7f1472c4cc3b55044d7875d", "message": "Added KafkaUtils.", "committedDate": "2020-03-26T15:09:16Z", "type": "commit"}, {"oid": "6430247232ecb718121812f05be55a2636c59b01", "url": "https://github.com/splicemachine/spliceengine/commit/6430247232ecb718121812f05be55a2636c59b01", "message": "DB-8649 Checking message count in topic to advance iterator during retry.", "committedDate": "2020-03-26T15:13:12Z", "type": "commit"}, {"oid": "124e8530cb5bf316dd2aeccb1e978d848e9d5987", "url": "https://github.com/splicemachine/spliceengine/commit/124e8530cb5bf316dd2aeccb1e978d848e9d5987", "message": "Added some error checking and exception throwing in getSchema.", "committedDate": "2020-03-26T20:24:01Z", "type": "commit"}, {"oid": "0fba6daa1f2ac8a2f26891615a927312ca869c02", "url": "https://github.com/splicemachine/spliceengine/commit/0fba6daa1f2ac8a2f26891615a927312ca869c02", "message": "Adding support for more types of select statements.", "committedDate": "2020-03-30T20:45:46Z", "type": "commit"}, {"oid": "f2e59338e098c2a38934c91345aa3781722cf537", "url": "https://github.com/splicemachine/spliceengine/commit/f2e59338e098c2a38934c91345aa3781722cf537", "message": "Added support for more types of select statements. Support for rdd and internalRdd functions to be callable from pysplice. Checking for empty input dataframe in export function. Improved datatypes returned from getSchema.", "committedDate": "2020-03-30T20:48:55Z", "type": "commit"}, {"oid": "840c4f3ae84b44901306947d1019fe030a89cbf7", "url": "https://github.com/splicemachine/spliceengine/commit/840c4f3ae84b44901306947d1019fe030a89cbf7", "message": "Handling exception when parquet schema is not found externally.", "committedDate": "2020-03-31T01:15:21Z", "type": "commit"}, {"oid": "2251afd96404932c89fbdd77192f23fe9e166575", "url": "https://github.com/splicemachine/spliceengine/commit/2251afd96404932c89fbdd77192f23fe9e166575", "message": "In export, supporting single quote in quoteCharacter param.", "committedDate": "2020-04-01T01:11:49Z", "type": "commit"}, {"oid": "1aeef911cafe304c484a8e764389dc61f04d9414", "url": "https://github.com/splicemachine/spliceengine/commit/1aeef911cafe304c484a8e764389dc61f04d9414", "message": "Throwing exception if input sql contains property useSpark=false. Clarified some private function names.", "committedDate": "2020-04-01T18:53:50Z", "type": "commit"}, {"oid": "228e4143a33b96c899c3c0016e0120f95de498ea", "url": "https://github.com/splicemachine/spliceengine/commit/228e4143a33b96c899c3c0016e0120f95de498ea", "message": "Updated KafkaUtils messageCount.", "committedDate": "2020-04-02T22:56:16Z", "type": "commit"}, {"oid": "20082caded911198577202b8120d60e37f62d774", "url": "https://github.com/splicemachine/spliceengine/commit/20082caded911198577202b8120d60e37f62d774", "message": "Added param for kafka bootstrap servers.", "committedDate": "2020-04-04T17:44:08Z", "type": "commit"}, {"oid": "8b858fef1eaad9b539fc731db6cedde5bc8ab403", "url": "https://github.com/splicemachine/spliceengine/commit/8b858fef1eaad9b539fc731db6cedde5bc8ab403", "message": "For kafka bootstrap servers default value, checking system properties.", "committedDate": "2020-04-04T17:52:44Z", "type": "commit"}, {"oid": "bf148a1944f653652229aa68ab835ba6d017144a", "url": "https://github.com/splicemachine/spliceengine/commit/bf148a1944f653652229aa68ab835ba6d017144a", "message": "Setting kafka bootstrapServers in ctr of ExportKafkaOp*FlatMapFunction. Made kafka client ids unique per class.", "committedDate": "2020-04-04T18:05:19Z", "type": "commit"}, {"oid": "25627eaf5e9ea09357a66182a3dd6c3fdcdc5fb6", "url": "https://github.com/splicemachine/spliceengine/commit/25627eaf5e9ea09357a66182a3dd6c3fdcdc5fb6", "message": "Schema added to the returned data. Returning an empty object when a query returns no data.", "committedDate": "2020-04-06T01:30:57Z", "type": "commit"}, {"oid": "a2b05efc7316ee1ba05d9aa29aca13b7c3ba950c", "url": "https://github.com/splicemachine/spliceengine/commit/a2b05efc7316ee1ba05d9aa29aca13b7c3ba950c", "message": "Put recent updates from spark 2.4 code into 2.3 and 2.2 code.", "committedDate": "2020-04-06T15:30:18Z", "type": "commit"}, {"oid": "57e13178ec50143b341842f16ec53bf5364cb01e", "url": "https://github.com/splicemachine/spliceengine/commit/57e13178ec50143b341842f16ec53bf5364cb01e", "message": "Throwing exception when useSpark=false or useSpark = false is passed to df function.", "committedDate": "2020-04-06T22:38:19Z", "type": "commit"}, {"oid": "aa4613dce3bd9cd6bff88edbcf7d224771cda76e", "url": "https://github.com/splicemachine/spliceengine/commit/aa4613dce3bd9cd6bff88edbcf7d224771cda76e", "message": "Updated ITs in splice_spark2.", "committedDate": "2020-04-07T00:13:23Z", "type": "commit"}, {"oid": "b75b466363e2edfee89d7485610ca97f9d57e17a", "url": "https://github.com/splicemachine/spliceengine/commit/b75b466363e2edfee89d7485610ca97f9d57e17a", "message": "Supporting both splice_spark and splice_spark2 in assembly pom.", "committedDate": "2020-04-08T17:07:28Z", "type": "commit"}, {"oid": "0fcfe321edc0e12b1d05d9a45c605d518442c4c3", "url": "https://github.com/splicemachine/spliceengine/commit/0fcfe321edc0e12b1d05d9a45c605d518442c4c3", "message": "DB-8627 Merged to address conflicts.", "committedDate": "2020-04-15T19:38:45Z", "type": "commit"}, {"oid": "5a6d1f7e8ed404cdeee62e0cd97bafd7aa623c02", "url": "https://github.com/splicemachine/spliceengine/commit/5a6d1f7e8ed404cdeee62e0cd97bafd7aa623c02", "message": "DB-8627 Merge followup.", "committedDate": "2020-04-16T00:45:09Z", "type": "commit"}, {"oid": "5a457b233a45582b5257c1605210ed2b39cdac42", "url": "https://github.com/splicemachine/spliceengine/commit/5a457b233a45582b5257c1605210ed2b39cdac42", "message": "DB-8627 Removed Kafka dependencies from splice_machine.", "committedDate": "2020-04-16T00:48:30Z", "type": "commit"}, {"oid": "206063e762cfb202071910106a7f804e4f4d99df", "url": "https://github.com/splicemachine/spliceengine/commit/206063e762cfb202071910106a7f804e4f4d99df", "message": "DB-9376 For updates and deletes, using schema from jdbc metadata instead of spark schema.", "committedDate": "2020-04-22T20:32:39Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjkxNTM5OQ==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r416915399", "bodyText": "The same parameter is defined twice in HbaseConfigurations & OlapConfigurations, we probably only need one", "author": "arnaud-splice", "createdAt": "2020-04-28T20:53:57Z", "path": "splice_access_api/src/main/java/com/splicemachine/access/configuration/OlapConfigurations.java", "diffHunk": "@@ -117,6 +117,10 @@\n     public static final String SPARK_COMPACTION_BLOCKING = \"spark.compaction.blocking\";\n     public static final boolean DEFAULT_SPARK_COMPACTION_BLOCKING = true;\n \n+    // Kafka Bootstrap Servers\n+    public static final String KAFKA_BOOTSTRAP_SERVERS = \"splice.kafka.bootstrapServers\";", "originalCommit": "206063e762cfb202071910106a7f804e4f4d99df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczMjAwNg==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417732006", "bodyText": "Removed param from OlapConfigurations in new commit ce45a6e", "author": "jpanko1", "createdAt": "2020-04-30T03:22:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjkxNTM5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzQ2MDgzNg==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417460836", "bodyText": "change 2019 to 2020", "author": "jyuanca", "createdAt": "2020-04-29T16:47:09Z", "path": "db-engine/src/main/java/com/splicemachine/db/impl/sql/compile/KafkaExportNode.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Copyright (c) 2012 - 2019 Splice Machine, Inc.\n+ *", "originalCommit": "206063e762cfb202071910106a7f804e4f4d99df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczMzI5Nw==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417733297", "bodyText": "Fixed in new commit e15bf8e", "author": "jpanko1", "createdAt": "2020-04-30T03:28:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzQ2MDgzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzQ2MTA3Ng==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417461076", "bodyText": "KAFKA_EXPORT?", "author": "jyuanca", "createdAt": "2020-04-29T16:47:33Z", "path": "db-engine/src/main/java/com/splicemachine/db/impl/sql/compile/KafkaExportNode.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Copyright (c) 2012 - 2019 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.db.impl.sql.compile;\n+\n+import com.splicemachine.db.iapi.error.StandardException;\n+import com.splicemachine.db.iapi.reference.ClassName;\n+import com.splicemachine.db.iapi.reference.SQLState;\n+import com.splicemachine.db.iapi.services.classfile.VMOpcode;\n+import com.splicemachine.db.iapi.services.compiler.MethodBuilder;\n+import com.splicemachine.db.iapi.sql.ResultColumnDescriptor;\n+import com.splicemachine.db.iapi.sql.ResultDescription;\n+import com.splicemachine.db.iapi.sql.compile.Visitor;\n+import com.splicemachine.db.iapi.types.DataTypeDescriptor;\n+import com.splicemachine.db.iapi.types.TypeId;\n+import com.splicemachine.db.impl.sql.GenericColumnDescriptor;\n+\n+import java.util.List;\n+\n+/**\n+ * Export Node\n+ * <p/>\n+ * EXAMPLE:\n+ * <p/>\n+ * BINARY_EXPORT('/dir', true, 'parquet') select a, b, sqrt(c) from table1 where a > 100;\n+ */", "originalCommit": "206063e762cfb202071910106a7f804e4f4d99df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczMzE4MQ==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417733181", "bodyText": "Fixed in new commit e15bf8e", "author": "jpanko1", "createdAt": "2020-04-30T03:28:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzQ2MTA3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzQ2NTk5MQ==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417465991", "bodyText": "License header missing", "author": "jyuanca", "createdAt": "2020-04-29T16:54:40Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction.java", "diffHunk": "@@ -0,0 +1,110 @@\n+package com.splicemachine.derby.stream.spark;", "originalCommit": "206063e762cfb202071910106a7f804e4f4d99df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczMzEwMg==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417733102", "bodyText": "Fixed in new commit e15bf8e", "author": "jpanko1", "createdAt": "2020-04-30T03:27:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzQ2NTk5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzQ4ODQ1OA==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417488458", "bodyText": "remove this?", "author": "jyuanca", "createdAt": "2020-04-29T17:30:19Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction.java", "diffHunk": "@@ -0,0 +1,110 @@\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.splicemachine.db.iapi.sql.execute.ExecRow;\n+import com.splicemachine.derby.impl.sql.execute.operations.export.ExportKafkaOperation;\n+import com.splicemachine.derby.stream.function.SpliceFlatMapFunction;\n+import com.splicemachine.derby.stream.iapi.OperationContext;\n+import com.splicemachine.si.impl.driver.SIDriver;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.IntegerDeserializer;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.TaskKilledException;\n+\n+import java.io.Externalizable;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+class ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction extends SpliceFlatMapFunction<ExportKafkaOperation, Iterator<Integer>, ExecRow> {\n+    private String topicName;\n+    private String bootstrapServers;\n+\n+    public ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction() {\n+    }\n+\n+    public ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction(OperationContext context, String topicName) {\n+        this(context, topicName, SIDriver.driver().getConfiguration().getKafkaBootstrapServers());\n+    }\n+\n+    public ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction(OperationContext context, String topicName, String bootstrapServers) {\n+        super(context);\n+        this.topicName = topicName;\n+        this.bootstrapServers = bootstrapServers;\n+    }\n+\n+    @Override\n+    public Iterator<ExecRow> call(Iterator<Integer> partitions) throws Exception {\n+        Properties props = new Properties();\n+\n+        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n+\n+        String group_id = \"spark-consumer-dss-ekoiersfmf-\"+UUID.randomUUID();\n+        props.put(ConsumerConfig.GROUP_ID_CONFIG,group_id);\n+        props.put(ConsumerConfig.CLIENT_ID_CONFIG, group_id);\n+\n+        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());\n+        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ExternalizableDeserializer.class.getName());\n+        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+\n+        //For Debug\n+        System.out.println(\"System print group_id:\"+group_id);\n+", "originalCommit": "206063e762cfb202071910106a7f804e4f4d99df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczMzA2Mg==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417733062", "bodyText": "Fixed in new commit e15bf8e", "author": "jpanko1", "createdAt": "2020-04-30T03:27:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzQ4ODQ1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzUwNzAyNw==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417507027", "bodyText": "partition is never used, so it can be removed", "author": "jyuanca", "createdAt": "2020-04-29T17:59:39Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/KafkaStreamer.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import com.splicemachine.db.impl.sql.execute.ValueRow;\n+import com.splicemachine.derby.stream.ActivationHolder;\n+import com.splicemachine.derby.stream.iapi.OperationContext;\n+import com.splicemachine.derby.utils.marshall.dvd.KryoDescriptorSerializer;\n+import com.splicemachine.si.impl.driver.SIDriver;\n+import com.splicemachine.stream.StreamProtocol;\n+import com.splicemachine.stream.handlers.OpenHandler;\n+import io.netty.bootstrap.Bootstrap;\n+import io.netty.channel.ChannelFuture;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.channel.ChannelInboundHandlerAdapter;\n+import io.netty.channel.ChannelOption;\n+import io.netty.channel.nio.NioEventLoopGroup;\n+import io.netty.channel.socket.nio.NioSocketChannel;\n+import org.apache.hadoop.hive.ql.exec.spark.KryoSerializer;\n+import org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.serialization.IntegerSerializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.TaskKilledException;\n+import org.apache.spark.api.java.function.Function2;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.io.Externalizable;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.io.Serializable;\n+import java.net.InetSocketAddress;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.Semaphore;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+/**\n+ * Created by dgomezferro on 5/25/16.\n+ */\n+public class KafkaStreamer<T> implements Function2<Integer, Iterator<T>, Iterator<String>>, Serializable, Externalizable {\n+    private static final Logger LOG = Logger.getLogger(KafkaStreamer.class);\n+\n+    private int numPartitions;\n+    private String bootstrapServers;\n+    private String topicName;\n+    private Optional<StructType> schema;\n+    private int partition;", "originalCommit": "206063e762cfb202071910106a7f804e4f4d99df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczMzAwMQ==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417733001", "bodyText": "Fixed in new commit e15bf8e", "author": "jpanko1", "createdAt": "2020-04-30T03:27:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzUwNzAyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzUxOTQyOA==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417519428", "bodyText": "Why this is necessary?", "author": "jyuanca", "createdAt": "2020-04-29T18:21:03Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkKafkaDataSetWriter.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Copyright (c) 2012 - 2019 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.splicemachine.db.iapi.error.StandardException;\n+import com.splicemachine.db.iapi.sql.execute.ExecRow;\n+import com.splicemachine.db.iapi.types.SQLLongint;\n+import com.splicemachine.db.impl.sql.execute.ValueRow;\n+import com.splicemachine.derby.impl.SpliceSpark;\n+import com.splicemachine.derby.stream.iapi.DataSet;\n+import com.splicemachine.derby.stream.output.DataSetWriter;\n+import com.splicemachine.si.api.txn.TxnView;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.util.LongAccumulator;\n+\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.util.Collections;\n+import java.util.Optional;\n+\n+\n+public class SparkKafkaDataSetWriter<V> implements DataSetWriter{\n+    private String topicName;\n+    private JavaRDD<V> rdd;\n+    private Optional<StructType> schema;\n+\n+    public SparkKafkaDataSetWriter(){\n+    }\n+\n+    public SparkKafkaDataSetWriter(JavaRDD<V> rdd,\n+                                   String topicName){\n+        this(rdd, topicName, Optional.empty());\n+    }\n+\n+    public SparkKafkaDataSetWriter(JavaRDD<V> rdd,\n+                                   String topicName,\n+                                   Optional<StructType> schema){\n+        this.rdd = rdd;\n+        this.topicName = topicName;\n+        this.schema = schema;\n+    }\n+\n+    public void writeExternal(ObjectOutput out) throws IOException{\n+        out.writeUTF(topicName);\n+        out.writeObject(rdd);\n+        out.writeObject(schema.orElse(new StructType()));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException{\n+        topicName = in.readUTF();\n+        rdd = (JavaRDD)in.readObject();\n+        StructType structType = (StructType)in.readObject();\n+        schema = structType.length() > 0 ? Optional.of(structType) : Optional.empty();\n+    }\n+\n+    @Override\n+    public DataSet<ExecRow> write() throws StandardException{\n+        long start = System.currentTimeMillis();\n+\n+        CountFunction countFunction = new CountFunction<>();\n+        KafkaStreamer kafkaStreamer = new KafkaStreamer(rdd.getNumPartitions(), topicName, schema);\n+        JavaRDD streamed = rdd.map(countFunction).mapPartitionsWithIndex(kafkaStreamer, true);\n+        streamed.collect();\n+\n+        Long count = countFunction.getCount().value();\n+        if(count == 0) {\n+            try {\n+                kafkaStreamer.noData();\n+            } catch(Exception e) {", "originalCommit": "206063e762cfb202071910106a7f804e4f4d99df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4OTk4Mw==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417689983", "bodyText": "Previously, if a select statement legitimately returned no rows, the code block before this one would do nothing, and the client (KafkaToDF) would time out.  With this fix, the noData function sends an empty object with a schema so that the client is notified the query returned no rows and it also gets the schema.\u2028", "author": "jpanko1", "createdAt": "2020-04-30T00:31:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzUxOTQyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzUyNDk1NQ==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417524955", "bodyText": "remove them?", "author": "jyuanca", "createdAt": "2020-04-29T18:30:26Z", "path": "splice_machine/src/main/java/com/splicemachine/derby/stream/control/ControlDataSetProcessor.java", "diffHunk": "@@ -41,20 +41,38 @@\n import com.splicemachine.storage.Partition;\n import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n import org.apache.commons.collections.iterators.SingletonIterator;\n+//import org.apache.kafka.clients.consumer.CommitFailedException;\n+//import org.apache.kafka.clients.consumer.ConsumerConfig;\n+//import org.apache.kafka.clients.consumer.ConsumerRecords;\n+//import org.apache.kafka.clients.consumer.ConsumerRecord;", "originalCommit": "206063e762cfb202071910106a7f804e4f4d99df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczMjg5OQ==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417732899", "bodyText": "Fixed in new commit e15bf8e", "author": "jpanko1", "createdAt": "2020-04-30T03:26:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzUyNDk1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzUyNTIyNQ==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417525225", "bodyText": "Remove them?", "author": "jyuanca", "createdAt": "2020-04-29T18:30:55Z", "path": "splice_machine/src/main/java/com/splicemachine/derby/stream/control/ControlDataSetProcessor.java", "diffHunk": "@@ -404,4 +422,45 @@ public TableChecker getTableChecker(String schemaName, String tableName, DataSet\n     @Override public void incrementOpDepth() { }\n     @Override public void decrementOpDepth() { }\n     @Override public void resetOpDepth() { }\n+\n+    @Override\n+    public <V> DataSet<ExecRow> readKafkaTopic(String topicName, OperationContext context) throws StandardException {\n+//        Properties props = new Properties();\n+//        props.put(\"bootstrap.servers\", SIDriver.driver().getConfiguration().getKafkaBootstrapServers());\n+//        props.put(\"enable.auto.commit\", false);\n+//        props.put(\"auto.commit.interval.ms\", \"1000\");\n+//        props.put(\"session.timeout.ms\", \"30000\");\n+//        props.put(\"auto.offset.reset\", \"latest\");\n+//\n+//        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());\n+////        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ExternalizableDeserializer.class.getName());\n+//        //ExternalizableDeserializer was implemented in package com.splicemachine.derby.stream.spark\n+//\n+//        KafkaConsumer<Integer, Externalizable> consumer = new KafkaConsumer<Integer, Externalizable>(props);\n+//        consumer.subscribe(Arrays.asList(topicName));\n+//\n+//        try {\n+//            ConsumerRecords<Integer, Externalizable> msgList = consumer.poll(1000);\n+//            for (ConsumerRecord<Integer, Externalizable> record : msgList) {\n+//                if (LOG.isInfoEnabled())\n+//                    LOG.info(\"KAFKA==receive: key = \" + record.key() + \", value = \" + record.value() + \" offset===\" + record.offset());\n+//            }\n+//\n+//            PairDataSet<Integer, InputStream> streamSet;\n+//\n+//            consumer.commitAsync();\n+//\n+//            Dataset<Row> table = null;\n+//        } catch (CommitFailedException e){\n+//            throw new StandardException();\n+//        }\n+//        finally {\n+//            try {\n+//                consumer.commitSync();\n+//            }finally {\n+//                consumer.close();\n+//            }\n+//        }", "originalCommit": "206063e762cfb202071910106a7f804e4f4d99df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzczMjgyOA==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417732828", "bodyText": "Fixed in new commit e15bf8e", "author": "jpanko1", "createdAt": "2020-04-30T03:26:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzUyNTIyNQ=="}], "type": "inlineReview"}, {"oid": "f0f4c0d65d536eb81fb0fd65e321906d92aeb509", "url": "https://github.com/splicemachine/spliceengine/commit/f0f4c0d65d536eb81fb0fd65e321906d92aeb509", "message": "Fix merge conflicts.", "committedDate": "2020-04-30T03:11:32Z", "type": "commit"}, {"oid": "ce45a6e2c9126ce86c5a6b350bfbbaffe4ca2a5c", "url": "https://github.com/splicemachine/spliceengine/commit/ce45a6e2c9126ce86c5a6b350bfbbaffe4ca2a5c", "message": "Removed kafkaBootstrapServers config param.", "committedDate": "2020-04-30T03:13:15Z", "type": "commit"}, {"oid": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd", "url": "https://github.com/splicemachine/spliceengine/commit/e15bf8e4ed58fce49d5a3048e6bca875bad887bd", "message": "Cleaned up comments, debugs, and unused variables.", "committedDate": "2020-04-30T03:19:05Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzk4MzkwOA==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r417983908", "bodyText": "I feel that serializing the schema for every single ValueRow is too much overhead, where is this needed?", "author": "dgomezferro", "createdAt": "2020-04-30T12:49:14Z", "path": "db-engine/src/main/java/com/splicemachine/db/impl/sql/execute/ValueRow.java", "diffHunk": "@@ -400,6 +411,8 @@ public void readExternal(ObjectInput in) throws IOException, ClassNotFoundExcept\n \t\tfor (int i = 0; i < ncols; i++) {\n \t\t\tcolumn[i] = (DataValueDescriptor) in.readObject();\n \t\t}\n+\t\tStructType structType = (StructType)in.readObject();", "originalCommit": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODE3NDUwMg==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418174502", "bodyText": "The schema was added so that the dataframe returned from SplicemachineContext.df() would contain a schema having the correct column names instead of the C0, C1, etc, names generated by ValueRow.getNamedColumn().  Since individual rows are returned through Kafka rather than one collection object of rows, I put the schema on the row level.  org.apache.spark.sql.Row implemented by ValueRow also has a method for returning the schema on the row level.\nWould it be acceptable if ValueRow stores and serializes the schema as a string, and converts it to StructType only when it\u2019s needed in ValueRow.schema() ?", "author": "jpanko1", "createdAt": "2020-04-30T17:31:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzk4MzkwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODE4ODY1MQ==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418188651", "bodyText": "I would try to get the schema separately, maybe through the JDBC connection, and feed it to KafkaToDF() so that it has it from the beginning rather than expecting it on every ValueRow", "author": "dgomezferro", "createdAt": "2020-04-30T17:55:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzk4MzkwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTI1NTc1NA==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r419255754", "bodyText": "Fixed in new commit 5eb097b.", "author": "jpanko1", "createdAt": "2020-05-04T07:32:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzk4MzkwOA=="}], "type": "inlineReview"}, {"oid": "82fba10d7b75dfc54b03624186a18f9b18e50c9d", "url": "https://github.com/splicemachine/spliceengine/commit/82fba10d7b75dfc54b03624186a18f9b18e50c9d", "message": "Updated copyright date.", "committedDate": "2020-04-30T16:59:29Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODA2NjM4OQ==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418066389", "bodyText": "Nitpick: can we rename it to KafkaReadFunction or something similar, a bit more descriptive ?", "author": "dgomezferro", "createdAt": "2020-04-30T14:46:44Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.splicemachine.db.iapi.sql.execute.ExecRow;\n+import com.splicemachine.derby.impl.sql.execute.operations.export.ExportKafkaOperation;\n+import com.splicemachine.derby.stream.function.SpliceFlatMapFunction;\n+import com.splicemachine.derby.stream.iapi.OperationContext;\n+import com.splicemachine.si.impl.driver.SIDriver;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.IntegerDeserializer;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.TaskKilledException;\n+\n+import java.io.Externalizable;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+class ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction extends SpliceFlatMapFunction<ExportKafkaOperation, Iterator<Integer>, ExecRow> {", "originalCommit": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMTIwOQ==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418431209", "bodyText": "Renamed in new commits 5c1565b and ca75b78.", "author": "jpanko1", "createdAt": "2020-05-01T06:15:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODA2NjM4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODA2OTE0Nw==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418069147", "bodyText": "I think you expect a single value here, in that case I would make this function a SpliceFlatMapFunction<ExportKafkaOperation, Integer, ExecRow> and call it with rdd.map(new ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction(context, topicName, bootstrapServers));\n(instead of mapPartitions)", "author": "dgomezferro", "createdAt": "2020-04-30T14:50:26Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.splicemachine.db.iapi.sql.execute.ExecRow;\n+import com.splicemachine.derby.impl.sql.execute.operations.export.ExportKafkaOperation;\n+import com.splicemachine.derby.stream.function.SpliceFlatMapFunction;\n+import com.splicemachine.derby.stream.iapi.OperationContext;\n+import com.splicemachine.si.impl.driver.SIDriver;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.IntegerDeserializer;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.TaskKilledException;\n+\n+import java.io.Externalizable;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.Properties;\n+import java.util.UUID;\n+\n+class ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction extends SpliceFlatMapFunction<ExportKafkaOperation, Iterator<Integer>, ExecRow> {\n+    private String topicName;\n+    private String bootstrapServers;\n+\n+    public ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction() {\n+    }\n+\n+    public ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction(OperationContext context, String topicName) {\n+        this(context, topicName, SIDriver.driver().getConfiguration().getKafkaBootstrapServers());\n+    }\n+\n+    public ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction(OperationContext context, String topicName, String bootstrapServers) {\n+        super(context);\n+        this.topicName = topicName;\n+        this.bootstrapServers = bootstrapServers;\n+    }\n+\n+    @Override\n+    public Iterator<ExecRow> call(Iterator<Integer> partitions) throws Exception {", "originalCommit": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc1MTE3MA==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418751170", "bodyText": "Fixed in new commit c8b039b.", "author": "jpanko1", "createdAt": "2020-05-01T21:44:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODA2OTE0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODA3MDkyOQ==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418070929", "bodyText": "We need a follow up Jira to improve serialization, relying on Java's serialization is too slow.", "author": "dgomezferro", "createdAt": "2020-04-30T14:53:02Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/KafkaStreamer.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import com.splicemachine.db.impl.sql.execute.ValueRow;\n+import com.splicemachine.derby.stream.ActivationHolder;\n+import com.splicemachine.derby.stream.iapi.OperationContext;\n+import com.splicemachine.derby.utils.marshall.dvd.KryoDescriptorSerializer;\n+import com.splicemachine.si.impl.driver.SIDriver;\n+import com.splicemachine.stream.StreamProtocol;\n+import com.splicemachine.stream.handlers.OpenHandler;\n+import io.netty.bootstrap.Bootstrap;\n+import io.netty.channel.ChannelFuture;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.channel.ChannelInboundHandlerAdapter;\n+import io.netty.channel.ChannelOption;\n+import io.netty.channel.nio.NioEventLoopGroup;\n+import io.netty.channel.socket.nio.NioSocketChannel;\n+import org.apache.hadoop.hive.ql.exec.spark.KryoSerializer;\n+import org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.serialization.IntegerSerializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.TaskKilledException;\n+import org.apache.spark.api.java.function.Function2;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.io.Externalizable;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.io.Serializable;\n+import java.net.InetSocketAddress;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.Semaphore;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+/**\n+ * Created by dgomezferro on 5/25/16.\n+ */\n+public class KafkaStreamer<T> implements Function2<Integer, Iterator<T>, Iterator<String>>, Serializable, Externalizable {\n+    private static final Logger LOG = Logger.getLogger(KafkaStreamer.class);\n+\n+    private int numPartitions;\n+    private String bootstrapServers;\n+    private String topicName;\n+    private Optional<StructType> schema;\n+    private volatile TaskContext taskContext;\n+\n+    // Serialization\n+    public KafkaStreamer(){\n+    }\n+\n+    public KafkaStreamer(int numPartitions, String topicName) {\n+        this(numPartitions, topicName, Optional.empty());\n+    }\n+\n+    public KafkaStreamer(int numPartitions, String topicName, Optional<StructType> schema) {\n+        this.bootstrapServers = SIDriver.driver().getConfiguration().getKafkaBootstrapServers();\n+        this.numPartitions = numPartitions;\n+        this.topicName = topicName;\n+        this.schema = schema;\n+    }\n+\n+    public void noData() throws Exception {\n+        call(0, (Iterator<T>)Arrays.asList(new ValueRow(0)).iterator());\n+    }\n+\n+    @Override\n+    public Iterator<String> call(Integer partition, Iterator<T> locatedRowIterator) throws Exception {\n+        taskContext = TaskContext.get();\n+\n+        if (taskContext != null && taskContext.attemptNumber() > 0) {\n+            LOG.trace(\"KS.c attempts \"+taskContext.attemptNumber());\n+            long entriesInKafka = KafkaUtils.messageCount(bootstrapServers, topicName, partition);\n+            LOG.trace(\"KS.c entries \"+entriesInKafka);\n+            for (long i = 0; i < entriesInKafka; ++i) {\n+                locatedRowIterator.next();\n+            }\n+        }\n+\n+        Properties props = new Properties();\n+        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n+        props.put(ProducerConfig.CLIENT_ID_CONFIG, \"spark-producer-dss-ks-\"+UUID.randomUUID() );\n+        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class.getName());\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ExternalizableSerializer.class.getName());", "originalCommit": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM0ODAzNA==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418348034", "bodyText": "Added DB-9474.", "author": "jpanko1", "createdAt": "2020-04-30T23:38:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODA3MDkyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODA3MzQ0Ng==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418073446", "bodyText": "Remove comments", "author": "dgomezferro", "createdAt": "2020-04-30T14:56:18Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/KafkaStreamer.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import com.splicemachine.db.impl.sql.execute.ValueRow;\n+import com.splicemachine.derby.stream.ActivationHolder;\n+import com.splicemachine.derby.stream.iapi.OperationContext;\n+import com.splicemachine.derby.utils.marshall.dvd.KryoDescriptorSerializer;\n+import com.splicemachine.si.impl.driver.SIDriver;\n+import com.splicemachine.stream.StreamProtocol;\n+import com.splicemachine.stream.handlers.OpenHandler;\n+import io.netty.bootstrap.Bootstrap;\n+import io.netty.channel.ChannelFuture;\n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.channel.ChannelInboundHandlerAdapter;\n+import io.netty.channel.ChannelOption;\n+import io.netty.channel.nio.NioEventLoopGroup;\n+import io.netty.channel.socket.nio.NioSocketChannel;\n+import org.apache.hadoop.hive.ql.exec.spark.KryoSerializer;\n+import org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.ByteArraySerializer;\n+import org.apache.kafka.common.serialization.IntegerSerializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.TaskKilledException;\n+import org.apache.spark.api.java.function.Function2;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.io.Externalizable;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.io.Serializable;\n+import java.net.InetSocketAddress;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.UUID;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.Semaphore;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+/**\n+ * Created by dgomezferro on 5/25/16.\n+ */\n+public class KafkaStreamer<T> implements Function2<Integer, Iterator<T>, Iterator<String>>, Serializable, Externalizable {\n+    private static final Logger LOG = Logger.getLogger(KafkaStreamer.class);\n+\n+    private int numPartitions;\n+    private String bootstrapServers;\n+    private String topicName;\n+    private Optional<StructType> schema;\n+    private volatile TaskContext taskContext;\n+\n+    // Serialization\n+    public KafkaStreamer(){\n+    }\n+\n+    public KafkaStreamer(int numPartitions, String topicName) {\n+        this(numPartitions, topicName, Optional.empty());\n+    }\n+\n+    public KafkaStreamer(int numPartitions, String topicName, Optional<StructType> schema) {\n+        this.bootstrapServers = SIDriver.driver().getConfiguration().getKafkaBootstrapServers();\n+        this.numPartitions = numPartitions;\n+        this.topicName = topicName;\n+        this.schema = schema;\n+    }\n+\n+    public void noData() throws Exception {\n+        call(0, (Iterator<T>)Arrays.asList(new ValueRow(0)).iterator());\n+    }\n+\n+    @Override\n+    public Iterator<String> call(Integer partition, Iterator<T> locatedRowIterator) throws Exception {\n+        taskContext = TaskContext.get();\n+\n+        if (taskContext != null && taskContext.attemptNumber() > 0) {\n+            LOG.trace(\"KS.c attempts \"+taskContext.attemptNumber());\n+            long entriesInKafka = KafkaUtils.messageCount(bootstrapServers, topicName, partition);\n+            LOG.trace(\"KS.c entries \"+entriesInKafka);\n+            for (long i = 0; i < entriesInKafka; ++i) {\n+                locatedRowIterator.next();\n+            }\n+        }\n+\n+        Properties props = new Properties();\n+        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n+        props.put(ProducerConfig.CLIENT_ID_CONFIG, \"spark-producer-dss-ks-\"+UUID.randomUUID() );\n+        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class.getName());\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ExternalizableSerializer.class.getName());\n+        KafkaProducer<Integer, Externalizable> producer = new KafkaProducer<>(props);\n+        int count = 0 ;\n+        while (locatedRowIterator.hasNext()) {\n+            T lr = locatedRowIterator.next();\n+\n+            if(schema.isPresent() && lr instanceof ValueRow) {\n+                lr = ((T)new ValueRow((ValueRow)lr, schema));\n+            }\n+\n+            ProducerRecord<Integer, Externalizable> record = new ProducerRecord(topicName,\n+                    /*partition.intValue(),*/ count++, lr);\n+            producer.send(record);\n+            LOG.trace(\"KS.c sent \"+partition.intValue()+\" \"+count+\" \"+lr);\n+        }\n+        LOG.trace(\"KS.c count \"+partition.intValue()+\" \"+count);\n+\n+//        ProducerRecord<Integer, Externalizable> record = new ProducerRecord(topicName,\n+//                partition.intValue(), -1, new ValueRow());\n+//        producer.send(record); // termination marker", "originalCommit": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMTQ4MA==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418431480", "bodyText": "Removed in new commit d83e2a6.", "author": "jpanko1", "createdAt": "2020-05-01T06:16:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODA3MzQ0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyMTkzNw==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418121937", "bodyText": "Following up from the other comment, I think this should be return rdd.map(new ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction(context, topicName, bootstrapServers));", "author": "dgomezferro", "createdAt": "2020-04-30T16:05:26Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -1144,4 +1156,26 @@ public void decrementOpDepth() {\n \n     @Override\n     public void resetOpDepth() { opDepth = 0; }\n+\n+    public <V> DataSet<ExecRow> readKafkaTopic(String topicName, OperationContext context) throws StandardException {\n+        Properties props = new Properties();\n+        String consumerId = \"spark-consumer-dss-sdsp-\"+UUID.randomUUID();\n+        String bootstrapServers = SIDriver.driver().getConfiguration().getKafkaBootstrapServers();\n+        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n+        props.put(ConsumerConfig.GROUP_ID_CONFIG, consumerId);\n+        props.put(ConsumerConfig.CLIENT_ID_CONFIG, consumerId);\n+        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class.getName());\n+        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ExternalizableDeserializer.class.getName());\n+\n+        KafkaConsumer<Integer, Externalizable> consumer = new KafkaConsumer<Integer, Externalizable>(props);\n+        List ps = consumer.partitionsFor(topicName);\n+        List<Integer> partitions = new ArrayList<>(ps.size());\n+        for (int i = 0; i < ps.size(); ++i) {\n+            partitions.add(i);\n+        }\n+        consumer.close();\n+\n+        SparkDataSet rdd = new SparkDataSet(SpliceSpark.getContext().parallelize(partitions, partitions.size()));\n+        return rdd.mapPartitions(new ExportKafkaOperationIteratorExecRowSpliceFlatMapFunction(context, topicName, bootstrapServers));", "originalCommit": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc1MTI2MA==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418751260", "bodyText": "Tried to convert to map and had an issue, but changing to flatMap worked.  Fixed in new commit c8b039b.", "author": "jpanko1", "createdAt": "2020-05-01T21:44:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyMTkzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyMzE3Mw==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418123173", "bodyText": "Does this get used? If not remove.", "author": "dgomezferro", "createdAt": "2020-04-30T16:07:15Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkKafkaDataSetWriter.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Copyright (c) 2012 - 2019 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.stream.spark;\n+\n+import com.splicemachine.db.iapi.error.StandardException;\n+import com.splicemachine.db.iapi.sql.execute.ExecRow;\n+import com.splicemachine.db.iapi.types.SQLLongint;\n+import com.splicemachine.db.impl.sql.execute.ValueRow;\n+import com.splicemachine.derby.impl.SpliceSpark;\n+import com.splicemachine.derby.stream.iapi.DataSet;\n+import com.splicemachine.derby.stream.output.DataSetWriter;\n+import com.splicemachine.si.api.txn.TxnView;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.util.LongAccumulator;\n+\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.util.Collections;\n+import java.util.Optional;\n+\n+\n+public class SparkKafkaDataSetWriter<V> implements DataSetWriter{\n+    private String topicName;\n+    private JavaRDD<V> rdd;\n+    private Optional<StructType> schema;\n+\n+    public SparkKafkaDataSetWriter(){\n+    }\n+\n+    public SparkKafkaDataSetWriter(JavaRDD<V> rdd,\n+                                   String topicName){\n+        this(rdd, topicName, Optional.empty());\n+    }\n+\n+    public SparkKafkaDataSetWriter(JavaRDD<V> rdd,\n+                                   String topicName,\n+                                   Optional<StructType> schema){\n+        this.rdd = rdd;\n+        this.topicName = topicName;\n+        this.schema = schema;\n+    }\n+\n+    public void writeExternal(ObjectOutput out) throws IOException{\n+        out.writeUTF(topicName);\n+        out.writeObject(rdd);\n+        out.writeObject(schema.orElse(new StructType()));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException{\n+        topicName = in.readUTF();\n+        rdd = (JavaRDD)in.readObject();\n+        StructType structType = (StructType)in.readObject();\n+        schema = structType.length() > 0 ? Optional.of(structType) : Optional.empty();\n+    }\n+\n+    @Override\n+    public DataSet<ExecRow> write() throws StandardException{\n+        long start = System.currentTimeMillis();\n+\n+        CountFunction countFunction = new CountFunction<>();\n+        KafkaStreamer kafkaStreamer = new KafkaStreamer(rdd.getNumPartitions(), topicName, schema);\n+        JavaRDD streamed = rdd.map(countFunction).mapPartitionsWithIndex(kafkaStreamer, true);\n+        streamed.collect();\n+\n+        Long count = countFunction.getCount().value();\n+        if(count == 0) {\n+            try {\n+                kafkaStreamer.noData();\n+            } catch(Exception e) {\n+                throw StandardException.newException(\"\", e);\n+            }\n+        }\n+\n+        long end = System.currentTimeMillis();\n+        ValueRow valueRow=new ValueRow(2);\n+        valueRow.setColumn(1,new SQLLongint(count));\n+        valueRow.setColumn(2,new SQLLongint(end-start));\n+        return new SparkDataSet<>(SpliceSpark.getContext().parallelize(Collections.singletonList(valueRow), 1));\n+    }\n+\n+    @Override\n+    public void setTxn(TxnView childTxn){\n+        throw new UnsupportedOperationException(\"IMPLEMENT\");\n+    }\n+\n+    @Override\n+    public TxnView getTxn(){\n+        throw new UnsupportedOperationException(\"IMPLEMENT\");\n+    }\n+\n+    @Override\n+    public byte[] getDestinationTable(){\n+        throw new UnsupportedOperationException();\n+    }\n+\n+\n+    public static class NullFunction<V> implements org.apache.spark.api.java.function.Function<V, Object>{", "originalCommit": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMjA4Mw==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418432083", "bodyText": "Fixed in new commit 8d9468c.", "author": "jpanko1", "createdAt": "2020-05-01T06:19:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyMzE3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyMzMzNw==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418123337", "bodyText": "year", "author": "dgomezferro", "createdAt": "2020-04-30T16:07:31Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkKafkaDataSetWriter.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Copyright (c) 2012 - 2019 Splice Machine, Inc.", "originalCommit": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMjEwMQ==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418432101", "bodyText": "Fixed in new commit 8d9468c.", "author": "jpanko1", "createdAt": "2020-05-01T06:19:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyMzMzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyMzQwMw==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418123403", "bodyText": "year", "author": "dgomezferro", "createdAt": "2020-04-30T16:07:38Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/vti/KafkaVTI.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Copyright (c) 2012 - 2019 Splice Machine, Inc.", "originalCommit": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMjQ1NA==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418432454", "bodyText": "Fixed in new commit 05cba38.", "author": "jpanko1", "createdAt": "2020-05-01T06:21:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyMzQwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyNTk2OQ==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418125969", "bodyText": "year", "author": "dgomezferro", "createdAt": "2020-04-30T16:11:28Z", "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/sql/compile/SpliceNodeFactoryImpl.java", "diffHunk": "@@ -1,5 +1,5 @@\n /*\n- * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ * Copyright (c) 2012 - 2019 Splice Machine, Inc.", "originalCommit": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMjQ2Nw==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418432467", "bodyText": "Fixed in new commit 05cba38.", "author": "jpanko1", "createdAt": "2020-05-01T06:21:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyNTk2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyNjY2MA==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418126660", "bodyText": "Shouldn't be removed", "author": "dgomezferro", "createdAt": "2020-04-30T16:12:32Z", "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/sql/compile/SpliceNodeFactoryImpl.java", "diffHunk": "@@ -52,546 +52,542 @@\n      * compile system.\n      */\n     @Override\n-    public boolean canSupport(Properties startParams) {\n-        return Monitor.isDesiredType(startParams,EngineType.STANDALONE_DB);\n-    }\n+\tpublic boolean canSupport(Properties startParams) {\n+\t\treturn Monitor.isDesiredType(startParams,EngineType.STANDALONE_DB);\n+\t}\n+\n+\t/**\n+\t\t@see Monitor\n+\t\t@exception StandardException Ooops\n+\t  */\n+\n+\tpublic void boot(boolean create, Properties startParams) throws StandardException {\n+\t\t/*\n+\t\t** This system property determines whether to optimize join order\n+\t\t** by default.  It is used mainly for testing - there are many tests\n+\t\t** that assume the join order is fixed.\n+\t\t*/\n+\t\tString opt = PropertyUtil.getSystemProperty(Optimizer.JOIN_ORDER_OPTIMIZATION);\n+\t\tif (opt != null) {\n+\t\t\tjoinOrderOptimization = Boolean.valueOf(opt);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t\t@see Monitor\n+\t  */\n+\n+\tpublic void stop() {\n+\t}\n+\n+\t/**\n+\t  Every Module needs a public niladic constructor. It just does.\n+\t  */\n+    public\tSpliceNodeFactoryImpl() {}\n+\n+\t/** @see NodeFactory#doJoinOrderOptimization */\n+\tpublic Boolean doJoinOrderOptimization() {\n+\t\treturn joinOrderOptimization;\n+\t}\n+\n+\t/**\n+\t * @see NodeFactory#getNode\n+\t *\n+\t * @exception StandardException\t\tThrown on error\n+\t */\n+\tpublic Node getNode(int nodeType, ContextManager cm) throws StandardException {\n+\n+\t\tClassInfo ci = nodeCi[nodeType];\n+\t\tClass nodeClass = null;\n+\t\tif (ci == null) {\n+\t\t\tString nodeName = nodeName(nodeType);\n+\t\t\ttry {\n+\t\t\t\tnodeClass = Class.forName(nodeName);\n+\t\t\t}\n+\t\t\tcatch (ClassNotFoundException cnfe) {\n+\t\t\t\tif (SanityManager.DEBUG) {\n+\t\t\t\t\tSanityManager.THROWASSERT(\"Unexpected ClassNotFoundException\",\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcnfe);\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\tci = new ClassInfo(nodeClass);\n+\t\t\tnodeCi[nodeType] = ci;\n+\t\t}\n+\n+\t\tQueryTreeNode retval;\n+\n+\t\ttry {\n+\t\t\tretval = (QueryTreeNode) ci.getNewInstance();\n+\t\t\t//retval = (QueryTreeNode) nodeClass.newInstance();\n+\t\t} catch (Exception iae) {\n+\t\t\tthrow new RuntimeException(iae);\n+\t\t}\n+\n+\t\tretval.setContextManager(cm);\n+\t\tretval.setNodeType(nodeType);\n+\n+\t\treturn retval;\n+\t}\n+\n+\t/**\n+\t * Translate a node type from C_NodeTypes to a class name\n+\t *\n+\t * @param nodeType\tA node type identifier from C_NodeTypes\n+\t *\n+\t * @exception StandardException\t\tThrown on error\n+\t */\n+\tprotected String nodeName(int nodeType)\n+\t\t\tthrows StandardException\n+\t{\n+\t\tswitch (nodeType)\n+\t\t{\n+\t\t  // WARNING: WHEN ADDING NODE TYPES HERE, YOU MUST ALSO ADD\n+\t\t  // THEM TO tools/jar/DBMSnode.properties\n+\t\t\t// xxxRESOLVE: why not make this a giant array and simply index into\n+\t\t\t// it? manish Thu Feb 22 14:49:41 PST 2001\n+\t\t  case C_NodeTypes.CURRENT_ROW_LOCATION_NODE:\n+\t\t  \treturn C_NodeNames.CURRENT_ROW_LOCATION_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.GROUP_BY_LIST:\n+\t\t  \treturn C_NodeNames.GROUP_BY_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.ORDER_BY_LIST:\n+\t\t  \treturn C_NodeNames.ORDER_BY_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.PREDICATE_LIST:\n+\t\t  \treturn C_NodeNames.PREDICATE_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.RESULT_COLUMN_LIST:\n+\t\t  \treturn C_NodeNames.RESULT_COLUMN_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.SUBQUERY_LIST:\n+\t\t  \treturn C_NodeNames.SUBQUERY_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.TABLE_ELEMENT_LIST:\n+\t\t  \treturn C_NodeNames.TABLE_ELEMENT_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.UNTYPED_NULL_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.UNTYPED_NULL_CONSTANT_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.TABLE_ELEMENT_NODE:\n+\t\t  \treturn C_NodeNames.TABLE_ELEMENT_NODE_NAME;\n \n-    /**\n-     @see Monitor\n-     @exception StandardException Ooops\n-     */\n-\n-    public void boot(boolean create, Properties startParams) throws StandardException {\n-        /*\n-         ** This system property determines whether to optimize join order\n-         ** by default.  It is used mainly for testing - there are many tests\n-         ** that assume the join order is fixed.\n-         */\n-        String opt = PropertyUtil.getSystemProperty(Optimizer.JOIN_ORDER_OPTIMIZATION);\n-        if (opt != null) {\n-            joinOrderOptimization = Boolean.valueOf(opt);\n-        }\n-    }\n-\n-    /**\n-     @see Monitor\n-     */\n-\n-    public void stop() {\n-    }\n-\n-    /**\n-     Every Module needs a public niladic constructor. It just does.\n-     */\n-    public    SpliceNodeFactoryImpl() {}\n-\n-    /** @see NodeFactory#doJoinOrderOptimization */\n-    public Boolean doJoinOrderOptimization() {\n-        return joinOrderOptimization;\n-    }\n-\n-    /**\n-     * @see NodeFactory#getNode\n-     *\n-     * @exception StandardException        Thrown on error\n-     */\n-    public Node getNode(int nodeType, ContextManager cm) throws StandardException {\n-\n-        ClassInfo ci = nodeCi[nodeType];\n-        Class nodeClass = null;\n-        if (ci == null) {\n-            String nodeName = nodeName(nodeType);\n-            try {\n-                nodeClass = Class.forName(nodeName);\n-            }\n-            catch (ClassNotFoundException cnfe) {\n-                if (SanityManager.DEBUG) {\n-                    SanityManager.THROWASSERT(\"Unexpected ClassNotFoundException\",\n-                            cnfe);\n-                }\n-            }\n-\n-            ci = new ClassInfo(nodeClass);\n-            nodeCi[nodeType] = ci;\n-        }\n-\n-        QueryTreeNode retval;\n-\n-        try {\n-            retval = (QueryTreeNode) ci.getNewInstance();\n-            //retval = (QueryTreeNode) nodeClass.newInstance();\n-        } catch (Exception iae) {\n-            throw new RuntimeException(iae);\n-        }\n-\n-        retval.setContextManager(cm);\n-        retval.setNodeType(nodeType);\n-\n-        return retval;\n-    }\n-\n-    /**\n-     * Translate a node type from C_NodeTypes to a class name\n-     *\n-     * @param nodeType    A node type identifier from C_NodeTypes\n-     *\n-     * @exception StandardException        Thrown on error\n-     */\n-    protected String nodeName(int nodeType)\n-            throws StandardException\n-    {\n-        switch (nodeType)\n-        {\n-            // WARNING: WHEN ADDING NODE TYPES HERE, YOU MUST ALSO ADD\n-            // THEM TO tools/jar/DBMSnode.properties\n-            // xxxRESOLVE: why not make this a giant array and simply index into\n-            // it? manish Thu Feb 22 14:49:41 PST 2001\n-            case C_NodeTypes.CURRENT_ROW_LOCATION_NODE:\n-                return C_NodeNames.CURRENT_ROW_LOCATION_NODE_NAME;\n-\n-            case C_NodeTypes.GROUP_BY_LIST:\n-                return C_NodeNames.GROUP_BY_LIST_NAME;\n+\t\t  case C_NodeTypes.VALUE_NODE_LIST:\n+\t\t  \treturn C_NodeNames.VALUE_NODE_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.ALL_RESULT_COLUMN:\n+\t\t  \treturn C_NodeNames.ALL_RESULT_COLUMN_NAME;\n+\n+\t\t  case C_NodeTypes.GET_CURRENT_CONNECTION_NODE:\n+\t\t  \treturn C_NodeNames.GET_CURRENT_CONNECTION_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.NOP_STATEMENT_NODE:\n+\t\t  \treturn C_NodeNames.NOP_STATEMENT_NODE_NAME;\n \n-            case C_NodeTypes.ORDER_BY_LIST:\n-                return C_NodeNames.ORDER_BY_LIST_NAME;\n+\t\t  case C_NodeTypes.SET_TRANSACTION_ISOLATION_NODE:\n+\t\t  \treturn C_NodeNames.SET_TRANSACTION_ISOLATION_NODE_NAME;\n \n-            case C_NodeTypes.PREDICATE_LIST:\n-                return C_NodeNames.PREDICATE_LIST_NAME;\n+\t\t  case C_NodeTypes.CHAR_LENGTH_OPERATOR_NODE:\n+\t\t\treturn C_NodeNames.LENGTH_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.RESULT_COLUMN_LIST:\n-                return C_NodeNames.RESULT_COLUMN_LIST_NAME;\n+\t\t  // ISNOTNULL compressed into ISNULL\n+\t\t  case C_NodeTypes.IS_NOT_NULL_NODE:\n+\t\t  case C_NodeTypes.IS_NULL_NODE:\n+\t\t  \treturn C_NodeNames.IS_NULL_NODE_NAME;\n \n-            case C_NodeTypes.SUBQUERY_LIST:\n-                return C_NodeNames.SUBQUERY_LIST_NAME;\n+\t\t  case C_NodeTypes.NOT_NODE:\n+\t\t  \treturn C_NodeNames.NOT_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.SQL_TO_JAVA_VALUE_NODE:\n+\t\t  \treturn C_NodeNames.SQL_TO_JAVA_VALUE_NODE_NAME;\n \n-            case C_NodeTypes.TABLE_ELEMENT_LIST:\n-                return C_NodeNames.TABLE_ELEMENT_LIST_NAME;\n+\t\t  case C_NodeTypes.TABLE_NAME:\n+\t\t  \treturn C_NodeNames.TABLE_NAME_NAME;\n \n-            case C_NodeTypes.UNTYPED_NULL_CONSTANT_NODE:\n-                return C_NodeNames.UNTYPED_NULL_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.GROUP_BY_COLUMN:\n+\t\t  \treturn C_NodeNames.GROUP_BY_COLUMN_NAME;\n \n-            case C_NodeTypes.TABLE_ELEMENT_NODE:\n-                return C_NodeNames.TABLE_ELEMENT_NODE_NAME;\n+\t\t  case C_NodeTypes.JAVA_TO_SQL_VALUE_NODE:\n+\t\t  \treturn C_NodeNames.JAVA_TO_SQL_VALUE_NODE_NAME;\n \n-            case C_NodeTypes.VALUE_TUPLE_NODE:\n-                return C_NodeNames.VALUE_TUPLE_NODE_NAME;\n+\t\t  case C_NodeTypes.FROM_LIST:\n+\t\t  \treturn C_NodeNames.FROM_LIST_NAME;\n \n-            case C_NodeTypes.VALUE_NODE_LIST:\n-                return C_NodeNames.VALUE_NODE_LIST_NAME;\n+\t\t  case C_NodeTypes.VALUE_TUPLE_NODE:\n+\t\t\treturn C_NodeNames.VALUE_TUPLE_NODE_NAME;\n \n-            case C_NodeTypes.ALL_RESULT_COLUMN:\n-                return C_NodeNames.ALL_RESULT_COLUMN_NAME;\n+\t\t  case C_NodeTypes.BOOLEAN_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.BOOLEAN_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.GET_CURRENT_CONNECTION_NODE:\n-                return C_NodeNames.GET_CURRENT_CONNECTION_NODE_NAME;\n+\t\t  case C_NodeTypes.LIST_VALUE_NODE:\n+\t\t    return C_NodeNames.LIST_VALUE_NODE_NAME;\n \n-            case C_NodeTypes.NOP_STATEMENT_NODE:\n-                return C_NodeNames.NOP_STATEMENT_NODE_NAME;\n+\t\t  case C_NodeTypes.AND_NODE:\n+\t\t  \treturn C_NodeNames.AND_NODE_NAME;\n \n-            case C_NodeTypes.SET_TRANSACTION_ISOLATION_NODE:\n-                return C_NodeNames.SET_TRANSACTION_ISOLATION_NODE_NAME;\n+\t\t  case C_NodeTypes.BINARY_EQUALS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_GREATER_EQUALS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_GREATER_THAN_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_LESS_EQUALS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_LESS_THAN_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_NOT_EQUALS_OPERATOR_NODE:\n+\t\t\t  return C_NodeNames.BINARY_RELATIONAL_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.CHAR_LENGTH_OPERATOR_NODE:\n-                return C_NodeNames.LENGTH_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.BINARY_MINUS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_PLUS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_TIMES_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_DIVIDE_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.MOD_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.BINARY_ARITHMETIC_OPERATOR_NODE_NAME;\n \n-            // ISNOTNULL compressed into ISNULL\n-            case C_NodeTypes.IS_NOT_NULL_NODE:\n-            case C_NodeTypes.IS_NULL_NODE:\n-                return C_NodeNames.IS_NULL_NODE_NAME;\n+\t\t  case C_NodeTypes.COALESCE_FUNCTION_NODE:\n+\t\t  \treturn C_NodeNames.COALESCE_FUNCTION_NODE_NAME;\n \n-            case C_NodeTypes.NOT_NODE:\n-                return C_NodeNames.NOT_NODE_NAME;\n+\t\t  case C_NodeTypes.CONCATENATION_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.CONCATENATION_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.SQL_TO_JAVA_VALUE_NODE:\n-                return C_NodeNames.SQL_TO_JAVA_VALUE_NODE_NAME;\n+\t\t  case C_NodeTypes.LIKE_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.LIKE_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.TABLE_NAME:\n-                return C_NodeNames.TABLE_NAME_NAME;\n+\t\t  case C_NodeTypes.OR_NODE:\n+\t\t  \treturn C_NodeNames.OR_NODE_NAME;\n \n-            case C_NodeTypes.GROUP_BY_COLUMN:\n-                return C_NodeNames.GROUP_BY_COLUMN_NAME;\n+\t\t  case C_NodeTypes.BETWEEN_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.BETWEEN_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.JAVA_TO_SQL_VALUE_NODE:\n-                return C_NodeNames.JAVA_TO_SQL_VALUE_NODE_NAME;\n+\t\t  case C_NodeTypes.CONDITIONAL_NODE:\n+\t\t  \treturn C_NodeNames.CONDITIONAL_NODE_NAME;\n \n-            case C_NodeTypes.FROM_LIST:\n-                return C_NodeNames.FROM_LIST_NAME;\n+\t\t  case C_NodeTypes.IN_LIST_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.IN_LIST_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.BOOLEAN_CONSTANT_NODE:\n-                return C_NodeNames.BOOLEAN_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.BIT_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.BIT_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.LIST_VALUE_NODE:\n-                return C_NodeNames.LIST_VALUE_NODE_NAME;\n+\t\t  case C_NodeTypes.LONGVARBIT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.VARBIT_CONSTANT_NODE:\n+          case C_NodeTypes.BLOB_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.VARBIT_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.AND_NODE:\n-                return C_NodeNames.AND_NODE_NAME;\n+\t\t  case C_NodeTypes.CAST_NODE:\n+\t\t  \treturn C_NodeNames.CAST_NODE_NAME;\n \n-            case C_NodeTypes.BINARY_EQUALS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_GREATER_EQUALS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_GREATER_THAN_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_LESS_EQUALS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_LESS_THAN_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_NOT_EQUALS_OPERATOR_NODE:\n-                return C_NodeNames.BINARY_RELATIONAL_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.CHAR_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.LONGVARCHAR_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.VARCHAR_CONSTANT_NODE:\n+          case C_NodeTypes.CLOB_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.CHAR_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.BINARY_MINUS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_PLUS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_TIMES_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_DIVIDE_OPERATOR_NODE:\n-            case C_NodeTypes.MOD_OPERATOR_NODE:\n-                return C_NodeNames.BINARY_ARITHMETIC_OPERATOR_NODE_NAME;\n+          case C_NodeTypes.XML_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.XML_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.COALESCE_FUNCTION_NODE:\n-                return C_NodeNames.COALESCE_FUNCTION_NODE_NAME;\n+\t\t  case C_NodeTypes.COLUMN_REFERENCE:\n+\t\t  \treturn C_NodeNames.COLUMN_REFERENCE_NAME;\n \n-            case C_NodeTypes.CONCATENATION_OPERATOR_NODE:\n-                return C_NodeNames.CONCATENATION_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_INDEX_NODE:\n+\t\t  \treturn C_NodeNames.DROP_INDEX_NODE_NAME;\n \n-            case C_NodeTypes.LIKE_OPERATOR_NODE:\n-                return C_NodeNames.LIKE_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_TRIGGER_NODE:\n+\t\t  \treturn C_NodeNames.DROP_TRIGGER_NODE_NAME;\n \n-            case C_NodeTypes.OR_NODE:\n-                return C_NodeNames.OR_NODE_NAME;\n+\t\t  case C_NodeTypes.TINYINT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.SMALLINT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.INT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.LONGINT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.DECIMAL_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.DOUBLE_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.FLOAT_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.NUMERIC_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.BETWEEN_OPERATOR_NODE:\n-                return C_NodeNames.BETWEEN_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.USERTYPE_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.USERTYPE_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.CONDITIONAL_NODE:\n-                return C_NodeNames.CONDITIONAL_NODE_NAME;\n+\t\t  case C_NodeTypes.PREDICATE:\n+\t\t  \treturn C_NodeNames.PREDICATE_NAME;\n \n-            case C_NodeTypes.IN_LIST_OPERATOR_NODE:\n-                return C_NodeNames.IN_LIST_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.RESULT_COLUMN:\n+\t\t  \treturn C_NodeNames.RESULT_COLUMN_NAME;\n \n-            case C_NodeTypes.BIT_CONSTANT_NODE:\n-                return C_NodeNames.BIT_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.SET_ROLE_NODE:\n+\t\t  \treturn C_NodeNames.SET_ROLE_NODE_NAME;\n \n-            case C_NodeTypes.LONGVARBIT_CONSTANT_NODE:\n-            case C_NodeTypes.VARBIT_CONSTANT_NODE:\n-            case C_NodeTypes.BLOB_CONSTANT_NODE:\n-                return C_NodeNames.VARBIT_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.SET_SCHEMA_NODE:\n+\t\t  \treturn C_NodeNames.SET_SCHEMA_NODE_NAME;\n \n-            case C_NodeTypes.CAST_NODE:\n-                return C_NodeNames.CAST_NODE_NAME;\n+\t\t  case C_NodeTypes.SIMPLE_STRING_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.SIMPLE_STRING_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.CHAR_CONSTANT_NODE:\n-            case C_NodeTypes.LONGVARCHAR_CONSTANT_NODE:\n-            case C_NodeTypes.VARCHAR_CONSTANT_NODE:\n-            case C_NodeTypes.CLOB_CONSTANT_NODE:\n-                return C_NodeNames.CHAR_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.SIMPLE_LOCALE_STRING_OPERATOR_NODE:\n+\t\t\t\treturn C_NodeNames.SIMPLE_LOCALE_STRING_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.XML_CONSTANT_NODE:\n-                return C_NodeNames.XML_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.STATIC_CLASS_FIELD_REFERENCE_NODE:\n+\t\t  \treturn C_NodeNames.STATIC_CLASS_FIELD_REFERENCE_NODE_NAME;\n \n-            case C_NodeTypes.COLUMN_REFERENCE:\n-                return C_NodeNames.COLUMN_REFERENCE_NAME;\n+\t\t  case C_NodeTypes.STATIC_METHOD_CALL_NODE:\n+\t\t  \treturn C_NodeNames.STATIC_METHOD_CALL_NODE_NAME;\n \n-            case C_NodeTypes.DROP_INDEX_NODE:\n-                return C_NodeNames.DROP_INDEX_NODE_NAME;\n+\t\t  case C_NodeTypes.EXTRACT_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.EXTRACT_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.DROP_TRIGGER_NODE:\n-                return C_NodeNames.DROP_TRIGGER_NODE_NAME;\n+\t\t  case C_NodeTypes.PARAMETER_NODE:\n+\t\t  \treturn C_NodeNames.PARAMETER_NODE_NAME;\n \n-            case C_NodeTypes.TINYINT_CONSTANT_NODE:\n-            case C_NodeTypes.SMALLINT_CONSTANT_NODE:\n-            case C_NodeTypes.INT_CONSTANT_NODE:\n-            case C_NodeTypes.LONGINT_CONSTANT_NODE:\n-            case C_NodeTypes.DECIMAL_CONSTANT_NODE:\n-            case C_NodeTypes.DOUBLE_CONSTANT_NODE:\n-            case C_NodeTypes.FLOAT_CONSTANT_NODE:\n-                return C_NodeNames.NUMERIC_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_SCHEMA_NODE:\n+\t\t  \treturn C_NodeNames.DROP_SCHEMA_NODE_NAME;\n \n-            case C_NodeTypes.USERTYPE_CONSTANT_NODE:\n-                return C_NodeNames.USERTYPE_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_ROLE_NODE:\n+\t\t  \treturn C_NodeNames.DROP_ROLE_NODE_NAME;\n \n-            case C_NodeTypes.PREDICATE:\n-                return C_NodeNames.PREDICATE_NAME;\n+\t\t  case C_NodeTypes.DROP_TABLE_NODE:\n+\t\t  \treturn C_NodeNames.DROP_TABLE_NODE_NAME;\n \n-            case C_NodeTypes.RESULT_COLUMN:\n-                return C_NodeNames.RESULT_COLUMN_NAME;\n+\t\t  case C_NodeTypes.DROP_VIEW_NODE:\n+\t\t  \treturn C_NodeNames.DROP_VIEW_NODE_NAME;\n \n-            case C_NodeTypes.SET_ROLE_NODE:\n-                return C_NodeNames.SET_ROLE_NODE_NAME;\n+\t\t  case C_NodeTypes.SUBQUERY_NODE:\n+\t\t  \treturn C_NodeNames.SUBQUERY_NODE_NAME;\n \n-            case C_NodeTypes.SET_SCHEMA_NODE:\n-                return C_NodeNames.SET_SCHEMA_NODE_NAME;\n+\t\t  case C_NodeTypes.BASE_COLUMN_NODE:\n+\t\t  \treturn C_NodeNames.BASE_COLUMN_NODE_NAME;\n \n-            case C_NodeTypes.SIMPLE_STRING_OPERATOR_NODE:\n-                return C_NodeNames.SIMPLE_STRING_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.CALL_STATEMENT_NODE:\n+\t\t  \treturn C_NodeNames.CALL_STATEMENT_NODE_NAME;\n \n-            case C_NodeTypes.SIMPLE_LOCALE_STRING_OPERATOR_NODE:\n-                return C_NodeNames.SIMPLE_LOCALE_STRING_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.MODIFY_COLUMN_DEFAULT_NODE:\n+          case C_NodeTypes.MODIFY_COLUMN_TYPE_NODE:\n+\t\t  case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NODE:\n+\t\t  case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NOT_NULL_NODE:\n+\t\t  case C_NodeTypes.DROP_COLUMN_NODE:\n+\t\t\treturn C_NodeNames.MODIFY_COLUMN_NODE_NAME;\n \n-            case C_NodeTypes.STATIC_CLASS_FIELD_REFERENCE_NODE:\n-                return C_NodeNames.STATIC_CLASS_FIELD_REFERENCE_NODE_NAME;\n+\t\t  case C_NodeTypes.NON_STATIC_METHOD_CALL_NODE:\n+\t\t  \treturn C_NodeNames.NON_STATIC_METHOD_CALL_NODE_NAME;\n \n-            case C_NodeTypes.STATIC_METHOD_CALL_NODE:\n-                return C_NodeNames.STATIC_METHOD_CALL_NODE_NAME;\n+\t\t  case C_NodeTypes.CURRENT_OF_NODE:\n+\t\t  \treturn C_NodeNames.CURRENT_OF_NODE_NAME;\n \n-            case C_NodeTypes.EXTRACT_OPERATOR_NODE:\n-                return C_NodeNames.EXTRACT_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.DEFAULT_NODE:\n+\t\t  \treturn C_NodeNames.DEFAULT_NODE_NAME;\n \n-            case C_NodeTypes.PARAMETER_NODE:\n-                return C_NodeNames.PARAMETER_NODE_NAME;\n+\t\t  case C_NodeTypes.DELETE_NODE:\n+\t\t  \treturn C_NodeNames.DELETE_NODE_NAME;\n \n-            case C_NodeTypes.DROP_SCHEMA_NODE:\n-                return C_NodeNames.DROP_SCHEMA_NODE_NAME;\n+\t\t  case C_NodeTypes.UPDATE_NODE:\n+\t\t  \treturn C_NodeNames.UPDATE_NODE_NAME;\n \n-            case C_NodeTypes.DROP_ROLE_NODE:\n-                return C_NodeNames.DROP_ROLE_NODE_NAME;\n+\t\t  case C_NodeTypes.ORDER_BY_COLUMN:\n+\t\t  \treturn C_NodeNames.ORDER_BY_COLUMN_NAME;\n \n-            case C_NodeTypes.DROP_TABLE_NODE:\n-                return C_NodeNames.DROP_TABLE_NODE_NAME;\n+\t\t  case C_NodeTypes.ROW_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.ROW_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.DROP_VIEW_NODE:\n-                return C_NodeNames.DROP_VIEW_NODE_NAME;\n+\t\t  case C_NodeTypes.VIRTUAL_COLUMN_NODE:\n+\t\t  \treturn C_NodeNames.VIRTUAL_COLUMN_NODE_NAME;\n \n-            case C_NodeTypes.SUBQUERY_NODE:\n-                return C_NodeNames.SUBQUERY_NODE_NAME;\n+\t\t  case C_NodeTypes.CURRENT_DATETIME_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.CURRENT_DATETIME_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.BASE_COLUMN_NODE:\n-                return C_NodeNames.BASE_COLUMN_NODE_NAME;\n+\t\t  case C_NodeTypes.USER_NODE:\n+\t\t  case C_NodeTypes.CURRENT_USER_NODE:\n+\t\t  case C_NodeTypes.SESSION_USER_NODE:\n+\t\t  case C_NodeTypes.SYSTEM_USER_NODE:\n+\t\t  case C_NodeTypes.CURRENT_ISOLATION_NODE:\n+\t\t  case C_NodeTypes.IDENTITY_VAL_NODE:\n+\t\t  case C_NodeTypes.CURRENT_SCHEMA_NODE:\n+          case C_NodeTypes.CURRENT_ROLE_NODE:\n+\t\t  case C_NodeTypes.CURRENT_SESSION_PROPERTY_NODE:\n+\t\t  case C_NodeTypes.GROUP_USER_NODE:\n+\t\t  \treturn C_NodeNames.SPECIAL_FUNCTION_NODE_NAME;\n \n-            case C_NodeTypes.CALL_STATEMENT_NODE:\n-                return C_NodeNames.CALL_STATEMENT_NODE_NAME;\n+\t\t  case C_NodeTypes.IS_NODE:\n+\t\t  \treturn C_NodeNames.IS_NODE_NAME;\n \n-            case C_NodeTypes.MODIFY_COLUMN_DEFAULT_NODE:\n-            case C_NodeTypes.MODIFY_COLUMN_TYPE_NODE:\n-            case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NODE:\n-            case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NOT_NULL_NODE:\n-            case C_NodeTypes.DROP_COLUMN_NODE:\n-                return C_NodeNames.MODIFY_COLUMN_NODE_NAME;\n+\t\t  case C_NodeTypes.LOCK_TABLE_NODE:\n+\t\t  \treturn C_NodeNames.LOCK_TABLE_NODE_NAME;\n \n-            case C_NodeTypes.NON_STATIC_METHOD_CALL_NODE:\n-                return C_NodeNames.NON_STATIC_METHOD_CALL_NODE_NAME;\n+\t\t  case C_NodeTypes.ALTER_TABLE_NODE:\n+\t\t  \treturn C_NodeNames.ALTER_TABLE_NODE_NAME;\n \n-            case C_NodeTypes.CURRENT_OF_NODE:\n-                return C_NodeNames.CURRENT_OF_NODE_NAME;\n+\t\t  case C_NodeTypes.AGGREGATE_NODE:\n+\t\t  \treturn C_NodeNames.AGGREGATE_NODE_NAME;\n \n-            case C_NodeTypes.DEFAULT_NODE:\n-                return C_NodeNames.DEFAULT_NODE_NAME;\n+\t\t  case C_NodeTypes.COLUMN_DEFINITION_NODE:\n+\t\t  \treturn C_NodeNames.COLUMN_DEFINITION_NODE_NAME;\n \n-            case C_NodeTypes.DELETE_NODE:\n-                return C_NodeNames.DELETE_NODE_NAME;\n+\t\t  case C_NodeTypes.EXEC_SPS_NODE:\n+\t\t  \treturn C_NodeNames.EXEC_SPS_NODE_NAME;\n \n-            case C_NodeTypes.UPDATE_NODE:\n-                return C_NodeNames.UPDATE_NODE_NAME;\n+\t\t  case C_NodeTypes.FK_CONSTRAINT_DEFINITION_NODE:\n+\t\t  \treturn C_NodeNames.FK_CONSTRAINT_DEFINITION_NODE_NAME;\n \n-            case C_NodeTypes.ORDER_BY_COLUMN:\n-                return C_NodeNames.ORDER_BY_COLUMN_NAME;\n+\t\t  case C_NodeTypes.FROM_VTI:\n+\t\t  \treturn C_NodeNames.FROM_VTI_NAME;\n \n-            case C_NodeTypes.ROW_RESULT_SET_NODE:\n-                return C_NodeNames.ROW_RESULT_SET_NODE_NAME;\n+\t\t  case C_NodeTypes.MATERIALIZE_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.MATERIALIZE_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.VIRTUAL_COLUMN_NODE:\n-                return C_NodeNames.VIRTUAL_COLUMN_NODE_NAME;\n+\t\t  case C_NodeTypes.NORMALIZE_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.NORMALIZE_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.CURRENT_DATETIME_OPERATOR_NODE:\n-                return C_NodeNames.CURRENT_DATETIME_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.SCROLL_INSENSITIVE_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.SCROLL_INSENSITIVE_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.USER_NODE:\n-            case C_NodeTypes.CURRENT_USER_NODE:\n-            case C_NodeTypes.SESSION_USER_NODE:\n-            case C_NodeTypes.SYSTEM_USER_NODE:\n-            case C_NodeTypes.CURRENT_ISOLATION_NODE:\n-            case C_NodeTypes.IDENTITY_VAL_NODE:\n-            case C_NodeTypes.CURRENT_SCHEMA_NODE:\n-            case C_NodeTypes.CURRENT_ROLE_NODE:\n-            case C_NodeTypes.CURRENT_SESSION_PROPERTY_NODE:\n-            case C_NodeTypes.GROUP_USER_NODE:\n-            case C_NodeTypes.CURRENT_SERVER_NODE:", "originalCommit": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODk3NDYxMA==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418974610", "bodyText": "Fixed in new commit 96dd889.", "author": "jpanko1", "createdAt": "2020-05-02T15:54:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyNjY2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyNjc2NA==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418126764", "bodyText": "Shouldn't be removed", "author": "dgomezferro", "createdAt": "2020-04-30T16:12:42Z", "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/sql/compile/SpliceNodeFactoryImpl.java", "diffHunk": "@@ -52,546 +52,542 @@\n      * compile system.\n      */\n     @Override\n-    public boolean canSupport(Properties startParams) {\n-        return Monitor.isDesiredType(startParams,EngineType.STANDALONE_DB);\n-    }\n+\tpublic boolean canSupport(Properties startParams) {\n+\t\treturn Monitor.isDesiredType(startParams,EngineType.STANDALONE_DB);\n+\t}\n+\n+\t/**\n+\t\t@see Monitor\n+\t\t@exception StandardException Ooops\n+\t  */\n+\n+\tpublic void boot(boolean create, Properties startParams) throws StandardException {\n+\t\t/*\n+\t\t** This system property determines whether to optimize join order\n+\t\t** by default.  It is used mainly for testing - there are many tests\n+\t\t** that assume the join order is fixed.\n+\t\t*/\n+\t\tString opt = PropertyUtil.getSystemProperty(Optimizer.JOIN_ORDER_OPTIMIZATION);\n+\t\tif (opt != null) {\n+\t\t\tjoinOrderOptimization = Boolean.valueOf(opt);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t\t@see Monitor\n+\t  */\n+\n+\tpublic void stop() {\n+\t}\n+\n+\t/**\n+\t  Every Module needs a public niladic constructor. It just does.\n+\t  */\n+    public\tSpliceNodeFactoryImpl() {}\n+\n+\t/** @see NodeFactory#doJoinOrderOptimization */\n+\tpublic Boolean doJoinOrderOptimization() {\n+\t\treturn joinOrderOptimization;\n+\t}\n+\n+\t/**\n+\t * @see NodeFactory#getNode\n+\t *\n+\t * @exception StandardException\t\tThrown on error\n+\t */\n+\tpublic Node getNode(int nodeType, ContextManager cm) throws StandardException {\n+\n+\t\tClassInfo ci = nodeCi[nodeType];\n+\t\tClass nodeClass = null;\n+\t\tif (ci == null) {\n+\t\t\tString nodeName = nodeName(nodeType);\n+\t\t\ttry {\n+\t\t\t\tnodeClass = Class.forName(nodeName);\n+\t\t\t}\n+\t\t\tcatch (ClassNotFoundException cnfe) {\n+\t\t\t\tif (SanityManager.DEBUG) {\n+\t\t\t\t\tSanityManager.THROWASSERT(\"Unexpected ClassNotFoundException\",\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcnfe);\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\tci = new ClassInfo(nodeClass);\n+\t\t\tnodeCi[nodeType] = ci;\n+\t\t}\n+\n+\t\tQueryTreeNode retval;\n+\n+\t\ttry {\n+\t\t\tretval = (QueryTreeNode) ci.getNewInstance();\n+\t\t\t//retval = (QueryTreeNode) nodeClass.newInstance();\n+\t\t} catch (Exception iae) {\n+\t\t\tthrow new RuntimeException(iae);\n+\t\t}\n+\n+\t\tretval.setContextManager(cm);\n+\t\tretval.setNodeType(nodeType);\n+\n+\t\treturn retval;\n+\t}\n+\n+\t/**\n+\t * Translate a node type from C_NodeTypes to a class name\n+\t *\n+\t * @param nodeType\tA node type identifier from C_NodeTypes\n+\t *\n+\t * @exception StandardException\t\tThrown on error\n+\t */\n+\tprotected String nodeName(int nodeType)\n+\t\t\tthrows StandardException\n+\t{\n+\t\tswitch (nodeType)\n+\t\t{\n+\t\t  // WARNING: WHEN ADDING NODE TYPES HERE, YOU MUST ALSO ADD\n+\t\t  // THEM TO tools/jar/DBMSnode.properties\n+\t\t\t// xxxRESOLVE: why not make this a giant array and simply index into\n+\t\t\t// it? manish Thu Feb 22 14:49:41 PST 2001\n+\t\t  case C_NodeTypes.CURRENT_ROW_LOCATION_NODE:\n+\t\t  \treturn C_NodeNames.CURRENT_ROW_LOCATION_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.GROUP_BY_LIST:\n+\t\t  \treturn C_NodeNames.GROUP_BY_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.ORDER_BY_LIST:\n+\t\t  \treturn C_NodeNames.ORDER_BY_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.PREDICATE_LIST:\n+\t\t  \treturn C_NodeNames.PREDICATE_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.RESULT_COLUMN_LIST:\n+\t\t  \treturn C_NodeNames.RESULT_COLUMN_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.SUBQUERY_LIST:\n+\t\t  \treturn C_NodeNames.SUBQUERY_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.TABLE_ELEMENT_LIST:\n+\t\t  \treturn C_NodeNames.TABLE_ELEMENT_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.UNTYPED_NULL_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.UNTYPED_NULL_CONSTANT_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.TABLE_ELEMENT_NODE:\n+\t\t  \treturn C_NodeNames.TABLE_ELEMENT_NODE_NAME;\n \n-    /**\n-     @see Monitor\n-     @exception StandardException Ooops\n-     */\n-\n-    public void boot(boolean create, Properties startParams) throws StandardException {\n-        /*\n-         ** This system property determines whether to optimize join order\n-         ** by default.  It is used mainly for testing - there are many tests\n-         ** that assume the join order is fixed.\n-         */\n-        String opt = PropertyUtil.getSystemProperty(Optimizer.JOIN_ORDER_OPTIMIZATION);\n-        if (opt != null) {\n-            joinOrderOptimization = Boolean.valueOf(opt);\n-        }\n-    }\n-\n-    /**\n-     @see Monitor\n-     */\n-\n-    public void stop() {\n-    }\n-\n-    /**\n-     Every Module needs a public niladic constructor. It just does.\n-     */\n-    public    SpliceNodeFactoryImpl() {}\n-\n-    /** @see NodeFactory#doJoinOrderOptimization */\n-    public Boolean doJoinOrderOptimization() {\n-        return joinOrderOptimization;\n-    }\n-\n-    /**\n-     * @see NodeFactory#getNode\n-     *\n-     * @exception StandardException        Thrown on error\n-     */\n-    public Node getNode(int nodeType, ContextManager cm) throws StandardException {\n-\n-        ClassInfo ci = nodeCi[nodeType];\n-        Class nodeClass = null;\n-        if (ci == null) {\n-            String nodeName = nodeName(nodeType);\n-            try {\n-                nodeClass = Class.forName(nodeName);\n-            }\n-            catch (ClassNotFoundException cnfe) {\n-                if (SanityManager.DEBUG) {\n-                    SanityManager.THROWASSERT(\"Unexpected ClassNotFoundException\",\n-                            cnfe);\n-                }\n-            }\n-\n-            ci = new ClassInfo(nodeClass);\n-            nodeCi[nodeType] = ci;\n-        }\n-\n-        QueryTreeNode retval;\n-\n-        try {\n-            retval = (QueryTreeNode) ci.getNewInstance();\n-            //retval = (QueryTreeNode) nodeClass.newInstance();\n-        } catch (Exception iae) {\n-            throw new RuntimeException(iae);\n-        }\n-\n-        retval.setContextManager(cm);\n-        retval.setNodeType(nodeType);\n-\n-        return retval;\n-    }\n-\n-    /**\n-     * Translate a node type from C_NodeTypes to a class name\n-     *\n-     * @param nodeType    A node type identifier from C_NodeTypes\n-     *\n-     * @exception StandardException        Thrown on error\n-     */\n-    protected String nodeName(int nodeType)\n-            throws StandardException\n-    {\n-        switch (nodeType)\n-        {\n-            // WARNING: WHEN ADDING NODE TYPES HERE, YOU MUST ALSO ADD\n-            // THEM TO tools/jar/DBMSnode.properties\n-            // xxxRESOLVE: why not make this a giant array and simply index into\n-            // it? manish Thu Feb 22 14:49:41 PST 2001\n-            case C_NodeTypes.CURRENT_ROW_LOCATION_NODE:\n-                return C_NodeNames.CURRENT_ROW_LOCATION_NODE_NAME;\n-\n-            case C_NodeTypes.GROUP_BY_LIST:\n-                return C_NodeNames.GROUP_BY_LIST_NAME;\n+\t\t  case C_NodeTypes.VALUE_NODE_LIST:\n+\t\t  \treturn C_NodeNames.VALUE_NODE_LIST_NAME;\n+\n+\t\t  case C_NodeTypes.ALL_RESULT_COLUMN:\n+\t\t  \treturn C_NodeNames.ALL_RESULT_COLUMN_NAME;\n+\n+\t\t  case C_NodeTypes.GET_CURRENT_CONNECTION_NODE:\n+\t\t  \treturn C_NodeNames.GET_CURRENT_CONNECTION_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.NOP_STATEMENT_NODE:\n+\t\t  \treturn C_NodeNames.NOP_STATEMENT_NODE_NAME;\n \n-            case C_NodeTypes.ORDER_BY_LIST:\n-                return C_NodeNames.ORDER_BY_LIST_NAME;\n+\t\t  case C_NodeTypes.SET_TRANSACTION_ISOLATION_NODE:\n+\t\t  \treturn C_NodeNames.SET_TRANSACTION_ISOLATION_NODE_NAME;\n \n-            case C_NodeTypes.PREDICATE_LIST:\n-                return C_NodeNames.PREDICATE_LIST_NAME;\n+\t\t  case C_NodeTypes.CHAR_LENGTH_OPERATOR_NODE:\n+\t\t\treturn C_NodeNames.LENGTH_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.RESULT_COLUMN_LIST:\n-                return C_NodeNames.RESULT_COLUMN_LIST_NAME;\n+\t\t  // ISNOTNULL compressed into ISNULL\n+\t\t  case C_NodeTypes.IS_NOT_NULL_NODE:\n+\t\t  case C_NodeTypes.IS_NULL_NODE:\n+\t\t  \treturn C_NodeNames.IS_NULL_NODE_NAME;\n \n-            case C_NodeTypes.SUBQUERY_LIST:\n-                return C_NodeNames.SUBQUERY_LIST_NAME;\n+\t\t  case C_NodeTypes.NOT_NODE:\n+\t\t  \treturn C_NodeNames.NOT_NODE_NAME;\n+\n+\t\t  case C_NodeTypes.SQL_TO_JAVA_VALUE_NODE:\n+\t\t  \treturn C_NodeNames.SQL_TO_JAVA_VALUE_NODE_NAME;\n \n-            case C_NodeTypes.TABLE_ELEMENT_LIST:\n-                return C_NodeNames.TABLE_ELEMENT_LIST_NAME;\n+\t\t  case C_NodeTypes.TABLE_NAME:\n+\t\t  \treturn C_NodeNames.TABLE_NAME_NAME;\n \n-            case C_NodeTypes.UNTYPED_NULL_CONSTANT_NODE:\n-                return C_NodeNames.UNTYPED_NULL_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.GROUP_BY_COLUMN:\n+\t\t  \treturn C_NodeNames.GROUP_BY_COLUMN_NAME;\n \n-            case C_NodeTypes.TABLE_ELEMENT_NODE:\n-                return C_NodeNames.TABLE_ELEMENT_NODE_NAME;\n+\t\t  case C_NodeTypes.JAVA_TO_SQL_VALUE_NODE:\n+\t\t  \treturn C_NodeNames.JAVA_TO_SQL_VALUE_NODE_NAME;\n \n-            case C_NodeTypes.VALUE_TUPLE_NODE:\n-                return C_NodeNames.VALUE_TUPLE_NODE_NAME;\n+\t\t  case C_NodeTypes.FROM_LIST:\n+\t\t  \treturn C_NodeNames.FROM_LIST_NAME;\n \n-            case C_NodeTypes.VALUE_NODE_LIST:\n-                return C_NodeNames.VALUE_NODE_LIST_NAME;\n+\t\t  case C_NodeTypes.VALUE_TUPLE_NODE:\n+\t\t\treturn C_NodeNames.VALUE_TUPLE_NODE_NAME;\n \n-            case C_NodeTypes.ALL_RESULT_COLUMN:\n-                return C_NodeNames.ALL_RESULT_COLUMN_NAME;\n+\t\t  case C_NodeTypes.BOOLEAN_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.BOOLEAN_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.GET_CURRENT_CONNECTION_NODE:\n-                return C_NodeNames.GET_CURRENT_CONNECTION_NODE_NAME;\n+\t\t  case C_NodeTypes.LIST_VALUE_NODE:\n+\t\t    return C_NodeNames.LIST_VALUE_NODE_NAME;\n \n-            case C_NodeTypes.NOP_STATEMENT_NODE:\n-                return C_NodeNames.NOP_STATEMENT_NODE_NAME;\n+\t\t  case C_NodeTypes.AND_NODE:\n+\t\t  \treturn C_NodeNames.AND_NODE_NAME;\n \n-            case C_NodeTypes.SET_TRANSACTION_ISOLATION_NODE:\n-                return C_NodeNames.SET_TRANSACTION_ISOLATION_NODE_NAME;\n+\t\t  case C_NodeTypes.BINARY_EQUALS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_GREATER_EQUALS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_GREATER_THAN_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_LESS_EQUALS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_LESS_THAN_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_NOT_EQUALS_OPERATOR_NODE:\n+\t\t\t  return C_NodeNames.BINARY_RELATIONAL_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.CHAR_LENGTH_OPERATOR_NODE:\n-                return C_NodeNames.LENGTH_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.BINARY_MINUS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_PLUS_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_TIMES_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.BINARY_DIVIDE_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.MOD_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.BINARY_ARITHMETIC_OPERATOR_NODE_NAME;\n \n-            // ISNOTNULL compressed into ISNULL\n-            case C_NodeTypes.IS_NOT_NULL_NODE:\n-            case C_NodeTypes.IS_NULL_NODE:\n-                return C_NodeNames.IS_NULL_NODE_NAME;\n+\t\t  case C_NodeTypes.COALESCE_FUNCTION_NODE:\n+\t\t  \treturn C_NodeNames.COALESCE_FUNCTION_NODE_NAME;\n \n-            case C_NodeTypes.NOT_NODE:\n-                return C_NodeNames.NOT_NODE_NAME;\n+\t\t  case C_NodeTypes.CONCATENATION_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.CONCATENATION_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.SQL_TO_JAVA_VALUE_NODE:\n-                return C_NodeNames.SQL_TO_JAVA_VALUE_NODE_NAME;\n+\t\t  case C_NodeTypes.LIKE_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.LIKE_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.TABLE_NAME:\n-                return C_NodeNames.TABLE_NAME_NAME;\n+\t\t  case C_NodeTypes.OR_NODE:\n+\t\t  \treturn C_NodeNames.OR_NODE_NAME;\n \n-            case C_NodeTypes.GROUP_BY_COLUMN:\n-                return C_NodeNames.GROUP_BY_COLUMN_NAME;\n+\t\t  case C_NodeTypes.BETWEEN_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.BETWEEN_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.JAVA_TO_SQL_VALUE_NODE:\n-                return C_NodeNames.JAVA_TO_SQL_VALUE_NODE_NAME;\n+\t\t  case C_NodeTypes.CONDITIONAL_NODE:\n+\t\t  \treturn C_NodeNames.CONDITIONAL_NODE_NAME;\n \n-            case C_NodeTypes.FROM_LIST:\n-                return C_NodeNames.FROM_LIST_NAME;\n+\t\t  case C_NodeTypes.IN_LIST_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.IN_LIST_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.BOOLEAN_CONSTANT_NODE:\n-                return C_NodeNames.BOOLEAN_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.BIT_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.BIT_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.LIST_VALUE_NODE:\n-                return C_NodeNames.LIST_VALUE_NODE_NAME;\n+\t\t  case C_NodeTypes.LONGVARBIT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.VARBIT_CONSTANT_NODE:\n+          case C_NodeTypes.BLOB_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.VARBIT_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.AND_NODE:\n-                return C_NodeNames.AND_NODE_NAME;\n+\t\t  case C_NodeTypes.CAST_NODE:\n+\t\t  \treturn C_NodeNames.CAST_NODE_NAME;\n \n-            case C_NodeTypes.BINARY_EQUALS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_GREATER_EQUALS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_GREATER_THAN_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_LESS_EQUALS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_LESS_THAN_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_NOT_EQUALS_OPERATOR_NODE:\n-                return C_NodeNames.BINARY_RELATIONAL_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.CHAR_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.LONGVARCHAR_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.VARCHAR_CONSTANT_NODE:\n+          case C_NodeTypes.CLOB_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.CHAR_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.BINARY_MINUS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_PLUS_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_TIMES_OPERATOR_NODE:\n-            case C_NodeTypes.BINARY_DIVIDE_OPERATOR_NODE:\n-            case C_NodeTypes.MOD_OPERATOR_NODE:\n-                return C_NodeNames.BINARY_ARITHMETIC_OPERATOR_NODE_NAME;\n+          case C_NodeTypes.XML_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.XML_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.COALESCE_FUNCTION_NODE:\n-                return C_NodeNames.COALESCE_FUNCTION_NODE_NAME;\n+\t\t  case C_NodeTypes.COLUMN_REFERENCE:\n+\t\t  \treturn C_NodeNames.COLUMN_REFERENCE_NAME;\n \n-            case C_NodeTypes.CONCATENATION_OPERATOR_NODE:\n-                return C_NodeNames.CONCATENATION_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_INDEX_NODE:\n+\t\t  \treturn C_NodeNames.DROP_INDEX_NODE_NAME;\n \n-            case C_NodeTypes.LIKE_OPERATOR_NODE:\n-                return C_NodeNames.LIKE_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_TRIGGER_NODE:\n+\t\t  \treturn C_NodeNames.DROP_TRIGGER_NODE_NAME;\n \n-            case C_NodeTypes.OR_NODE:\n-                return C_NodeNames.OR_NODE_NAME;\n+\t\t  case C_NodeTypes.TINYINT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.SMALLINT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.INT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.LONGINT_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.DECIMAL_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.DOUBLE_CONSTANT_NODE:\n+\t\t  case C_NodeTypes.FLOAT_CONSTANT_NODE:\n+\t\t\treturn C_NodeNames.NUMERIC_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.BETWEEN_OPERATOR_NODE:\n-                return C_NodeNames.BETWEEN_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.USERTYPE_CONSTANT_NODE:\n+\t\t  \treturn C_NodeNames.USERTYPE_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.CONDITIONAL_NODE:\n-                return C_NodeNames.CONDITIONAL_NODE_NAME;\n+\t\t  case C_NodeTypes.PREDICATE:\n+\t\t  \treturn C_NodeNames.PREDICATE_NAME;\n \n-            case C_NodeTypes.IN_LIST_OPERATOR_NODE:\n-                return C_NodeNames.IN_LIST_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.RESULT_COLUMN:\n+\t\t  \treturn C_NodeNames.RESULT_COLUMN_NAME;\n \n-            case C_NodeTypes.BIT_CONSTANT_NODE:\n-                return C_NodeNames.BIT_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.SET_ROLE_NODE:\n+\t\t  \treturn C_NodeNames.SET_ROLE_NODE_NAME;\n \n-            case C_NodeTypes.LONGVARBIT_CONSTANT_NODE:\n-            case C_NodeTypes.VARBIT_CONSTANT_NODE:\n-            case C_NodeTypes.BLOB_CONSTANT_NODE:\n-                return C_NodeNames.VARBIT_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.SET_SCHEMA_NODE:\n+\t\t  \treturn C_NodeNames.SET_SCHEMA_NODE_NAME;\n \n-            case C_NodeTypes.CAST_NODE:\n-                return C_NodeNames.CAST_NODE_NAME;\n+\t\t  case C_NodeTypes.SIMPLE_STRING_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.SIMPLE_STRING_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.CHAR_CONSTANT_NODE:\n-            case C_NodeTypes.LONGVARCHAR_CONSTANT_NODE:\n-            case C_NodeTypes.VARCHAR_CONSTANT_NODE:\n-            case C_NodeTypes.CLOB_CONSTANT_NODE:\n-                return C_NodeNames.CHAR_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.SIMPLE_LOCALE_STRING_OPERATOR_NODE:\n+\t\t\t\treturn C_NodeNames.SIMPLE_LOCALE_STRING_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.XML_CONSTANT_NODE:\n-                return C_NodeNames.XML_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.STATIC_CLASS_FIELD_REFERENCE_NODE:\n+\t\t  \treturn C_NodeNames.STATIC_CLASS_FIELD_REFERENCE_NODE_NAME;\n \n-            case C_NodeTypes.COLUMN_REFERENCE:\n-                return C_NodeNames.COLUMN_REFERENCE_NAME;\n+\t\t  case C_NodeTypes.STATIC_METHOD_CALL_NODE:\n+\t\t  \treturn C_NodeNames.STATIC_METHOD_CALL_NODE_NAME;\n \n-            case C_NodeTypes.DROP_INDEX_NODE:\n-                return C_NodeNames.DROP_INDEX_NODE_NAME;\n+\t\t  case C_NodeTypes.EXTRACT_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.EXTRACT_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.DROP_TRIGGER_NODE:\n-                return C_NodeNames.DROP_TRIGGER_NODE_NAME;\n+\t\t  case C_NodeTypes.PARAMETER_NODE:\n+\t\t  \treturn C_NodeNames.PARAMETER_NODE_NAME;\n \n-            case C_NodeTypes.TINYINT_CONSTANT_NODE:\n-            case C_NodeTypes.SMALLINT_CONSTANT_NODE:\n-            case C_NodeTypes.INT_CONSTANT_NODE:\n-            case C_NodeTypes.LONGINT_CONSTANT_NODE:\n-            case C_NodeTypes.DECIMAL_CONSTANT_NODE:\n-            case C_NodeTypes.DOUBLE_CONSTANT_NODE:\n-            case C_NodeTypes.FLOAT_CONSTANT_NODE:\n-                return C_NodeNames.NUMERIC_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_SCHEMA_NODE:\n+\t\t  \treturn C_NodeNames.DROP_SCHEMA_NODE_NAME;\n \n-            case C_NodeTypes.USERTYPE_CONSTANT_NODE:\n-                return C_NodeNames.USERTYPE_CONSTANT_NODE_NAME;\n+\t\t  case C_NodeTypes.DROP_ROLE_NODE:\n+\t\t  \treturn C_NodeNames.DROP_ROLE_NODE_NAME;\n \n-            case C_NodeTypes.PREDICATE:\n-                return C_NodeNames.PREDICATE_NAME;\n+\t\t  case C_NodeTypes.DROP_TABLE_NODE:\n+\t\t  \treturn C_NodeNames.DROP_TABLE_NODE_NAME;\n \n-            case C_NodeTypes.RESULT_COLUMN:\n-                return C_NodeNames.RESULT_COLUMN_NAME;\n+\t\t  case C_NodeTypes.DROP_VIEW_NODE:\n+\t\t  \treturn C_NodeNames.DROP_VIEW_NODE_NAME;\n \n-            case C_NodeTypes.SET_ROLE_NODE:\n-                return C_NodeNames.SET_ROLE_NODE_NAME;\n+\t\t  case C_NodeTypes.SUBQUERY_NODE:\n+\t\t  \treturn C_NodeNames.SUBQUERY_NODE_NAME;\n \n-            case C_NodeTypes.SET_SCHEMA_NODE:\n-                return C_NodeNames.SET_SCHEMA_NODE_NAME;\n+\t\t  case C_NodeTypes.BASE_COLUMN_NODE:\n+\t\t  \treturn C_NodeNames.BASE_COLUMN_NODE_NAME;\n \n-            case C_NodeTypes.SIMPLE_STRING_OPERATOR_NODE:\n-                return C_NodeNames.SIMPLE_STRING_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.CALL_STATEMENT_NODE:\n+\t\t  \treturn C_NodeNames.CALL_STATEMENT_NODE_NAME;\n \n-            case C_NodeTypes.SIMPLE_LOCALE_STRING_OPERATOR_NODE:\n-                return C_NodeNames.SIMPLE_LOCALE_STRING_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.MODIFY_COLUMN_DEFAULT_NODE:\n+          case C_NodeTypes.MODIFY_COLUMN_TYPE_NODE:\n+\t\t  case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NODE:\n+\t\t  case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NOT_NULL_NODE:\n+\t\t  case C_NodeTypes.DROP_COLUMN_NODE:\n+\t\t\treturn C_NodeNames.MODIFY_COLUMN_NODE_NAME;\n \n-            case C_NodeTypes.STATIC_CLASS_FIELD_REFERENCE_NODE:\n-                return C_NodeNames.STATIC_CLASS_FIELD_REFERENCE_NODE_NAME;\n+\t\t  case C_NodeTypes.NON_STATIC_METHOD_CALL_NODE:\n+\t\t  \treturn C_NodeNames.NON_STATIC_METHOD_CALL_NODE_NAME;\n \n-            case C_NodeTypes.STATIC_METHOD_CALL_NODE:\n-                return C_NodeNames.STATIC_METHOD_CALL_NODE_NAME;\n+\t\t  case C_NodeTypes.CURRENT_OF_NODE:\n+\t\t  \treturn C_NodeNames.CURRENT_OF_NODE_NAME;\n \n-            case C_NodeTypes.EXTRACT_OPERATOR_NODE:\n-                return C_NodeNames.EXTRACT_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.DEFAULT_NODE:\n+\t\t  \treturn C_NodeNames.DEFAULT_NODE_NAME;\n \n-            case C_NodeTypes.PARAMETER_NODE:\n-                return C_NodeNames.PARAMETER_NODE_NAME;\n+\t\t  case C_NodeTypes.DELETE_NODE:\n+\t\t  \treturn C_NodeNames.DELETE_NODE_NAME;\n \n-            case C_NodeTypes.DROP_SCHEMA_NODE:\n-                return C_NodeNames.DROP_SCHEMA_NODE_NAME;\n+\t\t  case C_NodeTypes.UPDATE_NODE:\n+\t\t  \treturn C_NodeNames.UPDATE_NODE_NAME;\n \n-            case C_NodeTypes.DROP_ROLE_NODE:\n-                return C_NodeNames.DROP_ROLE_NODE_NAME;\n+\t\t  case C_NodeTypes.ORDER_BY_COLUMN:\n+\t\t  \treturn C_NodeNames.ORDER_BY_COLUMN_NAME;\n \n-            case C_NodeTypes.DROP_TABLE_NODE:\n-                return C_NodeNames.DROP_TABLE_NODE_NAME;\n+\t\t  case C_NodeTypes.ROW_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.ROW_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.DROP_VIEW_NODE:\n-                return C_NodeNames.DROP_VIEW_NODE_NAME;\n+\t\t  case C_NodeTypes.VIRTUAL_COLUMN_NODE:\n+\t\t  \treturn C_NodeNames.VIRTUAL_COLUMN_NODE_NAME;\n \n-            case C_NodeTypes.SUBQUERY_NODE:\n-                return C_NodeNames.SUBQUERY_NODE_NAME;\n+\t\t  case C_NodeTypes.CURRENT_DATETIME_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.CURRENT_DATETIME_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.BASE_COLUMN_NODE:\n-                return C_NodeNames.BASE_COLUMN_NODE_NAME;\n+\t\t  case C_NodeTypes.USER_NODE:\n+\t\t  case C_NodeTypes.CURRENT_USER_NODE:\n+\t\t  case C_NodeTypes.SESSION_USER_NODE:\n+\t\t  case C_NodeTypes.SYSTEM_USER_NODE:\n+\t\t  case C_NodeTypes.CURRENT_ISOLATION_NODE:\n+\t\t  case C_NodeTypes.IDENTITY_VAL_NODE:\n+\t\t  case C_NodeTypes.CURRENT_SCHEMA_NODE:\n+          case C_NodeTypes.CURRENT_ROLE_NODE:\n+\t\t  case C_NodeTypes.CURRENT_SESSION_PROPERTY_NODE:\n+\t\t  case C_NodeTypes.GROUP_USER_NODE:\n+\t\t  \treturn C_NodeNames.SPECIAL_FUNCTION_NODE_NAME;\n \n-            case C_NodeTypes.CALL_STATEMENT_NODE:\n-                return C_NodeNames.CALL_STATEMENT_NODE_NAME;\n+\t\t  case C_NodeTypes.IS_NODE:\n+\t\t  \treturn C_NodeNames.IS_NODE_NAME;\n \n-            case C_NodeTypes.MODIFY_COLUMN_DEFAULT_NODE:\n-            case C_NodeTypes.MODIFY_COLUMN_TYPE_NODE:\n-            case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NODE:\n-            case C_NodeTypes.MODIFY_COLUMN_CONSTRAINT_NOT_NULL_NODE:\n-            case C_NodeTypes.DROP_COLUMN_NODE:\n-                return C_NodeNames.MODIFY_COLUMN_NODE_NAME;\n+\t\t  case C_NodeTypes.LOCK_TABLE_NODE:\n+\t\t  \treturn C_NodeNames.LOCK_TABLE_NODE_NAME;\n \n-            case C_NodeTypes.NON_STATIC_METHOD_CALL_NODE:\n-                return C_NodeNames.NON_STATIC_METHOD_CALL_NODE_NAME;\n+\t\t  case C_NodeTypes.ALTER_TABLE_NODE:\n+\t\t  \treturn C_NodeNames.ALTER_TABLE_NODE_NAME;\n \n-            case C_NodeTypes.CURRENT_OF_NODE:\n-                return C_NodeNames.CURRENT_OF_NODE_NAME;\n+\t\t  case C_NodeTypes.AGGREGATE_NODE:\n+\t\t  \treturn C_NodeNames.AGGREGATE_NODE_NAME;\n \n-            case C_NodeTypes.DEFAULT_NODE:\n-                return C_NodeNames.DEFAULT_NODE_NAME;\n+\t\t  case C_NodeTypes.COLUMN_DEFINITION_NODE:\n+\t\t  \treturn C_NodeNames.COLUMN_DEFINITION_NODE_NAME;\n \n-            case C_NodeTypes.DELETE_NODE:\n-                return C_NodeNames.DELETE_NODE_NAME;\n+\t\t  case C_NodeTypes.EXEC_SPS_NODE:\n+\t\t  \treturn C_NodeNames.EXEC_SPS_NODE_NAME;\n \n-            case C_NodeTypes.UPDATE_NODE:\n-                return C_NodeNames.UPDATE_NODE_NAME;\n+\t\t  case C_NodeTypes.FK_CONSTRAINT_DEFINITION_NODE:\n+\t\t  \treturn C_NodeNames.FK_CONSTRAINT_DEFINITION_NODE_NAME;\n \n-            case C_NodeTypes.ORDER_BY_COLUMN:\n-                return C_NodeNames.ORDER_BY_COLUMN_NAME;\n+\t\t  case C_NodeTypes.FROM_VTI:\n+\t\t  \treturn C_NodeNames.FROM_VTI_NAME;\n \n-            case C_NodeTypes.ROW_RESULT_SET_NODE:\n-                return C_NodeNames.ROW_RESULT_SET_NODE_NAME;\n+\t\t  case C_NodeTypes.MATERIALIZE_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.MATERIALIZE_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.VIRTUAL_COLUMN_NODE:\n-                return C_NodeNames.VIRTUAL_COLUMN_NODE_NAME;\n+\t\t  case C_NodeTypes.NORMALIZE_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.NORMALIZE_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.CURRENT_DATETIME_OPERATOR_NODE:\n-                return C_NodeNames.CURRENT_DATETIME_OPERATOR_NODE_NAME;\n+\t\t  case C_NodeTypes.SCROLL_INSENSITIVE_RESULT_SET_NODE:\n+\t\t  \treturn C_NodeNames.SCROLL_INSENSITIVE_RESULT_SET_NODE_NAME;\n \n-            case C_NodeTypes.USER_NODE:\n-            case C_NodeTypes.CURRENT_USER_NODE:\n-            case C_NodeTypes.SESSION_USER_NODE:\n-            case C_NodeTypes.SYSTEM_USER_NODE:\n-            case C_NodeTypes.CURRENT_ISOLATION_NODE:\n-            case C_NodeTypes.IDENTITY_VAL_NODE:\n-            case C_NodeTypes.CURRENT_SCHEMA_NODE:\n-            case C_NodeTypes.CURRENT_ROLE_NODE:\n-            case C_NodeTypes.CURRENT_SESSION_PROPERTY_NODE:\n-            case C_NodeTypes.GROUP_USER_NODE:\n-            case C_NodeTypes.CURRENT_SERVER_NODE:\n-                return C_NodeNames.SPECIAL_FUNCTION_NODE_NAME;\n+\t\t  case C_NodeTypes.ORDER_BY_NODE:\n+              return C_NodeNames.ORDER_BY_NODE_NAME;\n \n-            case C_NodeTypes.IS_NODE:\n-                return C_NodeNames.IS_NODE_NAME;\n+\t\t  case C_NodeTypes.DISTINCT_NODE:\n+\t\t  \treturn C_NodeNames.DISTINCT_NODE_NAME;\n \n-            case C_NodeTypes.LOCK_TABLE_NODE:\n-                return C_NodeNames.LOCK_TABLE_NODE_NAME;\n+          case C_NodeTypes.LOCATE_FUNCTION_NODE:\n+\t\t  case C_NodeTypes.SUBSTRING_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.RIGHT_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.TRIM_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.LEFT_OPERATOR_NODE:\n+\t\t  case C_NodeTypes.TIMESTAMP_ADD_FN_NODE:\n+\t\t  case C_NodeTypes.TIMESTAMP_DIFF_FN_NODE:\n+\t\t  case C_NodeTypes.REPLACE_OPERATOR_NODE:\n+\t\t  \treturn C_NodeNames.TERNARY_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.ALTER_TABLE_NODE:\n-                return C_NodeNames.ALTER_TABLE_NODE_NAME;\n+\t\t  case C_NodeTypes.SELECT_NODE:\n+\t\t  \treturn C_NodeNames.SELECT_NODE_NAME;\n \n-            case C_NodeTypes.AGGREGATE_NODE:\n-                return C_NodeNames.AGGREGATE_NODE_NAME;\n+\t\t  case C_NodeTypes.CREATE_VIEW_NODE:\n+\t\t  \treturn C_NodeNames.CREATE_VIEW_NODE_NAME;\n \n-            case C_NodeTypes.STRING_AGGREGATE_NODE:\n-                return C_NodeNames.STRING_AGGREGATE_NODE_NAME;\n+\t\t  case C_NodeTypes.CONSTRAINT_DEFINITION_NODE:\n+\t\t  \treturn C_NodeNames.CONSTRAINT_DEFINITION_NODE_NAME;\n ", "originalCommit": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODk3NDYyMA==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418974620", "bodyText": "Fixed in new commit 96dd889.", "author": "jpanko1", "createdAt": "2020-05-02T15:54:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyNjc2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyNzAwOQ==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418127009", "bodyText": "This looks like a bad merge, these nodes shouldn't be removed", "author": "dgomezferro", "createdAt": "2020-04-30T16:13:07Z", "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/sql/compile/SpliceNodeFactoryImpl.java", "diffHunk": "@@ -608,71 +604,62 @@ protected String nodeName(int nodeType)\n             case C_NodeTypes.RANK_FUNCTION_NODE:\n                 return C_NodeNames.RANK_FUNCTION_NAME;\n \n-            case C_NodeTypes.GROUPING_FUNCTION_NODE:\n-                return C_NodeNames.GROUPING_FUNCTION_NODE_NAME;\n+\t\t\tcase C_NodeTypes.GROUPING_FUNCTION_NODE:\n+\t\t\t\treturn C_NodeNames.GROUPING_FUNCTION_NODE_NAME;\n \n-            case C_NodeTypes.CREATE_SEQUENCE_NODE:\n-                return C_NodeNames.CREATE_SEQUENCE_NODE_NAME;\n+          case C_NodeTypes.CREATE_SEQUENCE_NODE:\n+            return C_NodeNames.CREATE_SEQUENCE_NODE_NAME;\n \n-            case C_NodeTypes.DROP_SEQUENCE_NODE:\n-                return C_NodeNames.DROP_SEQUENCE_NODE_NAME;\n+          case C_NodeTypes.DROP_SEQUENCE_NODE:\n+            return C_NodeNames.DROP_SEQUENCE_NODE_NAME;\n \n-            case C_NodeTypes.NEXT_SEQUENCE_NODE:\n-                return C_NodeNames.NEXT_SEQUENCE_NODE_NAME;\n+          case C_NodeTypes.NEXT_SEQUENCE_NODE:\n+            return C_NodeNames.NEXT_SEQUENCE_NODE_NAME;\n \n-            case C_NodeTypes.EXPLAIN_NODE:\n+          case C_NodeTypes.EXPLAIN_NODE:\n                 return C_NodeNames.EXPLAIN_NODE_NAME;\n \n-            case C_NodeTypes.EXPORT_NODE:\n+          case C_NodeTypes.EXPORT_NODE:\n                 return C_NodeNames.EXPORT_NODE_NAME;\n \n-            case C_NodeTypes.BINARY_EXPORT_NODE:\n-                return C_NodeNames.BINARY_EXPORT_NODE_NAME;\n+          case C_NodeTypes.BINARY_EXPORT_NODE:\n+\t\t\t\treturn C_NodeNames.BINARY_EXPORT_NODE_NAME;\n \n-            case C_NodeTypes.TRUNC_NODE:\n-                return C_NodeNames.TRUNC_NODE_NAME;\n+          case C_NodeTypes.KAFKA_EXPORT_NODE:\n+\t\t\t\treturn C_NodeNames.KAFKA_EXPORT_NODE_NAME;\n \n-            case C_NodeTypes.CREATE_PIN_NODE:\n-                return C_NodeNames.CREATE_PIN_NODE_NAME;\n-\n-            case C_NodeTypes.DROP_PIN_NODE:\n-                return C_NodeNames.DROP_PIN_NODE_NAME;\n-\n-            case C_NodeTypes.ARRAY_OPERATOR_NODE:\n-                return C_NodeNames.ARRAY_OPERATOR_NODE_NAME;\n-\n-            case C_NodeTypes.ARRAY_CONSTANT_NODE:\n-                return C_NodeNames.ARRAY_CONSTANT_NODE_NAME;\n+          case C_NodeTypes.TRUNC_NODE:\n+                return C_NodeNames.TRUNC_NODE_NAME;\n \n-            case C_NodeTypes.SET_SESSION_PROPERTY_NODE:\n-                return C_NodeNames.SET_SESSION_PROPERTY_NAME;\n+\t\t\tcase C_NodeTypes.CREATE_PIN_NODE:\n+\t\t\t\treturn C_NodeNames.CREATE_PIN_NODE_NAME;\n \n-            case C_NodeTypes.SELF_REFERENCE_NODE:\n-                return C_NodeNames.SELF_REFERENCE_NODE_NAME;\n+\t\t\tcase C_NodeTypes.DROP_PIN_NODE:\n+\t\t\t\treturn C_NodeNames.DROP_PIN_NODE_NAME;\n \n-            case C_NodeTypes.DIGITS_OPERATOR_NODE:\n-                return C_NodeNames.UNARY_OPERATOR_NODE_NAME;\n+\t\t\tcase C_NodeTypes.ARRAY_OPERATOR_NODE:\n+\t\t\t\treturn C_NodeNames.ARRAY_OPERATOR_NODE_NAME;\n \n-            case C_NodeTypes.SIGNAL_NODE:\n-                return C_NodeNames.SIGNAL_NAME;\n+\t\t\tcase C_NodeTypes.ARRAY_CONSTANT_NODE:\n+\t\t\t\treturn C_NodeNames.ARRAY_CONSTANT_NODE_NAME;\n \n-            case C_NodeTypes.SET_NODE:\n-                return C_NodeNames.SET_NAME;\n+\t\t\tcase C_NodeTypes.SET_SESSION_PROPERTY_NODE:\n+\t\t\t\treturn C_NodeNames.SET_SESSION_PROPERTY_NAME;\n \n-            case C_NodeTypes.FULL_OUTER_JOIN_NODE:\n-                return C_NodeNames.FULL_OUTER_JOIN_NODE_NAME;\n+\t\t\tcase C_NodeTypes.SELF_REFERENCE_NODE:\n+\t\t\t\treturn C_NodeNames.SELF_REFERENCE_NODE_NAME;\n \n-            case C_NodeTypes.EMPTY_DEFAULT_CONSTANT_NODE:\n-                return C_NodeNames.EMPTY_DEFAULT_CONSTANT_NODE;\n+\t\t\tcase C_NodeTypes.DIGITS_OPERATOR_NODE:\n+\t\t\t\treturn C_NodeNames.UNARY_OPERATOR_NODE_NAME;\n ", "originalCommit": "e15bf8e4ed58fce49d5a3048e6bca875bad887bd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODk3NDYzNA==", "url": "https://github.com/splicemachine/spliceengine/pull/3391#discussion_r418974634", "bodyText": "Fixed in new commit 96dd889.", "author": "jpanko1", "createdAt": "2020-05-02T15:54:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODEyNzAwOQ=="}], "type": "inlineReview"}, {"oid": "5c1565b3c5e22da839d09e3834a035db15ac1f0a", "url": "https://github.com/splicemachine/spliceengine/commit/5c1565b3c5e22da839d09e3834a035db15ac1f0a", "message": "Changed class name. Changed kafka consumer group and client ids.", "committedDate": "2020-05-01T05:40:07Z", "type": "commit"}, {"oid": "8d9468c8f8ccafc3300e2987557953531a282b07", "url": "https://github.com/splicemachine/spliceengine/commit/8d9468c8f8ccafc3300e2987557953531a282b07", "message": "Removed unused NullFunction. Updated copyright date.", "committedDate": "2020-05-01T05:44:43Z", "type": "commit"}, {"oid": "05cba38f63c3f1990d162aea2fce8cdac8db5838", "url": "https://github.com/splicemachine/spliceengine/commit/05cba38f63c3f1990d162aea2fce8cdac8db5838", "message": "Updated copyright date.", "committedDate": "2020-05-01T05:46:48Z", "type": "commit"}, {"oid": "d83e2a643435901b8202f843ddda7cf71afaf343", "url": "https://github.com/splicemachine/spliceengine/commit/d83e2a643435901b8202f843ddda7cf71afaf343", "message": "Removed commented code.", "committedDate": "2020-05-01T05:50:09Z", "type": "commit"}, {"oid": "2c35ac17df023c5b8357d22a6384464c4bb30676", "url": "https://github.com/splicemachine/spliceengine/commit/2c35ac17df023c5b8357d22a6384464c4bb30676", "message": "Deleted NativeTransformationsIT.", "committedDate": "2020-05-01T05:51:15Z", "type": "commit"}, {"oid": "ca75b788a2dba3c41e2b14f952e550d9fdd7308a", "url": "https://github.com/splicemachine/spliceengine/commit/ca75b788a2dba3c41e2b14f952e550d9fdd7308a", "message": "Updated class name of dependency KafkaReadFunction. Changed kafka consumer group and client ids.", "committedDate": "2020-05-01T06:00:19Z", "type": "commit"}, {"oid": "c8b039b10aa0cbb3251644fafdfeb88ba8096465", "url": "https://github.com/splicemachine/spliceengine/commit/c8b039b10aa0cbb3251644fafdfeb88ba8096465", "message": "Changed KafkaReadFunction to be used in flatMap instead of mapPartitions. Updated Kafka consumer poll from deprecated version.", "committedDate": "2020-05-01T21:36:34Z", "type": "commit"}, {"oid": "96dd88926dadc180b632abe595441ef38255525b", "url": "https://github.com/splicemachine/spliceengine/commit/96dd88926dadc180b632abe595441ef38255525b", "message": "Merge followup.", "committedDate": "2020-05-02T15:40:42Z", "type": "commit"}, {"oid": "4606a8b3150b1ee5d22ee4289ff0ca5052bbb45e", "url": "https://github.com/splicemachine/spliceengine/commit/4606a8b3150b1ee5d22ee4289ff0ca5052bbb45e", "message": "Removed unused code.", "committedDate": "2020-05-03T03:47:22Z", "type": "commit"}, {"oid": "0bd8e2964e1366bb644d9ef9faae11602bf00af5", "url": "https://github.com/splicemachine/spliceengine/commit/0bd8e2964e1366bb644d9ef9faae11602bf00af5", "message": "Added retry logic.", "committedDate": "2020-05-03T23:02:44Z", "type": "commit"}, {"oid": "5eb097b492ce989c69231349fcecfa88c5554952", "url": "https://github.com/splicemachine/spliceengine/commit/5eb097b492ce989c69231349fcecfa88c5554952", "message": "Getting schema from jdbc instead of from backend.", "committedDate": "2020-05-04T07:26:14Z", "type": "commit"}, {"oid": "f299177ca3be6d8fcb4cc47a36b4fd800afa250f", "url": "https://github.com/splicemachine/spliceengine/commit/f299177ca3be6d8fcb4cc47a36b4fd800afa250f", "message": "Removed spark2.1 from splice_spark2.", "committedDate": "2020-05-04T08:07:07Z", "type": "commit"}, {"oid": "abbd8ae8b8760b73393b0e417af2f902352f83dd", "url": "https://github.com/splicemachine/spliceengine/commit/abbd8ae8b8760b73393b0e417af2f902352f83dd", "message": "DB-8173 NSDSv2 Enabled primary key creation in createTable.", "committedDate": "2020-05-05T07:17:38Z", "type": "commit"}, {"oid": "c3f136e285bf82ffa559f25b245453ae9f373a8a", "url": "https://github.com/splicemachine/spliceengine/commit/c3f136e285bf82ffa559f25b245453ae9f373a8a", "message": "Resolve conflict.", "committedDate": "2020-05-12T17:05:28Z", "type": "commit"}, {"oid": "cba51f0e019e70fe61f55be09e641c017adb9c5b", "url": "https://github.com/splicemachine/spliceengine/commit/cba51f0e019e70fe61f55be09e641c017adb9c5b", "message": "Merge branch 'master' into eNSDS-3.1.0.1951", "committedDate": "2020-05-12T17:21:14Z", "type": "commit"}]}