{"pr_number": 4243, "pr_title": "SPLICE-2289 MultiRowRangeFilter for MultiProbeScan and MergeJoin.", "pr_createdAt": "2020-10-08T05:15:38Z", "pr_url": "https://github.com/splicemachine/spliceengine/pull/4243", "timeline": [{"oid": "9ba8c4f8dd47eaa49c2296a11ba8f2fd5a43f6e6", "url": "https://github.com/splicemachine/spliceengine/commit/9ba8c4f8dd47eaa49c2296a11ba8f2fd5a43f6e6", "message": "SPLICE-2289 Fix Spotbugs.", "committedDate": "2020-10-08T23:15:35Z", "type": "forcePushed"}, {"oid": "dce4fd907dffc5d829c838732532314ed730ea1b", "url": "https://github.com/splicemachine/spliceengine/commit/dce4fd907dffc5d829c838732532314ed730ea1b", "message": "SPLICE-2289 Favor old merge join if probing isn't expected\n            to reduce the number of right rows sent to join\n\t    with the left rows.", "committedDate": "2020-10-13T16:17:39Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTI0ODE0Nw==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r505248147", "bodyText": "else throw?", "author": "martinrupp", "createdAt": "2020-10-15T07:14:37Z", "path": "db-engine/src/main/java/com/splicemachine/db/impl/sql/compile/JoinNode.java", "diffHunk": "@@ -1296,19 +1304,34 @@ protected void generateCore(ActivationClassBuilder acb,\n          */\n         String joinResultSetString;\n \n+        AccessPath ap = ((Optimizable)rightResultSet).getTrulyTheBestAccessPath();\n         if (joinType==FULLOUTERJOIN) {\n-            joinResultSetString=((Optimizable)rightResultSet).getTrulyTheBestAccessPath().\n-                    getJoinStrategy().fullOuterJoinResultSetMethodName();\n+            joinResultSetString=ap.getJoinStrategy().fullOuterJoinResultSetMethodName();\n         } else if(joinType==LEFTOUTERJOIN){\n-            joinResultSetString=((Optimizable)rightResultSet).getTrulyTheBestAccessPath().\n-                    getJoinStrategy().halfOuterJoinResultSetMethodName();\n+            joinResultSetString=ap.getJoinStrategy().halfOuterJoinResultSetMethodName();\n         }else{\n-            joinResultSetString=((Optimizable)rightResultSet).getTrulyTheBestAccessPath().\n-                    getJoinStrategy().joinResultSetMethodName();\n+            joinResultSetString=ap.getJoinStrategy().joinResultSetMethodName();\n         }\n \n         acb.pushGetResultSetFactoryExpression(mb);\n         int nargs=getJoinArguments(acb,mb,joinClause);\n+        if (RSUtils.isMJ(ap)) {\n+            nargs++;\n+\n+            CompilerContext.NewMergeJoinExecutionType\n+                newMergeJoin = getCompilerContext().getNewMergeJoin();\n+\n+            // Favor Old Merge Join if we estimate probing won't reduce the scanned row count much.\n+            boolean chooseOldMergeJoin = favorOldMergeJoin(ap.getCostEstimate());\n+            if (chooseOldMergeJoin) {\n+                if (newMergeJoin == SYSTEM)\n+                    newMergeJoin = SYSTEM_OFF;\n+                else if (newMergeJoin == ON)\n+                    newMergeJoin = OFF;\n+            }", "originalCommit": "6cca67bb5d7d90db00dae368ebbd90f8894e9281", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzAxNjczNQ==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r507016735", "bodyText": "The values OFF and FORCED don't need to be checked because they can't be overridden.  I added a check for SYSTEM_OFF, because that setting is not expected:\n\n            else if (newMergeJoin == SYSTEM_OFF) {\n                // SYSTEM_OFF should not be allowed as a setting of the property.\n                // It is only used for communication from the parser to the\n                // execution engine.\n                if(SanityManager.DEBUG)\n                    SanityManager.THROWASSERT(\n                            \"Illegal setting of newMergeJoin in \"+this.getClass().getName()+this);\n            }\n\n\nI chose not to do a plain throw that would be hit on a cluster because this isn't a critical condition, and it would be good if we still allowed the query to run.  We don't have any locations where the property is allowed to be set to SYSTEM_OFF, so I don't anticipate any issues.", "author": "msirek", "createdAt": "2020-10-18T04:51:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTI0ODE0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTI1MjkyMg==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r505252922", "bodyText": "i know we don't do a lot of unit tests, but this function would be very easy to unit test, please add a test.", "author": "martinrupp", "createdAt": "2020-10-15T07:19:04Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java", "diffHunk": "@@ -169,4 +179,177 @@ public ServiceDiscovery serviceDiscovery() {\n     }\n \n \n+    /**\n+     * Parse a Spark or Hadoop size parameter value, that may use k, m, g or t\n+     * to represent kilobytes, megabytes, gigabytes or terabytes, respectively,\n+     * and return back the corresponding number of bytes.\n+     * @param sizeString the parameter value string to parse\n+     * @param defaultValue the default value of the parameter if an invalid\n+     *                     <code>sizeString</code> was passed.\n+     * @return The value in bytes of <code>sizeString</code>, or <code>defaultValue</code>\n+     *         if a <code>sizeString</code> was passed that could not be parsed.\n+     */\n+    private static long parseSizeString(String sizeString, long defaultValue) {", "originalCommit": "6cca67bb5d7d90db00dae368ebbd90f8894e9281", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzAxNjgxOA==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r507016818", "bodyText": "I've added unit tests.", "author": "msirek", "createdAt": "2020-10-18T04:52:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTI1MjkyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTI1NTc2Nw==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r505255767", "bodyText": "if converted to\nprivate static int calculateMaxExecutorCores(String memorySize, String sparkDynamicAllocationString, String executorInstancesString,  String executorCoresString, String sparkExecutorMemory)\nwe can also unit test this function whose logic isn't trivial", "author": "martinrupp", "createdAt": "2020-10-15T07:21:20Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java", "diffHunk": "@@ -169,4 +179,177 @@ public ServiceDiscovery serviceDiscovery() {\n     }\n \n \n+    /**\n+     * Parse a Spark or Hadoop size parameter value, that may use k, m, g or t\n+     * to represent kilobytes, megabytes, gigabytes or terabytes, respectively,\n+     * and return back the corresponding number of bytes.\n+     * @param sizeString the parameter value string to parse\n+     * @param defaultValue the default value of the parameter if an invalid\n+     *                     <code>sizeString</code> was passed.\n+     * @return The value in bytes of <code>sizeString</code>, or <code>defaultValue</code>\n+     *         if a <code>sizeString</code> was passed that could not be parsed.\n+     */\n+    private static long parseSizeString(String sizeString, long defaultValue) {\n+        long retVal = defaultValue;\n+        Pattern sizePattern = Pattern.compile(\"([\\\\d.]+)([kmgt])\", Pattern.CASE_INSENSITIVE);\n+        Matcher matcher = sizePattern.matcher(sizeString);\n+        Map<String, Integer> suffixes = new HashMap<>();\n+        suffixes.put(\"k\", 1);\n+        suffixes.put(\"m\", 2);\n+        suffixes.put(\"g\", 3);\n+        suffixes.put(\"t\", 4);\n+        if (matcher.find()) {\n+            BigInteger value;\n+            String digits = matcher.group(1);\n+            try {\n+              value = new BigInteger(digits);\n+            }\n+            catch (NumberFormatException e) {\n+              return defaultValue;\n+            }\n+            int power = suffixes.get(matcher.group(2).toLowerCase());\n+            BigInteger multiplicand = BigInteger.valueOf(1024).pow(power);\n+            value = value.multiply(multiplicand);\n+            if (value.compareTo(BigInteger.valueOf(Long.MAX_VALUE)) > 0)\n+              return Long.MAX_VALUE;\n+            if (value.compareTo(BigInteger.valueOf(1)) < 0)\n+              return defaultValue;\n+\n+            retVal = value.longValue();\n+        }\n+        else {\n+            try {\n+                retVal = Long.parseLong(sizeString);\n+            }\n+            catch (NumberFormatException e) {\n+                return defaultValue;\n+            }\n+            if (retVal < 1)\n+                retVal = defaultValue;\n+        }\n+        return retVal;\n+    }\n+\n+    @Override\n+    public int getMaxExecutorCores() {\n+        return MAX_EXECUTOR_CORES;\n+    }\n+\n+    /**\n+     * Estimate the maximum number of Spark executor cores that could be simultaneously\n+     * be running given the current YARN and splice.spark settings.\n+     *\n+     * @return The maximum number of Spark executor cores.\n+     */\n+    private static int calculateMaxExecutorCores() {\n+        String memorySize = HConfiguration.unwrapDelegate().get(\"yarn.nodemanager.resource.memory-mb\");\n+        String sparkDynamicAllocationString = getProperty(\"splice.spark.dynamicAllocation.enabled\");\n+        String executorInstancesString = getProperty(\"splice.spark.executor.instances\");\n+        String executorCoresString = getProperty(\"splice.spark.executor.cores\");\n+        String sparkExecutorMemory = getProperty(\"splice.spark.executor.memory\");\n+", "originalCommit": "6cca67bb5d7d90db00dae368ebbd90f8894e9281", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzAxNjgzNg==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r507016836", "bodyText": "I've added unit tests for this method as well.", "author": "msirek", "createdAt": "2020-10-18T04:52:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTI1NTc2Nw=="}], "type": "inlineReview"}, {"oid": "ef0023ed7e5bf6837efe112f31f51bb7868f6c24", "url": "https://github.com/splicemachine/spliceengine/commit/ef0023ed7e5bf6837efe112f31f51bb7868f6c24", "message": "SPLICE-2289 MultiRowRangeFilter for MultiProbeScan and MergeJoin.", "committedDate": "2020-10-18T05:31:10Z", "type": "commit"}, {"oid": "8d6e058c44595d7524623a461a4bf272daf2cd71", "url": "https://github.com/splicemachine/spliceengine/commit/8d6e058c44595d7524623a461a4bf272daf2cd71", "message": "SPLICE-2289 Fixes for HBase 2.0.", "committedDate": "2020-10-18T05:31:14Z", "type": "commit"}, {"oid": "7910f393b5db1664a3d47694ef2eff2915814725", "url": "https://github.com/splicemachine/spliceengine/commit/7910f393b5db1664a3d47694ef2eff2915814725", "message": "SPLICE-2289 Fix Spotbugs.", "committedDate": "2020-10-18T05:31:14Z", "type": "commit"}, {"oid": "f5be38629407d0452cf924f55ffcb1f24da5397e", "url": "https://github.com/splicemachine/spliceengine/commit/f5be38629407d0452cf924f55ffcb1f24da5397e", "message": "SPLICE-2289 Fix issue in DB-8896 trying to access lcc from wrong thread.", "committedDate": "2020-10-18T05:55:27Z", "type": "commit"}, {"oid": "0f68598e6aca1d7977161032958bf93d504fbb06", "url": "https://github.com/splicemachine/spliceengine/commit/0f68598e6aca1d7977161032958bf93d504fbb06", "message": "SPLICE-2289 Fix Spotbugs.", "committedDate": "2020-10-18T05:55:30Z", "type": "commit"}, {"oid": "386d0b2797191b68fb0ff889faa8fcc6cd0530a2", "url": "https://github.com/splicemachine/spliceengine/commit/386d0b2797191b68fb0ff889faa8fcc6cd0530a2", "message": "SPLICE-2289 Fix KillOperationIT.", "committedDate": "2020-10-18T05:55:30Z", "type": "commit"}, {"oid": "5054e44dd613d1802174feadaed38464f0214cc5", "url": "https://github.com/splicemachine/spliceengine/commit/5054e44dd613d1802174feadaed38464f0214cc5", "message": "SPLICE-2289 Fix KillOperation_IT, take 2.", "committedDate": "2020-10-18T05:55:30Z", "type": "commit"}, {"oid": "61986bc97ead8d7b0320009e8d694ebd9d5c6589", "url": "https://github.com/splicemachine/spliceengine/commit/61986bc97ead8d7b0320009e8d694ebd9d5c6589", "message": "SPLICE-2289 Allow fallback to old merge join algorithm for\n            small table cases, and add a property to force fallback\n\t    to old merge join.", "committedDate": "2020-10-18T06:03:55Z", "type": "commit"}, {"oid": "44739d885356c8680a6d32f1a411c51ce5359630", "url": "https://github.com/splicemachine/spliceengine/commit/44739d885356c8680a6d32f1a411c51ce5359630", "message": "SPLICE-2289 splice.execution.newMergeJoin database and system property.", "committedDate": "2020-10-18T06:30:11Z", "type": "commit"}, {"oid": "e0374a40cd5ac41436ebab75222816ff9989e74d", "url": "https://github.com/splicemachine/spliceengine/commit/e0374a40cd5ac41436ebab75222816ff9989e74d", "message": "SPLICE-2289 Favor old merge join if probing isn't expected\n            to reduce the number of right rows sent to join\n\t    with the left rows.", "committedDate": "2020-10-18T06:30:14Z", "type": "commit"}, {"oid": "f8dac331e4f50a369a5fe8a2dc8f11802f4b3113", "url": "https://github.com/splicemachine/spliceengine/commit/f8dac331e4f50a369a5fe8a2dc8f11802f4b3113", "message": "SPLICE-2289 Handle upgrade scenarios.", "committedDate": "2020-10-18T06:46:08Z", "type": "commit"}, {"oid": "236d2dde74247b7947da8b554c5cf092f9c49615", "url": "https://github.com/splicemachine/spliceengine/commit/236d2dde74247b7947da8b554c5cf092f9c49615", "message": "SPLICE-2289 Fix some formulas.", "committedDate": "2020-10-18T06:46:11Z", "type": "commit"}, {"oid": "4197e164e6da6a900291985f47eea363acd0c7c9", "url": "https://github.com/splicemachine/spliceengine/commit/4197e164e6da6a900291985f47eea363acd0c7c9", "message": "SPLICE-2289 Fix HEngineSqlEnv methods and add unit tests.", "committedDate": "2020-10-18T06:46:11Z", "type": "commit"}, {"oid": "20892e371d6ac13494b2c8dc7c3b543dde010c8a", "url": "https://github.com/splicemachine/spliceengine/commit/20892e371d6ac13494b2c8dc7c3b543dde010c8a", "message": "SPLICE-2289 Store the number of Spark nodes in Zookeeper.", "committedDate": "2020-10-18T06:47:15Z", "type": "commit"}, {"oid": "20892e371d6ac13494b2c8dc7c3b543dde010c8a", "url": "https://github.com/splicemachine/spliceengine/commit/20892e371d6ac13494b2c8dc7c3b543dde010c8a", "message": "SPLICE-2289 Store the number of Spark nodes in Zookeeper.", "committedDate": "2020-10-18T06:47:15Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODg2NTMwOQ==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r508865309", "bodyText": "Please remove the extra semi-column.", "author": "yxia92", "createdAt": "2020-10-20T21:58:04Z", "path": "splice_machine/src/main/java/com/splicemachine/derby/stream/function/merge/AbstractMergeJoinFlatMapFunction.java", "diffHunk": "@@ -32,98 +33,281 @@\n import splice.com.google.common.base.Function;\n import splice.com.google.common.collect.Iterators;\n import splice.com.google.common.collect.PeekingIterator;\n+import com.splicemachine.si.impl.driver.SIDriver;\n+import com.splicemachine.utils.Pair;\n+import splice.com.google.common.base.Preconditions;\n \n import javax.annotation.Nullable;\n-import java.io.Closeable;\n-import java.io.IOException;\n+import java.io.*;\n+import java.util.ArrayList;\n import java.util.Collections;\n import java.util.Iterator;\n+import java.util.List;\n+\n+import static com.splicemachine.EngineDriver.isMemPlatform;\n+import static com.splicemachine.db.shared.common.reference.SQLState.LANG_INTERNAL_ERROR;\n \n /**\n  * Created by jleach on 6/9/15.\n  */\n-public abstract class AbstractMergeJoinFlatMapFunction extends SpliceFlatMapFunction<JoinOperation,Iterator<ExecRow>,ExecRow> {\n+public abstract class AbstractMergeJoinFlatMapFunction extends SpliceFlatMapFunction<JoinOperation,Iterator<ExecRow>, ExecRow> {\n     boolean initialized;\n     protected JoinOperation joinOperation;\n+    protected SpliceOperation leftSide;\n+    protected SpliceOperation rightSide;\n+    private PeekingIterator<ExecRow> leftPeekingIterator;\n+    private Iterator<ExecRow> mergeJoinIterator;\n+    private static final boolean IS_MEM_PLATFORM = isMemPlatform();\n+    private final SIDriver driver = SIDriver.driver();\n+    private boolean useOldMergeJoin = false;\n \n     public AbstractMergeJoinFlatMapFunction() {\n         super();\n     }\n \n-    public AbstractMergeJoinFlatMapFunction(OperationContext<JoinOperation> operationContext) {\n+    public AbstractMergeJoinFlatMapFunction(OperationContext<JoinOperation> operationContext, boolean useOldMergeJoin) {\n         super(operationContext);\n+        this.useOldMergeJoin = useOldMergeJoin || isMemPlatform();\n     }\n \n-    @Override\n-    public Iterator<ExecRow> call(Iterator<ExecRow> locatedRows) throws Exception {\n-        PeekingIterator<ExecRow> leftPeekingIterator = Iterators.peekingIterator(locatedRows);\n-        if (!initialized) {\n-            joinOperation = getOperation();\n-            initialized = true;\n-            if (!leftPeekingIterator.hasNext())\n-                return Collections.EMPTY_LIST.iterator();\n-            initRightScan(leftPeekingIterator);\n-        }\n-        final SpliceOperation rightSide = joinOperation.getRightOperation();\n-        rightSide.reset();\n-        DataSetProcessor dsp =EngineDriver.driver().processorFactory().bulkProcessor(getOperation().getActivation(), rightSide);\n-        final Iterator<ExecRow> rightIterator = Iterators.transform(rightSide.getDataSet(dsp).toLocalIterator(), new Function<ExecRow, ExecRow>() {\n-            @Override\n-            public ExecRow apply(@Nullable ExecRow locatedRow) {\n-                operationContext.recordJoinedRight();\n-                return locatedRow;\n+    protected class BufferedMergeJoinIterator implements PeekingIterator<ExecRow> {\n+        protected static final int BUFFERSIZE=400;\n+        private static final int INITIALCAPACITY=50;\n+        private ArrayList<ExecRow> bufferedRowList = new ArrayList<>(INITIALCAPACITY);\n+        private PeekingIterator<ExecRow> sourceIterator;\n+        private boolean hasPeeked;\n+        private boolean firstTime = true;\n+\n+        // A pointer to the next row to return when next() is called.\n+        private int bufferPosition;\n+\n+        private void fillBuffer() throws StandardException {\n+            bufferPosition = 0;\n+\n+            if (firstTime) {\n+                bufferedRowList.clear();\n+                for (int i = 0; i < BUFFERSIZE && sourceIterator.hasNext(); i++) {\n+                    bufferedRowList.add((ExecRow) sourceIterator.next().getClone());\n+                }\n             }\n-        });\n-        ((BaseActivation)joinOperation.getActivation()).setScanStartOverride(null); // reset to null to avoid any side effects\n-        ((BaseActivation)joinOperation.getActivation()).setScanKeys(null);\n-        ((BaseActivation)joinOperation.getActivation()).setScanStopOverride(null);\n-        AbstractMergeJoinIterator iterator = createMergeJoinIterator(leftPeekingIterator,\n-                Iterators.peekingIterator(rightIterator),\n-                joinOperation.getLeftHashKeys(), joinOperation.getRightHashKeys(),\n-                joinOperation, operationContext);\n-        iterator.registerCloseable(new Closeable() {\n-            @Override\n-            public void close() throws IOException {\n-                try {\n-                    rightSide.close();\n-                } catch (StandardException e) {\n-                    throw new RuntimeException(e);\n+            else {\n+                // Re-use the buffer rows on subsequent filling of the buffer\n+                // to reduce memory usage and GC pressure.\n+                int i;\n+                for (i = 0; i < BUFFERSIZE && sourceIterator.hasNext(); i++) {\n+                    bufferedRowList.get(i).transfer(sourceIterator.next());\n                 }\n+                if (i < bufferedRowList.size())\n+                    for (int j = bufferedRowList.size()-1; j >= i; j--)\n+                        bufferedRowList.remove(j);\n             }\n-        });\n-        return iterator;\n-    }\n+            firstTime = false;\n+            if (!bufferedRowList.isEmpty())\n+                startNewRightSideScan(this);\n+        }\n \n-    private int[] getColumnOrdering(SpliceOperation op) throws StandardException {\n-        SpliceOperation operation = op;\n-        while (operation != null && !(operation instanceof ScanOperation)) {\n-            operation = operation.getLeftOperation();\n+        public List<ExecRow> getBufferList() {\n+            return bufferedRowList;\n         }\n-        assert operation != null;\n \n-        return ((ScanOperation)operation).getColumnOrdering();\n-    }\n+        protected BufferedMergeJoinIterator(Iterator<ExecRow> sourceIterator) throws StandardException {\n+            this.sourceIterator = Iterators.peekingIterator(sourceIterator);;", "originalCommit": "20892e371d6ac13494b2c8dc7c3b543dde010c8a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTgxMDMxNA==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r509810314", "bodyText": "Done.", "author": "msirek", "createdAt": "2020-10-22T00:27:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODg2NTMwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODg5ODQ5MQ==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r508898491", "bodyText": "The reset() will reset modifedRowCount, badRecords. Since here we may reset the right operation for every batch of left rows, would we have a side effect of only track the info of the last batch?", "author": "yxia92", "createdAt": "2020-10-20T23:28:10Z", "path": "splice_machine/src/main/java/com/splicemachine/derby/stream/function/merge/AbstractMergeJoinFlatMapFunction.java", "diffHunk": "@@ -232,10 +416,137 @@ else if (!leftKeyIsNull) {\n             if (scanInfo != null && scanInfo.getSameStartStopPosition())\n                 ((BaseActivation)joinOperation.getActivation()).setScanStopOverride(startPosition);\n         }\n+        return true;\n+    }\n+\n+    private void startNewRightSideScan(PeekingIterator<ExecRow> leftRows) throws StandardException {\n+        Iterator<ExecRow> rightIterator;\n+\n+        ArrayList<Pair<ExecRow, ExecRow>> keyRows = null;\n+\n+        boolean skipRightSideRead = false;\n+\n+        // The mem platform doesn't support the HBase MultiRangeRowFilter.\n+        if (!IS_MEM_PLATFORM && leftRows instanceof BufferedMergeJoinIterator) {\n+            BufferedMergeJoinIterator mjIter = (BufferedMergeJoinIterator)leftRows;\n+            keyRows = mjIter.getKeyRows();\n+            ((BaseActivation) joinOperation.getActivation()).setKeyRows(keyRows);\n+            skipRightSideRead = (keyRows == null);\n+        }\n+        else\n+            skipRightSideRead = !initRightScanWithStartStopKeys(leftRows);\n+\n+        // If there are no join keys to look up in the right table,\n+        // don't even read the right table.\n+        if (skipRightSideRead)\n+            rightIterator = Collections.emptyIterator();\n+        else {\n+            rightSide = joinOperation.getRightOperation();\n+            rightSide.reset();", "originalCommit": "20892e371d6ac13494b2c8dc7c3b543dde010c8a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0MjA3Mg==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r509842072", "bodyText": "You're right.  The rightSide.close() calls don't actually get applied until the stream of left rows is complete, so the call to reset can be removed.  I will remove it.", "author": "msirek", "createdAt": "2020-10-22T02:27:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODg5ODQ5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDY3ODcwMQ==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r510678701", "bodyText": "It turns out when the merge join is the source of the right side of a nested loop join it may close the operation and then restart it.  I added in a new method called reOpen, which does not effect the other fields like badRecords.", "author": "msirek", "createdAt": "2020-10-23T07:09:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODg5ODQ5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk1MzU1OQ==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r508953559", "bodyText": "I'm wondering how the MultiRowRangerFilter works with the start and stop key in the following example:\ncreate table t2 (a2 int, b2 int, c2 int, primary key (a2,b2,c2));\ninsert into t1 values (1,6,6), (5,5,5);\ninsert into t2 values (1,6,6), (5,5,5);\ncall syscs_util.syscs_set_global_database_property('splice.execution.newMergeJoin', 'forced');\nselect * from t1 left join t2 --splice-properties useSpark=true\non a1=a2 and b1=b2 and c1=c2 and a2>=5;\n\nIn this example, the right table t2 has a start key of (5), and from the left table, we also get two ranges [(1,6,6), (1,6,6)) and [(5,5,5),(5,5,5)). The above code seems to update the start row to the start point of the first range, that is (1,6,6). Is this expected? The result returned looks correct, so somewhere we must have logic to honor the original start key of (5) which is more tight than (1,6,6). Could you point me to where the logic is?", "author": "yxia92", "createdAt": "2020-10-21T02:37:55Z", "path": "hbase_storage/src/main/java/com/splicemachine/storage/HScan.java", "diffHunk": "@@ -43,6 +48,57 @@ public boolean isDescendingScan(){\n         return scan.isReversed();\n     }\n \n+    /**\n+     * Take a list of [startKey, stopKey) rowkey pairs, where the stopKey is excluded,\n+     * convert it to the corresponding {@link MultiRowRangeFilter} and attach\n+     * it to the {@link Scan} held in this {@link HScan} as a {@link Filter}.\n+     *\n+     * @param rowkeyPairs the list of [startKey, stopKey) pairs to convert.\n+     *\n+     * @return if this {@link HScan} currently has no filter, build a new\n+     * {@link MultiRowRangeFilter} out of {@code rowkeyPairs} and attach it\n+     * as a filter.  If this {@link HScan} already has a {@link MultiRowRangeFilter},\n+     * replace it with a new MultiRowRangeFilter built from {@code rowkeyPairs}.\n+     * If {@code rowkeyPairs} has no elements, do not build a filter.\n+     *\n+     * @throws IOException\n+     *\n+     * @notes A possible future enhancement is, instead of replacing an old\n+     * MultiRowRangeFilter with a new one, build the intersection of the\n+     * old filter and the new one.\n+     *\n+     * @see     Scan\n+     */\n+    @Override\n+    public void addRowkeyRangesFilter(List<Pair<byte[],byte[]>> rowkeyPairs) throws IOException {\n+        if (rowkeyPairs == null || rowkeyPairs.size() < 1) {\n+            return;\n+        }\n+        Filter currentFilter = scan.getFilter();\n+        if (currentFilter != null && !(currentFilter instanceof MultiRowRangeFilter))\n+            return;\n+\n+        List<MultiRowRangeFilter.RowRange> ranges = new ArrayList<>(rowkeyPairs.size());\n+        byte[] startKey;\n+        byte[] stopKey;\n+        for (Pair<byte[],byte[]> startStopKey: rowkeyPairs) {\n+            startKey=startStopKey.getFirst();\n+            stopKey=startStopKey.getSecond();\n+            MultiRowRangeFilter.RowRange rr =\n+            new MultiRowRangeFilter.RowRange(startKey, true,\n+                                             stopKey, false);\n+            ranges.add(rr);\n+        }\n+\n+        if (ranges.size() > 0) {\n+            MultiRowRangeFilter filter = new MultiRowRangeFilter(ranges);\n+            scan.setFilter(filter);\n+            List<MultiRowRangeFilter.RowRange> sortedRanges = filter.getRowRanges();\n+            scan.withStartRow(sortedRanges.get(0).getStartRow());", "originalCommit": "20892e371d6ac13494b2c8dc7c3b543dde010c8a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTgwMTQ2OA==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r509801468", "bodyText": "The original start key of 5 is never getting restored, instead the scan reads row (1,6,6), then calls Scans.qualifyRecordFromRow to apply the a2>=5 predicate which was built into a qualifier, filtering out the row.  It would be more efficient to use the more restrictive start key.  In HScan.addRowkeyRangesFilter, I will check the pre-existing start and stop keys and only replace them if the key from the filter is more restrictive.", "author": "msirek", "createdAt": "2020-10-21T23:58:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk1MzU1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDU5NTcxMg==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r510595712", "bodyText": "Yes, it would be good to honor the more restrictive start/stop key. Thanks!", "author": "yxia92", "createdAt": "2020-10-23T05:09:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk1MzU1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk4MjE2OA==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r508982168", "bodyText": "Why stopKey should be exclusive? If it is a point range, shouldn't the stopKey also be inclusive?", "author": "yxia92", "createdAt": "2020-10-21T04:28:14Z", "path": "hbase_storage/src/main/java/com/splicemachine/storage/HScan.java", "diffHunk": "@@ -43,6 +48,57 @@ public boolean isDescendingScan(){\n         return scan.isReversed();\n     }\n \n+    /**\n+     * Take a list of [startKey, stopKey) rowkey pairs, where the stopKey is excluded,\n+     * convert it to the corresponding {@link MultiRowRangeFilter} and attach\n+     * it to the {@link Scan} held in this {@link HScan} as a {@link Filter}.\n+     *\n+     * @param rowkeyPairs the list of [startKey, stopKey) pairs to convert.\n+     *\n+     * @return if this {@link HScan} currently has no filter, build a new\n+     * {@link MultiRowRangeFilter} out of {@code rowkeyPairs} and attach it\n+     * as a filter.  If this {@link HScan} already has a {@link MultiRowRangeFilter},\n+     * replace it with a new MultiRowRangeFilter built from {@code rowkeyPairs}.\n+     * If {@code rowkeyPairs} has no elements, do not build a filter.\n+     *\n+     * @throws IOException\n+     *\n+     * @notes A possible future enhancement is, instead of replacing an old\n+     * MultiRowRangeFilter with a new one, build the intersection of the\n+     * old filter and the new one.\n+     *\n+     * @see     Scan\n+     */\n+    @Override\n+    public void addRowkeyRangesFilter(List<Pair<byte[],byte[]>> rowkeyPairs) throws IOException {\n+        if (rowkeyPairs == null || rowkeyPairs.size() < 1) {\n+            return;\n+        }\n+        Filter currentFilter = scan.getFilter();\n+        if (currentFilter != null && !(currentFilter instanceof MultiRowRangeFilter))\n+            return;\n+\n+        List<MultiRowRangeFilter.RowRange> ranges = new ArrayList<>(rowkeyPairs.size());\n+        byte[] startKey;\n+        byte[] stopKey;\n+        for (Pair<byte[],byte[]> startStopKey: rowkeyPairs) {\n+            startKey=startStopKey.getFirst();\n+            stopKey=startStopKey.getSecond();\n+            MultiRowRangeFilter.RowRange rr =\n+            new MultiRowRangeFilter.RowRange(startKey, true,\n+                                             stopKey, false);", "originalCommit": "20892e371d6ac13494b2c8dc7c3b543dde010c8a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTgwNzUwNg==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r509807506", "bodyText": "The startKey/stopKey pair isn't really a point range.  We always add a trailing byte of 01xB on to the stop key so that we read all the matching keys.  01xB is higher than anything that could match.  For example, if we have primary key (a,b), a start key of -127 might represent primary key value a = 1, with b not specified.  But, we also want to read, for example, a row with the key a=1, b=1, which is an internal rowkey of :\nstartRow = {byte[3]@11822}\n0 = -127\n1 = 0\n2 = -127\nThe stop row that we build : (-127, 1) covers all possible values of b, because byte 1 in a valid rowkey will always be less than 1.  So, if we did mark the stopKey as inclusive, it would never match any valid primary key values, and have no effect.  Also, stopKeys are usually exclusive in HBase.  Perhaps this is why splice also builds exclusive stop keys.\nIn the call to buildStartAndStopKeys, you can also see that the start key is marked GE ( >= ), and the stop key is marked GT ( > ) :\nbuildStartAndStopKeys(startKeyDVDs, ScanController.GE,\nstopKeyDVDs, null,\nScanController.GT,\nNot sure why we don't call it LT ( < ).", "author": "msirek", "createdAt": "2020-10-22T00:17:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk4MjE2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDYwNzIwNQ==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r510607205", "bodyText": "I see, thanks for the explanation!", "author": "yxia92", "createdAt": "2020-10-23T05:23:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk4MjE2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk5MzU1OA==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r508993558", "bodyText": "It seems that MultiRowRangeFilter.RowRange.isAscendingOrder() always returns true, so I assume the range is always in ascending order. Would it be a problem if the rows in the conglomerate are actually in descending order, for example, index with key in descending order?", "author": "yxia92", "createdAt": "2020-10-21T05:11:39Z", "path": "hbase_sql/src/main/java/com/splicemachine/mrio/api/core/AbstractSMInputFormat.java", "diffHunk": "@@ -107,6 +108,100 @@\n         return null;\n     }\n \n+    /**\n+     * Given a Scan with a MultiRangeRowFilter, and a list of InputSplits,\n+     * generate a new list of InputSplits that only contain the splits\n+     * required to read the rowkey ranges included in the MultiRangeRowFilter.\n+     *\n+     * @param scan describes the attributes of a read from an HBase table.\n+     * @param splits the list of {@link InputSplit InputSplits} to prune.\n+     * @return if {@code scan} contains a filter of class {@link MultiRowRangeFilter},\n+     * a pruned list of {@link org.apache.hadoop.mapreduce.InputSplit InputSplits}\n+     * which have rowkey range overlap with at least one rowkey range in the\n+     * {@link MultiRowRangeFilter}.  If no {@link MultiRowRangeFilter} exists,\n+     * the original list of splits is returned.\n+     *\n+     * @see     Scan\n+     */\n+     private List<InputSplit> pruneFilteredInputSplits(Scan scan, List<InputSplit> splits) {\n+        // If no row range filter, we need to read all InputSplits.\n+        if (! (scan.getFilter() instanceof MultiRowRangeFilter))\n+            return splits;\n+\n+        List<InputSplit> newList = new ArrayList<>();\n+\n+        MultiRowRangeFilter rangeFilter = (MultiRowRangeFilter)scan.getFilter();\n+\n+        List<MultiRowRangeFilter.RowRange> ranges = rangeFilter.getRowRanges();\n+\n+        for(InputSplit split:splits) {\n+            if (!(split instanceof SMSplit))\n+                return splits;\n+\n+            TableSplit tableSplit = ((SMSplit)split).getSplit();\n+\n+            byte[] regionStartKey = tableSplit.getStartRow();\n+            byte[] regionStopKey = tableSplit.getEndRow();\n+\n+            MultiRowRangeFilter.RowRange startRange =\n+              new MultiRowRangeFilter.RowRange(regionStartKey, true,\n+                                               regionStartKey, true);\n+            MultiRowRangeFilter.RowRange stopRange =\n+              new MultiRowRangeFilter.RowRange(regionStopKey, true,\n+                                               regionStopKey, true);\n+\n+            int startKeyIndex = Collections.binarySearch(ranges, startRange);\n+            // If we find the exact key, we know it's included.\n+            if (startKeyIndex >= 0) {\n+                newList.add(split);\n+                continue;\n+            }\n+            else if (regionStartKey.length == 0) {\n+                // This region has no begin limit, so it is considered to\n+                // be larger than all values.\n+\n+                // If there is no end limit, the region covers all ranges.\n+                // If the first range's start value is less than or equal to\n+                // the region's stop key, there is overlap.\n+                // The region's stop key is not inclusive, but we'll treat\n+                // it as such just to be safe.\n+                if (regionStopKey.length == 0 || ranges.get(0).compareTo(stopRange) <= 0) {", "originalCommit": "20892e371d6ac13494b2c8dc7c3b543dde010c8a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTgwOTkyNg==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r509809926", "bodyText": "This refers to whether HBase tables have rowkeys in ascending order.  Even if a splice table has a descending key, we don't change the byte ordering of the actual rowkeys in HBase, we just encode the values in a way such that high column values are encoded into low byte values (in the rowkey) and vice versa for ascending keys.  For example, look at ScalarEncoding.encodeBody:\n\n        if(desc)\n            nextByte^=0xff;", "author": "msirek", "createdAt": "2020-10-22T00:26:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk5MzU1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDYyMjU5MQ==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r510622591", "bodyText": "I see, thanks!", "author": "yxia92", "createdAt": "2020-10-23T05:40:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk5MzU1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTAwMDM1OA==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r509000358", "bodyText": "Could you elaborate the relational behind it? For the example that you gave, if we have one row in the outer (left) table, and a million rows in the inner table, and a join condition of outer.col1 = inner.col1, I think that the inner table's scan selectivity would be min((the left table's cardinality*right table's rows per value)/rightRowCount, 1), that is, the left row has a match for  right table rows with the same value.", "author": "yxia92", "createdAt": "2020-10-21T05:34:52Z", "path": "db-engine/src/main/java/com/splicemachine/db/impl/sql/compile/BinaryRelationalOperatorNode.java", "diffHunk": "@@ -1628,6 +1629,18 @@ public double scanSelectivity(Optimizable innerTable) throws StandardException {\n             selectivity *= innerTableCostController.getSelectivity(innerColumn.getSource().getColumnPosition(),\n                     startKey, true, endKey, true, false);\n         }\n+        else if (this.operatorType == EQUALS_RELOP) {\n+            // Use a more realistic selectivity that takes the\n+            // inner table RPV into account instead of defaulting\n+            // to a selectivity of 1.\n+            double innerCardinality =\n+                        innerTableCostController.cardinality(innerColumnPos);\n+            double outerRowCount = outerTableCostController.getEstimatedRowCount();\n+\n+            double tempSelectivity = outerRowCount / innerCardinality;", "originalCommit": "20892e371d6ac13494b2c8dc7c3b543dde010c8a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg1MDU2OA==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r509850568", "bodyText": "Use of outerRowCount here is wrong.  It should be outerCardinality / innerCardinality, which is the same as your formula.\nRowsPerValue is the same as rightRowCount / rightCardinality.  So, replacing RPV with this gives:\n(the left table's cardinality*(rightRowCount/rightCardinality))/rightRowCount, which simplifies to leftCardinality / rightCardinality, so I will change the formula to min(outerCardinality/innerCardinality, 1).", "author": "msirek", "createdAt": "2020-10-22T02:59:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTAwMDM1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDYwOTAzNA==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r510609034", "bodyText": "I agree!", "author": "yxia92", "createdAt": "2020-10-23T05:25:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTAwMDM1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTAxMjQ4MA==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r509012480", "bodyText": "The size of columnOrdering array is different for unique and non-unique index(See DB-10351). For non-unique index, the size is number of index columns + 1(which represents the rowid column), for unique index, the size is the number of index columns, should we keep it consistent here too? ascDescInfo's size is equal to the number of index columns.", "author": "yxia92", "createdAt": "2020-10-21T06:10:47Z", "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/store/access/btree/IndexConglomerate.java", "diffHunk": "@@ -525,6 +525,16 @@ private void localReadExternal(ObjectInput in) throws IOException, ClassNotFound\n         }\n         int len=in.readInt();\n         columnOrdering=ConglomerateUtil.readFormatIdArray(len,in);\n+\n+        // DataDictionaryImpl.bootstrapOneIndex creates system indexes with a null\n+        // column ordering, making the IndexConglomerate inconsistent, which may\n+        // lead to broken logic in places that call ScanOperation.getColumnOrdering.\n+        // Fill in the missing information here so the index may be properly used.\n+        if (columnOrdering == null || columnOrdering.length == 0) {", "originalCommit": "20892e371d6ac13494b2c8dc7c3b543dde010c8a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg2NDMzOQ==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r509864339", "bodyText": "OK, I updated the code to handle non-unique indexes.  It seems all the system indexes are created as unique.  I added in some temp code to try to force them to be unique for testing, and ran a couple ITs.  I'm not entirely sure if this actually tested the new code.", "author": "msirek", "createdAt": "2020-10-22T03:53:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTAxMjQ4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDYxMDMyNA==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r510610324", "bodyText": "Thanks!", "author": "yxia92", "createdAt": "2020-10-23T05:27:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTAxMjQ4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA0Njk5Mg==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r509046992", "bodyText": "I'm concerned about the impact of the call for getNumSplits() for OLTP queries. Now that we've separated the costing for OLTP and OLAP in two passes, I think we can let the OLTP also goes through the IF path here, that is, parallelism =1. We can trigger the computation of the number of splits only in the pass of OLAP costing.", "author": "yxia92", "createdAt": "2020-10-21T07:25:02Z", "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/stats/StoreCostControllerImpl.java", "diffHunk": "@@ -183,6 +191,18 @@ public StoreCostControllerImpl(TableDescriptor td, ConglomerateDescriptor conglo\n             tableStatistics = new TableStatisticsImpl(tableId, partitionStats,fallbackNullFraction,extraQualifierMultiplier);\n             useRealTableStatistics = true;\n         }\n+\n+        long tableSize = tableStatistics.rowCount() * tableStatistics.avgRowWidth();\n+        if (isMemPlatform())\n+            parallelism = 1;\n+        else {\n+            if (requestedSplits > 0)\n+                parallelism = requestedSplits;\n+            else\n+                parallelism = EngineDriver.getNumSplits(tableSize, getNumPartitions());\n+            if (parallelism > getMaxExecutorCores())\n+                parallelism = getMaxExecutorCores();\n+        }", "originalCommit": "20892e371d6ac13494b2c8dc7c3b543dde010c8a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg2Nzg1OA==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r509867858", "bodyText": "The StoreCostController for a table is a fixed entity that should only be built once per query (ideally).  Think of the field \"parallelism\" really as \"parallelismIfWeRunOnSpark\".  The value of parallelism returned by SimpleCostEstimate.getParallelism is always 1, unless we are in the isForSpark pass:\n\n@Override public int getParallelism() {\n    if (optimizer == null || optimizer.isForSpark())\n        return parallelism;\n    else\n        return 1;\n}\n\n\nThe reason I have to special-case isMemPlatform here, is that even though we might do a spark pass on mem platform, we actually can never send the query to spark when running mem, therefore it is a fake spark pass, and we should force parallelism to 1.\nIf you think the name \"parallelism\" is not clear, I could rename it to \"parallelismIfWeRunOnSpark\", or something else.", "author": "msirek", "createdAt": "2020-10-22T04:07:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA0Njk5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDYzNjI4Mg==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r510636282", "bodyText": "I got the wrong impression that the call of EngineDriver.getNumSplits() is expensive, so I had the concern that OLTP query will pay the price to call EngineDriver.getNumSplits() even though OLTP costing does not need it. Now looking at this function again, it is not really expensive, so my concern is gone.\nThere is no need to rename, thanks!", "author": "yxia92", "createdAt": "2020-10-23T05:55:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA0Njk5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDY3Nzk0MA==", "url": "https://github.com/splicemachine/spliceengine/pull/4243#discussion_r510677940", "bodyText": "OK, thanks.", "author": "msirek", "createdAt": "2020-10-23T07:07:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA0Njk5Mg=="}], "type": "inlineReview"}, {"oid": "1ac753d8d057b30a9a4e0fc6ba29aa75f3a220d9", "url": "https://github.com/splicemachine/spliceengine/commit/1ac753d8d057b30a9a4e0fc6ba29aa75f3a220d9", "message": "SPLICE-2289 Address review comments.", "committedDate": "2020-10-22T04:10:35Z", "type": "commit"}, {"oid": "883194ccd6bed04bd154f85976e38170adbf39cd", "url": "https://github.com/splicemachine/spliceengine/commit/883194ccd6bed04bd154f85976e38170adbf39cd", "message": "Merge branch 'master' into SPLICE-2289_main", "committedDate": "2020-10-22T04:21:15Z", "type": "commit"}, {"oid": "dd16b72c945cda2ff4bb7d09fed9f7664e5f0add", "url": "https://github.com/splicemachine/spliceengine/commit/dd16b72c945cda2ff4bb7d09fed9f7664e5f0add", "message": "SPLICE-2289 Fix setting of startKey in addRowkeyRangesFilter.", "committedDate": "2020-10-22T15:15:57Z", "type": "commit"}, {"oid": "8d71e116b7a29f5009441b76ed2ae3a5d7c0cecb", "url": "https://github.com/splicemachine/spliceengine/commit/8d71e116b7a29f5009441b76ed2ae3a5d7c0cecb", "message": "SPLICE-2289 Fix MultiProbeScan error on cross join or broadcast join.", "committedDate": "2020-10-23T05:00:59Z", "type": "commit"}, {"oid": "67b4278adf3cd7ec477b9a0c12f37e28a294c4fa", "url": "https://github.com/splicemachine/spliceengine/commit/67b4278adf3cd7ec477b9a0c12f37e28a294c4fa", "message": "SPLICE-2289 Fix scannedBaseTableRows estimation (use OLTP execution when appropriate).", "committedDate": "2020-10-23T06:20:49Z", "type": "commit"}, {"oid": "09a3b5177605ce0160936e40da401d7d9e9c5cde", "url": "https://github.com/splicemachine/spliceengine/commit/09a3b5177605ce0160936e40da401d7d9e9c5cde", "message": "SPLICE-2289 Fix assertion error getting numSparkNodes from Zookeeper.", "committedDate": "2020-10-23T08:12:12Z", "type": "commit"}, {"oid": "096dbcdf051a8f04ed2955c3ad015c9f865c6b82", "url": "https://github.com/splicemachine/spliceengine/commit/096dbcdf051a8f04ed2955c3ad015c9f865c6b82", "message": "SPLICE-2289 Add splice.optimizer.disablePerParallelTaskJoinCosting database property.", "committedDate": "2020-10-24T21:09:08Z", "type": "commit"}, {"oid": "238e65157c3c1e97a8bd19bd00c82c171378b64a", "url": "https://github.com/splicemachine/spliceengine/commit/238e65157c3c1e97a8bd19bd00c82c171378b64a", "message": "SPLICE-2289 Fix DB-10571 test case.", "committedDate": "2020-10-27T00:14:17Z", "type": "commit"}]}