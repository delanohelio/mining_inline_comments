{"pr_number": 3822, "pr_title": "DB-9843 External Tables: Own directory partition parsing", "pr_createdAt": "2020-07-14T11:44:35Z", "pr_url": "https://github.com/splicemachine/spliceengine/pull/3822", "timeline": [{"oid": "333538dd654315baad564dde0e27deab2bfbf5a6", "url": "https://github.com/splicemachine/spliceengine/commit/333538dd654315baad564dde0e27deab2bfbf5a6", "message": "DB-9843 External Tables: Don't copy files for schema infer", "committedDate": "2020-07-14T11:44:53Z", "type": "forcePushed"}, {"oid": "049d627b5c410f8da02e4104df334b888f7d75e2", "url": "https://github.com/splicemachine/spliceengine/commit/049d627b5c410f8da02e4104df334b888f7d75e2", "message": "DB-9843 External Tables: Don't copy files for schema infer", "committedDate": "2020-07-14T13:05:13Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg5ODEyMg==", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r454898122", "bodyText": "I think this is doing an explicit cast to Date when we expect that because the inferred type might not be it.", "author": "dgomezferro", "createdAt": "2020-07-15T08:55:30Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -383,33 +381,34 @@ public Partitioner getPartitioner(DataSet<ExecRow> dataSet, ExecRow template, in\n                                        boolean useSample, double sampleFraction) throws StandardException {\n         try {\n             Dataset<Row> table = null;\n-            try {\n-                DataSet<V> empty_ds = checkExistingOrEmpty( location, context );\n-                if( empty_ds != null ) return empty_ds;\n-\n-                StructType copy = new StructType(Arrays.copyOf(tableSchema.fields(), tableSchema.fields().length));\n+            StructType copy = new StructType(Arrays.copyOf(tableSchema.fields(), tableSchema.fields().length));\n \n-                // Infer schema from external files\\\n-                StructType dataSchema = ExternalTableUtils.getDataSchema(this, tableSchema, partitionColumnMap, location, \"a\");\n+            // Infer schema from external files\n+            // todo: this is slow on bigger directories, as it's calling getExternalFileSchema,\n+            // which will do a spark.read() before doing the spark.read() here ...\n+            StructType dataSchema = ExternalTableUtils.getDataSchema(this, tableSchema, partitionColumnMap, location, \"a\");\n+            if(dataSchema == null)\n+                return getEmpty(RDDName.EMPTY_DATA_SET.displayName(), context);\n \n+            try {\n                 SparkSession spark = SpliceSpark.getSession();\n                 // Creates a DataFrame from a specified file\n                 table = spark.read().schema(dataSchema).format(\"com.databricks.spark.avro\").load(location);\n+            } catch (Exception e) {\n+                return handleExceptionSparkRead(e, location, false);\n+            }\n \n-                int i = 0;\n-                for (StructField sf : copy.fields()) {\n-                    if (sf.dataType().sameType(DataTypes.DateType)) {\n-                        String colName = table.schema().fields()[i].name();\n-                        table = table.withColumn(colName, table.col(colName).cast(DataTypes.DateType));\n-                    }\n-                    i++;\n+            // todo: find out what this is doing and comment it", "originalCommit": "049d627b5c410f8da02e4104df334b888f7d75e2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjkzOTQ1Ng==", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r516939456", "bodyText": "thanks, comment removed!", "author": "martinrupp", "createdAt": "2020-11-03T20:33:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg5ODEyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg5OTU3MA==", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r454899570", "bodyText": "We borrowed Presto's ORC reader because it was faster than Spark's reader at some point in the past. We need to revisit that decision, and even if we stick with Presto's we'd need to bring all changes from the last few months/years", "author": "dgomezferro", "createdAt": "2020-07-15T08:57:46Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -707,6 +698,10 @@ public Boolean isCached(long conglomerateId) throws StandardException {\n         assert baseColumnMap != null:\"baseColumnMap Null\";\n         assert partitionColumnMap != null:\"partitionColumnMap Null\";\n         try {\n+            // todo: check why we're using our own reader here.", "originalCommit": "049d627b5c410f8da02e4104df334b888f7d75e2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkwMTgyNg==", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r454901826", "bodyText": "Since you made the effort to document the class (thanks!) please use Javadoc comments to visibilize them", "author": "dgomezferro", "createdAt": "2020-07-15T09:01:08Z", "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/CreateTableTypeHelper.java", "diffHunk": "@@ -0,0 +1,247 @@\n+package com.splicemachine.derby.impl.sql.execute.operations;\n+\n+import org.junit.Assert;\n+\n+import java.sql.Clob;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.time.format.DateTimeFormatter;\n+import java.util.ArrayList;\n+import java.util.Locale;\n+\n+/// a helper class to define column types, and then create external tables and insert data into them", "originalCommit": "049d627b5c410f8da02e4104df334b888f7d75e2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAzMzMwMQ==", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r455033301", "bodyText": "fixed in #3811", "author": "martinrupp", "createdAt": "2020-07-15T13:02:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkwMTgyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkwMTk4MA==", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r454901980", "bodyText": "Ditto here and all other methods", "author": "dgomezferro", "createdAt": "2020-07-15T09:01:26Z", "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/CreateTableTypeHelper.java", "diffHunk": "@@ -0,0 +1,247 @@\n+package com.splicemachine.derby.impl.sql.execute.operations;\n+\n+import org.junit.Assert;\n+\n+import java.sql.Clob;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Types;\n+import java.time.format.DateTimeFormatter;\n+import java.util.ArrayList;\n+import java.util.Locale;\n+\n+/// a helper class to define column types, and then create external tables and insert data into them\n+/// to make writing tests for all column types easier.\n+\n+public class CreateTableTypeHelper {\n+    /// @param types: an array of Types that should be used", "originalCommit": "049d627b5c410f8da02e4104df334b888f7d75e2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAzMzM3MQ==", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r455033371", "bodyText": "fixed in #3811", "author": "martinrupp", "createdAt": "2020-07-15T13:02:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkwMTk4MA=="}], "type": "inlineReview"}, {"oid": "d6428f4548be6134177362c7c70588125c893a79", "url": "https://github.com/splicemachine/spliceengine/commit/d6428f4548be6134177362c7c70588125c893a79", "message": "DB-9843 External Tables: Don't copy files for schema infer", "committedDate": "2020-07-24T08:52:02Z", "type": "forcePushed"}, {"oid": "56e4cd65dc3f5d9801671994e987dcf688601fc5", "url": "https://github.com/splicemachine/spliceengine/commit/56e4cd65dc3f5d9801671994e987dcf688601fc5", "message": "DB-9843 External Tables: Don't copy files for schema infer", "committedDate": "2020-07-30T11:13:25Z", "type": "forcePushed"}, {"oid": "ee19754c5ff8befec111463d28b5ceaa2b34c050", "url": "https://github.com/splicemachine/spliceengine/commit/ee19754c5ff8befec111463d28b5ceaa2b34c050", "message": "DB-9843 External Tables: Don't copy files for schema infer", "committedDate": "2020-07-31T07:47:58Z", "type": "forcePushed"}, {"oid": "f6927d24471239535d2ba8f983689260487a0d17", "url": "https://github.com/splicemachine/spliceengine/commit/f6927d24471239535d2ba8f983689260487a0d17", "message": "DB-9843 External Tables: Don't copy files for schema infer", "committedDate": "2020-08-03T10:21:11Z", "type": "forcePushed"}, {"oid": "1900c94cc1048c83443414ebf2312b8403554a90", "url": "https://github.com/splicemachine/spliceengine/commit/1900c94cc1048c83443414ebf2312b8403554a90", "message": "DB-9843 fix avro", "committedDate": "2020-10-01T16:28:01Z", "type": "forcePushed"}, {"oid": "1c363f4678850596720d5e61ecc693bf9146b408", "url": "https://github.com/splicemachine/spliceengine/commit/1c363f4678850596720d5e61ecc693bf9146b408", "message": "DB-9843 extract CountingListener", "committedDate": "2020-10-02T09:40:42Z", "type": "forcePushed"}, {"oid": "6bea59b580ea1baf9b14cb8d0c31d697f8277c12", "url": "https://github.com/splicemachine/spliceengine/commit/6bea59b580ea1baf9b14cb8d0c31d697f8277c12", "message": "DB-9843 External Tables: Don't copy files for schema infer", "committedDate": "2020-10-22T15:03:39Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI0NDExNw==", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r510244117", "bodyText": "note: this small function is added to remove a bigger Avro write function in SparkDataSet", "author": "martinrupp", "createdAt": "2020-10-22T15:14:12Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/NativeSparkDataSet.java", "diffHunk": "@@ -1136,11 +1026,29 @@ public static boolean joinProjectsOnlyLeftTableColumns(JoinType joinType) {\n     }\n \n     @SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n-    public DataSet<ExecRow> writeAvroFile(DataSetProcessor dsp, int[] partitionBy, String location,\n-                                          String compression, OperationContext context) throws StandardException {\n+    public DataSet<ExecRow> writeAvroFile(DataSetProcessor dsp,", "originalCommit": "6bea59b580ea1baf9b14cb8d0c31d697f8277c12", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "fe64535ef9e6750853fbedca1a181f1ca211f5ae", "url": "https://github.com/splicemachine/spliceengine/commit/fe64535ef9e6750853fbedca1a181f1ca211f5ae", "message": "DB-9843 External Tables: Don't copy files for schema infer", "committedDate": "2020-10-22T15:39:56Z", "type": "forcePushed"}, {"oid": "f020859648f047e4b42c540360f7824cdf189f01", "url": "https://github.com/splicemachine/spliceengine/commit/f020859648f047e4b42c540360f7824cdf189f01", "message": "DB-9843 External Tables: Don't copy files for schema infer", "committedDate": "2020-10-25T21:18:43Z", "type": "forcePushed"}, {"oid": "ac41c923c40e9707832645e8e0345e378d46ceb8", "url": "https://github.com/splicemachine/spliceengine/commit/ac41c923c40e9707832645e8e0345e378d46ceb8", "message": "DB-9843 External Tables: Own directory partition parsing\n\nfixing issues:\n- DB-9843: Don't copy files for schema infer, but also don't check all files for schema -> overall faster\n- DB-10568 Partitioned by using VARCHAR but value is 1 will be infered as INT and can't be read", "committedDate": "2020-10-27T11:10:45Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA3NTYyOA==", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r513075628", "bodyText": "Remove context because it is not used", "author": "jyuanca", "createdAt": "2020-10-27T22:41:36Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/NativeSparkDataSet.java", "diffHunk": "@@ -1207,9 +1078,8 @@ public void close() {\n         }\n     }\n \n-    private DataFrameWriter getDataFrameWriter(int[] partitionBy, OperationContext context) throws StandardException {\n-        StructType tableSchema = SparkDataSet.generateTableSchema(context);\n-\n+    private DataFrameWriter getDataFrameWriter(StructType tableSchema, int[] partitionBy,\n+                                               OperationContext context) throws StandardException {", "originalCommit": "a6b959e5bc583d0e2729866297cb3c7cfe061394", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzMyNzIxMw==", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r513327213", "bodyText": "thanks, done!", "author": "martinrupp", "createdAt": "2020-10-28T10:17:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA3NTYyOA=="}], "type": "inlineReview"}, {"oid": "354bef78388c19c336f0b653777c517f1a23e002", "url": "https://github.com/splicemachine/spliceengine/commit/354bef78388c19c336f0b653777c517f1a23e002", "message": "DB-9843 fix mem", "committedDate": "2020-10-29T08:46:49Z", "type": "forcePushed"}, {"oid": "90e6b41e66291bf1d689db5582bda9d6b15c5bd3", "url": "https://github.com/splicemachine/spliceengine/commit/90e6b41e66291bf1d689db5582bda9d6b15c5bd3", "message": "DB-9843 fix rebase issues, add unit test getCsvOptions", "committedDate": "2020-11-03T10:48:29Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAzOTMzOQ==", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r517039339", "bodyText": "Good documentation.", "author": "msirek", "createdAt": "2020-11-04T00:54:39Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -487,41 +458,65 @@ public StructType getExternalFileSchema(String storedAs, String location, boolea\n                         dataset = SpliceSpark.getSession()\n                                 .read()\n                                 .option(\"mergeSchema\", mergeSchemaOption)\n-                                .parquet(location);\n+                                .parquet(rootPath);\n                     } else if (storedAs.toLowerCase().equals(\"a\")) {\n                         // spark does not support schema merging for avro\n                         dataset = SpliceSpark.getSession()\n                                 .read()\n                                 .option(\"ignoreExtension\", false)\n                                 .format(\"com.databricks.spark.avro\")\n-                                .load(location);\n+                                .load(rootPath);\n                     } else if (storedAs.toLowerCase().equals(\"o\")) {\n                         // spark does not support schema merging for orc\n                         dataset = SpliceSpark.getSession()\n                                 .read()\n-                                .orc(location);\n+                                .orc(rootPath);\n                     } else if (storedAs.toLowerCase().equals(\"t\")) {\n                         // spark-2.2.0: commons-lang3-3.3.2 does not support 'XXX' timezone, specify 'ZZ' instead\n-                        dataset = SpliceSpark.getSession().read().options(getCsvOptions(csvOptions)).csv(location);\n+                        dataset = SpliceSpark.getSession().read().options(getCsvOptions(csvOptions)).csv(rootPath);\n                     } else {\n                         throw new UnsupportedOperationException(\"Unsupported storedAs \" + storedAs);\n                     }\n-                    dataset.printSchema();\n+                    //dataset.printSchema();\n                     schema = dataset.schema();\n                 }\n             } catch (Exception e) {\n-                handleExceptionInferSchema(e, location);\n-            } finally {\n-                if (!mergeSchema && fs != null && temp!= null && fs.exists(temp)){\n-                    fs.delete(temp, true);\n-                    SpliceLogUtils.info(LOG, \"deleted temporary directory %s\", temp);\n-                }\n+                handleExceptionInferSchema(e, rootPath);\n             }\n         }catch (Exception e) {\n             throw StandardException.newException(SQLState.EXTERNAL_TABLES_READ_FAILURE, e, e.getMessage());\n         }\n \n-        return schema;\n+        return new GetSchemaExternalResult(partition_schema, schema);\n+    }\n+\n+    /**\n+     * get the directory partitioning\n+     * @param rootPath root path of the dataset e.g. hdfs://cluster:123/path/to/directory\n+     * @param fileName file name below that path (including directories), e.g. firstCol=34/column2=HELLO/file.parquet\n+     * @param givenPartitionColumns the types as specified in CREATE TABLE ... PARTITIONED BY ( X ).\n+     *                              this is important as you can have e.g. firstCol = VARCHAR, and without this\n+     *                              we would infere firstCol=34 to be INTEGER.\n+     * @return if successful, the StructType for the partitioned columns compatible with partitionColumns.\n+     *         if we can't apply partitionColumns (e.g. have column2=HELLO, but we want column2 to be INT),\n+     *         we infere the schema without the given information, which is then used for suggest schema.\n+     */", "originalCommit": "90e6b41e66291bf1d689db5582bda9d6b15c5bd3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "2db7687aa88892b701c7bb012d3f48de7886b7fd", "url": "https://github.com/splicemachine/spliceengine/commit/2db7687aa88892b701c7bb012d3f48de7886b7fd", "message": "DB-9843 fix spotbugs", "committedDate": "2020-11-04T11:14:09Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzMwMDM4Ng==", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r517300386", "bodyText": "Do we need the EmptyFunction here?", "author": "dgomezferro", "createdAt": "2020-11-04T12:13:06Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSet.java", "diffHunk": "@@ -789,43 +773,17 @@ public static StructType generateTableSchema(OperationContext context) throws St\n                                           String compression,\n                                           OperationContext context) throws StandardException\n     {\n-        compression = SparkDataSet.getAvroCompression(compression);\n \n-        StructType dataSchema = null;\n         StructType tableSchema = generateTableSchema(context);\n-\n-        // what is this? why is this so different from parquet/orc ?\n-        // actually very close to NativeSparkDataSet.writeFile\n-        dataSchema = ExternalTableUtils.getDataSchema(dsp, tableSchema, partitionBy, location, \"a\");\n-\n+        StructType dataSchema = SparkExternalTableUtil.getDataSchemaAvro(dsp, tableSchema, partitionBy, location);\n         if (dataSchema == null)\n             dataSchema = tableSchema;\n-\n         Dataset<Row> insertDF = SpliceSpark.getSession().createDataFrame(\n-                rdd.map(new SparkSpliceFunctionWrapper<>(new CountWriteFunction(context))).map(new LocatedRowToRowAvroFunction()),\n+                rdd.map(new SparkSpliceFunctionWrapper<>(new EmptyFunction())).map(new LocatedRowToRowAvroFunction()),", "originalCommit": "2db7687aa88892b701c7bb012d3f48de7886b7fd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzAxMzI2MQ==", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r523013261", "bodyText": "i'm not exactly sure how to remove it", "author": "martinrupp", "createdAt": "2020-11-13T15:13:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzMwMDM4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzAxMzQ5MQ==", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r523013491", "bodyText": "if i remove it, it doesn't compile anymore", "author": "martinrupp", "createdAt": "2020-11-13T15:13:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzMwMDM4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzMwMTU4Mg==", "url": "https://github.com/splicemachine/spliceengine/pull/3822#discussion_r517301582", "bodyText": "It'd be nice to check for a specific Exception / Error code here", "author": "dgomezferro", "createdAt": "2020-11-04T12:15:34Z", "path": "hbase_sql/src/test/java/com/splicemachine/derby/impl/sql/execute/operations/ExternalTableUnitTests.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ *\n+ */\n+\n+package com.splicemachine.derby.impl.sql.execute.operations;\n+\n+import com.splicemachine.db.iapi.error.StandardException;\n+import com.splicemachine.derby.stream.spark.SparkExternalTableUtil;\n+import com.splicemachine.derby.stream.utils.ExternalTableUtils;\n+import com.splicemachine.system.CsvOptions;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructType;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.*;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+public class ExternalTableUnitTests {\n+\n+    @Test\n+    public void testParsePartitionsFromFiles() {\n+        String root = \"hdfs://host:123/partition_test/web_sales5/\";\n+        String[] spaths = {\n+                root + \".DS_Store\",\n+                root + \"c=3.14/ws_sold_date_sk=__HIVE_DEFAULT_PARTITION__/part-00042.c000.snappy.parquet\",\n+                root + \"_SUCCESS\",\n+                root + \"c=3.14/ws_sold_date_sk=2450817/part-01434.c000.snappy.parquet\",\n+                root + \"c=3.14/ws_sold_date_sk=2450816/part-00026.c000.snappy.parquet\",\n+                root + \"c=3.14/ws_sold_date_sk=2450818/part-00780.c000.snappy.parquet\",\n+                root + \"c=3.14/ws_sold_date_sk=2450818/part-00780.c001.snappy.parquet\",\n+                root + \"c=3.14/ws_sold_date_sk=2450818/part-00780.c002.snappy.parquet\"\n+        };\n+        Path basePath = new Path( root );\n+        HashSet<Path> basePaths = new HashSet<>(); basePaths.add(basePath);\n+\n+        List<Path> files = Arrays.stream(spaths).map(Path::new).collect(toList());\n+\n+        com.splicemachine.spark.splicemachine.PartitionSpec ps = SparkExternalTableUtil.parsePartitionsFromFiles(\n+                files, true, basePaths, null, null );\n+        Assert.assertEquals(\"StructType(StructField(c,DoubleType,true), StructField(ws_sold_date_sk,IntegerType,true))\",\n+                ps.partitionColumns().toString());\n+        Assert.assertEquals(\"List(\" +\n+                \"PartitionPath([3.14,null],\" + root + \"c=3.14/ws_sold_date_sk=__HIVE_DEFAULT_PARTITION__), \" +\n+                \"PartitionPath([3.14,2450817],\" + root + \"c=3.14/ws_sold_date_sk=2450817), \" +\n+                \"PartitionPath([3.14,2450816],\" + root + \"c=3.14/ws_sold_date_sk=2450816), \" +\n+                \"PartitionPath([3.14,2450818],\" + root + \"c=3.14/ws_sold_date_sk=2450818))\",\n+                ps.partitions().toString());\n+    }\n+\n+    @Test\n+    public void testParsePartitionsFromFiles_one_userDefined() {\n+        String root = \"hdfs://host:123/partition_test/web_sales5/\";\n+        String[] spaths = {\n+                root + \"c=3.14/ws_sold_date_sk=2450818/part-00780.c002.snappy.parquet\"\n+        };\n+        Path basePath = new Path( root );\n+        HashSet<Path> basePaths = new HashSet<>(); basePaths.add(basePath);\n+\n+        List<Path> files = Arrays.stream(spaths).map(Path::new).collect(toList());\n+\n+        StructType s = new StructType();\n+        s = s.add(\"c\", DataTypes.StringType);\n+        s = s.add(\"ws_sold_date_sk\", DataTypes.DoubleType);\n+\n+        com.splicemachine.spark.splicemachine.PartitionSpec ps = SparkExternalTableUtil.parsePartitionsFromFiles(\n+                files, true, basePaths, s, null );\n+        Assert.assertEquals(\"StructType(StructField(c,StringType,true), StructField(ws_sold_date_sk,DoubleType,true))\",\n+                ps.partitionColumns().toString());\n+        Assert.assertEquals(\"List(PartitionPath([3.14,2450818.0],\" + root + \"c=3.14/ws_sold_date_sk=2450818))\",\n+                ps.partitions().toString());\n+    }\n+\n+    @Test\n+    public void testParsePartitionsFromFiles_wrong_type() {\n+        String root = \"hdfs://host:123/partition_test/web_sales5/\";\n+        String[] spaths = {\n+                root + \"c=3.14/ws_sold_date_sk=__HIVE_DEFAULT_PARTITION__/part-00780.c002.snappy.parquet\",\n+                root + \"c=3.14/ws_sold_date_sk=HELLO/part-00780.c002.snappy.parquet\"\n+        };\n+        Path basePath = new Path( \"hdfs://host:123/partition_test/web_sales5/\" );\n+        HashSet<Path> basePaths = new HashSet<>(); basePaths.add(basePath);\n+\n+        List<Path> files = Arrays.stream(spaths).map(Path::new).collect(toList());\n+\n+        StructType s = new StructType();\n+        s = s.add(\"ws_sold_date_sk\", DataTypes.DoubleType);\n+\n+        try {\n+            com.splicemachine.spark.splicemachine.PartitionSpec ps = SparkExternalTableUtil.parsePartitionsFromFiles(\n+                    files, true, basePaths, s,null);\n+            Assert.fail(\"no exception\");\n+        }\n+        catch(Exception e)\n+        {\n+", "originalCommit": "2db7687aa88892b701c7bb012d3f48de7886b7fd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "aec060af0cf56ccefa1eb1a102e7e0c95a040691", "url": "https://github.com/splicemachine/spliceengine/commit/aec060af0cf56ccefa1eb1a102e7e0c95a040691", "message": "DB-9843 parameter DateFormat to avoid hdp/cdp differences", "committedDate": "2020-11-05T09:23:04Z", "type": "forcePushed"}, {"oid": "94dbdfba6d788d45fb0bb0f0a9a44c5528a0d763", "url": "https://github.com/splicemachine/spliceengine/commit/94dbdfba6d788d45fb0bb0f0a9a44c5528a0d763", "message": "DB-9843 fix testStringIntPartitionParsing, move license", "committedDate": "2020-11-05T16:06:39Z", "type": "forcePushed"}, {"oid": "8588b9c7a74062726bfaf57ea4b3a964d1b698c5", "url": "https://github.com/splicemachine/spliceengine/commit/8588b9c7a74062726bfaf57ea4b3a964d1b698c5", "message": "DB-9843 External Tables: Own directory partition parsing\n\nfixing issues:\n- DB-9843: Don't copy files for schema infer, but also don't check all files for schema -> overall faster\n- DB-10568 Partitioned by using VARCHAR but value is 1 will be infered as INT and can't be read", "committedDate": "2020-11-20T12:18:00Z", "type": "commit"}, {"oid": "9e3f8fc9fd61ee40cc5de550a616afcd631d12ef", "url": "https://github.com/splicemachine/spliceengine/commit/9e3f8fc9fd61ee40cc5de550a616afcd631d12ef", "message": "DB-9843 add unit tests", "committedDate": "2020-11-20T12:18:01Z", "type": "commit"}, {"oid": "391cccccd118ab319ba8ca790372245dfa56d4a0", "url": "https://github.com/splicemachine/spliceengine/commit/391cccccd118ab319ba8ca790372245dfa56d4a0", "message": "DB-9843 address review", "committedDate": "2020-11-20T12:18:01Z", "type": "commit"}, {"oid": "affa79b9740702ac4a193cbb3de45296a6b66dc7", "url": "https://github.com/splicemachine/spliceengine/commit/affa79b9740702ac4a193cbb3de45296a6b66dc7", "message": "DB-9843 fix mem", "committedDate": "2020-11-20T12:18:01Z", "type": "commit"}, {"oid": "feb1f430dff7c421eb27281d19e5e4e70cb0401a", "url": "https://github.com/splicemachine/spliceengine/commit/feb1f430dff7c421eb27281d19e5e4e70cb0401a", "message": "DB-9843 fix rebase issues, add unit test getCsvOptions", "committedDate": "2020-11-20T12:18:01Z", "type": "commit"}, {"oid": "d9e7efdfcc176a4528b4abb639901a28f120e481", "url": "https://github.com/splicemachine/spliceengine/commit/d9e7efdfcc176a4528b4abb639901a28f120e481", "message": "DB-9843 fix spotbugs", "committedDate": "2020-11-20T12:18:01Z", "type": "commit"}, {"oid": "566da997ba47984d20f6ee60ef4bdafaad07cc06", "url": "https://github.com/splicemachine/spliceengine/commit/566da997ba47984d20f6ee60ef4bdafaad07cc06", "message": "DB-9843 align with 2.8 code", "committedDate": "2020-11-20T12:18:01Z", "type": "commit"}, {"oid": "c4057236790cc6ab814cd400e710dec2b47f3350", "url": "https://github.com/splicemachine/spliceengine/commit/c4057236790cc6ab814cd400e710dec2b47f3350", "message": "DB-9843 parameter DateFormat to avoid hdp/cdp differences", "committedDate": "2020-11-20T12:18:01Z", "type": "commit"}, {"oid": "de8f6b25bd13778cf059330b328c130e2d65faf2", "url": "https://github.com/splicemachine/spliceengine/commit/de8f6b25bd13778cf059330b328c130e2d65faf2", "message": "DB-9843 fix findbugs-exclude-filter.xml", "committedDate": "2020-11-20T12:18:01Z", "type": "commit"}, {"oid": "fdf0afcba03858d7b4e95aed25688af61b3dd7b5", "url": "https://github.com/splicemachine/spliceengine/commit/fdf0afcba03858d7b4e95aed25688af61b3dd7b5", "message": "DB-9843 fix testStringIntPartitionParsing, move license", "committedDate": "2020-11-20T12:18:01Z", "type": "commit"}, {"oid": "b5a7bd221f5915274252ebd04771457f2612acc8", "url": "https://github.com/splicemachine/spliceengine/commit/b5a7bd221f5915274252ebd04771457f2612acc8", "message": "DB-9843 address code review", "committedDate": "2020-11-20T23:09:44Z", "type": "commit"}, {"oid": "b5a7bd221f5915274252ebd04771457f2612acc8", "url": "https://github.com/splicemachine/spliceengine/commit/b5a7bd221f5915274252ebd04771457f2612acc8", "message": "DB-9843 address code review", "committedDate": "2020-11-20T23:09:44Z", "type": "forcePushed"}]}