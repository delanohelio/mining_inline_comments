{"pr_number": 174, "pr_title": "Add result indices retention period", "pr_createdAt": "2020-06-24T15:54:13Z", "pr_url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/174", "timeline": [{"oid": "020f509d2f882e619ce3fff3a05be41845be2126", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/020f509d2f882e619ce3fff3a05be41845be2126", "message": "Add result indices retention period\n\nCurrently we never delete the result index, even though customers have deleted the detector.\u00a0\u00a0 An increasing amount of result indices can use significant disk space, as well as memory pressure due to the creation of rolled over indices. This PR adds retention period to anomaly results.\u00a0 We delete result indices when they are older than a retention period, which is 90 days by default. We use 90 days because that's the maximum days we allow users to view results on Kibana.\u00a0 Users can configure the retention period via the setting opendistro.anomaly_detection.ad_result_history_retention_period dynamically.\n\nAlso, previously we roll over empty result indices.\u00a0 This PR fixes that by removing the max age condition of result indices.\u00a0 So we only roll over the result index when the maximum number of documents in the index is reached.\n\nTesting done:\n*\u00a0manually tested result indices would be deleted after passing retention period.", "committedDate": "2020-06-24T15:53:27Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTAzNjkxNA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/174#discussion_r445036914", "bodyText": "Why put into two lines? How about we use logger.error(\"Fail to roll over result index\", exception); ? So we don't need to go to two lines when check log. There are maybe other request's log between these two lines, if check log manually, we need to skip other request's log.", "author": "ylwu-amzn", "createdAt": "2020-06-24T16:55:13Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/indices/AnomalyDetectionIndices.java", "diffHunk": "@@ -275,15 +285,86 @@ private boolean rolloverHistoryIndex() {\n             adResultMapping = getAnomalyResultMappings();\n         } catch (IOException e) {\n             logger.error(\"Fail to roll over AD result index, as can't get AD result index mapping\");\n-            return false;\n+            return;\n         }\n         request.getCreateIndexRequest().index(AD_RESULT_HISTORY_INDEX_PATTERN).mapping(MAPPING_TYPE, adResultMapping, XContentType.JSON);\n         request.addMaxIndexDocsCondition(historyMaxDocs);\n-        request.addMaxIndexAgeCondition(historyMaxAge);\n-        RolloverResponse response = adminClient.indices().rolloverIndex(request).actionGet(requestTimeout);\n-        if (!response.isRolledOver()) {\n-            logger.warn(\"{} not rolled over. Conditions were: {}\", AD_RESULT_HISTORY_WRITE_INDEX_ALIAS, response.getConditionStatus());\n+        adminClient.indices().rolloverIndex(request, ActionListener.wrap(response -> {\n+            if (!response.isRolledOver()) {\n+                logger.warn(\"{} not rolled over. Conditions were: {}\", AD_RESULT_HISTORY_WRITE_INDEX_ALIAS, response.getConditionStatus());\n+            } else {\n+                logger.info(\"{} rolled over. Conditions were: {}\", AD_RESULT_HISTORY_WRITE_INDEX_ALIAS, response.getConditionStatus());\n+                deleteOldHistoryIndices();\n+            }\n+        }, exception -> {\n+            logger.error(\"Fail to roll over result index\");", "originalCommit": "020f509d2f882e619ce3fff3a05be41845be2126", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA4MDE4Ng==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/174#discussion_r445080186", "bodyText": "good catch.  Fixed.", "author": "kaituo", "createdAt": "2020-06-24T18:11:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTAzNjkxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA0MTc2Ng==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/174#discussion_r445041766", "bodyText": "How about we just don't add latestToDelete to candidates  in the for loop above?", "author": "ylwu-amzn", "createdAt": "2020-06-24T17:03:11Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/indices/AnomalyDetectionIndices.java", "diffHunk": "@@ -275,15 +285,86 @@ private boolean rolloverHistoryIndex() {\n             adResultMapping = getAnomalyResultMappings();\n         } catch (IOException e) {\n             logger.error(\"Fail to roll over AD result index, as can't get AD result index mapping\");\n-            return false;\n+            return;\n         }\n         request.getCreateIndexRequest().index(AD_RESULT_HISTORY_INDEX_PATTERN).mapping(MAPPING_TYPE, adResultMapping, XContentType.JSON);\n         request.addMaxIndexDocsCondition(historyMaxDocs);\n-        request.addMaxIndexAgeCondition(historyMaxAge);\n-        RolloverResponse response = adminClient.indices().rolloverIndex(request).actionGet(requestTimeout);\n-        if (!response.isRolledOver()) {\n-            logger.warn(\"{} not rolled over. Conditions were: {}\", AD_RESULT_HISTORY_WRITE_INDEX_ALIAS, response.getConditionStatus());\n+        adminClient.indices().rolloverIndex(request, ActionListener.wrap(response -> {\n+            if (!response.isRolledOver()) {\n+                logger.warn(\"{} not rolled over. Conditions were: {}\", AD_RESULT_HISTORY_WRITE_INDEX_ALIAS, response.getConditionStatus());\n+            } else {\n+                logger.info(\"{} rolled over. Conditions were: {}\", AD_RESULT_HISTORY_WRITE_INDEX_ALIAS, response.getConditionStatus());\n+                deleteOldHistoryIndices();\n+            }\n+        }, exception -> {\n+            logger.error(\"Fail to roll over result index\");\n+            logger.error(exception);\n+        }));\n+    }\n+\n+    private void deleteOldHistoryIndices() {\n+        Set<String> candidates = new HashSet<String>();\n+\n+        ClusterStateRequest clusterStateRequest = new ClusterStateRequest()\n+            .clear()\n+            .indices(AnomalyDetectionIndices.ALL_AD_RESULTS_INDEX_PATTERN)\n+            .metadata(true)\n+            .local(true)\n+            .indicesOptions(IndicesOptions.strictExpand());\n+\n+        adminClient.cluster().state(clusterStateRequest, ActionListener.wrap(clusterStateResponse -> {\n+            String latestToDelete = null;\n+            long latest = Long.MIN_VALUE;\n+            for (ObjectCursor<IndexMetadata> cursor : clusterStateResponse.getState().metadata().indices().values()) {\n+                IndexMetadata indexMetaData = cursor.value;\n+                long creationTime = indexMetaData.getCreationDate();\n+\n+                if ((Instant.now().toEpochMilli() - creationTime) > historyRetentionPeriod.millis()) {\n+                    String indexName = indexMetaData.getIndex().getName();\n+                    candidates.add(indexName);\n+                    if (latest < creationTime) {\n+                        latest = creationTime;\n+                        latestToDelete = indexName;\n+                    }\n+                }\n+            }\n+\n+            if (candidates.size() > 1) {\n+                // delete all indices except the last one because the last one may contain docs newer than the retention period\n+                candidates.remove(latestToDelete);", "originalCommit": "020f509d2f882e619ce3fff3a05be41845be2126", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA3NjQ2Mw==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/174#discussion_r445076463", "bodyText": "How do we do that?  We don't know latestToDelete before looping through all indices.", "author": "kaituo", "createdAt": "2020-06-24T18:04:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA0MTc2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE2OTE2OQ==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/174#discussion_r445169169", "bodyText": "Can we iterate indices reversely and ignore the first index which meet this condition (Instant.now().toEpochMilli() - creationTime) > historyRetentionPeriod.millis() ?\nThis is a minor suggestion. Ignore it if it's hard/impossible to do so.", "author": "ylwu-amzn", "createdAt": "2020-06-24T21:00:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA0MTc2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI4ODU3MA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/174#discussion_r446288570", "bodyText": "The indices has no order. So we cannot do that.", "author": "kaituo", "createdAt": "2020-06-26T16:33:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA0MTc2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA0NDg5NQ==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/174#discussion_r445044895", "bodyText": "This exception is for catching exception of clusterStateRequest? Not quite get why log Fail to get creation dates here.  The exception only caused by get creation dates ?", "author": "ylwu-amzn", "createdAt": "2020-06-24T17:08:37Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/indices/AnomalyDetectionIndices.java", "diffHunk": "@@ -275,15 +285,86 @@ private boolean rolloverHistoryIndex() {\n             adResultMapping = getAnomalyResultMappings();\n         } catch (IOException e) {\n             logger.error(\"Fail to roll over AD result index, as can't get AD result index mapping\");\n-            return false;\n+            return;\n         }\n         request.getCreateIndexRequest().index(AD_RESULT_HISTORY_INDEX_PATTERN).mapping(MAPPING_TYPE, adResultMapping, XContentType.JSON);\n         request.addMaxIndexDocsCondition(historyMaxDocs);\n-        request.addMaxIndexAgeCondition(historyMaxAge);\n-        RolloverResponse response = adminClient.indices().rolloverIndex(request).actionGet(requestTimeout);\n-        if (!response.isRolledOver()) {\n-            logger.warn(\"{} not rolled over. Conditions were: {}\", AD_RESULT_HISTORY_WRITE_INDEX_ALIAS, response.getConditionStatus());\n+        adminClient.indices().rolloverIndex(request, ActionListener.wrap(response -> {\n+            if (!response.isRolledOver()) {\n+                logger.warn(\"{} not rolled over. Conditions were: {}\", AD_RESULT_HISTORY_WRITE_INDEX_ALIAS, response.getConditionStatus());\n+            } else {\n+                logger.info(\"{} rolled over. Conditions were: {}\", AD_RESULT_HISTORY_WRITE_INDEX_ALIAS, response.getConditionStatus());\n+                deleteOldHistoryIndices();\n+            }\n+        }, exception -> {\n+            logger.error(\"Fail to roll over result index\");\n+            logger.error(exception);\n+        }));\n+    }\n+\n+    private void deleteOldHistoryIndices() {\n+        Set<String> candidates = new HashSet<String>();\n+\n+        ClusterStateRequest clusterStateRequest = new ClusterStateRequest()\n+            .clear()\n+            .indices(AnomalyDetectionIndices.ALL_AD_RESULTS_INDEX_PATTERN)\n+            .metadata(true)\n+            .local(true)\n+            .indicesOptions(IndicesOptions.strictExpand());\n+\n+        adminClient.cluster().state(clusterStateRequest, ActionListener.wrap(clusterStateResponse -> {\n+            String latestToDelete = null;\n+            long latest = Long.MIN_VALUE;\n+            for (ObjectCursor<IndexMetadata> cursor : clusterStateResponse.getState().metadata().indices().values()) {\n+                IndexMetadata indexMetaData = cursor.value;\n+                long creationTime = indexMetaData.getCreationDate();\n+\n+                if ((Instant.now().toEpochMilli() - creationTime) > historyRetentionPeriod.millis()) {\n+                    String indexName = indexMetaData.getIndex().getName();\n+                    candidates.add(indexName);\n+                    if (latest < creationTime) {\n+                        latest = creationTime;\n+                        latestToDelete = indexName;\n+                    }\n+                }\n+            }\n+\n+            if (candidates.size() > 1) {\n+                // delete all indices except the last one because the last one may contain docs newer than the retention period\n+                candidates.remove(latestToDelete);\n+                String[] toDelete = candidates.toArray(Strings.EMPTY_ARRAY);\n+                DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(toDelete);\n+                adminClient.indices().delete(deleteIndexRequest, ActionListener.wrap(deleteIndexResponse -> {\n+                    if (!deleteIndexResponse.isAcknowledged()) {\n+                        logger\n+                            .error(\n+                                \"Could not delete one or more Anomaly result indices: {}. Retrying one by one.\",\n+                                Arrays.toString(toDelete)\n+                            );\n+                        deleteIndexIteration(toDelete);\n+                    } else {\n+                        logger.error(\"Succeeded in deleting expired anomaly result indices: {}.\", Arrays.toString(toDelete));\n+                    }\n+                }, exception -> { deleteIndexIteration(toDelete); }));\n+            }\n+        }, exception -> { logger.error(\"Fail to get creation dates of result indices\"); }));", "originalCommit": "020f509d2f882e619ce3fff3a05be41845be2126", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA3ODgwMw==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/174#discussion_r445078803", "bodyText": "good catch.  Changed to more general error message.", "author": "kaituo", "createdAt": "2020-06-24T18:09:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA0NDg5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA0NjA5Ng==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/174#discussion_r445046096", "bodyText": "Same here, why put into two lines?", "author": "ylwu-amzn", "createdAt": "2020-06-24T17:10:46Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/indices/AnomalyDetectionIndices.java", "diffHunk": "@@ -275,15 +285,86 @@ private boolean rolloverHistoryIndex() {\n             adResultMapping = getAnomalyResultMappings();\n         } catch (IOException e) {\n             logger.error(\"Fail to roll over AD result index, as can't get AD result index mapping\");\n-            return false;\n+            return;\n         }\n         request.getCreateIndexRequest().index(AD_RESULT_HISTORY_INDEX_PATTERN).mapping(MAPPING_TYPE, adResultMapping, XContentType.JSON);\n         request.addMaxIndexDocsCondition(historyMaxDocs);\n-        request.addMaxIndexAgeCondition(historyMaxAge);\n-        RolloverResponse response = adminClient.indices().rolloverIndex(request).actionGet(requestTimeout);\n-        if (!response.isRolledOver()) {\n-            logger.warn(\"{} not rolled over. Conditions were: {}\", AD_RESULT_HISTORY_WRITE_INDEX_ALIAS, response.getConditionStatus());\n+        adminClient.indices().rolloverIndex(request, ActionListener.wrap(response -> {\n+            if (!response.isRolledOver()) {\n+                logger.warn(\"{} not rolled over. Conditions were: {}\", AD_RESULT_HISTORY_WRITE_INDEX_ALIAS, response.getConditionStatus());\n+            } else {\n+                logger.info(\"{} rolled over. Conditions were: {}\", AD_RESULT_HISTORY_WRITE_INDEX_ALIAS, response.getConditionStatus());\n+                deleteOldHistoryIndices();\n+            }\n+        }, exception -> {\n+            logger.error(\"Fail to roll over result index\");\n+            logger.error(exception);\n+        }));\n+    }\n+\n+    private void deleteOldHistoryIndices() {\n+        Set<String> candidates = new HashSet<String>();\n+\n+        ClusterStateRequest clusterStateRequest = new ClusterStateRequest()\n+            .clear()\n+            .indices(AnomalyDetectionIndices.ALL_AD_RESULTS_INDEX_PATTERN)\n+            .metadata(true)\n+            .local(true)\n+            .indicesOptions(IndicesOptions.strictExpand());\n+\n+        adminClient.cluster().state(clusterStateRequest, ActionListener.wrap(clusterStateResponse -> {\n+            String latestToDelete = null;\n+            long latest = Long.MIN_VALUE;\n+            for (ObjectCursor<IndexMetadata> cursor : clusterStateResponse.getState().metadata().indices().values()) {\n+                IndexMetadata indexMetaData = cursor.value;\n+                long creationTime = indexMetaData.getCreationDate();\n+\n+                if ((Instant.now().toEpochMilli() - creationTime) > historyRetentionPeriod.millis()) {\n+                    String indexName = indexMetaData.getIndex().getName();\n+                    candidates.add(indexName);\n+                    if (latest < creationTime) {\n+                        latest = creationTime;\n+                        latestToDelete = indexName;\n+                    }\n+                }\n+            }\n+\n+            if (candidates.size() > 1) {\n+                // delete all indices except the last one because the last one may contain docs newer than the retention period\n+                candidates.remove(latestToDelete);\n+                String[] toDelete = candidates.toArray(Strings.EMPTY_ARRAY);\n+                DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(toDelete);\n+                adminClient.indices().delete(deleteIndexRequest, ActionListener.wrap(deleteIndexResponse -> {\n+                    if (!deleteIndexResponse.isAcknowledged()) {\n+                        logger\n+                            .error(\n+                                \"Could not delete one or more Anomaly result indices: {}. Retrying one by one.\",\n+                                Arrays.toString(toDelete)\n+                            );\n+                        deleteIndexIteration(toDelete);\n+                    } else {\n+                        logger.error(\"Succeeded in deleting expired anomaly result indices: {}.\", Arrays.toString(toDelete));\n+                    }\n+                }, exception -> { deleteIndexIteration(toDelete); }));\n+            }\n+        }, exception -> { logger.error(\"Fail to get creation dates of result indices\"); }));\n+    }\n+\n+    private void deleteIndexIteration(String[] toDelete) {\n+        for (String index : toDelete) {\n+            DeleteIndexRequest singleDeleteRequest = new DeleteIndexRequest(index);\n+            adminClient.indices().delete(singleDeleteRequest, ActionListener.wrap(singleDeleteResponse -> {\n+                if (!singleDeleteResponse.isAcknowledged()) {\n+                    logger.error(\"Retrying deleting {} does not succeed.\", index);\n+                }\n+            }, exception -> {\n+                if (exception instanceof IndexNotFoundException) {\n+                    logger.info(\"{} was already deleted.\", index);\n+                } else {\n+                    logger.error(\"Retrying deleting {} does not succeed.\", index);", "originalCommit": "020f509d2f882e619ce3fff3a05be41845be2126", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA4MjUyNA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/174#discussion_r445082524", "bodyText": "Fixed", "author": "kaituo", "createdAt": "2020-06-24T18:15:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA0NjA5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA1MDkyMw==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/174#discussion_r445050923", "bodyText": "Is it necessary to use two rounds of deletion? Roll over will be triggered periodically, so next run can re-delete the failed indices.\nIf fail to delete all indices due to some error, will it help we re-delete them immediately? Error may be still there", "author": "ylwu-amzn", "createdAt": "2020-06-24T17:19:28Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/indices/AnomalyDetectionIndices.java", "diffHunk": "@@ -275,15 +285,86 @@ private boolean rolloverHistoryIndex() {\n             adResultMapping = getAnomalyResultMappings();\n         } catch (IOException e) {\n             logger.error(\"Fail to roll over AD result index, as can't get AD result index mapping\");\n-            return false;\n+            return;\n         }\n         request.getCreateIndexRequest().index(AD_RESULT_HISTORY_INDEX_PATTERN).mapping(MAPPING_TYPE, adResultMapping, XContentType.JSON);\n         request.addMaxIndexDocsCondition(historyMaxDocs);\n-        request.addMaxIndexAgeCondition(historyMaxAge);\n-        RolloverResponse response = adminClient.indices().rolloverIndex(request).actionGet(requestTimeout);\n-        if (!response.isRolledOver()) {\n-            logger.warn(\"{} not rolled over. Conditions were: {}\", AD_RESULT_HISTORY_WRITE_INDEX_ALIAS, response.getConditionStatus());\n+        adminClient.indices().rolloverIndex(request, ActionListener.wrap(response -> {\n+            if (!response.isRolledOver()) {\n+                logger.warn(\"{} not rolled over. Conditions were: {}\", AD_RESULT_HISTORY_WRITE_INDEX_ALIAS, response.getConditionStatus());\n+            } else {\n+                logger.info(\"{} rolled over. Conditions were: {}\", AD_RESULT_HISTORY_WRITE_INDEX_ALIAS, response.getConditionStatus());\n+                deleteOldHistoryIndices();\n+            }\n+        }, exception -> {\n+            logger.error(\"Fail to roll over result index\");\n+            logger.error(exception);\n+        }));\n+    }\n+\n+    private void deleteOldHistoryIndices() {\n+        Set<String> candidates = new HashSet<String>();\n+\n+        ClusterStateRequest clusterStateRequest = new ClusterStateRequest()\n+            .clear()\n+            .indices(AnomalyDetectionIndices.ALL_AD_RESULTS_INDEX_PATTERN)\n+            .metadata(true)\n+            .local(true)\n+            .indicesOptions(IndicesOptions.strictExpand());\n+\n+        adminClient.cluster().state(clusterStateRequest, ActionListener.wrap(clusterStateResponse -> {\n+            String latestToDelete = null;\n+            long latest = Long.MIN_VALUE;\n+            for (ObjectCursor<IndexMetadata> cursor : clusterStateResponse.getState().metadata().indices().values()) {\n+                IndexMetadata indexMetaData = cursor.value;\n+                long creationTime = indexMetaData.getCreationDate();\n+\n+                if ((Instant.now().toEpochMilli() - creationTime) > historyRetentionPeriod.millis()) {\n+                    String indexName = indexMetaData.getIndex().getName();\n+                    candidates.add(indexName);\n+                    if (latest < creationTime) {\n+                        latest = creationTime;\n+                        latestToDelete = indexName;\n+                    }\n+                }\n+            }\n+\n+            if (candidates.size() > 1) {\n+                // delete all indices except the last one because the last one may contain docs newer than the retention period\n+                candidates.remove(latestToDelete);\n+                String[] toDelete = candidates.toArray(Strings.EMPTY_ARRAY);\n+                DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(toDelete);\n+                adminClient.indices().delete(deleteIndexRequest, ActionListener.wrap(deleteIndexResponse -> {\n+                    if (!deleteIndexResponse.isAcknowledged()) {\n+                        logger\n+                            .error(\n+                                \"Could not delete one or more Anomaly result indices: {}. Retrying one by one.\",\n+                                Arrays.toString(toDelete)\n+                            );\n+                        deleteIndexIteration(toDelete);", "originalCommit": "020f509d2f882e619ce3fff3a05be41845be2126", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA3NzI3MQ==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/174#discussion_r445077271", "bodyText": "By default, we need to wait for 12 hours before re-deleting.  Adding a retry can mitigate some disk/memory issue without waiting for too long.  This is best effort and we cannot guarantee this would help.  We don't retry endlessly.  Just once.", "author": "kaituo", "createdAt": "2020-06-24T18:06:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA1MDkyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE3MTE1Ng==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/174#discussion_r445171156", "bodyText": "Can we get which index failed to delete? Can we delete these failed indices in one call?", "author": "ylwu-amzn", "createdAt": "2020-06-24T21:04:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA1MDkyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI5NjQ3NA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/174#discussion_r446296474", "bodyText": "I don't know how to.  Delete response is of AcknowledgedResponse type that contains only isAcknowledged field.", "author": "kaituo", "createdAt": "2020-06-26T16:48:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA1MDkyMw=="}], "type": "inlineReview"}, {"oid": "bbf1b5466f1a2082e5da389720a51d7e684f1387", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/bbf1b5466f1a2082e5da389720a51d7e684f1387", "message": "refactor log lines", "committedDate": "2020-06-24T18:21:29Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE3NzAwMw==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/174#discussion_r445177003", "bodyText": "logger.info", "author": "yizheliu-amazon", "createdAt": "2020-06-24T21:17:23Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/indices/AnomalyDetectionIndices.java", "diffHunk": "@@ -275,15 +286,82 @@ private boolean rolloverHistoryIndex() {\n             adResultMapping = getAnomalyResultMappings();\n         } catch (IOException e) {\n             logger.error(\"Fail to roll over AD result index, as can't get AD result index mapping\");\n-            return false;\n+            return;\n         }\n         request.getCreateIndexRequest().index(AD_RESULT_HISTORY_INDEX_PATTERN).mapping(MAPPING_TYPE, adResultMapping, XContentType.JSON);\n         request.addMaxIndexDocsCondition(historyMaxDocs);\n-        request.addMaxIndexAgeCondition(historyMaxAge);\n-        RolloverResponse response = adminClient.indices().rolloverIndex(request).actionGet(requestTimeout);\n-        if (!response.isRolledOver()) {\n-            logger.warn(\"{} not rolled over. Conditions were: {}\", AD_RESULT_HISTORY_WRITE_INDEX_ALIAS, response.getConditionStatus());\n+        adminClient.indices().rolloverIndex(request, ActionListener.wrap(response -> {\n+            if (!response.isRolledOver()) {\n+                logger.warn(\"{} not rolled over. Conditions were: {}\", AD_RESULT_HISTORY_WRITE_INDEX_ALIAS, response.getConditionStatus());\n+            } else {\n+                logger.info(\"{} rolled over. Conditions were: {}\", AD_RESULT_HISTORY_WRITE_INDEX_ALIAS, response.getConditionStatus());\n+                deleteOldHistoryIndices();\n+            }\n+        }, exception -> { logger.error(\"Fail to roll over result index\", exception); }));\n+    }\n+\n+    private void deleteOldHistoryIndices() {\n+        Set<String> candidates = new HashSet<String>();\n+\n+        ClusterStateRequest clusterStateRequest = new ClusterStateRequest()\n+            .clear()\n+            .indices(AnomalyDetectionIndices.ALL_AD_RESULTS_INDEX_PATTERN)\n+            .metadata(true)\n+            .local(true)\n+            .indicesOptions(IndicesOptions.strictExpand());\n+\n+        adminClient.cluster().state(clusterStateRequest, ActionListener.wrap(clusterStateResponse -> {\n+            String latestToDelete = null;\n+            long latest = Long.MIN_VALUE;\n+            for (ObjectCursor<IndexMetadata> cursor : clusterStateResponse.getState().metadata().indices().values()) {\n+                IndexMetadata indexMetaData = cursor.value;\n+                long creationTime = indexMetaData.getCreationDate();\n+\n+                if ((Instant.now().toEpochMilli() - creationTime) > historyRetentionPeriod.millis()) {\n+                    String indexName = indexMetaData.getIndex().getName();\n+                    candidates.add(indexName);\n+                    if (latest < creationTime) {\n+                        latest = creationTime;\n+                        latestToDelete = indexName;\n+                    }\n+                }\n+            }\n+\n+            if (candidates.size() > 1) {\n+                // delete all indices except the last one because the last one may contain docs newer than the retention period\n+                candidates.remove(latestToDelete);\n+                String[] toDelete = candidates.toArray(Strings.EMPTY_ARRAY);\n+                DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(toDelete);\n+                adminClient.indices().delete(deleteIndexRequest, ActionListener.wrap(deleteIndexResponse -> {\n+                    if (!deleteIndexResponse.isAcknowledged()) {\n+                        logger\n+                            .error(\n+                                \"Could not delete one or more Anomaly result indices: {}. Retrying one by one.\",\n+                                Arrays.toString(toDelete)\n+                            );\n+                        deleteIndexIteration(toDelete);\n+                    } else {\n+                        logger.error(\"Succeeded in deleting expired anomaly result indices: {}.\", Arrays.toString(toDelete));", "originalCommit": "bbf1b5466f1a2082e5da389720a51d7e684f1387", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI5NjcyNA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/174#discussion_r446296724", "bodyText": "good catch. Fixed.", "author": "kaituo", "createdAt": "2020-06-26T16:48:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE3NzAwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE3NzIzMQ==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/174#discussion_r445177231", "bodyText": "maybe we can log the exception before retrying", "author": "yizheliu-amazon", "createdAt": "2020-06-24T21:17:53Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/indices/AnomalyDetectionIndices.java", "diffHunk": "@@ -275,15 +286,82 @@ private boolean rolloverHistoryIndex() {\n             adResultMapping = getAnomalyResultMappings();\n         } catch (IOException e) {\n             logger.error(\"Fail to roll over AD result index, as can't get AD result index mapping\");\n-            return false;\n+            return;\n         }\n         request.getCreateIndexRequest().index(AD_RESULT_HISTORY_INDEX_PATTERN).mapping(MAPPING_TYPE, adResultMapping, XContentType.JSON);\n         request.addMaxIndexDocsCondition(historyMaxDocs);\n-        request.addMaxIndexAgeCondition(historyMaxAge);\n-        RolloverResponse response = adminClient.indices().rolloverIndex(request).actionGet(requestTimeout);\n-        if (!response.isRolledOver()) {\n-            logger.warn(\"{} not rolled over. Conditions were: {}\", AD_RESULT_HISTORY_WRITE_INDEX_ALIAS, response.getConditionStatus());\n+        adminClient.indices().rolloverIndex(request, ActionListener.wrap(response -> {\n+            if (!response.isRolledOver()) {\n+                logger.warn(\"{} not rolled over. Conditions were: {}\", AD_RESULT_HISTORY_WRITE_INDEX_ALIAS, response.getConditionStatus());\n+            } else {\n+                logger.info(\"{} rolled over. Conditions were: {}\", AD_RESULT_HISTORY_WRITE_INDEX_ALIAS, response.getConditionStatus());\n+                deleteOldHistoryIndices();\n+            }\n+        }, exception -> { logger.error(\"Fail to roll over result index\", exception); }));\n+    }\n+\n+    private void deleteOldHistoryIndices() {\n+        Set<String> candidates = new HashSet<String>();\n+\n+        ClusterStateRequest clusterStateRequest = new ClusterStateRequest()\n+            .clear()\n+            .indices(AnomalyDetectionIndices.ALL_AD_RESULTS_INDEX_PATTERN)\n+            .metadata(true)\n+            .local(true)\n+            .indicesOptions(IndicesOptions.strictExpand());\n+\n+        adminClient.cluster().state(clusterStateRequest, ActionListener.wrap(clusterStateResponse -> {\n+            String latestToDelete = null;\n+            long latest = Long.MIN_VALUE;\n+            for (ObjectCursor<IndexMetadata> cursor : clusterStateResponse.getState().metadata().indices().values()) {\n+                IndexMetadata indexMetaData = cursor.value;\n+                long creationTime = indexMetaData.getCreationDate();\n+\n+                if ((Instant.now().toEpochMilli() - creationTime) > historyRetentionPeriod.millis()) {\n+                    String indexName = indexMetaData.getIndex().getName();\n+                    candidates.add(indexName);\n+                    if (latest < creationTime) {\n+                        latest = creationTime;\n+                        latestToDelete = indexName;\n+                    }\n+                }\n+            }\n+\n+            if (candidates.size() > 1) {\n+                // delete all indices except the last one because the last one may contain docs newer than the retention period\n+                candidates.remove(latestToDelete);\n+                String[] toDelete = candidates.toArray(Strings.EMPTY_ARRAY);\n+                DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(toDelete);\n+                adminClient.indices().delete(deleteIndexRequest, ActionListener.wrap(deleteIndexResponse -> {\n+                    if (!deleteIndexResponse.isAcknowledged()) {\n+                        logger\n+                            .error(\n+                                \"Could not delete one or more Anomaly result indices: {}. Retrying one by one.\",\n+                                Arrays.toString(toDelete)\n+                            );\n+                        deleteIndexIteration(toDelete);\n+                    } else {\n+                        logger.error(\"Succeeded in deleting expired anomaly result indices: {}.\", Arrays.toString(toDelete));\n+                    }\n+                }, exception -> { deleteIndexIteration(toDelete); }));", "originalCommit": "bbf1b5466f1a2082e5da389720a51d7e684f1387", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI5ODA4OQ==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/174#discussion_r446298089", "bodyText": "added", "author": "kaituo", "createdAt": "2020-06-26T16:50:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE3NzIzMQ=="}], "type": "inlineReview"}, {"oid": "7fec24da087feda154d15756f5cea17a14faa630", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/7fec24da087feda154d15756f5cea17a14faa630", "message": "Add UT", "committedDate": "2020-06-29T16:25:10Z", "type": "commit"}]}