{"pr_number": 1297, "pr_title": "SOLR-14253 Replace various sleep calls with ZK waits", "pr_createdAt": "2020-02-27T18:08:02Z", "pr_url": "https://github.com/apache/lucene-solr/pull/1297", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNDU1Ng==", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385434556", "bodyText": "Do we need any synchronization, since this will now be running on a different thread?", "author": "tflobbe", "createdAt": "2020-02-27T23:44:23Z", "path": "solr/core/src/java/org/apache/solr/cloud/ZkController.java", "diffHunk": "@@ -1684,58 +1685,37 @@ private void doGetShardIdAndNodeNameProcess(CoreDescriptor cd) {\n   }\n \n   private void waitForCoreNodeName(CoreDescriptor descriptor) {\n-    int retryCount = 320;\n-    log.debug(\"look for our core node name\");\n-    while (retryCount-- > 0) {\n-      final DocCollection docCollection = zkStateReader.getClusterState()\n-          .getCollectionOrNull(descriptor.getCloudDescriptor().getCollectionName());\n-      if (docCollection != null && docCollection.getSlicesMap() != null) {\n-        final Map<String, Slice> slicesMap = docCollection.getSlicesMap();\n-        for (Slice slice : slicesMap.values()) {\n-          for (Replica replica : slice.getReplicas()) {\n-            // TODO: for really large clusters, we could 'index' on this\n-\n-            String nodeName = replica.getStr(ZkStateReader.NODE_NAME_PROP);\n-            String core = replica.getStr(ZkStateReader.CORE_NAME_PROP);\n-\n-            String msgNodeName = getNodeName();\n-            String msgCore = descriptor.getName();\n-\n-            if (msgNodeName.equals(nodeName) && core.equals(msgCore)) {\n-              descriptor.getCloudDescriptor()\n-                  .setCoreNodeName(replica.getName());\n-              getCoreContainer().getCoresLocator().persist(getCoreContainer(), descriptor);\n-              return;\n-            }\n-          }\n+    log.debug(\"waitForCoreNodeName >>> look for our core node name\");\n+    try {\n+      zkStateReader.waitForState(descriptor.getCollectionName(), 320, TimeUnit.SECONDS, c -> {\n+        String name = ClusterStateMutator.getAssignedCoreNodeName(c, getNodeName(), descriptor.getName());\n+        if (name == null) {\n+          return false;\n         }\n-      }\n-      try {\n-        Thread.sleep(1000);\n-      } catch (InterruptedException e) {\n-        Thread.currentThread().interrupt();\n-      }\n+        descriptor.getCloudDescriptor().setCoreNodeName(name);", "originalCommit": "531383c9010a739093639a0c825dc59d618517b7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTc2ODk2OA==", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385768968", "bodyText": "This thread will block on the wait call, so I don't think we're introducing any new races. It was always possible that two threads could be trying to access the CoreDescriptor, I think.", "author": "madrob", "createdAt": "2020-02-28T15:45:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNDU1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTgyMjQ4Mw==", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385822483", "bodyText": "OK. I was thinking more on data visibility than race conditions", "author": "tflobbe", "createdAt": "2020-02-28T17:20:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNDU1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjcyNjcwOQ==", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r386726709", "bodyText": "Dug into this deeper, and I believe that the latch in waitForState will guarantee data visibility.", "author": "madrob", "createdAt": "2020-03-03T00:08:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNDU1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNDY1MA==", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385434650", "bodyText": "We still want to reset interruption, right?\nAlso, am I reading right? before we'd just continue running normally even after a \"timeout\", while now we throw an exception. Sounds like a good change, but that's intended, right?", "author": "tflobbe", "createdAt": "2020-02-27T23:44:42Z", "path": "solr/core/src/java/org/apache/solr/cloud/ZkController.java", "diffHunk": "@@ -1684,58 +1685,37 @@ private void doGetShardIdAndNodeNameProcess(CoreDescriptor cd) {\n   }\n \n   private void waitForCoreNodeName(CoreDescriptor descriptor) {\n-    int retryCount = 320;\n-    log.debug(\"look for our core node name\");\n-    while (retryCount-- > 0) {\n-      final DocCollection docCollection = zkStateReader.getClusterState()\n-          .getCollectionOrNull(descriptor.getCloudDescriptor().getCollectionName());\n-      if (docCollection != null && docCollection.getSlicesMap() != null) {\n-        final Map<String, Slice> slicesMap = docCollection.getSlicesMap();\n-        for (Slice slice : slicesMap.values()) {\n-          for (Replica replica : slice.getReplicas()) {\n-            // TODO: for really large clusters, we could 'index' on this\n-\n-            String nodeName = replica.getStr(ZkStateReader.NODE_NAME_PROP);\n-            String core = replica.getStr(ZkStateReader.CORE_NAME_PROP);\n-\n-            String msgNodeName = getNodeName();\n-            String msgCore = descriptor.getName();\n-\n-            if (msgNodeName.equals(nodeName) && core.equals(msgCore)) {\n-              descriptor.getCloudDescriptor()\n-                  .setCoreNodeName(replica.getName());\n-              getCoreContainer().getCoresLocator().persist(getCoreContainer(), descriptor);\n-              return;\n-            }\n-          }\n+    log.debug(\"waitForCoreNodeName >>> look for our core node name\");\n+    try {\n+      zkStateReader.waitForState(descriptor.getCollectionName(), 320, TimeUnit.SECONDS, c -> {\n+        String name = ClusterStateMutator.getAssignedCoreNodeName(c, getNodeName(), descriptor.getName());\n+        if (name == null) {\n+          return false;\n         }\n-      }\n-      try {\n-        Thread.sleep(1000);\n-      } catch (InterruptedException e) {\n-        Thread.currentThread().interrupt();\n-      }\n+        descriptor.getCloudDescriptor().setCoreNodeName(name);\n+        return true;\n+      });\n+    } catch (TimeoutException | InterruptedException e) {\n+      throw new SolrException(ErrorCode.SERVER_ERROR, \"Timeout waiting for collection state\", e);", "originalCommit": "531383c9010a739093639a0c825dc59d618517b7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTc3NjI1Mg==", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385776252", "bodyText": "A couple of logic changes here, yes. 1) Before we would continue to retry on interrupt, i.e. the interruption would only count against the current attempt not the whole method. That's probably wrong. 2) We wouldn't fail if we don't see the result state. Also probably wrong, and I suspect that we would end up failing later when this was missing?", "author": "madrob", "createdAt": "2020-02-28T15:57:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNDY1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTgyMzc3Mw==", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385823773", "bodyText": "+1 for the changes. Just wanted to make sure they were intentional. Regarding the InterruptedException handling, This LGTM, but if you want to keep the multicatch you could use SolrZkClient.checkInterrupted(Throwable e)", "author": "tflobbe", "createdAt": "2020-02-28T17:23:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNDY1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjcyNjc1OA==", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r386726758", "bodyText": "Will do!", "author": "madrob", "createdAt": "2020-03-03T00:08:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNDY1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNTk0NA==", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385435944", "bodyText": "Do we need any synchronization?", "author": "tflobbe", "createdAt": "2020-02-27T23:48:42Z", "path": "solr/core/src/java/org/apache/solr/cloud/ZkController.java", "diffHunk": "@@ -1684,58 +1685,37 @@ private void doGetShardIdAndNodeNameProcess(CoreDescriptor cd) {\n   }\n \n   private void waitForCoreNodeName(CoreDescriptor descriptor) {\n-    int retryCount = 320;\n-    log.debug(\"look for our core node name\");\n-    while (retryCount-- > 0) {\n-      final DocCollection docCollection = zkStateReader.getClusterState()\n-          .getCollectionOrNull(descriptor.getCloudDescriptor().getCollectionName());\n-      if (docCollection != null && docCollection.getSlicesMap() != null) {\n-        final Map<String, Slice> slicesMap = docCollection.getSlicesMap();\n-        for (Slice slice : slicesMap.values()) {\n-          for (Replica replica : slice.getReplicas()) {\n-            // TODO: for really large clusters, we could 'index' on this\n-\n-            String nodeName = replica.getStr(ZkStateReader.NODE_NAME_PROP);\n-            String core = replica.getStr(ZkStateReader.CORE_NAME_PROP);\n-\n-            String msgNodeName = getNodeName();\n-            String msgCore = descriptor.getName();\n-\n-            if (msgNodeName.equals(nodeName) && core.equals(msgCore)) {\n-              descriptor.getCloudDescriptor()\n-                  .setCoreNodeName(replica.getName());\n-              getCoreContainer().getCoresLocator().persist(getCoreContainer(), descriptor);\n-              return;\n-            }\n-          }\n+    log.debug(\"waitForCoreNodeName >>> look for our core node name\");\n+    try {\n+      zkStateReader.waitForState(descriptor.getCollectionName(), 320, TimeUnit.SECONDS, c -> {\n+        String name = ClusterStateMutator.getAssignedCoreNodeName(c, getNodeName(), descriptor.getName());\n+        if (name == null) {\n+          return false;\n         }\n-      }\n-      try {\n-        Thread.sleep(1000);\n-      } catch (InterruptedException e) {\n-        Thread.currentThread().interrupt();\n-      }\n+        descriptor.getCloudDescriptor().setCoreNodeName(name);\n+        return true;\n+      });\n+    } catch (TimeoutException | InterruptedException e) {\n+      throw new SolrException(ErrorCode.SERVER_ERROR, \"Timeout waiting for collection state\", e);\n     }\n+    getCoreContainer().getCoresLocator().persist(getCoreContainer(), descriptor);\n   }\n \n-  private void waitForShardId(CoreDescriptor cd) {\n+  private void waitForShardId(final CoreDescriptor cd) {\n     log.debug(\"waiting to find shard id in clusterstate for \" + cd.getName());\n-    int retryCount = 320;\n-    while (retryCount-- > 0) {\n-      final String shardId = zkStateReader.getClusterState().getShardId(cd.getCollectionName(), getNodeName(), cd.getName());\n-      if (shardId != null) {\n-        cd.getCloudDescriptor().setShardId(shardId);\n-        return;\n-      }\n-      try {\n-        Thread.sleep(1000);\n-      } catch (InterruptedException e) {\n-        Thread.currentThread().interrupt();\n-      }\n+    try {\n+      zkStateReader.waitForState(cd.getCollectionName(), 320, TimeUnit.SECONDS, c -> {\n+        if (c == null) return false;\n+        final String shardId = c.getShardId(getNodeName(), cd.getName());\n+        if (shardId != null) {\n+          cd.getCloudDescriptor().setShardId(shardId);", "originalCommit": "531383c9010a739093639a0c825dc59d618517b7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNjA5NA==", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385436094", "bodyText": "Same as before, we should probably re set the interruption. Also, did you intentionally not wrap the exception?", "author": "tflobbe", "createdAt": "2020-02-27T23:49:12Z", "path": "solr/core/src/java/org/apache/solr/cloud/ZkController.java", "diffHunk": "@@ -1684,58 +1685,37 @@ private void doGetShardIdAndNodeNameProcess(CoreDescriptor cd) {\n   }\n \n   private void waitForCoreNodeName(CoreDescriptor descriptor) {\n-    int retryCount = 320;\n-    log.debug(\"look for our core node name\");\n-    while (retryCount-- > 0) {\n-      final DocCollection docCollection = zkStateReader.getClusterState()\n-          .getCollectionOrNull(descriptor.getCloudDescriptor().getCollectionName());\n-      if (docCollection != null && docCollection.getSlicesMap() != null) {\n-        final Map<String, Slice> slicesMap = docCollection.getSlicesMap();\n-        for (Slice slice : slicesMap.values()) {\n-          for (Replica replica : slice.getReplicas()) {\n-            // TODO: for really large clusters, we could 'index' on this\n-\n-            String nodeName = replica.getStr(ZkStateReader.NODE_NAME_PROP);\n-            String core = replica.getStr(ZkStateReader.CORE_NAME_PROP);\n-\n-            String msgNodeName = getNodeName();\n-            String msgCore = descriptor.getName();\n-\n-            if (msgNodeName.equals(nodeName) && core.equals(msgCore)) {\n-              descriptor.getCloudDescriptor()\n-                  .setCoreNodeName(replica.getName());\n-              getCoreContainer().getCoresLocator().persist(getCoreContainer(), descriptor);\n-              return;\n-            }\n-          }\n+    log.debug(\"waitForCoreNodeName >>> look for our core node name\");\n+    try {\n+      zkStateReader.waitForState(descriptor.getCollectionName(), 320, TimeUnit.SECONDS, c -> {\n+        String name = ClusterStateMutator.getAssignedCoreNodeName(c, getNodeName(), descriptor.getName());\n+        if (name == null) {\n+          return false;\n         }\n-      }\n-      try {\n-        Thread.sleep(1000);\n-      } catch (InterruptedException e) {\n-        Thread.currentThread().interrupt();\n-      }\n+        descriptor.getCloudDescriptor().setCoreNodeName(name);\n+        return true;\n+      });\n+    } catch (TimeoutException | InterruptedException e) {\n+      throw new SolrException(ErrorCode.SERVER_ERROR, \"Timeout waiting for collection state\", e);\n     }\n+    getCoreContainer().getCoresLocator().persist(getCoreContainer(), descriptor);\n   }\n \n-  private void waitForShardId(CoreDescriptor cd) {\n+  private void waitForShardId(final CoreDescriptor cd) {\n     log.debug(\"waiting to find shard id in clusterstate for \" + cd.getName());\n-    int retryCount = 320;\n-    while (retryCount-- > 0) {\n-      final String shardId = zkStateReader.getClusterState().getShardId(cd.getCollectionName(), getNodeName(), cd.getName());\n-      if (shardId != null) {\n-        cd.getCloudDescriptor().setShardId(shardId);\n-        return;\n-      }\n-      try {\n-        Thread.sleep(1000);\n-      } catch (InterruptedException e) {\n-        Thread.currentThread().interrupt();\n-      }\n+    try {\n+      zkStateReader.waitForState(cd.getCollectionName(), 320, TimeUnit.SECONDS, c -> {\n+        if (c == null) return false;\n+        final String shardId = c.getShardId(getNodeName(), cd.getName());\n+        if (shardId != null) {\n+          cd.getCloudDescriptor().setShardId(shardId);\n+          return true;\n+        }\n+        return false;\n+      });\n+    } catch (TimeoutException | InterruptedException e) {\n+      throw new SolrException(ErrorCode.SERVER_ERROR, \"Could not get shard id for core: \" + cd.getName());", "originalCommit": "531383c9010a739093639a0c825dc59d618517b7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNjU1OA==", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385436558", "bodyText": "same comment/questions as with the other methods", "author": "tflobbe", "createdAt": "2020-02-27T23:50:49Z", "path": "solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerCollectionMessageHandler.java", "diffHunk": "@@ -471,29 +471,21 @@ void checkResults(String label, NamedList<Object> results, boolean failureIsFata\n   private void migrateStateFormat(ClusterState state, ZkNodeProps message, NamedList results) throws Exception {\n     final String collectionName = message.getStr(COLLECTION_PROP);\n \n-    boolean firstLoop = true;\n-    // wait for a while until the state format changes\n-    TimeOut timeout = new TimeOut(30, TimeUnit.SECONDS, timeSource);\n-    while (! timeout.hasTimedOut()) {\n-      DocCollection collection = zkStateReader.getClusterState().getCollection(collectionName);\n-      if (collection == null) {\n-        throw new SolrException(ErrorCode.BAD_REQUEST, \"Collection: \" + collectionName + \" not found\");\n-      }\n-      if (collection.getStateFormat() == 2) {\n-        // Done.\n-        results.add(\"success\", new SimpleOrderedMap<>());\n-        return;\n-      }\n+    ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, MIGRATESTATEFORMAT.toLower(), COLLECTION_PROP, collectionName);\n+    overseer.offerStateUpdate(Utils.toJSON(m));\n \n-      if (firstLoop) {\n-        // Actually queue the migration command.\n-        firstLoop = false;\n-        ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, MIGRATESTATEFORMAT.toLower(), COLLECTION_PROP, collectionName);\n-        overseer.offerStateUpdate(Utils.toJSON(m));\n-      }\n-      timeout.sleep(100);\n+    try {\n+      zkStateReader.waitForState(collectionName, 30, TimeUnit.SECONDS, c -> {", "originalCommit": "531383c9010a739093639a0c825dc59d618517b7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNjg1Ng==", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385436856", "bodyText": "Thread.currentThread().interrupt()?", "author": "tflobbe", "createdAt": "2020-02-27T23:51:39Z", "path": "solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerCollectionMessageHandler.java", "diffHunk": "@@ -526,57 +518,31 @@ static UpdateResponse softCommit(String url) throws SolrServerException, IOExcep\n   }\n \n   String waitForCoreNodeName(String collectionName, String msgNodeName, String msgCore) {\n-    int retryCount = 320;\n-    while (retryCount-- > 0) {\n-      final DocCollection docCollection = zkStateReader.getClusterState().getCollectionOrNull(collectionName);\n-      if (docCollection != null && docCollection.getSlicesMap() != null) {\n-        Map<String,Slice> slicesMap = docCollection.getSlicesMap();\n-        for (Slice slice : slicesMap.values()) {\n-          for (Replica replica : slice.getReplicas()) {\n-            // TODO: for really large clusters, we could 'index' on this\n-\n-            String nodeName = replica.getStr(ZkStateReader.NODE_NAME_PROP);\n-            String core = replica.getStr(ZkStateReader.CORE_NAME_PROP);\n-\n-            if (nodeName.equals(msgNodeName) && core.equals(msgCore)) {\n-              return replica.getName();\n-            }\n-          }\n+    AtomicReference<String> coreNodeName = new AtomicReference<>();\n+    try {\n+      zkStateReader.waitForState(collectionName, 320, TimeUnit.SECONDS, c -> {\n+        String name = ClusterStateMutator.getAssignedCoreNodeName(c, msgNodeName, msgCore);\n+        if (name == null) {\n+          return false;\n         }\n-      }\n-      try {\n-        Thread.sleep(1000);\n-      } catch (InterruptedException e) {\n-        Thread.currentThread().interrupt();\n-      }\n+        coreNodeName.set(name);\n+        return true;\n+      });\n+    } catch (TimeoutException | InterruptedException e) {\n+      throw new SolrException(ErrorCode.SERVER_ERROR, \"Timeout waiting for collection state\", e);", "originalCommit": "531383c9010a739093639a0c825dc59d618517b7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzNzE5NA==", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385437194", "bodyText": "interruption?", "author": "tflobbe", "createdAt": "2020-02-27T23:52:42Z", "path": "solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerCollectionMessageHandler.java", "diffHunk": "@@ -526,57 +518,31 @@ static UpdateResponse softCommit(String url) throws SolrServerException, IOExcep\n   }\n \n   String waitForCoreNodeName(String collectionName, String msgNodeName, String msgCore) {\n-    int retryCount = 320;\n-    while (retryCount-- > 0) {\n-      final DocCollection docCollection = zkStateReader.getClusterState().getCollectionOrNull(collectionName);\n-      if (docCollection != null && docCollection.getSlicesMap() != null) {\n-        Map<String,Slice> slicesMap = docCollection.getSlicesMap();\n-        for (Slice slice : slicesMap.values()) {\n-          for (Replica replica : slice.getReplicas()) {\n-            // TODO: for really large clusters, we could 'index' on this\n-\n-            String nodeName = replica.getStr(ZkStateReader.NODE_NAME_PROP);\n-            String core = replica.getStr(ZkStateReader.CORE_NAME_PROP);\n-\n-            if (nodeName.equals(msgNodeName) && core.equals(msgCore)) {\n-              return replica.getName();\n-            }\n-          }\n+    AtomicReference<String> coreNodeName = new AtomicReference<>();\n+    try {\n+      zkStateReader.waitForState(collectionName, 320, TimeUnit.SECONDS, c -> {\n+        String name = ClusterStateMutator.getAssignedCoreNodeName(c, msgNodeName, msgCore);\n+        if (name == null) {\n+          return false;\n         }\n-      }\n-      try {\n-        Thread.sleep(1000);\n-      } catch (InterruptedException e) {\n-        Thread.currentThread().interrupt();\n-      }\n+        coreNodeName.set(name);\n+        return true;\n+      });\n+    } catch (TimeoutException | InterruptedException e) {\n+      throw new SolrException(ErrorCode.SERVER_ERROR, \"Timeout waiting for collection state\", e);\n     }\n-    throw new SolrException(ErrorCode.SERVER_ERROR, \"Could not find coreNodeName\");\n+    return coreNodeName.get();\n   }\n \n-  ClusterState waitForNewShard(String collectionName, String sliceName) throws KeeperException, InterruptedException {\n+  ClusterState waitForNewShard(String collectionName, String sliceName) {\n     log.debug(\"Waiting for slice {} of collection {} to be available\", sliceName, collectionName);\n-    RTimer timer = new RTimer();\n-    int retryCount = 320;\n-    while (retryCount-- > 0) {\n-      ClusterState clusterState = zkStateReader.getClusterState();\n-      DocCollection collection = clusterState.getCollection(collectionName);\n-\n-      if (collection == null) {\n-        throw new SolrException(ErrorCode.SERVER_ERROR,\n-            \"Unable to find collection: \" + collectionName + \" in clusterstate\");\n-      }\n-      Slice slice = collection.getSlice(sliceName);\n-      if (slice != null) {\n-        log.debug(\"Waited for {}ms for slice {} of collection {} to be available\",\n-            timer.getTime(), sliceName, collectionName);\n-        return clusterState;\n-      }\n-      Thread.sleep(1000);\n+    try {\n+      zkStateReader.waitForState(collectionName, 320, TimeUnit.SECONDS, c -> c != null && c.getSlice(sliceName) != null);\n+    } catch (TimeoutException | InterruptedException e) {\n+      throw new SolrException(ErrorCode.SERVER_ERROR, \"Timeout waiting for new slice\", e);", "originalCommit": "531383c9010a739093639a0c825dc59d618517b7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzODIxMA==", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385438210", "bodyText": "Isn't that the case with most of this methods? While the predicate is being executed for example, there is no watch in ZooKeeper AFAICT, unless we go back and write in ZooKeeper and use the version.", "author": "tflobbe", "createdAt": "2020-02-27T23:55:58Z", "path": "solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerCollectionMessageHandler.java", "diffHunk": "@@ -526,57 +518,31 @@ static UpdateResponse softCommit(String url) throws SolrServerException, IOExcep\n   }\n \n   String waitForCoreNodeName(String collectionName, String msgNodeName, String msgCore) {\n-    int retryCount = 320;\n-    while (retryCount-- > 0) {\n-      final DocCollection docCollection = zkStateReader.getClusterState().getCollectionOrNull(collectionName);\n-      if (docCollection != null && docCollection.getSlicesMap() != null) {\n-        Map<String,Slice> slicesMap = docCollection.getSlicesMap();\n-        for (Slice slice : slicesMap.values()) {\n-          for (Replica replica : slice.getReplicas()) {\n-            // TODO: for really large clusters, we could 'index' on this\n-\n-            String nodeName = replica.getStr(ZkStateReader.NODE_NAME_PROP);\n-            String core = replica.getStr(ZkStateReader.CORE_NAME_PROP);\n-\n-            if (nodeName.equals(msgNodeName) && core.equals(msgCore)) {\n-              return replica.getName();\n-            }\n-          }\n+    AtomicReference<String> coreNodeName = new AtomicReference<>();\n+    try {\n+      zkStateReader.waitForState(collectionName, 320, TimeUnit.SECONDS, c -> {\n+        String name = ClusterStateMutator.getAssignedCoreNodeName(c, msgNodeName, msgCore);\n+        if (name == null) {\n+          return false;\n         }\n-      }\n-      try {\n-        Thread.sleep(1000);\n-      } catch (InterruptedException e) {\n-        Thread.currentThread().interrupt();\n-      }\n+        coreNodeName.set(name);\n+        return true;\n+      });\n+    } catch (TimeoutException | InterruptedException e) {\n+      throw new SolrException(ErrorCode.SERVER_ERROR, \"Timeout waiting for collection state\", e);\n     }\n-    throw new SolrException(ErrorCode.SERVER_ERROR, \"Could not find coreNodeName\");\n+    return coreNodeName.get();\n   }\n \n-  ClusterState waitForNewShard(String collectionName, String sliceName) throws KeeperException, InterruptedException {\n+  ClusterState waitForNewShard(String collectionName, String sliceName) {\n     log.debug(\"Waiting for slice {} of collection {} to be available\", sliceName, collectionName);\n-    RTimer timer = new RTimer();\n-    int retryCount = 320;\n-    while (retryCount-- > 0) {\n-      ClusterState clusterState = zkStateReader.getClusterState();\n-      DocCollection collection = clusterState.getCollection(collectionName);\n-\n-      if (collection == null) {\n-        throw new SolrException(ErrorCode.SERVER_ERROR,\n-            \"Unable to find collection: \" + collectionName + \" in clusterstate\");\n-      }\n-      Slice slice = collection.getSlice(sliceName);\n-      if (slice != null) {\n-        log.debug(\"Waited for {}ms for slice {} of collection {} to be available\",\n-            timer.getTime(), sliceName, collectionName);\n-        return clusterState;\n-      }\n-      Thread.sleep(1000);\n+    try {\n+      zkStateReader.waitForState(collectionName, 320, TimeUnit.SECONDS, c -> c != null && c.getSlice(sliceName) != null);\n+    } catch (TimeoutException | InterruptedException e) {\n+      throw new SolrException(ErrorCode.SERVER_ERROR, \"Timeout waiting for new slice\", e);\n     }\n-    throw new SolrException(ErrorCode.SERVER_ERROR,\n-        \"Could not find new slice \" + sliceName + \" in collection \" + collectionName\n-            + \" even after waiting for \" + timer.getTime() + \"ms\"\n-    );\n+    // nocommit is there a race condition here since we're not returning the same clusterstate we inspected?", "originalCommit": "531383c9010a739093639a0c825dc59d618517b7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzODcxMw==", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385438713", "bodyText": "Can this happen?", "author": "tflobbe", "createdAt": "2020-02-27T23:57:34Z", "path": "solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerCollectionMessageHandler.java", "diffHunk": "@@ -630,34 +596,32 @@ private void modifyCollection(ClusterState clusterState, ZkNodeProps message, Na\n \n     overseer.offerStateUpdate(Utils.toJSON(message));\n \n-    TimeOut timeout = new TimeOut(30, TimeUnit.SECONDS, timeSource);\n-    boolean areChangesVisible = true;\n-    while (!timeout.hasTimedOut()) {\n-      DocCollection collection = cloudManager.getClusterStateProvider().getClusterState().getCollection(collectionName);\n-      areChangesVisible = true;\n-      for (Map.Entry<String,Object> updateEntry : message.getProperties().entrySet()) {\n-        String updateKey = updateEntry.getKey();\n-\n-        if (!updateKey.equals(ZkStateReader.COLLECTION_PROP)\n-            && !updateKey.equals(Overseer.QUEUE_OPERATION)\n-            && updateEntry.getValue() != null // handled below in a separate conditional\n-            && !updateEntry.getValue().equals(collection.get(updateKey))) {\n-          areChangesVisible = false;\n-          break;\n+    try {\n+      zkStateReader.waitForState(collectionName, 30, TimeUnit.SECONDS, c -> {\n+        if (c == null) {", "originalCommit": "531383c9010a739093639a0c825dc59d618517b7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTc3OTM3MA==", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385779370", "bodyText": "It could happen if there is a concurrent delete", "author": "madrob", "createdAt": "2020-02-28T16:02:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzODcxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzOTA4NQ==", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385439085", "bodyText": "Can we use parametrized logging here?", "author": "tflobbe", "createdAt": "2020-02-27T23:58:43Z", "path": "solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerCollectionMessageHandler.java", "diffHunk": "@@ -630,34 +596,32 @@ private void modifyCollection(ClusterState clusterState, ZkNodeProps message, Na\n \n     overseer.offerStateUpdate(Utils.toJSON(message));\n \n-    TimeOut timeout = new TimeOut(30, TimeUnit.SECONDS, timeSource);\n-    boolean areChangesVisible = true;\n-    while (!timeout.hasTimedOut()) {\n-      DocCollection collection = cloudManager.getClusterStateProvider().getClusterState().getCollection(collectionName);\n-      areChangesVisible = true;\n-      for (Map.Entry<String,Object> updateEntry : message.getProperties().entrySet()) {\n-        String updateKey = updateEntry.getKey();\n-\n-        if (!updateKey.equals(ZkStateReader.COLLECTION_PROP)\n-            && !updateKey.equals(Overseer.QUEUE_OPERATION)\n-            && updateEntry.getValue() != null // handled below in a separate conditional\n-            && !updateEntry.getValue().equals(collection.get(updateKey))) {\n-          areChangesVisible = false;\n-          break;\n+    try {\n+      zkStateReader.waitForState(collectionName, 30, TimeUnit.SECONDS, c -> {\n+        if (c == null) {\n+          return false;\n         }\n+        for (Map.Entry<String,Object> updateEntry : message.getProperties().entrySet()) {\n+          String updateKey = updateEntry.getKey();\n+\n+          if (!updateKey.equals(ZkStateReader.COLLECTION_PROP)\n+                  && !updateKey.equals(Overseer.QUEUE_OPERATION)\n+                  && updateEntry.getValue() != null // handled below in a separate conditional\n+                  && !updateEntry.getValue().equals(c.get(updateKey))) {\n+            return false;\n+          }\n \n-        if (updateEntry.getValue() == null && collection.containsKey(updateKey)) {\n-          areChangesVisible = false;\n-          break;\n+          if (updateEntry.getValue() == null && c.containsKey(updateKey)) {\n+            return false;\n+          }\n         }\n-      }\n-      if (areChangesVisible) break;\n-      timeout.sleep(100);\n+        return true;\n+      });\n+    } catch (TimeoutException | InterruptedException e) {\n+      log.debug(\"modifyCollection(ClusterState=\" + clusterState + \", ZkNodeProps=\" + message + \", NamedList=\" + results + \")\", e);", "originalCommit": "531383c9010a739093639a0c825dc59d618517b7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQzOTMzNg==", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385439336", "bodyText": "reset interruption", "author": "tflobbe", "createdAt": "2020-02-27T23:59:32Z", "path": "solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerCollectionMessageHandler.java", "diffHunk": "@@ -630,34 +596,32 @@ private void modifyCollection(ClusterState clusterState, ZkNodeProps message, Na\n \n     overseer.offerStateUpdate(Utils.toJSON(message));\n \n-    TimeOut timeout = new TimeOut(30, TimeUnit.SECONDS, timeSource);\n-    boolean areChangesVisible = true;\n-    while (!timeout.hasTimedOut()) {\n-      DocCollection collection = cloudManager.getClusterStateProvider().getClusterState().getCollection(collectionName);\n-      areChangesVisible = true;\n-      for (Map.Entry<String,Object> updateEntry : message.getProperties().entrySet()) {\n-        String updateKey = updateEntry.getKey();\n-\n-        if (!updateKey.equals(ZkStateReader.COLLECTION_PROP)\n-            && !updateKey.equals(Overseer.QUEUE_OPERATION)\n-            && updateEntry.getValue() != null // handled below in a separate conditional\n-            && !updateEntry.getValue().equals(collection.get(updateKey))) {\n-          areChangesVisible = false;\n-          break;\n+    try {\n+      zkStateReader.waitForState(collectionName, 30, TimeUnit.SECONDS, c -> {\n+        if (c == null) {\n+          return false;\n         }\n+        for (Map.Entry<String,Object> updateEntry : message.getProperties().entrySet()) {\n+          String updateKey = updateEntry.getKey();\n+\n+          if (!updateKey.equals(ZkStateReader.COLLECTION_PROP)\n+                  && !updateKey.equals(Overseer.QUEUE_OPERATION)\n+                  && updateEntry.getValue() != null // handled below in a separate conditional\n+                  && !updateEntry.getValue().equals(c.get(updateKey))) {\n+            return false;\n+          }\n \n-        if (updateEntry.getValue() == null && collection.containsKey(updateKey)) {\n-          areChangesVisible = false;\n-          break;\n+          if (updateEntry.getValue() == null && c.containsKey(updateKey)) {\n+            return false;\n+          }\n         }\n-      }\n-      if (areChangesVisible) break;\n-      timeout.sleep(100);\n+        return true;\n+      });\n+    } catch (TimeoutException | InterruptedException e) {\n+      log.debug(\"modifyCollection(ClusterState=\" + clusterState + \", ZkNodeProps=\" + message + \", NamedList=\" + results + \")\", e);", "originalCommit": "531383c9010a739093639a0c825dc59d618517b7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ0MDY3OA==", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385440678", "bodyText": "Can't we iterate shards/replicas, and for each one check if coreNames .contains(replica.getStr(ZkStateReader.CORE_NAME_PROP)). Maybe make a set with all the elements in coreNames and remove them as you find them, and break if empty? I guess it depends on how big coreNames will be", "author": "tflobbe", "createdAt": "2020-02-28T00:04:10Z", "path": "solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerCollectionMessageHandler.java", "diffHunk": "@@ -672,35 +636,35 @@ void cleanupCollection(String collectionName, NamedList results) throws Exceptio\n     commandMap.get(DELETE).call(zkStateReader.getClusterState(), new ZkNodeProps(props), results);\n   }\n \n-  Map<String, Replica> waitToSeeReplicasInState(String collectionName, Collection<String> coreNames) throws InterruptedException {\n-    assert coreNames.size() > 0;\n-    Map<String, Replica> result = new HashMap<>();\n-    TimeOut timeout = new TimeOut(Integer.getInteger(\"solr.waitToSeeReplicasInStateTimeoutSeconds\", 120), TimeUnit.SECONDS, timeSource); // could be a big cluster\n-    while (true) {\n-      DocCollection coll = zkStateReader.getClusterState().getCollection(collectionName);\n-      for (String coreName : coreNames) {\n-        if (result.containsKey(coreName)) continue;\n-        for (Slice slice : coll.getSlices()) {\n-          for (Replica replica : slice.getReplicas()) {\n-            if (coreName.equals(replica.getStr(ZkStateReader.CORE_NAME_PROP))) {\n-              result.put(coreName, replica);\n-              break;\n+  Map<String, Replica> waitToSeeReplicasInState(String collectionName, Collection<String> coreNames) {\n+    final Map<String, Replica> result = new HashMap<>();\n+    int timeout = Integer.getInteger(\"solr.waitToSeeReplicasInStateTimeoutSeconds\", 120); // could be a big cluster\n+    try {\n+      zkStateReader.waitForState(collectionName, timeout, TimeUnit.SECONDS, c -> {\n+        // todo this is ugly, but I'm not sure there is a better way to fix it?", "originalCommit": "531383c9010a739093639a0c825dc59d618517b7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTc3OTczNQ==", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r385779735", "bodyText": "I think I found something that makes sense.", "author": "madrob", "createdAt": "2020-02-28T16:03:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ0MDY3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMjY5Mw==", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r389132693", "bodyText": "If this change is being made, should the number of retries be configurable?  This hardcoded value seems to be used a lot in the code.", "author": "beettlle", "createdAt": "2020-03-06T20:39:56Z", "path": "solr/core/src/java/org/apache/solr/cloud/ZkController.java", "diffHunk": "@@ -1684,58 +1685,39 @@ private void doGetShardIdAndNodeNameProcess(CoreDescriptor cd) {\n   }\n \n   private void waitForCoreNodeName(CoreDescriptor descriptor) {\n-    int retryCount = 320;\n-    log.debug(\"look for our core node name\");\n-    while (retryCount-- > 0) {\n-      final DocCollection docCollection = zkStateReader.getClusterState()\n-          .getCollectionOrNull(descriptor.getCloudDescriptor().getCollectionName());\n-      if (docCollection != null && docCollection.getSlicesMap() != null) {\n-        final Map<String, Slice> slicesMap = docCollection.getSlicesMap();\n-        for (Slice slice : slicesMap.values()) {\n-          for (Replica replica : slice.getReplicas()) {\n-            // TODO: for really large clusters, we could 'index' on this\n-\n-            String nodeName = replica.getStr(ZkStateReader.NODE_NAME_PROP);\n-            String core = replica.getStr(ZkStateReader.CORE_NAME_PROP);\n-\n-            String msgNodeName = getNodeName();\n-            String msgCore = descriptor.getName();\n-\n-            if (msgNodeName.equals(nodeName) && core.equals(msgCore)) {\n-              descriptor.getCloudDescriptor()\n-                  .setCoreNodeName(replica.getName());\n-              getCoreContainer().getCoresLocator().persist(getCoreContainer(), descriptor);\n-              return;\n-            }\n-          }\n+    log.debug(\"waitForCoreNodeName >>> look for our core node name\");\n+    try {\n+      zkStateReader.waitForState(descriptor.getCollectionName(), 320, TimeUnit.SECONDS, c -> {", "originalCommit": "5efd9737f4943aacd0524fbf590951672ce0657f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE5MzczOA==", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r389193738", "bodyText": "In general, I think it's good to have knobs, but there's definitely the possibility of having too many things available to configure and overwhelming operators. Can you describe what conditions would lead to wanting to tweak this?", "author": "madrob", "createdAt": "2020-03-06T23:35:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMjY5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTIwODM0OQ==", "url": "https://github.com/apache/lucene-solr/pull/1297#discussion_r389208349", "bodyText": "Agreed bout having too many settings, we're already drowning in them.\nLooking back looks like the number was added as part of SOLR-9140 and there's no comment of where the \"320\" came from.  As well, there's another retry number here of \"30\" but no idea why.  So we already have 2 different numbers of retries.\nIf the numbers come from empirical experiments then I agree with them being constants but because they seem arbitrary seems like good candidates of per-application tuning.", "author": "beettlle", "createdAt": "2020-03-07T00:52:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMjY5Mw=="}], "type": "inlineReview"}, {"oid": "b2a2b16d04e3ead029b4948496c52fa3fc48606a", "url": "https://github.com/apache/lucene-solr/commit/b2a2b16d04e3ead029b4948496c52fa3fc48606a", "message": "Localized some exception logic", "committedDate": "2020-03-11T15:53:58Z", "type": "forcePushed"}, {"oid": "21ada18313c5e40c101a67c27d64fc61ef1bfb3d", "url": "https://github.com/apache/lucene-solr/commit/21ada18313c5e40c101a67c27d64fc61ef1bfb3d", "message": "SOLR-14253 Replace sleep calls with ZK waits\n\nCo-Authored-By: markrmiller <markrmiller@apache.org>", "committedDate": "2021-01-29T21:11:22Z", "type": "commit"}, {"oid": "21ada18313c5e40c101a67c27d64fc61ef1bfb3d", "url": "https://github.com/apache/lucene-solr/commit/21ada18313c5e40c101a67c27d64fc61ef1bfb3d", "message": "SOLR-14253 Replace sleep calls with ZK waits\n\nCo-Authored-By: markrmiller <markrmiller@apache.org>", "committedDate": "2021-01-29T21:11:22Z", "type": "forcePushed"}]}