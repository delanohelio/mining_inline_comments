{"pr_number": 1234, "pr_title": "LUCENE-9211 Add compression for Binary doc value fields", "pr_createdAt": "2020-02-03T17:31:55Z", "pr_url": "https://github.com/apache/lucene-solr/pull/1234", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc0NDg3OQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r374744879", "bodyText": "Extra space character before implements?", "author": "mikemccand", "createdAt": "2020-02-04T15:34:50Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,168 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter  implements Closeable {", "originalCommit": "47db4963385869f0fe84f565d55466637d5c0305", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc0NTgxMQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r374745811", "bodyText": "Add space after if before (?", "author": "mikemccand", "createdAt": "2020-02-04T15:36:19Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,168 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter  implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+      } catch (Throwable exception) {\n+        IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        throw exception;\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void  addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }\n+      docLengths[numDocsInCurrentBlock] = v.length;\n+      block = ArrayUtil.grow(block, uncompressedBlockLength + v.length);\n+      System.arraycopy(v.bytes, v.offset, block, uncompressedBlockLength, v.length);\n+      uncompressedBlockLength += v.length;\n+      numDocsInCurrentBlock++;\n+      if (numDocsInCurrentBlock == Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK) {\n+        flushData();\n+      }      \n     }\n \n-    meta.writeInt(numDocsWithField);\n-    meta.writeInt(minLength);\n-    meta.writeInt(maxLength);\n-    if (maxLength > minLength) {\n-      start = data.getFilePointer();\n-      meta.writeLong(start);\n+    private void flushData() throws IOException {\n+      if(numDocsInCurrentBlock > 0) {", "originalCommit": "47db4963385869f0fe84f565d55466637d5c0305", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc0Njk3OA==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r374746978", "bodyText": "Remove space after ( before blockId?", "author": "mikemccand", "createdAt": "2020-02-04T15:38:02Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -742,6 +755,107 @@ public BytesRef binaryValue() throws IOException {\n         };\n       }\n     }\n+  }  \n+  \n+  // Decompresses blocks of binary values to retrieve content\n+  class BinaryDecoder {\n+    \n+    private final LongValues addresses;\n+    private final IndexInput compressedData;\n+    // Cache of last uncompressed block \n+    private long lastBlockId = -1;\n+    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];\n+    private int uncompressedBlockLength = 0;        \n+    private int numDocsInBlock = 0;\n+    private final byte[] uncompressedBlock;\n+    private BytesRef uncompressedBytesRef;\n+    \n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize) {\n+      super();\n+      this.addresses = addresses;\n+      this.compressedData = compressedData;\n+      // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n+      this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n+      \n+    }\n+\n+    BytesRef decode(int docNumber) throws IOException {\n+      int blockId = docNumber >> Lucene80DocValuesFormat.BINARY_BLOCK_SHIFT; \n+      int docInBlockId = docNumber % Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      assert docInBlockId < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      \n+      \n+      // already read and uncompressed?\n+      if ( blockId != lastBlockId) {", "originalCommit": "47db4963385869f0fe84f565d55466637d5c0305", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc0NzIwMw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r374747203", "bodyText": "s/Bytes ref/BytesRef?", "author": "mikemccand", "createdAt": "2020-02-04T15:38:23Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -742,6 +755,107 @@ public BytesRef binaryValue() throws IOException {\n         };\n       }\n     }\n+  }  \n+  \n+  // Decompresses blocks of binary values to retrieve content\n+  class BinaryDecoder {\n+    \n+    private final LongValues addresses;\n+    private final IndexInput compressedData;\n+    // Cache of last uncompressed block \n+    private long lastBlockId = -1;\n+    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];\n+    private int uncompressedBlockLength = 0;        \n+    private int numDocsInBlock = 0;\n+    private final byte[] uncompressedBlock;\n+    private BytesRef uncompressedBytesRef;\n+    \n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize) {\n+      super();\n+      this.addresses = addresses;\n+      this.compressedData = compressedData;\n+      // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n+      this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n+      \n+    }\n+\n+    BytesRef decode(int docNumber) throws IOException {\n+      int blockId = docNumber >> Lucene80DocValuesFormat.BINARY_BLOCK_SHIFT; \n+      int docInBlockId = docNumber % Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      assert docInBlockId < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      \n+      \n+      // already read and uncompressed?\n+      if ( blockId != lastBlockId) {\n+        lastBlockId = blockId;\n+        long blockStartOffset = addresses.get(blockId);\n+        compressedData.seek(blockStartOffset);\n+        \n+        numDocsInBlock = compressedData.readVInt();\n+        assert numDocsInBlock <= Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+        uncompressedDocEnds = new int[numDocsInBlock];\n+        uncompressedBlockLength = 0;        \n+        for (int i = 0; i < numDocsInBlock; i++) {\n+          uncompressedBlockLength += compressedData.readVInt();\n+          uncompressedDocEnds[i] = uncompressedBlockLength;\n+        }\n+        \n+        if (uncompressedBlockLength == 0) {\n+          uncompressedBytesRef = new BytesRef(BytesRef.EMPTY_BYTES);\n+        } else {\n+          assert uncompressedBlockLength <= uncompressedBlock.length;\n+          LZ4.decompress(compressedData, uncompressedBlockLength, uncompressedBlock, 0);\n+          uncompressedBytesRef = new BytesRef(uncompressedBlock);\n+        }\n+      }\n+      \n+      // Position the Bytes ref to the relevant part of the uncompressed block", "originalCommit": "47db4963385869f0fe84f565d55466637d5c0305", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc0ODUxOQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r374748519", "bodyText": "Remove one of the spaces after void before addDoc?", "author": "mikemccand", "createdAt": "2020-02-04T15:40:31Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,168 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter  implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+      } catch (Throwable exception) {\n+        IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        throw exception;\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void  addDoc(int doc, BytesRef v) throws IOException {", "originalCommit": "47db4963385869f0fe84f565d55466637d5c0305", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc0OTI5MQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r374749291", "bodyText": "Probably we could (later, separate issue) optimize writing these lengths -- often all docs will have the same length?", "author": "mikemccand", "createdAt": "2020-02-04T15:41:46Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,168 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter  implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+      } catch (Throwable exception) {\n+        IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        throw exception;\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void  addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }\n+      docLengths[numDocsInCurrentBlock] = v.length;\n+      block = ArrayUtil.grow(block, uncompressedBlockLength + v.length);\n+      System.arraycopy(v.bytes, v.offset, block, uncompressedBlockLength, v.length);\n+      uncompressedBlockLength += v.length;\n+      numDocsInCurrentBlock++;\n+      if (numDocsInCurrentBlock == Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK) {\n+        flushData();\n+      }      \n     }\n \n-    meta.writeInt(numDocsWithField);\n-    meta.writeInt(minLength);\n-    meta.writeInt(maxLength);\n-    if (maxLength > minLength) {\n-      start = data.getFilePointer();\n-      meta.writeLong(start);\n+    private void flushData() throws IOException {\n+      if(numDocsInCurrentBlock > 0) {\n+        // Write offset to this block to temporary offsets file\n+        totalChunks++;\n+        long thisBlockStartPointer = data.getFilePointer();\n+        data.writeVInt(numDocsInCurrentBlock);\n+        for (int i = 0; i < numDocsInCurrentBlock; i++) {", "originalCommit": "47db4963385869f0fe84f565d55466637d5c0305", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI3MzczNg==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375273736", "bodyText": "+1", "author": "jpountz", "createdAt": "2020-02-05T14:06:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc0OTI5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTk3NDM3MA==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375974370", "bodyText": "Done", "author": "markharwood", "createdAt": "2020-02-06T17:24:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc0OTI5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDc1MDIwNA==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r374750204", "bodyText": "Can you include maxPointer and fp in this exception message?", "author": "mikemccand", "createdAt": "2020-02-04T15:43:10Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,168 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter  implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+      } catch (Throwable exception) {\n+        IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        throw exception;\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void  addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }\n+      docLengths[numDocsInCurrentBlock] = v.length;\n+      block = ArrayUtil.grow(block, uncompressedBlockLength + v.length);\n+      System.arraycopy(v.bytes, v.offset, block, uncompressedBlockLength, v.length);\n+      uncompressedBlockLength += v.length;\n+      numDocsInCurrentBlock++;\n+      if (numDocsInCurrentBlock == Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK) {\n+        flushData();\n+      }      \n     }\n \n-    meta.writeInt(numDocsWithField);\n-    meta.writeInt(minLength);\n-    meta.writeInt(maxLength);\n-    if (maxLength > minLength) {\n-      start = data.getFilePointer();\n-      meta.writeLong(start);\n+    private void flushData() throws IOException {\n+      if(numDocsInCurrentBlock > 0) {\n+        // Write offset to this block to temporary offsets file\n+        totalChunks++;\n+        long thisBlockStartPointer = data.getFilePointer();\n+        data.writeVInt(numDocsInCurrentBlock);\n+        for (int i = 0; i < numDocsInCurrentBlock; i++) {\n+          data.writeVInt(docLengths[i]);\n+        }\n+        maxUncompressedBlockLength = Math.max(maxUncompressedBlockLength, uncompressedBlockLength);\n+        LZ4.compress(block,  0, uncompressedBlockLength, data, ht);\n+        numDocsInCurrentBlock = 0;\n+        uncompressedBlockLength = 0;\n+        maxPointer = data.getFilePointer();\n+        tempBinaryOffsets.writeVLong(maxPointer - thisBlockStartPointer);\n+      }\n+    }\n+    \n+    void writeMetaData() throws IOException {\n+      if (blockAddressesStart < 0 ) {\n+        return;\n+      }\n+      \n+      long startDMW = data.getFilePointer();\n+      meta.writeLong(startDMW);\n+      \n+      meta.writeInt(totalChunks);\n+      meta.writeInt(maxUncompressedBlockLength);\n       meta.writeVInt(DIRECT_MONOTONIC_BLOCK_SHIFT);\n+      \n+    \n+      CodecUtil.writeFooter(tempBinaryOffsets);\n+      IOUtils.close(tempBinaryOffsets);             \n+      //write the compressed block offsets info to the meta file by reading from temp file\n+      try (ChecksumIndexInput filePointersIn = state.directory.openChecksumInput(tempBinaryOffsets.getName(), IOContext.READONCE)) {\n+        CodecUtil.checkHeader(filePointersIn, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT,\n+          Lucene80DocValuesFormat.VERSION_CURRENT);\n+        Throwable priorE = null;\n+        try {\n+          final DirectMonotonicWriter filePointers = DirectMonotonicWriter.getInstance(meta, data, totalChunks, DIRECT_MONOTONIC_BLOCK_SHIFT);\n+          long fp = blockAddressesStart;\n+          for (int i = 0; i < totalChunks; ++i) {\n+            filePointers.add(fp);\n+            fp += filePointersIn.readVLong();\n+          }\n+          if (maxPointer < fp) {\n+            throw new CorruptIndexException(\"File pointers don't add up\", filePointersIn);", "originalCommit": "47db4963385869f0fe84f565d55466637d5c0305", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI1MjEzMw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375252133", "bodyText": "make it final?", "author": "jpountz", "createdAt": "2020-02-05T13:25:54Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -61,11 +66,13 @@\n \n   IndexOutput data, meta;\n   final int maxDoc;\n+  private SegmentWriteState state;", "originalCommit": "1569fe08c8bb219df45927b102c2cbe080a43b31", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI1MjgzNg==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375252836", "bodyText": "we usually don't let spaces between the type of array elements and []\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n          \n          \n            \n                int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];", "author": "jpountz", "createdAt": "2020-02-05T13:27:14Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,168 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; ", "originalCommit": "1569fe08c8bb219df45927b102c2cbe080a43b31", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI1MjkwNw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375252907", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                byte [] block = new byte [1024 * 16];\n          \n          \n            \n                byte[] block = new byte [1024 * 16];", "author": "jpountz", "createdAt": "2020-02-05T13:27:23Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,168 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];", "originalCommit": "1569fe08c8bb219df45927b102c2cbe080a43b31", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI3NDU2Mw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375274563", "bodyText": "we usually do this like that instead, which helps avoid catching Throwable\nboolean success = false;\ntry {\n  // write header\n} finally {\n  if (success == false) {\n    // close\n  }\n}", "author": "jpountz", "createdAt": "2020-02-05T14:07:52Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,168 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+      } catch (Throwable exception) {\n+        IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        throw exception;\n+      }", "originalCommit": "1569fe08c8bb219df45927b102c2cbe080a43b31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTkyMjM3Mw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375922373", "bodyText": "What was the \"+1\" comment for line 407 about?\nI've seen encoding elsewhere that have n+1 offsets to record start of each value and the last offset is effectively the end of the last value. In this scenario I'm writing n value lengths.", "author": "markharwood", "createdAt": "2020-02-06T15:58:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI3NDU2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTkyNzk2Nw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375927967", "bodyText": "It was about optimizing for the case that all values have the same length. In that case we could still one bit of the first length to mean that all values have the same length for instance?", "author": "jpountz", "createdAt": "2020-02-06T16:07:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI3NDU2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI3NTQ5Nw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375275497", "bodyText": "it looks like we could set blockAddressesStart in the constructor instead?", "author": "jpountz", "createdAt": "2020-02-05T14:09:36Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,168 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+      } catch (Throwable exception) {\n+        IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        throw exception;\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }", "originalCommit": "1569fe08c8bb219df45927b102c2cbe080a43b31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTkwMzM0Nw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375903347", "bodyText": "I tried that and it didn't work - something else was writing to data in between constructor and addDoc calls", "author": "markharwood", "createdAt": "2020-02-06T15:30:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI3NTQ5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5NzgwMw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379297803", "bodyText": "Have you found what this something else is?", "author": "jpountz", "createdAt": "2020-02-14T08:16:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI3NTQ5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI3Nzg5OA==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375277898", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    LZ4.compress(block,  0, uncompressedBlockLength, data, ht);\n          \n          \n            \n                    LZ4.compress(block, 0, uncompressedBlockLength, data, ht);", "author": "jpountz", "createdAt": "2020-02-05T14:13:59Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,168 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+      } catch (Throwable exception) {\n+        IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        throw exception;\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }\n+      docLengths[numDocsInCurrentBlock] = v.length;\n+      block = ArrayUtil.grow(block, uncompressedBlockLength + v.length);\n+      System.arraycopy(v.bytes, v.offset, block, uncompressedBlockLength, v.length);\n+      uncompressedBlockLength += v.length;\n+      numDocsInCurrentBlock++;\n+      if (numDocsInCurrentBlock == Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK) {\n+        flushData();\n+      }      \n     }\n \n-    meta.writeInt(numDocsWithField);\n-    meta.writeInt(minLength);\n-    meta.writeInt(maxLength);\n-    if (maxLength > minLength) {\n-      start = data.getFilePointer();\n-      meta.writeLong(start);\n+    private void flushData() throws IOException {\n+      if (numDocsInCurrentBlock > 0) {\n+        // Write offset to this block to temporary offsets file\n+        totalChunks++;\n+        long thisBlockStartPointer = data.getFilePointer();\n+        data.writeVInt(numDocsInCurrentBlock);\n+        for (int i = 0; i < numDocsInCurrentBlock; i++) {\n+          data.writeVInt(docLengths[i]);\n+        }\n+        maxUncompressedBlockLength = Math.max(maxUncompressedBlockLength, uncompressedBlockLength);\n+        LZ4.compress(block,  0, uncompressedBlockLength, data, ht);", "originalCommit": "1569fe08c8bb219df45927b102c2cbe080a43b31", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI3ODMyMw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375278323", "bodyText": "this only happens if there are no values? when do we run into this condition?", "author": "jpountz", "createdAt": "2020-02-05T14:14:49Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,168 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int [] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte [] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+      } catch (Throwable exception) {\n+        IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        throw exception;\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }\n+      docLengths[numDocsInCurrentBlock] = v.length;\n+      block = ArrayUtil.grow(block, uncompressedBlockLength + v.length);\n+      System.arraycopy(v.bytes, v.offset, block, uncompressedBlockLength, v.length);\n+      uncompressedBlockLength += v.length;\n+      numDocsInCurrentBlock++;\n+      if (numDocsInCurrentBlock == Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK) {\n+        flushData();\n+      }      \n     }\n \n-    meta.writeInt(numDocsWithField);\n-    meta.writeInt(minLength);\n-    meta.writeInt(maxLength);\n-    if (maxLength > minLength) {\n-      start = data.getFilePointer();\n-      meta.writeLong(start);\n+    private void flushData() throws IOException {\n+      if (numDocsInCurrentBlock > 0) {\n+        // Write offset to this block to temporary offsets file\n+        totalChunks++;\n+        long thisBlockStartPointer = data.getFilePointer();\n+        data.writeVInt(numDocsInCurrentBlock);\n+        for (int i = 0; i < numDocsInCurrentBlock; i++) {\n+          data.writeVInt(docLengths[i]);\n+        }\n+        maxUncompressedBlockLength = Math.max(maxUncompressedBlockLength, uncompressedBlockLength);\n+        LZ4.compress(block,  0, uncompressedBlockLength, data, ht);\n+        numDocsInCurrentBlock = 0;\n+        uncompressedBlockLength = 0;\n+        maxPointer = data.getFilePointer();\n+        tempBinaryOffsets.writeVLong(maxPointer - thisBlockStartPointer);\n+      }\n+    }\n+    \n+    void writeMetaData() throws IOException {\n+      if (blockAddressesStart < 0 ) {", "originalCommit": "1569fe08c8bb219df45927b102c2cbe080a43b31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTkxNDgzNg==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375914836", "bodyText": "Looks to be when merges clear out deleted docs leaving no values.", "author": "markharwood", "createdAt": "2020-02-06T15:47:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI3ODMyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5OTM4MA==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379299380", "bodyText": "This makes sense, can you leave a comment about it?", "author": "jpountz", "createdAt": "2020-02-14T08:20:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTI3ODMyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTgyNzM0Ng==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r375827346", "bodyText": "we could initialize uncompressedBytesRef from the uncompressed block:\nuncompressedBytesRef = new BytesRef(uncompressedBlock)\nand avoid creating new BytesRefs over and over in decode", "author": "jpountz", "createdAt": "2020-02-06T13:18:18Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -742,6 +755,107 @@ public BytesRef binaryValue() throws IOException {\n         };\n       }\n     }\n+  }  \n+  \n+  // Decompresses blocks of binary values to retrieve content\n+  class BinaryDecoder {\n+    \n+    private final LongValues addresses;\n+    private final IndexInput compressedData;\n+    // Cache of last uncompressed block \n+    private long lastBlockId = -1;\n+    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];\n+    private int uncompressedBlockLength = 0;        \n+    private int numDocsInBlock = 0;\n+    private final byte[] uncompressedBlock;\n+    private BytesRef uncompressedBytesRef;\n+    \n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize) {\n+      super();\n+      this.addresses = addresses;\n+      this.compressedData = compressedData;\n+      // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n+      this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n+      ", "originalCommit": "1569fe08c8bb219df45927b102c2cbe080a43b31", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyNzc1Mw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r376527753", "bodyText": "maybe we could call it uncompressedDocStarts and set the index at i+1 which would then help below to remove the else block of the docInBlockId > 0 condition below?", "author": "jpountz", "createdAt": "2020-02-07T18:02:14Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -742,6 +755,131 @@ public BytesRef binaryValue() throws IOException {\n         };\n       }\n     }\n+  }  \n+  \n+  // Decompresses blocks of binary values to retrieve content\n+  class BinaryDecoder {\n+    \n+    private final LongValues addresses;\n+    private final IndexInput compressedData;\n+    // Cache of last uncompressed block \n+    private long lastBlockId = -1;\n+    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];\n+    private int uncompressedBlockLength = 0;        \n+    private int numDocsInBlock = 0;\n+    private final byte[] uncompressedBlock;\n+    private final BytesRef uncompressedBytesRef;\n+    \n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize) {\n+      super();\n+      this.addresses = addresses;\n+      this.compressedData = compressedData;\n+      // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n+      this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n+      uncompressedBytesRef = new BytesRef(uncompressedBlock);\n+      \n+    }\n+\n+    BytesRef decode(int docNumber) throws IOException {\n+      int blockId = docNumber >> Lucene80DocValuesFormat.BINARY_BLOCK_SHIFT; \n+      int docInBlockId = docNumber % Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      assert docInBlockId < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      \n+      \n+      // already read and uncompressed?\n+      if (blockId != lastBlockId) {\n+        lastBlockId = blockId;\n+        long blockStartOffset = addresses.get(blockId);\n+        compressedData.seek(blockStartOffset);\n+        \n+        numDocsInBlock = compressedData.readVInt();\n+        assert numDocsInBlock <= Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+        uncompressedDocEnds = new int[numDocsInBlock];\n+        uncompressedBlockLength = 0;        \n+\n+        int onlyLength = -1;\n+        for (int i = 0; i < numDocsInBlock; i++) {\n+          if (i == 0) {\n+            // The first length value is special. It is shifted and has a bit to denote if\n+            // all other values are the same length\n+            int lengthPlusSameInd = compressedData.readVInt();\n+            int sameIndicator = lengthPlusSameInd & 1;\n+            int firstValLength = lengthPlusSameInd >>1;\n+            if (sameIndicator == 1) {\n+              onlyLength = firstValLength;\n+            }\n+            uncompressedBlockLength += firstValLength;            \n+          } else {\n+            if (onlyLength == -1) {\n+              // Various lengths are stored - read each from disk\n+              uncompressedBlockLength += compressedData.readVInt();            \n+            } else {\n+              // Only one length \n+              uncompressedBlockLength += onlyLength;\n+            }\n+          }\n+          uncompressedDocEnds[i] = uncompressedBlockLength;", "originalCommit": "8892d7fead3135f7b5c161095d69a7bf64745c0d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyODE2OQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r376528169", "bodyText": "do we really need to record the number of documents in the block? It should be 32 for all blocks except for the last one? Maybe at index-time we could append dummy values to the last block to make sure it has 32 values too, and we wouldn't need this vInt anymore?", "author": "jpountz", "createdAt": "2020-02-07T18:03:15Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -742,6 +755,131 @@ public BytesRef binaryValue() throws IOException {\n         };\n       }\n     }\n+  }  \n+  \n+  // Decompresses blocks of binary values to retrieve content\n+  class BinaryDecoder {\n+    \n+    private final LongValues addresses;\n+    private final IndexInput compressedData;\n+    // Cache of last uncompressed block \n+    private long lastBlockId = -1;\n+    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];\n+    private int uncompressedBlockLength = 0;        \n+    private int numDocsInBlock = 0;\n+    private final byte[] uncompressedBlock;\n+    private final BytesRef uncompressedBytesRef;\n+    \n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize) {\n+      super();\n+      this.addresses = addresses;\n+      this.compressedData = compressedData;\n+      // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n+      this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n+      uncompressedBytesRef = new BytesRef(uncompressedBlock);\n+      \n+    }\n+\n+    BytesRef decode(int docNumber) throws IOException {\n+      int blockId = docNumber >> Lucene80DocValuesFormat.BINARY_BLOCK_SHIFT; \n+      int docInBlockId = docNumber % Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      assert docInBlockId < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      \n+      \n+      // already read and uncompressed?\n+      if (blockId != lastBlockId) {\n+        lastBlockId = blockId;\n+        long blockStartOffset = addresses.get(blockId);\n+        compressedData.seek(blockStartOffset);\n+        \n+        numDocsInBlock = compressedData.readVInt();", "originalCommit": "8892d7fead3135f7b5c161095d69a7bf64745c0d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyOTE5NQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r376529195", "bodyText": "Since you are stealing a bit, we should do an unsigned shift (>>>) instead.\nThis would never be a problem in practice, but imagine than the length was a 31-bits integer. Shifting by one bit on the left at index time would make this number negative. So here we need an unsigned shift rather than a signed shift that preserves the sign.", "author": "jpountz", "createdAt": "2020-02-07T18:05:45Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -742,6 +755,131 @@ public BytesRef binaryValue() throws IOException {\n         };\n       }\n     }\n+  }  \n+  \n+  // Decompresses blocks of binary values to retrieve content\n+  class BinaryDecoder {\n+    \n+    private final LongValues addresses;\n+    private final IndexInput compressedData;\n+    // Cache of last uncompressed block \n+    private long lastBlockId = -1;\n+    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];\n+    private int uncompressedBlockLength = 0;        \n+    private int numDocsInBlock = 0;\n+    private final byte[] uncompressedBlock;\n+    private final BytesRef uncompressedBytesRef;\n+    \n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize) {\n+      super();\n+      this.addresses = addresses;\n+      this.compressedData = compressedData;\n+      // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n+      this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n+      uncompressedBytesRef = new BytesRef(uncompressedBlock);\n+      \n+    }\n+\n+    BytesRef decode(int docNumber) throws IOException {\n+      int blockId = docNumber >> Lucene80DocValuesFormat.BINARY_BLOCK_SHIFT; \n+      int docInBlockId = docNumber % Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      assert docInBlockId < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      \n+      \n+      // already read and uncompressed?\n+      if (blockId != lastBlockId) {\n+        lastBlockId = blockId;\n+        long blockStartOffset = addresses.get(blockId);\n+        compressedData.seek(blockStartOffset);\n+        \n+        numDocsInBlock = compressedData.readVInt();\n+        assert numDocsInBlock <= Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+        uncompressedDocEnds = new int[numDocsInBlock];\n+        uncompressedBlockLength = 0;        \n+\n+        int onlyLength = -1;\n+        for (int i = 0; i < numDocsInBlock; i++) {\n+          if (i == 0) {\n+            // The first length value is special. It is shifted and has a bit to denote if\n+            // all other values are the same length\n+            int lengthPlusSameInd = compressedData.readVInt();\n+            int sameIndicator = lengthPlusSameInd & 1;\n+            int firstValLength = lengthPlusSameInd >>1;", "originalCommit": "8892d7fead3135f7b5c161095d69a7bf64745c0d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUzMTk1Mg==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r376531952", "bodyText": "in the past we've put these constants in the meta file and BinaryEntry so that it's easier to change values over time", "author": "jpountz", "createdAt": "2020-02-07T18:12:12Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -742,6 +755,131 @@ public BytesRef binaryValue() throws IOException {\n         };\n       }\n     }\n+  }  \n+  \n+  // Decompresses blocks of binary values to retrieve content\n+  class BinaryDecoder {\n+    \n+    private final LongValues addresses;\n+    private final IndexInput compressedData;\n+    // Cache of last uncompressed block \n+    private long lastBlockId = -1;\n+    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];", "originalCommit": "8892d7fead3135f7b5c161095d69a7bf64745c0d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzU3OTk0Mw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r377579943", "bodyText": "@jpountz we should use the same structure while writing the data, in that case you will see all the properties of the class instead of adding comments in the code", "author": "juanka588", "createdAt": "2020-02-11T11:31:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUzMTk1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUzMjE4OQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r376532189", "bodyText": "can we reuse the same array across blocks?", "author": "jpountz", "createdAt": "2020-02-07T18:12:46Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -742,6 +755,131 @@ public BytesRef binaryValue() throws IOException {\n         };\n       }\n     }\n+  }  \n+  \n+  // Decompresses blocks of binary values to retrieve content\n+  class BinaryDecoder {\n+    \n+    private final LongValues addresses;\n+    private final IndexInput compressedData;\n+    // Cache of last uncompressed block \n+    private long lastBlockId = -1;\n+    private int []uncompressedDocEnds = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK];\n+    private int uncompressedBlockLength = 0;        \n+    private int numDocsInBlock = 0;\n+    private final byte[] uncompressedBlock;\n+    private final BytesRef uncompressedBytesRef;\n+    \n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize) {\n+      super();\n+      this.addresses = addresses;\n+      this.compressedData = compressedData;\n+      // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n+      this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n+      uncompressedBytesRef = new BytesRef(uncompressedBlock);\n+      \n+    }\n+\n+    BytesRef decode(int docNumber) throws IOException {\n+      int blockId = docNumber >> Lucene80DocValuesFormat.BINARY_BLOCK_SHIFT; \n+      int docInBlockId = docNumber % Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      assert docInBlockId < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+      \n+      \n+      // already read and uncompressed?\n+      if (blockId != lastBlockId) {\n+        lastBlockId = blockId;\n+        long blockStartOffset = addresses.get(blockId);\n+        compressedData.seek(blockStartOffset);\n+        \n+        numDocsInBlock = compressedData.readVInt();\n+        assert numDocsInBlock <= Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK;\n+        uncompressedDocEnds = new int[numDocsInBlock];", "originalCommit": "8892d7fead3135f7b5c161095d69a7bf64745c0d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzU0NDQ3OA==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r377544478", "bodyText": "I think we should use the BinaryEntry object here, and the just make the object \"Writable\" to a given DataOutput and \"Readable\" from a DataInput (which is already the case: readBinaryEntry). This will avoid the comments in the code -2 == docsWithFieldOffset etc.", "author": "juanka588", "createdAt": "2020-02-11T10:18:04Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,193 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      boolean success = false;\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+        success = true;\n+      } finally {\n+        if (success == false) {\n+          IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        }\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }\n+      docLengths[numDocsInCurrentBlock] = v.length;\n+      block = ArrayUtil.grow(block, uncompressedBlockLength + v.length);\n+      System.arraycopy(v.bytes, v.offset, block, uncompressedBlockLength, v.length);\n+      uncompressedBlockLength += v.length;\n+      numDocsInCurrentBlock++;\n+      if (numDocsInCurrentBlock == Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK) {\n+        flushData();\n+      }      \n     }\n \n-    meta.writeInt(numDocsWithField);\n-    meta.writeInt(minLength);\n-    meta.writeInt(maxLength);\n-    if (maxLength > minLength) {\n-      start = data.getFilePointer();\n-      meta.writeLong(start);\n+    private void flushData() throws IOException {\n+      if (numDocsInCurrentBlock > 0) {\n+        // Write offset to this block to temporary offsets file\n+        totalChunks++;\n+        long thisBlockStartPointer = data.getFilePointer();\n+        \n+        // Optimisation - check if all lengths are same\n+        boolean allLengthsSame = true && numDocsInCurrentBlock >0  ;\n+        for (int i = 0; i < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK && allLengthsSame; i++) {\n+          if (i > 0 && docLengths[i] != docLengths[i-1]) {\n+            allLengthsSame = false;\n+          }\n+        }\n+        if (allLengthsSame) {\n+            // Only write one value shifted. Steal a bit to indicate all other lengths are the same\n+            int onlyOneLength = (docLengths[0] <<1) | 1;\n+            data.writeVInt(onlyOneLength);\n+        } else {\n+          for (int i = 0; i < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK; i++) {\n+            if(i == 0) {\n+              // Write first value shifted and steal a bit to indicate other lengths are to follow\n+              int multipleLengths = (docLengths[0] <<1);\n+              data.writeVInt(multipleLengths);              \n+            } else {\n+              data.writeVInt(docLengths[i]);\n+            }\n+          }\n+        }\n+        maxUncompressedBlockLength = Math.max(maxUncompressedBlockLength, uncompressedBlockLength);\n+        LZ4.compress(block, 0, uncompressedBlockLength, data, ht);\n+        numDocsInCurrentBlock = 0;\n+        // Ensure initialized with zeroes because full array is always written\n+        Arrays.fill(docLengths, 0);\n+        uncompressedBlockLength = 0;\n+        maxPointer = data.getFilePointer();\n+        tempBinaryOffsets.writeVLong(maxPointer - thisBlockStartPointer);\n+      }\n+    }\n+    \n+    void writeMetaData() throws IOException {\n+      if (blockAddressesStart < 0 ) {\n+        return;\n+      }\n+      \n+      long startDMW = data.getFilePointer();\n+      meta.writeLong(startDMW);\n+      \n+      meta.writeVInt(totalChunks);\n+      meta.writeVInt(Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK);\n+      meta.writeVInt(maxUncompressedBlockLength);\n       meta.writeVInt(DIRECT_MONOTONIC_BLOCK_SHIFT);\n+      \n+    \n+      CodecUtil.writeFooter(tempBinaryOffsets);\n+      IOUtils.close(tempBinaryOffsets);             \n+      //write the compressed block offsets info to the meta file by reading from temp file\n+      try (ChecksumIndexInput filePointersIn = state.directory.openChecksumInput(tempBinaryOffsets.getName(), IOContext.READONCE)) {\n+        CodecUtil.checkHeader(filePointersIn, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT,\n+          Lucene80DocValuesFormat.VERSION_CURRENT);\n+        Throwable priorE = null;\n+        try {\n+          final DirectMonotonicWriter filePointers = DirectMonotonicWriter.getInstance(meta, data, totalChunks, DIRECT_MONOTONIC_BLOCK_SHIFT);\n+          long fp = blockAddressesStart;\n+          for (int i = 0; i < totalChunks; ++i) {\n+            filePointers.add(fp);\n+            fp += filePointersIn.readVLong();\n+          }\n+          if (maxPointer < fp) {\n+            throw new CorruptIndexException(\"File pointers don't add up (\"+fp+\" vs expected \"+maxPointer+\")\", filePointersIn);\n+          }\n+          filePointers.finish();\n+        } catch (Throwable e) {\n+          priorE = e;\n+        } finally {\n+          CodecUtil.checkFooter(filePointersIn, priorE);\n+        }\n+      }\n+      // Write the length of the DMW block in the data \n+      meta.writeLong(data.getFilePointer() - startDMW);\n+    }\n \n-      final DirectMonotonicWriter writer = DirectMonotonicWriter.getInstance(meta, data, numDocsWithField + 1, DIRECT_MONOTONIC_BLOCK_SHIFT);\n-      long addr = 0;\n-      writer.add(addr);\n-      values = valuesProducer.getBinary(field);\n+    @Override\n+    public void close() throws IOException {\n+      if (tempBinaryOffsets != null) {\n+        IOUtils.close(tempBinaryOffsets);             \n+        state.directory.deleteFile(tempBinaryOffsets.getName());\n+        tempBinaryOffsets = null;\n+      }\n+    }\n+    \n+  }\n+  \n+\n+  @Override\n+  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n+    meta.writeInt(field.number);\n+    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n+\n+    try (CompressedBinaryBlockWriter blockWriter = new CompressedBinaryBlockWriter()){\n+      BinaryDocValues values = valuesProducer.getBinary(field);\n+      long start = data.getFilePointer();\n+      meta.writeLong(start); // dataOffset", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzYyMTAwMw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r377621003", "bodyText": "I like the idea but would prefer doing it in a separate PR.", "author": "jpountz", "createdAt": "2020-02-11T13:05:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzU0NDQ3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzU0NTkwOQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r377545909", "bodyText": "Currently I'm working in a refactor of this code by having a doc id set iterator serializer capable to provide the correct instance based on the stored metadata. As you might see this is quite repetitive for the other fields", "author": "juanka588", "createdAt": "2020-02-11T10:20:40Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,193 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      boolean success = false;\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+        success = true;\n+      } finally {\n+        if (success == false) {\n+          IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        }\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }\n+      docLengths[numDocsInCurrentBlock] = v.length;\n+      block = ArrayUtil.grow(block, uncompressedBlockLength + v.length);\n+      System.arraycopy(v.bytes, v.offset, block, uncompressedBlockLength, v.length);\n+      uncompressedBlockLength += v.length;\n+      numDocsInCurrentBlock++;\n+      if (numDocsInCurrentBlock == Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK) {\n+        flushData();\n+      }      \n     }\n \n-    meta.writeInt(numDocsWithField);\n-    meta.writeInt(minLength);\n-    meta.writeInt(maxLength);\n-    if (maxLength > minLength) {\n-      start = data.getFilePointer();\n-      meta.writeLong(start);\n+    private void flushData() throws IOException {\n+      if (numDocsInCurrentBlock > 0) {\n+        // Write offset to this block to temporary offsets file\n+        totalChunks++;\n+        long thisBlockStartPointer = data.getFilePointer();\n+        \n+        // Optimisation - check if all lengths are same\n+        boolean allLengthsSame = true && numDocsInCurrentBlock >0  ;\n+        for (int i = 0; i < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK && allLengthsSame; i++) {\n+          if (i > 0 && docLengths[i] != docLengths[i-1]) {\n+            allLengthsSame = false;\n+          }\n+        }\n+        if (allLengthsSame) {\n+            // Only write one value shifted. Steal a bit to indicate all other lengths are the same\n+            int onlyOneLength = (docLengths[0] <<1) | 1;\n+            data.writeVInt(onlyOneLength);\n+        } else {\n+          for (int i = 0; i < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK; i++) {\n+            if(i == 0) {\n+              // Write first value shifted and steal a bit to indicate other lengths are to follow\n+              int multipleLengths = (docLengths[0] <<1);\n+              data.writeVInt(multipleLengths);              \n+            } else {\n+              data.writeVInt(docLengths[i]);\n+            }\n+          }\n+        }\n+        maxUncompressedBlockLength = Math.max(maxUncompressedBlockLength, uncompressedBlockLength);\n+        LZ4.compress(block, 0, uncompressedBlockLength, data, ht);\n+        numDocsInCurrentBlock = 0;\n+        // Ensure initialized with zeroes because full array is always written\n+        Arrays.fill(docLengths, 0);\n+        uncompressedBlockLength = 0;\n+        maxPointer = data.getFilePointer();\n+        tempBinaryOffsets.writeVLong(maxPointer - thisBlockStartPointer);\n+      }\n+    }\n+    \n+    void writeMetaData() throws IOException {\n+      if (blockAddressesStart < 0 ) {\n+        return;\n+      }\n+      \n+      long startDMW = data.getFilePointer();\n+      meta.writeLong(startDMW);\n+      \n+      meta.writeVInt(totalChunks);\n+      meta.writeVInt(Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK);\n+      meta.writeVInt(maxUncompressedBlockLength);\n       meta.writeVInt(DIRECT_MONOTONIC_BLOCK_SHIFT);\n+      \n+    \n+      CodecUtil.writeFooter(tempBinaryOffsets);\n+      IOUtils.close(tempBinaryOffsets);             \n+      //write the compressed block offsets info to the meta file by reading from temp file\n+      try (ChecksumIndexInput filePointersIn = state.directory.openChecksumInput(tempBinaryOffsets.getName(), IOContext.READONCE)) {\n+        CodecUtil.checkHeader(filePointersIn, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT,\n+          Lucene80DocValuesFormat.VERSION_CURRENT);\n+        Throwable priorE = null;\n+        try {\n+          final DirectMonotonicWriter filePointers = DirectMonotonicWriter.getInstance(meta, data, totalChunks, DIRECT_MONOTONIC_BLOCK_SHIFT);\n+          long fp = blockAddressesStart;\n+          for (int i = 0; i < totalChunks; ++i) {\n+            filePointers.add(fp);\n+            fp += filePointersIn.readVLong();\n+          }\n+          if (maxPointer < fp) {\n+            throw new CorruptIndexException(\"File pointers don't add up (\"+fp+\" vs expected \"+maxPointer+\")\", filePointersIn);\n+          }\n+          filePointers.finish();\n+        } catch (Throwable e) {\n+          priorE = e;\n+        } finally {\n+          CodecUtil.checkFooter(filePointersIn, priorE);\n+        }\n+      }\n+      // Write the length of the DMW block in the data \n+      meta.writeLong(data.getFilePointer() - startDMW);\n+    }\n \n-      final DirectMonotonicWriter writer = DirectMonotonicWriter.getInstance(meta, data, numDocsWithField + 1, DIRECT_MONOTONIC_BLOCK_SHIFT);\n-      long addr = 0;\n-      writer.add(addr);\n-      values = valuesProducer.getBinary(field);\n+    @Override\n+    public void close() throws IOException {\n+      if (tempBinaryOffsets != null) {\n+        IOUtils.close(tempBinaryOffsets);             \n+        state.directory.deleteFile(tempBinaryOffsets.getName());\n+        tempBinaryOffsets = null;\n+      }\n+    }\n+    \n+  }\n+  \n+\n+  @Override\n+  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n+    meta.writeInt(field.number);\n+    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n+\n+    try (CompressedBinaryBlockWriter blockWriter = new CompressedBinaryBlockWriter()){\n+      BinaryDocValues values = valuesProducer.getBinary(field);\n+      long start = data.getFilePointer();\n+      meta.writeLong(start); // dataOffset\n+      int numDocsWithField = 0;\n+      int minLength = Integer.MAX_VALUE;\n+      int maxLength = 0;\n       for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-        addr += values.binaryValue().length;\n-        writer.add(addr);\n+        numDocsWithField++;\n+        BytesRef v = values.binaryValue();      \n+        blockWriter.addDoc(doc, v);      \n+        int length = v.length;      \n+        minLength = Math.min(length, minLength);\n+        maxLength = Math.max(length, maxLength);\n       }\n-      writer.finish();\n-      meta.writeLong(data.getFilePointer() - start);\n+      blockWriter.flushData();\n+\n+      assert numDocsWithField <= maxDoc;\n+      meta.writeLong(data.getFilePointer() - start); // dataLength\n+\n+      if (numDocsWithField == 0) {\n+        meta.writeLong(-2); // docsWithFieldOffset\n+        meta.writeLong(0L); // docsWithFieldLength\n+        meta.writeShort((short) -1); // jumpTableEntryCount\n+        meta.writeByte((byte) -1);   // denseRankPower\n+      } else if (numDocsWithField == maxDoc) {\n+        meta.writeLong(-1); // docsWithFieldOffset\n+        meta.writeLong(0L); // docsWithFieldLength\n+        meta.writeShort((short) -1); // jumpTableEntryCount\n+        meta.writeByte((byte) -1);   // denseRankPower\n+      } else {\n+        long offset = data.getFilePointer();\n+        meta.writeLong(offset); // docsWithFieldOffset\n+        values = valuesProducer.getBinary(field);\n+        final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+        meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n+        meta.writeShort(jumpTableEntryCount);\n+        meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+      }", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzU2NjU0Mw==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r377566543", "bodyText": "This could be potentially in the BinaryDocValuesFormat class", "author": "juanka588", "createdAt": "2020-02-11T11:02:17Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesFormat.java", "diffHunk": "@@ -151,7 +151,8 @@ public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOExcepti\n   static final String META_CODEC = \"Lucene80DocValuesMetadata\";\n   static final String META_EXTENSION = \"dvm\";\n   static final int VERSION_START = 0;\n-  static final int VERSION_CURRENT = VERSION_START;\n+  static final int VERSION_BIN_COMPRESSED = 1;  ", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5NDc5OQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379294799", "bodyText": "can we make ht, tempBinaryOffsets, docLengths final?", "author": "jpountz", "createdAt": "2020-02-14T08:06:54Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,193 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5NTQzMg==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379295432", "bodyText": "Depending on the data that will be indexed it's very hard to know what is the right initial size here. Maybe start with an empty array? This will also give increase confidence that the resizing logic works.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                byte[] block = new byte [1024 * 16];\n          \n          \n            \n                byte[] block = BytesRef.EMPTY_BYTES;", "author": "jpountz", "createdAt": "2020-02-14T08:08:55Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,193 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5ODIxMg==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379298212", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        if(i == 0) {\n          \n          \n            \n                        if (i == 0) {", "author": "jpountz", "createdAt": "2020-02-14T08:17:14Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,193 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      boolean success = false;\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+        success = true;\n+      } finally {\n+        if (success == false) {\n+          IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        }\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }\n+      docLengths[numDocsInCurrentBlock] = v.length;\n+      block = ArrayUtil.grow(block, uncompressedBlockLength + v.length);\n+      System.arraycopy(v.bytes, v.offset, block, uncompressedBlockLength, v.length);\n+      uncompressedBlockLength += v.length;\n+      numDocsInCurrentBlock++;\n+      if (numDocsInCurrentBlock == Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK) {\n+        flushData();\n+      }      \n     }\n \n-    meta.writeInt(numDocsWithField);\n-    meta.writeInt(minLength);\n-    meta.writeInt(maxLength);\n-    if (maxLength > minLength) {\n-      start = data.getFilePointer();\n-      meta.writeLong(start);\n+    private void flushData() throws IOException {\n+      if (numDocsInCurrentBlock > 0) {\n+        // Write offset to this block to temporary offsets file\n+        totalChunks++;\n+        long thisBlockStartPointer = data.getFilePointer();\n+        \n+        // Optimisation - check if all lengths are same\n+        boolean allLengthsSame = true && numDocsInCurrentBlock >0  ;\n+        for (int i = 0; i < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK && allLengthsSame; i++) {\n+          if (i > 0 && docLengths[i] != docLengths[i-1]) {\n+            allLengthsSame = false;\n+          }\n+        }\n+        if (allLengthsSame) {\n+            // Only write one value shifted. Steal a bit to indicate all other lengths are the same\n+            int onlyOneLength = (docLengths[0] <<1) | 1;\n+            data.writeVInt(onlyOneLength);\n+        } else {\n+          for (int i = 0; i < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK; i++) {\n+            if(i == 0) {", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5ODc2MQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379298761", "bodyText": "The second condition is necessary true given the parent if statement.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    boolean allLengthsSame = true && numDocsInCurrentBlock >0  ;\n          \n          \n            \n                    boolean allLengthsSame = true;", "author": "jpountz", "createdAt": "2020-02-14T08:18:50Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,193 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      boolean success = false;\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+        success = true;\n+      } finally {\n+        if (success == false) {\n+          IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        }\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }\n+      docLengths[numDocsInCurrentBlock] = v.length;\n+      block = ArrayUtil.grow(block, uncompressedBlockLength + v.length);\n+      System.arraycopy(v.bytes, v.offset, block, uncompressedBlockLength, v.length);\n+      uncompressedBlockLength += v.length;\n+      numDocsInCurrentBlock++;\n+      if (numDocsInCurrentBlock == Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK) {\n+        flushData();\n+      }      \n     }\n \n-    meta.writeInt(numDocsWithField);\n-    meta.writeInt(minLength);\n-    meta.writeInt(maxLength);\n-    if (maxLength > minLength) {\n-      start = data.getFilePointer();\n-      meta.writeLong(start);\n+    private void flushData() throws IOException {\n+      if (numDocsInCurrentBlock > 0) {\n+        // Write offset to this block to temporary offsets file\n+        totalChunks++;\n+        long thisBlockStartPointer = data.getFilePointer();\n+        \n+        // Optimisation - check if all lengths are same\n+        boolean allLengthsSame = true && numDocsInCurrentBlock >0  ;", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMwNDA3NA==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379304074", "bodyText": "maybe keep this variable actually, it would help make version final by doing this.version = version; after the try block?", "author": "jpountz", "createdAt": "2020-02-14T08:33:18Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -59,18 +60,18 @@\n   private long ramBytesUsed;\n   private final IndexInput data;\n   private final int maxDoc;\n+  private int version = -1;\n \n   /** expert: instantiates a new reader */\n   Lucene80DocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {\n     String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);\n     this.maxDoc = state.segmentInfo.maxDoc();\n     ramBytesUsed = RamUsageEstimator.shallowSizeOfInstance(getClass());\n \n-    int version = -1;", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMwNDM2OQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379304369", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                if ((version >= Lucene80DocValuesFormat.VERSION_BIN_COMPRESSED && entry.numDocsWithField >0)||  entry.minLength < entry.maxLength) {\n          \n          \n            \n                if ((version >= Lucene80DocValuesFormat.VERSION_BIN_COMPRESSED && entry.numDocsWithField > 0) ||  entry.minLength < entry.maxLength) {", "author": "jpountz", "createdAt": "2020-02-14T08:34:03Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -182,10 +183,21 @@ private BinaryEntry readBinary(ChecksumIndexInput meta) throws IOException {\n     entry.numDocsWithField = meta.readInt();\n     entry.minLength = meta.readInt();\n     entry.maxLength = meta.readInt();\n-    if (entry.minLength < entry.maxLength) {\n+    if ((version >= Lucene80DocValuesFormat.VERSION_BIN_COMPRESSED && entry.numDocsWithField >0)||  entry.minLength < entry.maxLength) {", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMwNTEyOA==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379305128", "bodyText": "let's use the shift from the BinaryEntry instead of the constant?", "author": "jpountz", "createdAt": "2020-02-14T08:36:08Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -742,6 +757,125 @@ public BytesRef binaryValue() throws IOException {\n         };\n       }\n     }\n+  }  \n+  \n+  // Decompresses blocks of binary values to retrieve content\n+  class BinaryDecoder {\n+    \n+    private final LongValues addresses;\n+    private final IndexInput compressedData;\n+    // Cache of last uncompressed block \n+    private long lastBlockId = -1;\n+    private final int []uncompressedDocStarts;\n+    private int uncompressedBlockLength = 0;        \n+    private final byte[] uncompressedBlock;\n+    private final BytesRef uncompressedBytesRef;\n+    private final int docsPerChunk;\n+    \n+    public BinaryDecoder(LongValues addresses, IndexInput compressedData, int biggestUncompressedBlockSize, int docsPerChunk) {\n+      super();\n+      this.addresses = addresses;\n+      this.compressedData = compressedData;\n+      // pre-allocate a byte array large enough for the biggest uncompressed block needed.\n+      this.uncompressedBlock = new byte[biggestUncompressedBlockSize];\n+      uncompressedBytesRef = new BytesRef(uncompressedBlock);\n+      this.docsPerChunk = docsPerChunk;\n+      uncompressedDocStarts = new int[docsPerChunk + 1];\n+      \n+    }\n+\n+    BytesRef decode(int docNumber) throws IOException {\n+      int blockId = docNumber >> Lucene80DocValuesFormat.BINARY_BLOCK_SHIFT; ", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQ2MzQ0MA==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379463440", "bodyText": "I guess that means I should serialize the shift value rather the absolute number of docs per block?", "author": "markharwood", "createdAt": "2020-02-14T14:37:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMwNTEyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQ3Mzg2MA==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379473860", "bodyText": "I think so.", "author": "jpountz", "createdAt": "2020-02-14T14:56:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMwNTEyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMwNjMyNg==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379306326", "bodyText": "maybe this should be the \"shift\" instead of the number of docs per chunk, so that you you directly have both the shift (as-is) and the mask ((1 << shift) - 1)", "author": "jpountz", "createdAt": "2020-02-14T08:39:11Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesProducer.java", "diffHunk": "@@ -182,10 +183,21 @@ private BinaryEntry readBinary(ChecksumIndexInput meta) throws IOException {\n     entry.numDocsWithField = meta.readInt();\n     entry.minLength = meta.readInt();\n     entry.maxLength = meta.readInt();\n-    if (entry.minLength < entry.maxLength) {\n+    if ((version >= Lucene80DocValuesFormat.VERSION_BIN_COMPRESSED && entry.numDocsWithField >0)||  entry.minLength < entry.maxLength) {\n       entry.addressesOffset = meta.readLong();\n+\n+      // Old count of uncompressed addresses \n+      long numAddresses = entry.numDocsWithField + 1L;\n+      // New count of compressed addresses - the number of compresseed blocks\n+      if (version >= Lucene80DocValuesFormat.VERSION_BIN_COMPRESSED) {\n+        entry.numCompressedChunks = meta.readVInt();\n+        entry.docsPerChunk = meta.readVInt();", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQ2MzY3NQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379463675", "bodyText": "Ah - ignore my previous comment.", "author": "markharwood", "createdAt": "2020-02-14T14:37:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMwNjMyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMwNjkwOQ==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379306909", "bodyText": "in general we do a break when setting allLengthsSame = false instead of adding it to the exit condition of the for statement", "author": "jpountz", "createdAt": "2020-02-14T08:40:46Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,193 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      boolean success = false;\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+        success = true;\n+      } finally {\n+        if (success == false) {\n+          IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        }\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }\n+      docLengths[numDocsInCurrentBlock] = v.length;\n+      block = ArrayUtil.grow(block, uncompressedBlockLength + v.length);\n+      System.arraycopy(v.bytes, v.offset, block, uncompressedBlockLength, v.length);\n+      uncompressedBlockLength += v.length;\n+      numDocsInCurrentBlock++;\n+      if (numDocsInCurrentBlock == Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK) {\n+        flushData();\n+      }      \n     }\n \n-    meta.writeInt(numDocsWithField);\n-    meta.writeInt(minLength);\n-    meta.writeInt(maxLength);\n-    if (maxLength > minLength) {\n-      start = data.getFilePointer();\n-      meta.writeLong(start);\n+    private void flushData() throws IOException {\n+      if (numDocsInCurrentBlock > 0) {\n+        // Write offset to this block to temporary offsets file\n+        totalChunks++;\n+        long thisBlockStartPointer = data.getFilePointer();\n+        \n+        // Optimisation - check if all lengths are same\n+        boolean allLengthsSame = true && numDocsInCurrentBlock >0  ;\n+        for (int i = 0; i < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK && allLengthsSame; i++) {", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTMwNzExNg==", "url": "https://github.com/apache/lucene-solr/pull/1234#discussion_r379307116", "bodyText": "if you're only doing it for i>0, let's make the loop start at i=1?", "author": "jpountz", "createdAt": "2020-02-14T08:41:20Z", "path": "lucene/core/src/java/org/apache/lucene/codecs/lucene80/Lucene80DocValuesConsumer.java", "diffHunk": "@@ -353,67 +360,193 @@ private void writeBlock(long[] values, int length, long gcd, ByteBuffersDataOutp\n     }\n   }\n \n-  @Override\n-  public void addBinaryField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n-    meta.writeInt(field.number);\n-    meta.writeByte(Lucene80DocValuesFormat.BINARY);\n-\n-    BinaryDocValues values = valuesProducer.getBinary(field);\n-    long start = data.getFilePointer();\n-    meta.writeLong(start); // dataOffset\n-    int numDocsWithField = 0;\n-    int minLength = Integer.MAX_VALUE;\n-    int maxLength = 0;\n-    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n-      numDocsWithField++;\n-      BytesRef v = values.binaryValue();\n-      int length = v.length;\n-      data.writeBytes(v.bytes, v.offset, v.length);\n-      minLength = Math.min(length, minLength);\n-      maxLength = Math.max(length, maxLength);\n+  class CompressedBinaryBlockWriter implements Closeable {\n+    FastCompressionHashTable ht = new LZ4.FastCompressionHashTable();    \n+    int uncompressedBlockLength = 0;\n+    int maxUncompressedBlockLength = 0;\n+    int numDocsInCurrentBlock = 0;\n+    int[] docLengths = new int[Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK]; \n+    byte[] block = new byte [1024 * 16];\n+    int totalChunks = 0;\n+    long maxPointer = 0;\n+    long blockAddressesStart = -1; \n+\n+    private IndexOutput tempBinaryOffsets;\n+    \n+    \n+    public CompressedBinaryBlockWriter() throws IOException {\n+      tempBinaryOffsets = state.directory.createTempOutput(state.segmentInfo.name, \"binary_pointers\", state.context);\n+      boolean success = false;\n+      try {\n+        CodecUtil.writeHeader(tempBinaryOffsets, Lucene80DocValuesFormat.META_CODEC + \"FilePointers\", Lucene80DocValuesFormat.VERSION_CURRENT);\n+        success = true;\n+      } finally {\n+        if (success == false) {\n+          IOUtils.closeWhileHandlingException(this); //self-close because constructor caller can't \n+        }\n+      }\n     }\n-    assert numDocsWithField <= maxDoc;\n-    meta.writeLong(data.getFilePointer() - start); // dataLength\n \n-    if (numDocsWithField == 0) {\n-      meta.writeLong(-2); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else if (numDocsWithField == maxDoc) {\n-      meta.writeLong(-1); // docsWithFieldOffset\n-      meta.writeLong(0L); // docsWithFieldLength\n-      meta.writeShort((short) -1); // jumpTableEntryCount\n-      meta.writeByte((byte) -1);   // denseRankPower\n-    } else {\n-      long offset = data.getFilePointer();\n-      meta.writeLong(offset); // docsWithFieldOffset\n-      values = valuesProducer.getBinary(field);\n-      final short jumpTableEntryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n-      meta.writeLong(data.getFilePointer() - offset); // docsWithFieldLength\n-      meta.writeShort(jumpTableEntryCount);\n-      meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);\n+    void addDoc(int doc, BytesRef v) throws IOException {\n+      if (blockAddressesStart < 0) {\n+        blockAddressesStart = data.getFilePointer();\n+      }\n+      docLengths[numDocsInCurrentBlock] = v.length;\n+      block = ArrayUtil.grow(block, uncompressedBlockLength + v.length);\n+      System.arraycopy(v.bytes, v.offset, block, uncompressedBlockLength, v.length);\n+      uncompressedBlockLength += v.length;\n+      numDocsInCurrentBlock++;\n+      if (numDocsInCurrentBlock == Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK) {\n+        flushData();\n+      }      \n     }\n \n-    meta.writeInt(numDocsWithField);\n-    meta.writeInt(minLength);\n-    meta.writeInt(maxLength);\n-    if (maxLength > minLength) {\n-      start = data.getFilePointer();\n-      meta.writeLong(start);\n+    private void flushData() throws IOException {\n+      if (numDocsInCurrentBlock > 0) {\n+        // Write offset to this block to temporary offsets file\n+        totalChunks++;\n+        long thisBlockStartPointer = data.getFilePointer();\n+        \n+        // Optimisation - check if all lengths are same\n+        boolean allLengthsSame = true && numDocsInCurrentBlock >0  ;\n+        for (int i = 0; i < Lucene80DocValuesFormat.BINARY_DOCS_PER_COMPRESSED_BLOCK && allLengthsSame; i++) {\n+          if (i > 0 && docLengths[i] != docLengths[i-1]) {", "originalCommit": "b1048ba517469b8523bab7de91c10922b1c35575", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "a20da17708f17f1b533ff515ad6d7ca249f610a9", "url": "https://github.com/apache/lucene-solr/commit/a20da17708f17f1b533ff515ad6d7ca249f610a9", "message": "Added variously compressible examples of content to tests as per @mikemccand suggestion", "committedDate": "2020-02-14T09:37:56Z", "type": "forcePushed"}, {"oid": "1dcbb2e2ceddd70ec5842c35083a440b267582e9", "url": "https://github.com/apache/lucene-solr/commit/1dcbb2e2ceddd70ec5842c35083a440b267582e9", "message": "Added compression for binary doc values - stores groups of 32 doc values in LZ4 compressed blocks", "committedDate": "2020-02-18T09:45:32Z", "type": "commit"}, {"oid": "1b0c227412370666a627cfada93ef288fab9111f", "url": "https://github.com/apache/lucene-solr/commit/1b0c227412370666a627cfada93ef288fab9111f", "message": "Formatting fixes (Thanks, Mike!)", "committedDate": "2020-02-18T09:45:32Z", "type": "commit"}, {"oid": "1b08f9b13194797629a2dea6130c0cacde6a5f4d", "url": "https://github.com/apache/lucene-solr/commit/1b08f9b13194797629a2dea6130c0cacde6a5f4d", "message": "Addressing review comments (thanks, jpountz!)", "committedDate": "2020-02-18T09:45:32Z", "type": "commit"}, {"oid": "6dd00af56e4eabfd5815f6dae1461185222576ed", "url": "https://github.com/apache/lucene-solr/commit/6dd00af56e4eabfd5815f6dae1461185222576ed", "message": "Optimisation - only write one value if all docs share the same length", "committedDate": "2020-02-18T09:45:32Z", "type": "commit"}, {"oid": "f69f99001c50294f70400c78f9c5a39c2e5e1018", "url": "https://github.com/apache/lucene-solr/commit/f69f99001c50294f70400c78f9c5a39c2e5e1018", "message": "Addressing @jpountz review comments - make num docs per chunk part of metadata, remove per-chunk storage of num docs and assume always same number of values. Change doc end pointers to be doc starts.", "committedDate": "2020-02-18T09:45:32Z", "type": "commit"}, {"oid": "289d549bc3003ce1d6a58d9f417123b1631bd7dd", "url": "https://github.com/apache/lucene-solr/commit/289d549bc3003ce1d6a58d9f417123b1631bd7dd", "message": "Added variously compressible examples of content to tests as per @mikemccand suggestion", "committedDate": "2020-02-18T09:45:32Z", "type": "commit"}, {"oid": "7d0db34b68145dfd0454b2ef5b3f0138aab431df", "url": "https://github.com/apache/lucene-solr/commit/7d0db34b68145dfd0454b2ef5b3f0138aab431df", "message": "Addressing review comments. Fields made final, initialise blockAddressesStart in CompressedBinaryBlockWriter constructor, serialise shift for number of docs in block,", "committedDate": "2020-02-18T09:45:32Z", "type": "commit"}, {"oid": "7d0db34b68145dfd0454b2ef5b3f0138aab431df", "url": "https://github.com/apache/lucene-solr/commit/7d0db34b68145dfd0454b2ef5b3f0138aab431df", "message": "Addressing review comments. Fields made final, initialise blockAddressesStart in CompressedBinaryBlockWriter constructor, serialise shift for number of docs in block,", "committedDate": "2020-02-18T09:45:32Z", "type": "forcePushed"}, {"oid": "3a3c30fca98e30e1159ecb1707d6874a32212056", "url": "https://github.com/apache/lucene-solr/commit/3a3c30fca98e30e1159ecb1707d6874a32212056", "message": "Added note to changes.txt", "committedDate": "2020-02-18T13:39:29Z", "type": "commit"}]}