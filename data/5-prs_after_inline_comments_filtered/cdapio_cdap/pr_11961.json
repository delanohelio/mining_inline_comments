{"pr_number": 11961, "pr_title": "[CDAP-15633] Adding dataproc runtime manager implementation", "pr_createdAt": "2020-03-16T05:02:13Z", "pr_url": "https://github.com/cdapio/cdap/pull/11961", "timeline": [{"oid": "0c5c24829959e00be25a2b904ff7c4e00575bf2a", "url": "https://github.com/cdapio/cdap/commit/0c5c24829959e00be25a2b904ff7c4e00575bf2a", "message": "[CDAP-15633] Adding dataproc runtime manager implementation", "committedDate": "2020-03-17T04:59:43Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ3MjcxNg==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393472716", "bodyText": "This is not needed. For SSH case, this is being detected. For Dataproc case, it should be returned by the RuntimeJobEnvironment.getProperties to contain a key-value pair \"app.program.spark.compat\" => SparkCompat.SPARK2_2_11.getCompat() since we only use Spark2 in dataproc.", "author": "chtyim", "createdAt": "2020-03-17T06:47:52Z", "path": "cdap-app-fabric/src/main/java/io/cdap/cdap/internal/app/runtime/distributed/runtimejob/DefaultRuntimeJob.java", "diffHunk": "@@ -103,6 +103,7 @@ public void run(RuntimeJobEnvironment runtimeJobEnv) throws Exception {\n     // Create cConf with provided properties. These properties can be used to set configs for twill runner such as\n     // connection string for discovery.\n     CConfiguration cConf = createCConf(runtimeJobEnv);\n+    System.setProperty(Constants.SPARK_COMPAT_ENV, cConf.get(Constants.AppFabric.SPARK_COMPAT));", "originalCommit": "0c5c24829959e00be25a2b904ff7c4e00575bf2a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzYwNTAyNQ==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393605025", "bodyText": "Actually spark version should be passed to DataprocJobMain from provisioner. This is because provisioner supports multiple spark compat based on image version\nhttps://github.com/cdapio/cdap/blob/develop/cdap-runtime-ext-dataproc/src/main/java/io/cdap/cdap/runtime/spi/provisioner/dataproc/DataprocProvisioner.java#L179\nLet me know your thoughts", "author": "CuriousVini", "createdAt": "2020-03-17T11:16:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ3MjcxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDAzNDQxOQ==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r394034419", "bodyText": "Yes, that's what I meant by returned from the job env properties. It is the DataprocProvisioner who creates the RuntimeJobManager, which in turn launch the job.", "author": "chtyim", "createdAt": "2020-03-17T23:48:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ3MjcxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDA2MjM0OA==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r394062348", "bodyText": "sounds good. \ud83d\udc4d", "author": "CuriousVini", "createdAt": "2020-03-18T01:43:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ3MjcxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ3MzU0NQ==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393473545", "bodyText": "GCS bucket is a global name, hence using a constant is almost for sure not appropriate. I think bucket has to be a required property.", "author": "chtyim", "createdAt": "2020-03-17T06:50:29Z", "path": "cdap-runtime-ext-dataproc/src/main/java/io/cdap/cdap/runtime/spi/provisioner/dataproc/DataprocProvisioner.java", "diffHunk": "@@ -62,6 +66,7 @@\n \n   // Key which is set to true if the instance only have private ip assigned to it else false\n   private static final String PRIVATE_INSTANCE = \"privateInstance\";\n+  private static final String BUCKET = \"bucket\";", "originalCommit": "0c5c24829959e00be25a2b904ff7c4e00575bf2a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzYwMTk0OA==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393601948", "bodyText": "This is a static constant for the system property that comes from system context in case user has not provided bucket. Bucket is a optional property for dataproc cluster. We are following similar pattern.", "author": "CuriousVini", "createdAt": "2020-03-17T11:10:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ3MzU0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzg5MjQ5MA==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393892490", "bodyText": "Why it needs a separate property?? If the DataprocConf already defines this bucket property, it can already be overridden by preferences/runtime arguments.", "author": "chtyim", "createdAt": "2020-03-17T18:39:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ3MzU0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzg5MzEwMg==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393893102", "bodyText": "Also, what if the user don't provide the value, isn't that it will always fail because we upload jar files there?", "author": "chtyim", "createdAt": "2020-03-17T18:40:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ3MzU0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDA5OTgyNA==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r394099824", "bodyText": "If user has not provided value, we will use checkpoint gcs bucket. We are using BUCKET property for that.\nIf user has provided bucket as property of provisioner, user provided bucket will be used.", "author": "CuriousVini", "createdAt": "2020-03-18T04:23:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ3MzU0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDUxODY1OQ==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r394518659", "bodyText": "Got it.", "author": "chtyim", "createdAt": "2020-03-18T17:26:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ3MzU0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ3NDU2MA==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393474560", "bodyText": "These are actually defined in twill-common module org.apache.twill.internal.Constants. You can use them directly without defining this class.", "author": "chtyim", "createdAt": "2020-03-17T06:53:58Z", "path": "cdap-runtime-ext-dataproc/src/main/java/io/cdap/cdap/runtime/spi/runtimejob/Constants.java", "diffHunk": "@@ -0,0 +1,31 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.runtime.spi.runtimejob;\n+\n+/**\n+ * Constants for dataproc runtime.\n+ */\n+final class Constants {\n+  static final String TWILL_JAR = \"twill.jar\";", "originalCommit": "0c5c24829959e00be25a2b904ff7c4e00575bf2a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ3NTk0NQ==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393475945", "bodyText": "The spark-assembly is assuming how the jar file is named, in which there is no guarantee that it won't change. However, do you know which class actually is coming from the spark-assembly jar? We should see why, because the classloader in CDAP shouldn't trace back to spark-assembly jar for any classes that is not coming from the spark runtime extension.", "author": "chtyim", "createdAt": "2020-03-17T06:58:16Z", "path": "cdap-runtime-ext-dataproc/src/main/java/io/cdap/cdap/runtime/spi/runtimejob/DataprocJarUtil.java", "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.runtime.spi.runtimejob;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.io.ByteStreams;\n+import joptsimple.OptionSpec;\n+import org.apache.twill.api.ClassAcceptor;\n+import org.apache.twill.filesystem.Location;\n+import org.apache.twill.filesystem.LocationFactory;\n+import org.apache.twill.internal.ApplicationBundler;\n+import org.apache.twill.internal.appmaster.ApplicationMasterMain;\n+import org.apache.twill.internal.container.TwillContainerMain;\n+import org.apache.twill.internal.utils.Dependencies;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URL;\n+import java.util.jar.JarEntry;\n+import java.util.jar.JarOutputStream;\n+\n+/**\n+ * Util class to build jar files needed by {@code DataprocRuntimeJobManager}.\n+ */\n+final class DataprocJarUtil {\n+\n+  /**\n+   * Returns twill bundle jar.\n+   *\n+   * @param locationFactory location factory to create location\n+   * @return a runtime jar file\n+   * @throws IOException any error while building the jar\n+   */\n+  static RuntimeLocalFile getTwillJar(LocationFactory locationFactory) throws IOException {\n+    ApplicationBundler bundler = new ApplicationBundler(new ClassAcceptor() {\n+      @Override\n+      public boolean accept(String className, URL classUrl, URL classPathUrl) {\n+        return !className.startsWith(\"org.apache.hadoop\") && !classPathUrl.toString().contains(\"spark-assembly\");", "originalCommit": "0c5c24829959e00be25a2b904ff7c4e00575bf2a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzU5NjYwNQ==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393596605", "bodyText": "Similar filtering is also being used in MapReduceRuntimeService\nhttps://github.com/cdapio/cdap/blob/develop/cdap-app-fabric/src/main/java/io/cdap/cdap/internal/app/runtime/batch/MapReduceRuntimeService.java#L782", "author": "CuriousVini", "createdAt": "2020-03-17T11:00:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ3NTk0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzYxOTcyMg==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393619722", "bodyText": "Since we already have a workaround, I opened a jira to investigate this further.\nhttps://issues.cask.co/browse/CDAP-16433\nThis is the spark-assembly class that had conflict.\nCaused by: java.lang.NoSuchMethodError: org.apache.hadoop.http.HttpConfig.isSecure()Z\n\tat org.apache.hadoop.yarn.conf.YarnConfiguration.<clinit>(YarnConfiguration.java:322)\n\tat io.cdap.cdap.runtime.spi.runtimejob.DataprocRuntimeEnvironment.initialize(DataprocRuntimeEnvironment.java:87)\n\t... 10 more", "author": "CuriousVini", "createdAt": "2020-03-17T11:44:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ3NTk0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ3NzEzMg==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393477132", "bodyText": "Better use Objects.equals", "author": "chtyim", "createdAt": "2020-03-17T07:01:45Z", "path": "cdap-runtime-spi/src/main/java/io/cdap/cdap/runtime/spi/runtimejob/RuntimeLocalFile.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.runtime.spi.runtimejob;\n+\n+import java.net.URI;\n+import java.util.Objects;\n+\n+/**\n+ * Represents runtime local files.\n+ */\n+public class RuntimeLocalFile {\n+  private final String name;\n+  private final URI fileUri;\n+  private final boolean isArchive;\n+\n+  public RuntimeLocalFile(String name, URI fileUri, boolean isArchive) {\n+    this.name = name;\n+    this.fileUri = fileUri;\n+    this.isArchive = isArchive;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public URI getFileUri() {\n+    return fileUri;\n+  }\n+\n+  public boolean isArchive() {\n+    return isArchive;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+    RuntimeLocalFile that = (RuntimeLocalFile) o;\n+    return isArchive == that.isArchive && name.equals(that.name) && fileUri.equals(that.fileUri);", "originalCommit": "0c5c24829959e00be25a2b904ff7c4e00575bf2a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ3NzM3OA==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393477378", "bodyText": "You also need programType?", "author": "chtyim", "createdAt": "2020-03-17T07:02:30Z", "path": "cdap-runtime-spi/src/main/java/io/cdap/cdap/runtime/spi/runtimejob/ProgramRunInfo.java", "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.runtime.spi.runtimejob;\n+\n+import java.util.Objects;\n+\n+/**\n+ * Program run information.\n+ */\n+public class ProgramRunInfo {\n+  private final String namespace;\n+  private final String application;\n+  private final String program;\n+  private final String run;\n+\n+  public ProgramRunInfo(String namespace, String application, String program, String run) {", "originalCommit": "0c5c24829959e00be25a2b904ff7c4e00575bf2a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzU5OTU1NA==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393599554", "bodyText": "From the extension perspective type is not needed. But we can add it.", "author": "CuriousVini", "createdAt": "2020-03-17T11:05:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ3NzM3OA=="}], "type": "inlineReview"}, {"oid": "0877b0523ff068dd672c2daab4b5679267d8dd68", "url": "https://github.com/cdapio/cdap/commit/0877b0523ff068dd672c2daab4b5679267d8dd68", "message": "[CDAP-15633] Adding dataproc runtime manager implementation", "committedDate": "2020-03-17T10:41:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkwMDc2MA==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393900760", "bodyText": "Can you unify this and the one in DataprocClient for the endpoint setting?", "author": "chtyim", "createdAt": "2020-03-17T18:54:01Z", "path": "cdap-runtime-ext-dataproc/src/main/java/io/cdap/cdap/runtime/spi/runtimejob/DataprocRuntimeJobManager.java", "diffHunk": "@@ -0,0 +1,419 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.runtime.spi.runtimejob;\n+\n+import com.google.api.gax.core.CredentialsProvider;\n+import com.google.api.gax.core.FixedCredentialsProvider;\n+import com.google.api.gax.rpc.ApiException;\n+import com.google.api.gax.rpc.StatusCode;\n+import com.google.auth.oauth2.GoogleCredentials;\n+import com.google.cloud.WriteChannel;\n+import com.google.cloud.dataproc.v1.GetJobRequest;\n+import com.google.cloud.dataproc.v1.HadoopJob;\n+import com.google.cloud.dataproc.v1.Job;\n+import com.google.cloud.dataproc.v1.JobControllerClient;\n+import com.google.cloud.dataproc.v1.JobControllerSettings;\n+import com.google.cloud.dataproc.v1.JobPlacement;\n+import com.google.cloud.dataproc.v1.JobReference;\n+import com.google.cloud.dataproc.v1.JobStatus;\n+import com.google.cloud.dataproc.v1.ListJobsRequest;\n+import com.google.cloud.dataproc.v1.SubmitJobRequest;\n+import com.google.cloud.storage.BlobId;\n+import com.google.cloud.storage.BlobInfo;\n+import com.google.cloud.storage.Storage;\n+import com.google.cloud.storage.StorageException;\n+import com.google.cloud.storage.StorageOptions;\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.io.ByteStreams;\n+import org.apache.twill.filesystem.LocalLocationFactory;\n+import org.apache.twill.filesystem.LocationFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.channels.Channels;\n+import java.nio.file.Files;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import javax.annotation.Nullable;\n+\n+/**\n+ * Dataproc runtime job manager. This class is responsible for launching a hadoop job on dataproc cluster and\n+ * managing it. An instance of this class is created by {@code DataprocProvisioner}.\n+ */\n+public class DataprocRuntimeJobManager implements RuntimeJobManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(DataprocRuntimeJobManager.class);\n+\n+  private static final String DATAPROC_GOOGLEAPIS = \"-dataproc.googleapis.com:443\";", "originalCommit": "0a1560eaa6478bff6d8f5e5e0405b4a2132e0c45", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkwMjE2OQ==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393902169", "bodyText": "Or the endpoint can be passed in the constructor? You can encapsulate all the information into a class DataprocClusterInfo (name, endpoint, project... etc)", "author": "chtyim", "createdAt": "2020-03-17T18:56:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkwMDc2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkwNTgzNw==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393905837", "bodyText": "Just say Launching program ....", "author": "chtyim", "createdAt": "2020-03-17T19:03:26Z", "path": "cdap-runtime-ext-dataproc/src/main/java/io/cdap/cdap/runtime/spi/runtimejob/DataprocRuntimeJobManager.java", "diffHunk": "@@ -0,0 +1,419 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.runtime.spi.runtimejob;\n+\n+import com.google.api.gax.core.CredentialsProvider;\n+import com.google.api.gax.core.FixedCredentialsProvider;\n+import com.google.api.gax.rpc.ApiException;\n+import com.google.api.gax.rpc.StatusCode;\n+import com.google.auth.oauth2.GoogleCredentials;\n+import com.google.cloud.WriteChannel;\n+import com.google.cloud.dataproc.v1.GetJobRequest;\n+import com.google.cloud.dataproc.v1.HadoopJob;\n+import com.google.cloud.dataproc.v1.Job;\n+import com.google.cloud.dataproc.v1.JobControllerClient;\n+import com.google.cloud.dataproc.v1.JobControllerSettings;\n+import com.google.cloud.dataproc.v1.JobPlacement;\n+import com.google.cloud.dataproc.v1.JobReference;\n+import com.google.cloud.dataproc.v1.JobStatus;\n+import com.google.cloud.dataproc.v1.ListJobsRequest;\n+import com.google.cloud.dataproc.v1.SubmitJobRequest;\n+import com.google.cloud.storage.BlobId;\n+import com.google.cloud.storage.BlobInfo;\n+import com.google.cloud.storage.Storage;\n+import com.google.cloud.storage.StorageException;\n+import com.google.cloud.storage.StorageOptions;\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.io.ByteStreams;\n+import org.apache.twill.filesystem.LocalLocationFactory;\n+import org.apache.twill.filesystem.LocationFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.channels.Channels;\n+import java.nio.file.Files;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import javax.annotation.Nullable;\n+\n+/**\n+ * Dataproc runtime job manager. This class is responsible for launching a hadoop job on dataproc cluster and\n+ * managing it. An instance of this class is created by {@code DataprocProvisioner}.\n+ */\n+public class DataprocRuntimeJobManager implements RuntimeJobManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(DataprocRuntimeJobManager.class);\n+\n+  private static final String DATAPROC_GOOGLEAPIS = \"-dataproc.googleapis.com:443\";\n+  private static final String CDAP_GCS_ROOT = \"cdap-job\";\n+  // dataproc job properties\n+  private static final String CDAP_RUNTIME_NAMESPACE = \"cdap.runtime.namespace\";\n+  private static final String CDAP_RUNTIME_APPLICATION = \"cdap.runtime.application\";\n+  private static final String CDAP_RUNTIME_PROGRAM = \"cdap.runtime.program\";\n+  private static final String CDAP_RUNTIME_PROGRAM_TYPE = \"cdap.runtime.program.type\";\n+  private static final String CDAP_RUNTIME_RUNID = \"cdap.runtime.runid\";\n+\n+  private final String clusterName;\n+  private final GoogleCredentials credentials;\n+  private final String projectId;\n+  private final String region;\n+  private final String bucket;\n+  private final Map<String, String> labels;\n+\n+  private Storage storageClient;\n+  private JobControllerClient jobControllerClient;\n+\n+  /**\n+   * Created by dataproc provisioner with properties that are needed by dataproc runtime job manager.\n+   *\n+   * @param clusterName name of the cluster on which job should be submitted\n+   * @param credentials google credentials\n+   * @param projectId project id\n+   * @param region region\n+   * @param bucket gcs bucket\n+   * @param labels system labels to be added on dataproc job\n+   */\n+  public DataprocRuntimeJobManager(String clusterName, GoogleCredentials credentials, String projectId, String region,\n+                                   String bucket, Map<String, String> labels) {\n+    this.clusterName = clusterName;\n+    this.credentials = credentials;\n+    this.projectId = projectId;\n+    this.region = region;\n+    this.bucket = bucket;\n+    this.labels = Collections.unmodifiableMap(new HashMap<>(labels));\n+  }\n+\n+  @Override\n+  public void initialize() throws Exception {\n+    // instantiate a gcs client\n+    this.storageClient = StorageOptions.newBuilder().setProjectId(projectId)\n+      .setCredentials(credentials).build().getService();\n+\n+    // instantiate a dataproc job controller client\n+    CredentialsProvider credentialsProvider = FixedCredentialsProvider.create(credentials);\n+    this.jobControllerClient = JobControllerClient.create(\n+      JobControllerSettings.newBuilder().setCredentialsProvider(credentialsProvider)\n+        .setEndpoint(region + DATAPROC_GOOGLEAPIS).build());\n+  }\n+\n+  @Override\n+  public RuntimeJobId launch(RuntimeJobInfo runtimeJobInfo) throws Exception {\n+    ProgramRunInfo runInfo = runtimeJobInfo.getProgramRunInfo();\n+    LOG.info(\"Starting to launch program run {} with following configurations: cluster {}, project {}, region {}, \" +", "originalCommit": "0a1560eaa6478bff6d8f5e5e0405b4a2132e0c45", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkwNzE0OA==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393907148", "bodyText": "Add a TODO to actually use a fixed directory and caching the twill/application/artifact jars, since they won't change if it is the same app.", "author": "chtyim", "createdAt": "2020-03-17T19:05:56Z", "path": "cdap-runtime-ext-dataproc/src/main/java/io/cdap/cdap/runtime/spi/runtimejob/DataprocRuntimeJobManager.java", "diffHunk": "@@ -0,0 +1,419 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.runtime.spi.runtimejob;\n+\n+import com.google.api.gax.core.CredentialsProvider;\n+import com.google.api.gax.core.FixedCredentialsProvider;\n+import com.google.api.gax.rpc.ApiException;\n+import com.google.api.gax.rpc.StatusCode;\n+import com.google.auth.oauth2.GoogleCredentials;\n+import com.google.cloud.WriteChannel;\n+import com.google.cloud.dataproc.v1.GetJobRequest;\n+import com.google.cloud.dataproc.v1.HadoopJob;\n+import com.google.cloud.dataproc.v1.Job;\n+import com.google.cloud.dataproc.v1.JobControllerClient;\n+import com.google.cloud.dataproc.v1.JobControllerSettings;\n+import com.google.cloud.dataproc.v1.JobPlacement;\n+import com.google.cloud.dataproc.v1.JobReference;\n+import com.google.cloud.dataproc.v1.JobStatus;\n+import com.google.cloud.dataproc.v1.ListJobsRequest;\n+import com.google.cloud.dataproc.v1.SubmitJobRequest;\n+import com.google.cloud.storage.BlobId;\n+import com.google.cloud.storage.BlobInfo;\n+import com.google.cloud.storage.Storage;\n+import com.google.cloud.storage.StorageException;\n+import com.google.cloud.storage.StorageOptions;\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.io.ByteStreams;\n+import org.apache.twill.filesystem.LocalLocationFactory;\n+import org.apache.twill.filesystem.LocationFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.channels.Channels;\n+import java.nio.file.Files;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import javax.annotation.Nullable;\n+\n+/**\n+ * Dataproc runtime job manager. This class is responsible for launching a hadoop job on dataproc cluster and\n+ * managing it. An instance of this class is created by {@code DataprocProvisioner}.\n+ */\n+public class DataprocRuntimeJobManager implements RuntimeJobManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(DataprocRuntimeJobManager.class);\n+\n+  private static final String DATAPROC_GOOGLEAPIS = \"-dataproc.googleapis.com:443\";\n+  private static final String CDAP_GCS_ROOT = \"cdap-job\";\n+  // dataproc job properties\n+  private static final String CDAP_RUNTIME_NAMESPACE = \"cdap.runtime.namespace\";\n+  private static final String CDAP_RUNTIME_APPLICATION = \"cdap.runtime.application\";\n+  private static final String CDAP_RUNTIME_PROGRAM = \"cdap.runtime.program\";\n+  private static final String CDAP_RUNTIME_PROGRAM_TYPE = \"cdap.runtime.program.type\";\n+  private static final String CDAP_RUNTIME_RUNID = \"cdap.runtime.runid\";\n+\n+  private final String clusterName;\n+  private final GoogleCredentials credentials;\n+  private final String projectId;\n+  private final String region;\n+  private final String bucket;\n+  private final Map<String, String> labels;\n+\n+  private Storage storageClient;\n+  private JobControllerClient jobControllerClient;\n+\n+  /**\n+   * Created by dataproc provisioner with properties that are needed by dataproc runtime job manager.\n+   *\n+   * @param clusterName name of the cluster on which job should be submitted\n+   * @param credentials google credentials\n+   * @param projectId project id\n+   * @param region region\n+   * @param bucket gcs bucket\n+   * @param labels system labels to be added on dataproc job\n+   */\n+  public DataprocRuntimeJobManager(String clusterName, GoogleCredentials credentials, String projectId, String region,\n+                                   String bucket, Map<String, String> labels) {\n+    this.clusterName = clusterName;\n+    this.credentials = credentials;\n+    this.projectId = projectId;\n+    this.region = region;\n+    this.bucket = bucket;\n+    this.labels = Collections.unmodifiableMap(new HashMap<>(labels));\n+  }\n+\n+  @Override\n+  public void initialize() throws Exception {\n+    // instantiate a gcs client\n+    this.storageClient = StorageOptions.newBuilder().setProjectId(projectId)\n+      .setCredentials(credentials).build().getService();\n+\n+    // instantiate a dataproc job controller client\n+    CredentialsProvider credentialsProvider = FixedCredentialsProvider.create(credentials);\n+    this.jobControllerClient = JobControllerClient.create(\n+      JobControllerSettings.newBuilder().setCredentialsProvider(credentialsProvider)\n+        .setEndpoint(region + DATAPROC_GOOGLEAPIS).build());\n+  }\n+\n+  @Override\n+  public RuntimeJobId launch(RuntimeJobInfo runtimeJobInfo) throws Exception {\n+    ProgramRunInfo runInfo = runtimeJobInfo.getProgramRunInfo();\n+    LOG.info(\"Starting to launch program run {} with following configurations: cluster {}, project {}, region {}, \" +\n+               \"bucket {}.\", runInfo.getRun(), clusterName, projectId, region, bucket);\n+\n+    File tempDir = Files.createTempDirectory(\"dataproc.launcher\").toFile();", "originalCommit": "0a1560eaa6478bff6d8f5e5e0405b4a2132e0c45", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDEwNDQ2NA==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r394104464", "bodyText": "Do you mean caching on local directory only right? what about gcs?", "author": "CuriousVini", "createdAt": "2020-03-18T04:46:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkwNzE0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDUxOTU0NQ==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r394519545", "bodyText": "Both. Caching on local disk saves the time to rebuild the jar. Caching on GCS saves the upload time.", "author": "chtyim", "createdAt": "2020-03-18T17:27:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkwNzE0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkwNzYwNg==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393907606", "bodyText": "debug", "author": "chtyim", "createdAt": "2020-03-17T19:06:48Z", "path": "cdap-runtime-ext-dataproc/src/main/java/io/cdap/cdap/runtime/spi/runtimejob/DataprocRuntimeJobManager.java", "diffHunk": "@@ -0,0 +1,419 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.runtime.spi.runtimejob;\n+\n+import com.google.api.gax.core.CredentialsProvider;\n+import com.google.api.gax.core.FixedCredentialsProvider;\n+import com.google.api.gax.rpc.ApiException;\n+import com.google.api.gax.rpc.StatusCode;\n+import com.google.auth.oauth2.GoogleCredentials;\n+import com.google.cloud.WriteChannel;\n+import com.google.cloud.dataproc.v1.GetJobRequest;\n+import com.google.cloud.dataproc.v1.HadoopJob;\n+import com.google.cloud.dataproc.v1.Job;\n+import com.google.cloud.dataproc.v1.JobControllerClient;\n+import com.google.cloud.dataproc.v1.JobControllerSettings;\n+import com.google.cloud.dataproc.v1.JobPlacement;\n+import com.google.cloud.dataproc.v1.JobReference;\n+import com.google.cloud.dataproc.v1.JobStatus;\n+import com.google.cloud.dataproc.v1.ListJobsRequest;\n+import com.google.cloud.dataproc.v1.SubmitJobRequest;\n+import com.google.cloud.storage.BlobId;\n+import com.google.cloud.storage.BlobInfo;\n+import com.google.cloud.storage.Storage;\n+import com.google.cloud.storage.StorageException;\n+import com.google.cloud.storage.StorageOptions;\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.io.ByteStreams;\n+import org.apache.twill.filesystem.LocalLocationFactory;\n+import org.apache.twill.filesystem.LocationFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.channels.Channels;\n+import java.nio.file.Files;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import javax.annotation.Nullable;\n+\n+/**\n+ * Dataproc runtime job manager. This class is responsible for launching a hadoop job on dataproc cluster and\n+ * managing it. An instance of this class is created by {@code DataprocProvisioner}.\n+ */\n+public class DataprocRuntimeJobManager implements RuntimeJobManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(DataprocRuntimeJobManager.class);\n+\n+  private static final String DATAPROC_GOOGLEAPIS = \"-dataproc.googleapis.com:443\";\n+  private static final String CDAP_GCS_ROOT = \"cdap-job\";\n+  // dataproc job properties\n+  private static final String CDAP_RUNTIME_NAMESPACE = \"cdap.runtime.namespace\";\n+  private static final String CDAP_RUNTIME_APPLICATION = \"cdap.runtime.application\";\n+  private static final String CDAP_RUNTIME_PROGRAM = \"cdap.runtime.program\";\n+  private static final String CDAP_RUNTIME_PROGRAM_TYPE = \"cdap.runtime.program.type\";\n+  private static final String CDAP_RUNTIME_RUNID = \"cdap.runtime.runid\";\n+\n+  private final String clusterName;\n+  private final GoogleCredentials credentials;\n+  private final String projectId;\n+  private final String region;\n+  private final String bucket;\n+  private final Map<String, String> labels;\n+\n+  private Storage storageClient;\n+  private JobControllerClient jobControllerClient;\n+\n+  /**\n+   * Created by dataproc provisioner with properties that are needed by dataproc runtime job manager.\n+   *\n+   * @param clusterName name of the cluster on which job should be submitted\n+   * @param credentials google credentials\n+   * @param projectId project id\n+   * @param region region\n+   * @param bucket gcs bucket\n+   * @param labels system labels to be added on dataproc job\n+   */\n+  public DataprocRuntimeJobManager(String clusterName, GoogleCredentials credentials, String projectId, String region,\n+                                   String bucket, Map<String, String> labels) {\n+    this.clusterName = clusterName;\n+    this.credentials = credentials;\n+    this.projectId = projectId;\n+    this.region = region;\n+    this.bucket = bucket;\n+    this.labels = Collections.unmodifiableMap(new HashMap<>(labels));\n+  }\n+\n+  @Override\n+  public void initialize() throws Exception {\n+    // instantiate a gcs client\n+    this.storageClient = StorageOptions.newBuilder().setProjectId(projectId)\n+      .setCredentials(credentials).build().getService();\n+\n+    // instantiate a dataproc job controller client\n+    CredentialsProvider credentialsProvider = FixedCredentialsProvider.create(credentials);\n+    this.jobControllerClient = JobControllerClient.create(\n+      JobControllerSettings.newBuilder().setCredentialsProvider(credentialsProvider)\n+        .setEndpoint(region + DATAPROC_GOOGLEAPIS).build());\n+  }\n+\n+  @Override\n+  public RuntimeJobId launch(RuntimeJobInfo runtimeJobInfo) throws Exception {\n+    ProgramRunInfo runInfo = runtimeJobInfo.getProgramRunInfo();\n+    LOG.info(\"Starting to launch program run {} with following configurations: cluster {}, project {}, region {}, \" +\n+               \"bucket {}.\", runInfo.getRun(), clusterName, projectId, region, bucket);\n+\n+    File tempDir = Files.createTempDirectory(\"dataproc.launcher\").toFile();\n+    // on dataproc bucket the run root will be <bucket>/cdap-job/<runid>/. All the files for this run will be copied\n+    // under that base dir.\n+    String runRootPath = getPath(CDAP_GCS_ROOT, runInfo.getRun());\n+    try {\n+      // step 1: build twill.jar and launcher.jar and add them to files to be copied to gcs\n+      List<RuntimeLocalFile> localFiles = getRuntimeLocalFiles(runtimeJobInfo.getLocalizeFiles(), tempDir);\n+\n+      // step 2: upload all the necessary files to gcs so that those files are available to dataproc job\n+      for (RuntimeLocalFile fileToUpload : localFiles) {\n+        String targetFilePath = getPath(runRootPath, fileToUpload.getName());\n+        LOG.debug(\"Uploading file {} to gcs bucket {}.\", targetFilePath, bucket);\n+        uploadFile(targetFilePath, fileToUpload);\n+        LOG.info(\"Uploaded file {} to gcs bucket {}.\", targetFilePath, bucket);", "originalCommit": "0a1560eaa6478bff6d8f5e5e0405b4a2132e0c45", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkwOTEwNg==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393909106", "bodyText": "No need to have this. Just return in place.\ntry {\n...\nreturn Optional.of(new RuntimeJobDetail(...));\n} catch (ApiException e) {\n...\n}\nreturn Optional.empty();", "author": "chtyim", "createdAt": "2020-03-17T19:09:42Z", "path": "cdap-runtime-ext-dataproc/src/main/java/io/cdap/cdap/runtime/spi/runtimejob/DataprocRuntimeJobManager.java", "diffHunk": "@@ -0,0 +1,419 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.runtime.spi.runtimejob;\n+\n+import com.google.api.gax.core.CredentialsProvider;\n+import com.google.api.gax.core.FixedCredentialsProvider;\n+import com.google.api.gax.rpc.ApiException;\n+import com.google.api.gax.rpc.StatusCode;\n+import com.google.auth.oauth2.GoogleCredentials;\n+import com.google.cloud.WriteChannel;\n+import com.google.cloud.dataproc.v1.GetJobRequest;\n+import com.google.cloud.dataproc.v1.HadoopJob;\n+import com.google.cloud.dataproc.v1.Job;\n+import com.google.cloud.dataproc.v1.JobControllerClient;\n+import com.google.cloud.dataproc.v1.JobControllerSettings;\n+import com.google.cloud.dataproc.v1.JobPlacement;\n+import com.google.cloud.dataproc.v1.JobReference;\n+import com.google.cloud.dataproc.v1.JobStatus;\n+import com.google.cloud.dataproc.v1.ListJobsRequest;\n+import com.google.cloud.dataproc.v1.SubmitJobRequest;\n+import com.google.cloud.storage.BlobId;\n+import com.google.cloud.storage.BlobInfo;\n+import com.google.cloud.storage.Storage;\n+import com.google.cloud.storage.StorageException;\n+import com.google.cloud.storage.StorageOptions;\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.io.ByteStreams;\n+import org.apache.twill.filesystem.LocalLocationFactory;\n+import org.apache.twill.filesystem.LocationFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.channels.Channels;\n+import java.nio.file.Files;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import javax.annotation.Nullable;\n+\n+/**\n+ * Dataproc runtime job manager. This class is responsible for launching a hadoop job on dataproc cluster and\n+ * managing it. An instance of this class is created by {@code DataprocProvisioner}.\n+ */\n+public class DataprocRuntimeJobManager implements RuntimeJobManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(DataprocRuntimeJobManager.class);\n+\n+  private static final String DATAPROC_GOOGLEAPIS = \"-dataproc.googleapis.com:443\";\n+  private static final String CDAP_GCS_ROOT = \"cdap-job\";\n+  // dataproc job properties\n+  private static final String CDAP_RUNTIME_NAMESPACE = \"cdap.runtime.namespace\";\n+  private static final String CDAP_RUNTIME_APPLICATION = \"cdap.runtime.application\";\n+  private static final String CDAP_RUNTIME_PROGRAM = \"cdap.runtime.program\";\n+  private static final String CDAP_RUNTIME_PROGRAM_TYPE = \"cdap.runtime.program.type\";\n+  private static final String CDAP_RUNTIME_RUNID = \"cdap.runtime.runid\";\n+\n+  private final String clusterName;\n+  private final GoogleCredentials credentials;\n+  private final String projectId;\n+  private final String region;\n+  private final String bucket;\n+  private final Map<String, String> labels;\n+\n+  private Storage storageClient;\n+  private JobControllerClient jobControllerClient;\n+\n+  /**\n+   * Created by dataproc provisioner with properties that are needed by dataproc runtime job manager.\n+   *\n+   * @param clusterName name of the cluster on which job should be submitted\n+   * @param credentials google credentials\n+   * @param projectId project id\n+   * @param region region\n+   * @param bucket gcs bucket\n+   * @param labels system labels to be added on dataproc job\n+   */\n+  public DataprocRuntimeJobManager(String clusterName, GoogleCredentials credentials, String projectId, String region,\n+                                   String bucket, Map<String, String> labels) {\n+    this.clusterName = clusterName;\n+    this.credentials = credentials;\n+    this.projectId = projectId;\n+    this.region = region;\n+    this.bucket = bucket;\n+    this.labels = Collections.unmodifiableMap(new HashMap<>(labels));\n+  }\n+\n+  @Override\n+  public void initialize() throws Exception {\n+    // instantiate a gcs client\n+    this.storageClient = StorageOptions.newBuilder().setProjectId(projectId)\n+      .setCredentials(credentials).build().getService();\n+\n+    // instantiate a dataproc job controller client\n+    CredentialsProvider credentialsProvider = FixedCredentialsProvider.create(credentials);\n+    this.jobControllerClient = JobControllerClient.create(\n+      JobControllerSettings.newBuilder().setCredentialsProvider(credentialsProvider)\n+        .setEndpoint(region + DATAPROC_GOOGLEAPIS).build());\n+  }\n+\n+  @Override\n+  public RuntimeJobId launch(RuntimeJobInfo runtimeJobInfo) throws Exception {\n+    ProgramRunInfo runInfo = runtimeJobInfo.getProgramRunInfo();\n+    LOG.info(\"Starting to launch program run {} with following configurations: cluster {}, project {}, region {}, \" +\n+               \"bucket {}.\", runInfo.getRun(), clusterName, projectId, region, bucket);\n+\n+    File tempDir = Files.createTempDirectory(\"dataproc.launcher\").toFile();\n+    // on dataproc bucket the run root will be <bucket>/cdap-job/<runid>/. All the files for this run will be copied\n+    // under that base dir.\n+    String runRootPath = getPath(CDAP_GCS_ROOT, runInfo.getRun());\n+    try {\n+      // step 1: build twill.jar and launcher.jar and add them to files to be copied to gcs\n+      List<RuntimeLocalFile> localFiles = getRuntimeLocalFiles(runtimeJobInfo.getLocalizeFiles(), tempDir);\n+\n+      // step 2: upload all the necessary files to gcs so that those files are available to dataproc job\n+      for (RuntimeLocalFile fileToUpload : localFiles) {\n+        String targetFilePath = getPath(runRootPath, fileToUpload.getName());\n+        LOG.debug(\"Uploading file {} to gcs bucket {}.\", targetFilePath, bucket);\n+        uploadFile(targetFilePath, fileToUpload);\n+        LOG.info(\"Uploaded file {} to gcs bucket {}.\", targetFilePath, bucket);\n+      }\n+\n+      // step 3: build the hadoop job request to be submitted to dataproc\n+      SubmitJobRequest request = getSubmitJobRequest(runtimeJobInfo.getRuntimeJobClass().getName(),\n+                                                     runInfo, localFiles);\n+\n+      // step 4: submit hadoop job to dataproc\n+      LOG.info(\"Submitting hadoop job {} to cluster {}.\", request.getJob().getReference().getJobId(), clusterName);\n+      Job job = jobControllerClient.submitJob(request);\n+      LOG.info(\"Successfully submitted hadoop job {} to cluster {}.\", job.getReference().getJobId(), clusterName);\n+\n+      return new RuntimeJobId(job.getReference().getJobId());\n+    } catch (Exception e) {\n+      // delete all uploaded gcs files in case of exception\n+      deleteGCSPath(runRootPath);\n+      throw new Exception(String.format(\"Error while launching job %s on cluster %s\",\n+                                        runInfo.getRun(), clusterName), e);\n+    } finally {\n+      // delete local temp directory\n+      deleteDirectoryContents(tempDir);\n+    }\n+  }\n+\n+  @Override\n+  public Optional<RuntimeJobDetail> getDetail(RuntimeJobId runtimeJobId) throws Exception {\n+    String jobId = runtimeJobId.getRuntimeJobId();\n+\n+    Optional<RuntimeJobDetail> jobDetails = Optional.empty();", "originalCommit": "0a1560eaa6478bff6d8f5e5e0405b4a2132e0c45", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkxODg3NQ==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393918875", "bodyText": "You can use foreach loop over Iterable.\nfor (Job job : listJobsPagedResponse.iterateAll()) {\n  ...\n}", "author": "chtyim", "createdAt": "2020-03-17T19:28:51Z", "path": "cdap-runtime-ext-dataproc/src/main/java/io/cdap/cdap/runtime/spi/runtimejob/DataprocRuntimeJobManager.java", "diffHunk": "@@ -0,0 +1,419 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.runtime.spi.runtimejob;\n+\n+import com.google.api.gax.core.CredentialsProvider;\n+import com.google.api.gax.core.FixedCredentialsProvider;\n+import com.google.api.gax.rpc.ApiException;\n+import com.google.api.gax.rpc.StatusCode;\n+import com.google.auth.oauth2.GoogleCredentials;\n+import com.google.cloud.WriteChannel;\n+import com.google.cloud.dataproc.v1.GetJobRequest;\n+import com.google.cloud.dataproc.v1.HadoopJob;\n+import com.google.cloud.dataproc.v1.Job;\n+import com.google.cloud.dataproc.v1.JobControllerClient;\n+import com.google.cloud.dataproc.v1.JobControllerSettings;\n+import com.google.cloud.dataproc.v1.JobPlacement;\n+import com.google.cloud.dataproc.v1.JobReference;\n+import com.google.cloud.dataproc.v1.JobStatus;\n+import com.google.cloud.dataproc.v1.ListJobsRequest;\n+import com.google.cloud.dataproc.v1.SubmitJobRequest;\n+import com.google.cloud.storage.BlobId;\n+import com.google.cloud.storage.BlobInfo;\n+import com.google.cloud.storage.Storage;\n+import com.google.cloud.storage.StorageException;\n+import com.google.cloud.storage.StorageOptions;\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.io.ByteStreams;\n+import org.apache.twill.filesystem.LocalLocationFactory;\n+import org.apache.twill.filesystem.LocationFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.channels.Channels;\n+import java.nio.file.Files;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import javax.annotation.Nullable;\n+\n+/**\n+ * Dataproc runtime job manager. This class is responsible for launching a hadoop job on dataproc cluster and\n+ * managing it. An instance of this class is created by {@code DataprocProvisioner}.\n+ */\n+public class DataprocRuntimeJobManager implements RuntimeJobManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(DataprocRuntimeJobManager.class);\n+\n+  private static final String DATAPROC_GOOGLEAPIS = \"-dataproc.googleapis.com:443\";\n+  private static final String CDAP_GCS_ROOT = \"cdap-job\";\n+  // dataproc job properties\n+  private static final String CDAP_RUNTIME_NAMESPACE = \"cdap.runtime.namespace\";\n+  private static final String CDAP_RUNTIME_APPLICATION = \"cdap.runtime.application\";\n+  private static final String CDAP_RUNTIME_PROGRAM = \"cdap.runtime.program\";\n+  private static final String CDAP_RUNTIME_PROGRAM_TYPE = \"cdap.runtime.program.type\";\n+  private static final String CDAP_RUNTIME_RUNID = \"cdap.runtime.runid\";\n+\n+  private final String clusterName;\n+  private final GoogleCredentials credentials;\n+  private final String projectId;\n+  private final String region;\n+  private final String bucket;\n+  private final Map<String, String> labels;\n+\n+  private Storage storageClient;\n+  private JobControllerClient jobControllerClient;\n+\n+  /**\n+   * Created by dataproc provisioner with properties that are needed by dataproc runtime job manager.\n+   *\n+   * @param clusterName name of the cluster on which job should be submitted\n+   * @param credentials google credentials\n+   * @param projectId project id\n+   * @param region region\n+   * @param bucket gcs bucket\n+   * @param labels system labels to be added on dataproc job\n+   */\n+  public DataprocRuntimeJobManager(String clusterName, GoogleCredentials credentials, String projectId, String region,\n+                                   String bucket, Map<String, String> labels) {\n+    this.clusterName = clusterName;\n+    this.credentials = credentials;\n+    this.projectId = projectId;\n+    this.region = region;\n+    this.bucket = bucket;\n+    this.labels = Collections.unmodifiableMap(new HashMap<>(labels));\n+  }\n+\n+  @Override\n+  public void initialize() throws Exception {\n+    // instantiate a gcs client\n+    this.storageClient = StorageOptions.newBuilder().setProjectId(projectId)\n+      .setCredentials(credentials).build().getService();\n+\n+    // instantiate a dataproc job controller client\n+    CredentialsProvider credentialsProvider = FixedCredentialsProvider.create(credentials);\n+    this.jobControllerClient = JobControllerClient.create(\n+      JobControllerSettings.newBuilder().setCredentialsProvider(credentialsProvider)\n+        .setEndpoint(region + DATAPROC_GOOGLEAPIS).build());\n+  }\n+\n+  @Override\n+  public RuntimeJobId launch(RuntimeJobInfo runtimeJobInfo) throws Exception {\n+    ProgramRunInfo runInfo = runtimeJobInfo.getProgramRunInfo();\n+    LOG.info(\"Starting to launch program run {} with following configurations: cluster {}, project {}, region {}, \" +\n+               \"bucket {}.\", runInfo.getRun(), clusterName, projectId, region, bucket);\n+\n+    File tempDir = Files.createTempDirectory(\"dataproc.launcher\").toFile();\n+    // on dataproc bucket the run root will be <bucket>/cdap-job/<runid>/. All the files for this run will be copied\n+    // under that base dir.\n+    String runRootPath = getPath(CDAP_GCS_ROOT, runInfo.getRun());\n+    try {\n+      // step 1: build twill.jar and launcher.jar and add them to files to be copied to gcs\n+      List<RuntimeLocalFile> localFiles = getRuntimeLocalFiles(runtimeJobInfo.getLocalizeFiles(), tempDir);\n+\n+      // step 2: upload all the necessary files to gcs so that those files are available to dataproc job\n+      for (RuntimeLocalFile fileToUpload : localFiles) {\n+        String targetFilePath = getPath(runRootPath, fileToUpload.getName());\n+        LOG.debug(\"Uploading file {} to gcs bucket {}.\", targetFilePath, bucket);\n+        uploadFile(targetFilePath, fileToUpload);\n+        LOG.info(\"Uploaded file {} to gcs bucket {}.\", targetFilePath, bucket);\n+      }\n+\n+      // step 3: build the hadoop job request to be submitted to dataproc\n+      SubmitJobRequest request = getSubmitJobRequest(runtimeJobInfo.getRuntimeJobClass().getName(),\n+                                                     runInfo, localFiles);\n+\n+      // step 4: submit hadoop job to dataproc\n+      LOG.info(\"Submitting hadoop job {} to cluster {}.\", request.getJob().getReference().getJobId(), clusterName);\n+      Job job = jobControllerClient.submitJob(request);\n+      LOG.info(\"Successfully submitted hadoop job {} to cluster {}.\", job.getReference().getJobId(), clusterName);\n+\n+      return new RuntimeJobId(job.getReference().getJobId());\n+    } catch (Exception e) {\n+      // delete all uploaded gcs files in case of exception\n+      deleteGCSPath(runRootPath);\n+      throw new Exception(String.format(\"Error while launching job %s on cluster %s\",\n+                                        runInfo.getRun(), clusterName), e);\n+    } finally {\n+      // delete local temp directory\n+      deleteDirectoryContents(tempDir);\n+    }\n+  }\n+\n+  @Override\n+  public Optional<RuntimeJobDetail> getDetail(RuntimeJobId runtimeJobId) throws Exception {\n+    String jobId = runtimeJobId.getRuntimeJobId();\n+\n+    Optional<RuntimeJobDetail> jobDetails = Optional.empty();\n+    try {\n+      LOG.info(\"Getting job details for {} under project {}, region {}.\", projectId, region, jobId);\n+      Job job = jobControllerClient.getJob(GetJobRequest.newBuilder()\n+                                             .setProjectId(projectId)\n+                                             .setRegion(region)\n+                                             .setJobId(jobId)\n+                                             .build());\n+      RuntimeJobStatus runtimeJobStatus = getRuntimeJobStatus(job);\n+\n+      jobDetails = Optional.of(new RuntimeJobDetail(runtimeJobId, runtimeJobStatus));\n+    } catch (ApiException e) {\n+      // this may happen if job is manually deleted by user\n+      if (e.getStatusCode().getCode() == StatusCode.Code.NOT_FOUND) {\n+        LOG.warn(\"Dataproc job {} does not exist in project {}, region {}.\", jobId, projectId, region);\n+      } else {\n+        throw new Exception(String.format(\"Error while getting details for job %s on cluster %s.\",\n+                                          jobId, clusterName), e);\n+      }\n+    }\n+    return jobDetails;\n+  }\n+\n+  @Override\n+  public List<RuntimeJobDetail> list() throws Exception {\n+    Set<String> filters = new HashSet<>();\n+    // Dataproc jobs can be filtered by status.state filter. In this case we only want ACTIVE jobs.\n+    filters.add(\"status.state=ACTIVE\");\n+    // Filter by labels that were added to the job when this runtime job manager submitted dataproc job. Note that\n+    // dataproc only supports AND filter.\n+    for (Map.Entry<String, String> entry : labels.entrySet()) {\n+      filters.add(\"labels.\" + entry.getKey() + \"=\" + entry.getValue());\n+    }\n+    String jobFilter = Joiner.on(\" AND \").join(filters);\n+\n+    LOG.info(\"Getting a list of jobs under project {}, region {}, cluster {} with filter {}.\", projectId, region,\n+             clusterName, jobFilter);\n+    JobControllerClient.ListJobsPagedResponse listJobsPagedResponse =\n+      jobControllerClient.listJobs(ListJobsRequest.newBuilder()\n+                                     .setProjectId(projectId).setRegion(region).setClusterName(clusterName)\n+                                     .setFilter(jobFilter).build());\n+\n+    Iterator<Job> jobsItor = listJobsPagedResponse.iterateAll().iterator();", "originalCommit": "0a1560eaa6478bff6d8f5e5e0405b4a2132e0c45", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkxOTM5Ng==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393919396", "bodyText": "Why the deletion on happen on stop call? Do we delete when the job completed by itself?", "author": "chtyim", "createdAt": "2020-03-17T19:29:55Z", "path": "cdap-runtime-ext-dataproc/src/main/java/io/cdap/cdap/runtime/spi/runtimejob/DataprocRuntimeJobManager.java", "diffHunk": "@@ -0,0 +1,419 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.runtime.spi.runtimejob;\n+\n+import com.google.api.gax.core.CredentialsProvider;\n+import com.google.api.gax.core.FixedCredentialsProvider;\n+import com.google.api.gax.rpc.ApiException;\n+import com.google.api.gax.rpc.StatusCode;\n+import com.google.auth.oauth2.GoogleCredentials;\n+import com.google.cloud.WriteChannel;\n+import com.google.cloud.dataproc.v1.GetJobRequest;\n+import com.google.cloud.dataproc.v1.HadoopJob;\n+import com.google.cloud.dataproc.v1.Job;\n+import com.google.cloud.dataproc.v1.JobControllerClient;\n+import com.google.cloud.dataproc.v1.JobControllerSettings;\n+import com.google.cloud.dataproc.v1.JobPlacement;\n+import com.google.cloud.dataproc.v1.JobReference;\n+import com.google.cloud.dataproc.v1.JobStatus;\n+import com.google.cloud.dataproc.v1.ListJobsRequest;\n+import com.google.cloud.dataproc.v1.SubmitJobRequest;\n+import com.google.cloud.storage.BlobId;\n+import com.google.cloud.storage.BlobInfo;\n+import com.google.cloud.storage.Storage;\n+import com.google.cloud.storage.StorageException;\n+import com.google.cloud.storage.StorageOptions;\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.io.ByteStreams;\n+import org.apache.twill.filesystem.LocalLocationFactory;\n+import org.apache.twill.filesystem.LocationFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.channels.Channels;\n+import java.nio.file.Files;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import javax.annotation.Nullable;\n+\n+/**\n+ * Dataproc runtime job manager. This class is responsible for launching a hadoop job on dataproc cluster and\n+ * managing it. An instance of this class is created by {@code DataprocProvisioner}.\n+ */\n+public class DataprocRuntimeJobManager implements RuntimeJobManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(DataprocRuntimeJobManager.class);\n+\n+  private static final String DATAPROC_GOOGLEAPIS = \"-dataproc.googleapis.com:443\";\n+  private static final String CDAP_GCS_ROOT = \"cdap-job\";\n+  // dataproc job properties\n+  private static final String CDAP_RUNTIME_NAMESPACE = \"cdap.runtime.namespace\";\n+  private static final String CDAP_RUNTIME_APPLICATION = \"cdap.runtime.application\";\n+  private static final String CDAP_RUNTIME_PROGRAM = \"cdap.runtime.program\";\n+  private static final String CDAP_RUNTIME_PROGRAM_TYPE = \"cdap.runtime.program.type\";\n+  private static final String CDAP_RUNTIME_RUNID = \"cdap.runtime.runid\";\n+\n+  private final String clusterName;\n+  private final GoogleCredentials credentials;\n+  private final String projectId;\n+  private final String region;\n+  private final String bucket;\n+  private final Map<String, String> labels;\n+\n+  private Storage storageClient;\n+  private JobControllerClient jobControllerClient;\n+\n+  /**\n+   * Created by dataproc provisioner with properties that are needed by dataproc runtime job manager.\n+   *\n+   * @param clusterName name of the cluster on which job should be submitted\n+   * @param credentials google credentials\n+   * @param projectId project id\n+   * @param region region\n+   * @param bucket gcs bucket\n+   * @param labels system labels to be added on dataproc job\n+   */\n+  public DataprocRuntimeJobManager(String clusterName, GoogleCredentials credentials, String projectId, String region,\n+                                   String bucket, Map<String, String> labels) {\n+    this.clusterName = clusterName;\n+    this.credentials = credentials;\n+    this.projectId = projectId;\n+    this.region = region;\n+    this.bucket = bucket;\n+    this.labels = Collections.unmodifiableMap(new HashMap<>(labels));\n+  }\n+\n+  @Override\n+  public void initialize() throws Exception {\n+    // instantiate a gcs client\n+    this.storageClient = StorageOptions.newBuilder().setProjectId(projectId)\n+      .setCredentials(credentials).build().getService();\n+\n+    // instantiate a dataproc job controller client\n+    CredentialsProvider credentialsProvider = FixedCredentialsProvider.create(credentials);\n+    this.jobControllerClient = JobControllerClient.create(\n+      JobControllerSettings.newBuilder().setCredentialsProvider(credentialsProvider)\n+        .setEndpoint(region + DATAPROC_GOOGLEAPIS).build());\n+  }\n+\n+  @Override\n+  public RuntimeJobId launch(RuntimeJobInfo runtimeJobInfo) throws Exception {\n+    ProgramRunInfo runInfo = runtimeJobInfo.getProgramRunInfo();\n+    LOG.info(\"Starting to launch program run {} with following configurations: cluster {}, project {}, region {}, \" +\n+               \"bucket {}.\", runInfo.getRun(), clusterName, projectId, region, bucket);\n+\n+    File tempDir = Files.createTempDirectory(\"dataproc.launcher\").toFile();\n+    // on dataproc bucket the run root will be <bucket>/cdap-job/<runid>/. All the files for this run will be copied\n+    // under that base dir.\n+    String runRootPath = getPath(CDAP_GCS_ROOT, runInfo.getRun());\n+    try {\n+      // step 1: build twill.jar and launcher.jar and add them to files to be copied to gcs\n+      List<RuntimeLocalFile> localFiles = getRuntimeLocalFiles(runtimeJobInfo.getLocalizeFiles(), tempDir);\n+\n+      // step 2: upload all the necessary files to gcs so that those files are available to dataproc job\n+      for (RuntimeLocalFile fileToUpload : localFiles) {\n+        String targetFilePath = getPath(runRootPath, fileToUpload.getName());\n+        LOG.debug(\"Uploading file {} to gcs bucket {}.\", targetFilePath, bucket);\n+        uploadFile(targetFilePath, fileToUpload);\n+        LOG.info(\"Uploaded file {} to gcs bucket {}.\", targetFilePath, bucket);\n+      }\n+\n+      // step 3: build the hadoop job request to be submitted to dataproc\n+      SubmitJobRequest request = getSubmitJobRequest(runtimeJobInfo.getRuntimeJobClass().getName(),\n+                                                     runInfo, localFiles);\n+\n+      // step 4: submit hadoop job to dataproc\n+      LOG.info(\"Submitting hadoop job {} to cluster {}.\", request.getJob().getReference().getJobId(), clusterName);\n+      Job job = jobControllerClient.submitJob(request);\n+      LOG.info(\"Successfully submitted hadoop job {} to cluster {}.\", job.getReference().getJobId(), clusterName);\n+\n+      return new RuntimeJobId(job.getReference().getJobId());\n+    } catch (Exception e) {\n+      // delete all uploaded gcs files in case of exception\n+      deleteGCSPath(runRootPath);\n+      throw new Exception(String.format(\"Error while launching job %s on cluster %s\",\n+                                        runInfo.getRun(), clusterName), e);\n+    } finally {\n+      // delete local temp directory\n+      deleteDirectoryContents(tempDir);\n+    }\n+  }\n+\n+  @Override\n+  public Optional<RuntimeJobDetail> getDetail(RuntimeJobId runtimeJobId) throws Exception {\n+    String jobId = runtimeJobId.getRuntimeJobId();\n+\n+    Optional<RuntimeJobDetail> jobDetails = Optional.empty();\n+    try {\n+      LOG.info(\"Getting job details for {} under project {}, region {}.\", projectId, region, jobId);\n+      Job job = jobControllerClient.getJob(GetJobRequest.newBuilder()\n+                                             .setProjectId(projectId)\n+                                             .setRegion(region)\n+                                             .setJobId(jobId)\n+                                             .build());\n+      RuntimeJobStatus runtimeJobStatus = getRuntimeJobStatus(job);\n+\n+      jobDetails = Optional.of(new RuntimeJobDetail(runtimeJobId, runtimeJobStatus));\n+    } catch (ApiException e) {\n+      // this may happen if job is manually deleted by user\n+      if (e.getStatusCode().getCode() == StatusCode.Code.NOT_FOUND) {\n+        LOG.warn(\"Dataproc job {} does not exist in project {}, region {}.\", jobId, projectId, region);\n+      } else {\n+        throw new Exception(String.format(\"Error while getting details for job %s on cluster %s.\",\n+                                          jobId, clusterName), e);\n+      }\n+    }\n+    return jobDetails;\n+  }\n+\n+  @Override\n+  public List<RuntimeJobDetail> list() throws Exception {\n+    Set<String> filters = new HashSet<>();\n+    // Dataproc jobs can be filtered by status.state filter. In this case we only want ACTIVE jobs.\n+    filters.add(\"status.state=ACTIVE\");\n+    // Filter by labels that were added to the job when this runtime job manager submitted dataproc job. Note that\n+    // dataproc only supports AND filter.\n+    for (Map.Entry<String, String> entry : labels.entrySet()) {\n+      filters.add(\"labels.\" + entry.getKey() + \"=\" + entry.getValue());\n+    }\n+    String jobFilter = Joiner.on(\" AND \").join(filters);\n+\n+    LOG.info(\"Getting a list of jobs under project {}, region {}, cluster {} with filter {}.\", projectId, region,\n+             clusterName, jobFilter);\n+    JobControllerClient.ListJobsPagedResponse listJobsPagedResponse =\n+      jobControllerClient.listJobs(ListJobsRequest.newBuilder()\n+                                     .setProjectId(projectId).setRegion(region).setClusterName(clusterName)\n+                                     .setFilter(jobFilter).build());\n+\n+    Iterator<Job> jobsItor = listJobsPagedResponse.iterateAll().iterator();\n+    List<RuntimeJobDetail> jobsDetail = new ArrayList<>();\n+    while (jobsItor.hasNext()) {\n+      Job job = jobsItor.next();\n+      jobsDetail.add(new RuntimeJobDetail(new RuntimeJobId(job.getReference().getJobId()), getRuntimeJobStatus(job)));\n+    }\n+    return jobsDetail;\n+  }\n+\n+  @Override\n+  public void stop(RuntimeJobId runtimeJobId) throws Exception {\n+    Optional<RuntimeJobDetail> jobDetail = getDetail(runtimeJobId);\n+    // if the job does not exist, it can be safely assume that job has been deleted. Hence has reached terminal state.\n+    if (!jobDetail.isPresent()) {\n+      return;\n+    }\n+    // stop dataproc job\n+    Job stoppedJob = stopJob(runtimeJobId);\n+\n+    // delete gcs path for the job\n+    if (stoppedJob != null) {", "originalCommit": "0a1560eaa6478bff6d8f5e5e0405b4a2132e0c45", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDA5ODkwOQ==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r394098909", "bodyText": "yea it should happen in destroy() call. Modified", "author": "CuriousVini", "createdAt": "2020-03-18T04:18:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkxOTM5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkxOTcxNg==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393919716", "bodyText": "Do you need to close the storageClient as well?", "author": "chtyim", "createdAt": "2020-03-17T19:30:41Z", "path": "cdap-runtime-ext-dataproc/src/main/java/io/cdap/cdap/runtime/spi/runtimejob/DataprocRuntimeJobManager.java", "diffHunk": "@@ -0,0 +1,419 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.runtime.spi.runtimejob;\n+\n+import com.google.api.gax.core.CredentialsProvider;\n+import com.google.api.gax.core.FixedCredentialsProvider;\n+import com.google.api.gax.rpc.ApiException;\n+import com.google.api.gax.rpc.StatusCode;\n+import com.google.auth.oauth2.GoogleCredentials;\n+import com.google.cloud.WriteChannel;\n+import com.google.cloud.dataproc.v1.GetJobRequest;\n+import com.google.cloud.dataproc.v1.HadoopJob;\n+import com.google.cloud.dataproc.v1.Job;\n+import com.google.cloud.dataproc.v1.JobControllerClient;\n+import com.google.cloud.dataproc.v1.JobControllerSettings;\n+import com.google.cloud.dataproc.v1.JobPlacement;\n+import com.google.cloud.dataproc.v1.JobReference;\n+import com.google.cloud.dataproc.v1.JobStatus;\n+import com.google.cloud.dataproc.v1.ListJobsRequest;\n+import com.google.cloud.dataproc.v1.SubmitJobRequest;\n+import com.google.cloud.storage.BlobId;\n+import com.google.cloud.storage.BlobInfo;\n+import com.google.cloud.storage.Storage;\n+import com.google.cloud.storage.StorageException;\n+import com.google.cloud.storage.StorageOptions;\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.io.ByteStreams;\n+import org.apache.twill.filesystem.LocalLocationFactory;\n+import org.apache.twill.filesystem.LocationFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.channels.Channels;\n+import java.nio.file.Files;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import javax.annotation.Nullable;\n+\n+/**\n+ * Dataproc runtime job manager. This class is responsible for launching a hadoop job on dataproc cluster and\n+ * managing it. An instance of this class is created by {@code DataprocProvisioner}.\n+ */\n+public class DataprocRuntimeJobManager implements RuntimeJobManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(DataprocRuntimeJobManager.class);\n+\n+  private static final String DATAPROC_GOOGLEAPIS = \"-dataproc.googleapis.com:443\";\n+  private static final String CDAP_GCS_ROOT = \"cdap-job\";\n+  // dataproc job properties\n+  private static final String CDAP_RUNTIME_NAMESPACE = \"cdap.runtime.namespace\";\n+  private static final String CDAP_RUNTIME_APPLICATION = \"cdap.runtime.application\";\n+  private static final String CDAP_RUNTIME_PROGRAM = \"cdap.runtime.program\";\n+  private static final String CDAP_RUNTIME_PROGRAM_TYPE = \"cdap.runtime.program.type\";\n+  private static final String CDAP_RUNTIME_RUNID = \"cdap.runtime.runid\";\n+\n+  private final String clusterName;\n+  private final GoogleCredentials credentials;\n+  private final String projectId;\n+  private final String region;\n+  private final String bucket;\n+  private final Map<String, String> labels;\n+\n+  private Storage storageClient;\n+  private JobControllerClient jobControllerClient;\n+\n+  /**\n+   * Created by dataproc provisioner with properties that are needed by dataproc runtime job manager.\n+   *\n+   * @param clusterName name of the cluster on which job should be submitted\n+   * @param credentials google credentials\n+   * @param projectId project id\n+   * @param region region\n+   * @param bucket gcs bucket\n+   * @param labels system labels to be added on dataproc job\n+   */\n+  public DataprocRuntimeJobManager(String clusterName, GoogleCredentials credentials, String projectId, String region,\n+                                   String bucket, Map<String, String> labels) {\n+    this.clusterName = clusterName;\n+    this.credentials = credentials;\n+    this.projectId = projectId;\n+    this.region = region;\n+    this.bucket = bucket;\n+    this.labels = Collections.unmodifiableMap(new HashMap<>(labels));\n+  }\n+\n+  @Override\n+  public void initialize() throws Exception {\n+    // instantiate a gcs client\n+    this.storageClient = StorageOptions.newBuilder().setProjectId(projectId)\n+      .setCredentials(credentials).build().getService();\n+\n+    // instantiate a dataproc job controller client\n+    CredentialsProvider credentialsProvider = FixedCredentialsProvider.create(credentials);\n+    this.jobControllerClient = JobControllerClient.create(\n+      JobControllerSettings.newBuilder().setCredentialsProvider(credentialsProvider)\n+        .setEndpoint(region + DATAPROC_GOOGLEAPIS).build());\n+  }\n+\n+  @Override\n+  public RuntimeJobId launch(RuntimeJobInfo runtimeJobInfo) throws Exception {\n+    ProgramRunInfo runInfo = runtimeJobInfo.getProgramRunInfo();\n+    LOG.info(\"Starting to launch program run {} with following configurations: cluster {}, project {}, region {}, \" +\n+               \"bucket {}.\", runInfo.getRun(), clusterName, projectId, region, bucket);\n+\n+    File tempDir = Files.createTempDirectory(\"dataproc.launcher\").toFile();\n+    // on dataproc bucket the run root will be <bucket>/cdap-job/<runid>/. All the files for this run will be copied\n+    // under that base dir.\n+    String runRootPath = getPath(CDAP_GCS_ROOT, runInfo.getRun());\n+    try {\n+      // step 1: build twill.jar and launcher.jar and add them to files to be copied to gcs\n+      List<RuntimeLocalFile> localFiles = getRuntimeLocalFiles(runtimeJobInfo.getLocalizeFiles(), tempDir);\n+\n+      // step 2: upload all the necessary files to gcs so that those files are available to dataproc job\n+      for (RuntimeLocalFile fileToUpload : localFiles) {\n+        String targetFilePath = getPath(runRootPath, fileToUpload.getName());\n+        LOG.debug(\"Uploading file {} to gcs bucket {}.\", targetFilePath, bucket);\n+        uploadFile(targetFilePath, fileToUpload);\n+        LOG.info(\"Uploaded file {} to gcs bucket {}.\", targetFilePath, bucket);\n+      }\n+\n+      // step 3: build the hadoop job request to be submitted to dataproc\n+      SubmitJobRequest request = getSubmitJobRequest(runtimeJobInfo.getRuntimeJobClass().getName(),\n+                                                     runInfo, localFiles);\n+\n+      // step 4: submit hadoop job to dataproc\n+      LOG.info(\"Submitting hadoop job {} to cluster {}.\", request.getJob().getReference().getJobId(), clusterName);\n+      Job job = jobControllerClient.submitJob(request);\n+      LOG.info(\"Successfully submitted hadoop job {} to cluster {}.\", job.getReference().getJobId(), clusterName);\n+\n+      return new RuntimeJobId(job.getReference().getJobId());\n+    } catch (Exception e) {\n+      // delete all uploaded gcs files in case of exception\n+      deleteGCSPath(runRootPath);\n+      throw new Exception(String.format(\"Error while launching job %s on cluster %s\",\n+                                        runInfo.getRun(), clusterName), e);\n+    } finally {\n+      // delete local temp directory\n+      deleteDirectoryContents(tempDir);\n+    }\n+  }\n+\n+  @Override\n+  public Optional<RuntimeJobDetail> getDetail(RuntimeJobId runtimeJobId) throws Exception {\n+    String jobId = runtimeJobId.getRuntimeJobId();\n+\n+    Optional<RuntimeJobDetail> jobDetails = Optional.empty();\n+    try {\n+      LOG.info(\"Getting job details for {} under project {}, region {}.\", projectId, region, jobId);\n+      Job job = jobControllerClient.getJob(GetJobRequest.newBuilder()\n+                                             .setProjectId(projectId)\n+                                             .setRegion(region)\n+                                             .setJobId(jobId)\n+                                             .build());\n+      RuntimeJobStatus runtimeJobStatus = getRuntimeJobStatus(job);\n+\n+      jobDetails = Optional.of(new RuntimeJobDetail(runtimeJobId, runtimeJobStatus));\n+    } catch (ApiException e) {\n+      // this may happen if job is manually deleted by user\n+      if (e.getStatusCode().getCode() == StatusCode.Code.NOT_FOUND) {\n+        LOG.warn(\"Dataproc job {} does not exist in project {}, region {}.\", jobId, projectId, region);\n+      } else {\n+        throw new Exception(String.format(\"Error while getting details for job %s on cluster %s.\",\n+                                          jobId, clusterName), e);\n+      }\n+    }\n+    return jobDetails;\n+  }\n+\n+  @Override\n+  public List<RuntimeJobDetail> list() throws Exception {\n+    Set<String> filters = new HashSet<>();\n+    // Dataproc jobs can be filtered by status.state filter. In this case we only want ACTIVE jobs.\n+    filters.add(\"status.state=ACTIVE\");\n+    // Filter by labels that were added to the job when this runtime job manager submitted dataproc job. Note that\n+    // dataproc only supports AND filter.\n+    for (Map.Entry<String, String> entry : labels.entrySet()) {\n+      filters.add(\"labels.\" + entry.getKey() + \"=\" + entry.getValue());\n+    }\n+    String jobFilter = Joiner.on(\" AND \").join(filters);\n+\n+    LOG.info(\"Getting a list of jobs under project {}, region {}, cluster {} with filter {}.\", projectId, region,\n+             clusterName, jobFilter);\n+    JobControllerClient.ListJobsPagedResponse listJobsPagedResponse =\n+      jobControllerClient.listJobs(ListJobsRequest.newBuilder()\n+                                     .setProjectId(projectId).setRegion(region).setClusterName(clusterName)\n+                                     .setFilter(jobFilter).build());\n+\n+    Iterator<Job> jobsItor = listJobsPagedResponse.iterateAll().iterator();\n+    List<RuntimeJobDetail> jobsDetail = new ArrayList<>();\n+    while (jobsItor.hasNext()) {\n+      Job job = jobsItor.next();\n+      jobsDetail.add(new RuntimeJobDetail(new RuntimeJobId(job.getReference().getJobId()), getRuntimeJobStatus(job)));\n+    }\n+    return jobsDetail;\n+  }\n+\n+  @Override\n+  public void stop(RuntimeJobId runtimeJobId) throws Exception {\n+    Optional<RuntimeJobDetail> jobDetail = getDetail(runtimeJobId);\n+    // if the job does not exist, it can be safely assume that job has been deleted. Hence has reached terminal state.\n+    if (!jobDetail.isPresent()) {\n+      return;\n+    }\n+    // stop dataproc job\n+    Job stoppedJob = stopJob(runtimeJobId);\n+\n+    // delete gcs path for the job\n+    if (stoppedJob != null) {\n+      String runRootPath = getPath(CDAP_GCS_ROOT, stoppedJob.getHadoopJob().getPropertiesMap().get(CDAP_RUNTIME_RUNID));\n+      deleteGCSPath(runRootPath);\n+    }\n+  }\n+\n+  @Override\n+  public void kill(RuntimeJobId runtimeJobId) throws Exception {\n+    stop(runtimeJobId);\n+  }\n+\n+  @Override\n+  public void destroy() {\n+    jobControllerClient.close();", "originalCommit": "0a1560eaa6478bff6d8f5e5e0405b4a2132e0c45", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDA2Njg3OQ==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r394066879", "bodyText": "no storage client does not have close method.", "author": "CuriousVini", "createdAt": "2020-03-18T02:02:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkxOTcxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkyMDcyNw==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393920727", "bodyText": "You can use Files.copy instead to simplify the nesting.\nFile file = new File(localFile.getFileUri());\nFiles.copy(file.toPath(), Channels.newOutputStream(writer));", "author": "chtyim", "createdAt": "2020-03-17T19:32:35Z", "path": "cdap-runtime-ext-dataproc/src/main/java/io/cdap/cdap/runtime/spi/runtimejob/DataprocRuntimeJobManager.java", "diffHunk": "@@ -0,0 +1,419 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.runtime.spi.runtimejob;\n+\n+import com.google.api.gax.core.CredentialsProvider;\n+import com.google.api.gax.core.FixedCredentialsProvider;\n+import com.google.api.gax.rpc.ApiException;\n+import com.google.api.gax.rpc.StatusCode;\n+import com.google.auth.oauth2.GoogleCredentials;\n+import com.google.cloud.WriteChannel;\n+import com.google.cloud.dataproc.v1.GetJobRequest;\n+import com.google.cloud.dataproc.v1.HadoopJob;\n+import com.google.cloud.dataproc.v1.Job;\n+import com.google.cloud.dataproc.v1.JobControllerClient;\n+import com.google.cloud.dataproc.v1.JobControllerSettings;\n+import com.google.cloud.dataproc.v1.JobPlacement;\n+import com.google.cloud.dataproc.v1.JobReference;\n+import com.google.cloud.dataproc.v1.JobStatus;\n+import com.google.cloud.dataproc.v1.ListJobsRequest;\n+import com.google.cloud.dataproc.v1.SubmitJobRequest;\n+import com.google.cloud.storage.BlobId;\n+import com.google.cloud.storage.BlobInfo;\n+import com.google.cloud.storage.Storage;\n+import com.google.cloud.storage.StorageException;\n+import com.google.cloud.storage.StorageOptions;\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.io.ByteStreams;\n+import org.apache.twill.filesystem.LocalLocationFactory;\n+import org.apache.twill.filesystem.LocationFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.channels.Channels;\n+import java.nio.file.Files;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import javax.annotation.Nullable;\n+\n+/**\n+ * Dataproc runtime job manager. This class is responsible for launching a hadoop job on dataproc cluster and\n+ * managing it. An instance of this class is created by {@code DataprocProvisioner}.\n+ */\n+public class DataprocRuntimeJobManager implements RuntimeJobManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(DataprocRuntimeJobManager.class);\n+\n+  private static final String DATAPROC_GOOGLEAPIS = \"-dataproc.googleapis.com:443\";\n+  private static final String CDAP_GCS_ROOT = \"cdap-job\";\n+  // dataproc job properties\n+  private static final String CDAP_RUNTIME_NAMESPACE = \"cdap.runtime.namespace\";\n+  private static final String CDAP_RUNTIME_APPLICATION = \"cdap.runtime.application\";\n+  private static final String CDAP_RUNTIME_PROGRAM = \"cdap.runtime.program\";\n+  private static final String CDAP_RUNTIME_PROGRAM_TYPE = \"cdap.runtime.program.type\";\n+  private static final String CDAP_RUNTIME_RUNID = \"cdap.runtime.runid\";\n+\n+  private final String clusterName;\n+  private final GoogleCredentials credentials;\n+  private final String projectId;\n+  private final String region;\n+  private final String bucket;\n+  private final Map<String, String> labels;\n+\n+  private Storage storageClient;\n+  private JobControllerClient jobControllerClient;\n+\n+  /**\n+   * Created by dataproc provisioner with properties that are needed by dataproc runtime job manager.\n+   *\n+   * @param clusterName name of the cluster on which job should be submitted\n+   * @param credentials google credentials\n+   * @param projectId project id\n+   * @param region region\n+   * @param bucket gcs bucket\n+   * @param labels system labels to be added on dataproc job\n+   */\n+  public DataprocRuntimeJobManager(String clusterName, GoogleCredentials credentials, String projectId, String region,\n+                                   String bucket, Map<String, String> labels) {\n+    this.clusterName = clusterName;\n+    this.credentials = credentials;\n+    this.projectId = projectId;\n+    this.region = region;\n+    this.bucket = bucket;\n+    this.labels = Collections.unmodifiableMap(new HashMap<>(labels));\n+  }\n+\n+  @Override\n+  public void initialize() throws Exception {\n+    // instantiate a gcs client\n+    this.storageClient = StorageOptions.newBuilder().setProjectId(projectId)\n+      .setCredentials(credentials).build().getService();\n+\n+    // instantiate a dataproc job controller client\n+    CredentialsProvider credentialsProvider = FixedCredentialsProvider.create(credentials);\n+    this.jobControllerClient = JobControllerClient.create(\n+      JobControllerSettings.newBuilder().setCredentialsProvider(credentialsProvider)\n+        .setEndpoint(region + DATAPROC_GOOGLEAPIS).build());\n+  }\n+\n+  @Override\n+  public RuntimeJobId launch(RuntimeJobInfo runtimeJobInfo) throws Exception {\n+    ProgramRunInfo runInfo = runtimeJobInfo.getProgramRunInfo();\n+    LOG.info(\"Starting to launch program run {} with following configurations: cluster {}, project {}, region {}, \" +\n+               \"bucket {}.\", runInfo.getRun(), clusterName, projectId, region, bucket);\n+\n+    File tempDir = Files.createTempDirectory(\"dataproc.launcher\").toFile();\n+    // on dataproc bucket the run root will be <bucket>/cdap-job/<runid>/. All the files for this run will be copied\n+    // under that base dir.\n+    String runRootPath = getPath(CDAP_GCS_ROOT, runInfo.getRun());\n+    try {\n+      // step 1: build twill.jar and launcher.jar and add them to files to be copied to gcs\n+      List<RuntimeLocalFile> localFiles = getRuntimeLocalFiles(runtimeJobInfo.getLocalizeFiles(), tempDir);\n+\n+      // step 2: upload all the necessary files to gcs so that those files are available to dataproc job\n+      for (RuntimeLocalFile fileToUpload : localFiles) {\n+        String targetFilePath = getPath(runRootPath, fileToUpload.getName());\n+        LOG.debug(\"Uploading file {} to gcs bucket {}.\", targetFilePath, bucket);\n+        uploadFile(targetFilePath, fileToUpload);\n+        LOG.info(\"Uploaded file {} to gcs bucket {}.\", targetFilePath, bucket);\n+      }\n+\n+      // step 3: build the hadoop job request to be submitted to dataproc\n+      SubmitJobRequest request = getSubmitJobRequest(runtimeJobInfo.getRuntimeJobClass().getName(),\n+                                                     runInfo, localFiles);\n+\n+      // step 4: submit hadoop job to dataproc\n+      LOG.info(\"Submitting hadoop job {} to cluster {}.\", request.getJob().getReference().getJobId(), clusterName);\n+      Job job = jobControllerClient.submitJob(request);\n+      LOG.info(\"Successfully submitted hadoop job {} to cluster {}.\", job.getReference().getJobId(), clusterName);\n+\n+      return new RuntimeJobId(job.getReference().getJobId());\n+    } catch (Exception e) {\n+      // delete all uploaded gcs files in case of exception\n+      deleteGCSPath(runRootPath);\n+      throw new Exception(String.format(\"Error while launching job %s on cluster %s\",\n+                                        runInfo.getRun(), clusterName), e);\n+    } finally {\n+      // delete local temp directory\n+      deleteDirectoryContents(tempDir);\n+    }\n+  }\n+\n+  @Override\n+  public Optional<RuntimeJobDetail> getDetail(RuntimeJobId runtimeJobId) throws Exception {\n+    String jobId = runtimeJobId.getRuntimeJobId();\n+\n+    Optional<RuntimeJobDetail> jobDetails = Optional.empty();\n+    try {\n+      LOG.info(\"Getting job details for {} under project {}, region {}.\", projectId, region, jobId);\n+      Job job = jobControllerClient.getJob(GetJobRequest.newBuilder()\n+                                             .setProjectId(projectId)\n+                                             .setRegion(region)\n+                                             .setJobId(jobId)\n+                                             .build());\n+      RuntimeJobStatus runtimeJobStatus = getRuntimeJobStatus(job);\n+\n+      jobDetails = Optional.of(new RuntimeJobDetail(runtimeJobId, runtimeJobStatus));\n+    } catch (ApiException e) {\n+      // this may happen if job is manually deleted by user\n+      if (e.getStatusCode().getCode() == StatusCode.Code.NOT_FOUND) {\n+        LOG.warn(\"Dataproc job {} does not exist in project {}, region {}.\", jobId, projectId, region);\n+      } else {\n+        throw new Exception(String.format(\"Error while getting details for job %s on cluster %s.\",\n+                                          jobId, clusterName), e);\n+      }\n+    }\n+    return jobDetails;\n+  }\n+\n+  @Override\n+  public List<RuntimeJobDetail> list() throws Exception {\n+    Set<String> filters = new HashSet<>();\n+    // Dataproc jobs can be filtered by status.state filter. In this case we only want ACTIVE jobs.\n+    filters.add(\"status.state=ACTIVE\");\n+    // Filter by labels that were added to the job when this runtime job manager submitted dataproc job. Note that\n+    // dataproc only supports AND filter.\n+    for (Map.Entry<String, String> entry : labels.entrySet()) {\n+      filters.add(\"labels.\" + entry.getKey() + \"=\" + entry.getValue());\n+    }\n+    String jobFilter = Joiner.on(\" AND \").join(filters);\n+\n+    LOG.info(\"Getting a list of jobs under project {}, region {}, cluster {} with filter {}.\", projectId, region,\n+             clusterName, jobFilter);\n+    JobControllerClient.ListJobsPagedResponse listJobsPagedResponse =\n+      jobControllerClient.listJobs(ListJobsRequest.newBuilder()\n+                                     .setProjectId(projectId).setRegion(region).setClusterName(clusterName)\n+                                     .setFilter(jobFilter).build());\n+\n+    Iterator<Job> jobsItor = listJobsPagedResponse.iterateAll().iterator();\n+    List<RuntimeJobDetail> jobsDetail = new ArrayList<>();\n+    while (jobsItor.hasNext()) {\n+      Job job = jobsItor.next();\n+      jobsDetail.add(new RuntimeJobDetail(new RuntimeJobId(job.getReference().getJobId()), getRuntimeJobStatus(job)));\n+    }\n+    return jobsDetail;\n+  }\n+\n+  @Override\n+  public void stop(RuntimeJobId runtimeJobId) throws Exception {\n+    Optional<RuntimeJobDetail> jobDetail = getDetail(runtimeJobId);\n+    // if the job does not exist, it can be safely assume that job has been deleted. Hence has reached terminal state.\n+    if (!jobDetail.isPresent()) {\n+      return;\n+    }\n+    // stop dataproc job\n+    Job stoppedJob = stopJob(runtimeJobId);\n+\n+    // delete gcs path for the job\n+    if (stoppedJob != null) {\n+      String runRootPath = getPath(CDAP_GCS_ROOT, stoppedJob.getHadoopJob().getPropertiesMap().get(CDAP_RUNTIME_RUNID));\n+      deleteGCSPath(runRootPath);\n+    }\n+  }\n+\n+  @Override\n+  public void kill(RuntimeJobId runtimeJobId) throws Exception {\n+    stop(runtimeJobId);\n+  }\n+\n+  @Override\n+  public void destroy() {\n+    jobControllerClient.close();\n+  }\n+\n+  /**\n+   * Returns list of runtime local files with twill.jar and launcher.jar added to it.\n+   */\n+  private List<RuntimeLocalFile> getRuntimeLocalFiles(Collection<? extends RuntimeLocalFile> runtimeLocalFiles,\n+                                                      File tempDir) throws Exception {\n+    LocationFactory locationFactory = new LocalLocationFactory(tempDir);\n+    List<RuntimeLocalFile> localFiles = new ArrayList<>(runtimeLocalFiles);\n+    localFiles.add(DataprocJarUtil.getTwillJar(locationFactory));\n+    localFiles.add(DataprocJarUtil.getLauncherJar(locationFactory));\n+    return localFiles;\n+  }\n+\n+  /**\n+   * Uploads files to gcs.\n+   */\n+  private void uploadFile(String path, RuntimeLocalFile localFile) throws IOException, StorageException {\n+    try (InputStream inputStream = new FileInputStream(new File(localFile.getFileUri()))) {\n+      BlobId blobId = BlobId.of(bucket, path);\n+      BlobInfo blobInfo = BlobInfo.newBuilder(blobId).setContentType(\"application/octet-stream\").build();\n+      try (WriteChannel writer = storageClient.writer(blobInfo)) {\n+        ByteStreams.copy(inputStream, Channels.newOutputStream(writer));", "originalCommit": "0a1560eaa6478bff6d8f5e5e0405b4a2132e0c45", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkyMTY3MQ==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r393921671", "bodyText": "This variable is not needed. Just return directly.\ntry {\n  return jobControllerClient.cancelJob(...);\n} catch (...) {\n}\nreturn null;", "author": "chtyim", "createdAt": "2020-03-17T19:34:24Z", "path": "cdap-runtime-ext-dataproc/src/main/java/io/cdap/cdap/runtime/spi/runtimejob/DataprocRuntimeJobManager.java", "diffHunk": "@@ -0,0 +1,419 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.runtime.spi.runtimejob;\n+\n+import com.google.api.gax.core.CredentialsProvider;\n+import com.google.api.gax.core.FixedCredentialsProvider;\n+import com.google.api.gax.rpc.ApiException;\n+import com.google.api.gax.rpc.StatusCode;\n+import com.google.auth.oauth2.GoogleCredentials;\n+import com.google.cloud.WriteChannel;\n+import com.google.cloud.dataproc.v1.GetJobRequest;\n+import com.google.cloud.dataproc.v1.HadoopJob;\n+import com.google.cloud.dataproc.v1.Job;\n+import com.google.cloud.dataproc.v1.JobControllerClient;\n+import com.google.cloud.dataproc.v1.JobControllerSettings;\n+import com.google.cloud.dataproc.v1.JobPlacement;\n+import com.google.cloud.dataproc.v1.JobReference;\n+import com.google.cloud.dataproc.v1.JobStatus;\n+import com.google.cloud.dataproc.v1.ListJobsRequest;\n+import com.google.cloud.dataproc.v1.SubmitJobRequest;\n+import com.google.cloud.storage.BlobId;\n+import com.google.cloud.storage.BlobInfo;\n+import com.google.cloud.storage.Storage;\n+import com.google.cloud.storage.StorageException;\n+import com.google.cloud.storage.StorageOptions;\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.io.ByteStreams;\n+import org.apache.twill.filesystem.LocalLocationFactory;\n+import org.apache.twill.filesystem.LocationFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.channels.Channels;\n+import java.nio.file.Files;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import javax.annotation.Nullable;\n+\n+/**\n+ * Dataproc runtime job manager. This class is responsible for launching a hadoop job on dataproc cluster and\n+ * managing it. An instance of this class is created by {@code DataprocProvisioner}.\n+ */\n+public class DataprocRuntimeJobManager implements RuntimeJobManager {\n+  private static final Logger LOG = LoggerFactory.getLogger(DataprocRuntimeJobManager.class);\n+\n+  private static final String DATAPROC_GOOGLEAPIS = \"-dataproc.googleapis.com:443\";\n+  private static final String CDAP_GCS_ROOT = \"cdap-job\";\n+  // dataproc job properties\n+  private static final String CDAP_RUNTIME_NAMESPACE = \"cdap.runtime.namespace\";\n+  private static final String CDAP_RUNTIME_APPLICATION = \"cdap.runtime.application\";\n+  private static final String CDAP_RUNTIME_PROGRAM = \"cdap.runtime.program\";\n+  private static final String CDAP_RUNTIME_PROGRAM_TYPE = \"cdap.runtime.program.type\";\n+  private static final String CDAP_RUNTIME_RUNID = \"cdap.runtime.runid\";\n+\n+  private final String clusterName;\n+  private final GoogleCredentials credentials;\n+  private final String projectId;\n+  private final String region;\n+  private final String bucket;\n+  private final Map<String, String> labels;\n+\n+  private Storage storageClient;\n+  private JobControllerClient jobControllerClient;\n+\n+  /**\n+   * Created by dataproc provisioner with properties that are needed by dataproc runtime job manager.\n+   *\n+   * @param clusterName name of the cluster on which job should be submitted\n+   * @param credentials google credentials\n+   * @param projectId project id\n+   * @param region region\n+   * @param bucket gcs bucket\n+   * @param labels system labels to be added on dataproc job\n+   */\n+  public DataprocRuntimeJobManager(String clusterName, GoogleCredentials credentials, String projectId, String region,\n+                                   String bucket, Map<String, String> labels) {\n+    this.clusterName = clusterName;\n+    this.credentials = credentials;\n+    this.projectId = projectId;\n+    this.region = region;\n+    this.bucket = bucket;\n+    this.labels = Collections.unmodifiableMap(new HashMap<>(labels));\n+  }\n+\n+  @Override\n+  public void initialize() throws Exception {\n+    // instantiate a gcs client\n+    this.storageClient = StorageOptions.newBuilder().setProjectId(projectId)\n+      .setCredentials(credentials).build().getService();\n+\n+    // instantiate a dataproc job controller client\n+    CredentialsProvider credentialsProvider = FixedCredentialsProvider.create(credentials);\n+    this.jobControllerClient = JobControllerClient.create(\n+      JobControllerSettings.newBuilder().setCredentialsProvider(credentialsProvider)\n+        .setEndpoint(region + DATAPROC_GOOGLEAPIS).build());\n+  }\n+\n+  @Override\n+  public RuntimeJobId launch(RuntimeJobInfo runtimeJobInfo) throws Exception {\n+    ProgramRunInfo runInfo = runtimeJobInfo.getProgramRunInfo();\n+    LOG.info(\"Starting to launch program run {} with following configurations: cluster {}, project {}, region {}, \" +\n+               \"bucket {}.\", runInfo.getRun(), clusterName, projectId, region, bucket);\n+\n+    File tempDir = Files.createTempDirectory(\"dataproc.launcher\").toFile();\n+    // on dataproc bucket the run root will be <bucket>/cdap-job/<runid>/. All the files for this run will be copied\n+    // under that base dir.\n+    String runRootPath = getPath(CDAP_GCS_ROOT, runInfo.getRun());\n+    try {\n+      // step 1: build twill.jar and launcher.jar and add them to files to be copied to gcs\n+      List<RuntimeLocalFile> localFiles = getRuntimeLocalFiles(runtimeJobInfo.getLocalizeFiles(), tempDir);\n+\n+      // step 2: upload all the necessary files to gcs so that those files are available to dataproc job\n+      for (RuntimeLocalFile fileToUpload : localFiles) {\n+        String targetFilePath = getPath(runRootPath, fileToUpload.getName());\n+        LOG.debug(\"Uploading file {} to gcs bucket {}.\", targetFilePath, bucket);\n+        uploadFile(targetFilePath, fileToUpload);\n+        LOG.info(\"Uploaded file {} to gcs bucket {}.\", targetFilePath, bucket);\n+      }\n+\n+      // step 3: build the hadoop job request to be submitted to dataproc\n+      SubmitJobRequest request = getSubmitJobRequest(runtimeJobInfo.getRuntimeJobClass().getName(),\n+                                                     runInfo, localFiles);\n+\n+      // step 4: submit hadoop job to dataproc\n+      LOG.info(\"Submitting hadoop job {} to cluster {}.\", request.getJob().getReference().getJobId(), clusterName);\n+      Job job = jobControllerClient.submitJob(request);\n+      LOG.info(\"Successfully submitted hadoop job {} to cluster {}.\", job.getReference().getJobId(), clusterName);\n+\n+      return new RuntimeJobId(job.getReference().getJobId());\n+    } catch (Exception e) {\n+      // delete all uploaded gcs files in case of exception\n+      deleteGCSPath(runRootPath);\n+      throw new Exception(String.format(\"Error while launching job %s on cluster %s\",\n+                                        runInfo.getRun(), clusterName), e);\n+    } finally {\n+      // delete local temp directory\n+      deleteDirectoryContents(tempDir);\n+    }\n+  }\n+\n+  @Override\n+  public Optional<RuntimeJobDetail> getDetail(RuntimeJobId runtimeJobId) throws Exception {\n+    String jobId = runtimeJobId.getRuntimeJobId();\n+\n+    Optional<RuntimeJobDetail> jobDetails = Optional.empty();\n+    try {\n+      LOG.info(\"Getting job details for {} under project {}, region {}.\", projectId, region, jobId);\n+      Job job = jobControllerClient.getJob(GetJobRequest.newBuilder()\n+                                             .setProjectId(projectId)\n+                                             .setRegion(region)\n+                                             .setJobId(jobId)\n+                                             .build());\n+      RuntimeJobStatus runtimeJobStatus = getRuntimeJobStatus(job);\n+\n+      jobDetails = Optional.of(new RuntimeJobDetail(runtimeJobId, runtimeJobStatus));\n+    } catch (ApiException e) {\n+      // this may happen if job is manually deleted by user\n+      if (e.getStatusCode().getCode() == StatusCode.Code.NOT_FOUND) {\n+        LOG.warn(\"Dataproc job {} does not exist in project {}, region {}.\", jobId, projectId, region);\n+      } else {\n+        throw new Exception(String.format(\"Error while getting details for job %s on cluster %s.\",\n+                                          jobId, clusterName), e);\n+      }\n+    }\n+    return jobDetails;\n+  }\n+\n+  @Override\n+  public List<RuntimeJobDetail> list() throws Exception {\n+    Set<String> filters = new HashSet<>();\n+    // Dataproc jobs can be filtered by status.state filter. In this case we only want ACTIVE jobs.\n+    filters.add(\"status.state=ACTIVE\");\n+    // Filter by labels that were added to the job when this runtime job manager submitted dataproc job. Note that\n+    // dataproc only supports AND filter.\n+    for (Map.Entry<String, String> entry : labels.entrySet()) {\n+      filters.add(\"labels.\" + entry.getKey() + \"=\" + entry.getValue());\n+    }\n+    String jobFilter = Joiner.on(\" AND \").join(filters);\n+\n+    LOG.info(\"Getting a list of jobs under project {}, region {}, cluster {} with filter {}.\", projectId, region,\n+             clusterName, jobFilter);\n+    JobControllerClient.ListJobsPagedResponse listJobsPagedResponse =\n+      jobControllerClient.listJobs(ListJobsRequest.newBuilder()\n+                                     .setProjectId(projectId).setRegion(region).setClusterName(clusterName)\n+                                     .setFilter(jobFilter).build());\n+\n+    Iterator<Job> jobsItor = listJobsPagedResponse.iterateAll().iterator();\n+    List<RuntimeJobDetail> jobsDetail = new ArrayList<>();\n+    while (jobsItor.hasNext()) {\n+      Job job = jobsItor.next();\n+      jobsDetail.add(new RuntimeJobDetail(new RuntimeJobId(job.getReference().getJobId()), getRuntimeJobStatus(job)));\n+    }\n+    return jobsDetail;\n+  }\n+\n+  @Override\n+  public void stop(RuntimeJobId runtimeJobId) throws Exception {\n+    Optional<RuntimeJobDetail> jobDetail = getDetail(runtimeJobId);\n+    // if the job does not exist, it can be safely assume that job has been deleted. Hence has reached terminal state.\n+    if (!jobDetail.isPresent()) {\n+      return;\n+    }\n+    // stop dataproc job\n+    Job stoppedJob = stopJob(runtimeJobId);\n+\n+    // delete gcs path for the job\n+    if (stoppedJob != null) {\n+      String runRootPath = getPath(CDAP_GCS_ROOT, stoppedJob.getHadoopJob().getPropertiesMap().get(CDAP_RUNTIME_RUNID));\n+      deleteGCSPath(runRootPath);\n+    }\n+  }\n+\n+  @Override\n+  public void kill(RuntimeJobId runtimeJobId) throws Exception {\n+    stop(runtimeJobId);\n+  }\n+\n+  @Override\n+  public void destroy() {\n+    jobControllerClient.close();\n+  }\n+\n+  /**\n+   * Returns list of runtime local files with twill.jar and launcher.jar added to it.\n+   */\n+  private List<RuntimeLocalFile> getRuntimeLocalFiles(Collection<? extends RuntimeLocalFile> runtimeLocalFiles,\n+                                                      File tempDir) throws Exception {\n+    LocationFactory locationFactory = new LocalLocationFactory(tempDir);\n+    List<RuntimeLocalFile> localFiles = new ArrayList<>(runtimeLocalFiles);\n+    localFiles.add(DataprocJarUtil.getTwillJar(locationFactory));\n+    localFiles.add(DataprocJarUtil.getLauncherJar(locationFactory));\n+    return localFiles;\n+  }\n+\n+  /**\n+   * Uploads files to gcs.\n+   */\n+  private void uploadFile(String path, RuntimeLocalFile localFile) throws IOException, StorageException {\n+    try (InputStream inputStream = new FileInputStream(new File(localFile.getFileUri()))) {\n+      BlobId blobId = BlobId.of(bucket, path);\n+      BlobInfo blobInfo = BlobInfo.newBuilder(blobId).setContentType(\"application/octet-stream\").build();\n+      try (WriteChannel writer = storageClient.writer(blobInfo)) {\n+        ByteStreams.copy(inputStream, Channels.newOutputStream(writer));\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Creates and returns dataproc job submit request.\n+   */\n+  private SubmitJobRequest getSubmitJobRequest(String jobMainClassName,\n+                                               ProgramRunInfo runInfo, List<RuntimeLocalFile> localFiles) {\n+    String runId = runInfo.getRun();\n+    HadoopJob.Builder hadoopJobBuilder = HadoopJob.newBuilder()\n+      // set main class\n+      .setMainClass(DataprocJobMain.class.getName())\n+      // set main class arguments\n+      .addAllArgs(ImmutableList.of(jobMainClassName))\n+      .putAllProperties(ImmutableMap.of(CDAP_RUNTIME_NAMESPACE, runInfo.getNamespace(),\n+                                        CDAP_RUNTIME_APPLICATION, runInfo.getApplication(),\n+                                        CDAP_RUNTIME_PROGRAM, runInfo.getProgram(),\n+                                        CDAP_RUNTIME_PROGRAM_TYPE, runInfo.getProgramType(),\n+                                        CDAP_RUNTIME_RUNID, runId));\n+\n+    for (RuntimeLocalFile localFile : localFiles) {\n+      String localFileName = localFile.getName();\n+      String fileName = getPath(\"gs:/\", bucket, CDAP_GCS_ROOT, runId, localFileName);\n+\n+      // add archive file\n+      if (localFile.isArchive()) {\n+        LOG.debug(\"Adding {} as archive.\", localFileName);\n+        hadoopJobBuilder.addArchiveUris(fileName);\n+      }\n+\n+      // add jar file\n+      if (localFile.getName().endsWith(\"jar\")) {\n+        LOG.info(\"Adding {} as jar.\", localFileName);\n+        hadoopJobBuilder.addJarFileUris(fileName);\n+      } else {\n+        // add all the other files as file\n+        LOG.info(\"Adding {} as file.\", localFileName);\n+        hadoopJobBuilder.addFileUris(fileName);\n+      }\n+    }\n+\n+    return SubmitJobRequest.newBuilder()\n+      .setRegion(region)\n+      .setProjectId(projectId)\n+      .setJob(Job.newBuilder()\n+                // use program run uuid as hadoop job id on dataproc\n+                .setReference(JobReference.newBuilder().setJobId(runInfo.getRun()))\n+                // place the job on provisioned cluster\n+                .setPlacement(JobPlacement.newBuilder().setClusterName(clusterName).build())\n+                // add same labels as provisioned cluster\n+                .putAllLabels(labels)\n+                .setHadoopJob(hadoopJobBuilder.build())\n+                .build())\n+      .build();\n+  }\n+\n+  /**\n+   * Returns {@link RuntimeJobStatus}.\n+   */\n+  private RuntimeJobStatus getRuntimeJobStatus(Job job) {\n+    JobStatus.State state = job.getStatus().getState();\n+    LOG.debug(\"Dataproc job {} is in state {}.\", job.getReference().getJobId(), state);\n+\n+    RuntimeJobStatus runtimeJobStatus;\n+    switch (state) {\n+      case STATE_UNSPECIFIED:\n+      case SETUP_DONE:\n+      case PENDING:\n+        runtimeJobStatus = RuntimeJobStatus.STARTING;\n+        break;\n+      case RUNNING:\n+        runtimeJobStatus = RuntimeJobStatus.RUNNING;\n+        break;\n+      case DONE:\n+        runtimeJobStatus = RuntimeJobStatus.COMPLETED;\n+        break;\n+      case CANCEL_PENDING:\n+      case CANCEL_STARTED:\n+        runtimeJobStatus = RuntimeJobStatus.STOPPING;\n+        break;\n+      case CANCELLED:\n+        runtimeJobStatus = RuntimeJobStatus.STOPPED;\n+        break;\n+      case ERROR:\n+        runtimeJobStatus = RuntimeJobStatus.FAILED;\n+        break;\n+      default:\n+        // this needed for ATTEMPT_FAILURE state which is a state for restartable job. Currently we do not launch\n+        // restartable jobs\n+        throw new IllegalStateException(String.format(\"Unsupported job state %s of the dataproc job %s on cluster %s.\",\n+                                                      job.getStatus().getState(), job.getReference().getJobId(),\n+                                                      job.getPlacement().getClusterName()));\n+\n+    }\n+    return runtimeJobStatus;\n+  }\n+\n+  /**\n+   * Stops the dataproc job. Returns job object if it was stopped.\n+   */\n+  @Nullable\n+  private Job stopJob(RuntimeJobId runtimeJobId) throws Exception {\n+    String jobId = runtimeJobId.getRuntimeJobId();\n+    Job job = null;", "originalCommit": "0a1560eaa6478bff6d8f5e5e0405b4a2132e0c45", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDA2NzAyNg==", "url": "https://github.com/cdapio/cdap/pull/11961#discussion_r394067026", "bodyText": "modified it to not return job. The cleanup would happen in destroy() method.", "author": "CuriousVini", "createdAt": "2020-03-18T02:02:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzkyMTY3MQ=="}], "type": "inlineReview"}, {"oid": "b0ffcba8c1e579dc258d166965ac943ee4d6decb", "url": "https://github.com/cdapio/cdap/commit/b0ffcba8c1e579dc258d166965ac943ee4d6decb", "message": "Address comments", "committedDate": "2020-03-18T04:27:23Z", "type": "forcePushed"}, {"oid": "46b8301df11457dc7b56870b778ebf372f663e4d", "url": "https://github.com/cdapio/cdap/commit/46b8301df11457dc7b56870b778ebf372f663e4d", "message": "Address comments", "committedDate": "2020-03-18T04:31:38Z", "type": "forcePushed"}, {"oid": "dae7d406a9b111c1d1a5266960daf6de215dd8e4", "url": "https://github.com/cdapio/cdap/commit/dae7d406a9b111c1d1a5266960daf6de215dd8e4", "message": "Address comments", "committedDate": "2020-03-18T04:40:18Z", "type": "forcePushed"}, {"oid": "b2ac05273784ab1bd7c11713c7ac1821c17bd73e", "url": "https://github.com/cdapio/cdap/commit/b2ac05273784ab1bd7c11713c7ac1821c17bd73e", "message": "Address comments", "committedDate": "2020-03-18T04:42:07Z", "type": "forcePushed"}, {"oid": "87351932dd896af7ff9eca7b9c4e768f3bc5f9b0", "url": "https://github.com/cdapio/cdap/commit/87351932dd896af7ff9eca7b9c4e768f3bc5f9b0", "message": "[CDAP-15633] Adding dataproc runtime manager implementation", "committedDate": "2020-03-18T20:23:17Z", "type": "commit"}, {"oid": "87351932dd896af7ff9eca7b9c4e768f3bc5f9b0", "url": "https://github.com/cdapio/cdap/commit/87351932dd896af7ff9eca7b9c4e768f3bc5f9b0", "message": "[CDAP-15633] Adding dataproc runtime manager implementation", "committedDate": "2020-03-18T20:23:17Z", "type": "forcePushed"}]}