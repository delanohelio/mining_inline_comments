{"pr_number": 11880, "pr_title": "CDAP-13643 add streaming pipeline field lineage", "pr_createdAt": "2020-02-24T00:43:37Z", "pr_url": "https://github.com/cdapio/cdap/pull/11880", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQwMzg4Nw==", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383403887", "bodyText": "flushRecord() -> flushLineage()", "author": "albertshau", "createdAt": "2020-02-24T17:23:16Z", "path": "cdap-api/src/main/java/io/cdap/cdap/api/lineage/field/LineageRecorder.java", "diffHunk": "@@ -32,4 +32,11 @@\n    * @param operations the operations to be recorded\n    */\n   void record(Collection<? extends Operation> operations);\n+\n+  /**\n+   * Flush the existing record, this method will flush all the existing operations to the writer, and clear\n+   * the recorded operations. The existing operations should be complete, they should have at least one\n+   * operation of type {@link ReadOperation} and one operation of type {@link WriteOperation}.\n+   */\n+  void flushRecord();", "originalCommit": "70cff0073470002c7b8b0069c6df2676b51ca3dd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQwNDc1MA==", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383404750", "bodyText": "this involves IO so it seems like it should throw some type of exception. Depending on how this is being used, programs may want to fail or they may just want to log a warning and continue on.\nIf this is too much refactoring for the time being, please open a jira.", "author": "albertshau", "createdAt": "2020-02-24T17:24:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQwMzg4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzU1NDk1NA==", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383554954", "bodyText": "For some reason, the FieldLineageWriter does not throw any specific exception(only RuntimeException) when writing, I will add the Exception that consolidation logic will throw", "author": "yaojiefeng", "createdAt": "2020-02-24T22:33:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQwMzg4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzU2OTMwOA==", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383569308", "bodyText": "Modified all the places to write lineage to use this method", "author": "yaojiefeng", "createdAt": "2020-02-24T23:10:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQwMzg4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQwNDAwOQ==", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383404009", "bodyText": "what happens if they are not complete? Is an exception thrown?", "author": "albertshau", "createdAt": "2020-02-24T17:23:28Z", "path": "cdap-api/src/main/java/io/cdap/cdap/api/lineage/field/LineageRecorder.java", "diffHunk": "@@ -32,4 +32,11 @@\n    * @param operations the operations to be recorded\n    */\n   void record(Collection<? extends Operation> operations);\n+\n+  /**\n+   * Flush the existing record, this method will flush all the existing operations to the writer, and clear\n+   * the recorded operations. The existing operations should be complete, they should have at least one", "originalCommit": "70cff0073470002c7b8b0069c6df2676b51ca3dd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzU2MjIyNA==", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383562224", "bodyText": "Added the javadocs", "author": "yaojiefeng", "createdAt": "2020-02-24T22:51:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQwNDAwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQwNzYxMA==", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383407610", "bodyText": "this and record() should be synchronized, or the operations set changed to a concurrent set.", "author": "albertshau", "createdAt": "2020-02-24T17:30:12Z", "path": "cdap-app-fabric/src/main/java/io/cdap/cdap/internal/app/runtime/AbstractContext.java", "diffHunk": "@@ -873,4 +877,11 @@ public NamespacedEntityId getComponentId() {\n   public void record(Collection<? extends Operation> operations) {\n     fieldLineageOperations.addAll(operations);\n   }\n+\n+  @Override\n+  public void flushRecord() {", "originalCommit": "70cff0073470002c7b8b0069c6df2676b51ca3dd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQwODQyOQ==", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383408429", "bodyText": "It would be good to add some docs around when operations are automatically flushed by programs vs when you would need to manually flush them.", "author": "albertshau", "createdAt": "2020-02-24T17:31:41Z", "path": "cdap-api/src/main/java/io/cdap/cdap/api/lineage/field/LineageRecorder.java", "diffHunk": "@@ -32,4 +32,11 @@\n    * @param operations the operations to be recorded\n    */\n   void record(Collection<? extends Operation> operations);\n+\n+  /**\n+   * Flush the existing record, this method will flush all the existing operations to the writer, and clear\n+   * the recorded operations. The existing operations should be complete, they should have at least one\n+   * operation of type {@link ReadOperation} and one operation of type {@link WriteOperation}.", "originalCommit": "70cff0073470002c7b8b0069c6df2676b51ca3dd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQwODk1OA==", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383408958", "bodyText": "unused?", "author": "albertshau", "createdAt": "2020-02-24T17:32:41Z", "path": "cdap-app-templates/cdap-etl/cdap-data-pipeline/src/main/java/io/cdap/cdap/datapipeline/SmartWorkflow.java", "diffHunk": "@@ -78,6 +78,7 @@\n import io.cdap.cdap.etl.common.PipelineRuntime;\n import io.cdap.cdap.etl.common.TrackedIterator;\n import io.cdap.cdap.etl.common.plugin.PipelinePluginContext;\n+import io.cdap.cdap.etl.lineage.FieldLineageProcessor;", "originalCommit": "70cff0073470002c7b8b0069c6df2676b51ca3dd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzUzNzg1OQ==", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383537859", "bodyText": "No, this is because we moved this class to a different package, so we need this import now", "author": "yaojiefeng", "createdAt": "2020-02-24T21:54:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQwODk1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxNjIzOA==", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383416238", "bodyText": "this doesn't look like it should be a static util method, as it's only used in one place.\nAlso, the pluginContext, pipelineRuntime, macro evaluator, etc can be re-used.", "author": "albertshau", "createdAt": "2020-02-24T17:47:08Z", "path": "cdap-app-templates/cdap-etl/cdap-data-streams/src/main/java/io/cdap/cdap/datastreams/SparkFieldLineageRecorder.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.datastreams;\n+\n+import io.cdap.cdap.api.lineage.field.Operation;\n+import io.cdap.cdap.api.macro.MacroEvaluator;\n+import io.cdap.cdap.api.spark.JavaSparkExecutionContext;\n+import io.cdap.cdap.etl.api.lineage.field.FieldOperation;\n+import io.cdap.cdap.etl.common.DefaultMacroEvaluator;\n+import io.cdap.cdap.etl.common.PhaseSpec;\n+import io.cdap.cdap.etl.common.PipelinePhase;\n+import io.cdap.cdap.etl.common.PipelineRuntime;\n+import io.cdap.cdap.etl.common.plugin.PipelinePluginContext;\n+import io.cdap.cdap.etl.lineage.FieldLineageProcessor;\n+import io.cdap.cdap.etl.proto.v2.spec.PipelineSpec;\n+import io.cdap.cdap.etl.spark.SparkPipelineRuntime;\n+import io.cdap.cdap.etl.spark.streaming.SparkStreamingPreparer;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * Field lineage recorder for streaming pipeline\n+ */\n+public final class SparkFieldLineageRecorder {\n+  private SparkFieldLineageRecorder() {\n+  }\n+\n+  public static void recordFieldLineage(JavaSparkExecutionContext sec, PipelinePhase pipelinePhase,", "originalCommit": "70cff0073470002c7b8b0069c6df2676b51ca3dd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxNzM3OA==", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383417378", "bodyText": "this doesn't seem like a debug, but a warning.", "author": "albertshau", "createdAt": "2020-02-24T17:49:29Z", "path": "cdap-app-templates/cdap-etl/cdap-data-streams/src/main/java/io/cdap/cdap/datastreams/SparkStreamingPipelineDriver.java", "diffHunk": "@@ -180,37 +180,41 @@ public void run(DatasetContext context) throws Exception {\n \n   }\n \n-  private JavaStreamingContext run(final DataStreamsPipelineSpec pipelineSpec,\n-                                   final PipelinePhase pipelinePhase,\n-                                   final JavaSparkExecutionContext sec,\n-                                   @Nullable final String checkpointDir,\n-                                   @Nullable final JavaSparkContext context) throws Exception {\n-    Function0<JavaStreamingContext> contextFunction = new Function0<JavaStreamingContext>() {\n-      @Override\n-      public JavaStreamingContext call() throws Exception {\n-        JavaSparkContext javaSparkContext = context == null ? new JavaSparkContext() : context;\n-        JavaStreamingContext jssc = new JavaStreamingContext(\n-          javaSparkContext, Durations.milliseconds(pipelineSpec.getBatchIntervalMillis()));\n-        SparkStreamingPipelineRunner runner = new SparkStreamingPipelineRunner(sec, jssc, pipelineSpec,\n-                                                                               pipelineSpec.isCheckpointsDisabled());\n-        PipelinePluginContext pluginContext = new PipelinePluginContext(sec.getPluginContext(), sec.getMetrics(),\n-                                                                        pipelineSpec.isStageLoggingEnabled(),\n-                                                                        pipelineSpec.isProcessTimingEnabled());\n-        // TODO: figure out how to get partitions to use for aggregators and joiners.\n-        // Seems like they should be set at configure time instead of runtime? but that requires an API change.\n-        try {\n-          runner.runPipeline(pipelinePhase, StreamingSource.PLUGIN_TYPE,\n-                             sec, new HashMap<String, Integer>(), pluginContext,\n-                             new HashMap<String, StageStatisticsCollector>());\n-        } catch (Exception e) {\n-          throw new RuntimeException(e);\n-        }\n-        if (checkpointDir != null) {\n-          jssc.checkpoint(checkpointDir);\n-          jssc.sparkContext().hadoopConfiguration().set(\"fs.defaultFS\", checkpointDir);\n-        }\n-        return jssc;\n+  private JavaStreamingContext run(DataStreamsPipelineSpec pipelineSpec,\n+                                   PipelinePhase pipelinePhase,\n+                                   JavaSparkExecutionContext sec,\n+                                   @Nullable String checkpointDir,\n+                                   @Nullable JavaSparkContext context) throws Exception {\n+    try {\n+      SparkFieldLineageRecorder.recordFieldLineage(sec, pipelinePhase, pipelineSpec);\n+    } catch (Exception e) {\n+      LOG.debug(\"Failed to emit field lineage operations for streaming pipeline\", e);", "originalCommit": "70cff0073470002c7b8b0069c6df2676b51ca3dd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQyMzcwMg==", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383423702", "bodyText": "shouldn't add this method, we don't want spark and mapreduce dependencies to mix. The PipelinePhasePreparer has a catch-all create() method that can be used.", "author": "albertshau", "createdAt": "2020-02-24T18:02:01Z", "path": "cdap-app-templates/cdap-etl/cdap-etl-batch/src/main/java/io/cdap/cdap/etl/batch/mapreduce/MapReducePreparer.java", "diffHunk": "@@ -133,6 +133,12 @@ protected SubmitterPlugin createSource(BatchConfigurable<BatchSourceContext> bat\n     });\n   }\n \n+  @Nullable\n+  @Override\n+  protected SubmitterPlugin createStreamingSource(StreamingSource<?> streamingSource, StageSpec stageSpec) {", "originalCommit": "70cff0073470002c7b8b0069c6df2676b51ca3dd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzU3NzA3MQ==", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383577071", "bodyText": "names, or there", "author": "albertshau", "createdAt": "2020-02-24T23:33:08Z", "path": "cdap-api/src/main/java/io/cdap/cdap/api/lineage/field/LineageRecorder.java", "diffHunk": "@@ -36,7 +36,16 @@\n   /**\n    * Flush the existing record, this method will flush all the existing operations to the writer, and clear\n    * the recorded operations. The existing operations should be complete, they should have at least one\n-   * operation of type {@link ReadOperation} and one operation of type {@link WriteOperation}.\n+   * operation of type {@link ReadOperation} and one operation of type {@link WriteOperation}. If the operations are\n+   * not complete, {@link IllegalArgumentException} will be thrown and existing records will be preserved.\n+   *\n+   * For batch programs(workflow, batch spark, mapreduce), the method is called automatically at the end of the\n+   * successful run.\n+   * For realtime programs(worker, service, spark streaming), the method has to be manually called to write the\n+   * lineage since realtime program will not stop automatically.\n+   *\n+   * @throws IllegalArgumentException if the validation of the field operations fails, this can happen when there is\n+   * no read or write operations, operations have same names, there is a cycle in the operations.", "originalCommit": "8eb6b29c3db9a84306c72cf880a1dbd31fddd30c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzU3ODUxOA==", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383578518", "bodyText": "shouldn't this still be nullable?", "author": "albertshau", "createdAt": "2020-02-24T23:37:48Z", "path": "cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/common/submit/PipelinePhasePreparer.java", "diffHunk": "@@ -132,15 +128,9 @@ public PipelinePhasePreparer(PluginContext pluginContext, Metrics metrics, Macro\n   protected abstract SubmitterPlugin create(PipelinePluginInstantiator pluginInstantiator, StageSpec stageSpec)\n     throws InstantiationException;\n \n-  // for streaming pipeline, batch source cannot be created\n-  @Nullable", "originalCommit": "8eb6b29c3db9a84306c72cf880a1dbd31fddd30c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzU3ODYyMw==", "url": "https://github.com/cdapio/cdap/pull/11880#discussion_r383578623", "bodyText": "extra space after =", "author": "albertshau", "createdAt": "2020-02-24T23:38:05Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/streaming/SparkStreamingPreparer.java", "diffHunk": "@@ -57,8 +58,10 @@ protected SubmitterPlugin createSource(BatchConfigurable<BatchSourceContext> bat\n   }\n \n   @Override\n-  protected SubmitterPlugin createStreamingSource(StreamingSource<?> streamingSource, StageSpec stageSpec) {\n+  protected SubmitterPlugin createStreamingSource(PipelinePluginInstantiator pluginInstantiator,\n+                                                  StageSpec stageSpec) throws InstantiationException {\n     String stageName = stageSpec.getName();\n+    StreamingSource<?> streamingSource =  pluginInstantiator.newPluginInstance(stageName, macroEvaluator);", "originalCommit": "8eb6b29c3db9a84306c72cf880a1dbd31fddd30c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b8c4f57aae53ce7fe32a1e784d149ca35ff72160", "url": "https://github.com/cdapio/cdap/commit/b8c4f57aae53ce7fe32a1e784d149ca35ff72160", "message": "CDAP-13643 add streaming pipeline field lineage", "committedDate": "2020-02-24T23:45:57Z", "type": "forcePushed"}, {"oid": "3fce9227ec18131fc601b4f20d9246bf12e26c83", "url": "https://github.com/cdapio/cdap/commit/3fce9227ec18131fc601b4f20d9246bf12e26c83", "message": "CDAP-13643 add streaming pipeline field lineage", "committedDate": "2020-02-25T05:57:54Z", "type": "commit"}, {"oid": "3fce9227ec18131fc601b4f20d9246bf12e26c83", "url": "https://github.com/cdapio/cdap/commit/3fce9227ec18131fc601b4f20d9246bf12e26c83", "message": "CDAP-13643 add streaming pipeline field lineage", "committedDate": "2020-02-25T05:57:54Z", "type": "forcePushed"}]}