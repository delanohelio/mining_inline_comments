{"pr_number": 747, "pr_title": " Change task locks from Ephemeral to Persistent ", "pr_createdAt": "2020-08-24T19:22:12Z", "pr_url": "https://github.com/linkedin/brooklin/pull/747", "timeline": [{"oid": "c31cd4a15cc8dd69b0a653dbe4201de934ea6d65", "url": "https://github.com/linkedin/brooklin/commit/c31cd4a15cc8dd69b0a653dbe4201de934ea6d65", "message": "Merge pull request #1 from linkedin/master\n\nPull latest", "committedDate": "2019-11-18T20:06:44Z", "type": "commit"}, {"oid": "f3004d104a697e619b0715de5c135e7544fb4d25", "url": "https://github.com/linkedin/brooklin/commit/f3004d104a697e619b0715de5c135e7544fb4d25", "message": "Merge branch 'master' of github.com:vmaheshw/Brooklin", "committedDate": "2020-08-13T19:09:21Z", "type": "commit"}, {"oid": "0126f592a7abc5d97016898ee007758f0ac9cba6", "url": "https://github.com/linkedin/brooklin/commit/0126f592a7abc5d97016898ee007758f0ac9cba6", "message": "Make task lock persistent", "committedDate": "2020-08-17T21:04:11Z", "type": "commit"}, {"oid": "3fc04bb2d920774ae88df8dea886af59c3808eec", "url": "https://github.com/linkedin/brooklin/commit/3fc04bb2d920774ae88df8dea886af59c3808eec", "message": "Add tests for persistent lock and configurable debounce timer support", "committedDate": "2020-08-24T19:16:20Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjgxMjA2OA==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476812068", "bodyText": "Since we are adding lock cleanup support here, can we rename this function (performCleanupOrphanConnectorTasks) to indicate it cleans up orphan tasks and locks? Also, should the config be renamed too?", "author": "somandal", "createdAt": "2020-08-25T22:44:22Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/Coordinator.java", "diffHunk": "@@ -1252,6 +1252,8 @@ void performCleanupOrphanConnectorTasks() {\n     _log.info(\"performCleanupOrphanConnectorTasks called\");\n     int orphanCount = _adapter.cleanUpOrphanConnectorTasks(_config.getZkCleanUpOrphanConnectorTask());\n     _metrics.updateMeter(CoordinatorMetrics.Meter.NUM_ORPHAN_CONNECTOR_TASKS, orphanCount);\n+    int orphanLockCount = _adapter.cleanUpOrphanConnectorTaskLocks(_config.getZkCleanUpOrphanConnectorTask());", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzUyNTgyMQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477525821", "bodyText": "done.", "author": "vmaheshw", "createdAt": "2020-08-26T19:06:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjgxMjA2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg1Mzg0Mw==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476853843", "bodyText": "nit: delete empty line?", "author": "somandal", "createdAt": "2020-08-25T23:26:26Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/CoordinatorConfig.java", "diffHunk": "@@ -50,8 +52,10 @@ public CoordinatorConfig(Properties config) {\n     _zkConnectionTimeout = _properties.getInt(CONFIG_ZK_CONNECTION_TIMEOUT, ZkClient.DEFAULT_CONNECTION_TIMEOUT);\n     _retryIntervalMs = _properties.getInt(CONFIG_RETRY_INTERVAL, 1000 /* 1 second */);\n     _heartbeatPeriodMs = _properties.getLong(CONFIG_HEARTBEAT_PERIOD_MS, Duration.ofMinutes(1).toMillis());\n+    _debounceTimerMs = _properties.getLong(CONFIG_DEBOUNCE_TIMER_MS, Duration.ofSeconds(30).toMillis());\n     _defaultTransportProviderName = _properties.getString(CONFIG_DEFAULT_TRANSPORT_PROVIDER, \"\");\n     _zkCleanUpOrphanConnectorTask = _properties.getBoolean(CONFIG_ZK_CLEANUP_ORPHAN_CONNECTOR_TASK, false);\n+", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg1NDAxMg==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476854012", "bodyText": "nit: delete empty line?", "author": "somandal", "createdAt": "2020-08-25T23:26:37Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/CoordinatorConfig.java", "diffHunk": "@@ -89,4 +93,9 @@ public long getHeartbeatPeriodMs() {\n   public boolean getZkCleanUpOrphanConnectorTask() {\n     return _zkCleanUpOrphanConnectorTask;\n   }\n+\n+  public long getDebounceTimerMs() {\n+    return _debounceTimerMs;\n+  }\n+", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg1NDY5MQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476854691", "bodyText": "Should we add a comment explaining what this debounce timer is used for?", "author": "somandal", "createdAt": "2020-08-25T23:27:14Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/CoordinatorConfig.java", "diffHunk": "@@ -32,6 +33,7 @@\n   private final Properties _config;\n   private final VerifiableProperties _properties;\n   private final int _retryIntervalMs;\n+  private final long _debounceTimerMs;", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg1NjA1Mg==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476856052", "bodyText": "did you mean to merge these two sets of comments? I thought the first comment was a more overall comment for the file, and the second one specific to this String?", "author": "somandal", "createdAt": "2020-08-25T23:28:29Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/KeyBuilder.java", "diffHunk": "@@ -38,9 +38,7 @@ private KeyBuilder() {\n    *\n    * Below keys represent the various locations under connectors for parts of\n    * a DatastreamTask can be persisted.\n-   */\n \n-  /**", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg1NjkzOA==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476856938", "bodyText": "nit: {task} -> {taskName}", "author": "somandal", "createdAt": "2020-08-25T23:29:20Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/KeyBuilder.java", "diffHunk": "@@ -71,9 +69,14 @@ private KeyBuilder() {\n   private static final String DATASTREAM_TASK_LOCK_ROOT = CONNECTOR + \"/\" + DATASTREAM_TASK_LOCK_ROOT_NAME;\n \n   /**\n-   * Task lock node under connectorType/lock/{taskName}\n+   * Task lock node under connectorType/lock/{taskPrefix}\n+   */\n+  private static final String DATASTREAM_TASK_LOCK_PREFIX = DATASTREAM_TASK_LOCK_ROOT + \"/%s\";\n+\n+  /**\n+   * Task lock node under connectorType/lock/{taskPrefix}/{task}", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg2MDAwNQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476860005", "bodyText": "This comment talks about ephemeral nodes, and we are making the lock persistent. Can we update this comment and all other comments that mention ephemeral locks?", "author": "somandal", "createdAt": "2020-08-25T23:32:19Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/KeyBuilder.java", "diffHunk": "@@ -271,22 +274,39 @@ public static String datastreamTaskStateKey(String cluster, String connectorType\n    * <pre>Example: /{cluster}/connectors/{connectorType}/lock</pre>\n    * @param cluster Brooklin cluster name\n    * @param connectorType Connector\n-  \\   */\n+   * @return datastream task lock root\n+   */\n   public static String datastreamTaskLockRoot(String cluster, String connectorType) {\n     return String.format(DATASTREAM_TASK_LOCK_ROOT, cluster, connectorType).replaceAll(\"//\", \"/\");\n   }\n \n+  /**\n+   * Get the ZooKeeper znode for a specific datastream task's lock prefix\n+   * The lock is ephemeral node and it should not be stored under task node", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY1NDk1MQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477654951", "bodyText": "Sorry to bug you again about this again, but I still see mention of ephemeral nodes ...", "author": "somandal", "createdAt": "2020-08-26T23:13:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg2MDAwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg5NTAyOA==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476895028", "bodyText": "Let's rephrase this a bit? Feel free to modify the suggestion below, I'm just trying to lay out a suggested flow:\n\nIdentify orphan connector task locks for which there is no corresponding connector task node present and schedule their clean up after the debounce timer. Lock cleanup must be scheduled after a debounce timer because:\n\nThere is no easy way to know if the task is still running even though it is [un/re]assigned\nAll tasks which are unassigned must be stopped within a fixed amount of time which is <= debounce timer\n\nThus waiting for the debounce timer gives the guarantee that reassigned/dead tasks have actually stopped.", "author": "somandal", "createdAt": "2020-08-26T00:05:00Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg5NjU2OQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476896569", "bodyText": "nit: the  leader get ->  the leader gets", "author": "somandal", "createdAt": "2020-08-26T00:06:31Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg5NzM3NQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476897375", "bodyText": "nit: remove 'Boolean' since the type is implicit in the function definition.", "author": "somandal", "createdAt": "2020-08-26T00:07:23Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg5ODkwNQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476898905", "bodyText": "Just curious, I noticed that you spawn a separate thread to handle orphan lock cleanup, whereas you don't do the same for cleaning up orphan tasks. Any reason for that? Can both be done on separate threads?\nAlso, shall we add a log here indicating that this is being skipped?", "author": "somandal", "createdAt": "2020-08-26T00:09:40Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzUyNzI3Mg==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477527272", "bodyText": "The orphan task node can be cleaned up immediately, but task lock cannot be, since we want to wait for debounce timer.", "author": "vmaheshw", "createdAt": "2020-08-26T19:07:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg5ODkwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY1ODA1Nw==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477658057", "bodyText": "Thanks for explaining, makes sense.", "author": "somandal", "createdAt": "2020-08-26T23:15:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg5ODkwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkwODM0NQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476908345", "bodyText": "nit: should we also print cleanUpOrphanTaskLocksInConnector? If not here, then maybe at the start of the function or the caller should print this", "author": "somandal", "createdAt": "2020-08-26T00:23:43Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      return 0;\n+    }\n+\n+    Set<DatastreamTask> validTasks = getDatastreamTasks();\n+    List<String> allConnectors = getAllConnectors();\n+\n+    int orphanCount = 0;\n+    for (String connector : allConnectors) {\n+      Map<String, Set<String>> locksByTaskPrefix = getAllConnectorTaskLocks(connector);\n+\n+      Set<String> validTaskNamesSet = validTasks.stream()\n+          .filter(x -> x.getConnectorType().equals(connector))\n+          .map(DatastreamTask::getDatastreamTaskName)\n+          .collect(Collectors.toSet());\n+\n+      List<String> orphanLockList = new ArrayList<>();\n+\n+      locksByTaskPrefix.forEach((taskPrefix, taskList) -> {\n+        // if all the task locks in the task prefix needs to be cleaned up, delete the task prefix node as well.\n+        AtomicBoolean deleteTaskPrefixNode = new AtomicBoolean(true);\n+        taskList.forEach(taskName -> {\n+          if (!validTaskNamesSet.contains(taskName)) {\n+            orphanLockList.add(KeyBuilder.datastreamTaskLock(_cluster, connector, taskPrefix, taskName));\n+          } else {\n+            deleteTaskPrefixNode.set(false);\n+          }\n+        });\n+\n+        if (deleteTaskPrefixNode.get()) {\n+          orphanLockList.add(KeyBuilder.datastreamTaskLockPrefix(_cluster, connector, taskPrefix));\n+        }\n+      });\n+\n+      if (orphanLockList.size() > 0) {\n+        LOG.warn(\"Found orphan task locks: {} in connector: {}\", orphanLockList, connector);", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkwOTc1Mw==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476909753", "bodyText": "You can potentially move this out of the for loop by pre-grouping connector -> valid task lists:\nMap<String, Set>, where the key is the connector, and Set is the list of valid tasks for that connector. We won't have to walk the full set of validTasks for every connector.", "author": "somandal", "createdAt": "2020-08-26T00:25:47Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      return 0;\n+    }\n+\n+    Set<DatastreamTask> validTasks = getDatastreamTasks();\n+    List<String> allConnectors = getAllConnectors();\n+\n+    int orphanCount = 0;\n+    for (String connector : allConnectors) {\n+      Map<String, Set<String>> locksByTaskPrefix = getAllConnectorTaskLocks(connector);\n+\n+      Set<String> validTaskNamesSet = validTasks.stream()\n+          .filter(x -> x.getConnectorType().equals(connector))\n+          .map(DatastreamTask::getDatastreamTaskName)\n+          .collect(Collectors.toSet());", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzUyNzU4OQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477527589", "bodyText": "Did the optimization.", "author": "vmaheshw", "createdAt": "2020-08-26T19:07:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkwOTc1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkxNTk5MA==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476915990", "bodyText": "Feel free to ignore this, but wondering if we can simplify this a bit. Something along the lines of:\nint originalTaskListSize = taskList.size()\ntaskList.removeAll(validTaskNamesSet)   // make sure the types are compatible to do this\n\n// Only invalid tasks are left behind\ntaskList.forEach(taskName -> {\n     orphanLockList.add(KeyBuilder.datastreamTaskLock(_cluster, connector, taskPrefix, taskName));\n});\n\nif  (taskList.size() == originalTaskListSize) {\n    // None of the tasks are valid, thus we can safely remove the prefix node\n    orphanLockList.add(KeyBuilder.datastreamTaskLockPrefix(_cluster, connector, taskPrefix));\n}", "author": "somandal", "createdAt": "2020-08-26T00:35:41Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      return 0;\n+    }\n+\n+    Set<DatastreamTask> validTasks = getDatastreamTasks();\n+    List<String> allConnectors = getAllConnectors();\n+\n+    int orphanCount = 0;\n+    for (String connector : allConnectors) {\n+      Map<String, Set<String>> locksByTaskPrefix = getAllConnectorTaskLocks(connector);\n+\n+      Set<String> validTaskNamesSet = validTasks.stream()\n+          .filter(x -> x.getConnectorType().equals(connector))\n+          .map(DatastreamTask::getDatastreamTaskName)\n+          .collect(Collectors.toSet());\n+\n+      List<String> orphanLockList = new ArrayList<>();\n+\n+      locksByTaskPrefix.forEach((taskPrefix, taskList) -> {\n+        // if all the task locks in the task prefix needs to be cleaned up, delete the task prefix node as well.\n+        AtomicBoolean deleteTaskPrefixNode = new AtomicBoolean(true);\n+        taskList.forEach(taskName -> {\n+          if (!validTaskNamesSet.contains(taskName)) {\n+            orphanLockList.add(KeyBuilder.datastreamTaskLock(_cluster, connector, taskPrefix, taskName));\n+          } else {\n+            deleteTaskPrefixNode.set(false);\n+          }\n+        });\n+\n+        if (deleteTaskPrefixNode.get()) {\n+          orphanLockList.add(KeyBuilder.datastreamTaskLockPrefix(_cluster, connector, taskPrefix));\n+        }", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzUyODQxNw==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477528417", "bodyText": "I initially thought of doing this but decided not to do it, since I did not want to flatten the task list and keep it in the similar structure. Also, we need taskPrefix which we will not get from the String.", "author": "vmaheshw", "createdAt": "2020-08-26T19:09:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkxNTk5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY1OTI4Mg==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477659282", "bodyText": "Sure", "author": "somandal", "createdAt": "2020-08-26T23:17:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkxNTk5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkyMDE1Mg==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476920152", "bodyText": "nit: reword to:\nWait for the task lock to be released and delete it if the lock is held by a dead owner", "author": "somandal", "createdAt": "2020-08-26T00:41:52Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      return 0;\n+    }\n+\n+    Set<DatastreamTask> validTasks = getDatastreamTasks();\n+    List<String> allConnectors = getAllConnectors();\n+\n+    int orphanCount = 0;\n+    for (String connector : allConnectors) {\n+      Map<String, Set<String>> locksByTaskPrefix = getAllConnectorTaskLocks(connector);\n+\n+      Set<String> validTaskNamesSet = validTasks.stream()\n+          .filter(x -> x.getConnectorType().equals(connector))\n+          .map(DatastreamTask::getDatastreamTaskName)\n+          .collect(Collectors.toSet());\n+\n+      List<String> orphanLockList = new ArrayList<>();\n+\n+      locksByTaskPrefix.forEach((taskPrefix, taskList) -> {\n+        // if all the task locks in the task prefix needs to be cleaned up, delete the task prefix node as well.\n+        AtomicBoolean deleteTaskPrefixNode = new AtomicBoolean(true);\n+        taskList.forEach(taskName -> {\n+          if (!validTaskNamesSet.contains(taskName)) {\n+            orphanLockList.add(KeyBuilder.datastreamTaskLock(_cluster, connector, taskPrefix, taskName));\n+          } else {\n+            deleteTaskPrefixNode.set(false);\n+          }\n+        });\n+\n+        if (deleteTaskPrefixNode.get()) {\n+          orphanLockList.add(KeyBuilder.datastreamTaskLockPrefix(_cluster, connector, taskPrefix));\n+        }\n+      });\n+\n+      if (orphanLockList.size() > 0) {\n+        LOG.warn(\"Found orphan task locks: {} in connector: {}\", orphanLockList, connector);\n+        if (cleanUpOrphanTaskLocksInConnector) {\n+          _finalOrphanLockList.addAll(orphanLockList);\n+        }\n+        orphanCount += orphanLockList.size();\n+      }\n+    }\n+\n+    if (cleanUpOrphanTaskLocksInConnector) {\n+      // waiting for the debounce time to ensure that the task thread should stop processing by then.\n+      _orphanLockCleanupFuture = _scheduledExecutorServiceOrphanLockCleanup.schedule(this::cleanUpOrphanLocks,\n+          _debounceTimerMs, TimeUnit.MILLISECONDS);\n+    }\n+    return orphanCount;\n+  }\n+\n+  private void cleanUpOrphanLocks() {\n+    _finalOrphanLockList.forEach(t -> {\n+      LOG.info(\"Deleting task lock node {}\", t);\n+      _zkclient.delete(t);\n+    });\n+    _finalOrphanLockList.clear();\n+  }\n+\n+  /*\n+   * wait for the task lock to release and delete if the lock is owned by dead owner", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkyNDQ5Mw==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476924493", "bodyText": "What if this connect() races with the ZkClient's internal connect logic that you mentioned earlier? Can't this cause session leaks then (provided we are planning to remove the 'disconnect()' on connect that we currently have)?", "author": "somandal", "createdAt": "2020-08-26T00:48:10Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1613,7 +1757,10 @@ private void scheduleExpiryTimerAfterSessionTimeout() {\n   @VisibleForTesting\n   void onSessionExpired() {\n     LOG.error(\"Zookeeper session expired.\");\n+    //cancel the lock clean up\n+    _orphanLockCleanupFuture.cancel(true);\n     onBecomeFollower();\n+    connect();", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzUyOTUxMw==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477529513", "bodyText": "This is a temporary change. This is to make sure that we keep the execution path same till we dont fix the connect/disconnect. What will happen is after session expiry, it will try to disconnect and the zkThread will die and the instance will not try to join the cluster. This will be removed when we are ready with handling new session. Most of the things are already covered. Handling new session should be a small change, but I want to keep it separate.", "author": "vmaheshw", "createdAt": "2020-08-26T19:11:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkyNDQ5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY3MDgyNA==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477670824", "bodyText": "Got it. In your other PR to bring tasks down, I noticed you had a comment explaining that this is temporary. Want to add it here? Or add this as an action item in some document where you're tracking all the changes needed. Don't want to lose sight of these things.", "author": "somandal", "createdAt": "2020-08-26T23:27:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkyNDQ5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkyNDY0NQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476924645", "bodyText": "nit: space after '//'", "author": "somandal", "createdAt": "2020-08-26T00:48:24Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1613,7 +1757,10 @@ private void scheduleExpiryTimerAfterSessionTimeout() {\n   @VisibleForTesting\n   void onSessionExpired() {\n     LOG.error(\"Zookeeper session expired.\");\n+    //cancel the lock clean up", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkyNjg5Mw==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476926893", "bodyText": "Just to validate, the list of valid DatastreamTasks cannot change between when we get this list here, and when we check for valid tasks under the lock node, right? i.e. no LEADER_DO_ASSIGNMENT can run in parallel with this?", "author": "somandal", "createdAt": "2020-08-26T00:51:51Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      return 0;\n+    }\n+\n+    Set<DatastreamTask> validTasks = getDatastreamTasks();", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzUyOTg2MA==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477529860", "bodyText": "Ideally, it does not matter, since the task nodes deleted (marked as orphan) once will not get re-created.", "author": "vmaheshw", "createdAt": "2020-08-26T19:11:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkyNjg5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY4NjM4NQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477686385", "bodyText": "Got it, thanks!", "author": "somandal", "createdAt": "2020-08-26T23:38:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkyNjg5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkyNzg2OA==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476927868", "bodyText": "remove this comment, we don't need to release any lock here", "author": "somandal", "createdAt": "2020-08-26T00:53:24Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      return 0;\n+    }\n+\n+    Set<DatastreamTask> validTasks = getDatastreamTasks();\n+    List<String> allConnectors = getAllConnectors();\n+\n+    int orphanCount = 0;\n+    for (String connector : allConnectors) {\n+      Map<String, Set<String>> locksByTaskPrefix = getAllConnectorTaskLocks(connector);\n+\n+      Set<String> validTaskNamesSet = validTasks.stream()\n+          .filter(x -> x.getConnectorType().equals(connector))\n+          .map(DatastreamTask::getDatastreamTaskName)\n+          .collect(Collectors.toSet());\n+\n+      List<String> orphanLockList = new ArrayList<>();\n+\n+      locksByTaskPrefix.forEach((taskPrefix, taskList) -> {\n+        // if all the task locks in the task prefix needs to be cleaned up, delete the task prefix node as well.\n+        AtomicBoolean deleteTaskPrefixNode = new AtomicBoolean(true);\n+        taskList.forEach(taskName -> {\n+          if (!validTaskNamesSet.contains(taskName)) {\n+            orphanLockList.add(KeyBuilder.datastreamTaskLock(_cluster, connector, taskPrefix, taskName));\n+          } else {\n+            deleteTaskPrefixNode.set(false);\n+          }\n+        });\n+\n+        if (deleteTaskPrefixNode.get()) {\n+          orphanLockList.add(KeyBuilder.datastreamTaskLockPrefix(_cluster, connector, taskPrefix));\n+        }\n+      });\n+\n+      if (orphanLockList.size() > 0) {\n+        LOG.warn(\"Found orphan task locks: {} in connector: {}\", orphanLockList, connector);\n+        if (cleanUpOrphanTaskLocksInConnector) {\n+          _finalOrphanLockList.addAll(orphanLockList);\n+        }\n+        orphanCount += orphanLockList.size();\n+      }\n+    }\n+\n+    if (cleanUpOrphanTaskLocksInConnector) {\n+      // waiting for the debounce time to ensure that the task thread should stop processing by then.\n+      _orphanLockCleanupFuture = _scheduledExecutorServiceOrphanLockCleanup.schedule(this::cleanUpOrphanLocks,\n+          _debounceTimerMs, TimeUnit.MILLISECONDS);\n+    }\n+    return orphanCount;\n+  }\n+\n+  private void cleanUpOrphanLocks() {\n+    _finalOrphanLockList.forEach(t -> {\n+      LOG.info(\"Deleting task lock node {}\", t);\n+      _zkclient.delete(t);\n+    });\n+    _finalOrphanLockList.clear();\n+  }\n+\n+  /*\n+   * wait for the task lock to release and delete if the lock is owned by dead owner\n+   */\n+  private void waitForTaskReleaseOrForceIfOwnerIsDead(DatastreamTask task, long timeoutMs, String lockPath) {\n+    boolean deadOwner = false;\n+    if (_zkclient.exists(lockPath)) {\n+      String owner = _zkclient.readData(lockPath, true);\n+      if (owner != null && owner.equals(_instanceName)) {\n+        LOG.info(\"{} already owns the lock on {}\", _instanceName, task);\n+        //release the lock.", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkzNjA5Mg==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476936092", "bodyText": "I'm in two minds about this. Feel it's just better to enforce via a Validate that the timeoutMs must be >= the debounceTimerMs. If we decide not to enforce it, then we should clearly document the difference in the behavior in the comment above the class so that users of this API are aware of the behavior differences.\nAlso, can you rephrase:\nOnly try to identify dead owners of the task lock if the timeout is greater than the debounce timer.", "author": "somandal", "createdAt": "2020-08-26T01:05:13Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      return 0;\n+    }\n+\n+    Set<DatastreamTask> validTasks = getDatastreamTasks();\n+    List<String> allConnectors = getAllConnectors();\n+\n+    int orphanCount = 0;\n+    for (String connector : allConnectors) {\n+      Map<String, Set<String>> locksByTaskPrefix = getAllConnectorTaskLocks(connector);\n+\n+      Set<String> validTaskNamesSet = validTasks.stream()\n+          .filter(x -> x.getConnectorType().equals(connector))\n+          .map(DatastreamTask::getDatastreamTaskName)\n+          .collect(Collectors.toSet());\n+\n+      List<String> orphanLockList = new ArrayList<>();\n+\n+      locksByTaskPrefix.forEach((taskPrefix, taskList) -> {\n+        // if all the task locks in the task prefix needs to be cleaned up, delete the task prefix node as well.\n+        AtomicBoolean deleteTaskPrefixNode = new AtomicBoolean(true);\n+        taskList.forEach(taskName -> {\n+          if (!validTaskNamesSet.contains(taskName)) {\n+            orphanLockList.add(KeyBuilder.datastreamTaskLock(_cluster, connector, taskPrefix, taskName));\n+          } else {\n+            deleteTaskPrefixNode.set(false);\n+          }\n+        });\n+\n+        if (deleteTaskPrefixNode.get()) {\n+          orphanLockList.add(KeyBuilder.datastreamTaskLockPrefix(_cluster, connector, taskPrefix));\n+        }\n+      });\n+\n+      if (orphanLockList.size() > 0) {\n+        LOG.warn(\"Found orphan task locks: {} in connector: {}\", orphanLockList, connector);\n+        if (cleanUpOrphanTaskLocksInConnector) {\n+          _finalOrphanLockList.addAll(orphanLockList);\n+        }\n+        orphanCount += orphanLockList.size();\n+      }\n+    }\n+\n+    if (cleanUpOrphanTaskLocksInConnector) {\n+      // waiting for the debounce time to ensure that the task thread should stop processing by then.\n+      _orphanLockCleanupFuture = _scheduledExecutorServiceOrphanLockCleanup.schedule(this::cleanUpOrphanLocks,\n+          _debounceTimerMs, TimeUnit.MILLISECONDS);\n+    }\n+    return orphanCount;\n+  }\n+\n+  private void cleanUpOrphanLocks() {\n+    _finalOrphanLockList.forEach(t -> {\n+      LOG.info(\"Deleting task lock node {}\", t);\n+      _zkclient.delete(t);\n+    });\n+    _finalOrphanLockList.clear();\n+  }\n+\n+  /*\n+   * wait for the task lock to release and delete if the lock is owned by dead owner\n+   */\n+  private void waitForTaskReleaseOrForceIfOwnerIsDead(DatastreamTask task, long timeoutMs, String lockPath) {\n+    boolean deadOwner = false;\n+    if (_zkclient.exists(lockPath)) {\n+      String owner = _zkclient.readData(lockPath, true);\n+      if (owner != null && owner.equals(_instanceName)) {\n+        LOG.info(\"{} already owns the lock on {}\", _instanceName, task);\n+        //release the lock.\n+        return;\n+      }\n+\n+      long waitTimeout = timeoutMs;\n+      // if the timeout is greater than debounce timer, only then identify and clean the task lock.", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzUzMDQxOA==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477530418", "bodyText": "I also have the same feeling that should we make this timeout requirement greater than debounceTimer. Keeping this open for discussion, but wanted to add the support in case we decide the other way to not tie debounceTimer and acquire timer.", "author": "vmaheshw", "createdAt": "2020-08-26T19:13:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkzNjA5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY4ODQ3NQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477688475", "bodyText": "Sure, let's see what @ahmedahamid thinks about this API and make a call either way.", "author": "somandal", "createdAt": "2020-08-26T23:39:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkzNjA5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk0NTE3OQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476945179", "bodyText": "shouldn't we check for owner != null here too?", "author": "somandal", "createdAt": "2020-08-26T01:18:57Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      return 0;\n+    }\n+\n+    Set<DatastreamTask> validTasks = getDatastreamTasks();\n+    List<String> allConnectors = getAllConnectors();\n+\n+    int orphanCount = 0;\n+    for (String connector : allConnectors) {\n+      Map<String, Set<String>> locksByTaskPrefix = getAllConnectorTaskLocks(connector);\n+\n+      Set<String> validTaskNamesSet = validTasks.stream()\n+          .filter(x -> x.getConnectorType().equals(connector))\n+          .map(DatastreamTask::getDatastreamTaskName)\n+          .collect(Collectors.toSet());\n+\n+      List<String> orphanLockList = new ArrayList<>();\n+\n+      locksByTaskPrefix.forEach((taskPrefix, taskList) -> {\n+        // if all the task locks in the task prefix needs to be cleaned up, delete the task prefix node as well.\n+        AtomicBoolean deleteTaskPrefixNode = new AtomicBoolean(true);\n+        taskList.forEach(taskName -> {\n+          if (!validTaskNamesSet.contains(taskName)) {\n+            orphanLockList.add(KeyBuilder.datastreamTaskLock(_cluster, connector, taskPrefix, taskName));\n+          } else {\n+            deleteTaskPrefixNode.set(false);\n+          }\n+        });\n+\n+        if (deleteTaskPrefixNode.get()) {\n+          orphanLockList.add(KeyBuilder.datastreamTaskLockPrefix(_cluster, connector, taskPrefix));\n+        }\n+      });\n+\n+      if (orphanLockList.size() > 0) {\n+        LOG.warn(\"Found orphan task locks: {} in connector: {}\", orphanLockList, connector);\n+        if (cleanUpOrphanTaskLocksInConnector) {\n+          _finalOrphanLockList.addAll(orphanLockList);\n+        }\n+        orphanCount += orphanLockList.size();\n+      }\n+    }\n+\n+    if (cleanUpOrphanTaskLocksInConnector) {\n+      // waiting for the debounce time to ensure that the task thread should stop processing by then.\n+      _orphanLockCleanupFuture = _scheduledExecutorServiceOrphanLockCleanup.schedule(this::cleanUpOrphanLocks,\n+          _debounceTimerMs, TimeUnit.MILLISECONDS);\n+    }\n+    return orphanCount;\n+  }\n+\n+  private void cleanUpOrphanLocks() {\n+    _finalOrphanLockList.forEach(t -> {\n+      LOG.info(\"Deleting task lock node {}\", t);\n+      _zkclient.delete(t);\n+    });\n+    _finalOrphanLockList.clear();\n+  }\n+\n+  /*\n+   * wait for the task lock to release and delete if the lock is owned by dead owner\n+   */\n+  private void waitForTaskReleaseOrForceIfOwnerIsDead(DatastreamTask task, long timeoutMs, String lockPath) {\n+    boolean deadOwner = false;\n+    if (_zkclient.exists(lockPath)) {\n+      String owner = _zkclient.readData(lockPath, true);\n+      if (owner != null && owner.equals(_instanceName)) {\n+        LOG.info(\"{} already owns the lock on {}\", _instanceName, task);\n+        //release the lock.\n+        return;\n+      }\n+\n+      long waitTimeout = timeoutMs;\n+      // if the timeout is greater than debounce timer, only then identify and clean the task lock.\n+      if (timeoutMs >= _debounceTimerMs) {\n+        // check if the owner is dead.\n+        if (_liveInstancesProvider != null && !getLiveInstances().contains(owner)) {", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzUzMDk2Ng==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477530966", "bodyText": "Ideally, this node is never created without owner, its only created and deleted, so owner will never be null. But, even if it is null, it can be marked as dead owner since there is no owner on the lock.", "author": "vmaheshw", "createdAt": "2020-08-26T19:14:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk0NTE3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY2NzAxNw==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477667017", "bodyText": "Does the getLiveInstances() list type allow nulls? Otherwise contains() throws a NullPointerException.\n\n/**\n * Returns <tt>true</tt> if this list contains the specified element.\n * More formally, returns <tt>true</tt> if and only if this list contains\n * at least one element <tt>e</tt> such that\n * <tt>(o==null&nbsp;?&nbsp;e==null&nbsp;:&nbsp;o.equals(e))</tt>.\n *\n * @param o element whose presence in this list is to be tested\n * @return <tt>true</tt> if this list contains the specified element\n * @throws ClassCastException if the type of the specified element\n *         is incompatible with this list\n * (<a href=\"Collection.html#optional-restrictions\">optional</a>)\n * @throws NullPointerException if the specified element is null and this\n *         list does not permit null elements\n * (<a href=\"Collection.html#optional-restrictions\">optional</a>)\n */\nboolean contains(Object o);", "author": "somandal", "createdAt": "2020-08-26T23:24:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk0NTE3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk0ODA1Ng==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476948056", "bodyText": "Should we add a warn log indicating the owner changed underneath us?", "author": "somandal", "createdAt": "2020-08-26T01:23:11Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1059,130 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Set<DatastreamTask> getDatastreamTasks() {\n+    return getAllAssignedDatastreamTasks().values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce timer. There is no way for the leader to know that the task corresponding to the\n+   * lock has stopped or not. It makes sense to clear it after the debounce timer.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader get elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector Boolean whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      return 0;\n+    }\n+\n+    Set<DatastreamTask> validTasks = getDatastreamTasks();\n+    List<String> allConnectors = getAllConnectors();\n+\n+    int orphanCount = 0;\n+    for (String connector : allConnectors) {\n+      Map<String, Set<String>> locksByTaskPrefix = getAllConnectorTaskLocks(connector);\n+\n+      Set<String> validTaskNamesSet = validTasks.stream()\n+          .filter(x -> x.getConnectorType().equals(connector))\n+          .map(DatastreamTask::getDatastreamTaskName)\n+          .collect(Collectors.toSet());\n+\n+      List<String> orphanLockList = new ArrayList<>();\n+\n+      locksByTaskPrefix.forEach((taskPrefix, taskList) -> {\n+        // if all the task locks in the task prefix needs to be cleaned up, delete the task prefix node as well.\n+        AtomicBoolean deleteTaskPrefixNode = new AtomicBoolean(true);\n+        taskList.forEach(taskName -> {\n+          if (!validTaskNamesSet.contains(taskName)) {\n+            orphanLockList.add(KeyBuilder.datastreamTaskLock(_cluster, connector, taskPrefix, taskName));\n+          } else {\n+            deleteTaskPrefixNode.set(false);\n+          }\n+        });\n+\n+        if (deleteTaskPrefixNode.get()) {\n+          orphanLockList.add(KeyBuilder.datastreamTaskLockPrefix(_cluster, connector, taskPrefix));\n+        }\n+      });\n+\n+      if (orphanLockList.size() > 0) {\n+        LOG.warn(\"Found orphan task locks: {} in connector: {}\", orphanLockList, connector);\n+        if (cleanUpOrphanTaskLocksInConnector) {\n+          _finalOrphanLockList.addAll(orphanLockList);\n+        }\n+        orphanCount += orphanLockList.size();\n+      }\n+    }\n+\n+    if (cleanUpOrphanTaskLocksInConnector) {\n+      // waiting for the debounce time to ensure that the task thread should stop processing by then.\n+      _orphanLockCleanupFuture = _scheduledExecutorServiceOrphanLockCleanup.schedule(this::cleanUpOrphanLocks,\n+          _debounceTimerMs, TimeUnit.MILLISECONDS);\n+    }\n+    return orphanCount;\n+  }\n+\n+  private void cleanUpOrphanLocks() {\n+    _finalOrphanLockList.forEach(t -> {\n+      LOG.info(\"Deleting task lock node {}\", t);\n+      _zkclient.delete(t);\n+    });\n+    _finalOrphanLockList.clear();\n+  }\n+\n+  /*\n+   * wait for the task lock to release and delete if the lock is owned by dead owner\n+   */\n+  private void waitForTaskReleaseOrForceIfOwnerIsDead(DatastreamTask task, long timeoutMs, String lockPath) {\n+    boolean deadOwner = false;\n+    if (_zkclient.exists(lockPath)) {\n+      String owner = _zkclient.readData(lockPath, true);\n+      if (owner != null && owner.equals(_instanceName)) {\n+        LOG.info(\"{} already owns the lock on {}\", _instanceName, task);\n+        //release the lock.\n+        return;\n+      }\n+\n+      long waitTimeout = timeoutMs;\n+      // if the timeout is greater than debounce timer, only then identify and clean the task lock.\n+      if (timeoutMs >= _debounceTimerMs) {\n+        // check if the owner is dead.\n+        if (_liveInstancesProvider != null && !getLiveInstances().contains(owner)) {\n+          LOG.info(\"dead owner {} found for the lock on the task {}\", owner, task.getDatastreamTaskName());\n+          deadOwner = true;\n+        } else {\n+          waitForTaskRelease(task, timeoutMs - _debounceTimerMs, lockPath);\n+          if (!_zkclient.exists(lockPath)) {\n+            return;\n+          }\n+          owner = _zkclient.readData(lockPath, true);\n+          if (owner != null && _liveInstancesProvider != null && !getLiveInstances().contains(owner)) {\n+            LOG.info(\"dead owner {} found for the lock on the task {} after waiting {} ms\",\n+                owner, task.getDatastreamTaskName(), timeoutMs - _debounceTimerMs);\n+            deadOwner = true;\n+          }\n+        }\n+        waitTimeout = _debounceTimerMs;\n+      }\n+\n+      waitForTaskRelease(task, waitTimeout, lockPath);\n+      if (deadOwner && _zkclient.exists(lockPath)) {\n+        String tempOwner = _zkclient.readData(lockPath, true);\n+        if (tempOwner != null && tempOwner.equals(owner)) {", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk4MjkyMQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476982921", "bodyText": "Why have we changed this to readData() instead of ensureReadData()? Looks like ensureReadData() handles the situation where the ZK node is being modified by another process.\nDue to changing this to readData(), we need to check for 'null' owners everywhere, and it's not clear how 'null' owners will be handled. E.g. if throughout this whole call and the waitForTaskReleaseOrForceIfOwnerIsDead, if readData() returns 'null', does it mean we don't release the lock? Or is the expectation that eventually it will stabilize and we will retry?\nAdd a comment explaining why ensureReadData() here is not a good idea, if it really isn't a good idea?\nI also noticed that waitForTaskReleaseOrForceIfOwnerIsDead is already calling readData() and checking if the owner matches the instance in question. Can we directly call waitForTaskReleaseOrForceIfOwnerIsDead instead of doing this check here?  [see my comment where we call waitForTaskReleaseOrForceIfOwnerIsDead on waiting for dependencies]", "author": "somandal", "createdAt": "2020-08-26T02:15:02Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1082,34 +1219,38 @@ private void waitForTaskRelease(DatastreamTask task, long timeoutMs, String lock\n    * @see #releaseTask(DatastreamTaskImpl)\n    */\n   public void acquireTask(DatastreamTaskImpl task, Duration timeout) {\n-    _zkclient.ensurePath(KeyBuilder.datastreamTaskLockRoot(_cluster, task.getConnectorType()));\n-    String lockPath = KeyBuilder.datastreamTaskLock(_cluster, task.getConnectorType(), task.getDatastreamTaskName());\n+    _zkclient.ensurePath(KeyBuilder.datastreamTaskLockPrefix(_cluster, task.getConnectorType(), task.getTaskPrefix()));\n+    String lockPath = KeyBuilder.datastreamTaskLock(_cluster, task.getConnectorType(), task.getTaskPrefix(), task.getDatastreamTaskName());\n     String owner = null;\n     if (_zkclient.exists(lockPath)) {\n-      owner = _zkclient.ensureReadData(lockPath);\n-      if (owner.equals(_instanceName)) {\n+      owner = _zkclient.readData(lockPath, true);", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzUzMjExMQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477532111", "bodyText": "The reason to change from readData to ensureReadData is earlier it made sense, since the node was ephemeral and the node creation and deletion was happening in one instance code path. This will not be beneficial in case of persistent and multiple instances can try to delete (self-delete, or anyone trying to acquire lock after debounce timer.) I thought the api name was self-explainatary with the parameter.", "author": "vmaheshw", "createdAt": "2020-08-26T19:16:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk4MjkyMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY2ODY4OQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477668689", "bodyText": "Thanks for explaining offline, make sense to me now. Since leaving it as ensureReadData() causes tests to fail I'm okay with either adding a comment or leaving it out to explain why readData() is used here.", "author": "somandal", "createdAt": "2020-08-26T23:25:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk4MjkyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk4NjI1NQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476986255", "bodyText": "Is it necessary to validate this again? Won't create thrown an exception if the node already exists?\nZkConnection comments (please do correct me if I'm looking at the wrong thing):\n\n * If a node with the same actual path already exists in the ZooKeeper, a\n * KeeperException with error code KeeperException.NodeExists will be\n * thrown. Note that since a different actual path is used for each\n * invocation of creating sequential node with the same path argument, the\n * call will never throw \"file exists\" KeeperException.", "author": "somandal", "createdAt": "2020-08-26T02:19:57Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1082,34 +1219,38 @@ private void waitForTaskRelease(DatastreamTask task, long timeoutMs, String lock\n    * @see #releaseTask(DatastreamTaskImpl)\n    */\n   public void acquireTask(DatastreamTaskImpl task, Duration timeout) {\n-    _zkclient.ensurePath(KeyBuilder.datastreamTaskLockRoot(_cluster, task.getConnectorType()));\n-    String lockPath = KeyBuilder.datastreamTaskLock(_cluster, task.getConnectorType(), task.getDatastreamTaskName());\n+    _zkclient.ensurePath(KeyBuilder.datastreamTaskLockPrefix(_cluster, task.getConnectorType(), task.getTaskPrefix()));\n+    String lockPath = KeyBuilder.datastreamTaskLock(_cluster, task.getConnectorType(), task.getTaskPrefix(), task.getDatastreamTaskName());\n     String owner = null;\n     if (_zkclient.exists(lockPath)) {\n-      owner = _zkclient.ensureReadData(lockPath);\n-      if (owner.equals(_instanceName)) {\n+      owner = _zkclient.readData(lockPath, true);\n+      if (owner != null && owner.equals(_instanceName)) {\n         LOG.info(\"{} already owns the lock on {}\", _instanceName, task);\n         return;\n       }\n \n-      waitForTaskRelease(task, timeout.toMillis(), lockPath);\n+      waitForTaskReleaseOrForceIfOwnerIsDead(task, timeout.toMillis(), lockPath);\n     }\n \n     if (!_zkclient.exists(lockPath)) {\n-      _zkclient.createEphemeral(lockPath, _instanceName);\n-      LOG.info(\"{} successfully acquired the lock on {}\", _instanceName, task);\n-    } else {\n-      String msg = String.format(\"%s failed to acquire task %s in %dms, current owner: %s\", _instanceName, task,\n-          timeout.toMillis(), owner);\n-      ErrorLogger.logAndThrowDatastreamRuntimeException(LOG, msg, null);\n+      _zkclient.create(lockPath, _instanceName, CreateMode.PERSISTENT);\n+      owner = _zkclient.readData(lockPath, true);\n+      if ((owner != null && owner.equals(_instanceName))) {", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzUzMjYxNg==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477532616", "bodyText": "Added it for some reason, which I cannot recall. Removing it for now and will add with reason once I remember.", "author": "vmaheshw", "createdAt": "2020-08-26T19:17:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk4NjI1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk5ODMxMQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r476998311", "bodyText": "Is it correct for us to check if we are the owner of the dependency and then assume we don't need to release anything here? waitForTaskReleaseOrForceIfOwnerIsDead does  the following check:\n    if (_zkclient.exists(lockPath)) {\n      String owner = _zkclient.readData(lockPath, true);\n      if (owner != null && owner.equals(_instanceName)) {\n        LOG.info(\"{} already owns the lock on {}\", _instanceName, task);\n        //release the lock.\n        return;\n      }\n\nJust because we are the owner of that lock doesn't mean it's okay to grab it, since it's a dependency task here. The dependency may have just been assigned to the same node. We should instead wait for the task lock to be released right?\nPerhaps the code snippet that checks if the instance is the owner in waitForTaskReleaseOrForceIfOwnerIsDead should be moved out of that function.", "author": "somandal", "createdAt": "2020-08-26T02:40:36Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1120,9 +1261,9 @@ public boolean checkIsTaskLocked(String connectorType, String taskName) {\n    */\n   public void waitForDependencies(DatastreamTaskImpl task, Duration timeout) {\n     task.getDependencies().forEach(previousTask -> {\n-        String lockPath = KeyBuilder.datastreamTaskLock(_cluster, task.getConnectorType(), previousTask);\n+      String lockPath = KeyBuilder.datastreamTaskLock(_cluster, task.getConnectorType(), task.getTaskPrefix(), previousTask);\n       if (_zkclient.exists(lockPath)) {\n-        waitForTaskRelease(task, timeout.toMillis(), lockPath);\n+        waitForTaskReleaseOrForceIfOwnerIsDead(task, timeout.toMillis(), lockPath);", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzUzMjk2Nw==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477532967", "bodyText": "Thanks for telling this case. I was not aware. I cleaned up and added a test to catch this case.", "author": "vmaheshw", "createdAt": "2020-08-26T19:17:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk5ODMxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAwMTI1NQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477001255", "bodyText": "Quick question, if a Session expiry happens, the _instanceName remains the same? Just wondering if we could have a case where we're trying to release the lock but an expiry + connect happened before we call this, creating a  new liveinstance node for this host. Will the task still be releasable?", "author": "somandal", "createdAt": "2020-08-26T02:51:13Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1218,20 +1359,23 @@ public void cleanUpPartitionMovement(String connectorType, String datastreamGrou\n    * @see #acquireTask(DatastreamTaskImpl, Duration)\n    */\n   public void releaseTask(DatastreamTaskImpl task) {\n-    String lockPath = KeyBuilder.datastreamTaskLock(_cluster, task.getConnectorType(), task.getDatastreamTaskName());\n+    String lockPath =\n+        KeyBuilder.datastreamTaskLock(_cluster, task.getConnectorType(), task.getTaskPrefix(), task.getDatastreamTaskName());\n     if (!_zkclient.exists(lockPath)) {\n-      LOG.info(\"There is no lock on {}\", task);\n+      LOG.info(\"There is no lock on {}-{}/{}\", task.getConnectorType(), task.getTaskPrefix(), task.getDatastreamTaskName());\n       return;\n     }\n \n     String owner = _zkclient.ensureReadData(lockPath);\n     if (!owner.equals(_instanceName)) {", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzUzMzU1Nw==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477533557", "bodyText": "That's true. But, we will take care of this when we change the live instance number during reconnection. I will mark TODO here so that we don't forget to fix it.", "author": "vmaheshw", "createdAt": "2020-08-26T19:19:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAwMTI1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY2OTQ5Nw==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477669497", "bodyText": "Sure, are you keep track of all these changes somewhere? Just so that they're not forgotten.", "author": "somandal", "createdAt": "2020-08-26T23:26:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAwMTI1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDkwODQ0Mg==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r480908442", "bodyText": "yes", "author": "vmaheshw", "createdAt": "2020-09-01T07:17:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAwMTI1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAwMjYwOQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477002609", "bodyText": "The comment where this _liveInstancesProvider is declared indicates that we only maintain this for the Leader. Can you fix that comment and add details about why we need this for all nodes?", "author": "somandal", "createdAt": "2020-08-26T02:56:19Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -236,6 +245,7 @@ public void connect() {\n     _zkclient = createZkClient();\n     _stateChangeListener = new ZkStateChangeListener();\n     _leaderElectionListener = new ZkLeaderElectionListener();\n+    _liveInstancesProvider = new ZkBackedLiveInstanceListProvider();", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAxMTQxMQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477011411", "bodyText": "nit: space after '//', and all the other // without space comments too", "author": "somandal", "createdAt": "2020-08-26T03:29:32Z", "path": "datastream-server/src/test/java/com/linkedin/datastream/server/zk/TestZkAdapter.java", "diffHunk": "@@ -808,7 +865,13 @@ public void testDeleteTasksWithPrefix() {\n     leftOverTasks = zkClient.getChildren(KeyBuilder.connector(testCluster, connectorType));\n     Assert.assertEquals(leftOverTasks.size(), 3);\n \n-    adapter.cleanUpOrphanConnectorTasks(true);\n+    //Verify orphan locks, lockTask is the only orphan task.", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAxMTY1NQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477011655", "bodyText": "Should we also add a test for dependency locks? Feel free to defer this to a later point in time or have me take it up.", "author": "somandal", "createdAt": "2020-08-26T03:30:24Z", "path": "datastream-server/src/test/java/com/linkedin/datastream/server/zk/TestZkAdapter.java", "diffHunk": "@@ -751,14 +755,66 @@ public void testZookeeperSessionExpiry() throws InterruptedException {\n     Mockito.verify(adapter, Mockito.times(1)).onSessionExpired();\n   }\n \n+  @Test\n+  public void testZookeeperLockAcquire() throws InterruptedException {", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzUzNDYxMg==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477534612", "bodyText": "There is already an existing test for dependency lock. Please check and feel free to enhance it if you feel it does not cover everything.", "author": "vmaheshw", "createdAt": "2020-08-26T19:21:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAxMTY1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY3MjczMw==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477672733", "bodyText": "Oops didn't check the full file for tests. Thanks for pointing it out.", "author": "somandal", "createdAt": "2020-08-26T23:29:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAxMTY1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAxMzEwMw==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477013103", "bodyText": "Should this and other such getZkClient().exists() lines be an assert?", "author": "somandal", "createdAt": "2020-08-26T03:36:00Z", "path": "datastream-server/src/test/java/com/linkedin/datastream/server/zk/TestZkAdapter.java", "diffHunk": "@@ -751,14 +755,66 @@ public void testZookeeperSessionExpiry() throws InterruptedException {\n     Mockito.verify(adapter, Mockito.times(1)).onSessionExpired();\n   }\n \n+  @Test\n+  public void testZookeeperLockAcquire() throws InterruptedException {\n+    String testCluster = \"testLockAcquire\";\n+    String connectorType = \"connectorType\";\n+    //\n+    // start two ZkAdapters, which is corresponding to two Coordinator instances\n+    //\n+    ZkClientInterceptingAdapter adapter1 = createInterceptingZkAdapter(testCluster, 5000, ZK_DEBOUNCE_TIMER_MS * 10);\n+    adapter1.connect();\n+\n+    DatastreamTaskImpl task = new DatastreamTaskImpl();\n+    task.setId(\"3\");\n+    task.setConnectorType(connectorType);\n+    task.setZkAdapter(adapter1);\n+\n+    List<DatastreamTask> tasks = Collections.singletonList(task);\n+    updateInstanceAssignment(adapter1, adapter1.getInstanceName(), tasks);\n+\n+    LOG.info(\"Acquire from instance1 should succeed\");\n+    Duration timeout = Duration.ofSeconds(3);\n+    Assert.assertTrue(expectException(() -> adapter1.acquireTask(task, timeout), false));\n+    String owner = adapter1.getZkClient().readData(KeyBuilder.datastreamTaskLock(testCluster, task.getConnectorType(),\n+        task.getTaskPrefix(), task.getDatastreamTaskName()));\n+\n+    ZkClientInterceptingAdapter adapter2 = createInterceptingZkAdapter(testCluster, 5000, ZK_DEBOUNCE_TIMER_MS * 10);\n+    adapter2.connect();\n+    simulateSessionExpiration(adapter1);\n+\n+    Thread.sleep(1000);\n+    Assert.assertTrue(expectException(() -> adapter1._zkClient.waitUntilConnected(5, TimeUnit.SECONDS), false));\n+\n+    // adapter2 not able to acquire lock\n+    adapter2.getZkClient().exists(KeyBuilder.datastreamTaskLock(testCluster, task.getConnectorType(),", "originalCommit": "3fc04bb2d920774ae88df8dea886af59c3808eec", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "abf1d61f05842225cd8d218539e3a6acd3dd8e40", "url": "https://github.com/linkedin/brooklin/commit/abf1d61f05842225cd8d218539e3a6acd3dd8e40", "message": "Address fixes", "committedDate": "2020-08-26T19:05:03Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY5Mjc3OQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r477692779", "bodyText": "Can we print the value of cleanUpOrphanTaskLocksInConnector to know whether we are just identifying orphans vs. cleaning them up?", "author": "somandal", "createdAt": "2020-08-26T23:42:30Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -1046,6 +1062,135 @@ public int cleanUpOrphanConnectorTasks(boolean cleanUpOrphanTasksInConnector) {\n     return orphanCount;\n   }\n \n+  private Map<String, Set<String>> getDatastreamTaskNamesGroupedByConnector() {\n+    return getAllAssignedDatastreamTasks()\n+        .values()\n+        .stream()\n+        .flatMap(Collection::stream)\n+        .collect(groupingBy(DatastreamTask::getConnectorType, mapping(DatastreamTask::getDatastreamTaskName, toSet())));\n+  }\n+\n+  /**\n+   * Identify orphan connector task locks for which there is no corresponding connector task node present and schedule\n+   * the clean up after the debounce time.\n+   * Lock cleanup must be scheduled after a debounce timer because:\n+   *\n+   * a. There is no easy way to know if the task is still running even though it is [un/re]assigned.\n+   * b. All tasks which are unassigned must be stopped within a fixed amount of time which is <= debounce timer.\n+   *\n+   * Thus waiting for the debounce timer gives the guarantee that reassigned/dead tasks have actually stopped.\n+   *\n+   * NOTE: this should be called after the valid tasks have been reassigned or become safe to discard per\n+   * strategy requirement.\n+   *\n+   * This is a costly operation which involves getting all children of /cluster/connectors/lock from Zookeeper. So,\n+   * it should be called only once the leader gets elected and has finished the assignment and cleaned up dead tasks.\n+   * @param cleanUpOrphanTaskLocksInConnector whether orphan task locks should be removed from zookeeper or just\n+   *                                      print warning logs.\n+   * @return total orphan task locks identified/cleaned.\n+   */\n+  public int cleanUpOrphanConnectorTaskLocks(boolean cleanUpOrphanTaskLocksInConnector) {\n+    // do not perform this operation if not a leader or another operation is going on.\n+    if (!_isLeader || !_orphanLockCleanupFuture.isDone()) {\n+      LOG.info(\"Skipping cleanUpOrphanConnectorTaskLocks. isLeader: {} lock cleanup scheduled: {}\",\n+          _isLeader, _orphanLockCleanupFuture.isDone());\n+      return 0;\n+    }\n+\n+    LOG.info(\"cleanUpOrphanConnectorTaskLocks called\");", "originalCommit": "abf1d61f05842225cd8d218539e3a6acd3dd8e40", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDIxMTMzMQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r480211331", "bodyText": "nit: remove space after taskName", "author": "ahmedahamid", "createdAt": "2020-08-31T15:35:35Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/KeyBuilder.java", "diffHunk": "@@ -71,9 +71,14 @@ private KeyBuilder() {\n   private static final String DATASTREAM_TASK_LOCK_ROOT = CONNECTOR + \"/\" + DATASTREAM_TASK_LOCK_ROOT_NAME;\n \n   /**\n-   * Task lock node under connectorType/lock/{taskName}\n+   * Task lock node under connectorType/lock/{taskPrefix}\n    */\n-  private static final String DATASTREAM_TASK_LOCK = DATASTREAM_TASK_LOCK_ROOT + \"/%s\";\n+  private static final String DATASTREAM_TASK_LOCK_PREFIX = DATASTREAM_TASK_LOCK_ROOT + \"/%s\";\n+\n+  /**\n+   * Task lock node under connectorType/lock/{taskPrefix}/{taskName }", "originalCommit": "abf1d61f05842225cd8d218539e3a6acd3dd8e40", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDIzMjkwNg==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r480232906", "bodyText": "s/Scheduler/Thread", "author": "ahmedahamid", "createdAt": "2020-08-31T16:07:30Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -114,38 +119,43 @@\n \n   private final String _defaultTransportProviderName;\n \n+  private final ZkAdapterListener _listener;\n+  private final Random randomGenerator = new Random();\n   private final String _zkServers;\n   private final String _cluster;\n   private final int _sessionTimeoutMs;\n   private final int _connectionTimeoutMs;\n   private final int _operationRetryTimeoutMs;\n-  private ZkClient _zkclient;\n+  private final long _debounceTimerMs;\n \n+  private ZkClient _zkclient;\n   private String _instanceName;\n   private String _liveInstanceName;\n   private String _hostname;\n   private Set<String> _connectorTypes = new HashSet<>();\n \n   private volatile boolean _isLeader = false;\n-  private final ZkAdapterListener _listener;\n-\n   // the current znode this node is listening to\n   private String _currentSubscription = null;\n \n-  private final Random randomGenerator = new Random();\n-\n   private ZkLeaderElectionListener _leaderElectionListener = null;\n   private ZkBackedTaskListProvider _assignmentList = null;\n   private ZkStateChangeListener _stateChangeListener = null;\n+  private ZkBackedLiveInstanceListProvider _liveInstancesProvider = null;\n \n   // only the leader should maintain this list and listen to the changes of live instances\n   private ZkBackedDMSDatastreamList _datastreamList = null;\n-  private ZkBackedLiveInstanceListProvider _liveInstancesProvider = null;\n   private ZkTargetAssignmentProvider _targetAssignmentProvider = null;\n \n   // Cache all live DatastreamTasks per instance for assignment strategy\n   private Map<String, Set<DatastreamTask>> _liveTaskMap = new HashMap<>();\n \n+  // cleanup orphan lock in separate thread.\n+  private final ScheduledExecutorService _scheduledExecutorServiceOrphanLockCleanup = Executors.newScheduledThreadPool(1,\n+      new ThreadFactoryBuilder().setDaemon(true).setNameFormat(\"OrphanLockCleanupScheduler-%d\").build());", "originalCommit": "abf1d61f05842225cd8d218539e3a6acd3dd8e40", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU0Mjg5MQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r480542891", "bodyText": "s/x/prefix", "author": "ahmedahamid", "createdAt": "2020-09-01T01:19:34Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -827,6 +840,14 @@ public void addConnectorType(String connectorType) {\n     return new HashSet<>(_zkclient.getChildren(KeyBuilder.connector(_cluster, connector)));\n   }\n \n+  @VisibleForTesting\n+  Map<String, Set<String>> getAllConnectorTaskLocks(String connector) {\n+    return _zkclient.getChildren(KeyBuilder.datastreamTaskLockRoot(_cluster, connector))\n+        .stream()\n+        .collect(Collectors.toMap(Function.identity(),\n+            x -> new HashSet<>(_zkclient.getChildren(KeyBuilder.datastreamTaskLockPrefix(_cluster, connector, x)))));", "originalCommit": "abf1d61f05842225cd8d218539e3a6acd3dd8e40", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU0ODU2OQ==", "url": "https://github.com/linkedin/brooklin/pull/747#discussion_r480548569", "bodyText": "This method is going to make N roundtrips to ZK where N's the number of unique task prefixes in the cluster. This number is close to 1500 in one of our bigger change capture clusters atm. Just calling this out in case extended lock cleanup duration could cause issues.", "author": "ahmedahamid", "createdAt": "2020-09-01T01:24:52Z", "path": "datastream-server/src/main/java/com/linkedin/datastream/server/zk/ZkAdapter.java", "diffHunk": "@@ -827,6 +840,14 @@ public void addConnectorType(String connectorType) {\n     return new HashSet<>(_zkclient.getChildren(KeyBuilder.connector(_cluster, connector)));\n   }\n \n+  @VisibleForTesting\n+  Map<String, Set<String>> getAllConnectorTaskLocks(String connector) {", "originalCommit": "abf1d61f05842225cd8d218539e3a6acd3dd8e40", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "dbdb40d96665392013407eed193e27283f825e47", "url": "https://github.com/linkedin/brooklin/commit/dbdb40d96665392013407eed193e27283f825e47", "message": "Address fixes", "committedDate": "2020-09-01T07:16:00Z", "type": "commit"}]}