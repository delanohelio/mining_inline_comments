{"pr_number": 686, "pr_title": "Add diag support to report consumer groups and topic partitions", "pr_createdAt": "2020-02-19T22:00:25Z", "pr_url": "https://github.com/linkedin/brooklin/pull/686", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTc3NDk5MQ==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381774991", "bodyText": "unnecessary check; it would prolly take a custom List impl to return a negative size :)", "author": "ahmedahamid", "createdAt": "2020-02-20T05:40:23Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaConnector.java", "diffHunk": "@@ -463,6 +487,38 @@ public String reduce(String query, Map<String, String> responses) {\n     return null;\n   }\n \n+  private String reduceTopicPartitionStatsResponses(Map<String, String> responses) {\n+    Map<String, Map<String, Set<Integer>>> result = new HashMap<>();\n+\n+    responses.forEach((instance, json) -> {\n+      List<KafkaTopicPartitionStatsResponse> responseList;\n+      try {\n+        responseList = KafkaTopicPartitionStatsResponse.fromJson(json);\n+      } catch (Exception e) {\n+        _logger.error(\"Invalid response {} from instance {}\", json, instance);\n+        return;\n+      }\n+      if (responseList.size() >= 0) {", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTc3ODg0Mg==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381778842", "bodyText": "You can use either to improve the consumer group ID validation:\n// make sure you're importing org.apache.commons.lang3.StringUtils\n\n// This covers null/empty/all-whitespace\nStringUtils.isBlank(response.getConsumerGroupId())\n\n// This covers null/empty\nStringUtils.isEmpty(response.getConsumerGroupId())", "author": "ahmedahamid", "createdAt": "2020-02-20T05:48:06Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaConnector.java", "diffHunk": "@@ -463,6 +487,38 @@ public String reduce(String query, Map<String, String> responses) {\n     return null;\n   }\n \n+  private String reduceTopicPartitionStatsResponses(Map<String, String> responses) {\n+    Map<String, Map<String, Set<Integer>>> result = new HashMap<>();\n+\n+    responses.forEach((instance, json) -> {\n+      List<KafkaTopicPartitionStatsResponse> responseList;\n+      try {\n+        responseList = KafkaTopicPartitionStatsResponse.fromJson(json);\n+      } catch (Exception e) {\n+        _logger.error(\"Invalid response {} from instance {}\", json, instance);\n+        return;\n+      }\n+      if (responseList.size() >= 0) {\n+        responseList.forEach(response -> {\n+          if (response.getTopicPartitions() == null || response.getConsumerGroupId().isEmpty()) {", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTc4NTI3Mg==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381785272", "bodyText": "putIfAbsent() instantiates new HashMap() even if it's not needed. You can use computeIfAbsent() instead.\nThis block can be simplified since put/computeIfAbsent() returns the value.\nMap<String, Set<Integer>> topicPartitionsMap =\n    result.computeIfAbsent(response.getConsumerGroupId(), k -> new HashMap<>());\n\nSet<TopicPartition> topicPartitions = response.getTopicPartitions();\ntopicPartitions.forEach(topicPartition -> {\n  Set<Integer> partitions = \n      topicPartitionsMap.computeIfAbsent(topicPartition.topic(), k -> new HashSet<>());\n  partitions.add(topicPartition.partition());\n});", "author": "ahmedahamid", "createdAt": "2020-02-20T06:00:49Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaConnector.java", "diffHunk": "@@ -463,6 +487,38 @@ public String reduce(String query, Map<String, String> responses) {\n     return null;\n   }\n \n+  private String reduceTopicPartitionStatsResponses(Map<String, String> responses) {\n+    Map<String, Map<String, Set<Integer>>> result = new HashMap<>();\n+\n+    responses.forEach((instance, json) -> {\n+      List<KafkaTopicPartitionStatsResponse> responseList;\n+      try {\n+        responseList = KafkaTopicPartitionStatsResponse.fromJson(json);\n+      } catch (Exception e) {\n+        _logger.error(\"Invalid response {} from instance {}\", json, instance);\n+        return;\n+      }\n+      if (responseList.size() >= 0) {\n+        responseList.forEach(response -> {\n+          if (response.getTopicPartitions() == null || response.getConsumerGroupId().isEmpty()) {\n+            _logger.warn(\"Empty topic partition stats map from instance {}. Ignoring the result\", instance);\n+            return;\n+          }\n+          result.putIfAbsent(response.getConsumerGroupId(), new HashMap<>());\n+          Set<TopicPartition> topicPartitions = response.getTopicPartitions();\n+\n+          topicPartitions.forEach(topicPartition -> {\n+            Map<String, Set<Integer>> topicPartitionsMap = result.get(response.getConsumerGroupId());\n+            topicPartitionsMap.putIfAbsent(topicPartition.topic(), new HashSet<>());\n+            topicPartitionsMap.get(topicPartition.topic()).add(topicPartition.partition());\n+          });\n+        });\n+      }\n+    });", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTc4OTc4Mw==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381789783", "bodyText": "nit: omit result", "author": "ahmedahamid", "createdAt": "2020-02-20T06:09:57Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -916,4 +924,31 @@ public static String getTaskMetadataGroupId(DatastreamTask task, CommonConnector\n \n     return null;\n   }\n-}\n+\n+  /**\n+   *\n+   *  Gets the KafkaTopicPartition tracker\n+   */\n+  public KafkaTopicPartitionTracker getKafkaTopicPartitionTracker() {\n+    return _kafkaTopicPartitionTracker;\n+  }\n+\n+  /**\n+   * Get Kafka group ID of given task\n+   * @param task Task for which group ID is generated.\n+   * @param groupIdConstructor GroupIdConstructor to use for generating group ID.\n+   * @param consumerMetrics CommonConnectorMetrics to use for reporting errors.\n+   * @param logger Logger for logging information.\n+   */\n+  @VisibleForTesting\n+  public static String getKafkaGroupId(DatastreamTask task, GroupIdConstructor groupIdConstructor,\n+      CommonConnectorMetrics consumerMetrics, Logger logger) {\n+    try {\n+      String result = groupIdConstructor.getTaskGroupId(task, Optional.of(logger));", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTc5NzQ0Ng==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381797446", "bodyText": "unused", "author": "ahmedahamid", "createdAt": "2020-02-20T06:24:57Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaTopicPartitionTracker.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+\n+package com.linkedin.datastream.connectors.kafka;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.jetbrains.annotations.NotNull;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * KafkaTopicPartitionTracker contains information about consumer groups, topic partitions and\n+ * their consumer offsets.\n+ *\n+ * The information stored can then be queried via the /diag endpoint for diagnostic and analytic purposes.\n+ */\n+\n+public class KafkaTopicPartitionTracker {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(KafkaTopicPartitionTracker.class);", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTc5NzU2Mg==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381797562", "bodyText": "final", "author": "ahmedahamid", "createdAt": "2020-02-20T06:25:12Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaTopicPartitionStatsResponse.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.connectors.kafka;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Set;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.codehaus.jackson.JsonNode;\n+import org.codehaus.jackson.JsonParser;\n+import org.codehaus.jackson.Version;\n+import org.codehaus.jackson.annotate.JsonAutoDetect;\n+import org.codehaus.jackson.annotate.JsonGetter;\n+import org.codehaus.jackson.annotate.JsonMethod;\n+import org.codehaus.jackson.annotate.JsonSetter;\n+import org.codehaus.jackson.map.DeserializationConfig;\n+import org.codehaus.jackson.map.DeserializationContext;\n+import org.codehaus.jackson.map.JsonDeserializer;\n+import org.codehaus.jackson.map.ObjectMapper;\n+import org.codehaus.jackson.map.module.SimpleModule;\n+import org.codehaus.jackson.type.TypeReference;\n+\n+import com.linkedin.datastream.common.JsonUtils;\n+\n+\n+/**\n+ * Response structure used for Topic partition stats\n+ * @see AbstractKafkaConnector#process(String)\n+ */\n+public class KafkaTopicPartitionStatsResponse {\n+\n+  private String _consumerGroupId;", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTc5OTg0Nw==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381799847", "bodyText": "I think it would be useful to add an informational log statement at the beginning of this method", "author": "ahmedahamid", "createdAt": "2020-02-20T06:29:25Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaConnector.java", "diffHunk": "@@ -428,6 +434,20 @@ private String processDatastreamStateRequest(URI request) {\n         .map(KafkaDatastreamStatesResponse::toJson).orElse(null);\n   }\n \n+  private String processTopicPartitionStatsRequest() {", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTgwMDkzOA==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381800938", "bodyText": "nit: remove empty line + s/KafkaTopicPartition tracker/KafkaTopicPartitionTracker", "author": "ahmedahamid", "createdAt": "2020-02-20T06:31:32Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -916,4 +924,31 @@ public static String getTaskMetadataGroupId(DatastreamTask task, CommonConnector\n \n     return null;\n   }\n-}\n+\n+  /**\n+   *\n+   *  Gets the KafkaTopicPartition tracker", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTgwMTc3Mg==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381801772", "bodyText": "nit: I have nothing against final locals but it's inconsistent with the one on line 384", "author": "ahmedahamid", "createdAt": "2020-02-20T06:33:09Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaConnector.java", "diffHunk": "@@ -382,6 +384,10 @@ public String process(String query) {\n         String response = processDatastreamStateRequest(uri);\n         _logger.trace(\"Query: {} returns response: {}\", query, response);\n         return response;\n+      } else if (path != null && path.equalsIgnoreCase(DiagnosticsRequestType.TOPICPARTITION_STATS.toString())) {\n+        final String response = processTopicPartitionStatsRequest();", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTgwMjc5OQ==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381802799", "bodyText": "Great catch \ud83d\udc4d", "author": "ahmedahamid", "createdAt": "2020-02-20T06:35:14Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaConnectorTask.java", "diffHunk": "@@ -153,22 +150,4 @@ protected DatastreamProducerRecord translate(ConsumerRecord<?, ?> fromKafka, Ins\n \n     return builder.build();\n   }\n-\n-  /**\n-   * Get Kafka group ID of given task\n-   * @param task Task for which group ID is generated.\n-   * @param groupIdConstructor GroupIdConstructor to use for generating group ID.\n-   * @param consumerMetrics CommonConnectorMetrics to use for reporting errors.\n-   * @param logger Logger for logging information.\n-   */\n-  @VisibleForTesting\n-  public static String getKafkaGroupId(DatastreamTask task, GroupIdConstructor groupIdConstructor,\n-      CommonConnectorMetrics consumerMetrics, Logger logger) {\n-    try {\n-      return groupIdConstructor.getTaskGroupId(task, Optional.of(logger));\n-    } catch (Exception e) {\n-      consumerMetrics.updateErrorRate(1, \"Can't find group ID\", e);\n-      throw e;\n-    }\n-  }", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTgwODY2Nw==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381808667", "bodyText": "There's an exact copy of this method in TestKafkaMirrorMakerConnector. I suggest keeping the one in this file and deleting the one over there.", "author": "ahmedahamid", "createdAt": "2020-02-20T06:46:21Z", "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/mirrormaker/KafkaMirrorMakerConnectorTestUtils.java", "diffHunk": "@@ -159,4 +160,24 @@ static Properties getKafkaConsumerProperties() {\n     props.put(\"auditor.class\", NoOpAuditor.class.getCanonicalName());\n     return props;\n   }\n+\n+  /**\n+   * Get the default config properties of a Kafka-based connector\n+   * @param override Configuration properties to override default config properties\n+   */\n+  public static Properties getDefaultConfig(Optional<Properties> override) {\n+    Properties config = new Properties();\n+    config.put(KafkaBasedConnectorConfig.CONFIG_DEFAULT_KEY_SERDE, \"keySerde\");\n+    config.put(KafkaBasedConnectorConfig.CONFIG_DEFAULT_VALUE_SERDE, \"valueSerde\");\n+    config.put(KafkaBasedConnectorConfig.CONFIG_COMMIT_INTERVAL_MILLIS, \"10000\");\n+    config.put(KafkaBasedConnectorConfig.CONFIG_COMMIT_TIMEOUT_MILLIS, \"1000\");\n+    config.put(KafkaBasedConnectorConfig.CONFIG_POLL_TIMEOUT_MILLIS, \"5000\");\n+    config.put(KafkaBasedConnectorConfig.CONFIG_CONSUMER_FACTORY_CLASS, LiKafkaConsumerFactory.class.getName());\n+    config.put(KafkaBasedConnectorConfig.CONFIG_PAUSE_PARTITION_ON_ERROR, Boolean.TRUE.toString());\n+    config.put(KafkaBasedConnectorConfig.CONFIG_RETRY_SLEEP_DURATION_MILLIS, \"1000\");\n+    config.put(KafkaBasedConnectorConfig.CONFIG_PAUSE_ERROR_PARTITION_DURATION_MILLIS,\n+        String.valueOf(Duration.ofSeconds(5).toMillis()));\n+    override.ifPresent(config::putAll);\n+    return config;\n+  }", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTgxMDkxMQ==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381810911", "bodyText": "You can avoid having to fully qualify getDefaultConfig() throughout this file by using a static import.\nimport static com.linkedin.datastream.connectors.kafka.mirrormaker.KafkaMirrorMakerConnectorTestUtils.getDefaultConfig;", "author": "ahmedahamid", "createdAt": "2020-02-20T06:50:25Z", "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/mirrormaker/TestKafkaMirrorMakerConnector.java", "diffHunk": "@@ -102,7 +102,8 @@ public void testInitializeDatastream() throws Exception {\n     Datastream ds =\n         KafkaMirrorMakerConnectorTestUtils.createDatastream(\"testInitializeDatastream\", _broker, sourceRegex, metadata);\n     KafkaMirrorMakerConnector connector =\n-        new KafkaMirrorMakerConnector(\"testInitializeDatastream\", getDefaultConfig(Optional.empty()), \"testCluster\");\n+        new KafkaMirrorMakerConnector(\"testInitializeDatastream\",\n+            KafkaMirrorMakerConnectorTestUtils.getDefaultConfig(Optional.empty()), \"testCluster\");", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTgzOTY3NA==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381839674", "bodyText": "Could you, please, check if you can reuse this object for serialization and deserialization (e.g. declare as a static final)? I don't think we need to instantiate and configure one every time this method is invoked. Same goes for fromJson().", "author": "ahmedahamid", "createdAt": "2020-02-20T08:12:11Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaTopicPartitionStatsResponse.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.connectors.kafka;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Set;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.codehaus.jackson.JsonNode;\n+import org.codehaus.jackson.JsonParser;\n+import org.codehaus.jackson.Version;\n+import org.codehaus.jackson.annotate.JsonAutoDetect;\n+import org.codehaus.jackson.annotate.JsonGetter;\n+import org.codehaus.jackson.annotate.JsonMethod;\n+import org.codehaus.jackson.annotate.JsonSetter;\n+import org.codehaus.jackson.map.DeserializationConfig;\n+import org.codehaus.jackson.map.DeserializationContext;\n+import org.codehaus.jackson.map.JsonDeserializer;\n+import org.codehaus.jackson.map.ObjectMapper;\n+import org.codehaus.jackson.map.module.SimpleModule;\n+import org.codehaus.jackson.type.TypeReference;\n+\n+import com.linkedin.datastream.common.JsonUtils;\n+\n+\n+/**\n+ * Response structure used for Topic partition stats\n+ * @see AbstractKafkaConnector#process(String)\n+ */\n+public class KafkaTopicPartitionStatsResponse {\n+\n+  private String _consumerGroupId;\n+\n+  private Set<TopicPartition> _topicPartitions;\n+\n+  /**\n+   *  Default constructor for deserialization\n+   */\n+  public KafkaTopicPartitionStatsResponse() {\n+  }\n+\n+  /**\n+   * Constructor for KafkaTopicPartitionStatsResponse\n+   * @param consumerGroupId identifier for consumer group\n+   * @param topicPartitions a map of consumer offsets for topic partitions\n+   */\n+  public KafkaTopicPartitionStatsResponse(String consumerGroupId, Set<TopicPartition> topicPartitions) {\n+    _consumerGroupId = consumerGroupId;\n+    _topicPartitions = topicPartitions;\n+  }\n+\n+  @JsonGetter(\"_consumerGroupId\")\n+  public String getConsumerGroupId() {\n+    return _consumerGroupId;\n+  }\n+\n+  @JsonSetter(\"_consumerGroupId\")\n+  public void setConsumerGroupId(String consumerGroupId) {\n+    _consumerGroupId = consumerGroupId;\n+  }\n+\n+  @JsonGetter(\"_topicPartitions\")\n+  public Set<TopicPartition> getTopicPartitions() {\n+    return _topicPartitions;\n+  }\n+\n+  @JsonSetter(\"_topicPartitions\")\n+  public void setTopicPartitions(Set<TopicPartition> topicPartitions) {\n+    _topicPartitions = topicPartitions;\n+  }\n+\n+  /**\n+   * Serialize to JSON\n+   */\n+  public static String toJson(List<KafkaTopicPartitionStatsResponse> obj) {\n+    ObjectMapper mapper = new ObjectMapper();", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjE5ODI4Mw==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r382198283", "bodyText": "I cannot make it as final. But I have made it static.", "author": "vishwajith-s", "createdAt": "2020-02-20T19:05:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTgzOTY3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg0MTMwNg==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381841306", "bodyText": "Could you, please, have the existing fromJson() and toJson() invoke these methods after reconciling all differences, e.g.\n  public static <T> T fromJson(String json, TypeReference<T> typeRef) {\n    return fromJson(json, typeRef, MAPPER);\n  }\n\n  public static <T> T fromJson(String json, TypeReference<T> typeRef, ObjectMapper mapper) {\n    Validate.notNull(json, \"null JSON string\");\n    Validate.notNull(typeRef, \"null type reference\");\n    T object = null;\n    try {\n      object = mapper.readValue(json, typeRef);\n    } catch (IOException e) {\n      String errorMessage = \"Failed to parse json: \" + json;\n      ErrorLogger.logAndThrowDatastreamRuntimeException(LOG, errorMessage, e);\n    }\n    return object;\n  }\n\n  public static <T> String toJson(T object) {\n    return toJson(object, MAPPER);\n  }\n\n  public static <T> String toJson(T object, ObjectMapper mapper) {\n    Validate.notNull(object, \"null input object\");\n    StringWriter out = new StringWriter();   // Modified to match the old toJson()\n    try {\n      mapper.writeValue(out, object);        // ditto\n    } catch (IOException e) {\n      String errorMessage = \"Failed to serialize object: \" + object;\n      ErrorLogger.logAndThrowDatastreamRuntimeException(LOG, errorMessage, e);\n    }\n    return out.toString();\n  }", "author": "ahmedahamid", "createdAt": "2020-02-20T08:16:12Z", "path": "datastream-common/src/main/java/com/linkedin/datastream/common/JsonUtils.java", "diffHunk": "@@ -96,6 +96,46 @@\n     return object;\n   }\n \n+  /**\n+   * Deserialize a JSON string into an object based on a type reference.\n+   * This method allows the caller to specify precisely the desired output\n+   * type for the target object.\n+   * @param json JSON string\n+   * @param typeRef type reference of the target object\n+   * @param  mapper the ObjectMapper to use\n+   * @return deserialized Java object\n+   */\n+  public static <T> T fromJson(String json, TypeReference<T> typeRef, ObjectMapper mapper) {\n+    Validate.notNull(json, \"null JSON string\");\n+    Validate.notNull(typeRef, \"null type reference\");\n+    T object = null;\n+    try {\n+      object = mapper.readValue(json, typeRef);\n+    } catch (IOException e) {\n+      String errorMessage = \"Failed to parse json: \" + json;\n+      ErrorLogger.logAndThrowDatastreamRuntimeException(LOG, errorMessage, e);\n+    }\n+    return object;\n+  }\n+\n+  /**\n+   * Serialize a Java object into JSON string.\n+   * @param object object to be serialized\n+   * @param  mapper the ObjectMapper to use\n+   * @return JSON string\n+   */\n+  public static <T> String toJson(T object, ObjectMapper mapper) {\n+    Validate.notNull(object, \"null input object\");\n+    String out = null;\n+    try {\n+      out = mapper.writeValueAsString(object);\n+    } catch (IOException e) {\n+      String errorMessage = \"Failed to serialize object: \" + object;\n+      ErrorLogger.logAndThrowDatastreamRuntimeException(LOG, errorMessage, e);\n+    }\n+    return out;\n+  }", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjEzNjQ2NA==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r382136464", "bodyText": "+1", "author": "gauravrkulkarni", "createdAt": "2020-02-20T17:07:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg0MTMwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIyMjUzNg==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r382222536", "bodyText": "Good point. I should have done this at the start. I cannot combine the ones that takes class and typereference. But the rest is doable.", "author": "vishwajith-s", "createdAt": "2020-02-20T19:51:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg0MTMwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg0NDUwMA==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381844500", "bodyText": "With the auto-field discovery and the custom deserializer, could you please double-check if you need these getter/setter annotations? I have a somewhat strong suspicion we don't really need them.\nI think we can do w/o @JsonGetter, @JsonSetter, and the default ctor in favor of the newer @JsonCreator and @JsonProperty annotations.\n  @JsonCreator\n  public KafkaTopicPartitionStatsResponse(\n      @JsonProperty(\"consumerGroupId\") String consumerGroupId,\n      @JsonProperty(\"topicPartitions\") Set<TopicPartition> topicPartitions) {\n    ...\n  }", "author": "ahmedahamid", "createdAt": "2020-02-20T08:23:43Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaTopicPartitionStatsResponse.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.connectors.kafka;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Set;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.codehaus.jackson.JsonNode;\n+import org.codehaus.jackson.JsonParser;\n+import org.codehaus.jackson.Version;\n+import org.codehaus.jackson.annotate.JsonAutoDetect;\n+import org.codehaus.jackson.annotate.JsonGetter;\n+import org.codehaus.jackson.annotate.JsonMethod;\n+import org.codehaus.jackson.annotate.JsonSetter;\n+import org.codehaus.jackson.map.DeserializationConfig;\n+import org.codehaus.jackson.map.DeserializationContext;\n+import org.codehaus.jackson.map.JsonDeserializer;\n+import org.codehaus.jackson.map.ObjectMapper;\n+import org.codehaus.jackson.map.module.SimpleModule;\n+import org.codehaus.jackson.type.TypeReference;\n+\n+import com.linkedin.datastream.common.JsonUtils;\n+\n+\n+/**\n+ * Response structure used for Topic partition stats\n+ * @see AbstractKafkaConnector#process(String)\n+ */\n+public class KafkaTopicPartitionStatsResponse {\n+\n+  private String _consumerGroupId;\n+\n+  private Set<TopicPartition> _topicPartitions;\n+\n+  /**\n+   *  Default constructor for deserialization\n+   */\n+  public KafkaTopicPartitionStatsResponse() {\n+  }\n+\n+  /**\n+   * Constructor for KafkaTopicPartitionStatsResponse\n+   * @param consumerGroupId identifier for consumer group\n+   * @param topicPartitions a map of consumer offsets for topic partitions\n+   */\n+  public KafkaTopicPartitionStatsResponse(String consumerGroupId, Set<TopicPartition> topicPartitions) {\n+    _consumerGroupId = consumerGroupId;\n+    _topicPartitions = topicPartitions;\n+  }\n+\n+  @JsonGetter(\"_consumerGroupId\")", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg1NDc2Nw==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381854767", "bodyText": "nit: You don't need parenthesis for one liner lambdas\n    responseList.forEach(response -> actual.addAll(response.getTopicPartitions()));", "author": "ahmedahamid", "createdAt": "2020-02-20T08:45:34Z", "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/mirrormaker/TestKafkaTopicPartitionStats.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.connectors.kafka.mirrormaker;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.codehaus.jackson.type.TypeReference;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import com.google.common.collect.ImmutableSet;\n+\n+import com.linkedin.datastream.common.Datastream;\n+import com.linkedin.datastream.common.JsonUtils;\n+import com.linkedin.datastream.common.PollUtils;\n+import com.linkedin.datastream.connectors.kafka.KafkaTopicPartitionStatsResponse;\n+import com.linkedin.datastream.connectors.kafka.MockDatastreamEventProducer;\n+import com.linkedin.datastream.server.DatastreamTaskImpl;\n+import com.linkedin.datastream.testutil.BaseKafkaZkTest;\n+\n+\n+/**\n+ * Tests for TopicPartitionStats diag command\n+ */\n+@Test\n+public class TestKafkaTopicPartitionStats extends BaseKafkaZkTest {\n+\n+  private static final int PARTITION_COUNT = 2;\n+  private static final String TOPICPARTITION_STATS_QUERY = \"/topicpartition_stats\";\n+\n+  @Test\n+  public void testProcessTopicPartitionStats() {\n+    List<String> topics = Arrays.asList(\"topic1\", \"topic2\");\n+    topics.forEach(topic -> createTopic(_zkUtils, topic, PARTITION_COUNT));\n+\n+    Datastream datastream = KafkaMirrorMakerConnectorTestUtils.createDatastream(\"topicStream\",  _broker, \"topic\\\\d+\");\n+\n+    DatastreamTaskImpl task = new DatastreamTaskImpl(Collections.singletonList(datastream));\n+    MockDatastreamEventProducer datastreamProducer = new MockDatastreamEventProducer();\n+    task.setEventProducer(datastreamProducer);\n+\n+    KafkaMirrorMakerConnector connector =\n+        new KafkaMirrorMakerConnector(\"MirrorMakerConnector\",\n+            KafkaMirrorMakerConnectorTestUtils.getDefaultConfig(Optional.empty()), \"testCluster\");\n+    connector.start(null);\n+\n+    // Notify connector of paused partition update\n+    connector.onAssignmentChange(Collections.singletonList(task));\n+\n+    Set<TopicPartition> expected = new HashSet<>();\n+    topics.forEach(topic -> {\n+      for (int i = 0; i < PARTITION_COUNT; ++i) {\n+        expected.add(new TopicPartition(topic, i));\n+      }\n+    });\n+\n+    // Wait until the partitions are assigned\n+    if (!PollUtils.poll(() -> testProcessTopicPartitionsStatsInternal(connector, expected),\n+        KafkaMirrorMakerConnectorTestUtils.POLL_PERIOD_MS,\n+        KafkaMirrorMakerConnectorTestUtils.POLL_TIMEOUT_MS)) {\n+      Assert.fail(\"Topic partitions still not assigned\");\n+    }\n+\n+    // Delete the topic to revoke partitions from topic2\n+    deleteTopic(_zkUtils, topics.get(0));\n+    expected.clear();\n+    for (int i = 0; i < PARTITION_COUNT; ++i) {\n+      expected.add(new TopicPartition(topics.get(1), i));\n+    }\n+\n+    // Wait until the partitions from the deleted topic are revoked\n+    if (!PollUtils.poll(() -> testProcessTopicPartitionsStatsInternal(connector, expected),\n+        KafkaMirrorMakerConnectorTestUtils.POLL_PERIOD_MS,\n+        KafkaMirrorMakerConnectorTestUtils.POLL_TIMEOUT_MS)) {\n+      Assert.fail(\"Topic partitions still not revoked\");\n+    }\n+    connector.stop();\n+  }\n+\n+  boolean testProcessTopicPartitionsStatsInternal(KafkaMirrorMakerConnector connector, Set<TopicPartition> expected) {\n+    String jsonStr = connector.process(\"/topicpartition_stats\");\n+    List<KafkaTopicPartitionStatsResponse> responseList = KafkaTopicPartitionStatsResponse.fromJson(jsonStr);\n+\n+    Set<TopicPartition> actual = new HashSet<>();\n+    responseList.forEach(response -> {\n+      actual.addAll(response.getTopicPartitions());\n+    });", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIwMDI4Nw==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r382200287", "bodyText": "Thanks! I will retain this as it makes it more clearer.", "author": "vishwajith-s", "createdAt": "2020-02-20T19:09:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg1NDc2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg1NTA2NA==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381855064", "bodyText": "private", "author": "ahmedahamid", "createdAt": "2020-02-20T08:46:06Z", "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/mirrormaker/TestKafkaTopicPartitionStats.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.connectors.kafka.mirrormaker;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.codehaus.jackson.type.TypeReference;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import com.google.common.collect.ImmutableSet;\n+\n+import com.linkedin.datastream.common.Datastream;\n+import com.linkedin.datastream.common.JsonUtils;\n+import com.linkedin.datastream.common.PollUtils;\n+import com.linkedin.datastream.connectors.kafka.KafkaTopicPartitionStatsResponse;\n+import com.linkedin.datastream.connectors.kafka.MockDatastreamEventProducer;\n+import com.linkedin.datastream.server.DatastreamTaskImpl;\n+import com.linkedin.datastream.testutil.BaseKafkaZkTest;\n+\n+\n+/**\n+ * Tests for TopicPartitionStats diag command\n+ */\n+@Test\n+public class TestKafkaTopicPartitionStats extends BaseKafkaZkTest {\n+\n+  private static final int PARTITION_COUNT = 2;\n+  private static final String TOPICPARTITION_STATS_QUERY = \"/topicpartition_stats\";\n+\n+  @Test\n+  public void testProcessTopicPartitionStats() {\n+    List<String> topics = Arrays.asList(\"topic1\", \"topic2\");\n+    topics.forEach(topic -> createTopic(_zkUtils, topic, PARTITION_COUNT));\n+\n+    Datastream datastream = KafkaMirrorMakerConnectorTestUtils.createDatastream(\"topicStream\",  _broker, \"topic\\\\d+\");\n+\n+    DatastreamTaskImpl task = new DatastreamTaskImpl(Collections.singletonList(datastream));\n+    MockDatastreamEventProducer datastreamProducer = new MockDatastreamEventProducer();\n+    task.setEventProducer(datastreamProducer);\n+\n+    KafkaMirrorMakerConnector connector =\n+        new KafkaMirrorMakerConnector(\"MirrorMakerConnector\",\n+            KafkaMirrorMakerConnectorTestUtils.getDefaultConfig(Optional.empty()), \"testCluster\");\n+    connector.start(null);\n+\n+    // Notify connector of paused partition update\n+    connector.onAssignmentChange(Collections.singletonList(task));\n+\n+    Set<TopicPartition> expected = new HashSet<>();\n+    topics.forEach(topic -> {\n+      for (int i = 0; i < PARTITION_COUNT; ++i) {\n+        expected.add(new TopicPartition(topic, i));\n+      }\n+    });\n+\n+    // Wait until the partitions are assigned\n+    if (!PollUtils.poll(() -> testProcessTopicPartitionsStatsInternal(connector, expected),\n+        KafkaMirrorMakerConnectorTestUtils.POLL_PERIOD_MS,\n+        KafkaMirrorMakerConnectorTestUtils.POLL_TIMEOUT_MS)) {\n+      Assert.fail(\"Topic partitions still not assigned\");\n+    }\n+\n+    // Delete the topic to revoke partitions from topic2\n+    deleteTopic(_zkUtils, topics.get(0));\n+    expected.clear();\n+    for (int i = 0; i < PARTITION_COUNT; ++i) {\n+      expected.add(new TopicPartition(topics.get(1), i));\n+    }\n+\n+    // Wait until the partitions from the deleted topic are revoked\n+    if (!PollUtils.poll(() -> testProcessTopicPartitionsStatsInternal(connector, expected),\n+        KafkaMirrorMakerConnectorTestUtils.POLL_PERIOD_MS,\n+        KafkaMirrorMakerConnectorTestUtils.POLL_TIMEOUT_MS)) {\n+      Assert.fail(\"Topic partitions still not revoked\");\n+    }\n+    connector.stop();\n+  }\n+\n+  boolean testProcessTopicPartitionsStatsInternal(KafkaMirrorMakerConnector connector, Set<TopicPartition> expected) {", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTg1NjI5Mw==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r381856293", "bodyText": "Replace w/ TOPICPARTITION_STATS_QUERY", "author": "ahmedahamid", "createdAt": "2020-02-20T08:48:31Z", "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/mirrormaker/TestKafkaTopicPartitionStats.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.connectors.kafka.mirrormaker;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.codehaus.jackson.type.TypeReference;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import com.google.common.collect.ImmutableSet;\n+\n+import com.linkedin.datastream.common.Datastream;\n+import com.linkedin.datastream.common.JsonUtils;\n+import com.linkedin.datastream.common.PollUtils;\n+import com.linkedin.datastream.connectors.kafka.KafkaTopicPartitionStatsResponse;\n+import com.linkedin.datastream.connectors.kafka.MockDatastreamEventProducer;\n+import com.linkedin.datastream.server.DatastreamTaskImpl;\n+import com.linkedin.datastream.testutil.BaseKafkaZkTest;\n+\n+\n+/**\n+ * Tests for TopicPartitionStats diag command\n+ */\n+@Test\n+public class TestKafkaTopicPartitionStats extends BaseKafkaZkTest {\n+\n+  private static final int PARTITION_COUNT = 2;\n+  private static final String TOPICPARTITION_STATS_QUERY = \"/topicpartition_stats\";\n+\n+  @Test\n+  public void testProcessTopicPartitionStats() {\n+    List<String> topics = Arrays.asList(\"topic1\", \"topic2\");\n+    topics.forEach(topic -> createTopic(_zkUtils, topic, PARTITION_COUNT));\n+\n+    Datastream datastream = KafkaMirrorMakerConnectorTestUtils.createDatastream(\"topicStream\",  _broker, \"topic\\\\d+\");\n+\n+    DatastreamTaskImpl task = new DatastreamTaskImpl(Collections.singletonList(datastream));\n+    MockDatastreamEventProducer datastreamProducer = new MockDatastreamEventProducer();\n+    task.setEventProducer(datastreamProducer);\n+\n+    KafkaMirrorMakerConnector connector =\n+        new KafkaMirrorMakerConnector(\"MirrorMakerConnector\",\n+            KafkaMirrorMakerConnectorTestUtils.getDefaultConfig(Optional.empty()), \"testCluster\");\n+    connector.start(null);\n+\n+    // Notify connector of paused partition update\n+    connector.onAssignmentChange(Collections.singletonList(task));\n+\n+    Set<TopicPartition> expected = new HashSet<>();\n+    topics.forEach(topic -> {\n+      for (int i = 0; i < PARTITION_COUNT; ++i) {\n+        expected.add(new TopicPartition(topic, i));\n+      }\n+    });\n+\n+    // Wait until the partitions are assigned\n+    if (!PollUtils.poll(() -> testProcessTopicPartitionsStatsInternal(connector, expected),\n+        KafkaMirrorMakerConnectorTestUtils.POLL_PERIOD_MS,\n+        KafkaMirrorMakerConnectorTestUtils.POLL_TIMEOUT_MS)) {\n+      Assert.fail(\"Topic partitions still not assigned\");\n+    }\n+\n+    // Delete the topic to revoke partitions from topic2\n+    deleteTopic(_zkUtils, topics.get(0));\n+    expected.clear();\n+    for (int i = 0; i < PARTITION_COUNT; ++i) {\n+      expected.add(new TopicPartition(topics.get(1), i));\n+    }\n+\n+    // Wait until the partitions from the deleted topic are revoked\n+    if (!PollUtils.poll(() -> testProcessTopicPartitionsStatsInternal(connector, expected),\n+        KafkaMirrorMakerConnectorTestUtils.POLL_PERIOD_MS,\n+        KafkaMirrorMakerConnectorTestUtils.POLL_TIMEOUT_MS)) {\n+      Assert.fail(\"Topic partitions still not revoked\");\n+    }\n+    connector.stop();\n+  }\n+\n+  boolean testProcessTopicPartitionsStatsInternal(KafkaMirrorMakerConnector connector, Set<TopicPartition> expected) {\n+    String jsonStr = connector.process(\"/topicpartition_stats\");", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjE0MjUxMA==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r382142510", "bodyText": "nit: remove comma at the end?", "author": "gauravrkulkarni", "createdAt": "2020-02-20T17:18:34Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaConnector.java", "diffHunk": "@@ -94,6 +95,7 @@ public Thread newThread(@NotNull Runnable r) {\n \n   enum DiagnosticsRequestType {\n     DATASTREAM_STATE,\n+    TOPICPARTITION_STATS,", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjE1MTk5Nw==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r382151997", "bodyText": "ConcurrentHashMap guarantees concurrent access at individual map level (not at the map level). So this can potentially result in inconsistent results when onPartitionsAssigned/onPartitionsRevoked gets called at the same time getTopicPartitions is called. Alternative is to make all the accesses \"synchronized\", but since the map gets accessed in hot path during rebalance (and we can potentially address this at caller level) its probably OK to leave it as it is. Just something to be aware of.", "author": "gauravrkulkarni", "createdAt": "2020-02-20T17:36:06Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaTopicPartitionTracker.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+\n+package com.linkedin.datastream.connectors.kafka;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.jetbrains.annotations.NotNull;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * KafkaTopicPartitionTracker contains information about consumer groups, topic partitions and\n+ * their consumer offsets.\n+ *\n+ * The information stored can then be queried via the /diag endpoint for diagnostic and analytic purposes.\n+ */\n+\n+public class KafkaTopicPartitionTracker {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(KafkaTopicPartitionTracker.class);\n+\n+  private String _consumerGroupId;\n+\n+  @NotNull\n+  private Set<TopicPartition>  _topicPartitions = ConcurrentHashMap.newKeySet();\n+\n+  /**\n+   *  Constructor for KafkaTopicPartitionTracker\n+   *\n+   * @param consumerGroupId Identifier of the consumer group\n+   */\n+  public KafkaTopicPartitionTracker(String consumerGroupId) {\n+    _consumerGroupId = consumerGroupId;\n+  }\n+\n+  /**\n+   * Assigns paritions. This method should be called whenever the Connector's consumer\n+   * finishes assigning partitions.\n+   *\n+   * @param topicPartitions the topic partitions which have been assigned\n+   */\n+  public void onPartitionsAssigned(@NotNull Collection<TopicPartition> topicPartitions) {\n+    _topicPartitions.addAll(topicPartitions);\n+  }\n+\n+  /**\n+   * Frees partitions that have been revoked. This method should be called whenever the Connector's\n+   * consumer is about to re-balance (and thus unassign partitions).\n+   *\n+   * @param topicPartitions the topic partitions which were previously assigned\n+   */\n+  public void onPartitionsRevoked(@NotNull Collection<TopicPartition> topicPartitions) {\n+    _topicPartitions.removeAll(topicPartitions);\n+  }\n+\n+  public Set<TopicPartition> getTopicPartitions() {\n+    return Collections.unmodifiableSet(_topicPartitions);", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjE2NTAyNg==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r382165026", "bodyText": "should we throw exception here so that caller of the endpoint will know about it?", "author": "gauravrkulkarni", "createdAt": "2020-02-20T18:01:11Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaConnector.java", "diffHunk": "@@ -463,6 +487,38 @@ public String reduce(String query, Map<String, String> responses) {\n     return null;\n   }\n \n+  private String reduceTopicPartitionStatsResponses(Map<String, String> responses) {\n+    Map<String, Map<String, Set<Integer>>> result = new HashMap<>();\n+\n+    responses.forEach((instance, json) -> {\n+      List<KafkaTopicPartitionStatsResponse> responseList;\n+      try {\n+        responseList = KafkaTopicPartitionStatsResponse.fromJson(json);\n+      } catch (Exception e) {\n+        _logger.error(\"Invalid response {} from instance {}\", json, instance);\n+        return;\n+      }\n+      if (responseList.size() >= 0) {\n+        responseList.forEach(response -> {\n+          if (response.getTopicPartitions() == null || response.getConsumerGroupId().isEmpty()) {\n+            _logger.warn(\"Empty topic partition stats map from instance {}. Ignoring the result\", instance);\n+            return;", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIwMzIzOQ==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r382203239", "bodyText": "I think ignoring the result and proceeding further is good here.  At least we will return a warning and proceed further with best effort. This is a diag tool.", "author": "vishwajith-s", "createdAt": "2020-02-20T19:14:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjE2NTAyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjE2NjYxNQ==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r382166615", "bodyText": "Is the endpoint invoked for a particular datastream? If so, what are the cases where we would expect more than one consumer group?\nIf the response is supposed to include data for multiple unrelated datastreams, I think the response should include datastream name as well?", "author": "gauravrkulkarni", "createdAt": "2020-02-20T18:04:22Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaConnector.java", "diffHunk": "@@ -463,6 +487,38 @@ public String reduce(String query, Map<String, String> responses) {\n     return null;\n   }\n \n+  private String reduceTopicPartitionStatsResponses(Map<String, String> responses) {\n+    Map<String, Map<String, Set<Integer>>> result = new HashMap<>();", "originalCommit": "e0e1fd2a12a1ef82b2248080c9c11f3401ec3ce3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIwNTgxMw==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r382205813", "bodyText": "The end point is not invoked for a particular datastream. It just aggregates the consumer groups across all brooklin instances, datastreams.\nIf the response is supposed to include data for multiple unrelated datastreams, I think the response should include datastream name as well?\nI believe the intent here is to get enough info to query Kafka or lag monitor.  Since datastream is an internal brooklin concept it will not be reported.", "author": "vishwajith-s", "createdAt": "2020-02-20T19:19:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjE2NjYxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjIyNTA0MQ==", "url": "https://github.com/linkedin/brooklin/pull/686#discussion_r382225041", "bodyText": "I believe datastream name will be useful, as that's a fundamental unit of operation for brooklin (for example, if one needs to restart a datstream to recover from lags).\nAdmittedly there are other ways to map datastream to consumer group and some type of group IDs even have datastream names in them, but that can mean going through all the datastreams in a cluster.\nWe can discuss this offline and add the datastream in a separate PR if needed.", "author": "gauravrkulkarni", "createdAt": "2020-02-20T19:56:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjE2NjYxNQ=="}], "type": "inlineReview"}, {"oid": "d5e14f482e26449ee65c65e24fdbc4ae6c549348", "url": "https://github.com/linkedin/brooklin/commit/d5e14f482e26449ee65c65e24fdbc4ae6c549348", "message": "Add diag support to report consumer groups and topic partitions\n\nThis patch adds diag support to report consumer groups and topic\npartitions in each consumer group.", "committedDate": "2020-02-21T00:54:36Z", "type": "forcePushed"}, {"oid": "88c44452280ea373126078eaed5642e79d0c929c", "url": "https://github.com/linkedin/brooklin/commit/88c44452280ea373126078eaed5642e79d0c929c", "message": "Add diag support to report consumer groups and topic partitions\n\nThis patch adds diag support to report consumer groups and topic\npartitions in each consumer group.", "committedDate": "2020-02-21T01:44:59Z", "type": "forcePushed"}, {"oid": "9650997ba8d59998f37d3bf7c3a55530224bfc34", "url": "https://github.com/linkedin/brooklin/commit/9650997ba8d59998f37d3bf7c3a55530224bfc34", "message": "Add diag support to report consumer groups and topic partitions\n\nThis patch adds diag support to report consumer groups and topic\npartitions in each consumer group.", "committedDate": "2020-02-21T01:49:32Z", "type": "forcePushed"}, {"oid": "81c36c5d2a344ba7ef4e610f00391c9a17eb887b", "url": "https://github.com/linkedin/brooklin/commit/81c36c5d2a344ba7ef4e610f00391c9a17eb887b", "message": "Add diag support to report consumer groups and topic partitions\n\nThis patch adds diag support to report consumer groups and topic\npartitions in each consumer group.", "committedDate": "2020-02-21T01:59:25Z", "type": "commit"}, {"oid": "81c36c5d2a344ba7ef4e610f00391c9a17eb887b", "url": "https://github.com/linkedin/brooklin/commit/81c36c5d2a344ba7ef4e610f00391c9a17eb887b", "message": "Add diag support to report consumer groups and topic partitions\n\nThis patch adds diag support to report consumer groups and topic\npartitions in each consumer group.", "committedDate": "2020-02-21T01:59:25Z", "type": "forcePushed"}]}