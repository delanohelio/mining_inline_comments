{"pr_number": 705, "pr_title": "Avoid accessing consumer in send callback to avoid ConcurrentModificationException in the Kafka connector", "pr_createdAt": "2020-04-29T00:36:25Z", "pr_url": "https://github.com/linkedin/brooklin/pull/705", "timeline": [{"oid": "05797943cfa3dc6fcb673c60f514449e8b2b71aa", "url": "https://github.com/linkedin/brooklin/commit/05797943cfa3dc6fcb673c60f514449e8b2b71aa", "message": "Avoid accessing consumer in send callback to avoid ConcurrentModificationException in the Kafka connector", "committedDate": "2020-04-29T00:29:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzYzMTY1Ng==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417631656", "bodyText": "Do you see a value in clarifying in the warning whether skipping the record because of autoPausedSourcePartition or send failure ?", "author": "vmaheshw", "createdAt": "2020-04-29T21:46:07Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -229,8 +232,13 @@ protected void translateAndSendBatch(ConsumerRecords<?, ?> records, Instant read\n     for (TopicPartition topicPartition : records.partitions()) {\n       for (ConsumerRecord<?, ?> record : records.records(topicPartition)) {\n         try {\n-          if (_autoPausedSourcePartitions.containsKey(topicPartition)) {\n-            _logger.warn(\"Abort sending as {} is auto-paused, rewind offset\", topicPartition);\n+          boolean skipRecord;\n+          synchronized (_sendFailureTopicPartitionExceptionMap) {\n+            skipRecord = _autoPausedSourcePartitions.containsKey(topicPartition)\n+                || _sendFailureTopicPartitionExceptionMap.containsKey(topicPartition);\n+          }\n+          if (skipRecord) {\n+            _logger.warn(\"Abort sending as {} is auto-paused or saw a send failure, rewind offset\", topicPartition);", "originalCommit": "05797943cfa3dc6fcb673c60f514449e8b2b71aa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY1NjQ5Ng==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417656496", "bodyText": "Sure, I can make that distinction", "author": "somandal", "createdAt": "2020-04-29T22:46:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzYzMTY1Ng=="}], "type": "inlineReview"}, {"oid": "d4f1409f0c7f299257e2d5cb4b72ebf01d32284c", "url": "https://github.com/linkedin/brooklin/commit/d4f1409f0c7f299257e2d5cb4b72ebf01d32284c", "message": "Enhance log message", "committedDate": "2020-04-29T22:55:49Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzYzMjk2NA==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417632964", "bodyText": "nit: Can you please make both the comment lines of similar length ?", "author": "vmaheshw", "createdAt": "2020-04-29T21:49:03Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -257,18 +265,26 @@ protected void rewindAndPausePartitionOnException(TopicPartition srcTopicPartiti\n       seekToLastCheckpoint(Collections.singleton(srcTopicPartition));\n       partitionRewound = true;\n     } catch (Exception e) {\n-      _logger.error(\"Partition rewind failed due to \", e);\n+      _logger.error(String.format(\"Partition rewind for %s failed due to \", srcTopicPartition), e);\n     }\n     if (_pausePartitionOnError && (!partitionRewound || !containsTransientException(ex))) {\n-      // if doesn't contain DatastreamTransientException and it's configured to pause partition on error conditions,\n-      // add to auto-paused set\n+      // If partition rewind failed or the exception is not of type DatastreamTransientException and it is configured to", "originalCommit": "05797943cfa3dc6fcb673c60f514449e8b2b71aa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4Njk4OQ==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417686989", "bodyText": "done", "author": "somandal", "createdAt": "2020-04-30T00:22:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzYzMjk2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzYzNzI3Mw==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417637273", "bodyText": "Can you add a comment explaining why is it important to check this after flush?", "author": "vmaheshw", "createdAt": "2020-04-29T21:58:26Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -522,6 +540,7 @@ protected void maybeCommitOffsetsInternal(Consumer<?, ?> consumer, boolean force\n     if (force || timeSinceLastCommit > _offsetCommitInterval) {\n       _logger.info(\"Trying to flush the producer and commit offsets.\");\n       _producer.flush();\n+      rewindAndPausePartitionsOnSendException();", "originalCommit": "05797943cfa3dc6fcb673c60f514449e8b2b71aa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4NzA4NA==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417687084", "bodyText": "done", "author": "somandal", "createdAt": "2020-04-30T00:22:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzYzNzI3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY2NTgxOA==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417665818", "bodyText": "We should set both the values in sychronized above and use them to display.", "author": "vmaheshw", "createdAt": "2020-04-29T23:13:40Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -229,8 +232,14 @@ protected void translateAndSendBatch(ConsumerRecords<?, ?> records, Instant read\n     for (TopicPartition topicPartition : records.partitions()) {\n       for (ConsumerRecord<?, ?> record : records.records(topicPartition)) {\n         try {\n-          if (_autoPausedSourcePartitions.containsKey(topicPartition)) {\n-            _logger.warn(\"Abort sending as {} is auto-paused, rewind offset\", topicPartition);\n+          boolean skipRecord;\n+          synchronized (_sendFailureTopicPartitionExceptionMap) {\n+            skipRecord = _autoPausedSourcePartitions.containsKey(topicPartition)\n+                || _sendFailureTopicPartitionExceptionMap.containsKey(topicPartition);\n+          }\n+          if (skipRecord) {\n+            _logger.warn(\"Abort sending as {} {}, rewind offset\", topicPartition,\n+                _autoPausedSourcePartitions.containsKey(topicPartition) ? \"is auto-paused\" : \"saw a send failure\");", "originalCommit": "d4f1409f0c7f299257e2d5cb4b72ebf01d32284c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY3Mjk2OA==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417672968", "bodyText": "Not sure I understood, which values?", "author": "somandal", "createdAt": "2020-04-29T23:36:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY2NTgxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY3OTM3NA==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417679374", "bodyText": "I meant:\n            partitionPaused = _autoPausedSourcePartitions.containsKey(topicPartition)\n            sendFailure =   _sendFailureTopicPartitionExceptionMap.containsKey(topicPartition);\n          }\n          if (partitionPaused || sendFailure) {\n            _logger.warn(\"Abort sending for {} paused: {} sendfailure: {}, rewind offset\", topicPartition,\n                partitionPaused, sendFailure\");```", "author": "vmaheshw", "createdAt": "2020-04-29T23:56:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY2NTgxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY3OTc3OQ==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417679779", "bodyText": "got it, thanks for clarifying!", "author": "somandal", "createdAt": "2020-04-29T23:57:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY2NTgxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4Njk4MQ==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417686981", "bodyText": "done", "author": "somandal", "createdAt": "2020-04-30T00:22:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY2NTgxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY2NjUwNQ==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417666505", "bodyText": "I think you should move this in a function in AbstractKafkaBasedConnectorTask and use that function and avoid using the map outside the class. You should also set the visibilty to private in that case.", "author": "vmaheshw", "createdAt": "2020-04-29T23:15:41Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/mirrormaker/KafkaMirrorMakerConnectorTask.java", "diffHunk": "@@ -264,9 +264,12 @@ protected void sendDatastreamProducerRecord(DatastreamProducerRecord datastreamP\n           ((metadata, exception) -> {\n             if (exception != null) {\n               _logger.warn(\n-                  String.format(\"Detected exception being throw from callback for src partition: %s while sending producer \"\n-                      + \"record: %s, exception: \", srcTopicPartition, datastreamProducerRecord), exception);\n-              rewindAndPausePartitionOnException(srcTopicPartition, exception);\n+                  String.format(\"Detected exception being throw from flushless send callback for source \"\n+                      + \"topic-partition: %s with metadata: %s, exception: \", srcTopicPartition, metadata),\n+                  exception);", "originalCommit": "d4f1409f0c7f299257e2d5cb4b72ebf01d32284c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY3NzExMA==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417677110", "bodyText": "Do you want me to move the comment or the synchronized block or both?", "author": "somandal", "createdAt": "2020-04-29T23:49:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY2NjUwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY3ODI5NQ==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417678295", "bodyText": "I initially meant just the synchronized block. Whether to move the warning log as well is your discretion.", "author": "vmaheshw", "createdAt": "2020-04-29T23:52:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY2NjUwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4NzEyOQ==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417687129", "bodyText": "done, didn't add the comment there.", "author": "somandal", "createdAt": "2020-04-30T00:22:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY2NjUwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY2NzQ3OA==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417667478", "bodyText": "You can extract this out into another function and reuse between both connectorTasks", "author": "vmaheshw", "createdAt": "2020-04-29T23:18:47Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -283,10 +300,12 @@ protected void sendDatastreamProducerRecord(DatastreamProducerRecord datastreamP\n       TopicPartition srcTopicPartition, int numBytes, SendCallback sendCallback) {\n     _producer.send(datastreamProducerRecord, ((metadata, exception) -> {\n       if (exception != null) {\n-        String msg = String.format(\"Detect exception being thrown from callback for src partition: %s while \"\n-            + \"sending, metadata: %s , exception: \", srcTopicPartition, metadata);\n+        String msg = String.format(\"Detected exception being thrown from send callback for source topic-partition: %s \"\n+            + \"with metadata: %s, exception: \", srcTopicPartition, metadata);\n         _logger.warn(msg, exception);\n-        rewindAndPausePartitionOnException(srcTopicPartition, exception);\n+        synchronized (_sendFailureTopicPartitionExceptionMap) {\n+          _sendFailureTopicPartitionExceptionMap.put(srcTopicPartition, exception);", "originalCommit": "d4f1409f0c7f299257e2d5cb4b72ebf01d32284c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4NzA1Mw==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417687053", "bodyText": "done", "author": "somandal", "createdAt": "2020-04-30T00:22:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY2NzQ3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY2OTYxMg==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417669612", "bodyText": "You should check if the topic-partition is already in the pause list and skip the rewind process.  Reason: suppose for a topic-partition, 10 records are polled and got 1 callback with error before next poll, this topic partition will be rewind and paused and poll will continue. Suppose the other 9 callbacks are coming with delay and come before next 2-3 poll iterations, we will do this rewind multiple-times, even though the partition is already paused and rewind. We should try to skip the same no-op  operation multiple times.", "author": "vmaheshw", "createdAt": "2020-04-29T23:25:25Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -257,18 +266,26 @@ protected void rewindAndPausePartitionOnException(TopicPartition srcTopicPartiti\n       seekToLastCheckpoint(Collections.singleton(srcTopicPartition));\n       partitionRewound = true;\n     } catch (Exception e) {\n-      _logger.error(\"Partition rewind failed due to \", e);\n+      _logger.error(String.format(\"Partition rewind for %s failed due to \", srcTopicPartition), e);\n     }\n     if (_pausePartitionOnError && (!partitionRewound || !containsTransientException(ex))) {\n-      // if doesn't contain DatastreamTransientException and it's configured to pause partition on error conditions,\n-      // add to auto-paused set\n+      // If partition rewind failed or the exception is not of type DatastreamTransientException and it is configured to\n+      // pause partition on error conditions, add it to the auto-paused set\n       _logger.warn(\"Adding source topic partition {} to auto-pause set\", srcTopicPartition);\n       _autoPausedSourcePartitions.put(srcTopicPartition,\n           PausedSourcePartitionMetadata.sendError(start, _pauseErrorPartitionDuration, ex));\n       _taskUpdates.add(DatastreamConstants.UpdateType.PAUSE_RESUME_PARTITIONS);\n     }\n   }\n \n+  protected void rewindAndPausePartitionsOnSendException() {\n+    // For all topic partitions which have seen send exceptions, attempt to rewind them to the last checkpoint\n+    synchronized (_sendFailureTopicPartitionExceptionMap) {\n+      _sendFailureTopicPartitionExceptionMap.forEach(this::rewindAndPausePartitionOnException);", "originalCommit": "d4f1409f0c7f299257e2d5cb4b72ebf01d32284c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY3Njg0OQ==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417676849", "bodyText": "Yeah so I did think that, but decided against it. When we add a TopicPartition to the auto-pause list on send failure, we can do it for two reasons: a) the send exception is not transient, b) the rewind failed. For (b) I think it is safer to potentially attempt the rewind on the next send failure if possible, in case the first rewind failed for some transient reason. When we resume TopicPartitions from the auto-pause list, we don't attempt to rewind them then.\nThe maximum number of rewinds we'll do for a TopicPartition = number of polls between flushes, since on each poll/flush we'll rewind the TopicPartition only once in spite of receiving multiple send callbacks.\nWe can potentially differentiate between rewind failure vs. non-transient exception and use that to decide whether to rewind in the future. This may need more thought though. The current code is no worse than the existing code in terms of doing no-op rewinds. What do you think? Okay to leave this as is for now?", "author": "somandal", "createdAt": "2020-04-29T23:48:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY2OTYxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY3NzcyNw==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417677727", "bodyText": "Do you want to mark it as TODO then with the explanation?", "author": "vmaheshw", "createdAt": "2020-04-29T23:51:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY2OTYxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY3ODQ4Nw==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417678487", "bodyText": "Sure, will do, and I'll open a ticket. Will not leave this change hanging forever.", "author": "somandal", "createdAt": "2020-04-29T23:53:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY2OTYxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY3ODUxOA==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417678518", "bodyText": "Can you add TODO and the explanation so that the thought is not lost in the process.", "author": "vmaheshw", "createdAt": "2020-04-29T23:53:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY2OTYxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4NzAxMA==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r417687010", "bodyText": "done", "author": "somandal", "createdAt": "2020-04-30T00:22:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY2OTYxMg=="}], "type": "inlineReview"}, {"oid": "9037fad190aabc304c4b5f143c016f6a2f8b509d", "url": "https://github.com/linkedin/brooklin/commit/9037fad190aabc304c4b5f143c016f6a2f8b509d", "message": "Address review comments", "committedDate": "2020-04-30T00:41:06Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODE3OTU5Nw==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r418179597", "bodyText": "You might have mentioned this to me before, but isnt a ConcurrentHashMap useful here?", "author": "DEEPTHIKORAT", "createdAt": "2020-04-30T17:40:39Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -109,6 +109,9 @@\n   protected Consumer<?, ?> _consumer;\n   protected final Set<TopicPartition> _consumerAssignment = new HashSet<>();\n \n+  // TopicPartitions which have seen exceptions on send. Access to this map must be synchronized.\n+  private final Map<TopicPartition, Exception> _sendFailureTopicPartitionExceptionMap = new HashMap<>();", "originalCommit": "9037fad190aabc304c4b5f143c016f6a2f8b509d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODE5MTk1NA==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r418191954", "bodyText": "So there is this one place where I iterate over all the elements and then clear the map. I wanted that block to be synchronized as a whole, which is why I decided to use HashMap instead. That block can be contended with the callbacks on code paths were we skip flush() between calls to poll(). Due to this my worry is that in the middle of iterating, if a new entry gets added due to a send callback completion, what if i miss processing it? Since I clear the map after it, we might miss rewinding some topic partitions.\nDocumentation for Concurrent collections seem to indicate that there is weak consistency while iterating (https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/package-summary.html#Weakly):\n\nMost concurrent Collection implementations (including most Queues) also differ from the usual java.util conventions in that their Iterators and Spliterators provide weakly consistent rather than fast-fail traversal:\nthey may proceed concurrently with other operations\nthey will never throw ConcurrentModificationException\nthey are guaranteed to traverse elements as they existed upon construction exactly once, and may (but are not guaranteed to) reflect any modifications subsequent to construction.", "author": "somandal", "createdAt": "2020-04-30T18:01:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODE3OTU5Nw=="}], "type": "inlineReview"}, {"oid": "69be31a94b9fa2ace31df0cb9569bcd651484bd1", "url": "https://github.com/linkedin/brooklin/commit/69be31a94b9fa2ace31df0cb9569bcd651484bd1", "message": "Add comment explaining why HashMap vs. ConcurrentHashMap", "committedDate": "2020-04-30T18:04:39Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTIxNTA1Ng==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r419215056", "bodyText": "Do we need this extra explanation? It might become a norm every time to put this in comment. I understand this will make the understanding easier, but do we then want to put every time going forward. It might become cumbersome.", "author": "vmaheshw", "createdAt": "2020-05-04T05:04:50Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -109,6 +109,11 @@\n   protected Consumer<?, ?> _consumer;\n   protected final Set<TopicPartition> _consumerAssignment = new HashSet<>();\n \n+  // TopicPartitions which have seen exceptions on send. Access to this map must be synchronized.\n+  // A ConcurrentHashMap is not used here due to the need for having more than one operation performed together as an", "originalCommit": "69be31a94b9fa2ace31df0cb9569bcd651484bd1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTIxNjA5NQ==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r419216095", "bodyText": "Deepthi pointed out that in the future someone may accidentally change this to a concurrent hashmap, and having a comment explaining why may help prevent that. I think it\u2019s okay to explain when important so that people aren\u2019t left wondering why one data structure is chosen over another. What do you think?", "author": "somandal", "createdAt": "2020-05-04T05:11:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTIxNTA1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTIxNjQwMQ==", "url": "https://github.com/linkedin/brooklin/pull/705#discussion_r419216401", "bodyText": "Sure. sounds good.", "author": "vmaheshw", "createdAt": "2020-05-04T05:13:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTIxNTA1Ng=="}], "type": "inlineReview"}]}