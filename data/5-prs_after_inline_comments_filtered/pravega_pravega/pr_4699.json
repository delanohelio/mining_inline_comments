{"pr_number": 4699, "pr_title": "Issue 4676: (PDP-34) Part 2 of 4 - Implementation of ChunkStorage for HDFS, FileSystem and ExtendedS3.", "pr_createdAt": "2020-04-16T01:47:36Z", "pr_url": "https://github.com/pravega/pravega/pull/4699", "timeline": [{"oid": "21221c46235598cd3ea78f0255cfeb7457fad145", "url": "https://github.com/pravega/pravega/commit/21221c46235598cd3ea78f0255cfeb7457fad145", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Implementation of ChunkStorageProvider for HDFS, FileSystem and ExtendedS3.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-06-04T23:18:00Z", "type": "forcePushed"}, {"oid": "931c0f24ae41c1e3d21e609b9270dc5fb45e0b47", "url": "https://github.com/pravega/pravega/commit/931c0f24ae41c1e3d21e609b9270dc5fb45e0b47", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Implementation of ChunkStorageProvider for HDFS, FileSystem and ExtendedS3.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-06-12T02:40:50Z", "type": "forcePushed"}, {"oid": "5b0e178c83b487d6bf5b13c23e55d69ed0c4ce4b", "url": "https://github.com/pravega/pravega/commit/5b0e178c83b487d6bf5b13c23e55d69ed0c4ce4b", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Implementation of ChunkStorageProvider for HDFS, FileSystem and ExtendedS3.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-06-24T21:48:59Z", "type": "forcePushed"}, {"oid": "000dba2d0d433b1ec4f45c6f8dc52517a2e6e614", "url": "https://github.com/pravega/pravega/commit/000dba2d0d433b1ec4f45c6f8dc52517a2e6e614", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Fix review comments.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-02T15:06:31Z", "type": "forcePushed"}, {"oid": "2915a6f096ffbf738c471619c5495ec5b98716a8", "url": "https://github.com/pravega/pravega/commit/2915a6f096ffbf738c471619c5495ec5b98716a8", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Rename ChunkManager to ChunkedSegmentStorage.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-08T17:05:59Z", "type": "forcePushed"}, {"oid": "8e2a57e4543698481347fec8f5cc603c5e4e3777", "url": "https://github.com/pravega/pravega/commit/8e2a57e4543698481347fec8f5cc603c5e4e3777", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Implementation of ChunkStorage for HDFS, FileSystem and ExtendedS3.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T23:13:16Z", "type": "commit"}, {"oid": "8e2a57e4543698481347fec8f5cc603c5e4e3777", "url": "https://github.com/pravega/pravega/commit/8e2a57e4543698481347fec8f5cc603c5e4e3777", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Implementation of ChunkStorage for HDFS, FileSystem and ExtendedS3.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T23:13:16Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDE4ODMwMQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454188301", "bodyText": "I see that we are concatenating very often config.getPrefix() + handle.getChunkName():\n\nit is better to have a common function to compose the final object name\nwe should validate the chunkName and check for invalid values for S3, probably sanitize it by replacing invalid characters\nwhat about adding some know separator between the prefix and the chunkName ? this way it won't be possible to mix data from two separate Pravega clusters that share the same S3 bucket. I know this is a remote possibility because \"prefix\" should be unique and checkName will be some kind of UUID, but adding a check does not cost very much and will make this safer", "author": "eolivelli", "createdAt": "2020-07-14T08:23:22Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,325 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        int totalBytesConcatenated = 0;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            String targetPath = config.getPrefix() + chunks[0].getName();\n+            String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new ChunkNotFoundException(chunks[0].getName(), \"Target segment does not exist\");\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName());\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName(),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcatenated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                client.deleteObject(config.getBucket(), config.getPrefix() + chunks[i].getName());\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.\n+            // Error message is REC_CATCH_EXCEPTION: Exception is caught when Exception is not thrown\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } catch (Exception e) {\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        }\n+        return totalBytesConcatenated;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException {\n+        try {\n+            setPermission(handle, isReadOnly ? Permission.READ : Permission.FULL_CONTROL);\n+            return true;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+    }\n+\n+    private void setPermission(ChunkHandle handle, Permission permission) {\n+        AccessControlList acl = client.getObjectAcl(config.getBucket(), config.getPrefix() + handle.getChunkName());", "originalCommit": "8e2a57e4543698481347fec8f5cc603c5e4e3777", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYzNDI4MA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454634280", "bodyText": "I fixed the first issue.\nNot sure about 2nd and 3rd. These names are generated by us and should not contain invalid chars.", "author": "sachin-j-joshi", "createdAt": "2020-07-14T20:46:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDE4ODMwMQ=="}], "type": "inlineReview"}, {"oid": "dd27958f57c16d871c7bb1eb390817d5787a7c17", "url": "https://github.com/pravega/pravega/commit/dd27958f57c16d871c7bb1eb390817d5787a7c17", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Code cleanup.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-14T19:45:12Z", "type": "commit"}, {"oid": "7070fa7fd2549b9bcf3577e4260be3882218524d", "url": "https://github.com/pravega/pravega/commit/7070fa7fd2549b9bcf3577e4260be3882218524d", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Code cleanup.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-14T20:31:30Z", "type": "commit"}, {"oid": "9cb3101676aff678eb0bf589ddfbdcaa933a6847", "url": "https://github.com/pravega/pravega/commit/9cb3101676aff678eb0bf589ddfbdcaa933a6847", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Update comment.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-15T15:12:01Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4MzkwMw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455083903", "bodyText": "This can be a long process for a single operation. Irrespective of the time this operation can take, my question is: what happens if this operation fails within this loop? Do we leave chunks half concatenated? How do we complete this operation if it is interrupted in the middle of the processing (some chunks are concatenated, some others not yet)?", "author": "RaulGracia", "createdAt": "2020-07-15T14:13:33Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.AbortMultipartUploadRequest;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    getObjectPath(handle.getChunkName()), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            val objectPath = getObjectPath(handle.getChunkName());\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), objectPath);\n+            client.putObject(this.config.getBucket(), objectPath,\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        int totalBytesConcatenated = 0;\n+        String targetPath = getObjectPath(chunks[0].getName());\n+        String uploadId = null;\n+        boolean isCompleted = false;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new ChunkNotFoundException(chunks[0].getName(), \"Target segment does not exist\");\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {", "originalCommit": "7070fa7fd2549b9bcf3577e4260be3882218524d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0MDU2Ng==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455140566", "bodyText": "Extended S3 guarantees that MPU operation is automatic.", "author": "sachin-j-joshi", "createdAt": "2020-07-15T15:27:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4MzkwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4NzA4OA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455087088", "bodyText": "Are we expecting false here if this process cannot complete? Otherwise, why to return anything at all? It can either execute normally or throw and do not return anything.", "author": "RaulGracia", "createdAt": "2020-07-15T14:17:45Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.AbortMultipartUploadRequest;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    getObjectPath(handle.getChunkName()), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            val objectPath = getObjectPath(handle.getChunkName());\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), objectPath);\n+            client.putObject(this.config.getBucket(), objectPath,\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        int totalBytesConcatenated = 0;\n+        String targetPath = getObjectPath(chunks[0].getName());\n+        String uploadId = null;\n+        boolean isCompleted = false;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new ChunkNotFoundException(chunks[0].getName(), \"Target segment does not exist\");\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()));\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcatenated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            isCompleted = true;\n+\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                try {\n+                    client.deleteObject(config.getBucket(), getObjectPath(chunks[i].getName()));\n+                } catch (Exception e) {\n+                    log.warn(\"Could not delete source chunk - chunk={}.\", chunks[i].getName(), e);\n+                }\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.\n+            // Error message is REC_CATCH_EXCEPTION: Exception is caught when Exception is not thrown\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } catch (Exception e) {\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } finally {\n+            if (!isCompleted && null != uploadId) {\n+                client.abortMultipartUpload(new AbortMultipartUploadRequest(config.getBucket(), targetPath, uploadId));\n+            }\n+        }\n+        return totalBytesConcatenated;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException {\n+        try {\n+            setPermission(handle, isReadOnly ? Permission.READ : Permission.FULL_CONTROL);\n+            return true;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }", "originalCommit": "7070fa7fd2549b9bcf3577e4260be3882218524d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTI2MDIyOQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455260229", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-07-15T18:34:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4NzA4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4ODcyMA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455088720", "bodyText": "If this \"NoSuchKey\" is something of the protocol, put it at least as a constant for readability", "author": "RaulGracia", "createdAt": "2020-07-15T14:19:55Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.AbortMultipartUploadRequest;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    getObjectPath(handle.getChunkName()), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            val objectPath = getObjectPath(handle.getChunkName());\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), objectPath);\n+            client.putObject(this.config.getBucket(), objectPath,\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        int totalBytesConcatenated = 0;\n+        String targetPath = getObjectPath(chunks[0].getName());\n+        String uploadId = null;\n+        boolean isCompleted = false;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new ChunkNotFoundException(chunks[0].getName(), \"Target segment does not exist\");\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()));\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcatenated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            isCompleted = true;\n+\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                try {\n+                    client.deleteObject(config.getBucket(), getObjectPath(chunks[i].getName()));\n+                } catch (Exception e) {\n+                    log.warn(\"Could not delete source chunk - chunk={}.\", chunks[i].getName(), e);\n+                }\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.\n+            // Error message is REC_CATCH_EXCEPTION: Exception is caught when Exception is not thrown\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } catch (Exception e) {\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } finally {\n+            if (!isCompleted && null != uploadId) {\n+                client.abortMultipartUpload(new AbortMultipartUploadRequest(config.getBucket(), targetPath, uploadId));\n+            }\n+        }\n+        return totalBytesConcatenated;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException {\n+        try {\n+            setPermission(handle, isReadOnly ? Permission.READ : Permission.FULL_CONTROL);\n+            return true;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+    }\n+\n+    private void setPermission(ChunkHandle handle, Permission permission) {\n+        AccessControlList acl = client.getObjectAcl(config.getBucket(), getObjectPath(handle.getChunkName()));\n+        acl.getGrants().clear();\n+        acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), permission));\n+\n+        client.setObjectAcl(\n+                new SetObjectAclRequest(config.getBucket(), getObjectPath(handle.getChunkName())).withAcl(acl));\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(),\n+                    getObjectPath(chunkName));\n+\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(result.getContentLength())\n+                    .build();\n+\n+            return information;\n+        } catch (Exception e) {\n+            throw convertException(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            if (!client.listObjects(config.getBucket(), getObjectPath(chunkName)).getObjects().isEmpty()) {\n+                throw new ChunkAlreadyExistsException(chunkName, \"Chunk already exists\");\n+            }\n+\n+            S3ObjectMetadata metadata = new S3ObjectMetadata();\n+            metadata.setContentLength((long) 0);\n+\n+            PutObjectRequest request = new PutObjectRequest(config.getBucket(), getObjectPath(chunkName), null);\n+\n+            AccessControlList acl = new AccessControlList();\n+            acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), Permission.FULL_CONTROL));\n+            request.setAcl(acl);\n+\n+            if (config.isUseNoneMatch()) {\n+                request.setIfNoneMatch(\"*\");\n+            }\n+            client.putObject(request);\n+\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (Exception e) {\n+            throw convertException(chunkName, \"doCreate\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            client.getObjectMetadata(config.getBucket(), getObjectPath(chunkName));\n+            return true;\n+        } catch (S3Exception e) {\n+            if (e.getErrorCode().equals(\"NoSuchKey\")) {\n+                return false;\n+            } else {\n+                throw convertException(chunkName, \"checkExists\", e);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            client.deleteObject(config.getBucket(), getObjectPath(handle.getChunkName()));\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    private ChunkStorageException convertException(String chunkName, String message, Exception e)  {\n+        ChunkStorageException retValue = null;\n+        if (e instanceof ChunkStorageException) {\n+            return (ChunkStorageException) e;\n+        }\n+        if (e instanceof S3Exception) {\n+            S3Exception s3Exception = (S3Exception) e;\n+            String errorCode = Strings.nullToEmpty(s3Exception.getErrorCode());\n+\n+            if (errorCode.equals(\"NoSuchKey\")) {", "originalCommit": "7070fa7fd2549b9bcf3577e4260be3882218524d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE2MTkwMg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455161902", "bodyText": "see below", "author": "sachin-j-joshi", "createdAt": "2020-07-15T16:01:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4ODcyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4OTE4Mg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455089182", "bodyText": "I don't know if we would need a place to set all these hardcoded strings for better readability.", "author": "RaulGracia", "createdAt": "2020-07-15T14:20:32Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.AbortMultipartUploadRequest;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    getObjectPath(handle.getChunkName()), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            val objectPath = getObjectPath(handle.getChunkName());\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), objectPath);\n+            client.putObject(this.config.getBucket(), objectPath,\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        int totalBytesConcatenated = 0;\n+        String targetPath = getObjectPath(chunks[0].getName());\n+        String uploadId = null;\n+        boolean isCompleted = false;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new ChunkNotFoundException(chunks[0].getName(), \"Target segment does not exist\");\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()));\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcatenated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            isCompleted = true;\n+\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                try {\n+                    client.deleteObject(config.getBucket(), getObjectPath(chunks[i].getName()));\n+                } catch (Exception e) {\n+                    log.warn(\"Could not delete source chunk - chunk={}.\", chunks[i].getName(), e);\n+                }\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.\n+            // Error message is REC_CATCH_EXCEPTION: Exception is caught when Exception is not thrown\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } catch (Exception e) {\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } finally {\n+            if (!isCompleted && null != uploadId) {\n+                client.abortMultipartUpload(new AbortMultipartUploadRequest(config.getBucket(), targetPath, uploadId));\n+            }\n+        }\n+        return totalBytesConcatenated;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException {\n+        try {\n+            setPermission(handle, isReadOnly ? Permission.READ : Permission.FULL_CONTROL);\n+            return true;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+    }\n+\n+    private void setPermission(ChunkHandle handle, Permission permission) {\n+        AccessControlList acl = client.getObjectAcl(config.getBucket(), getObjectPath(handle.getChunkName()));\n+        acl.getGrants().clear();\n+        acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), permission));\n+\n+        client.setObjectAcl(\n+                new SetObjectAclRequest(config.getBucket(), getObjectPath(handle.getChunkName())).withAcl(acl));\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(),\n+                    getObjectPath(chunkName));\n+\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(result.getContentLength())\n+                    .build();\n+\n+            return information;\n+        } catch (Exception e) {\n+            throw convertException(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            if (!client.listObjects(config.getBucket(), getObjectPath(chunkName)).getObjects().isEmpty()) {\n+                throw new ChunkAlreadyExistsException(chunkName, \"Chunk already exists\");\n+            }\n+\n+            S3ObjectMetadata metadata = new S3ObjectMetadata();\n+            metadata.setContentLength((long) 0);\n+\n+            PutObjectRequest request = new PutObjectRequest(config.getBucket(), getObjectPath(chunkName), null);\n+\n+            AccessControlList acl = new AccessControlList();\n+            acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), Permission.FULL_CONTROL));\n+            request.setAcl(acl);\n+\n+            if (config.isUseNoneMatch()) {\n+                request.setIfNoneMatch(\"*\");\n+            }\n+            client.putObject(request);\n+\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (Exception e) {\n+            throw convertException(chunkName, \"doCreate\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            client.getObjectMetadata(config.getBucket(), getObjectPath(chunkName));\n+            return true;\n+        } catch (S3Exception e) {\n+            if (e.getErrorCode().equals(\"NoSuchKey\")) {\n+                return false;\n+            } else {\n+                throw convertException(chunkName, \"checkExists\", e);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            client.deleteObject(config.getBucket(), getObjectPath(handle.getChunkName()));\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    private ChunkStorageException convertException(String chunkName, String message, Exception e)  {\n+        ChunkStorageException retValue = null;\n+        if (e instanceof ChunkStorageException) {\n+            return (ChunkStorageException) e;\n+        }\n+        if (e instanceof S3Exception) {\n+            S3Exception s3Exception = (S3Exception) e;\n+            String errorCode = Strings.nullToEmpty(s3Exception.getErrorCode());\n+\n+            if (errorCode.equals(\"NoSuchKey\")) {\n+                retValue =  new ChunkNotFoundException(chunkName, message, e);\n+            }\n+\n+            if (errorCode.equals(\"PreconditionFailed\")) {\n+                retValue =  new ChunkAlreadyExistsException(chunkName, message, e);\n+            }\n+\n+            if (errorCode.equals(\"InvalidRange\")\n+                    || errorCode.equals(\"InvalidArgument\")", "originalCommit": "7070fa7fd2549b9bcf3577e4260be3882218524d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTEzNzgwMA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455137800", "bodyText": "IMO, This is the best place to keep them. They are ExtendedS3 specific and nobody else should need to know about them.", "author": "andreipaduroiu", "createdAt": "2020-07-15T15:23:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4OTE4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE2MTc5Nw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455161797", "bodyText": "agree.", "author": "sachin-j-joshi", "createdAt": "2020-07-15T16:01:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4OTE4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA5NDQ3OA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455094478", "bodyText": "This kind of string concatenations may be expensive if executed too often.", "author": "RaulGracia", "createdAt": "2020-07-15T14:27:22Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.AbortMultipartUploadRequest;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    getObjectPath(handle.getChunkName()), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            val objectPath = getObjectPath(handle.getChunkName());\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), objectPath);\n+            client.putObject(this.config.getBucket(), objectPath,\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        int totalBytesConcatenated = 0;\n+        String targetPath = getObjectPath(chunks[0].getName());\n+        String uploadId = null;\n+        boolean isCompleted = false;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new ChunkNotFoundException(chunks[0].getName(), \"Target segment does not exist\");\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()));\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcatenated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            isCompleted = true;\n+\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                try {\n+                    client.deleteObject(config.getBucket(), getObjectPath(chunks[i].getName()));\n+                } catch (Exception e) {\n+                    log.warn(\"Could not delete source chunk - chunk={}.\", chunks[i].getName(), e);\n+                }\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.\n+            // Error message is REC_CATCH_EXCEPTION: Exception is caught when Exception is not thrown\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } catch (Exception e) {\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } finally {\n+            if (!isCompleted && null != uploadId) {\n+                client.abortMultipartUpload(new AbortMultipartUploadRequest(config.getBucket(), targetPath, uploadId));\n+            }\n+        }\n+        return totalBytesConcatenated;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException {\n+        try {\n+            setPermission(handle, isReadOnly ? Permission.READ : Permission.FULL_CONTROL);\n+            return true;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+    }\n+\n+    private void setPermission(ChunkHandle handle, Permission permission) {\n+        AccessControlList acl = client.getObjectAcl(config.getBucket(), getObjectPath(handle.getChunkName()));\n+        acl.getGrants().clear();\n+        acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), permission));\n+\n+        client.setObjectAcl(\n+                new SetObjectAclRequest(config.getBucket(), getObjectPath(handle.getChunkName())).withAcl(acl));\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(),\n+                    getObjectPath(chunkName));\n+\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(result.getContentLength())\n+                    .build();\n+\n+            return information;\n+        } catch (Exception e) {\n+            throw convertException(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            if (!client.listObjects(config.getBucket(), getObjectPath(chunkName)).getObjects().isEmpty()) {\n+                throw new ChunkAlreadyExistsException(chunkName, \"Chunk already exists\");\n+            }\n+\n+            S3ObjectMetadata metadata = new S3ObjectMetadata();\n+            metadata.setContentLength((long) 0);\n+\n+            PutObjectRequest request = new PutObjectRequest(config.getBucket(), getObjectPath(chunkName), null);\n+\n+            AccessControlList acl = new AccessControlList();\n+            acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), Permission.FULL_CONTROL));\n+            request.setAcl(acl);\n+\n+            if (config.isUseNoneMatch()) {\n+                request.setIfNoneMatch(\"*\");\n+            }\n+            client.putObject(request);\n+\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (Exception e) {\n+            throw convertException(chunkName, \"doCreate\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            client.getObjectMetadata(config.getBucket(), getObjectPath(chunkName));\n+            return true;\n+        } catch (S3Exception e) {\n+            if (e.getErrorCode().equals(\"NoSuchKey\")) {\n+                return false;\n+            } else {\n+                throw convertException(chunkName, \"checkExists\", e);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            client.deleteObject(config.getBucket(), getObjectPath(handle.getChunkName()));\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    private ChunkStorageException convertException(String chunkName, String message, Exception e)  {\n+        ChunkStorageException retValue = null;\n+        if (e instanceof ChunkStorageException) {\n+            return (ChunkStorageException) e;\n+        }\n+        if (e instanceof S3Exception) {\n+            S3Exception s3Exception = (S3Exception) e;\n+            String errorCode = Strings.nullToEmpty(s3Exception.getErrorCode());\n+\n+            if (errorCode.equals(\"NoSuchKey\")) {\n+                retValue =  new ChunkNotFoundException(chunkName, message, e);\n+            }\n+\n+            if (errorCode.equals(\"PreconditionFailed\")) {\n+                retValue =  new ChunkAlreadyExistsException(chunkName, message, e);\n+            }\n+\n+            if (errorCode.equals(\"InvalidRange\")\n+                    || errorCode.equals(\"InvalidArgument\")\n+                    || errorCode.equals(\"MethodNotAllowed\")\n+                    || s3Exception.getHttpCode() == HttpStatus.SC_REQUESTED_RANGE_NOT_SATISFIABLE) {\n+                retValue =  new ChunkStorageException(chunkName, String.format(\"IllegalArgumentException\", e));\n+            }\n+\n+            if (errorCode.equals(\"AccessDenied\")) {\n+                retValue =  new ChunkStorageException(chunkName, String.format(\"Access denied for chunk %s - %s.\", chunkName, message), e);\n+            }\n+        }\n+\n+        if (retValue == null) {\n+            retValue = new ChunkStorageException(chunkName, message, e);\n+        }\n+\n+        return retValue;\n+    }\n+\n+    private String getObjectPath(String objectName) {\n+        return config.getPrefix() + objectName;", "originalCommit": "7070fa7fd2549b9bcf3577e4260be3882218524d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE1MDk5NQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455150995", "bodyText": "Not really. concatenation is not in loop or anything. This concat is unavoidable we need to add object name to prefix and then pass it to S3 client as parameter.\nI am sure this is the fastest way to concat two strings. String.format or other things like StringBuffer etc would have additional overhead that is not necessary.", "author": "sachin-j-joshi", "createdAt": "2020-07-15T15:43:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA5NDQ3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTExNjY5Mw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455116693", "bodyText": "Could it be possible that by some error we end up with a negative length that prevent this loop from finishing? Would a condition like length > 0 be safer in this case?", "author": "RaulGracia", "createdAt": "2020-07-15T14:54:59Z", "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,305 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        return FileChannel.open(path, openOption);\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        return Files.size(path);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throw  convertExeption(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            FileAttribute<Set<PosixFilePermission>> fileAttributes = PosixFilePermissions.asFileAttribute(FileSystemUtils.READ_WRITE_PERMISSION);\n+\n+            Path path = Paths.get(config.getRoot(), chunkName);\n+            Path parent = path.getParent();\n+            assert parent != null;\n+            Files.createDirectories(parent);\n+            Files.createFile(path, fileAttributes);\n+\n+        } catch (IOException e) {\n+            throw convertExeption(chunkName, \"doCreate\", e);\n+        }\n+\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    public ChunkStorageException convertExeption(String chunkName, String message, Exception e) throws ChunkStorageException {\n+        if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {\n+            return new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        if (e instanceof FileAlreadyExistsException) {\n+            return  new ChunkAlreadyExistsException(chunkName, message, e);\n+        }\n+        return new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws IllegalArgumentException {\n+        return Files.exists(Paths.get(config.getRoot(), chunkName));\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            Files.delete(Paths.get(config.getRoot(), handle.getChunkName()));\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenRead\");\n+        }\n+\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenWrite\");\n+        } else if (Files.isWritable(path)) {\n+            return ChunkHandle.writeHandle(chunkName);\n+        } else {\n+            return ChunkHandle.readHandle(chunkName);\n+        }\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        Timer timer = new Timer();\n+\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+        try {\n+            long fileSize = getFileSize(path);\n+            if (fileSize < fromOffset) {\n+                throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the \" +\n+                        \"current size of chunk (%d).\", fromOffset, fileSize));\n+            }\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.READ)) {\n+            int totalBytesRead = 0;\n+            long readOffset = fromOffset;\n+            do {\n+                ByteBuffer readBuffer = ByteBuffer.wrap(buffer, bufferOffset, length);\n+                int bytesRead = channel.read(readBuffer, readOffset);\n+                bufferOffset += bytesRead;\n+                totalBytesRead += bytesRead;\n+                length -= bytesRead;\n+                readOffset += bytesRead;\n+            } while (length != 0);", "originalCommit": "7070fa7fd2549b9bcf3577e4260be3882218524d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTEzNDE0Mw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455134143", "bodyText": "+1.\nI'm always advocating for a broader condition in these cases for the same reason as Raul mentioned.\nIf you look below, you already use this pattern. Let's be consistent.", "author": "andreipaduroiu", "createdAt": "2020-07-15T15:19:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTExNjY5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMxOTQ4MA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455319480", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-07-15T20:23:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTExNjY5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0NzE5NQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455147195", "bodyText": "Why would client be null? you set it in the constructor", "author": "andreipaduroiu", "createdAt": "2020-07-15T15:37:41Z", "path": "bindings/src/test/java/io/pravega/storage/extendeds3/ExtendedS3TestContext.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.s3.S3Config;\n+import com.emc.object.s3.bean.ObjectKey;\n+import com.emc.object.s3.jersey.S3JerseyClient;\n+import com.emc.object.s3.request.DeleteObjectsRequest;\n+import com.emc.object.util.ConfigUri;\n+import io.pravega.test.common.TestUtils;\n+\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Test context Extended S3 tests.\n+ */\n+public class ExtendedS3TestContext {\n+    public static final String BUCKET_NAME_PREFIX = \"pravegatest-\";\n+    public final ExtendedS3StorageConfig adapterConfig;\n+    public final S3JerseyClient client;\n+    public final S3ImplBase s3Proxy;\n+    public final int port = TestUtils.getAvailableListenPort();\n+    public final String configUri = \"http://127.0.0.1:\" + port + \"?identity=x&secretKey=x\";\n+    public final S3Config s3Config;\n+\n+    public ExtendedS3TestContext() throws Exception {\n+        String bucketName = BUCKET_NAME_PREFIX + UUID.randomUUID().toString();\n+        this.adapterConfig = ExtendedS3StorageConfig.builder()\n+                .with(ExtendedS3StorageConfig.CONFIGURI, configUri)\n+                .with(ExtendedS3StorageConfig.BUCKET, bucketName)\n+                .with(ExtendedS3StorageConfig.PREFIX, \"samplePrefix\")\n+                .build();\n+        s3Config = new ConfigUri<>(S3Config.class).parseUri(configUri);\n+        s3Proxy = new S3ProxyImpl(configUri, s3Config);\n+        s3Proxy.start();\n+        client = new S3JerseyClientWrapper(s3Config, s3Proxy);\n+        client.createBucket(bucketName);\n+        List<ObjectKey> keys = client.listObjects(bucketName).getObjects().stream()\n+                .map(object -> new ObjectKey(object.getKey()))\n+                .collect(Collectors.toList());\n+\n+        if (!keys.isEmpty()) {\n+            client.deleteObjects(new DeleteObjectsRequest(bucketName).withKeys(keys));\n+        }\n+    }\n+\n+    public void close() throws Exception {\n+        if (client != null) {", "originalCommit": "7070fa7fd2549b9bcf3577e4260be3882218524d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE5ODgwNg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455198806", "bodyText": "in case close is called on half initialized object.", "author": "sachin-j-joshi", "createdAt": "2020-07-15T17:02:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0NzE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTI5OTk4NA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455299984", "bodyText": "What is a half initialized object? Can you show me an example where only half of a constructor executes and then you get an object as a result?", "author": "andreipaduroiu", "createdAt": "2020-07-15T19:46:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0NzE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMxMTcwOA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455311708", "bodyText": "line 56 above.", "author": "sachin-j-joshi", "createdAt": "2020-07-15T20:08:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0NzE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMxODk1OA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455318958", "bodyText": "You can not write following code\ntry {\n} catch (Exception e) {\n         if (client != null) {\n             client.destroy();\n         }\n         if (s3Proxy != null) {\n             s3Proxy.stop();\n         }\n         throw e;\n     }\n\n\nTask :bindings:compileTestJava\n/home/sachin/workspace/active5/pravega/bindings/src/test/java/io/pravega/storage/extendeds3/ExtendedS3TestContext.java:56: error: variable client might not have been initialized\nif (client != null) {\n^\n/home/sachin/workspace/active5/pravega/bindings/src/test/java/io/pravega/storage/extendeds3/ExtendedS3TestContext.java:59: error: variable s3Proxy might not have been initialized\nif (s3Proxy != null) {\n^\n2 errors\n\n\nTask :bindings:compileTestJava FAILED\n\n\nbut following works so keeping it that way.\n} catch (Exception e) {\n            close();\n            throw e;\n        }\n\n\n  public void close() throws Exception {\n        if (client != null) {\n            client.destroy();\n        }\n        if (s3Proxy != null) {\n            s3Proxy.stop();\n        }\n    }\n}", "author": "sachin-j-joshi", "createdAt": "2020-07-15T20:22:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0NzE5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0NzY0Ng==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455147646", "bodyText": "There is a chance that something in the constructor fails. We want to put a try-catch block here to shut down the s3proxy if we fail before we exit the constructor.", "author": "andreipaduroiu", "createdAt": "2020-07-15T15:38:25Z", "path": "bindings/src/test/java/io/pravega/storage/extendeds3/ExtendedS3TestContext.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.s3.S3Config;\n+import com.emc.object.s3.bean.ObjectKey;\n+import com.emc.object.s3.jersey.S3JerseyClient;\n+import com.emc.object.s3.request.DeleteObjectsRequest;\n+import com.emc.object.util.ConfigUri;\n+import io.pravega.test.common.TestUtils;\n+\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Test context Extended S3 tests.\n+ */\n+public class ExtendedS3TestContext {\n+    public static final String BUCKET_NAME_PREFIX = \"pravegatest-\";\n+    public final ExtendedS3StorageConfig adapterConfig;\n+    public final S3JerseyClient client;\n+    public final S3ImplBase s3Proxy;\n+    public final int port = TestUtils.getAvailableListenPort();\n+    public final String configUri = \"http://127.0.0.1:\" + port + \"?identity=x&secretKey=x\";\n+    public final S3Config s3Config;\n+\n+    public ExtendedS3TestContext() throws Exception {\n+        String bucketName = BUCKET_NAME_PREFIX + UUID.randomUUID().toString();\n+        this.adapterConfig = ExtendedS3StorageConfig.builder()\n+                .with(ExtendedS3StorageConfig.CONFIGURI, configUri)\n+                .with(ExtendedS3StorageConfig.BUCKET, bucketName)\n+                .with(ExtendedS3StorageConfig.PREFIX, \"samplePrefix\")\n+                .build();\n+        s3Config = new ConfigUri<>(S3Config.class).parseUri(configUri);\n+        s3Proxy = new S3ProxyImpl(configUri, s3Config);\n+        s3Proxy.start();", "originalCommit": "7070fa7fd2549b9bcf3577e4260be3882218524d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE3NDg2Ng==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455174866", "bodyText": "The close is called in  @after in test classes.", "author": "sachin-j-joshi", "createdAt": "2020-07-15T16:23:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0NzY0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTI5OTU4NA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455299584", "bodyText": "You will have nothing to call close on since this is the constructor. If the constructor throws, there will be no object to call close on, but the side effects will remain. It is your responsibility to clean up any such side effects in the constructor should it fail.", "author": "andreipaduroiu", "createdAt": "2020-07-15T19:45:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0NzY0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMxMjc2Mg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455312762", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-07-15T20:10:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0NzY0Ng=="}], "type": "inlineReview"}, {"oid": "76093576b69ac136b985a396a397ca56089069f1", "url": "https://github.com/pravega/pravega/commit/76093576b69ac136b985a396a397ca56089069f1", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Address review comments.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-15T17:44:09Z", "type": "commit"}, {"oid": "6b364af35c7dbda36d481b27a3d70501523c64c5", "url": "https://github.com/pravega/pravega/commit/6b364af35c7dbda36d481b27a3d70501523c64c5", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - make return value of doSetReadOnly as void.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-15T18:30:49Z", "type": "commit"}, {"oid": "d7bdf6a6254ac1ae1e6227a80851fa22031d142a", "url": "https://github.com/pravega/pravega/commit/d7bdf6a6254ac1ae1e6227a80851fa22031d142a", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - After concat delete chunks properly in ChunkedSegmentStorage instead of ChunkStorage.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-15T20:02:12Z", "type": "commit"}, {"oid": "52bdaef9b6ee8aa9ba1bcf5e9bff3b577e25c1f5", "url": "https://github.com/pravega/pravega/commit/52bdaef9b6ee8aa9ba1bcf5e9bff3b577e25c1f5", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Better parameter comment for defrag.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-15T20:34:20Z", "type": "commit"}, {"oid": "30225a38a4b6dde90ff3154ef14651097df8dd63", "url": "https://github.com/pravega/pravega/commit/30225a38a4b6dde90ff3154ef14651097df8dd63", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - More code coverage.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-16T01:55:08Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNDc0Nw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455734747", "bodyText": "as for S3 storage, we should have a common method to perform Paths.get(config.getRoot(), chunkName)\nit can also be a security issue to let chunkName be anystring.\nwe should have some validation that at least blocks:\n\nnull chars\nrelative path sequences (\"..\", \"/\",\"\",\".\")\nanche chunkname cannot be empty\n\nwe cannot assume that the caller is a good guy, even if this is our own code", "author": "eolivelli", "createdAt": "2020-07-16T12:03:09Z", "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        return FileChannel.open(path, openOption);\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        return Files.size(path);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),", "originalCommit": "30225a38a4b6dde90ff3154ef14651097df8dd63", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxOTU1Mg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456619552", "bodyText": "fixed the empty name part.\nNow also checking that path is not directory, that should handle second case.", "author": "sachin-j-joshi", "createdAt": "2020-07-17T18:55:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNDc0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNTU3Nw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455735577", "bodyText": "isn't Files.size(path)  enough ?\nwhy are we passing thru PosixFileAttributes ?", "author": "eolivelli", "createdAt": "2020-07-16T12:04:54Z", "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        return FileChannel.open(path, openOption);\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        return Files.size(path);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())", "originalCommit": "30225a38a4b6dde90ff3154ef14651097df8dd63", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNjI3Ng==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455736276", "bodyText": "you also have this getFileSize\nhttps://github.com/pravega/pravega/pull/4699/files#diff-17a1fa6c3b4ce4b99b95fde90189ccc5R190", "author": "eolivelli", "createdAt": "2020-07-16T12:06:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNTU3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwOTc0Nw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456609747", "bodyText": "This was from the time when I had isReadOnly as property of Chunk .\nFixed", "author": "sachin-j-joshi", "createdAt": "2020-07-17T18:34:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNTU3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNjEwNg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455736106", "bodyText": "please also check that this is a RegularFile, not a directory or a link (for instance)", "author": "eolivelli", "createdAt": "2020-07-16T12:05:55Z", "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        return FileChannel.open(path, openOption);\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        return Files.size(path);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throw  convertExeption(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            FileAttribute<Set<PosixFilePermission>> fileAttributes = PosixFilePermissions.asFileAttribute(FileSystemUtils.READ_WRITE_PERMISSION);\n+\n+            Path path = Paths.get(config.getRoot(), chunkName);\n+            Path parent = path.getParent();\n+            assert parent != null;\n+            Files.createDirectories(parent);\n+            Files.createFile(path, fileAttributes);\n+\n+        } catch (IOException e) {\n+            throw convertExeption(chunkName, \"doCreate\", e);\n+        }\n+\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    private ChunkStorageException convertExeption(String chunkName, String message, Exception e) {\n+        if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {\n+            return new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        if (e instanceof FileAlreadyExistsException) {\n+            return  new ChunkAlreadyExistsException(chunkName, message, e);\n+        }\n+        return new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws IllegalArgumentException {\n+        return Files.exists(Paths.get(config.getRoot(), chunkName));\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            Files.delete(Paths.get(config.getRoot(), handle.getChunkName()));\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenRead\");\n+        }\n+\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+        if (!Files.exists(path)) {", "originalCommit": "30225a38a4b6dde90ff3154ef14651097df8dd63", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIwNzA3OA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456207078", "bodyText": "Good check to add. Adding", "author": "sachin-j-joshi", "createdAt": "2020-07-17T04:08:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNjEwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwOTgzMw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456609833", "bodyText": "Fixed", "author": "sachin-j-joshi", "createdAt": "2020-07-17T18:34:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNjEwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNzMxNQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455737315", "bodyText": "this is bad from a security perspective\nString.valueOf(sourcePath)\nyou can use sourcePath.toFile()", "author": "eolivelli", "createdAt": "2020-07-16T12:08:19Z", "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        return FileChannel.open(path, openOption);\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        return Files.size(path);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throw  convertExeption(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            FileAttribute<Set<PosixFilePermission>> fileAttributes = PosixFilePermissions.asFileAttribute(FileSystemUtils.READ_WRITE_PERMISSION);\n+\n+            Path path = Paths.get(config.getRoot(), chunkName);\n+            Path parent = path.getParent();\n+            assert parent != null;\n+            Files.createDirectories(parent);\n+            Files.createFile(path, fileAttributes);\n+\n+        } catch (IOException e) {\n+            throw convertExeption(chunkName, \"doCreate\", e);\n+        }\n+\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    private ChunkStorageException convertExeption(String chunkName, String message, Exception e) {\n+        if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {\n+            return new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        if (e instanceof FileAlreadyExistsException) {\n+            return  new ChunkAlreadyExistsException(chunkName, message, e);\n+        }\n+        return new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws IllegalArgumentException {\n+        return Files.exists(Paths.get(config.getRoot(), chunkName));\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            Files.delete(Paths.get(config.getRoot(), handle.getChunkName()));\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenRead\");\n+        }\n+\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenWrite\");\n+        } else if (Files.isWritable(path)) {\n+            return ChunkHandle.writeHandle(chunkName);\n+        } else {\n+            return ChunkHandle.readHandle(chunkName);\n+        }\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+        try {\n+            long fileSize = getFileSize(path);\n+            if (fileSize < fromOffset) {\n+                throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the \" +\n+                        \"current size of chunk (%d).\", fromOffset, fileSize));\n+            }\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.READ)) {\n+            int totalBytesRead = 0;\n+            long readOffset = fromOffset;\n+            do {\n+                ByteBuffer readBuffer = ByteBuffer.wrap(buffer, bufferOffset, length);\n+                int bytesRead = channel.read(readBuffer, readOffset);\n+                bufferOffset += bytesRead;\n+                totalBytesRead += bytesRead;\n+                length -= bytesRead;\n+                readOffset += bytesRead;\n+            } while (length > 0);\n+            return totalBytesRead;\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException {\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+\n+        long totalBytesWritten = 0;\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.WRITE)) {\n+            long fileSize = channel.size();\n+            if (fileSize != offset) {\n+                throw new IndexOutOfBoundsException(String.format(\"fileSize (%d) did not match offset (%d) for chunk %s\", fileSize, offset, handle.getChunkName()));\n+            }\n+\n+            // Wrap the input data into a ReadableByteChannel, but do not close it. Doing so will result in closing\n+            // the underlying InputStream, which is not desirable if it is to be reused.\n+            ReadableByteChannel sourceChannel = Channels.newChannel(data);\n+            while (length > 0) {\n+                long bytesWritten = channel.transferFrom(sourceChannel, offset, length);\n+                assert bytesWritten > 0 : \"Unable to make any progress transferring data.\";\n+                offset += bytesWritten;\n+                totalBytesWritten += bytesWritten;\n+                length -= bytesWritten;\n+            }\n+            channel.force(true);\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return (int) totalBytesWritten;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        try {\n+            int totalBytesConcated = 0;\n+            Path targetPath = Paths.get(config.getRoot(), chunks[0].getName());\n+            long offset = chunks[0].getLength();\n+\n+            for (int i = 1; i < chunks.length; i++) {\n+                val source = chunks[i];\n+                Preconditions.checkArgument(!chunks[0].getName().equals(source.getName()), \"target and source can not be same.\");\n+                Path sourcePath = Paths.get(config.getRoot(), source.getName());\n+                long length = chunks[i].getLength();\n+                Preconditions.checkState(offset <= getFileSize(targetPath));\n+                Preconditions.checkState(length <= getFileSize(sourcePath));\n+                try (FileChannel targetChannel = getFileChannel(targetPath, StandardOpenOption.WRITE);\n+                     RandomAccessFile sourceFile = new RandomAccessFile(String.valueOf(sourcePath), \"r\")) {", "originalCommit": "30225a38a4b6dde90ff3154ef14651097df8dd63", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNzgwMw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455737803", "bodyText": "why are you using RandomAccessFile ?", "author": "eolivelli", "createdAt": "2020-07-16T12:09:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNzMxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIwNDQxMw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456204413", "bodyText": "Good catch.  Fixing it.", "author": "sachin-j-joshi", "createdAt": "2020-07-17T03:56:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNzMxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczODcxNA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455738714", "bodyText": "why are you opening the same file more times ?\nit is better to open the file for write only once.\nI suggest you also to:\n\ncreate a temporary file\nfill it\nrename new the file with ATOMIC_MOVE flag to the targetPath\nThis way we are protected from partial writes", "author": "eolivelli", "createdAt": "2020-07-16T12:10:53Z", "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        return FileChannel.open(path, openOption);\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        return Files.size(path);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throw  convertExeption(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            FileAttribute<Set<PosixFilePermission>> fileAttributes = PosixFilePermissions.asFileAttribute(FileSystemUtils.READ_WRITE_PERMISSION);\n+\n+            Path path = Paths.get(config.getRoot(), chunkName);\n+            Path parent = path.getParent();\n+            assert parent != null;\n+            Files.createDirectories(parent);\n+            Files.createFile(path, fileAttributes);\n+\n+        } catch (IOException e) {\n+            throw convertExeption(chunkName, \"doCreate\", e);\n+        }\n+\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    private ChunkStorageException convertExeption(String chunkName, String message, Exception e) {\n+        if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {\n+            return new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        if (e instanceof FileAlreadyExistsException) {\n+            return  new ChunkAlreadyExistsException(chunkName, message, e);\n+        }\n+        return new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws IllegalArgumentException {\n+        return Files.exists(Paths.get(config.getRoot(), chunkName));\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            Files.delete(Paths.get(config.getRoot(), handle.getChunkName()));\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenRead\");\n+        }\n+\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenWrite\");\n+        } else if (Files.isWritable(path)) {\n+            return ChunkHandle.writeHandle(chunkName);\n+        } else {\n+            return ChunkHandle.readHandle(chunkName);\n+        }\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+        try {\n+            long fileSize = getFileSize(path);\n+            if (fileSize < fromOffset) {\n+                throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the \" +\n+                        \"current size of chunk (%d).\", fromOffset, fileSize));\n+            }\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.READ)) {\n+            int totalBytesRead = 0;\n+            long readOffset = fromOffset;\n+            do {\n+                ByteBuffer readBuffer = ByteBuffer.wrap(buffer, bufferOffset, length);\n+                int bytesRead = channel.read(readBuffer, readOffset);\n+                bufferOffset += bytesRead;\n+                totalBytesRead += bytesRead;\n+                length -= bytesRead;\n+                readOffset += bytesRead;\n+            } while (length > 0);\n+            return totalBytesRead;\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException {\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+\n+        long totalBytesWritten = 0;\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.WRITE)) {\n+            long fileSize = channel.size();\n+            if (fileSize != offset) {\n+                throw new IndexOutOfBoundsException(String.format(\"fileSize (%d) did not match offset (%d) for chunk %s\", fileSize, offset, handle.getChunkName()));\n+            }\n+\n+            // Wrap the input data into a ReadableByteChannel, but do not close it. Doing so will result in closing\n+            // the underlying InputStream, which is not desirable if it is to be reused.\n+            ReadableByteChannel sourceChannel = Channels.newChannel(data);\n+            while (length > 0) {\n+                long bytesWritten = channel.transferFrom(sourceChannel, offset, length);\n+                assert bytesWritten > 0 : \"Unable to make any progress transferring data.\";\n+                offset += bytesWritten;\n+                totalBytesWritten += bytesWritten;\n+                length -= bytesWritten;\n+            }\n+            channel.force(true);\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return (int) totalBytesWritten;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        try {\n+            int totalBytesConcated = 0;\n+            Path targetPath = Paths.get(config.getRoot(), chunks[0].getName());\n+            long offset = chunks[0].getLength();\n+\n+            for (int i = 1; i < chunks.length; i++) {\n+                val source = chunks[i];\n+                Preconditions.checkArgument(!chunks[0].getName().equals(source.getName()), \"target and source can not be same.\");\n+                Path sourcePath = Paths.get(config.getRoot(), source.getName());\n+                long length = chunks[i].getLength();\n+                Preconditions.checkState(offset <= getFileSize(targetPath));\n+                Preconditions.checkState(length <= getFileSize(sourcePath));\n+                try (FileChannel targetChannel = getFileChannel(targetPath, StandardOpenOption.WRITE);", "originalCommit": "30225a38a4b6dde90ff3154ef14651097df8dd63", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIwNTQwOQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456205409", "bodyText": "Good catch that I was opening same file multiple times. Fixing it.\nthe target chunk could be multi GB file.\nAs for why partial writes are still okay because,\nunless the whole operation succeeds..\n\nchunk metadata is not updated So even if resultant chunk has more data we never consider it valid part of chunk.\nThe source chunks are not deleted.", "author": "sachin-j-joshi", "createdAt": "2020-07-17T04:00:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczODcxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMDAxNQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456610015", "bodyText": "Fixed for loop", "author": "sachin-j-joshi", "createdAt": "2020-07-17T18:35:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczODcxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczOTA4NQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455739085", "bodyText": "Maybe this throw new UnsupportedOperationException()  should be the default implementation in the base class", "author": "eolivelli", "createdAt": "2020-07-16T12:11:35Z", "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        return FileChannel.open(path, openOption);\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        return Files.size(path);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throw  convertExeption(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            FileAttribute<Set<PosixFilePermission>> fileAttributes = PosixFilePermissions.asFileAttribute(FileSystemUtils.READ_WRITE_PERMISSION);\n+\n+            Path path = Paths.get(config.getRoot(), chunkName);\n+            Path parent = path.getParent();\n+            assert parent != null;\n+            Files.createDirectories(parent);\n+            Files.createFile(path, fileAttributes);\n+\n+        } catch (IOException e) {\n+            throw convertExeption(chunkName, \"doCreate\", e);\n+        }\n+\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    private ChunkStorageException convertExeption(String chunkName, String message, Exception e) {\n+        if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {\n+            return new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        if (e instanceof FileAlreadyExistsException) {\n+            return  new ChunkAlreadyExistsException(chunkName, message, e);\n+        }\n+        return new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws IllegalArgumentException {\n+        return Files.exists(Paths.get(config.getRoot(), chunkName));\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            Files.delete(Paths.get(config.getRoot(), handle.getChunkName()));\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenRead\");\n+        }\n+\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenWrite\");\n+        } else if (Files.isWritable(path)) {\n+            return ChunkHandle.writeHandle(chunkName);\n+        } else {\n+            return ChunkHandle.readHandle(chunkName);\n+        }\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+        try {\n+            long fileSize = getFileSize(path);\n+            if (fileSize < fromOffset) {\n+                throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the \" +\n+                        \"current size of chunk (%d).\", fromOffset, fileSize));\n+            }\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.READ)) {\n+            int totalBytesRead = 0;\n+            long readOffset = fromOffset;\n+            do {\n+                ByteBuffer readBuffer = ByteBuffer.wrap(buffer, bufferOffset, length);\n+                int bytesRead = channel.read(readBuffer, readOffset);\n+                bufferOffset += bytesRead;\n+                totalBytesRead += bytesRead;\n+                length -= bytesRead;\n+                readOffset += bytesRead;\n+            } while (length > 0);\n+            return totalBytesRead;\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException {\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+\n+        long totalBytesWritten = 0;\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.WRITE)) {\n+            long fileSize = channel.size();\n+            if (fileSize != offset) {\n+                throw new IndexOutOfBoundsException(String.format(\"fileSize (%d) did not match offset (%d) for chunk %s\", fileSize, offset, handle.getChunkName()));\n+            }\n+\n+            // Wrap the input data into a ReadableByteChannel, but do not close it. Doing so will result in closing\n+            // the underlying InputStream, which is not desirable if it is to be reused.\n+            ReadableByteChannel sourceChannel = Channels.newChannel(data);\n+            while (length > 0) {\n+                long bytesWritten = channel.transferFrom(sourceChannel, offset, length);\n+                assert bytesWritten > 0 : \"Unable to make any progress transferring data.\";\n+                offset += bytesWritten;\n+                totalBytesWritten += bytesWritten;\n+                length -= bytesWritten;\n+            }\n+            channel.force(true);\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return (int) totalBytesWritten;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        try {\n+            int totalBytesConcated = 0;\n+            Path targetPath = Paths.get(config.getRoot(), chunks[0].getName());\n+            long offset = chunks[0].getLength();\n+\n+            for (int i = 1; i < chunks.length; i++) {\n+                val source = chunks[i];\n+                Preconditions.checkArgument(!chunks[0].getName().equals(source.getName()), \"target and source can not be same.\");\n+                Path sourcePath = Paths.get(config.getRoot(), source.getName());\n+                long length = chunks[i].getLength();\n+                Preconditions.checkState(offset <= getFileSize(targetPath));\n+                Preconditions.checkState(length <= getFileSize(sourcePath));\n+                try (FileChannel targetChannel = getFileChannel(targetPath, StandardOpenOption.WRITE);\n+                     RandomAccessFile sourceFile = new RandomAccessFile(String.valueOf(sourcePath), \"r\")) {\n+                    while (length > 0) {\n+                        long bytesTransferred = targetChannel.transferFrom(sourceFile.getChannel(), offset, length);\n+                        offset += bytesTransferred;\n+                        length -= bytesTransferred;\n+                    }\n+                    targetChannel.force(true);\n+                    totalBytesConcated += length;\n+                    offset += length;\n+                }\n+\n+            }\n+            return totalBytesConcated;\n+        } catch (IOException e) {\n+            throw convertExeption(chunks[0].getName(), \"doConcat\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException();", "originalCommit": "30225a38a4b6dde90ff3154ef14651097df8dd63", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE3OTczOA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456179738", "bodyText": "Ok. Makes sense.", "author": "sachin-j-joshi", "createdAt": "2020-07-17T02:14:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczOTA4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMDExOQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456610119", "bodyText": "done", "author": "sachin-j-joshi", "createdAt": "2020-07-17T18:35:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczOTA4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczOTc2Nw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455739767", "bodyText": "what happens in case of failure during the execution of this method ?\nwe are losing the contents of the file IMHO", "author": "eolivelli", "createdAt": "2020-07-16T12:12:56Z", "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,350 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+import org.apache.hadoop.ipc.RemoteException;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            return ChunkInfo.builder().name(chunkName).length(status.getLen()).build();\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doCreate\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            // Try accessing file.\n+            fileSystem.getFileStatus(getFilePath(chunkName));\n+            return true;\n+        } catch (FileNotFoundException e) {\n+            return false;\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"checkExists\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            val path = getFilePath(handle.getChunkName());\n+            if (!this.fileSystem.delete(path, true)) {\n+                // File was not deleted. Check if exists.\n+                checkFileExists(handle.getChunkName(), \"doDelete\");\n+            }\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        checkFileExists(chunkName, \"doOpenRead\");\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            val status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            if (status.getPermission().getUserAction() == FsAction.READ) {\n+                return ChunkHandle.readHandle(chunkName);\n+            } else {\n+                return ChunkHandle.writeHandle(chunkName);\n+            }\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doOpenWrite\", e);\n+        }\n+\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataInputStream stream = this.fileSystem.open(getFilePath(handle.getChunkName()))) {\n+            stream.readFully(fromOffset, buffer, bufferOffset, length);\n+        } catch (EOFException e) {\n+            throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the current size of chunk.\", fromOffset));\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataOutputStream stream = this.fileSystem.append(getFilePath(handle.getChunkName()))) {\n+            if (stream.getPos() != offset) {\n+                // Looks like the filesystem changed from underneath us. This could be our bug, but it could be something else.\n+                throw new IndexOutOfBoundsException();\n+            }\n+\n+            if (length == 0) {\n+                // Note: IOUtils.copyBytes with length == 0 will enter an infinite loop, hence the need for this check.\n+                return 0;\n+            }\n+\n+            // We need to be very careful with IOUtils.copyBytes. There are many overloads with very similar signatures.\n+            // There is a difference between (InputStream, OutputStream, int, boolean) and (InputStream, OutputStream, long, boolean),\n+            // in that the one with \"int\" uses the third arg as a buffer size, and the one with \"long\" uses it as the number\n+            // of bytes to copy.\n+            IOUtils.copyBytes(data, stream, (long) length, false);\n+\n+            stream.flush();\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        ensureInitializedAndNotClosed();\n+        int length = 0;\n+        try {\n+            val sources = new Path[chunks.length - 1];\n+            this.fileSystem.truncate(getFilePath(chunks[0].getName()), chunks[0].getLength());", "originalCommit": "30225a38a4b6dde90ff3154ef14651097df8dd63", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE3OTI5MA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456179290", "bodyText": "There is a very specific segment store fail over situation when actual chunk has more data than its length recorded in metadata. This can happen when zombie instance keeps writing to chunk even after it is no longer the owner of the segment. The new segment store instance will effectively \"seal\" that chunk by recording the most recent length and adding a fresh new chunk. In short data after recorded length is garbage that needs to be ignored anyway.\nSee https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2#segment-store-failover\nHDFS concat does not have a concept of concatenating a partial  file. So we need to remove the end part.\nEven if we fail after truncate it still leaves the valid data untouched.\nSegment concat is called in only two cases\n\nfor cases where transaction segment is merged into the main segment.\nde-fragmenting an existing segment by merging smaller chunks into bigger chunks.\n\nIn both the cases above there is no possibility of having garbage data/invalid data at the front of the chunks.", "author": "sachin-j-joshi", "createdAt": "2020-07-17T02:12:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczOTc2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTc0MDEzMw==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455740133", "bodyText": "what about org.apache.hadoop.hdfs.DistributedFileSystem.class.getName() ?\nthis way we will have a compile time guarantee that the name is good", "author": "eolivelli", "createdAt": "2020-07-16T12:13:34Z", "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,350 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+import org.apache.hadoop.ipc.RemoteException;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            return ChunkInfo.builder().name(chunkName).length(status.getLen()).build();\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doCreate\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            // Try accessing file.\n+            fileSystem.getFileStatus(getFilePath(chunkName));\n+            return true;\n+        } catch (FileNotFoundException e) {\n+            return false;\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"checkExists\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            val path = getFilePath(handle.getChunkName());\n+            if (!this.fileSystem.delete(path, true)) {\n+                // File was not deleted. Check if exists.\n+                checkFileExists(handle.getChunkName(), \"doDelete\");\n+            }\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        checkFileExists(chunkName, \"doOpenRead\");\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            val status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            if (status.getPermission().getUserAction() == FsAction.READ) {\n+                return ChunkHandle.readHandle(chunkName);\n+            } else {\n+                return ChunkHandle.writeHandle(chunkName);\n+            }\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doOpenWrite\", e);\n+        }\n+\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataInputStream stream = this.fileSystem.open(getFilePath(handle.getChunkName()))) {\n+            stream.readFully(fromOffset, buffer, bufferOffset, length);\n+        } catch (EOFException e) {\n+            throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the current size of chunk.\", fromOffset));\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataOutputStream stream = this.fileSystem.append(getFilePath(handle.getChunkName()))) {\n+            if (stream.getPos() != offset) {\n+                // Looks like the filesystem changed from underneath us. This could be our bug, but it could be something else.\n+                throw new IndexOutOfBoundsException();\n+            }\n+\n+            if (length == 0) {\n+                // Note: IOUtils.copyBytes with length == 0 will enter an infinite loop, hence the need for this check.\n+                return 0;\n+            }\n+\n+            // We need to be very careful with IOUtils.copyBytes. There are many overloads with very similar signatures.\n+            // There is a difference between (InputStream, OutputStream, int, boolean) and (InputStream, OutputStream, long, boolean),\n+            // in that the one with \"int\" uses the third arg as a buffer size, and the one with \"long\" uses it as the number\n+            // of bytes to copy.\n+            IOUtils.copyBytes(data, stream, (long) length, false);\n+\n+            stream.flush();\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        ensureInitializedAndNotClosed();\n+        int length = 0;\n+        try {\n+            val sources = new Path[chunks.length - 1];\n+            this.fileSystem.truncate(getFilePath(chunks[0].getName()), chunks[0].getLength());\n+            for (int i = 1; i < chunks.length; i++) {\n+                val chunkLength = chunks[i].getLength();\n+                this.fileSystem.truncate(getFilePath(chunks[i].getName()), chunkLength);\n+                sources[i - 1] = getFilePath(chunks[i].getName());\n+                length += chunkLength;\n+            }\n+            // Concat source file into target.\n+            this.fileSystem.concat(getFilePath(chunks[0].getName()), sources);\n+        } catch (IOException e) {\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException(getClass().getName() + \" does not support chunk truncation.\");\n+    }\n+\n+    @Override\n+    protected void doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException {\n+        try {\n+            this.fileSystem.setPermission(getFilePath(handle.getChunkName()), isReadOnly ? READONLY_PERMISSION : READWRITE_PERMISSION);\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+    }\n+\n+    //endregion\n+\n+    //region Storage Implementation\n+\n+    @SneakyThrows(IOException.class)\n+    public void initialize() {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        Preconditions.checkState(this.fileSystem == null, \"HDFSStorage has already been initialized.\");\n+        Configuration conf = new Configuration();\n+        conf.set(\"fs.default.name\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.default.fs\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.hdfs.impl\", \"org.apache.hadoop.hdfs.DistributedFileSystem\");", "originalCommit": "30225a38a4b6dde90ff3154ef14651097df8dd63", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjUwODM5MA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456508390", "bodyText": "This sounds sensible.\n@sachin-j-joshi since this this not new code (it's borrowed from the other HDFSStorage.java), may I suggest collecting all these HDFS improvement ideas (there's at least one other below) in a new GitHub issue and we can address them later?\nThis particular change is unrelated to PDP34 so I don't want to tackle it here.", "author": "andreipaduroiu", "createdAt": "2020-07-17T15:17:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTc0MDEzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMDkzNA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456610934", "bodyText": "Fixed\nconf.set(\"fs.hdfs.impl\", org.apache.hadoop.hdfs.DistributedFileSystem.class.getName());", "author": "sachin-j-joshi", "createdAt": "2020-07-17T18:36:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTc0MDEzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTc0MTA1MA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455741050", "bodyText": "do we have a way to pass additional configuration parameters to this client ?\nI mean, probably users would like to add additional configurations, in segment store configuration file\nhdfs.customconfig.xxxx=yyyyy", "author": "eolivelli", "createdAt": "2020-07-16T12:15:16Z", "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,350 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+import org.apache.hadoop.ipc.RemoteException;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            return ChunkInfo.builder().name(chunkName).length(status.getLen()).build();\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doCreate\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            // Try accessing file.\n+            fileSystem.getFileStatus(getFilePath(chunkName));\n+            return true;\n+        } catch (FileNotFoundException e) {\n+            return false;\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"checkExists\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            val path = getFilePath(handle.getChunkName());\n+            if (!this.fileSystem.delete(path, true)) {\n+                // File was not deleted. Check if exists.\n+                checkFileExists(handle.getChunkName(), \"doDelete\");\n+            }\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        checkFileExists(chunkName, \"doOpenRead\");\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            val status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            if (status.getPermission().getUserAction() == FsAction.READ) {\n+                return ChunkHandle.readHandle(chunkName);\n+            } else {\n+                return ChunkHandle.writeHandle(chunkName);\n+            }\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doOpenWrite\", e);\n+        }\n+\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataInputStream stream = this.fileSystem.open(getFilePath(handle.getChunkName()))) {\n+            stream.readFully(fromOffset, buffer, bufferOffset, length);\n+        } catch (EOFException e) {\n+            throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the current size of chunk.\", fromOffset));\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataOutputStream stream = this.fileSystem.append(getFilePath(handle.getChunkName()))) {\n+            if (stream.getPos() != offset) {\n+                // Looks like the filesystem changed from underneath us. This could be our bug, but it could be something else.\n+                throw new IndexOutOfBoundsException();\n+            }\n+\n+            if (length == 0) {\n+                // Note: IOUtils.copyBytes with length == 0 will enter an infinite loop, hence the need for this check.\n+                return 0;\n+            }\n+\n+            // We need to be very careful with IOUtils.copyBytes. There are many overloads with very similar signatures.\n+            // There is a difference between (InputStream, OutputStream, int, boolean) and (InputStream, OutputStream, long, boolean),\n+            // in that the one with \"int\" uses the third arg as a buffer size, and the one with \"long\" uses it as the number\n+            // of bytes to copy.\n+            IOUtils.copyBytes(data, stream, (long) length, false);\n+\n+            stream.flush();\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        ensureInitializedAndNotClosed();\n+        int length = 0;\n+        try {\n+            val sources = new Path[chunks.length - 1];\n+            this.fileSystem.truncate(getFilePath(chunks[0].getName()), chunks[0].getLength());\n+            for (int i = 1; i < chunks.length; i++) {\n+                val chunkLength = chunks[i].getLength();\n+                this.fileSystem.truncate(getFilePath(chunks[i].getName()), chunkLength);\n+                sources[i - 1] = getFilePath(chunks[i].getName());\n+                length += chunkLength;\n+            }\n+            // Concat source file into target.\n+            this.fileSystem.concat(getFilePath(chunks[0].getName()), sources);\n+        } catch (IOException e) {\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException(getClass().getName() + \" does not support chunk truncation.\");\n+    }\n+\n+    @Override\n+    protected void doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException {\n+        try {\n+            this.fileSystem.setPermission(getFilePath(handle.getChunkName()), isReadOnly ? READONLY_PERMISSION : READWRITE_PERMISSION);\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+    }\n+\n+    //endregion\n+\n+    //region Storage Implementation\n+\n+    @SneakyThrows(IOException.class)\n+    public void initialize() {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        Preconditions.checkState(this.fileSystem == null, \"HDFSStorage has already been initialized.\");\n+        Configuration conf = new Configuration();\n+        conf.set(\"fs.default.name\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.default.fs\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.hdfs.impl\", \"org.apache.hadoop.hdfs.DistributedFileSystem\");\n+\n+        // We do not want FileSystem to cache clients/instances based on target URI.\n+        // This allows us to close instances without affecting other clients/instances. This should not affect performance.\n+        conf.set(\"fs.hdfs.impl.disable.cache\", \"true\");\n+        if (!this.config.isReplaceDataNodesOnFailure()) {\n+            // Default is DEFAULT, so we only set this if we want it disabled.\n+            conf.set(\"dfs.client.block.write.replace-datanode-on-failure.policy\", \"NEVER\");\n+        }\n+\n+        this.fileSystem = openFileSystem(conf);", "originalCommit": "30225a38a4b6dde90ff3154ef14651097df8dd63", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjUwNzI1NA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456507254", "bodyText": "The purpose of this PR was not to tweak the HDFS client. We can take that challenge in a separate issue. We are setting all the configs that we believe we need, and define the \"pass-through\" values in our own config. Otherwise we'd have an unmanageable zoo of configurations, most of which will be beyond our control.", "author": "andreipaduroiu", "createdAt": "2020-07-17T15:15:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTc0MTA1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU0NDUzNQ==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456544535", "bodyText": "I'll create a tracking issue for this.", "author": "sachin-j-joshi", "createdAt": "2020-07-17T16:23:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTc0MTA1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU0NjY0MA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456546640", "bodyText": "@andreipaduroiu agreed.\n@sachin-j-joshi probably there is not need to create a ticket if no user asks for it.", "author": "eolivelli", "createdAt": "2020-07-17T16:27:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTc0MTA1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTc0MTM5Mg==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455741392", "bodyText": "what happens in case of failure ? where is recovery handled ?", "author": "eolivelli", "createdAt": "2020-07-16T12:15:53Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -847,6 +857,11 @@ private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, St\n                     concatUsingAppend(concatArgs);\n                 }\n \n+                // Delete chunks.\n+                for (int i = 1; i < chunksToConcat.size(); i++) {\n+                    chunksToDelete.add(chunksToConcat.get(i).getName());", "originalCommit": "30225a38a4b6dde90ff3154ef14651097df8dd63", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE3NTcxOA==", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456175718", "bodyText": "The list of chunks to delete is returns to the caller in this case ChunkedSegmentStorage::concat\nChunkedSegmentStorage::concat calls ChunkedSegmentStorage::collectGarbage which then immediately deletes the garbage chunks. This is called only after the whole concat operation and the metadata commit succeeds.\nWhat happens when delete fails ? We add it a list of \"to delete list\" of chunks which will be deleted by a background task.\nThere is a pending issue to create background garbage collection task #4903", "author": "sachin-j-joshi", "createdAt": "2020-07-17T01:58:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTc0MTM5Mg=="}], "type": "inlineReview"}, {"oid": "76d4c5537de5353dcdeaf5dd97afdd5aab313344", "url": "https://github.com/pravega/pravega/commit/76d4c5537de5353dcdeaf5dd97afdd5aab313344", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - More code coverage.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-17T18:16:49Z", "type": "commit"}]}