{"pr_number": 5371, "pr_title": "Data recovery commands in CLI Tool", "pr_createdAt": "2020-11-24T19:04:09Z", "pr_url": "https://github.com/pravega/pravega/pull/5371", "timeline": [{"oid": "728375dd090634baaf2020b891e04cf646b44c2b", "url": "https://github.com/pravega/pravega/commit/728375dd090634baaf2020b891e04cf646b44c2b", "message": "dr cli commands.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-11-23T03:33:41Z", "type": "commit"}, {"oid": "6ab24e9f35488a05b0ed39f1a8e842b64f1af847", "url": "https://github.com/pravega/pravega/commit/6ab24e9f35488a05b0ed39f1a8e842b64f1af847", "message": "Updating storage.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-11-24T04:59:53Z", "type": "commit"}, {"oid": "4c2adc8f811e74866a71bef07dc4a1b94c036492", "url": "https://github.com/pravega/pravega/commit/4c2adc8f811e74866a71bef07dc4a1b94c036492", "message": "Updating properties file.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-11-24T05:02:11Z", "type": "commit"}, {"oid": "e47695e3a970facb5c1ae8875b79e3bf376c1023", "url": "https://github.com/pravega/pravega/commit/e47695e3a970facb5c1ae8875b79e3bf376c1023", "message": "Removing update attributes.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-11-24T05:04:27Z", "type": "commit"}, {"oid": "0246589b1c8e1fe9ed210a010cf8ffe2281a2f08", "url": "https://github.com/pravega/pravega/commit/0246589b1c8e1fe9ed210a010cf8ffe2281a2f08", "message": "Removing update attributes.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-11-24T20:51:00Z", "type": "forcePushed"}, {"oid": "e47695e3a970facb5c1ae8875b79e3bf376c1023", "url": "https://github.com/pravega/pravega/commit/e47695e3a970facb5c1ae8875b79e3bf376c1023", "message": "Removing update attributes.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-11-24T05:04:27Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0MTU1Mw==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r532741553", "bodyText": "Why can't we do this in the constructor? You get all your args there anyway and that way you can make your logger final.", "author": "andreipaduroiu", "createdAt": "2020-11-30T16:47:02Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/DataRecoveryCommand.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.AdminCommand;\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.segmentstore.server.host.StorageLoader;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+\n+import java.io.File;\n+import java.text.DateFormat;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.FileHandler;\n+import java.util.logging.Formatter;\n+import java.util.logging.Handler;\n+import java.util.logging.Level;\n+import java.util.logging.LogRecord;\n+import java.util.logging.Logger;\n+\n+import static java.util.logging.Level.FINE;\n+import static java.util.logging.Level.FINER;\n+import static java.util.logging.Level.INFO;\n+import static java.util.logging.Level.SEVERE;\n+\n+/**\n+ * Base for any data recovery related commands.\n+ */\n+public abstract class DataRecoveryCommand extends AdminCommand {\n+    protected final static String COMPONENT = \"storage\";\n+    protected Logger logger;\n+\n+    /**\n+     * Creates a new instance of the DataRecoveryCommand class.\n+     *\n+     * @param args The arguments for the command.\n+     */\n+    DataRecoveryCommand(CommandArgs args) {\n+        super(args);\n+    }\n+\n+    /**\n+     * Creates the {@link StorageFactory} instance by reading the config values.\n+     *\n+     * @param executorService   A thread pool for execution.\n+     * @return                  A newly created {@link StorageFactory} instance.\n+     */\n+    StorageFactory createStorageFactory(ScheduledExecutorService executorService) {\n+        ServiceBuilder.ConfigSetupHelper configSetupHelper = new ServiceBuilder.ConfigSetupHelper(getCommandArgs().getState().getConfigBuilder().build());\n+        StorageLoader loader = new StorageLoader();\n+        return loader.load(configSetupHelper, getServiceConfig().getStorageImplementation().toString(),\n+                getServiceConfig().getStorageLayout(), executorService);\n+    }\n+\n+    /**\n+     * Creates logging directory and file for the command to be run. The path to the directory can be supplied on the\n+     * command run. By default, the path is set as current user path.\n+     *\n+     * @param commandName   The name of the command to be run.\n+     * @return              The path to the directory created.\n+     * @throws Exception    In case of a failure in creating the directory or the file.\n+     */\n+    String setLogging(String commandName) throws Exception {", "originalCommit": "e47695e3a970facb5c1ae8875b79e3bf376c1023", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg4MzkxMQ==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r533883911", "bodyText": "Removed it. Logging through an xml file now config/admin-cli-logback.xml.", "author": "ManishKumarKeshri", "createdAt": "2020-12-02T04:09:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0MTU1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0MjM2OA==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r532742368", "bodyText": "/ is also some system-level constant. I believe you can find it in Paths or Path.\nFix it below too.", "author": "andreipaduroiu", "createdAt": "2020-11-30T16:48:10Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/DataRecoveryCommand.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.AdminCommand;\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.segmentstore.server.host.StorageLoader;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+\n+import java.io.File;\n+import java.text.DateFormat;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.FileHandler;\n+import java.util.logging.Formatter;\n+import java.util.logging.Handler;\n+import java.util.logging.Level;\n+import java.util.logging.LogRecord;\n+import java.util.logging.Logger;\n+\n+import static java.util.logging.Level.FINE;\n+import static java.util.logging.Level.FINER;\n+import static java.util.logging.Level.INFO;\n+import static java.util.logging.Level.SEVERE;\n+\n+/**\n+ * Base for any data recovery related commands.\n+ */\n+public abstract class DataRecoveryCommand extends AdminCommand {\n+    protected final static String COMPONENT = \"storage\";\n+    protected Logger logger;\n+\n+    /**\n+     * Creates a new instance of the DataRecoveryCommand class.\n+     *\n+     * @param args The arguments for the command.\n+     */\n+    DataRecoveryCommand(CommandArgs args) {\n+        super(args);\n+    }\n+\n+    /**\n+     * Creates the {@link StorageFactory} instance by reading the config values.\n+     *\n+     * @param executorService   A thread pool for execution.\n+     * @return                  A newly created {@link StorageFactory} instance.\n+     */\n+    StorageFactory createStorageFactory(ScheduledExecutorService executorService) {\n+        ServiceBuilder.ConfigSetupHelper configSetupHelper = new ServiceBuilder.ConfigSetupHelper(getCommandArgs().getState().getConfigBuilder().build());\n+        StorageLoader loader = new StorageLoader();\n+        return loader.load(configSetupHelper, getServiceConfig().getStorageImplementation().toString(),\n+                getServiceConfig().getStorageLayout(), executorService);\n+    }\n+\n+    /**\n+     * Creates logging directory and file for the command to be run. The path to the directory can be supplied on the\n+     * command run. By default, the path is set as current user path.\n+     *\n+     * @param commandName   The name of the command to be run.\n+     * @return              The path to the directory created.\n+     * @throws Exception    In case of a failure in creating the directory or the file.\n+     */\n+    String setLogging(String commandName) throws Exception {\n+        logger = Logger.getLogger(commandName);\n+        logger.setUseParentHandlers(false);\n+\n+        String fileSuffix = new SimpleDateFormat(\"yyyyMMddHHmmss\").format(new Date());\n+        String fileName = commandName + fileSuffix + \".log\";\n+\n+        String filePath = System.getProperty(\"user.dir\") + \"/\" + commandName + \"_\" + fileSuffix;", "originalCommit": "e47695e3a970facb5c1ae8875b79e3bf376c1023", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg4NDY4NQ==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r533884685", "bodyText": "Fixed it.", "author": "ManishKumarKeshri", "createdAt": "2020-12-02T04:12:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0MjM2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0MjYyNw==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r532742627", "bodyText": "Checkstyle.\nAre you trying to get the file name? If so, there may be a method for that in Paths.", "author": "andreipaduroiu", "createdAt": "2020-11-30T16:48:32Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/DataRecoveryCommand.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.AdminCommand;\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.segmentstore.server.host.StorageLoader;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+\n+import java.io.File;\n+import java.text.DateFormat;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.FileHandler;\n+import java.util.logging.Formatter;\n+import java.util.logging.Handler;\n+import java.util.logging.Level;\n+import java.util.logging.LogRecord;\n+import java.util.logging.Logger;\n+\n+import static java.util.logging.Level.FINE;\n+import static java.util.logging.Level.FINER;\n+import static java.util.logging.Level.INFO;\n+import static java.util.logging.Level.SEVERE;\n+\n+/**\n+ * Base for any data recovery related commands.\n+ */\n+public abstract class DataRecoveryCommand extends AdminCommand {\n+    protected final static String COMPONENT = \"storage\";\n+    protected Logger logger;\n+\n+    /**\n+     * Creates a new instance of the DataRecoveryCommand class.\n+     *\n+     * @param args The arguments for the command.\n+     */\n+    DataRecoveryCommand(CommandArgs args) {\n+        super(args);\n+    }\n+\n+    /**\n+     * Creates the {@link StorageFactory} instance by reading the config values.\n+     *\n+     * @param executorService   A thread pool for execution.\n+     * @return                  A newly created {@link StorageFactory} instance.\n+     */\n+    StorageFactory createStorageFactory(ScheduledExecutorService executorService) {\n+        ServiceBuilder.ConfigSetupHelper configSetupHelper = new ServiceBuilder.ConfigSetupHelper(getCommandArgs().getState().getConfigBuilder().build());\n+        StorageLoader loader = new StorageLoader();\n+        return loader.load(configSetupHelper, getServiceConfig().getStorageImplementation().toString(),\n+                getServiceConfig().getStorageLayout(), executorService);\n+    }\n+\n+    /**\n+     * Creates logging directory and file for the command to be run. The path to the directory can be supplied on the\n+     * command run. By default, the path is set as current user path.\n+     *\n+     * @param commandName   The name of the command to be run.\n+     * @return              The path to the directory created.\n+     * @throws Exception    In case of a failure in creating the directory or the file.\n+     */\n+    String setLogging(String commandName) throws Exception {\n+        logger = Logger.getLogger(commandName);\n+        logger.setUseParentHandlers(false);\n+\n+        String fileSuffix = new SimpleDateFormat(\"yyyyMMddHHmmss\").format(new Date());\n+        String fileName = commandName + fileSuffix + \".log\";\n+\n+        String filePath = System.getProperty(\"user.dir\") + \"/\" + commandName + \"_\" + fileSuffix;\n+        if (getArgCount() >= 1) {\n+            filePath = getCommandArgs().getArgs().get(0);\n+            if (filePath.endsWith(\"/\")) {\n+                filePath = filePath.substring(0, filePath.length()-1);", "originalCommit": "e47695e3a970facb5c1ae8875b79e3bf376c1023", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg4NTE3Nw==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r533885177", "bodyText": "I am trying to get the file path provided as command args during the command execution to store the csv files.", "author": "ManishKumarKeshri", "createdAt": "2020-12-02T04:14:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0MjYyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0Mjc3Mw==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r532742773", "bodyText": "same here", "author": "andreipaduroiu", "createdAt": "2020-11-30T16:48:45Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/DataRecoveryCommand.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.AdminCommand;\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.segmentstore.server.host.StorageLoader;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+\n+import java.io.File;\n+import java.text.DateFormat;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.FileHandler;\n+import java.util.logging.Formatter;\n+import java.util.logging.Handler;\n+import java.util.logging.Level;\n+import java.util.logging.LogRecord;\n+import java.util.logging.Logger;\n+\n+import static java.util.logging.Level.FINE;\n+import static java.util.logging.Level.FINER;\n+import static java.util.logging.Level.INFO;\n+import static java.util.logging.Level.SEVERE;\n+\n+/**\n+ * Base for any data recovery related commands.\n+ */\n+public abstract class DataRecoveryCommand extends AdminCommand {\n+    protected final static String COMPONENT = \"storage\";\n+    protected Logger logger;\n+\n+    /**\n+     * Creates a new instance of the DataRecoveryCommand class.\n+     *\n+     * @param args The arguments for the command.\n+     */\n+    DataRecoveryCommand(CommandArgs args) {\n+        super(args);\n+    }\n+\n+    /**\n+     * Creates the {@link StorageFactory} instance by reading the config values.\n+     *\n+     * @param executorService   A thread pool for execution.\n+     * @return                  A newly created {@link StorageFactory} instance.\n+     */\n+    StorageFactory createStorageFactory(ScheduledExecutorService executorService) {\n+        ServiceBuilder.ConfigSetupHelper configSetupHelper = new ServiceBuilder.ConfigSetupHelper(getCommandArgs().getState().getConfigBuilder().build());\n+        StorageLoader loader = new StorageLoader();\n+        return loader.load(configSetupHelper, getServiceConfig().getStorageImplementation().toString(),\n+                getServiceConfig().getStorageLayout(), executorService);\n+    }\n+\n+    /**\n+     * Creates logging directory and file for the command to be run. The path to the directory can be supplied on the\n+     * command run. By default, the path is set as current user path.\n+     *\n+     * @param commandName   The name of the command to be run.\n+     * @return              The path to the directory created.\n+     * @throws Exception    In case of a failure in creating the directory or the file.\n+     */\n+    String setLogging(String commandName) throws Exception {\n+        logger = Logger.getLogger(commandName);\n+        logger.setUseParentHandlers(false);\n+\n+        String fileSuffix = new SimpleDateFormat(\"yyyyMMddHHmmss\").format(new Date());\n+        String fileName = commandName + fileSuffix + \".log\";\n+\n+        String filePath = System.getProperty(\"user.dir\") + \"/\" + commandName + \"_\" + fileSuffix;\n+        if (getArgCount() >= 1) {\n+            filePath = getCommandArgs().getArgs().get(0);\n+            if (filePath.endsWith(\"/\")) {\n+                filePath = filePath.substring(0, filePath.length()-1);\n+            }\n+        }\n+\n+        // Create a directory for storing files.\n+        File dir = new File(filePath);\n+        if (!dir.exists()) {\n+            dir.mkdir();\n+        }\n+\n+        FileHandler fh = new FileHandler(filePath + \"/\" + commandName + fileSuffix + \".log\");", "originalCommit": "e47695e3a970facb5c1ae8875b79e3bf376c1023", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg4NTM2Nw==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r533885367", "bodyText": "Removed it.", "author": "ManishKumarKeshri", "createdAt": "2020-12-02T04:15:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0Mjc3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0MzQyNg==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r532743426", "bodyText": "You don't have to do this. When you set you logger, you can configure it to have a FileAppender and a ConsoleAppender. That will do all the work for you", "author": "andreipaduroiu", "createdAt": "2020-11-30T16:49:35Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/DataRecoveryCommand.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.AdminCommand;\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.segmentstore.server.host.StorageLoader;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+\n+import java.io.File;\n+import java.text.DateFormat;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.FileHandler;\n+import java.util.logging.Formatter;\n+import java.util.logging.Handler;\n+import java.util.logging.Level;\n+import java.util.logging.LogRecord;\n+import java.util.logging.Logger;\n+\n+import static java.util.logging.Level.FINE;\n+import static java.util.logging.Level.FINER;\n+import static java.util.logging.Level.INFO;\n+import static java.util.logging.Level.SEVERE;\n+\n+/**\n+ * Base for any data recovery related commands.\n+ */\n+public abstract class DataRecoveryCommand extends AdminCommand {\n+    protected final static String COMPONENT = \"storage\";\n+    protected Logger logger;\n+\n+    /**\n+     * Creates a new instance of the DataRecoveryCommand class.\n+     *\n+     * @param args The arguments for the command.\n+     */\n+    DataRecoveryCommand(CommandArgs args) {\n+        super(args);\n+    }\n+\n+    /**\n+     * Creates the {@link StorageFactory} instance by reading the config values.\n+     *\n+     * @param executorService   A thread pool for execution.\n+     * @return                  A newly created {@link StorageFactory} instance.\n+     */\n+    StorageFactory createStorageFactory(ScheduledExecutorService executorService) {\n+        ServiceBuilder.ConfigSetupHelper configSetupHelper = new ServiceBuilder.ConfigSetupHelper(getCommandArgs().getState().getConfigBuilder().build());\n+        StorageLoader loader = new StorageLoader();\n+        return loader.load(configSetupHelper, getServiceConfig().getStorageImplementation().toString(),\n+                getServiceConfig().getStorageLayout(), executorService);\n+    }\n+\n+    /**\n+     * Creates logging directory and file for the command to be run. The path to the directory can be supplied on the\n+     * command run. By default, the path is set as current user path.\n+     *\n+     * @param commandName   The name of the command to be run.\n+     * @return              The path to the directory created.\n+     * @throws Exception    In case of a failure in creating the directory or the file.\n+     */\n+    String setLogging(String commandName) throws Exception {\n+        logger = Logger.getLogger(commandName);\n+        logger.setUseParentHandlers(false);\n+\n+        String fileSuffix = new SimpleDateFormat(\"yyyyMMddHHmmss\").format(new Date());\n+        String fileName = commandName + fileSuffix + \".log\";\n+\n+        String filePath = System.getProperty(\"user.dir\") + \"/\" + commandName + \"_\" + fileSuffix;\n+        if (getArgCount() >= 1) {\n+            filePath = getCommandArgs().getArgs().get(0);\n+            if (filePath.endsWith(\"/\")) {\n+                filePath = filePath.substring(0, filePath.length()-1);\n+            }\n+        }\n+\n+        // Create a directory for storing files.\n+        File dir = new File(filePath);\n+        if (!dir.exists()) {\n+            dir.mkdir();\n+        }\n+\n+        FileHandler fh = new FileHandler(filePath + \"/\" + commandName + fileSuffix + \".log\");\n+        fh.setLevel(FINER);\n+        DataRecoveryLogFormatter drFormatter = new DataRecoveryLogFormatter();\n+        fh.setFormatter(drFormatter);\n+        logger.addHandler(fh);\n+\n+        output(FINER, \"Logs are written to file '%s'\", filePath + \"/\" + fileName);\n+        return filePath;\n+    }\n+\n+    /**\n+     * Outputs the message to the console as well as to the log file.\n+     *\n+     * @param level             The log level of the message.\n+     * @param messageTemplate   The message.\n+     * @param args              The arguments with the message.\n+     */\n+    protected void output(Level level, String messageTemplate, Object... args) {\n+        if (INFO.equals(level)) {", "originalCommit": "e47695e3a970facb5c1ae8875b79e3bf376c1023", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0MzY1NA==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r532743654", "bodyText": "Plus the String.format format is different than the log format, so this won't work.", "author": "andreipaduroiu", "createdAt": "2020-11-30T16:49:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0MzQyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg4NjA3Mg==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r533886072", "bodyText": "Using FileAppender now, consoleAppender doesn't work while running the CLI command.\nString.format worked here.\nRemoved this method, and using a different one.", "author": "ManishKumarKeshri", "createdAt": "2020-12-02T04:17:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0MzQyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0NDEwOQ==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r532744109", "bodyText": "We use Slf4j everywhere. Why use this type of logging here?", "author": "andreipaduroiu", "createdAt": "2020-11-30T16:50:30Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/DataRecoveryCommand.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.AdminCommand;\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.segmentstore.server.host.StorageLoader;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+\n+import java.io.File;\n+import java.text.DateFormat;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.FileHandler;", "originalCommit": "e47695e3a970facb5c1ae8875b79e3bf376c1023", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg4NjI2Mg==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r533886262", "bodyText": "Yes, removed that logging.", "author": "ManishKumarKeshri", "createdAt": "2020-12-02T04:18:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0NDEwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0NDkzOA==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r532744938", "bodyText": "Use ExecutorServiceHelpers to create the executor.\nWhere are you closing this executor???", "author": "andreipaduroiu", "createdAt": "2020-11-30T16:51:40Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/StorageListSegmentsCommand.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.Cleanup;\n+\n+import java.io.File;\n+import java.io.FileWriter;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+\n+/**\n+ * Lists all non-shadow segments from there from the storage. The storage is loaded using the config properties.\n+ */\n+public class StorageListSegmentsCommand extends DataRecoveryCommand {\n+    /**\n+     * Header line for writing segments' details to csv files.\n+     */\n+    private static final List<String> HEADER = Arrays.asList(\"Sealed Status\", \"Length\", \"Segment Name\");\n+    private static final int CONTAINER_EPOCH = 1;\n+    private final int containerCount;\n+    private final ScheduledExecutorService scheduledExecutorService = ExecutorServiceHelpers.newScheduledThreadPool(10,", "originalCommit": "e47695e3a970facb5c1ae8875b79e3bf376c1023", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg4OTEzNA==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r533889134", "bodyText": "closing it now. I think I was already using ExecutorServiceHelpers.", "author": "ManishKumarKeshri", "createdAt": "2020-12-02T04:28:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0NDkzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0NTIwMg==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r532745202", "bodyText": "What is this field? Why can't it be final?", "author": "andreipaduroiu", "createdAt": "2020-11-30T16:52:03Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/StorageListSegmentsCommand.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.Cleanup;\n+\n+import java.io.File;\n+import java.io.FileWriter;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+\n+/**\n+ * Lists all non-shadow segments from there from the storage. The storage is loaded using the config properties.\n+ */\n+public class StorageListSegmentsCommand extends DataRecoveryCommand {\n+    /**\n+     * Header line for writing segments' details to csv files.\n+     */\n+    private static final List<String> HEADER = Arrays.asList(\"Sealed Status\", \"Length\", \"Segment Name\");\n+    private static final int CONTAINER_EPOCH = 1;\n+    private final int containerCount;\n+    private final ScheduledExecutorService scheduledExecutorService = ExecutorServiceHelpers.newScheduledThreadPool(10,\n+            \"listSegmentsProcessor\");\n+    private final SegmentToContainerMapper segToConMapper;\n+    private final StorageFactory storageFactory;\n+    private final FileWriter[] csvWriters;\n+    private String filePath;", "originalCommit": "e47695e3a970facb5c1ae8875b79e3bf376c1023", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0NjQ0OQ==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r532746449", "bodyText": "If you see my comment in the above class, you can set this field in the constructor.", "author": "andreipaduroiu", "createdAt": "2020-11-30T16:53:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0NTIwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg4OTkwOQ==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r533889909", "bodyText": "I assign this field to user's directory in case no filePath to store CSV files has been provided during command execution. I can't keep this as final, as it needs reassignment,.", "author": "ManishKumarKeshri", "createdAt": "2020-12-02T04:31:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0NTIwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0NzM2NA==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r532747364", "bodyText": "output supports formatted text. Use that as it will make your file more readable.", "author": "andreipaduroiu", "createdAt": "2020-11-30T16:54:37Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/StorageListSegmentsCommand.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.Cleanup;\n+\n+import java.io.File;\n+import java.io.FileWriter;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+\n+/**\n+ * Lists all non-shadow segments from there from the storage. The storage is loaded using the config properties.\n+ */\n+public class StorageListSegmentsCommand extends DataRecoveryCommand {\n+    /**\n+     * Header line for writing segments' details to csv files.\n+     */\n+    private static final List<String> HEADER = Arrays.asList(\"Sealed Status\", \"Length\", \"Segment Name\");\n+    private static final int CONTAINER_EPOCH = 1;\n+    private final int containerCount;\n+    private final ScheduledExecutorService scheduledExecutorService = ExecutorServiceHelpers.newScheduledThreadPool(10,\n+            \"listSegmentsProcessor\");\n+    private final SegmentToContainerMapper segToConMapper;\n+    private final StorageFactory storageFactory;\n+    private final FileWriter[] csvWriters;\n+    private String filePath;\n+\n+    /**\n+     * Creates an instance of StorageListSegmentsCommand class.\n+     *\n+     * @param args The arguments for the command.\n+     */\n+    public StorageListSegmentsCommand(CommandArgs args) {\n+        super(args);\n+        this.containerCount = getServiceConfig().getContainerCount();\n+        this.segToConMapper = new SegmentToContainerMapper(this.containerCount);\n+        this.storageFactory = createStorageFactory(scheduledExecutorService);\n+        this.csvWriters = new FileWriter[this.containerCount];\n+    }\n+\n+    /**\n+     * Creates a csv file for each container. All segments belonging to a containerId have their details written to the\n+     * csv file for that container.\n+     *\n+     * @throws Exception   When failed to create/delete file(s).\n+     */\n+    private void createCSVFiles() throws Exception {\n+        for (int containerId = 0; containerId < this.containerCount; containerId++) {\n+            File f = new File(this.filePath + \"/\" + \"Container_\" + containerId + \".csv\");\n+            if (f.exists()) {\n+                output(Level.FINE, \"File '%s' already exists.\", f.getAbsolutePath());\n+                if (!f.delete()) {\n+                    output(Level.SEVERE, \"Failed to delete the file '%s'.\", f.getAbsolutePath());\n+                    throw new Exception(\"Failed to delete the file \" + f.getAbsolutePath());\n+                }\n+            }\n+            if (!f.createNewFile()) {\n+                output(Level.SEVERE, \"Failed to create file '%s'.\", f.getAbsolutePath());\n+                throw new Exception(\"Failed to create file \" + f.getAbsolutePath());\n+            }\n+            this.csvWriters[containerId] = new FileWriter(f.getName());\n+            output(Level.INFO, \"Created file '%s'\", f.getAbsolutePath());\n+            this.csvWriters[containerId].append(String.join(\",\", HEADER));\n+            this.csvWriters[containerId].append(\"\\n\");\n+        }\n+    }\n+\n+    @Override\n+    public void execute() throws Exception {\n+        // set up logging\n+        this.filePath = setLogging(descriptor().getName());\n+\n+        output(Level.INFO, \"Container Count = %d\", this.containerCount);\n+        // Get the storage using the config.\n+        @Cleanup\n+        Storage storage = this.storageFactory.createStorageAdapter();\n+        storage.initialize(CONTAINER_EPOCH);\n+        output(Level.INFO, \"Loaded %s Storage.\", getServiceConfig().getStorageImplementation().toString());\n+\n+        // Gets total number of segments listed.\n+        int segmentsCount = 0;\n+\n+        createCSVFiles();\n+\n+        output(Level.INFO, \"Writing segments' details to the csv files...\");\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            segmentsCount++;\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            output(Level.FINE, containerId + \"\\t\" + currentSegment.isSealed() + \"\\t\" + currentSegment.getLength() + \"\\t\" +", "originalCommit": "e47695e3a970facb5c1ae8875b79e3bf376c1023", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0Nzg4MA==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r532747880", "bodyText": "same here", "author": "andreipaduroiu", "createdAt": "2020-11-30T16:55:20Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/Tier1RecoveryCommand.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.ContainerRecoveryUtils;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.shared.NameUtils;\n+import lombok.Cleanup;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.logging.Level;\n+\n+/**\n+ * Loads the storage instance, recovers all segments from there.\n+ */\n+public class Tier1RecoveryCommand extends DataRecoveryCommand implements AutoCloseable {\n+    private static final int CONTAINER_EPOCH = 1;\n+    private static final Duration TIMEOUT = Duration.ofMillis(100 * 1000);\n+\n+    private final ScheduledExecutorService executorService = ExecutorServiceHelpers.newScheduledThreadPool(100, \"recoveryProcessor\");", "originalCommit": "e47695e3a970facb5c1ae8875b79e3bf376c1023", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0ODI3OA==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r532748278", "bodyText": "There is no such thing as Tier1 in the code. Call it DurableLogRecoveryCommand", "author": "andreipaduroiu", "createdAt": "2020-11-30T16:55:47Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/Tier1RecoveryCommand.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.ContainerRecoveryUtils;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.shared.NameUtils;\n+import lombok.Cleanup;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.logging.Level;\n+\n+/**\n+ * Loads the storage instance, recovers all segments from there.\n+ */\n+public class Tier1RecoveryCommand extends DataRecoveryCommand implements AutoCloseable {", "originalCommit": "e47695e3a970facb5c1ae8875b79e3bf376c1023", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg5MTk4MQ==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r533891981", "bodyText": "Done.", "author": "ManishKumarKeshri", "createdAt": "2020-12-02T04:39:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0ODI3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0ODUyNw==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r532748527", "bodyText": "Why 1KB?", "author": "andreipaduroiu", "createdAt": "2020-11-30T16:56:07Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/Tier1RecoveryCommand.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.ContainerRecoveryUtils;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.shared.NameUtils;\n+import lombok.Cleanup;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.logging.Level;\n+\n+/**\n+ * Loads the storage instance, recovers all segments from there.\n+ */\n+public class Tier1RecoveryCommand extends DataRecoveryCommand implements AutoCloseable {\n+    private static final int CONTAINER_EPOCH = 1;\n+    private static final Duration TIMEOUT = Duration.ofMillis(100 * 1000);\n+\n+    private final ScheduledExecutorService executorService = ExecutorServiceHelpers.newScheduledThreadPool(100, \"recoveryProcessor\");\n+    private final int containerCount;\n+    private final StorageFactory storageFactory;\n+    private final DurableDataLogFactory dataLogFactory;\n+    private final StreamSegmentContainerFactory containerFactory;\n+    private final OperationLogFactory operationLogFactory;\n+    private final ReadIndexFactory readIndexFactory;\n+    private final AttributeIndexFactory attributeIndexFactory;\n+    private final WriterFactory writerFactory;\n+    private final CacheStorage cacheStorage;\n+    private final CacheManager cacheManager;\n+    // DL config that can be used to simulate no DurableLog truncations.\n+    private static final DurableLogConfig NO_TRUNCATIONS_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 10000)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 50000)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 1024 * 1024 * 1024L)\n+            .build();\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();", "originalCommit": "e47695e3a970facb5c1ae8875b79e3bf376c1023", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg5NTIzOQ==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r533895239", "bodyText": "I guess default is fine. I am actually not reading anything through debug segment container.", "author": "ManishKumarKeshri", "createdAt": "2020-12-02T04:51:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0ODUyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0ODg4OA==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r532748888", "bodyText": "Why do you make the Attribute BTree page so small? Why made you change this from the default values?", "author": "andreipaduroiu", "createdAt": "2020-11-30T16:56:36Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/Tier1RecoveryCommand.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.ContainerRecoveryUtils;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.shared.NameUtils;\n+import lombok.Cleanup;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.logging.Level;\n+\n+/**\n+ * Loads the storage instance, recovers all segments from there.\n+ */\n+public class Tier1RecoveryCommand extends DataRecoveryCommand implements AutoCloseable {\n+    private static final int CONTAINER_EPOCH = 1;\n+    private static final Duration TIMEOUT = Duration.ofMillis(100 * 1000);\n+\n+    private final ScheduledExecutorService executorService = ExecutorServiceHelpers.newScheduledThreadPool(100, \"recoveryProcessor\");\n+    private final int containerCount;\n+    private final StorageFactory storageFactory;\n+    private final DurableDataLogFactory dataLogFactory;\n+    private final StreamSegmentContainerFactory containerFactory;\n+    private final OperationLogFactory operationLogFactory;\n+    private final ReadIndexFactory readIndexFactory;\n+    private final AttributeIndexFactory attributeIndexFactory;\n+    private final WriterFactory writerFactory;\n+    private final CacheStorage cacheStorage;\n+    private final CacheManager cacheManager;\n+    // DL config that can be used to simulate no DurableLog truncations.\n+    private static final DurableLogConfig NO_TRUNCATIONS_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 10000)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 50000)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 1024 * 1024 * 1024L)\n+            .build();\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)", "originalCommit": "e47695e3a970facb5c1ae8875b79e3bf376c1023", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0OTAzMg==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r532749032", "bodyText": "Same with the next line", "author": "andreipaduroiu", "createdAt": "2020-11-30T16:56:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0ODg4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg5NTMxNA==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r533895314", "bodyText": "Using default values now.", "author": "ManishKumarKeshri", "createdAt": "2020-12-02T04:51:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0ODg4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0OTUxNQ==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r532749515", "bodyText": "3000 will still cause a flush after writing 3000 attributes. Do you think this is large enough? What is the point of these config settings?", "author": "andreipaduroiu", "createdAt": "2020-11-30T16:57:26Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/Tier1RecoveryCommand.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.ContainerRecoveryUtils;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.shared.NameUtils;\n+import lombok.Cleanup;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.logging.Level;\n+\n+/**\n+ * Loads the storage instance, recovers all segments from there.\n+ */\n+public class Tier1RecoveryCommand extends DataRecoveryCommand implements AutoCloseable {\n+    private static final int CONTAINER_EPOCH = 1;\n+    private static final Duration TIMEOUT = Duration.ofMillis(100 * 1000);\n+\n+    private final ScheduledExecutorService executorService = ExecutorServiceHelpers.newScheduledThreadPool(100, \"recoveryProcessor\");\n+    private final int containerCount;\n+    private final StorageFactory storageFactory;\n+    private final DurableDataLogFactory dataLogFactory;\n+    private final StreamSegmentContainerFactory containerFactory;\n+    private final OperationLogFactory operationLogFactory;\n+    private final ReadIndexFactory readIndexFactory;\n+    private final AttributeIndexFactory attributeIndexFactory;\n+    private final WriterFactory writerFactory;\n+    private final CacheStorage cacheStorage;\n+    private final CacheManager cacheManager;\n+    // DL config that can be used to simulate no DurableLog truncations.\n+    private static final DurableLogConfig NO_TRUNCATIONS_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 10000)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 50000)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 1024 * 1024 * 1024L)\n+            .build();\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig INFREQUENT_FLUSH_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1024 * 1024 * 1024)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3000)", "originalCommit": "e47695e3a970facb5c1ae8875b79e3bf376c1023", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg5NzYxMw==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r533897613", "bodyText": "I am not using debug segment container to write anything, apart from registering the segments on to new container metadata segment. So, changed it to default writer config.", "author": "ManishKumarKeshri", "createdAt": "2020-12-02T05:00:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0OTUxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0OTk5Ng==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r532749996", "bodyText": "So you close the client, print out a stack trace, then move on. How is the rest of the code going to execute afterwards if this isn't properly initialized?", "author": "andreipaduroiu", "createdAt": "2020-11-30T16:58:04Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/Tier1RecoveryCommand.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.ContainerRecoveryUtils;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.shared.NameUtils;\n+import lombok.Cleanup;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.logging.Level;\n+\n+/**\n+ * Loads the storage instance, recovers all segments from there.\n+ */\n+public class Tier1RecoveryCommand extends DataRecoveryCommand implements AutoCloseable {\n+    private static final int CONTAINER_EPOCH = 1;\n+    private static final Duration TIMEOUT = Duration.ofMillis(100 * 1000);\n+\n+    private final ScheduledExecutorService executorService = ExecutorServiceHelpers.newScheduledThreadPool(100, \"recoveryProcessor\");\n+    private final int containerCount;\n+    private final StorageFactory storageFactory;\n+    private final DurableDataLogFactory dataLogFactory;\n+    private final StreamSegmentContainerFactory containerFactory;\n+    private final OperationLogFactory operationLogFactory;\n+    private final ReadIndexFactory readIndexFactory;\n+    private final AttributeIndexFactory attributeIndexFactory;\n+    private final WriterFactory writerFactory;\n+    private final CacheStorage cacheStorage;\n+    private final CacheManager cacheManager;\n+    // DL config that can be used to simulate no DurableLog truncations.\n+    private static final DurableLogConfig NO_TRUNCATIONS_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 10000)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 50000)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 1024 * 1024 * 1024L)\n+            .build();\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig INFREQUENT_FLUSH_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1024 * 1024 * 1024)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3000)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 250000L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 100L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 500L)\n+            .build();\n+    private Storage storage;\n+\n+    /**\n+     * Creates an instance of Tier1RecoveryCommand class.\n+     *\n+     * @param args The arguments for the command.\n+     */\n+    public Tier1RecoveryCommand(CommandArgs args) {\n+        super(args);\n+        this.containerCount = getServiceConfig().getContainerCount();\n+        this.storageFactory = createStorageFactory(executorService);\n+\n+        val config = getCommandArgs().getState().getConfigBuilder().build().getConfig(ContainerConfig::builder);\n+\n+        // Start a zk client and create a bookKeeperLogFactory\n+        val bkConfig = getCommandArgs().getState().getConfigBuilder()\n+                .include(BookKeeperConfig.builder().with(BookKeeperConfig.ZK_ADDRESS, getServiceConfig().getZkURL()))\n+                .build().getConfig(BookKeeperConfig::builder);\n+\n+        val zkClient = createZKClient();\n+        this.dataLogFactory = new BookKeeperLogFactory(bkConfig, zkClient, executorService);\n+        try {\n+            this.dataLogFactory.initialize();\n+        } catch (DurableDataLogException ex) {\n+            zkClient.close();\n+            ex.printStackTrace();", "originalCommit": "e47695e3a970facb5c1ae8875b79e3bf376c1023", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg5ODc0Mg==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r533898742", "bodyText": "Now, throwing the same exception.", "author": "ManishKumarKeshri", "createdAt": "2020-12-02T05:04:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc0OTk5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc1MDI3Mg==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r532750272", "bodyText": "You need to store this in a class-level field and close it when you close your class.", "author": "andreipaduroiu", "createdAt": "2020-11-30T16:58:27Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/Tier1RecoveryCommand.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.ContainerRecoveryUtils;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.shared.NameUtils;\n+import lombok.Cleanup;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.logging.Level;\n+\n+/**\n+ * Loads the storage instance, recovers all segments from there.\n+ */\n+public class Tier1RecoveryCommand extends DataRecoveryCommand implements AutoCloseable {\n+    private static final int CONTAINER_EPOCH = 1;\n+    private static final Duration TIMEOUT = Duration.ofMillis(100 * 1000);\n+\n+    private final ScheduledExecutorService executorService = ExecutorServiceHelpers.newScheduledThreadPool(100, \"recoveryProcessor\");\n+    private final int containerCount;\n+    private final StorageFactory storageFactory;\n+    private final DurableDataLogFactory dataLogFactory;\n+    private final StreamSegmentContainerFactory containerFactory;\n+    private final OperationLogFactory operationLogFactory;\n+    private final ReadIndexFactory readIndexFactory;\n+    private final AttributeIndexFactory attributeIndexFactory;\n+    private final WriterFactory writerFactory;\n+    private final CacheStorage cacheStorage;\n+    private final CacheManager cacheManager;\n+    // DL config that can be used to simulate no DurableLog truncations.\n+    private static final DurableLogConfig NO_TRUNCATIONS_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 10000)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 50000)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 1024 * 1024 * 1024L)\n+            .build();\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig INFREQUENT_FLUSH_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1024 * 1024 * 1024)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3000)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 250000L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 100L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 500L)\n+            .build();\n+    private Storage storage;\n+\n+    /**\n+     * Creates an instance of Tier1RecoveryCommand class.\n+     *\n+     * @param args The arguments for the command.\n+     */\n+    public Tier1RecoveryCommand(CommandArgs args) {\n+        super(args);\n+        this.containerCount = getServiceConfig().getContainerCount();\n+        this.storageFactory = createStorageFactory(executorService);\n+\n+        val config = getCommandArgs().getState().getConfigBuilder().build().getConfig(ContainerConfig::builder);\n+\n+        // Start a zk client and create a bookKeeperLogFactory\n+        val bkConfig = getCommandArgs().getState().getConfigBuilder()\n+                .include(BookKeeperConfig.builder().with(BookKeeperConfig.ZK_ADDRESS, getServiceConfig().getZkURL()))\n+                .build().getConfig(BookKeeperConfig::builder);\n+\n+        val zkClient = createZKClient();", "originalCommit": "e47695e3a970facb5c1ae8875b79e3bf376c1023", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc1MzUyMQ==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r532753521", "bodyText": "Same with all other objects that need closing.", "author": "andreipaduroiu", "createdAt": "2020-11-30T17:02:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc1MDI3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzkwMTAyNA==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r533901024", "bodyText": "Done.", "author": "ManishKumarKeshri", "createdAt": "2020-12-02T05:12:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc1MDI3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc1NDAyNA==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r532754024", "bodyText": "Make this final and set it in the constructor", "author": "andreipaduroiu", "createdAt": "2020-11-30T17:03:41Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/Tier1RecoveryCommand.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.ContainerRecoveryUtils;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.shared.NameUtils;\n+import lombok.Cleanup;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.logging.Level;\n+\n+/**\n+ * Loads the storage instance, recovers all segments from there.\n+ */\n+public class Tier1RecoveryCommand extends DataRecoveryCommand implements AutoCloseable {\n+    private static final int CONTAINER_EPOCH = 1;\n+    private static final Duration TIMEOUT = Duration.ofMillis(100 * 1000);\n+\n+    private final ScheduledExecutorService executorService = ExecutorServiceHelpers.newScheduledThreadPool(100, \"recoveryProcessor\");\n+    private final int containerCount;\n+    private final StorageFactory storageFactory;\n+    private final DurableDataLogFactory dataLogFactory;\n+    private final StreamSegmentContainerFactory containerFactory;\n+    private final OperationLogFactory operationLogFactory;\n+    private final ReadIndexFactory readIndexFactory;\n+    private final AttributeIndexFactory attributeIndexFactory;\n+    private final WriterFactory writerFactory;\n+    private final CacheStorage cacheStorage;\n+    private final CacheManager cacheManager;\n+    // DL config that can be used to simulate no DurableLog truncations.\n+    private static final DurableLogConfig NO_TRUNCATIONS_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 10000)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 50000)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 1024 * 1024 * 1024L)\n+            .build();\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig INFREQUENT_FLUSH_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1024 * 1024 * 1024)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3000)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 250000L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 100L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 500L)\n+            .build();\n+    private Storage storage;", "originalCommit": "e47695e3a970facb5c1ae8875b79e3bf376c1023", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzkwMTQ4Ng==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r533901486", "bodyText": "Done.", "author": "ManishKumarKeshri", "createdAt": "2020-12-02T05:13:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc1NDAyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc1NDQwNg==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r532754406", "bodyText": "Do not check in commented out code.", "author": "andreipaduroiu", "createdAt": "2020-11-30T17:04:13Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/Tier1RecoveryCommand.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.ContainerRecoveryUtils;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.shared.NameUtils;\n+import lombok.Cleanup;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.logging.Level;\n+\n+/**\n+ * Loads the storage instance, recovers all segments from there.\n+ */\n+public class Tier1RecoveryCommand extends DataRecoveryCommand implements AutoCloseable {\n+    private static final int CONTAINER_EPOCH = 1;\n+    private static final Duration TIMEOUT = Duration.ofMillis(100 * 1000);\n+\n+    private final ScheduledExecutorService executorService = ExecutorServiceHelpers.newScheduledThreadPool(100, \"recoveryProcessor\");\n+    private final int containerCount;\n+    private final StorageFactory storageFactory;\n+    private final DurableDataLogFactory dataLogFactory;\n+    private final StreamSegmentContainerFactory containerFactory;\n+    private final OperationLogFactory operationLogFactory;\n+    private final ReadIndexFactory readIndexFactory;\n+    private final AttributeIndexFactory attributeIndexFactory;\n+    private final WriterFactory writerFactory;\n+    private final CacheStorage cacheStorage;\n+    private final CacheManager cacheManager;\n+    // DL config that can be used to simulate no DurableLog truncations.\n+    private static final DurableLogConfig NO_TRUNCATIONS_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 10000)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 50000)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 1024 * 1024 * 1024L)\n+            .build();\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig INFREQUENT_FLUSH_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1024 * 1024 * 1024)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3000)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 250000L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 100L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 500L)\n+            .build();\n+    private Storage storage;\n+\n+    /**\n+     * Creates an instance of Tier1RecoveryCommand class.\n+     *\n+     * @param args The arguments for the command.\n+     */\n+    public Tier1RecoveryCommand(CommandArgs args) {\n+        super(args);\n+        this.containerCount = getServiceConfig().getContainerCount();\n+        this.storageFactory = createStorageFactory(executorService);\n+\n+        val config = getCommandArgs().getState().getConfigBuilder().build().getConfig(ContainerConfig::builder);\n+\n+        // Start a zk client and create a bookKeeperLogFactory\n+        val bkConfig = getCommandArgs().getState().getConfigBuilder()\n+                .include(BookKeeperConfig.builder().with(BookKeeperConfig.ZK_ADDRESS, getServiceConfig().getZkURL()))\n+                .build().getConfig(BookKeeperConfig::builder);\n+\n+        val zkClient = createZKClient();\n+        this.dataLogFactory = new BookKeeperLogFactory(bkConfig, zkClient, executorService);\n+        try {\n+            this.dataLogFactory.initialize();\n+        } catch (DurableDataLogException ex) {\n+            zkClient.close();\n+            ex.printStackTrace();\n+        }\n+\n+        this.operationLogFactory = new DurableLogFactory(NO_TRUNCATIONS_DURABLE_LOG_CONFIG, this.dataLogFactory, executorService);\n+        this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);\n+        this.cacheManager = new CacheManager(CachePolicy.INFINITE, this.cacheStorage, executorService);\n+        this.readIndexFactory = new ContainerReadIndexFactory(DEFAULT_READ_INDEX_CONFIG, this.cacheManager, executorService);\n+        this.attributeIndexFactory = new ContainerAttributeIndexFactoryImpl(DEFAULT_ATTRIBUTE_INDEX_CONFIG, this.cacheManager, executorService);\n+        this.writerFactory = new StorageWriterFactory(INFREQUENT_FLUSH_WRITER_CONFIG, executorService);\n+        this.containerFactory = new StreamSegmentContainerFactory(config, this.operationLogFactory,\n+                this.readIndexFactory, this.attributeIndexFactory, this.writerFactory, this.storageFactory,\n+                this::createContainerExtensions, executorService);\n+    }\n+\n+    private Map<Class<? extends SegmentContainerExtension>, SegmentContainerExtension> createContainerExtensions(\n+            SegmentContainer container, ScheduledExecutorService executor) {\n+        return Collections.singletonMap(ContainerTableExtension.class, new ContainerTableExtensionImpl(container, this.cacheManager, executor));\n+    }\n+\n+    @Override\n+    public void execute() throws Exception {\n+        // set up logging\n+        setLogging(descriptor().getName());\n+        output(Level.INFO, \"Container Count = %d\", this.containerCount);\n+\n+        this.storage = this.storageFactory.createStorageAdapter();\n+        storage.initialize(CONTAINER_EPOCH);\n+        output(Level.INFO, \"Loaded %s Storage.\", getServiceConfig().getStorageImplementation().toString());\n+\n+        output(Level.INFO, \"Starting recovery...\");\n+        // create back up of metadata segments\n+        Map<Integer, String> backUpMetadataSegments = ContainerRecoveryUtils.createBackUpMetadataSegments(storage,\n+                this.containerCount, executorService, TIMEOUT);\n+\n+        // Start debug segment containers\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = createContainers();\n+        output(Level.INFO, \"Debug segment containers started.\");\n+\n+        output(Level.INFO, \"Recovering all segments...\");\n+        ContainerRecoveryUtils.recoverAllSegments(storage, debugStreamSegmentContainerMap, executorService, TIMEOUT);\n+        output(Level.INFO, \"All segments recovered.\");\n+\n+        // Update core attributes from the backUp Metadata segments\n+        output(Level.INFO, \"Updating core attributes for segments registered.\");\n+//        ContainerRecoveryUtils.updateCoreAttributes(backUpMetadataSegments, debugStreamSegmentContainerMap, executorService,", "originalCommit": "e47695e3a970facb5c1ae8875b79e3bf376c1023", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjc1NDgzMw==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r532754833", "bodyText": "If you don't make this field final, then this may throw a NullPtrEx. Make the field final.", "author": "andreipaduroiu", "createdAt": "2020-11-30T17:04:49Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/Tier1RecoveryCommand.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.ContainerRecoveryUtils;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.shared.NameUtils;\n+import lombok.Cleanup;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.logging.Level;\n+\n+/**\n+ * Loads the storage instance, recovers all segments from there.\n+ */\n+public class Tier1RecoveryCommand extends DataRecoveryCommand implements AutoCloseable {\n+    private static final int CONTAINER_EPOCH = 1;\n+    private static final Duration TIMEOUT = Duration.ofMillis(100 * 1000);\n+\n+    private final ScheduledExecutorService executorService = ExecutorServiceHelpers.newScheduledThreadPool(100, \"recoveryProcessor\");\n+    private final int containerCount;\n+    private final StorageFactory storageFactory;\n+    private final DurableDataLogFactory dataLogFactory;\n+    private final StreamSegmentContainerFactory containerFactory;\n+    private final OperationLogFactory operationLogFactory;\n+    private final ReadIndexFactory readIndexFactory;\n+    private final AttributeIndexFactory attributeIndexFactory;\n+    private final WriterFactory writerFactory;\n+    private final CacheStorage cacheStorage;\n+    private final CacheManager cacheManager;\n+    // DL config that can be used to simulate no DurableLog truncations.\n+    private static final DurableLogConfig NO_TRUNCATIONS_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 10000)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 50000)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 1024 * 1024 * 1024L)\n+            .build();\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig INFREQUENT_FLUSH_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1024 * 1024 * 1024)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3000)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 250000L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 100L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 500L)\n+            .build();\n+    private Storage storage;\n+\n+    /**\n+     * Creates an instance of Tier1RecoveryCommand class.\n+     *\n+     * @param args The arguments for the command.\n+     */\n+    public Tier1RecoveryCommand(CommandArgs args) {\n+        super(args);\n+        this.containerCount = getServiceConfig().getContainerCount();\n+        this.storageFactory = createStorageFactory(executorService);\n+\n+        val config = getCommandArgs().getState().getConfigBuilder().build().getConfig(ContainerConfig::builder);\n+\n+        // Start a zk client and create a bookKeeperLogFactory\n+        val bkConfig = getCommandArgs().getState().getConfigBuilder()\n+                .include(BookKeeperConfig.builder().with(BookKeeperConfig.ZK_ADDRESS, getServiceConfig().getZkURL()))\n+                .build().getConfig(BookKeeperConfig::builder);\n+\n+        val zkClient = createZKClient();\n+        this.dataLogFactory = new BookKeeperLogFactory(bkConfig, zkClient, executorService);\n+        try {\n+            this.dataLogFactory.initialize();\n+        } catch (DurableDataLogException ex) {\n+            zkClient.close();\n+            ex.printStackTrace();\n+        }\n+\n+        this.operationLogFactory = new DurableLogFactory(NO_TRUNCATIONS_DURABLE_LOG_CONFIG, this.dataLogFactory, executorService);\n+        this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);\n+        this.cacheManager = new CacheManager(CachePolicy.INFINITE, this.cacheStorage, executorService);\n+        this.readIndexFactory = new ContainerReadIndexFactory(DEFAULT_READ_INDEX_CONFIG, this.cacheManager, executorService);\n+        this.attributeIndexFactory = new ContainerAttributeIndexFactoryImpl(DEFAULT_ATTRIBUTE_INDEX_CONFIG, this.cacheManager, executorService);\n+        this.writerFactory = new StorageWriterFactory(INFREQUENT_FLUSH_WRITER_CONFIG, executorService);\n+        this.containerFactory = new StreamSegmentContainerFactory(config, this.operationLogFactory,\n+                this.readIndexFactory, this.attributeIndexFactory, this.writerFactory, this.storageFactory,\n+                this::createContainerExtensions, executorService);\n+    }\n+\n+    private Map<Class<? extends SegmentContainerExtension>, SegmentContainerExtension> createContainerExtensions(\n+            SegmentContainer container, ScheduledExecutorService executor) {\n+        return Collections.singletonMap(ContainerTableExtension.class, new ContainerTableExtensionImpl(container, this.cacheManager, executor));\n+    }\n+\n+    @Override\n+    public void execute() throws Exception {\n+        // set up logging\n+        setLogging(descriptor().getName());\n+        output(Level.INFO, \"Container Count = %d\", this.containerCount);\n+\n+        this.storage = this.storageFactory.createStorageAdapter();\n+        storage.initialize(CONTAINER_EPOCH);\n+        output(Level.INFO, \"Loaded %s Storage.\", getServiceConfig().getStorageImplementation().toString());\n+\n+        output(Level.INFO, \"Starting recovery...\");\n+        // create back up of metadata segments\n+        Map<Integer, String> backUpMetadataSegments = ContainerRecoveryUtils.createBackUpMetadataSegments(storage,\n+                this.containerCount, executorService, TIMEOUT);\n+\n+        // Start debug segment containers\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = createContainers();\n+        output(Level.INFO, \"Debug segment containers started.\");\n+\n+        output(Level.INFO, \"Recovering all segments...\");\n+        ContainerRecoveryUtils.recoverAllSegments(storage, debugStreamSegmentContainerMap, executorService, TIMEOUT);\n+        output(Level.INFO, \"All segments recovered.\");\n+\n+        // Update core attributes from the backUp Metadata segments\n+        output(Level.INFO, \"Updating core attributes for segments registered.\");\n+//        ContainerRecoveryUtils.updateCoreAttributes(backUpMetadataSegments, debugStreamSegmentContainerMap, executorService,\n+//                TIMEOUT);\n+\n+        // Waits for metadata segments to be flushed to LTS and then stops the debug segment containers\n+        stopDebugSegmentContainersPostFlush(debugStreamSegmentContainerMap);\n+\n+        output(Level.INFO, \"Segments have been recovered.\");\n+        output(Level.INFO, \"Recovery Done!\");\n+    }\n+\n+    // Closes the debug segment container instances in the given map after waiting for the metadata segment to be flushed to\n+    // the given storage.\n+    private void stopDebugSegmentContainersPostFlush(Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap)\n+            throws Exception {\n+        for (val debugSegmentContainer : debugStreamSegmentContainerMap.values()) {\n+            output(Level.FINE, \"Waiting for metadata segment of container %d to be flushed to the Long-Term storage.\",\n+                    debugSegmentContainer.getId());\n+            debugSegmentContainer.flushToStorage(TIMEOUT).join();\n+        }\n+\n+        for (val debugSegmentContainerEntry : debugStreamSegmentContainerMap.entrySet()) {\n+            Services.stopAsync(debugSegmentContainerEntry.getValue(), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            output(Level.FINE, \"Stopping debug segment container %d.\", debugSegmentContainerEntry.getKey());\n+            debugSegmentContainerEntry.getValue().close();\n+        }\n+    }\n+\n+    public static CommandDescriptor descriptor() {\n+        return new CommandDescriptor(COMPONENT, \"Tier1-recovery\", \"Recover Tier1 state from the storage.\");\n+    }\n+\n+    // Creates debug segment container instances, puts them in a map and returns it.\n+    private Map<Integer, DebugStreamSegmentContainer> createContainers() {\n+        // Start a debug segment container corresponding to the given container Id and put it in the Hashmap with the Id.\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = new HashMap<>();\n+\n+        // Create a debug segment container instances using a\n+        for (int containerId = 0; containerId < this.containerCount; containerId++) {\n+            DebugStreamSegmentContainer debugStreamSegmentContainer = (DebugStreamSegmentContainer)\n+                    this.containerFactory.createDebugStreamSegmentContainer(containerId);\n+            Services.startAsync(debugStreamSegmentContainer, executorService).join();\n+            debugStreamSegmentContainerMap.put(containerId, debugStreamSegmentContainer);\n+            output(Level.FINE, \"Container %d started.\", containerId);\n+        }\n+        return debugStreamSegmentContainerMap;\n+    }\n+\n+    @Override\n+    public void close() {\n+        this.cacheManager.close();\n+        this.cacheStorage.close();\n+        this.readIndexFactory.close();\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+        }\n+        storage.close();", "originalCommit": "e47695e3a970facb5c1ae8875b79e3bf376c1023", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "2c247aa25155ead7afed86c91cd0fda2a81a5334", "url": "https://github.com/pravega/pravega/commit/2c247aa25155ead7afed86c91cd0fda2a81a5334", "message": "Updating.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-12-02T03:47:38Z", "type": "commit"}, {"oid": "2df64b36e3e2438d85b778de92e68c3ebcf69fbd", "url": "https://github.com/pravega/pravega/commit/2df64b36e3e2438d85b778de92e68c3ebcf69fbd", "message": "Updating.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-12-02T04:01:09Z", "type": "commit"}, {"oid": "da40eb138c3fcaaf2f97c800ea1740627b073eb3", "url": "https://github.com/pravega/pravega/commit/da40eb138c3fcaaf2f97c800ea1740627b073eb3", "message": "Fixing comments.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-12-02T05:23:50Z", "type": "commit"}, {"oid": "368a5d25286dd58e1b622095c92986c868e10d21", "url": "https://github.com/pravega/pravega/commit/368a5d25286dd58e1b622095c92986c868e10d21", "message": "Removing logs directory from DataRecoveryTest\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-12-02T05:41:28Z", "type": "commit"}, {"oid": "c935a17a429a2334515a95fd8ecd3a6b22acb7c9", "url": "https://github.com/pravega/pravega/commit/c935a17a429a2334515a95fd8ecd3a6b22acb7c9", "message": "Minor changes.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-12-02T06:10:21Z", "type": "commit"}, {"oid": "def5bafbf7c89dccce44cee1cc750ce9d707307c", "url": "https://github.com/pravega/pravega/commit/def5bafbf7c89dccce44cee1cc750ce9d707307c", "message": "Minor changes.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-12-02T06:49:20Z", "type": "commit"}, {"oid": "85331e444249645b0c569b8f06813cb425b8a771", "url": "https://github.com/pravega/pravega/commit/85331e444249645b0c569b8f06813cb425b8a771", "message": "Updating dr test with new command name.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-12-02T17:01:28Z", "type": "commit"}, {"oid": "9867c311fee3b26408fc787fdfec7e40e164e280", "url": "https://github.com/pravega/pravega/commit/9867c311fee3b26408fc787fdfec7e40e164e280", "message": "Removing unnecessary file.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-12-02T17:12:54Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU3NTIzMw==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r535575233", "bodyText": "@andreipaduroiu @sachin-j-joshi  Should I remove this? This method was included for testing purpose to verify if the attributes for each of the segment is correctly updated or not?", "author": "ManishKumarKeshri", "createdAt": "2020-12-03T20:31:49Z", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -441,4 +443,106 @@ public static void updateCoreAttributes(Map<Integer, String> backUpMetadataSegme\n         Futures.allOf(futures).get(timeout.toMillis(), TimeUnit.MILLISECONDS);\n         return backUpMetadataSegments;\n     }\n+\n+    public static boolean matchAttributes(Map<Integer, String> backUpMetadataSegments,", "originalCommit": "9867c311fee3b26408fc787fdfec7e40e164e280", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY4NDEyMg==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r535684122", "bodyText": "Actually, I removed it. Please let me know if it's required.", "author": "ManishKumarKeshri", "createdAt": "2020-12-03T22:17:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU3NTIzMw=="}], "type": "inlineReview"}, {"oid": "4dab0ebcc9fcf9d9252d23dd54c776bd00b69992", "url": "https://github.com/pravega/pravega/commit/4dab0ebcc9fcf9d9252d23dd54c776bd00b69992", "message": "Removing match attributes.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-12-03T21:32:36Z", "type": "commit"}, {"oid": "7e45b4b6684402aee3ca376d4e785229db155e0f", "url": "https://github.com/pravega/pravega/commit/7e45b4b6684402aee3ca376d4e785229db155e0f", "message": "Some checkstyle fails.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-12-03T22:01:54Z", "type": "commit"}, {"oid": "3791cc8325db5126254b303ac0fb2e7a1559b075", "url": "https://github.com/pravega/pravega/commit/3791cc8325db5126254b303ac0fb2e7a1559b075", "message": "Removing unnecessary parts.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-12-03T22:11:23Z", "type": "commit"}, {"oid": "7c87aca72068e2676924a7c35af8b5d1f706328d", "url": "https://github.com/pravega/pravega/commit/7c87aca72068e2676924a7c35af8b5d1f706328d", "message": "Updating file path.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-12-04T02:26:15Z", "type": "commit"}, {"oid": "7772ced295c870f581960fd16a51c1020c72134e", "url": "https://github.com/pravega/pravega/commit/7772ced295c870f581960fd16a51c1020c72134e", "message": "Updating properties file.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-12-04T05:14:20Z", "type": "commit"}, {"oid": "0a79b063b0ce3a48135dd878c4ade2bec390c6a6", "url": "https://github.com/pravega/pravega/commit/0a79b063b0ce3a48135dd878c4ade2bec390c6a6", "message": "Updating properties file.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-12-04T05:15:56Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg5MjExMw==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r538892113", "bodyText": "Do you really need 100 threads? Are 10 enough?\nThe entire segment store runs on 30 threads...", "author": "andreipaduroiu", "createdAt": "2020-12-08T23:38:52Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/DurableLogRecoveryCommand.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.ContainerRecoveryUtils;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import lombok.Cleanup;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+\n+/**\n+ * Loads the storage instance, recovers all segments from there.\n+ */\n+public class DurableLogRecoveryCommand extends DataRecoveryCommand implements AutoCloseable {\n+    private static final int CONTAINER_EPOCH = 1;\n+    private static final Duration TIMEOUT = Duration.ofMillis(100 * 1000);\n+\n+    private static final DurableLogConfig NO_TRUNCATIONS_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 10000)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 50000)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 1024 * 1024 * 1024L)\n+            .build();\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig.builder().build();\n+\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .build();\n+\n+    private static final WriterConfig WRITER_CONFIG = WriterConfig.builder().build();\n+\n+    private final ScheduledExecutorService executorService = ExecutorServiceHelpers.newScheduledThreadPool(100, \"recoveryProcessor\");", "originalCommit": "0a79b063b0ce3a48135dd878c4ade2bec390c6a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTEzNDU3Mg==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r539134572", "bodyText": "10 is enough I think.", "author": "ManishKumarKeshri", "createdAt": "2020-12-09T09:14:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg5MjExMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg5MjMyNw==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r538892327", "bodyText": "cleanup the data log factory if you get an error here", "author": "andreipaduroiu", "createdAt": "2020-12-08T23:39:25Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/DurableLogRecoveryCommand.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.ContainerRecoveryUtils;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import lombok.Cleanup;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+\n+/**\n+ * Loads the storage instance, recovers all segments from there.\n+ */\n+public class DurableLogRecoveryCommand extends DataRecoveryCommand implements AutoCloseable {\n+    private static final int CONTAINER_EPOCH = 1;\n+    private static final Duration TIMEOUT = Duration.ofMillis(100 * 1000);\n+\n+    private static final DurableLogConfig NO_TRUNCATIONS_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 10000)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 50000)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 1024 * 1024 * 1024L)\n+            .build();\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig.builder().build();\n+\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .build();\n+\n+    private static final WriterConfig WRITER_CONFIG = WriterConfig.builder().build();\n+\n+    private final ScheduledExecutorService executorService = ExecutorServiceHelpers.newScheduledThreadPool(100, \"recoveryProcessor\");\n+    private final int containerCount;\n+    private final StorageFactory storageFactory;\n+    private final CuratorFramework zkClient;\n+    private final Storage storage;\n+    private BookKeeperLogFactory dataLogFactory = null;\n+\n+    /**\n+     * Creates an instance of DurableLogRecoveryCommand class.\n+     *\n+     * @param args The arguments for the command.\n+     */\n+    public DurableLogRecoveryCommand(CommandArgs args) {\n+        super(args);\n+        this.containerCount = getServiceConfig().getContainerCount();\n+        this.storageFactory = createStorageFactory(executorService);\n+        this.storage = this.storageFactory.createStorageAdapter();\n+        this.zkClient = createZKClient();\n+    }\n+\n+\n+    @Override\n+    public void execute() throws Exception {\n+        outputInfo(\"Container Count = %d\", this.containerCount);\n+\n+        // Start a zk client and create a bookKeeperLogFactory\n+        val bkConfig = getCommandArgs().getState().getConfigBuilder()\n+                .include(BookKeeperConfig.builder().with(BookKeeperConfig.ZK_ADDRESS, getServiceConfig().getZkURL()))\n+                .build().getConfig(BookKeeperConfig::builder);\n+\n+        this.dataLogFactory = new BookKeeperLogFactory(bkConfig, this.zkClient, executorService);\n+        try {\n+            this.dataLogFactory.initialize();\n+        } catch (DurableDataLogException ex) {\n+            this.zkClient.close();", "originalCommit": "0a79b063b0ce3a48135dd878c4ade2bec390c6a6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg5MjYyOA==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r538892628", "bodyText": "Why don't you create this in the constructor? That's where you create everything else and you have everything else you need.\nYou can leave the initialization here.", "author": "andreipaduroiu", "createdAt": "2020-12-08T23:40:09Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/DurableLogRecoveryCommand.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.ContainerRecoveryUtils;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import lombok.Cleanup;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+\n+/**\n+ * Loads the storage instance, recovers all segments from there.\n+ */\n+public class DurableLogRecoveryCommand extends DataRecoveryCommand implements AutoCloseable {\n+    private static final int CONTAINER_EPOCH = 1;\n+    private static final Duration TIMEOUT = Duration.ofMillis(100 * 1000);\n+\n+    private static final DurableLogConfig NO_TRUNCATIONS_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 10000)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 50000)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 1024 * 1024 * 1024L)\n+            .build();\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig.builder().build();\n+\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .build();\n+\n+    private static final WriterConfig WRITER_CONFIG = WriterConfig.builder().build();\n+\n+    private final ScheduledExecutorService executorService = ExecutorServiceHelpers.newScheduledThreadPool(100, \"recoveryProcessor\");\n+    private final int containerCount;\n+    private final StorageFactory storageFactory;\n+    private final CuratorFramework zkClient;\n+    private final Storage storage;\n+    private BookKeeperLogFactory dataLogFactory = null;\n+\n+    /**\n+     * Creates an instance of DurableLogRecoveryCommand class.\n+     *\n+     * @param args The arguments for the command.\n+     */\n+    public DurableLogRecoveryCommand(CommandArgs args) {\n+        super(args);\n+        this.containerCount = getServiceConfig().getContainerCount();\n+        this.storageFactory = createStorageFactory(executorService);\n+        this.storage = this.storageFactory.createStorageAdapter();\n+        this.zkClient = createZKClient();\n+    }\n+\n+\n+    @Override\n+    public void execute() throws Exception {\n+        outputInfo(\"Container Count = %d\", this.containerCount);\n+\n+        // Start a zk client and create a bookKeeperLogFactory\n+        val bkConfig = getCommandArgs().getState().getConfigBuilder()\n+                .include(BookKeeperConfig.builder().with(BookKeeperConfig.ZK_ADDRESS, getServiceConfig().getZkURL()))\n+                .build().getConfig(BookKeeperConfig::builder);\n+\n+        this.dataLogFactory = new BookKeeperLogFactory(bkConfig, this.zkClient, executorService);", "originalCommit": "0a79b063b0ce3a48135dd878c4ade2bec390c6a6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg5MjcwMg==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r538892702", "bodyText": "This is not needed.", "author": "andreipaduroiu", "createdAt": "2020-12-08T23:40:20Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/DurableLogRecoveryCommand.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.ContainerRecoveryUtils;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import lombok.Cleanup;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+\n+/**\n+ * Loads the storage instance, recovers all segments from there.\n+ */\n+public class DurableLogRecoveryCommand extends DataRecoveryCommand implements AutoCloseable {\n+    private static final int CONTAINER_EPOCH = 1;\n+    private static final Duration TIMEOUT = Duration.ofMillis(100 * 1000);\n+\n+    private static final DurableLogConfig NO_TRUNCATIONS_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 10000)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 50000)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 1024 * 1024 * 1024L)\n+            .build();\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig.builder().build();\n+\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .build();\n+\n+    private static final WriterConfig WRITER_CONFIG = WriterConfig.builder().build();\n+\n+    private final ScheduledExecutorService executorService = ExecutorServiceHelpers.newScheduledThreadPool(100, \"recoveryProcessor\");\n+    private final int containerCount;\n+    private final StorageFactory storageFactory;\n+    private final CuratorFramework zkClient;\n+    private final Storage storage;\n+    private BookKeeperLogFactory dataLogFactory = null;\n+\n+    /**\n+     * Creates an instance of DurableLogRecoveryCommand class.\n+     *\n+     * @param args The arguments for the command.\n+     */\n+    public DurableLogRecoveryCommand(CommandArgs args) {\n+        super(args);\n+        this.containerCount = getServiceConfig().getContainerCount();\n+        this.storageFactory = createStorageFactory(executorService);\n+        this.storage = this.storageFactory.createStorageAdapter();\n+        this.zkClient = createZKClient();\n+    }\n+\n+\n+    @Override\n+    public void execute() throws Exception {\n+        outputInfo(\"Container Count = %d\", this.containerCount);\n+\n+        // Start a zk client and create a bookKeeperLogFactory\n+        val bkConfig = getCommandArgs().getState().getConfigBuilder()\n+                .include(BookKeeperConfig.builder().with(BookKeeperConfig.ZK_ADDRESS, getServiceConfig().getZkURL()))\n+                .build().getConfig(BookKeeperConfig::builder);\n+\n+        this.dataLogFactory = new BookKeeperLogFactory(bkConfig, this.zkClient, executorService);\n+        try {\n+            this.dataLogFactory.initialize();\n+        } catch (DurableDataLogException ex) {\n+            this.zkClient.close();\n+            ex.printStackTrace();", "originalCommit": "0a79b063b0ce3a48135dd878c4ade2bec390c6a6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg5NDAxNQ==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r538894015", "bodyText": "In your close method, you need to close all the (non-null) writers in this array.", "author": "andreipaduroiu", "createdAt": "2020-12-08T23:43:32Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/StorageListSegmentsCommand.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import org.apache.hadoop.fs.Path;\n+\n+import java.io.File;\n+import java.io.FileWriter;\n+import java.text.SimpleDateFormat;\n+import java.time.Duration;\n+import java.util.Arrays;\n+import java.util.Date;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+/**\n+ * Lists all non-shadow segments from there from the storage. The storage is loaded using the config properties.\n+ */\n+public class StorageListSegmentsCommand extends DataRecoveryCommand implements AutoCloseable {\n+    /**\n+     * Header line for writing segments' details to csv files.\n+     */\n+    private static final List<String> HEADER = Arrays.asList(\"Sealed Status\", \"Length\", \"Segment Name\");\n+    private static final int CONTAINER_EPOCH = 1;\n+    private final int containerCount;\n+    private final ScheduledExecutorService scheduledExecutorService = ExecutorServiceHelpers.newScheduledThreadPool(10,\n+            \"listSegmentsProcessor\");\n+    private final SegmentToContainerMapper segToConMapper;\n+    private final StorageFactory storageFactory;\n+    private final Storage storage;\n+    private final FileWriter[] csvWriters;\n+    private String filePath;\n+\n+    /**\n+     * Creates an instance of StorageListSegmentsCommand class.\n+     *\n+     * @param args The arguments for the command.\n+     */\n+    public StorageListSegmentsCommand(CommandArgs args) {\n+        super(args);\n+        this.containerCount = getServiceConfig().getContainerCount();\n+        this.segToConMapper = new SegmentToContainerMapper(this.containerCount);\n+        this.storageFactory = createStorageFactory(scheduledExecutorService);\n+        this.storage = this.storageFactory.createStorageAdapter();\n+        this.csvWriters = new FileWriter[this.containerCount];", "originalCommit": "0a79b063b0ce3a48135dd878c4ade2bec390c6a6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg5NDYzNA==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r538894634", "bodyText": "Please inherit from ThreadPooledTestSuite for any unit test suites that require an executor. That will create an executor (of your desired size - override the method) and you don't need to worry about shutting it down .", "author": "andreipaduroiu", "createdAt": "2020-12-08T23:44:57Z", "path": "cli/admin/src/test/java/io/pravega/cli/admin/dataRecovery/DataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,490 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.AdminCommandState;\n+import io.pravega.cli.admin.utils.TestUtils;\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.connection.impl.ConnectionFactory;\n+import io.pravega.client.connection.impl.ConnectionPool;\n+import io.pravega.client.connection.impl.ConnectionPoolImpl;\n+import io.pravega.client.connection.impl.SocketConnectionFactoryImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.tables.TableStore;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.store.ServiceConfig;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.AssertExtensions;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Properties;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+/**\n+ * Tests Data recovery commands.\n+ */\n+@Slf4j\n+public class DataRecoveryTest {\n+    private static final Duration TIMEOUT = Duration.ofMillis(30 * 1000);\n+    private static final int NUM_EVENTS = 10;\n+    private static final String EVENT = \"12345\";\n+    private static final String SCOPE = \"testScope\";\n+    // Setup utility.\n+    private static final Duration READ_TIMEOUT = Duration.ofMillis(1000);\n+    private static final AtomicReference<AdminCommandState> STATE = new AtomicReference<>();\n+\n+    @Rule\n+    public final Timeout globalTimeout = new Timeout(120, TimeUnit.SECONDS);\n+\n+    private final ScheduledExecutorService executor = ExecutorServiceHelpers.newScheduledThreadPool(10, \"Data recovery test pool\");", "originalCommit": "0a79b063b0ce3a48135dd878c4ade2bec390c6a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTEyODU0Mw==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r539128543", "bodyText": "Done.", "author": "ManishKumarKeshri", "createdAt": "2020-12-09T09:05:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg5NDYzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg5NDg3Ng==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r538894876", "bodyText": "It is a best practice to keep your tearDown next to your setUp methods. That way you can clearly see what you create and what you cleanup.", "author": "andreipaduroiu", "createdAt": "2020-12-08T23:45:34Z", "path": "cli/admin/src/test/java/io/pravega/cli/admin/dataRecovery/DataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,490 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.AdminCommandState;\n+import io.pravega.cli.admin.utils.TestUtils;\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.connection.impl.ConnectionFactory;\n+import io.pravega.client.connection.impl.ConnectionPool;\n+import io.pravega.client.connection.impl.ConnectionPoolImpl;\n+import io.pravega.client.connection.impl.SocketConnectionFactoryImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.tables.TableStore;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.store.ServiceConfig;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.AssertExtensions;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Properties;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+/**\n+ * Tests Data recovery commands.\n+ */\n+@Slf4j\n+public class DataRecoveryTest {\n+    private static final Duration TIMEOUT = Duration.ofMillis(30 * 1000);\n+    private static final int NUM_EVENTS = 10;\n+    private static final String EVENT = \"12345\";\n+    private static final String SCOPE = \"testScope\";\n+    // Setup utility.\n+    private static final Duration READ_TIMEOUT = Duration.ofMillis(1000);\n+    private static final AtomicReference<AdminCommandState> STATE = new AtomicReference<>();\n+\n+    @Rule\n+    public final Timeout globalTimeout = new Timeout(120, TimeUnit.SECONDS);\n+\n+    private final ScheduledExecutorService executor = ExecutorServiceHelpers.newScheduledThreadPool(10, \"Data recovery test pool\");\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    /**\n+     * A directory for FILESYSTEM storage as LTS.\n+     */\n+    private File baseDir = null;\n+    private FileSystemStorageConfig adapterConfig;\n+    private StorageFactory storageFactory = null;\n+\n+    /**\n+     * A directory for storing logs and CSV files generated during the test..\n+     */\n+    private File logsDir = null;\n+    private BookKeeperLogFactory factory = null;\n+\n+    @Before\n+    public void setUp() throws Exception {", "originalCommit": "0a79b063b0ce3a48135dd878c4ade2bec390c6a6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg5NTIwMw==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r538895203", "bodyText": "wait, why is there a capital D in this command? I thought all commands were lowercase.", "author": "andreipaduroiu", "createdAt": "2020-12-08T23:46:22Z", "path": "cli/admin/src/test/java/io/pravega/cli/admin/dataRecovery/DataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,490 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.AdminCommandState;\n+import io.pravega.cli.admin.utils.TestUtils;\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.connection.impl.ConnectionFactory;\n+import io.pravega.client.connection.impl.ConnectionPool;\n+import io.pravega.client.connection.impl.ConnectionPoolImpl;\n+import io.pravega.client.connection.impl.SocketConnectionFactoryImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.tables.TableStore;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.store.ServiceConfig;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.AssertExtensions;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Properties;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+/**\n+ * Tests Data recovery commands.\n+ */\n+@Slf4j\n+public class DataRecoveryTest {\n+    private static final Duration TIMEOUT = Duration.ofMillis(30 * 1000);\n+    private static final int NUM_EVENTS = 10;\n+    private static final String EVENT = \"12345\";\n+    private static final String SCOPE = \"testScope\";\n+    // Setup utility.\n+    private static final Duration READ_TIMEOUT = Duration.ofMillis(1000);\n+    private static final AtomicReference<AdminCommandState> STATE = new AtomicReference<>();\n+\n+    @Rule\n+    public final Timeout globalTimeout = new Timeout(120, TimeUnit.SECONDS);\n+\n+    private final ScheduledExecutorService executor = ExecutorServiceHelpers.newScheduledThreadPool(10, \"Data recovery test pool\");\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    /**\n+     * A directory for FILESYSTEM storage as LTS.\n+     */\n+    private File baseDir = null;\n+    private FileSystemStorageConfig adapterConfig;\n+    private StorageFactory storageFactory = null;\n+\n+    /**\n+     * A directory for storing logs and CSV files generated during the test..\n+     */\n+    private File logsDir = null;\n+    private BookKeeperLogFactory factory = null;\n+\n+    @Before\n+    public void setUp() throws Exception {\n+        this.baseDir = Files.createTempDirectory(\"TestDataRecovery\").toFile().getAbsoluteFile();\n+        this.logsDir = Files.createTempDirectory(\"DataRecovery\").toFile().getAbsoluteFile();\n+        this.adapterConfig = FileSystemStorageConfig.builder()\n+                .with(FileSystemStorageConfig.ROOT, this.baseDir.getAbsolutePath())\n+                .with(FileSystemStorageConfig.REPLACE_ENABLED, true)\n+                .build();\n+\n+        this.storageFactory = new FileSystemStorageFactory(adapterConfig, this.executor);\n+    }\n+\n+    // Creates the given scope and stream using the given controller instance.\n+    private void createScopeStream(Controller controller, String scopeName, String streamName) {\n+        ClientConfig clientConfig = ClientConfig.builder().build();\n+        try (ConnectionPool cp = new ConnectionPoolImpl(clientConfig, new SocketConnectionFactoryImpl(clientConfig));\n+             StreamManager streamManager = new StreamManagerImpl(controller, cp)) {\n+            //create scope\n+            Boolean createScopeStatus = streamManager.createScope(scopeName);\n+            log.info(\"Create scope status {}\", createScopeStatus);\n+            //create stream\n+            Boolean createStreamStatus = streamManager.createStream(scopeName, streamName, config);\n+            log.info(\"Create stream status {}\", createStreamStatus);\n+        }\n+    }\n+\n+    /**\n+     * Tests DurableLog recovery command.\n+     * @throws Exception    In case of any exception thrown while execution.\n+     */\n+    @Test\n+    public void testDataRecoveryCommand() throws Exception {\n+        int instanceId = 0;\n+        int bookieCount = 3;\n+        int containerCount = 1;\n+        @Cleanup\n+        PravegaRunner pravegaRunner = new PravegaRunner(instanceId++, bookieCount, containerCount, this.storageFactory);\n+        String streamName = \"testDataRecoveryCommand\";\n+\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, streamName);\n+        try (val clientRunner = new ClientRunner(pravegaRunner.controllerRunner)) {\n+            // Write events to the streams.\n+            writeEvents(streamName, clientRunner.clientFactory);\n+        }\n+        pravegaRunner.controllerRunner.close(); // Shut down the controller\n+\n+        // Flush all Tier 1 to LTS\n+        ServiceBuilder.ComponentSetup componentSetup = new ServiceBuilder.ComponentSetup(pravegaRunner.segmentStoreRunner.serviceBuilder);\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            componentSetup.getContainerRegistry().getContainer(containerId).flushToStorage(TIMEOUT).join();\n+        }\n+\n+        pravegaRunner.segmentStoreRunner.close(); // Shutdown SegmentStore\n+        pravegaRunner.bookKeeperRunner.close(); // Shutdown BookKeeper & ZooKeeper\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        pravegaRunner.bookKeeperRunner = new BookKeeperRunner(instanceId++, bookieCount);\n+\n+        // set pravega properties for the test\n+        STATE.set(new AdminCommandState());\n+        Properties pravegaProperties = new Properties();\n+        pravegaProperties.setProperty(\"pravegaservice.container.count\", \"1\");\n+        pravegaProperties.setProperty(\"pravegaservice.storage.impl.name\", \"FILESYSTEM\");\n+        pravegaProperties.setProperty(\"pravegaservice.storage.layout\", \"ROLLING_STORAGE\");\n+        pravegaProperties.setProperty(\"filesystem.root\", this.baseDir.getAbsolutePath());\n+        pravegaProperties.setProperty(\"pravegaservice.zk.connect.uri\", \"localhost:\" + pravegaRunner.bookKeeperRunner.bkPort);\n+        pravegaProperties.setProperty(\"bookkeeper.ledger.path\", pravegaRunner.bookKeeperRunner.ledgerPath);\n+        pravegaProperties.setProperty(\"bookkeeper.zk.metadata.path\", pravegaRunner.bookKeeperRunner.logMetaNamespace);\n+        pravegaProperties.setProperty(\"pravegaservice.clusterName\", pravegaRunner.bookKeeperRunner.baseNamespace);\n+        STATE.get().getConfigBuilder().include(pravegaProperties);\n+\n+        // Command under test\n+        TestUtils.executeCommand(\"storage DurableLog-recovery\", STATE.get());", "originalCommit": "0a79b063b0ce3a48135dd878c4ade2bec390c6a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTEyNjI5Ng==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r539126296", "bodyText": "made it small d.", "author": "ManishKumarKeshri", "createdAt": "2020-12-09T09:02:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg5NTIwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg5NTM2OA==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r538895368", "bodyText": "@Cleanup\nThat will guarantee it's closed even if your test fails.", "author": "andreipaduroiu", "createdAt": "2020-12-08T23:46:48Z", "path": "cli/admin/src/test/java/io/pravega/cli/admin/dataRecovery/DataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,490 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.AdminCommandState;\n+import io.pravega.cli.admin.utils.TestUtils;\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.connection.impl.ConnectionFactory;\n+import io.pravega.client.connection.impl.ConnectionPool;\n+import io.pravega.client.connection.impl.ConnectionPoolImpl;\n+import io.pravega.client.connection.impl.SocketConnectionFactoryImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.tables.TableStore;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.store.ServiceConfig;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.AssertExtensions;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Properties;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+/**\n+ * Tests Data recovery commands.\n+ */\n+@Slf4j\n+public class DataRecoveryTest {\n+    private static final Duration TIMEOUT = Duration.ofMillis(30 * 1000);\n+    private static final int NUM_EVENTS = 10;\n+    private static final String EVENT = \"12345\";\n+    private static final String SCOPE = \"testScope\";\n+    // Setup utility.\n+    private static final Duration READ_TIMEOUT = Duration.ofMillis(1000);\n+    private static final AtomicReference<AdminCommandState> STATE = new AtomicReference<>();\n+\n+    @Rule\n+    public final Timeout globalTimeout = new Timeout(120, TimeUnit.SECONDS);\n+\n+    private final ScheduledExecutorService executor = ExecutorServiceHelpers.newScheduledThreadPool(10, \"Data recovery test pool\");\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    /**\n+     * A directory for FILESYSTEM storage as LTS.\n+     */\n+    private File baseDir = null;\n+    private FileSystemStorageConfig adapterConfig;\n+    private StorageFactory storageFactory = null;\n+\n+    /**\n+     * A directory for storing logs and CSV files generated during the test..\n+     */\n+    private File logsDir = null;\n+    private BookKeeperLogFactory factory = null;\n+\n+    @Before\n+    public void setUp() throws Exception {\n+        this.baseDir = Files.createTempDirectory(\"TestDataRecovery\").toFile().getAbsoluteFile();\n+        this.logsDir = Files.createTempDirectory(\"DataRecovery\").toFile().getAbsoluteFile();\n+        this.adapterConfig = FileSystemStorageConfig.builder()\n+                .with(FileSystemStorageConfig.ROOT, this.baseDir.getAbsolutePath())\n+                .with(FileSystemStorageConfig.REPLACE_ENABLED, true)\n+                .build();\n+\n+        this.storageFactory = new FileSystemStorageFactory(adapterConfig, this.executor);\n+    }\n+\n+    // Creates the given scope and stream using the given controller instance.\n+    private void createScopeStream(Controller controller, String scopeName, String streamName) {\n+        ClientConfig clientConfig = ClientConfig.builder().build();\n+        try (ConnectionPool cp = new ConnectionPoolImpl(clientConfig, new SocketConnectionFactoryImpl(clientConfig));\n+             StreamManager streamManager = new StreamManagerImpl(controller, cp)) {\n+            //create scope\n+            Boolean createScopeStatus = streamManager.createScope(scopeName);\n+            log.info(\"Create scope status {}\", createScopeStatus);\n+            //create stream\n+            Boolean createStreamStatus = streamManager.createStream(scopeName, streamName, config);\n+            log.info(\"Create stream status {}\", createStreamStatus);\n+        }\n+    }\n+\n+    /**\n+     * Tests DurableLog recovery command.\n+     * @throws Exception    In case of any exception thrown while execution.\n+     */\n+    @Test\n+    public void testDataRecoveryCommand() throws Exception {\n+        int instanceId = 0;\n+        int bookieCount = 3;\n+        int containerCount = 1;\n+        @Cleanup\n+        PravegaRunner pravegaRunner = new PravegaRunner(instanceId++, bookieCount, containerCount, this.storageFactory);\n+        String streamName = \"testDataRecoveryCommand\";\n+\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, streamName);\n+        try (val clientRunner = new ClientRunner(pravegaRunner.controllerRunner)) {\n+            // Write events to the streams.\n+            writeEvents(streamName, clientRunner.clientFactory);\n+        }\n+        pravegaRunner.controllerRunner.close(); // Shut down the controller\n+\n+        // Flush all Tier 1 to LTS\n+        ServiceBuilder.ComponentSetup componentSetup = new ServiceBuilder.ComponentSetup(pravegaRunner.segmentStoreRunner.serviceBuilder);\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            componentSetup.getContainerRegistry().getContainer(containerId).flushToStorage(TIMEOUT).join();\n+        }\n+\n+        pravegaRunner.segmentStoreRunner.close(); // Shutdown SegmentStore\n+        pravegaRunner.bookKeeperRunner.close(); // Shutdown BookKeeper & ZooKeeper\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        pravegaRunner.bookKeeperRunner = new BookKeeperRunner(instanceId++, bookieCount);\n+\n+        // set pravega properties for the test\n+        STATE.set(new AdminCommandState());\n+        Properties pravegaProperties = new Properties();\n+        pravegaProperties.setProperty(\"pravegaservice.container.count\", \"1\");\n+        pravegaProperties.setProperty(\"pravegaservice.storage.impl.name\", \"FILESYSTEM\");\n+        pravegaProperties.setProperty(\"pravegaservice.storage.layout\", \"ROLLING_STORAGE\");\n+        pravegaProperties.setProperty(\"filesystem.root\", this.baseDir.getAbsolutePath());\n+        pravegaProperties.setProperty(\"pravegaservice.zk.connect.uri\", \"localhost:\" + pravegaRunner.bookKeeperRunner.bkPort);\n+        pravegaProperties.setProperty(\"bookkeeper.ledger.path\", pravegaRunner.bookKeeperRunner.ledgerPath);\n+        pravegaProperties.setProperty(\"bookkeeper.zk.metadata.path\", pravegaRunner.bookKeeperRunner.logMetaNamespace);\n+        pravegaProperties.setProperty(\"pravegaservice.clusterName\", pravegaRunner.bookKeeperRunner.baseNamespace);\n+        STATE.get().getConfigBuilder().include(pravegaProperties);\n+\n+        // Command under test\n+        TestUtils.executeCommand(\"storage DurableLog-recovery\", STATE.get());\n+\n+        // Start a new segment store and controller\n+        this.factory = new BookKeeperLogFactory(pravegaRunner.bookKeeperRunner.bkConfig.get(), pravegaRunner.bookKeeperRunner.zkClient.get(),\n+                executor);\n+        pravegaRunner.restartControllerAndSegmentStore(this.storageFactory, this.factory);\n+        log.info(\"Started a controller and segment store.\");\n+        // Create the client with new controller.\n+        try (val clientRunner = new ClientRunner(pravegaRunner.controllerRunner)) {\n+            // Try reading all events to verify that the recovery was successful.\n+            readAllEvents(streamName, clientRunner.clientFactory, clientRunner.readerGroupManager, \"RG\", \"R\");\n+            log.info(\"Read all events again to verify that segments were recovered.\");\n+        }\n+        Assert.assertNotNull(StorageListSegmentsCommand.descriptor());\n+    }\n+\n+    /**\n+     * Tests list segments command.\n+     * @throws Exception    In case of any exception thrown while execution.\n+     */\n+    @Test\n+    public void testListSegmentsCommand() throws Exception {\n+        int instanceId = 0;\n+        int bookieCount = 3;\n+        int containerCount = 1;\n+        @Cleanup\n+        PravegaRunner pravegaRunner = new PravegaRunner(instanceId++, bookieCount, containerCount, this.storageFactory);\n+        String streamName = \"testListSegmentsCommand\";\n+\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, streamName);\n+        try (val clientRunner = new ClientRunner(pravegaRunner.controllerRunner)) {\n+            // Write events to the streams.\n+            writeEvents(streamName, clientRunner.clientFactory);\n+        }\n+        pravegaRunner.controllerRunner.close(); // Shut down the controller\n+\n+        // Flush all Tier 1 to LTS\n+        ServiceBuilder.ComponentSetup componentSetup = new ServiceBuilder.ComponentSetup(pravegaRunner.segmentStoreRunner.serviceBuilder);\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            componentSetup.getContainerRegistry().getContainer(containerId).flushToStorage(TIMEOUT).join();\n+        }\n+\n+        pravegaRunner.segmentStoreRunner.close(); // Shutdown SegmentStore\n+        pravegaRunner.bookKeeperRunner.close(); // Shutdown BookKeeper & ZooKeeper\n+\n+        // set pravega properties for the test\n+        STATE.set(new AdminCommandState());\n+        Properties pravegaProperties = new Properties();\n+        pravegaProperties.setProperty(\"pravegaservice.container.count\", \"1\");\n+        pravegaProperties.setProperty(\"pravegaservice.storage.impl.name\", \"FILESYSTEM\");\n+        pravegaProperties.setProperty(\"pravegaservice.storage.layout\", \"ROLLING_STORAGE\");\n+        pravegaProperties.setProperty(\"filesystem.root\", this.baseDir.getAbsolutePath());\n+        STATE.get().getConfigBuilder().include(pravegaProperties);\n+\n+        // Execute the command for list segments\n+        TestUtils.executeCommand(\"storage list-segments \" + this.logsDir.getAbsolutePath(), STATE.get());\n+        // There should be a csv file created for storing segments in Container 0\n+        Assert.assertTrue(new File(this.logsDir.getAbsolutePath(), \"Container_0.csv\").exists());\n+        // Check if the file has segments listed in it\n+        Path path = Paths.get(this.logsDir.getAbsolutePath() + \"/Container_0.csv\");\n+        long lines = Files.lines(path).count();\n+        AssertExtensions.assertGreaterThan(\"There should be at least one segment.\", 1, lines);\n+        Assert.assertNotNull(StorageListSegmentsCommand.descriptor());\n+    }\n+\n+    @After\n+    public void tearDown() throws IOException {\n+        STATE.get().close();\n+        if (this.factory != null) {\n+            this.factory.close();\n+        }\n+        FileHelpers.deleteFileOrDirectory(this.baseDir);\n+        FileHelpers.deleteFileOrDirectory(this.logsDir);\n+        ExecutorServiceHelpers.shutdown(Duration.ofSeconds(2), executor);\n+    }\n+\n+    // write events to the given stream\n+    private void writeEvents(String streamName, ClientFactoryImpl clientFactory) {\n+        EventStreamWriter<String> writer = clientFactory.createEventWriter(streamName,", "originalCommit": "0a79b063b0ce3a48135dd878c4ade2bec390c6a6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg5NTYzNQ==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r538895635", "bodyText": "Same here", "author": "andreipaduroiu", "createdAt": "2020-12-08T23:47:21Z", "path": "cli/admin/src/test/java/io/pravega/cli/admin/dataRecovery/DataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,490 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.AdminCommandState;\n+import io.pravega.cli.admin.utils.TestUtils;\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.connection.impl.ConnectionFactory;\n+import io.pravega.client.connection.impl.ConnectionPool;\n+import io.pravega.client.connection.impl.ConnectionPoolImpl;\n+import io.pravega.client.connection.impl.SocketConnectionFactoryImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.tables.TableStore;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.store.ServiceConfig;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.AssertExtensions;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Properties;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+/**\n+ * Tests Data recovery commands.\n+ */\n+@Slf4j\n+public class DataRecoveryTest {\n+    private static final Duration TIMEOUT = Duration.ofMillis(30 * 1000);\n+    private static final int NUM_EVENTS = 10;\n+    private static final String EVENT = \"12345\";\n+    private static final String SCOPE = \"testScope\";\n+    // Setup utility.\n+    private static final Duration READ_TIMEOUT = Duration.ofMillis(1000);\n+    private static final AtomicReference<AdminCommandState> STATE = new AtomicReference<>();\n+\n+    @Rule\n+    public final Timeout globalTimeout = new Timeout(120, TimeUnit.SECONDS);\n+\n+    private final ScheduledExecutorService executor = ExecutorServiceHelpers.newScheduledThreadPool(10, \"Data recovery test pool\");\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    /**\n+     * A directory for FILESYSTEM storage as LTS.\n+     */\n+    private File baseDir = null;\n+    private FileSystemStorageConfig adapterConfig;\n+    private StorageFactory storageFactory = null;\n+\n+    /**\n+     * A directory for storing logs and CSV files generated during the test..\n+     */\n+    private File logsDir = null;\n+    private BookKeeperLogFactory factory = null;\n+\n+    @Before\n+    public void setUp() throws Exception {\n+        this.baseDir = Files.createTempDirectory(\"TestDataRecovery\").toFile().getAbsoluteFile();\n+        this.logsDir = Files.createTempDirectory(\"DataRecovery\").toFile().getAbsoluteFile();\n+        this.adapterConfig = FileSystemStorageConfig.builder()\n+                .with(FileSystemStorageConfig.ROOT, this.baseDir.getAbsolutePath())\n+                .with(FileSystemStorageConfig.REPLACE_ENABLED, true)\n+                .build();\n+\n+        this.storageFactory = new FileSystemStorageFactory(adapterConfig, this.executor);\n+    }\n+\n+    // Creates the given scope and stream using the given controller instance.\n+    private void createScopeStream(Controller controller, String scopeName, String streamName) {\n+        ClientConfig clientConfig = ClientConfig.builder().build();\n+        try (ConnectionPool cp = new ConnectionPoolImpl(clientConfig, new SocketConnectionFactoryImpl(clientConfig));\n+             StreamManager streamManager = new StreamManagerImpl(controller, cp)) {\n+            //create scope\n+            Boolean createScopeStatus = streamManager.createScope(scopeName);\n+            log.info(\"Create scope status {}\", createScopeStatus);\n+            //create stream\n+            Boolean createStreamStatus = streamManager.createStream(scopeName, streamName, config);\n+            log.info(\"Create stream status {}\", createStreamStatus);\n+        }\n+    }\n+\n+    /**\n+     * Tests DurableLog recovery command.\n+     * @throws Exception    In case of any exception thrown while execution.\n+     */\n+    @Test\n+    public void testDataRecoveryCommand() throws Exception {\n+        int instanceId = 0;\n+        int bookieCount = 3;\n+        int containerCount = 1;\n+        @Cleanup\n+        PravegaRunner pravegaRunner = new PravegaRunner(instanceId++, bookieCount, containerCount, this.storageFactory);\n+        String streamName = \"testDataRecoveryCommand\";\n+\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, streamName);\n+        try (val clientRunner = new ClientRunner(pravegaRunner.controllerRunner)) {\n+            // Write events to the streams.\n+            writeEvents(streamName, clientRunner.clientFactory);\n+        }\n+        pravegaRunner.controllerRunner.close(); // Shut down the controller\n+\n+        // Flush all Tier 1 to LTS\n+        ServiceBuilder.ComponentSetup componentSetup = new ServiceBuilder.ComponentSetup(pravegaRunner.segmentStoreRunner.serviceBuilder);\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            componentSetup.getContainerRegistry().getContainer(containerId).flushToStorage(TIMEOUT).join();\n+        }\n+\n+        pravegaRunner.segmentStoreRunner.close(); // Shutdown SegmentStore\n+        pravegaRunner.bookKeeperRunner.close(); // Shutdown BookKeeper & ZooKeeper\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        pravegaRunner.bookKeeperRunner = new BookKeeperRunner(instanceId++, bookieCount);\n+\n+        // set pravega properties for the test\n+        STATE.set(new AdminCommandState());\n+        Properties pravegaProperties = new Properties();\n+        pravegaProperties.setProperty(\"pravegaservice.container.count\", \"1\");\n+        pravegaProperties.setProperty(\"pravegaservice.storage.impl.name\", \"FILESYSTEM\");\n+        pravegaProperties.setProperty(\"pravegaservice.storage.layout\", \"ROLLING_STORAGE\");\n+        pravegaProperties.setProperty(\"filesystem.root\", this.baseDir.getAbsolutePath());\n+        pravegaProperties.setProperty(\"pravegaservice.zk.connect.uri\", \"localhost:\" + pravegaRunner.bookKeeperRunner.bkPort);\n+        pravegaProperties.setProperty(\"bookkeeper.ledger.path\", pravegaRunner.bookKeeperRunner.ledgerPath);\n+        pravegaProperties.setProperty(\"bookkeeper.zk.metadata.path\", pravegaRunner.bookKeeperRunner.logMetaNamespace);\n+        pravegaProperties.setProperty(\"pravegaservice.clusterName\", pravegaRunner.bookKeeperRunner.baseNamespace);\n+        STATE.get().getConfigBuilder().include(pravegaProperties);\n+\n+        // Command under test\n+        TestUtils.executeCommand(\"storage DurableLog-recovery\", STATE.get());\n+\n+        // Start a new segment store and controller\n+        this.factory = new BookKeeperLogFactory(pravegaRunner.bookKeeperRunner.bkConfig.get(), pravegaRunner.bookKeeperRunner.zkClient.get(),\n+                executor);\n+        pravegaRunner.restartControllerAndSegmentStore(this.storageFactory, this.factory);\n+        log.info(\"Started a controller and segment store.\");\n+        // Create the client with new controller.\n+        try (val clientRunner = new ClientRunner(pravegaRunner.controllerRunner)) {\n+            // Try reading all events to verify that the recovery was successful.\n+            readAllEvents(streamName, clientRunner.clientFactory, clientRunner.readerGroupManager, \"RG\", \"R\");\n+            log.info(\"Read all events again to verify that segments were recovered.\");\n+        }\n+        Assert.assertNotNull(StorageListSegmentsCommand.descriptor());\n+    }\n+\n+    /**\n+     * Tests list segments command.\n+     * @throws Exception    In case of any exception thrown while execution.\n+     */\n+    @Test\n+    public void testListSegmentsCommand() throws Exception {\n+        int instanceId = 0;\n+        int bookieCount = 3;\n+        int containerCount = 1;\n+        @Cleanup\n+        PravegaRunner pravegaRunner = new PravegaRunner(instanceId++, bookieCount, containerCount, this.storageFactory);\n+        String streamName = \"testListSegmentsCommand\";\n+\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, streamName);\n+        try (val clientRunner = new ClientRunner(pravegaRunner.controllerRunner)) {\n+            // Write events to the streams.\n+            writeEvents(streamName, clientRunner.clientFactory);\n+        }\n+        pravegaRunner.controllerRunner.close(); // Shut down the controller\n+\n+        // Flush all Tier 1 to LTS\n+        ServiceBuilder.ComponentSetup componentSetup = new ServiceBuilder.ComponentSetup(pravegaRunner.segmentStoreRunner.serviceBuilder);\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            componentSetup.getContainerRegistry().getContainer(containerId).flushToStorage(TIMEOUT).join();\n+        }\n+\n+        pravegaRunner.segmentStoreRunner.close(); // Shutdown SegmentStore\n+        pravegaRunner.bookKeeperRunner.close(); // Shutdown BookKeeper & ZooKeeper\n+\n+        // set pravega properties for the test\n+        STATE.set(new AdminCommandState());\n+        Properties pravegaProperties = new Properties();\n+        pravegaProperties.setProperty(\"pravegaservice.container.count\", \"1\");\n+        pravegaProperties.setProperty(\"pravegaservice.storage.impl.name\", \"FILESYSTEM\");\n+        pravegaProperties.setProperty(\"pravegaservice.storage.layout\", \"ROLLING_STORAGE\");\n+        pravegaProperties.setProperty(\"filesystem.root\", this.baseDir.getAbsolutePath());\n+        STATE.get().getConfigBuilder().include(pravegaProperties);\n+\n+        // Execute the command for list segments\n+        TestUtils.executeCommand(\"storage list-segments \" + this.logsDir.getAbsolutePath(), STATE.get());\n+        // There should be a csv file created for storing segments in Container 0\n+        Assert.assertTrue(new File(this.logsDir.getAbsolutePath(), \"Container_0.csv\").exists());\n+        // Check if the file has segments listed in it\n+        Path path = Paths.get(this.logsDir.getAbsolutePath() + \"/Container_0.csv\");\n+        long lines = Files.lines(path).count();\n+        AssertExtensions.assertGreaterThan(\"There should be at least one segment.\", 1, lines);\n+        Assert.assertNotNull(StorageListSegmentsCommand.descriptor());\n+    }\n+\n+    @After\n+    public void tearDown() throws IOException {\n+        STATE.get().close();\n+        if (this.factory != null) {\n+            this.factory.close();\n+        }\n+        FileHelpers.deleteFileOrDirectory(this.baseDir);\n+        FileHelpers.deleteFileOrDirectory(this.logsDir);\n+        ExecutorServiceHelpers.shutdown(Duration.ofSeconds(2), executor);\n+    }\n+\n+    // write events to the given stream\n+    private void writeEvents(String streamName, ClientFactoryImpl clientFactory) {\n+        EventStreamWriter<String> writer = clientFactory.createEventWriter(streamName,\n+                new UTF8StringSerializer(),\n+                EventWriterConfig.builder().build());\n+        for (int i = 0; i < NUM_EVENTS;) {\n+            writer.writeEvent(\"\", EVENT).join();\n+            i++;\n+        }\n+        writer.flush();\n+        writer.close();\n+    }\n+\n+    // read all events from the given stream\n+    private void readAllEvents(String streamName, ClientFactoryImpl clientFactory, ReaderGroupManager readerGroupManager,\n+                               String readerGroupName, String readerName) {\n+        readerGroupManager.createReaderGroup(readerGroupName,\n+                ReaderGroupConfig\n+                        .builder()\n+                        .stream(Stream.of(SCOPE, streamName))\n+                        .build());\n+\n+        EventStreamReader<String> reader = clientFactory.createReader(readerName,", "originalCommit": "0a79b063b0ce3a48135dd878c4ade2bec390c6a6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg5NjI3OQ==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r538896279", "bodyText": "This one can be made final.", "author": "andreipaduroiu", "createdAt": "2020-12-08T23:49:00Z", "path": "cli/admin/src/test/java/io/pravega/cli/admin/dataRecovery/DataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,490 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.AdminCommandState;\n+import io.pravega.cli.admin.utils.TestUtils;\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.connection.impl.ConnectionFactory;\n+import io.pravega.client.connection.impl.ConnectionPool;\n+import io.pravega.client.connection.impl.ConnectionPoolImpl;\n+import io.pravega.client.connection.impl.SocketConnectionFactoryImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.tables.TableStore;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.store.ServiceConfig;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.AssertExtensions;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Properties;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+/**\n+ * Tests Data recovery commands.\n+ */\n+@Slf4j\n+public class DataRecoveryTest {\n+    private static final Duration TIMEOUT = Duration.ofMillis(30 * 1000);\n+    private static final int NUM_EVENTS = 10;\n+    private static final String EVENT = \"12345\";\n+    private static final String SCOPE = \"testScope\";\n+    // Setup utility.\n+    private static final Duration READ_TIMEOUT = Duration.ofMillis(1000);\n+    private static final AtomicReference<AdminCommandState> STATE = new AtomicReference<>();\n+\n+    @Rule\n+    public final Timeout globalTimeout = new Timeout(120, TimeUnit.SECONDS);\n+\n+    private final ScheduledExecutorService executor = ExecutorServiceHelpers.newScheduledThreadPool(10, \"Data recovery test pool\");\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    /**\n+     * A directory for FILESYSTEM storage as LTS.\n+     */\n+    private File baseDir = null;\n+    private FileSystemStorageConfig adapterConfig;\n+    private StorageFactory storageFactory = null;\n+\n+    /**\n+     * A directory for storing logs and CSV files generated during the test..\n+     */\n+    private File logsDir = null;\n+    private BookKeeperLogFactory factory = null;\n+\n+    @Before\n+    public void setUp() throws Exception {\n+        this.baseDir = Files.createTempDirectory(\"TestDataRecovery\").toFile().getAbsoluteFile();\n+        this.logsDir = Files.createTempDirectory(\"DataRecovery\").toFile().getAbsoluteFile();\n+        this.adapterConfig = FileSystemStorageConfig.builder()\n+                .with(FileSystemStorageConfig.ROOT, this.baseDir.getAbsolutePath())\n+                .with(FileSystemStorageConfig.REPLACE_ENABLED, true)\n+                .build();\n+\n+        this.storageFactory = new FileSystemStorageFactory(adapterConfig, this.executor);\n+    }\n+\n+    // Creates the given scope and stream using the given controller instance.\n+    private void createScopeStream(Controller controller, String scopeName, String streamName) {\n+        ClientConfig clientConfig = ClientConfig.builder().build();\n+        try (ConnectionPool cp = new ConnectionPoolImpl(clientConfig, new SocketConnectionFactoryImpl(clientConfig));\n+             StreamManager streamManager = new StreamManagerImpl(controller, cp)) {\n+            //create scope\n+            Boolean createScopeStatus = streamManager.createScope(scopeName);\n+            log.info(\"Create scope status {}\", createScopeStatus);\n+            //create stream\n+            Boolean createStreamStatus = streamManager.createStream(scopeName, streamName, config);\n+            log.info(\"Create stream status {}\", createStreamStatus);\n+        }\n+    }\n+\n+    /**\n+     * Tests DurableLog recovery command.\n+     * @throws Exception    In case of any exception thrown while execution.\n+     */\n+    @Test\n+    public void testDataRecoveryCommand() throws Exception {\n+        int instanceId = 0;\n+        int bookieCount = 3;\n+        int containerCount = 1;\n+        @Cleanup\n+        PravegaRunner pravegaRunner = new PravegaRunner(instanceId++, bookieCount, containerCount, this.storageFactory);\n+        String streamName = \"testDataRecoveryCommand\";\n+\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, streamName);\n+        try (val clientRunner = new ClientRunner(pravegaRunner.controllerRunner)) {\n+            // Write events to the streams.\n+            writeEvents(streamName, clientRunner.clientFactory);\n+        }\n+        pravegaRunner.controllerRunner.close(); // Shut down the controller\n+\n+        // Flush all Tier 1 to LTS\n+        ServiceBuilder.ComponentSetup componentSetup = new ServiceBuilder.ComponentSetup(pravegaRunner.segmentStoreRunner.serviceBuilder);\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            componentSetup.getContainerRegistry().getContainer(containerId).flushToStorage(TIMEOUT).join();\n+        }\n+\n+        pravegaRunner.segmentStoreRunner.close(); // Shutdown SegmentStore\n+        pravegaRunner.bookKeeperRunner.close(); // Shutdown BookKeeper & ZooKeeper\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        pravegaRunner.bookKeeperRunner = new BookKeeperRunner(instanceId++, bookieCount);\n+\n+        // set pravega properties for the test\n+        STATE.set(new AdminCommandState());\n+        Properties pravegaProperties = new Properties();\n+        pravegaProperties.setProperty(\"pravegaservice.container.count\", \"1\");\n+        pravegaProperties.setProperty(\"pravegaservice.storage.impl.name\", \"FILESYSTEM\");\n+        pravegaProperties.setProperty(\"pravegaservice.storage.layout\", \"ROLLING_STORAGE\");\n+        pravegaProperties.setProperty(\"filesystem.root\", this.baseDir.getAbsolutePath());\n+        pravegaProperties.setProperty(\"pravegaservice.zk.connect.uri\", \"localhost:\" + pravegaRunner.bookKeeperRunner.bkPort);\n+        pravegaProperties.setProperty(\"bookkeeper.ledger.path\", pravegaRunner.bookKeeperRunner.ledgerPath);\n+        pravegaProperties.setProperty(\"bookkeeper.zk.metadata.path\", pravegaRunner.bookKeeperRunner.logMetaNamespace);\n+        pravegaProperties.setProperty(\"pravegaservice.clusterName\", pravegaRunner.bookKeeperRunner.baseNamespace);\n+        STATE.get().getConfigBuilder().include(pravegaProperties);\n+\n+        // Command under test\n+        TestUtils.executeCommand(\"storage DurableLog-recovery\", STATE.get());\n+\n+        // Start a new segment store and controller\n+        this.factory = new BookKeeperLogFactory(pravegaRunner.bookKeeperRunner.bkConfig.get(), pravegaRunner.bookKeeperRunner.zkClient.get(),\n+                executor);\n+        pravegaRunner.restartControllerAndSegmentStore(this.storageFactory, this.factory);\n+        log.info(\"Started a controller and segment store.\");\n+        // Create the client with new controller.\n+        try (val clientRunner = new ClientRunner(pravegaRunner.controllerRunner)) {\n+            // Try reading all events to verify that the recovery was successful.\n+            readAllEvents(streamName, clientRunner.clientFactory, clientRunner.readerGroupManager, \"RG\", \"R\");\n+            log.info(\"Read all events again to verify that segments were recovered.\");\n+        }\n+        Assert.assertNotNull(StorageListSegmentsCommand.descriptor());\n+    }\n+\n+    /**\n+     * Tests list segments command.\n+     * @throws Exception    In case of any exception thrown while execution.\n+     */\n+    @Test\n+    public void testListSegmentsCommand() throws Exception {\n+        int instanceId = 0;\n+        int bookieCount = 3;\n+        int containerCount = 1;\n+        @Cleanup\n+        PravegaRunner pravegaRunner = new PravegaRunner(instanceId++, bookieCount, containerCount, this.storageFactory);\n+        String streamName = \"testListSegmentsCommand\";\n+\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, streamName);\n+        try (val clientRunner = new ClientRunner(pravegaRunner.controllerRunner)) {\n+            // Write events to the streams.\n+            writeEvents(streamName, clientRunner.clientFactory);\n+        }\n+        pravegaRunner.controllerRunner.close(); // Shut down the controller\n+\n+        // Flush all Tier 1 to LTS\n+        ServiceBuilder.ComponentSetup componentSetup = new ServiceBuilder.ComponentSetup(pravegaRunner.segmentStoreRunner.serviceBuilder);\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            componentSetup.getContainerRegistry().getContainer(containerId).flushToStorage(TIMEOUT).join();\n+        }\n+\n+        pravegaRunner.segmentStoreRunner.close(); // Shutdown SegmentStore\n+        pravegaRunner.bookKeeperRunner.close(); // Shutdown BookKeeper & ZooKeeper\n+\n+        // set pravega properties for the test\n+        STATE.set(new AdminCommandState());\n+        Properties pravegaProperties = new Properties();\n+        pravegaProperties.setProperty(\"pravegaservice.container.count\", \"1\");\n+        pravegaProperties.setProperty(\"pravegaservice.storage.impl.name\", \"FILESYSTEM\");\n+        pravegaProperties.setProperty(\"pravegaservice.storage.layout\", \"ROLLING_STORAGE\");\n+        pravegaProperties.setProperty(\"filesystem.root\", this.baseDir.getAbsolutePath());\n+        STATE.get().getConfigBuilder().include(pravegaProperties);\n+\n+        // Execute the command for list segments\n+        TestUtils.executeCommand(\"storage list-segments \" + this.logsDir.getAbsolutePath(), STATE.get());\n+        // There should be a csv file created for storing segments in Container 0\n+        Assert.assertTrue(new File(this.logsDir.getAbsolutePath(), \"Container_0.csv\").exists());\n+        // Check if the file has segments listed in it\n+        Path path = Paths.get(this.logsDir.getAbsolutePath() + \"/Container_0.csv\");\n+        long lines = Files.lines(path).count();\n+        AssertExtensions.assertGreaterThan(\"There should be at least one segment.\", 1, lines);\n+        Assert.assertNotNull(StorageListSegmentsCommand.descriptor());\n+    }\n+\n+    @After\n+    public void tearDown() throws IOException {\n+        STATE.get().close();\n+        if (this.factory != null) {\n+            this.factory.close();\n+        }\n+        FileHelpers.deleteFileOrDirectory(this.baseDir);\n+        FileHelpers.deleteFileOrDirectory(this.logsDir);\n+        ExecutorServiceHelpers.shutdown(Duration.ofSeconds(2), executor);\n+    }\n+\n+    // write events to the given stream\n+    private void writeEvents(String streamName, ClientFactoryImpl clientFactory) {\n+        EventStreamWriter<String> writer = clientFactory.createEventWriter(streamName,\n+                new UTF8StringSerializer(),\n+                EventWriterConfig.builder().build());\n+        for (int i = 0; i < NUM_EVENTS;) {\n+            writer.writeEvent(\"\", EVENT).join();\n+            i++;\n+        }\n+        writer.flush();\n+        writer.close();\n+    }\n+\n+    // read all events from the given stream\n+    private void readAllEvents(String streamName, ClientFactoryImpl clientFactory, ReaderGroupManager readerGroupManager,\n+                               String readerGroupName, String readerName) {\n+        readerGroupManager.createReaderGroup(readerGroupName,\n+                ReaderGroupConfig\n+                        .builder()\n+                        .stream(Stream.of(SCOPE, streamName))\n+                        .build());\n+\n+        EventStreamReader<String> reader = clientFactory.createReader(readerName,\n+                readerGroupName,\n+                new UTF8StringSerializer(),\n+                ReaderConfig.builder().build());\n+\n+        for (int q = 0; q < NUM_EVENTS;) {\n+            String eventRead = reader.readNextEvent(READ_TIMEOUT.toMillis()).getEvent();\n+            Assert.assertEquals(\"Event written and read back don't match\", EVENT, eventRead);\n+            q++;\n+        }\n+        reader.close();\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BookKeeperRunner implements AutoCloseable {\n+        private final int bkPort;\n+        private final BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private final AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private final AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private final AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private final String ledgerPath;\n+        private final String logMetaNamespace;\n+        private final String baseNamespace;\n+        BookKeeperRunner(int instanceId, int bookieCount) throws Exception {\n+            this.ledgerPath = \"/pravega/bookkeeper/ledgers\" + instanceId;\n+            this.bkPort = io.pravega.test.common.TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(io.pravega.test.common.TestUtils.getAvailableListenPort());\n+            }\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(ledgerPath)\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            try {\n+                this.bookKeeperServiceRunner.startAll();\n+            } catch (Exception e) {\n+                log.error(\"Exception occurred while starting bookKeeper service.\", e);\n+                this.close();\n+                throw e;\n+            }\n+            this.bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            this.baseNamespace = \"pravega\" + instanceId;\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 10))\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, ledgerPath)\n+                    .build());\n+        }\n+\n+        @Override\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Creates a segment store.\n+     */\n+    private static class SegmentStoreRunner implements AutoCloseable {\n+        private final int servicePort = io.pravega.test.common.TestUtils.getAvailableListenPort();\n+        private final ServiceBuilder serviceBuilder;\n+        private final PravegaConnectionListener server;\n+        private final StreamSegmentStore streamSegmentStore;\n+        private final TableStore tableStore;\n+\n+        SegmentStoreRunner(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory, int containerCount)\n+                throws DurableDataLogException {\n+            ServiceBuilderConfig.Builder configBuilder = ServiceBuilderConfig\n+                    .builder()\n+                    .include(ServiceConfig.builder()\n+                            .with(ServiceConfig.CONTAINER_COUNT, containerCount))\n+                    .include(WriterConfig.builder()\n+                            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 100L)\n+                            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 500L)\n+                    );\n+            if (storageFactory != null) {\n+                if (dataLogFactory != null) {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(configBuilder.build())\n+                            .withStorageFactory(setup -> storageFactory)\n+                            .withDataLogFactory(setup -> dataLogFactory);\n+                } else {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(configBuilder.build())\n+                            .withStorageFactory(setup -> storageFactory);\n+                }\n+            } else {\n+                this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n+            }\n+            this.serviceBuilder.initialize();\n+            this.streamSegmentStore = this.serviceBuilder.createStreamSegmentService();\n+            this.tableStore = this.serviceBuilder.createTableStoreService();\n+            this.server = new PravegaConnectionListener(false, servicePort, this.streamSegmentStore, this.tableStore,\n+                    this.serviceBuilder.getLowPriorityExecutor());\n+            this.server.startListening();\n+        }\n+\n+        @Override\n+        public void close() {\n+            this.server.close();\n+            this.serviceBuilder.close();\n+        }\n+    }\n+\n+    /**\n+     * Creates a controller instance and runs it.\n+     */\n+    private static class ControllerRunner implements AutoCloseable {\n+        private final int controllerPort = io.pravega.test.common.TestUtils.getAvailableListenPort();\n+        private final String serviceHost = \"localhost\";\n+        private final ControllerWrapper controllerWrapper;\n+        private final Controller controller;\n+        private final URI controllerURI = URI.create(\"tcp://\" + serviceHost + \":\" + controllerPort);\n+\n+        ControllerRunner(int bkPort, int servicePort, int containerCount) throws InterruptedException {\n+            this.controllerWrapper = new ControllerWrapper(\"localhost:\" + bkPort, false,\n+                    controllerPort, serviceHost, servicePort, containerCount);\n+            this.controllerWrapper.awaitRunning();\n+            this.controller = controllerWrapper.getController();\n+        }\n+\n+        @Override\n+        public void close() throws Exception {\n+            this.controller.close();\n+            this.controllerWrapper.close();\n+        }\n+    }\n+\n+    /**\n+     * Creates a client to read and write events.\n+     */\n+    private static class ClientRunner implements AutoCloseable {\n+        private final ConnectionFactory connectionFactory;\n+        private final ClientFactoryImpl clientFactory;\n+        private final ReaderGroupManager readerGroupManager;\n+\n+        ClientRunner(ControllerRunner controllerRunner) {\n+            this.connectionFactory = new SocketConnectionFactoryImpl(ClientConfig.builder()\n+                    .controllerURI(controllerRunner.controllerURI).build());\n+            this.clientFactory = new ClientFactoryImpl(SCOPE, controllerRunner.controller, connectionFactory);\n+            this.readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerRunner.controller, clientFactory);\n+        }\n+\n+        @Override\n+        public void close() {\n+            this.readerGroupManager.close();\n+            this.clientFactory.close();\n+            this.connectionFactory.close();\n+        }\n+    }\n+\n+    /**\n+     * Creates a Pravega instance.\n+     */\n+    private static class PravegaRunner implements AutoCloseable {\n+        private final int containerCount;\n+        private BookKeeperRunner bookKeeperRunner;", "originalCommit": "0a79b063b0ce3a48135dd878c4ade2bec390c6a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTEyNTkyOQ==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r539125929", "bodyText": "No. It can't be. I am reassigning it to clear zk/bk and then start a new one.", "author": "ManishKumarKeshri", "createdAt": "2020-12-09T09:02:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg5NjI3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTY4NDgxNw==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r539684817", "bodyText": "This is a real bad practice. If you let an external class tinker with your private fields, then you are essentially breaking encapsulation, which can lead to a lot of bugs.\nI suggest you make a method on this class which will properly close the existing runner and reassign it to a new one.\nThat way nobody else will need to know to close your instances or whatever. See below. You are closing your other runners here and also in the test above. How can you track that? If you want to debug something it will be next to impossible to figure out your program flow.\nPlease respect the encapsulation rules and move all management of these runners within this class.", "author": "andreipaduroiu", "createdAt": "2020-12-09T22:08:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg5NjI3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTczNTA5MQ==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r539735091", "bodyText": "Thanks for letting me know. I made some changes on other parts as well, considering this.", "author": "ManishKumarKeshri", "createdAt": "2020-12-09T23:49:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg5NjI3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg5NjczMg==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r538896732", "bodyText": "Can this method be invoked multiple times? If not, then enforce it.\nIf it does, then you need to close whatever segmentStoreRunner and controllerRunner you had in your member variables before assigning them to new instances. Otherwise you'll have resource leaks.", "author": "andreipaduroiu", "createdAt": "2020-12-08T23:49:52Z", "path": "cli/admin/src/test/java/io/pravega/cli/admin/dataRecovery/DataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,490 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.AdminCommandState;\n+import io.pravega.cli.admin.utils.TestUtils;\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.connection.impl.ConnectionFactory;\n+import io.pravega.client.connection.impl.ConnectionPool;\n+import io.pravega.client.connection.impl.ConnectionPoolImpl;\n+import io.pravega.client.connection.impl.SocketConnectionFactoryImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.tables.TableStore;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.store.ServiceConfig;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.AssertExtensions;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Properties;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+/**\n+ * Tests Data recovery commands.\n+ */\n+@Slf4j\n+public class DataRecoveryTest {\n+    private static final Duration TIMEOUT = Duration.ofMillis(30 * 1000);\n+    private static final int NUM_EVENTS = 10;\n+    private static final String EVENT = \"12345\";\n+    private static final String SCOPE = \"testScope\";\n+    // Setup utility.\n+    private static final Duration READ_TIMEOUT = Duration.ofMillis(1000);\n+    private static final AtomicReference<AdminCommandState> STATE = new AtomicReference<>();\n+\n+    @Rule\n+    public final Timeout globalTimeout = new Timeout(120, TimeUnit.SECONDS);\n+\n+    private final ScheduledExecutorService executor = ExecutorServiceHelpers.newScheduledThreadPool(10, \"Data recovery test pool\");\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    /**\n+     * A directory for FILESYSTEM storage as LTS.\n+     */\n+    private File baseDir = null;\n+    private FileSystemStorageConfig adapterConfig;\n+    private StorageFactory storageFactory = null;\n+\n+    /**\n+     * A directory for storing logs and CSV files generated during the test..\n+     */\n+    private File logsDir = null;\n+    private BookKeeperLogFactory factory = null;\n+\n+    @Before\n+    public void setUp() throws Exception {\n+        this.baseDir = Files.createTempDirectory(\"TestDataRecovery\").toFile().getAbsoluteFile();\n+        this.logsDir = Files.createTempDirectory(\"DataRecovery\").toFile().getAbsoluteFile();\n+        this.adapterConfig = FileSystemStorageConfig.builder()\n+                .with(FileSystemStorageConfig.ROOT, this.baseDir.getAbsolutePath())\n+                .with(FileSystemStorageConfig.REPLACE_ENABLED, true)\n+                .build();\n+\n+        this.storageFactory = new FileSystemStorageFactory(adapterConfig, this.executor);\n+    }\n+\n+    // Creates the given scope and stream using the given controller instance.\n+    private void createScopeStream(Controller controller, String scopeName, String streamName) {\n+        ClientConfig clientConfig = ClientConfig.builder().build();\n+        try (ConnectionPool cp = new ConnectionPoolImpl(clientConfig, new SocketConnectionFactoryImpl(clientConfig));\n+             StreamManager streamManager = new StreamManagerImpl(controller, cp)) {\n+            //create scope\n+            Boolean createScopeStatus = streamManager.createScope(scopeName);\n+            log.info(\"Create scope status {}\", createScopeStatus);\n+            //create stream\n+            Boolean createStreamStatus = streamManager.createStream(scopeName, streamName, config);\n+            log.info(\"Create stream status {}\", createStreamStatus);\n+        }\n+    }\n+\n+    /**\n+     * Tests DurableLog recovery command.\n+     * @throws Exception    In case of any exception thrown while execution.\n+     */\n+    @Test\n+    public void testDataRecoveryCommand() throws Exception {\n+        int instanceId = 0;\n+        int bookieCount = 3;\n+        int containerCount = 1;\n+        @Cleanup\n+        PravegaRunner pravegaRunner = new PravegaRunner(instanceId++, bookieCount, containerCount, this.storageFactory);\n+        String streamName = \"testDataRecoveryCommand\";\n+\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, streamName);\n+        try (val clientRunner = new ClientRunner(pravegaRunner.controllerRunner)) {\n+            // Write events to the streams.\n+            writeEvents(streamName, clientRunner.clientFactory);\n+        }\n+        pravegaRunner.controllerRunner.close(); // Shut down the controller\n+\n+        // Flush all Tier 1 to LTS\n+        ServiceBuilder.ComponentSetup componentSetup = new ServiceBuilder.ComponentSetup(pravegaRunner.segmentStoreRunner.serviceBuilder);\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            componentSetup.getContainerRegistry().getContainer(containerId).flushToStorage(TIMEOUT).join();\n+        }\n+\n+        pravegaRunner.segmentStoreRunner.close(); // Shutdown SegmentStore\n+        pravegaRunner.bookKeeperRunner.close(); // Shutdown BookKeeper & ZooKeeper\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        pravegaRunner.bookKeeperRunner = new BookKeeperRunner(instanceId++, bookieCount);\n+\n+        // set pravega properties for the test\n+        STATE.set(new AdminCommandState());\n+        Properties pravegaProperties = new Properties();\n+        pravegaProperties.setProperty(\"pravegaservice.container.count\", \"1\");\n+        pravegaProperties.setProperty(\"pravegaservice.storage.impl.name\", \"FILESYSTEM\");\n+        pravegaProperties.setProperty(\"pravegaservice.storage.layout\", \"ROLLING_STORAGE\");\n+        pravegaProperties.setProperty(\"filesystem.root\", this.baseDir.getAbsolutePath());\n+        pravegaProperties.setProperty(\"pravegaservice.zk.connect.uri\", \"localhost:\" + pravegaRunner.bookKeeperRunner.bkPort);\n+        pravegaProperties.setProperty(\"bookkeeper.ledger.path\", pravegaRunner.bookKeeperRunner.ledgerPath);\n+        pravegaProperties.setProperty(\"bookkeeper.zk.metadata.path\", pravegaRunner.bookKeeperRunner.logMetaNamespace);\n+        pravegaProperties.setProperty(\"pravegaservice.clusterName\", pravegaRunner.bookKeeperRunner.baseNamespace);\n+        STATE.get().getConfigBuilder().include(pravegaProperties);\n+\n+        // Command under test\n+        TestUtils.executeCommand(\"storage DurableLog-recovery\", STATE.get());\n+\n+        // Start a new segment store and controller\n+        this.factory = new BookKeeperLogFactory(pravegaRunner.bookKeeperRunner.bkConfig.get(), pravegaRunner.bookKeeperRunner.zkClient.get(),\n+                executor);\n+        pravegaRunner.restartControllerAndSegmentStore(this.storageFactory, this.factory);\n+        log.info(\"Started a controller and segment store.\");\n+        // Create the client with new controller.\n+        try (val clientRunner = new ClientRunner(pravegaRunner.controllerRunner)) {\n+            // Try reading all events to verify that the recovery was successful.\n+            readAllEvents(streamName, clientRunner.clientFactory, clientRunner.readerGroupManager, \"RG\", \"R\");\n+            log.info(\"Read all events again to verify that segments were recovered.\");\n+        }\n+        Assert.assertNotNull(StorageListSegmentsCommand.descriptor());\n+    }\n+\n+    /**\n+     * Tests list segments command.\n+     * @throws Exception    In case of any exception thrown while execution.\n+     */\n+    @Test\n+    public void testListSegmentsCommand() throws Exception {\n+        int instanceId = 0;\n+        int bookieCount = 3;\n+        int containerCount = 1;\n+        @Cleanup\n+        PravegaRunner pravegaRunner = new PravegaRunner(instanceId++, bookieCount, containerCount, this.storageFactory);\n+        String streamName = \"testListSegmentsCommand\";\n+\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, streamName);\n+        try (val clientRunner = new ClientRunner(pravegaRunner.controllerRunner)) {\n+            // Write events to the streams.\n+            writeEvents(streamName, clientRunner.clientFactory);\n+        }\n+        pravegaRunner.controllerRunner.close(); // Shut down the controller\n+\n+        // Flush all Tier 1 to LTS\n+        ServiceBuilder.ComponentSetup componentSetup = new ServiceBuilder.ComponentSetup(pravegaRunner.segmentStoreRunner.serviceBuilder);\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            componentSetup.getContainerRegistry().getContainer(containerId).flushToStorage(TIMEOUT).join();\n+        }\n+\n+        pravegaRunner.segmentStoreRunner.close(); // Shutdown SegmentStore\n+        pravegaRunner.bookKeeperRunner.close(); // Shutdown BookKeeper & ZooKeeper\n+\n+        // set pravega properties for the test\n+        STATE.set(new AdminCommandState());\n+        Properties pravegaProperties = new Properties();\n+        pravegaProperties.setProperty(\"pravegaservice.container.count\", \"1\");\n+        pravegaProperties.setProperty(\"pravegaservice.storage.impl.name\", \"FILESYSTEM\");\n+        pravegaProperties.setProperty(\"pravegaservice.storage.layout\", \"ROLLING_STORAGE\");\n+        pravegaProperties.setProperty(\"filesystem.root\", this.baseDir.getAbsolutePath());\n+        STATE.get().getConfigBuilder().include(pravegaProperties);\n+\n+        // Execute the command for list segments\n+        TestUtils.executeCommand(\"storage list-segments \" + this.logsDir.getAbsolutePath(), STATE.get());\n+        // There should be a csv file created for storing segments in Container 0\n+        Assert.assertTrue(new File(this.logsDir.getAbsolutePath(), \"Container_0.csv\").exists());\n+        // Check if the file has segments listed in it\n+        Path path = Paths.get(this.logsDir.getAbsolutePath() + \"/Container_0.csv\");\n+        long lines = Files.lines(path).count();\n+        AssertExtensions.assertGreaterThan(\"There should be at least one segment.\", 1, lines);\n+        Assert.assertNotNull(StorageListSegmentsCommand.descriptor());\n+    }\n+\n+    @After\n+    public void tearDown() throws IOException {\n+        STATE.get().close();\n+        if (this.factory != null) {\n+            this.factory.close();\n+        }\n+        FileHelpers.deleteFileOrDirectory(this.baseDir);\n+        FileHelpers.deleteFileOrDirectory(this.logsDir);\n+        ExecutorServiceHelpers.shutdown(Duration.ofSeconds(2), executor);\n+    }\n+\n+    // write events to the given stream\n+    private void writeEvents(String streamName, ClientFactoryImpl clientFactory) {\n+        EventStreamWriter<String> writer = clientFactory.createEventWriter(streamName,\n+                new UTF8StringSerializer(),\n+                EventWriterConfig.builder().build());\n+        for (int i = 0; i < NUM_EVENTS;) {\n+            writer.writeEvent(\"\", EVENT).join();\n+            i++;\n+        }\n+        writer.flush();\n+        writer.close();\n+    }\n+\n+    // read all events from the given stream\n+    private void readAllEvents(String streamName, ClientFactoryImpl clientFactory, ReaderGroupManager readerGroupManager,\n+                               String readerGroupName, String readerName) {\n+        readerGroupManager.createReaderGroup(readerGroupName,\n+                ReaderGroupConfig\n+                        .builder()\n+                        .stream(Stream.of(SCOPE, streamName))\n+                        .build());\n+\n+        EventStreamReader<String> reader = clientFactory.createReader(readerName,\n+                readerGroupName,\n+                new UTF8StringSerializer(),\n+                ReaderConfig.builder().build());\n+\n+        for (int q = 0; q < NUM_EVENTS;) {\n+            String eventRead = reader.readNextEvent(READ_TIMEOUT.toMillis()).getEvent();\n+            Assert.assertEquals(\"Event written and read back don't match\", EVENT, eventRead);\n+            q++;\n+        }\n+        reader.close();\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BookKeeperRunner implements AutoCloseable {\n+        private final int bkPort;\n+        private final BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private final AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private final AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private final AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private final String ledgerPath;\n+        private final String logMetaNamespace;\n+        private final String baseNamespace;\n+        BookKeeperRunner(int instanceId, int bookieCount) throws Exception {\n+            this.ledgerPath = \"/pravega/bookkeeper/ledgers\" + instanceId;\n+            this.bkPort = io.pravega.test.common.TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(io.pravega.test.common.TestUtils.getAvailableListenPort());\n+            }\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(ledgerPath)\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            try {\n+                this.bookKeeperServiceRunner.startAll();\n+            } catch (Exception e) {\n+                log.error(\"Exception occurred while starting bookKeeper service.\", e);\n+                this.close();\n+                throw e;\n+            }\n+            this.bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            this.baseNamespace = \"pravega\" + instanceId;\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 10))\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, ledgerPath)\n+                    .build());\n+        }\n+\n+        @Override\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Creates a segment store.\n+     */\n+    private static class SegmentStoreRunner implements AutoCloseable {\n+        private final int servicePort = io.pravega.test.common.TestUtils.getAvailableListenPort();\n+        private final ServiceBuilder serviceBuilder;\n+        private final PravegaConnectionListener server;\n+        private final StreamSegmentStore streamSegmentStore;\n+        private final TableStore tableStore;\n+\n+        SegmentStoreRunner(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory, int containerCount)\n+                throws DurableDataLogException {\n+            ServiceBuilderConfig.Builder configBuilder = ServiceBuilderConfig\n+                    .builder()\n+                    .include(ServiceConfig.builder()\n+                            .with(ServiceConfig.CONTAINER_COUNT, containerCount))\n+                    .include(WriterConfig.builder()\n+                            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 100L)\n+                            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 500L)\n+                    );\n+            if (storageFactory != null) {\n+                if (dataLogFactory != null) {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(configBuilder.build())\n+                            .withStorageFactory(setup -> storageFactory)\n+                            .withDataLogFactory(setup -> dataLogFactory);\n+                } else {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(configBuilder.build())\n+                            .withStorageFactory(setup -> storageFactory);\n+                }\n+            } else {\n+                this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n+            }\n+            this.serviceBuilder.initialize();\n+            this.streamSegmentStore = this.serviceBuilder.createStreamSegmentService();\n+            this.tableStore = this.serviceBuilder.createTableStoreService();\n+            this.server = new PravegaConnectionListener(false, servicePort, this.streamSegmentStore, this.tableStore,\n+                    this.serviceBuilder.getLowPriorityExecutor());\n+            this.server.startListening();\n+        }\n+\n+        @Override\n+        public void close() {\n+            this.server.close();\n+            this.serviceBuilder.close();\n+        }\n+    }\n+\n+    /**\n+     * Creates a controller instance and runs it.\n+     */\n+    private static class ControllerRunner implements AutoCloseable {\n+        private final int controllerPort = io.pravega.test.common.TestUtils.getAvailableListenPort();\n+        private final String serviceHost = \"localhost\";\n+        private final ControllerWrapper controllerWrapper;\n+        private final Controller controller;\n+        private final URI controllerURI = URI.create(\"tcp://\" + serviceHost + \":\" + controllerPort);\n+\n+        ControllerRunner(int bkPort, int servicePort, int containerCount) throws InterruptedException {\n+            this.controllerWrapper = new ControllerWrapper(\"localhost:\" + bkPort, false,\n+                    controllerPort, serviceHost, servicePort, containerCount);\n+            this.controllerWrapper.awaitRunning();\n+            this.controller = controllerWrapper.getController();\n+        }\n+\n+        @Override\n+        public void close() throws Exception {\n+            this.controller.close();\n+            this.controllerWrapper.close();\n+        }\n+    }\n+\n+    /**\n+     * Creates a client to read and write events.\n+     */\n+    private static class ClientRunner implements AutoCloseable {\n+        private final ConnectionFactory connectionFactory;\n+        private final ClientFactoryImpl clientFactory;\n+        private final ReaderGroupManager readerGroupManager;\n+\n+        ClientRunner(ControllerRunner controllerRunner) {\n+            this.connectionFactory = new SocketConnectionFactoryImpl(ClientConfig.builder()\n+                    .controllerURI(controllerRunner.controllerURI).build());\n+            this.clientFactory = new ClientFactoryImpl(SCOPE, controllerRunner.controller, connectionFactory);\n+            this.readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerRunner.controller, clientFactory);\n+        }\n+\n+        @Override\n+        public void close() {\n+            this.readerGroupManager.close();\n+            this.clientFactory.close();\n+            this.connectionFactory.close();\n+        }\n+    }\n+\n+    /**\n+     * Creates a Pravega instance.\n+     */\n+    private static class PravegaRunner implements AutoCloseable {\n+        private final int containerCount;\n+        private BookKeeperRunner bookKeeperRunner;\n+        private SegmentStoreRunner segmentStoreRunner;\n+        private ControllerRunner controllerRunner;\n+\n+        PravegaRunner(int instanceId, int bookieCount, int containerCount, StorageFactory storageFactory) throws Exception {\n+            this.containerCount = containerCount;\n+            this.bookKeeperRunner = new BookKeeperRunner(instanceId, bookieCount);\n+            restartControllerAndSegmentStore(storageFactory, null);\n+        }\n+\n+        public void restartControllerAndSegmentStore(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory)\n+                throws DurableDataLogException, InterruptedException {\n+            this.segmentStoreRunner = new SegmentStoreRunner(storageFactory, dataLogFactory, this.containerCount);", "originalCommit": "0a79b063b0ce3a48135dd878c4ade2bec390c6a6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTEyMzg4NA==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r539123884", "bodyText": "Already doing the second thing.", "author": "ManishKumarKeshri", "createdAt": "2020-12-09T08:59:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg5NjczMg=="}], "type": "inlineReview"}, {"oid": "62f2336396050361f306bd51fb879fa7cd55768b", "url": "https://github.com/pravega/pravega/commit/62f2336396050361f306bd51fb879fa7cd55768b", "message": "Fixing comments.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-12-09T09:24:15Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTY4MDQxMA==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r539680410", "bodyText": "Why do you have 2 ContainerConfigs in here ? Which one do you use?", "author": "andreipaduroiu", "createdAt": "2020-12-09T22:00:37Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/DurableLogRecoveryCommand.java", "diffHunk": "@@ -0,0 +1,246 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.ContainerRecoveryUtils;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import lombok.Cleanup;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+\n+/**\n+ * Loads the storage instance, recovers all segments from there.\n+ */\n+public class DurableLogRecoveryCommand extends DataRecoveryCommand implements AutoCloseable {\n+    private static final int CONTAINER_EPOCH = 1;\n+    private static final Duration TIMEOUT = Duration.ofMillis(100 * 1000);\n+\n+    private static final DurableLogConfig NO_TRUNCATIONS_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 10000)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 50000)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 1024 * 1024 * 1024L)\n+            .build();\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig.builder().build();\n+\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig", "originalCommit": "62f2336396050361f306bd51fb879fa7cd55768b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTczMjkwOA==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r539732908", "bodyText": "The first one was used in the second. I realized that it's not required, so removed it. Now, using only one.", "author": "ManishKumarKeshri", "createdAt": "2020-12-09T23:44:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTY4MDQxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTY4MjI1MA==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r539682250", "bodyText": "FYI you can just do for(FileWriter w : this.csvWriters) instead of this loop. It's cleaner.\nIf you change it, do it in the close method too.", "author": "andreipaduroiu", "createdAt": "2020-12-09T22:04:11Z", "path": "cli/admin/src/main/java/io/pravega/cli/admin/dataRecovery/StorageListSegmentsCommand.java", "diffHunk": "@@ -0,0 +1,167 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.cli.admin.dataRecovery;\n+\n+import io.pravega.cli.admin.CommandArgs;\n+import io.pravega.common.concurrent.ExecutorServiceHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import org.apache.hadoop.fs.Path;\n+\n+import java.io.File;\n+import java.io.FileWriter;\n+import java.text.SimpleDateFormat;\n+import java.time.Duration;\n+import java.util.Arrays;\n+import java.util.Date;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+/**\n+ * Lists all non-shadow segments from there from the storage. The storage is loaded using the config properties.\n+ */\n+public class StorageListSegmentsCommand extends DataRecoveryCommand implements AutoCloseable {\n+    /**\n+     * Header line for writing segments' details to csv files.\n+     */\n+    private static final List<String> HEADER = Arrays.asList(\"Sealed Status\", \"Length\", \"Segment Name\");\n+    private static final int CONTAINER_EPOCH = 1;\n+    private final int containerCount;\n+    private final ScheduledExecutorService scheduledExecutorService = ExecutorServiceHelpers.newScheduledThreadPool(1,\n+            \"listSegmentsProcessor\");\n+    private final SegmentToContainerMapper segToConMapper;\n+    private final StorageFactory storageFactory;\n+    private final Storage storage;\n+    private final FileWriter[] csvWriters;\n+    private String filePath;\n+\n+    /**\n+     * Creates an instance of StorageListSegmentsCommand class.\n+     *\n+     * @param args The arguments for the command.\n+     */\n+    public StorageListSegmentsCommand(CommandArgs args) {\n+        super(args);\n+        this.containerCount = getServiceConfig().getContainerCount();\n+        this.segToConMapper = new SegmentToContainerMapper(this.containerCount);\n+        this.storageFactory = createStorageFactory(scheduledExecutorService);\n+        this.storage = this.storageFactory.createStorageAdapter();\n+        this.csvWriters = new FileWriter[this.containerCount];\n+    }\n+\n+    @Override\n+    public void close() throws Exception {\n+        for (int containerId = 0; containerId < this.containerCount; containerId++) {\n+            if (this.csvWriters[containerId] != null) {\n+                this.csvWriters[containerId].close();\n+            }\n+        }\n+        this.storage.close();\n+        ExecutorServiceHelpers.shutdown(Duration.ofSeconds(2), scheduledExecutorService);\n+    }\n+\n+    /**\n+     * Creates a csv file for each container. All segments belonging to a containerId have their details written to the\n+     * csv file for that container.\n+     *\n+     * @throws Exception   When failed to create/delete file(s).\n+     */\n+    private void createCSVFiles() throws Exception {\n+        String fileSuffix = new SimpleDateFormat(\"yyyyMMddHHmmss\").format(new Date());\n+\n+        // Set up directory for storing csv files\n+        this.filePath = System.getProperty(\"user.dir\") + Path.SEPARATOR + descriptor().getName() + \"_\" + fileSuffix;\n+\n+        // If path given as command args, use it\n+        if (getArgCount() >= 1) {\n+            this.filePath = getCommandArgs().getArgs().get(0);\n+            if (this.filePath.endsWith(Path.SEPARATOR)) {\n+                this.filePath = this.filePath.substring(0, this.filePath.length()-1);\n+            }\n+        }\n+\n+        // Create a directory for storing files.\n+        File dir = new File(this.filePath);\n+        if (!dir.exists()) {\n+            dir.mkdir();\n+        }\n+\n+        for (int containerId = 0; containerId < this.containerCount; containerId++) {\n+            File f = new File(this.filePath + Path.SEPARATOR + \"Container_\" + containerId + \".csv\");\n+            if (f.exists()) {\n+                outputInfo(\"File '%s' already exists.\", f.getAbsolutePath());\n+                if (!f.delete()) {\n+                    outputError(\"Failed to delete the file '%s'.\", f.getAbsolutePath());\n+                    throw new Exception(\"Failed to delete the file \" + f.getAbsolutePath());\n+                }\n+            }\n+            if (!f.createNewFile()) {\n+                outputError(\"Failed to create file '%s'.\", f.getAbsolutePath());\n+                throw new Exception(\"Failed to create file \" + f.getAbsolutePath());\n+            }\n+            this.csvWriters[containerId] = new FileWriter(f.getAbsolutePath());\n+            outputInfo(\"Created file '%s'\", f.getAbsolutePath());\n+            this.csvWriters[containerId].append(String.join(\",\", HEADER));\n+            this.csvWriters[containerId].append(\"\\n\");\n+        }\n+    }\n+\n+    @Override\n+    public void execute() throws Exception {\n+        outputInfo(\"Container Count = %d\", this.containerCount);\n+\n+        // Get the storage using the config.\n+        this.storage.initialize(CONTAINER_EPOCH);\n+        outputInfo(\"Loaded %s Storage.\", getServiceConfig().getStorageImplementation().toString());\n+\n+        // Gets total number of segments listed.\n+        int segmentsCount = 0;\n+\n+        // create CSV files for listing the segments and their details\n+        createCSVFiles();\n+\n+        outputInfo(\"Writing segments' details to the csv files...\");\n+        Iterator<SegmentProperties> segmentIterator = this.storage.listSegments();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip, if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            segmentsCount++;\n+            int containerId = this.segToConMapper.getContainerId(currentSegment.getName());\n+            outputInfo(containerId + \"\\t\" + currentSegment.isSealed() + \"\\t\" + currentSegment.getLength() + \"\\t\" +\n+                    currentSegment.getName());\n+            this.csvWriters[containerId].append(currentSegment.isSealed() + \",\" + currentSegment.getLength() + \",\" +\n+                    currentSegment.getName() + \"\\n\");\n+        }\n+\n+        outputInfo(\"Closing all csv files...\");\n+        for (int containerId = 0; containerId < this.containerCount; containerId++) {", "originalCommit": "62f2336396050361f306bd51fb879fa7cd55768b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTczMzA4MA==", "url": "https://github.com/pravega/pravega/pull/5371#discussion_r539733080", "bodyText": "Done. Thanks.", "author": "ManishKumarKeshri", "createdAt": "2020-12-09T23:44:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTY4MjI1MA=="}], "type": "inlineReview"}, {"oid": "66550e463d6ab5550074d8d1708f88fb07de26b1", "url": "https://github.com/pravega/pravega/commit/66550e463d6ab5550074d8d1708f88fb07de26b1", "message": "Fixing comments.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-12-09T23:36:13Z", "type": "commit"}, {"oid": "26ca74233d17b96785c7017dca1feb89d64a1c84", "url": "https://github.com/pravega/pravega/commit/26ca74233d17b96785c7017dca1feb89d64a1c84", "message": "Minor changes.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-12-09T23:42:27Z", "type": "commit"}]}