{"pr_number": 4686, "pr_title": "Issue 4676: (PDP-34 )  Part 1 of 4. - ChunkedSegmentStorage and BaseChunkMetadataStore implementation", "pr_createdAt": "2020-04-10T23:42:45Z", "pr_url": "https://github.com/pravega/pravega/pull/4686", "timeline": [{"oid": "a9d7cf04741f5b13c229025a8c694b31bc7e45c0", "url": "https://github.com/pravega/pravega/commit/a9d7cf04741f5b13c229025a8c694b31bc7e45c0", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Core functionality.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-06-04T21:39:46Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc1MjIzMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r435752231", "bodyText": "this list is only added elements", "author": "eolivelli", "createdAt": "2020-06-05T07:56:42Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1407 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(String streamSegmentName) {\n+        return null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName);\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<String> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        val systemLogRecord = systemJournal.getChunkAddedRecord(streamSegmentName,\n+                offset,\n+                oldChunkName == null ? null : oldChunkName,\n+                newChunkName);\n+        systemLogRecords.add(systemLogRecord);\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                if (chunkStorage.exists(chunkTodelete)) {\n+                    chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                    log.debug(\"{} collectGarbage - chunk={}.\", logPrefix, chunkTodelete);\n+                }\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);", "originalCommit": "a9d7cf04741f5b13c229025a8c694b31bc7e45c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjI3NjMyMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r436276320", "bodyText": "Yes, you are right. No background threads for additional processing yet. They are coming in next PR.", "author": "sachin-j-joshi", "createdAt": "2020-06-06T15:14:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc1MjIzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc1NDM3OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r435754379", "bodyText": "how big is expected to be this buffer ?\nshall we consider reusing the same buffer, with a fixed size ?", "author": "eolivelli", "createdAt": "2020-06-05T08:00:52Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1407 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(String streamSegmentName) {\n+        return null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName);\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<String> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        val systemLogRecord = systemJournal.getChunkAddedRecord(streamSegmentName,\n+                offset,\n+                oldChunkName == null ? null : oldChunkName,\n+                newChunkName);\n+        systemLogRecords.add(systemLogRecord);\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                if (chunkStorage.exists(chunkTodelete)) {\n+                    chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                    log.debug(\"{} collectGarbage - chunk={}.\", logPrefix, chunkTodelete);\n+                }\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            // This is a critical assumption at this point which should not be broken,\n+            if (null != systemJournal) {\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(targetSegmentName));\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(sourceSegment));\n+            }\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return chunkStorage.supportsAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In general without defrag the number of chunks in the system just keeps on increasing.\n+     * In addition when we have too many small chunks (say because too many small transactions), the segment is fragmented -\n+     * this may impact the read throughput but also performance of metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (eg. vanilla S3) and each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If underlying storage provides some facility to stitch together smaller chunks into larger chunks then we do actually\n+     * want to exploit that. Especially when this operation is supposed to be \"metadata only operation\" even for them.\n+     * Obviously both ECS and S3 have MPU and is supposed to be metadata only operation for them.\n+     * HDFS also has native concat (I think metadata only). NFS has no concept of native concat.\n+     * As chunks become larger, it no longer makes sense to concat them using append writes (read source completely and append -ie. write- it back at the end of target.)\n+     * We do not always use native concat to implement concat. We also use appends.\n+     * </li>\n+     * <li>\n+     * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+     * We can then fine tune that background task to run optimally with low overhead.\n+     * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+     * </li>\n+     * <li>\n+     * <li>\n+     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+     * </li>\n+     * </ul>\n+     *\n+     * <div>\n+     * What controls whether we invoke native concat or simulate through appends?\n+     * There are a few different capabilities that ChunkStorageProvider needs to provide.\n+     * <ul>\n+     * <li>Does ChunkStorageProvider support appending to existing chunks? For vanilla S3 compatible this would return false.\n+     * This is indicated by supportsAppend.</li>\n+     * <li>Does ChunkStorageProvider support for concatenating chunks natively? This is indicated by supportsConcat.\n+     * If this is true then native concat operation concat will be invoked otherwise concatWithAppend is invoked.</li>\n+     * <li>There are some obvious constraints - For ChunkStorageProvider support any concat functionality it must support either append or native concat.</li>\n+     * <li>Also when ChunkStorageProvider supports both native and append, ChunkStorageManager will invoke appropriate method\n+     * depending on size of target and source chunks. (Eg. ECS)</li>\n+     * </ul>\n+     * </div>\n+     * <li>\n+     * What controls defrag?\n+     * There are two additional parameters that control when native concat\n+     * <li>minSizeLimitForNativeConcat : Size of chunk in bytes above which it is no longer considered a small object. For small source objects, concatWithAppend is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n+     * <li>maxSizeLimitForNativeConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n+     * In short there is a size beyond which using append is not advisable. Conversely there is a size below which native concat is not efficient.(minSizeLimitForNativeConcat )\n+     * Then there is limit which concating does not make sense maxSizeLimitForNativeConcat\n+     * </li>\n+     * <li>\n+     * What is the defrag algorithm\n+     * <pre>\n+     * While(segment.hasConcatableChunks()){\n+     *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n+     *     For (List<chunk> list : s){\n+     *        ConcatChunks (list);\n+     *     }\n+     * }\n+     * </pre>\n+     * </li>\n+     * </ul>\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @param startChunkName  Name of the first chunk to start defragmentation.\n+     * @param lastChunkName   Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n+     * @throws Exception In case of any errors.\n+     */\n+    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, String startChunkName, String lastChunkName) throws Exception {\n+        // The algorithm is actually very simple.\n+        // It tries to concat all small chunks using appends first.\n+        // Then it tries to concat remaining chunks using native concat if available.\n+        // To implement it using single loop we toggle between concat with append and native concat modes. (Instead of two passes.)\n+        boolean useAppend = true;\n+        String targetChunkName = startChunkName;\n+\n+        // Iterate through chunk list\n+        while (null != targetChunkName && !targetChunkName.equals(lastChunkName)) {\n+            ChunkMetadata target = (ChunkMetadata) txn.get(targetChunkName);\n+\n+            ArrayList<ChunkInfo> chunksToConcat = new ArrayList<>();\n+            long targetSizeAfterConcat = target.getLength();\n+\n+            // Add target to the list of chunks\n+            chunksToConcat.add(new ChunkInfo(targetSizeAfterConcat, targetChunkName));\n+\n+            String nextChunkName = target.getNextChunk();\n+            ChunkMetadata next = null;\n+\n+            // Gather list of chunks that can be appended together.\n+            while (null != nextChunkName) {\n+                next = (ChunkMetadata) txn.get(nextChunkName);\n+\n+                if (useAppend && config.getMinSizeLimitForNativeConcat() < next.getLength()) {\n+                    break;\n+                }\n+\n+                if (targetSizeAfterConcat + next.getLength() > segmentMetadata.getMaxRollinglength() || next.getLength() > config.getMaxSizeLimitForNativeConcat()) {\n+                    break;\n+                }\n+\n+                chunksToConcat.add(new ChunkInfo(next.getLength(), nextChunkName));\n+                targetSizeAfterConcat += next.getLength();\n+\n+                nextChunkName = next.getNextChunk();\n+            }\n+            // Note - After above while loop is exited nextChunkName points to chunk next to last one to be concat.\n+            // Which means target should now point to it as next after concat is complete.\n+\n+            // If there are chunks that can be appended together then concat them.\n+            if (chunksToConcat.size() > 1) {\n+                // Concat\n+\n+                ConcatArgument[] concatArgs = new ConcatArgument[chunksToConcat.size()];\n+                for (int i = 0; i < chunksToConcat.size(); i++) {\n+                    concatArgs[i] = ConcatArgument.fromChunkInfo(chunksToConcat.get(i));\n+                }\n+\n+                if (!useAppend && chunkStorage.supportsConcat()) {\n+                    int length = chunkStorage.concat(concatArgs);\n+                } else {\n+                    concatUsingAppend(concatArgs);\n+                }\n+\n+                // Set the pointers\n+                target.setLength(targetSizeAfterConcat);\n+                target.setNextChunk(nextChunkName);\n+\n+                // If target is the last chunk after this then update metadata accordingly\n+                if (null == nextChunkName) {\n+                    segmentMetadata.setLastChunk(target.getName());\n+                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength() - target.getLength());\n+                }\n+\n+                // Update metadata for affected chunks.\n+                for (int i = 1; i < concatArgs.length; i++) {\n+                    txn.delete(concatArgs[i].getName());\n+                }\n+                txn.update(target);\n+                txn.update(segmentMetadata);\n+            }\n+\n+            // Move on to next place in list where we can concat if we are done with append based concats.\n+            if (!useAppend) {\n+                targetChunkName = nextChunkName;\n+            }\n+\n+            // Toggle\n+            useAppend = !useAppend;\n+        }\n+    }\n+\n+    private void concatUsingAppend(ConcatArgument[] concatArgs) throws ChunkStorageException {\n+        long writeAtOffset = concatArgs[0].getLength();\n+        val writeHandle = ChunkHandle.writeHandle(concatArgs[0].getName());\n+        for (int i = 1; i < concatArgs.length; i++) {\n+            int readAtOffset = 0;\n+            val arg = concatArgs[i];\n+            int bytesToRead = Math.toIntExact(arg.getLength());\n+\n+            while (bytesToRead > 0) {\n+                byte[] buffer = new byte[Math.min(config.getMaxBufferSizeForChunkDataTransfer(), bytesToRead)];", "originalCommit": "a9d7cf04741f5b13c229025a8c694b31bc7e45c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjI3NjUzOQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r436276539", "bodyText": "Currently it is set to be 1MB. (ChunkStorageManagerConfig.maxBufferSizeForChunkDataTransfer).", "author": "sachin-j-joshi", "createdAt": "2020-06-06T15:17:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc1NDM3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE3MjA0Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439172046", "bodyText": "There are several threads and this method should be called rarely. (only called when data is too small for calling native concat eg. for HDFS or ExtendedS3)\nDefinitely need to  measure and fix it in future if required.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T02:20:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc1NDM3OQ=="}], "type": "inlineReview"}, {"oid": "8a3a4e02c66a7b68d5b0dd4ee4303e0571768510", "url": "https://github.com/pravega/pravega/commit/8a3a4e02c66a7b68d5b0dd4ee4303e0571768510", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Serialize with VersionedSerializer.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-06-12T01:49:06Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ4Mjg0MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439482841", "bodyText": "what is the impact of changing this value ?\nis there any backward compatibility concern ?", "author": "eolivelli", "createdAt": "2020-06-12T15:18:05Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/SegmentRollingPolicy.java", "diffHunk": "@@ -16,7 +16,10 @@\n  * A generic rolling policy that can be applied to any Storage unit.\n  */\n public final class SegmentRollingPolicy {\n-    public static final SegmentRollingPolicy NO_ROLLING = new SegmentRollingPolicy(Long.MAX_VALUE);\n+    /**\n+     * Max rolling length is 2^62 so that we can use CompactLong in serialization everywhere.\n+     */\n+    public static final SegmentRollingPolicy NO_ROLLING = new SegmentRollingPolicy(Long.MAX_VALUE / 4 );", "originalCommit": "8a3a4e02c66a7b68d5b0dd4ee4303e0571768510", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY0OTc3MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439649770", "bodyText": "For new installations that shouldn't be problem. For migration we will have to  set it to new  max rolling length. From practical point of view max rolling length will now be 2305843009213693951 bytes which is still really big.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T21:29:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ4Mjg0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI2MTkwMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442261903", "bodyText": "I don't understand what the context for changing this constant.", "author": "fpj", "createdAt": "2020-06-18T14:18:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ4Mjg0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwNTk4Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442505983", "bodyText": "For serialization we want to use compact long representation. If not we'll require 8 bytes for each length related field.  Using compact serialization allows us to save size of records. However compact can handle up to 2^62.  Long.MAX_VALUE is an arbitrary limit there is no real reason the default value has to be that high. With 2^62 the new limit is 2305843009213693951 which is still really big default value,", "author": "sachin-j-joshi", "createdAt": "2020-06-18T21:13:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ4Mjg0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ4NDQ3OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439484478", "bodyText": "what happens in case that data returns less than length bytes ?\nwhat happens in case that offset points to a part of the chunk that has already be written ?\nis it supposed to be overwritten ?\nor is it only expected that we are appending data to the end of the chunk ?", "author": "eolivelli", "createdAt": "2020-06-12T15:21:10Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,531 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorageProvider}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public abstract class BaseChunkStorageProvider implements ChunkStorageProvider {\n+\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Constructor.\n+     *\n+     */\n+    public BaseChunkStorageProvider() {\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public boolean exists(String chunkName) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+\n+        // Call concrete implementation.\n+        boolean retValue = doesExist(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Creates a new file.\n+     *\n+     * @param chunkName Name of the chunk to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     *\n+     */\n+    @Override\n+    final public ChunkHandle create(String chunkName) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"create\", chunkName);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doCreate(chunkName);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageProviderMetrics.CREATE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.CREATE_COUNT.inc();\n+\n+        log.debug(\"Create - chunk={}, latency={}.\", chunkName, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"create\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Deletes a chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to delete.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public void delete(ChunkHandle handle) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle.getChunkName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        doDelete(handle);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageProviderMetrics.DELETE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.DELETE_COUNT.inc();\n+\n+        log.debug(\"Delete - chunk={}, latency={}.\", handle.getChunkName(), elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"delete\", traceId, handle.getChunkName());\n+\n+    }\n+\n+    /**\n+     * Opens chunk for Read.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkHandle A readable handle for the given chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openRead\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenRead(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openRead\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Opens chunk for Write (or modifications).\n+     *\n+     * @param chunkName String name of the chunk to write to or modify.\n+     * @return ChunkHandle A writable handle for the given chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenWrite(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openWrite\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Retrieves the ChunkInfo for given name.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkInfo Information about the given chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkInfo getInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkNotNull(chunkName);\n+        long traceId = LoggerHelpers.traceEnter(log, \"getInfo\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkInfo info = doGetInfo(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"getInfo\", traceId, chunkName);\n+\n+        return info;\n+    }\n+\n+    /**\n+     * Reads a range of bytes from the underlying chunk.\n+     *\n+     * @param handle       ChunkHandle of the chunk to read from.\n+     * @param fromOffset   Offset in the chunk from which to start reading.\n+     * @param length       Number of bytes to read.\n+     * @param buffer       Byte buffer to which data is copied.\n+     * @param bufferOffset Offset in the buffer at which to start copying read data.\n+     * @return int Number of bytes read.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     * @throws IndexOutOfBoundsException If the index is out of bounds.\n+     */\n+    @Override\n+    final public int read(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle\");\n+        Preconditions.checkArgument(null != buffer, \"buffer\");\n+        Preconditions.checkArgument(fromOffset >= 0, \"fromOffset must be non-negative\");\n+        Preconditions.checkArgument(length >= 0 && length <= buffer.length, \"length\");\n+        Preconditions.checkElementIndex(bufferOffset, buffer.length, \"bufferOffset\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"read\", handle.getChunkName(), fromOffset, bufferOffset, length);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int bytesRead = doRead(handle, fromOffset, length, buffer, bufferOffset);\n+\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageProviderMetrics.READ_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.READ_BYTES.add(bytesRead);\n+\n+        log.debug(\"Read - chunk={}, offset={}, bytesRead={}, latency={}.\", handle.getChunkName(), fromOffset, length, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"read\", traceId, bytesRead);\n+\n+        return bytesRead;\n+    }\n+\n+    /**\n+     * Writes the given data to the underlying chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to write to.\n+     * @param offset Offset in the chunk to start writing.\n+     * @param length Number of bytes to write.\n+     * @param data   An InputStream representing the data to write.\n+     * @return int Number of bytes written.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public int write(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException {", "originalCommit": "8a3a4e02c66a7b68d5b0dd4ee4303e0571768510", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUwMzA1Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439503052", "bodyText": "It can write less bytes than length. In this case ChunkStorageManeger will make another call.\nThe writes are supposed to create same effect as if they are append only. The upper layers will guarantee that either data is never overwritten or that same data is written back at same offset.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T15:55:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ4NDQ3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ4Njc3MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439486770", "bodyText": "I apologize,\nI am trying to understand better the final API that we are defining.\nIt looks like that this ChunkHandle is only a wrapper for a chunkName.\nIt looks mostly useless.\nAs you previously said, it would be really useful that concrete implementations subclass this class and enrich it with custom properties.", "author": "eolivelli", "createdAt": "2020-06-12T15:25:14Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkHandle.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Data;\n+import lombok.NonNull;\n+\n+/**\n+ * Handle to a chunk.\n+ */\n+@Data\n+final public class ChunkHandle {", "originalCommit": "8a3a4e02c66a7b68d5b0dd4ee4303e0571768510", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUwOTk0NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439509944", "bodyText": "I agree - In fact it is useless, and I'm now inclined to remove the concept of ChunkHandle .\nThe problem with extensible ChunkHandle is that now to create read/write handle instances in the code , we have to  add openRead and openWrite as new apis on ChunkStorageProvider. (so that we create an instance of correct concrete type).\nIt initially seemed useful to have ability to put implementation specific data in ChunkHandle.\nHowever then problem is that when you have two separate ChunkHandle instances pointing to the same underlying object, the data stored in them diverges and can have unintended side effects .\nIn this case most likely implementation choice is to keep single copy of internal state in some other data structure and handle merely points to that data. So having extensible ChunkHandle isn't really helping here.\nGiven that ChunkHandle creates more problem than it solves - I am not sure having it is useful.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T16:08:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ4Njc3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkzNzQ2NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443937465", "bodyText": "For now keeping the ChunkHandle.", "author": "sachin-j-joshi", "createdAt": "2020-06-23T03:14:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ4Njc3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ4ODgwNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439488804", "bodyText": "What is the purpose of this class ?\nGiven a chunkname you can get its current length\nis it expected to be used by tools or only internally ?", "author": "eolivelli", "createdAt": "2020-06-12T15:28:50Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkInfo.java", "diffHunk": "@@ -0,0 +1,32 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.NonNull;\n+\n+/**\n+ * Chunk Information.\n+ */\n+@Builder\n+@Data\n+final public class ChunkInfo {", "originalCommit": "8a3a4e02c66a7b68d5b0dd4ee4303e0571768510", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUyNDY5OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439524698", "bodyText": "The purpose of this class is to get information related to chunk from ChunkStorageProvider.\nThe only relevant property today is length, but in future their could be more.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T16:32:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ4ODgwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5MjM2NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439492365", "bodyText": "this cast  looks very dangerous.\nA chunk is expected to be longer than Integer.MAX_VALUE", "author": "eolivelli", "createdAt": "2020-06-12T15:35:32Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1432 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());", "originalCommit": "8a3a4e02c66a7b68d5b0dd4ee4303e0571768510", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUyMzQ5Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439523492", "bodyText": "Good catch. That cast is not needed at all. leftover from old version of code. (will fix)", "author": "sachin-j-joshi", "createdAt": "2020-06-12T16:30:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5MjM2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTYxNzI4MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439617280", "bodyText": "I am actually surprised that spotbugs didn't catch it.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T20:00:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5MjM2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY0NjU5Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439646593", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-06-12T21:19:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5MjM2NQ=="}], "type": "inlineReview"}, {"oid": "d023bb71a22e0be2b1585566468e76cd89806831", "url": "https://github.com/pravega/pravega/commit/d023bb71a22e0be2b1585566468e76cd89806831", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Renaming and other review changes.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-06-23T23:57:01Z", "type": "forcePushed"}, {"oid": "c1504d075982965e6a767ca931b47caa5fe60987", "url": "https://github.com/pravega/pravega/commit/c1504d075982965e6a767ca931b47caa5fe60987", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - More cleanup.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-06-24T21:02:52Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5NjQ0Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445896446", "bodyText": "This is actually 2^61, not 2^62. Long.MAX_VALUE is 2^63-1.", "author": "andreipaduroiu", "createdAt": "2020-06-25T23:44:05Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/SegmentRollingPolicy.java", "diffHunk": "@@ -16,7 +16,10 @@\n  * A generic rolling policy that can be applied to any Storage unit.\n  */\n public final class SegmentRollingPolicy {\n-    public static final SegmentRollingPolicy NO_ROLLING = new SegmentRollingPolicy(Long.MAX_VALUE);\n+    /**\n+     * Max rolling length is 2^62 so that we can use CompactLong in serialization everywhere.\n+     */\n+    public static final SegmentRollingPolicy NO_ROLLING = new SegmentRollingPolicy(Long.MAX_VALUE / 4);", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkxOTc0Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445919743", "bodyText": "I want 62 bit number : Long.MAX_VALUE / 4 makes it 2 bits less than 64.\nSo comment is wrong - 2^61 , but logic is not.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T01:18:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5NjQ0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk1ODU5OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445958598", "bodyText": "fixed comment.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T04:17:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5NjQ0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAyOTk2MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447029960", "bodyText": "Long is a signed type, so its values are from -2^63 to 2^63-1. Therefore Long.MAX_VALUE is 2^63-1. Dividing that number by 4 gives you the wrong result.\nTry this code, for example:\n        System.out.println(1L << 63);\n        System.out.println(Long.MAX_VALUE);\n        System.out.println(Long.MAX_VALUE / 4);\n        System.out.println(1L << 61);\n        System.out.println(1L << 62);\n\nYou'll get\n-9223372036854775808\n9223372036854775807\n2305843009213693951\n2305843009213693952\n4611686018427387904\n\nSo:\n\n2^63 overflows Long (it is 1 + Long.MAX_VALUE)\nLong.MAX_VALUE/4 == 2^61 - 1", "author": "andreipaduroiu", "createdAt": "2020-06-29T14:49:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5NjQ0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA0NDU5Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447044592", "bodyText": "System.out.println(Long.MAX_VALUE / 4);\nSystem.out.println(1L << 61);\n2305843009213693951\n2305843009213693952\n\n\nWhy do you think Long.MAX_VALUE / 4 requires more than 62 bits? (61 + 1 sign bit)", "author": "sachin-j-joshi", "createdAt": "2020-06-29T15:09:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5NjQ0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA2ODI2NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447068265", "bodyText": "Ok, I got confused about your comment.\nThen please set the explicit value to this field (1L << 61 -1). Do not rely on integer division to do the work for you.\nWhy do you need a sign bit? If offset is always >=0, then you can safely serialize up to 2^62 (excluded). so you can set that as your upper bound.", "author": "andreipaduroiu", "createdAt": "2020-06-29T15:41:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5NjQ0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzIyMDA4NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447220084", "bodyText": "updated.", "author": "sachin-j-joshi", "createdAt": "2020-06-29T20:01:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5NjQ0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI5Mjc3OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447292779", "bodyText": "Using RevisionDataOutput.COMPACT_LONG_MAX as max segment length.", "author": "sachin-j-joshi", "createdAt": "2020-06-29T22:24:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5NjQ0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5NjY4OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445896688", "bodyText": "a new chunk", "author": "andreipaduroiu", "createdAt": "2020-06-25T23:45:04Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,564 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ * <div>\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ * </div>\n+ * <div>\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkManager will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ * </div>\n+ *\n+ * <div>\n+ * The implementations in this repository are tested using following test suites.\n+ * <ul>\n+ * <li>SimpleStorageTests</li>\n+ * <li>ChunkManagerRollingTests</li>\n+ * <li>ChunkStorageProviderTests</li>\n+ * <li>SystemJournalTests</li>\n+ * </ul>\n+ * </div>\n+ */\n+@Slf4j\n+public abstract class BaseChunkStorage implements ChunkStorage {\n+\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Constructor.\n+     */\n+    public BaseChunkStorage() {\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public boolean exists(String chunkName) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+\n+        // Call concrete implementation.\n+        boolean retValue = checkExists(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Creates a new file.", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk1ODY1OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445958659", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-06-26T04:17:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5NjY4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5NzAxNg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445897016", "bodyText": "No need to declare throws exception. You are less restrictive than the interface you are overriding so it will compile even without.", "author": "andreipaduroiu", "createdAt": "2020-06-25T23:46:11Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,564 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ * <div>\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ * </div>\n+ * <div>\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkManager will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ * </div>\n+ *\n+ * <div>\n+ * The implementations in this repository are tested using following test suites.\n+ * <ul>\n+ * <li>SimpleStorageTests</li>\n+ * <li>ChunkManagerRollingTests</li>\n+ * <li>ChunkStorageProviderTests</li>\n+ * <li>SystemJournalTests</li>\n+ * </ul>\n+ * </div>\n+ */\n+@Slf4j\n+public abstract class BaseChunkStorage implements ChunkStorage {\n+\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Constructor.\n+     */\n+    public BaseChunkStorage() {\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public boolean exists(String chunkName) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+\n+        // Call concrete implementation.\n+        boolean retValue = checkExists(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Creates a new file.\n+     *\n+     * @param chunkName Name of the chunk to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public ChunkHandle create(String chunkName) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"create\", chunkName);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doCreate(chunkName);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageMetrics.CREATE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageMetrics.CREATE_COUNT.inc();\n+\n+        log.debug(\"Create - chunk={}, latency={}.\", chunkName, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"create\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Deletes a chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to delete.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public void delete(ChunkHandle handle) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle.getChunkName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        doDelete(handle);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageMetrics.DELETE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageMetrics.DELETE_COUNT.inc();\n+\n+        log.debug(\"Delete - chunk={}, latency={}.\", handle.getChunkName(), elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"delete\", traceId, handle.getChunkName());\n+\n+    }\n+\n+    /**\n+     * Opens chunk for Read.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkHandle A readable handle for the given chunk.\n+     * @throws ChunkStorageException    Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openRead\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenRead(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openRead\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Opens chunk for Write (or modifications).\n+     *\n+     * @param chunkName String name of the chunk to write to or modify.\n+     * @return ChunkHandle A writable handle for the given chunk.\n+     * @throws ChunkStorageException    Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenWrite(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openWrite\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Retrieves the ChunkInfo for given name.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkInfo Information about the given chunk.\n+     * @throws ChunkStorageException    Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkInfo getInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkNotNull(chunkName);\n+        long traceId = LoggerHelpers.traceEnter(log, \"getInfo\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkInfo info = doGetInfo(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"getInfo\", traceId, chunkName);\n+\n+        return info;\n+    }\n+\n+    /**\n+     * Reads a range of bytes from the underlying chunk.\n+     *\n+     * @param handle       ChunkHandle of the chunk to read from.\n+     * @param fromOffset   Offset in the chunk from which to start reading.\n+     * @param length       Number of bytes to read.\n+     * @param buffer       Byte buffer to which data is copied.\n+     * @param bufferOffset Offset in the buffer at which to start copying read data.\n+     * @return int Number of bytes read.\n+     * @throws ChunkStorageException     Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException  If argument is invalid.\n+     * @throws IndexOutOfBoundsException If the index is out of bounds or offset is not a valid offset in the underlying file/object.\n+     */\n+    @Override\n+    final public int read(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle\");\n+        Preconditions.checkArgument(null != buffer, \"buffer\");\n+        Preconditions.checkArgument(fromOffset >= 0, \"fromOffset must be non-negative\");\n+        Preconditions.checkArgument(length >= 0 && length <= buffer.length, \"length\");\n+        Preconditions.checkElementIndex(bufferOffset, buffer.length, \"bufferOffset\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"read\", handle.getChunkName(), fromOffset, bufferOffset, length);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int bytesRead = doRead(handle, fromOffset, length, buffer, bufferOffset);\n+\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageMetrics.READ_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageMetrics.READ_BYTES.add(bytesRead);\n+\n+        log.debug(\"Read - chunk={}, offset={}, bytesRead={}, latency={}.\", handle.getChunkName(), fromOffset, length, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"read\", traceId, bytesRead);\n+\n+        return bytesRead;\n+    }\n+\n+    /**\n+     * Writes the given data to the underlying chunk.\n+     *\n+     * <ul>\n+     * <li>It is expected that in cases where it can not overwrite the existing data at given offset, the implementation should throw IndexOutOfBoundsException.</li>\n+     * For storage where underlying files/objects are immutable once written, the implementation should return false on {@link ChunkStorage#supportsAppend()}.\n+     * <li>In such cases only valid offset is 0.</li>\n+     * <li>For storages where underlying files/objects can only be appended but not overwritten, it must match actual current length of underlying file/object.</li>\n+     * <li>In all cases the offset can not be greater that actual current length of underlying file/object. </li>\n+     * </ul>\n+     * @param handle ChunkHandle of the chunk to write to.\n+     * @param offset Offset in the chunk to start writing.\n+     * @param length Number of bytes to write.\n+     * @param data   An InputStream representing the data to write.\n+     * @return int Number of bytes written.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public int write(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        Preconditions.checkArgument(null != data, \"data must not be null\");\n+        Preconditions.checkArgument(offset >= 0, \"offset must be non-negative\");\n+        Preconditions.checkArgument(length >= 0, \"length must be non-negative\");\n+        if (!supportsAppend()) {\n+            Preconditions.checkArgument(offset == 0, \"offset must be 0 because storage does not support appends.\");\n+        }\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"write\", handle.getChunkName(), offset, length);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int bytesWritten = doWrite(handle, offset, length, data);\n+\n+        Duration elapsed = timer.getElapsed();\n+\n+        ChunkStorageMetrics.WRITE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageMetrics.WRITE_BYTES.add(bytesWritten);\n+\n+        log.debug(\"Write - chunk={}, offset={}, bytesWritten={}, latency={}.\", handle.getChunkName(), offset, length, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"read\", traceId, bytesWritten);\n+\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Concatenates two or more chunks. The first chunk is concatenated to.\n+     *\n+     * @param chunks Array of ConcatArgument objects containing info about existing chunks to be concatenated together.\n+     *               The chunks must be concatenated in the same sequence the arguments are provided.\n+     * @return int Number of bytes concatenated.\n+     * @throws ChunkStorageException         Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public int concat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        checkConcatArgs(chunks);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"concat\", chunks[0].getName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int retValue = doConcat(chunks);\n+\n+        Duration elapsed = timer.getElapsed();\n+        log.debug(\"concat - target={}, latency={}.\", chunks[0].getName(), elapsed.toMillis());\n+\n+        ChunkStorageMetrics.CONCAT_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageMetrics.CONCAT_BYTES.add(retValue);\n+        ChunkStorageMetrics.CONCAT_COUNT.inc();\n+        ChunkStorageMetrics.LARGE_CONCAT_COUNT.inc();\n+\n+        LoggerHelpers.traceLeave(log, \"concat\", traceId, chunks[0].getName());\n+\n+        return retValue;\n+    }\n+\n+    private void checkConcatArgs(ConcatArgument[] chunks) {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunks, \"chunks must not be null\");\n+        Preconditions.checkArgument(chunks.length >= 2, \"There must be at least two chunks\");\n+\n+        Preconditions.checkArgument(null != chunks[0], \"target chunk must not be null\");\n+        Preconditions.checkArgument(chunks[0].getLength() >= 0, \"target chunk lenth must be non negative.\");\n+\n+        for (int i = 1; i < chunks.length; i++) {\n+            Preconditions.checkArgument(null != chunks[i], \"source chunk must not be null\");\n+            Preconditions.checkArgument(chunks[i].getLength() >= 0, \"source chunk lenth must be non negative.\");\n+            Preconditions.checkArgument(!chunks[i].getName().equals(chunks[0].getName()), \"source chunk is same as target\");\n+            Preconditions.checkArgument(!chunks[i].getName().equals(chunks[i - 1].getName()), \"duplicate chunk found\");\n+        }\n+    }\n+\n+    /**\n+     * Truncates a given chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to truncate.\n+     * @param offset Offset to truncate to.\n+     * @return True if the object was truncated, false otherwise.\n+     * @throws ChunkStorageException         Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public boolean truncate(ChunkHandle handle, long offset) throws ChunkStorageException, UnsupportedOperationException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        Preconditions.checkArgument(offset > 0, \"handle must not be readonly\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle.getChunkName());\n+\n+        // Call concrete implementation.\n+        boolean retValue = doTruncate(handle, offset);\n+\n+        LoggerHelpers.traceLeave(log, \"truncate\", traceId, handle.getChunkName());\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Sets readonly attribute for the chunk.\n+     *\n+     * @param handle     ChunkHandle of the chunk.\n+     * @param isReadonly True if chunk is set to be readonly.\n+     * @return True if the operation was successful, false otherwise.\n+     * @throws ChunkStorageException         Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public boolean setReadOnly(ChunkHandle handle, boolean isReadonly) throws ChunkStorageException, UnsupportedOperationException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"setReadOnly\", handle.getChunkName());\n+\n+        // Call concrete implementation.\n+        boolean retValue = doSetReadOnly(handle, isReadonly);\n+\n+        LoggerHelpers.traceLeave(log, \"setReadOnly\", traceId, handle.getChunkName());\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Closes.\n+     *\n+     * @throws Exception In case of any error.\n+     */\n+    @Override\n+    public void close() throws Exception {", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5ODA5Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445898093", "bodyText": "getOrDefault will return null if it doesn't exist. Use that to avoid having to make 2 calls into the map.", "author": "andreipaduroiu", "createdAt": "2020-06-25T23:50:13Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.\n+ */\n+class ReadIndexCache {\n+\n+    /**\n+     * Max number of indexed segments to keep in cache.\n+     */\n+    private final int maxIndexedSegments;\n+\n+    /**\n+     * Max number of indexed chunks to keep in cache.\n+     */\n+    private final int maxIndexedChunks;\n+\n+    /**\n+     * Max number of indexed chunks to keep per segment in cache.\n+     */\n+    private final int maxIndexedChunksPerSegment;\n+\n+    /**\n+     * Current generation of cache entries.\n+     */\n+    private final AtomicLong currentGeneration = new AtomicLong();\n+\n+    /**\n+     * Lowest generation of cache entries.\n+     */\n+    private final AtomicLong oldestGeneration = new AtomicLong();\n+\n+    /**\n+     * Total number of chunks in the cache.\n+     */\n+    private final AtomicInteger totalChunkCount = new AtomicInteger();\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, SegmentReadIndex> segmentsToReadIndexMap = new ConcurrentHashMap<String, SegmentReadIndex>();\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param maxIndexedSegments\n+     * @param maxIndexedChunksPerSegment\n+     * @param maxIndexedChunks\n+     */\n+    public ReadIndexCache(int maxIndexedSegments, int maxIndexedChunksPerSegment, int maxIndexedChunks) {\n+        this.maxIndexedChunksPerSegment = maxIndexedChunksPerSegment;\n+        this.maxIndexedSegments = maxIndexedSegments;\n+        this.maxIndexedChunks = maxIndexedChunks;\n+    }\n+\n+    /**\n+     * Retrieves the read index for given segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @return Read index correpsonding to the given segment. A new empty index is created if it doesn't already exist.\n+     */\n+    private SegmentReadIndex getSegmentReadIndex(String streamSegmentName) {\n+        SegmentReadIndex readIndex;\n+        if (segmentsToReadIndexMap.containsKey(streamSegmentName)) {", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk2MTI2Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445961263", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T04:29:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5ODA5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5ODk1Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445898957", "bodyText": "This type of eviction is very similar to how we keep and evict Cache Entries. So I have a few questions:\n\nIs there a way we can use the existing Cache Manager to perform eviction?\nCan we actually use the active segments from the Container Metadata? That thing already has a way to track all segments and evicts them based on an LRU criteria. There should be hooks available in the Segment Container's Metadata Cleaner to do that.", "author": "andreipaduroiu", "createdAt": "2020-06-25T23:53:08Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.\n+ */\n+class ReadIndexCache {\n+\n+    /**\n+     * Max number of indexed segments to keep in cache.\n+     */\n+    private final int maxIndexedSegments;\n+\n+    /**\n+     * Max number of indexed chunks to keep in cache.\n+     */\n+    private final int maxIndexedChunks;\n+\n+    /**\n+     * Max number of indexed chunks to keep per segment in cache.\n+     */\n+    private final int maxIndexedChunksPerSegment;\n+\n+    /**\n+     * Current generation of cache entries.\n+     */\n+    private final AtomicLong currentGeneration = new AtomicLong();\n+\n+    /**\n+     * Lowest generation of cache entries.\n+     */\n+    private final AtomicLong oldestGeneration = new AtomicLong();\n+\n+    /**\n+     * Total number of chunks in the cache.\n+     */\n+    private final AtomicInteger totalChunkCount = new AtomicInteger();\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, SegmentReadIndex> segmentsToReadIndexMap = new ConcurrentHashMap<String, SegmentReadIndex>();\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param maxIndexedSegments\n+     * @param maxIndexedChunksPerSegment\n+     * @param maxIndexedChunks\n+     */\n+    public ReadIndexCache(int maxIndexedSegments, int maxIndexedChunksPerSegment, int maxIndexedChunks) {\n+        this.maxIndexedChunksPerSegment = maxIndexedChunksPerSegment;\n+        this.maxIndexedSegments = maxIndexedSegments;\n+        this.maxIndexedChunks = maxIndexedChunks;\n+    }\n+\n+    /**\n+     * Retrieves the read index for given segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @return Read index correpsonding to the given segment. A new empty index is created if it doesn't already exist.\n+     */\n+    private SegmentReadIndex getSegmentReadIndex(String streamSegmentName) {\n+        SegmentReadIndex readIndex;\n+        if (segmentsToReadIndexMap.containsKey(streamSegmentName)) {\n+            readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        } else {\n+            // Evict segments if required.\n+            if (maxIndexedSegments < segmentsToReadIndexMap.size() + 1 || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictSegmentsFromOldestGeneration();\n+            }\n+            val newReadIndex = SegmentReadIndex.builder()\n+                    .chunkIndex(new ConcurrentSkipListMap<Long, SegmentReadIndexEntry>())\n+                    .generation(currentGeneration.get())\n+                    .build();\n+            val oldReadIndex = segmentsToReadIndexMap.putIfAbsent(streamSegmentName, newReadIndex);\n+            readIndex = null != oldReadIndex ? oldReadIndex : newReadIndex;\n+        }\n+\n+        return readIndex;\n+    }\n+\n+    /**\n+     * Gets total number of chunks in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalChunksCount() {\n+        return totalChunkCount.get();\n+    }\n+\n+    /**\n+     * Gets total number of segments in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalSegmentCount() {\n+        return segmentsToReadIndexMap.size();\n+    }\n+\n+    /**\n+     * Gets oldest generation in cache.\n+     *\n+     * @return\n+     */\n+    public long getOldestGeneration() {\n+        return oldestGeneration.get();\n+    }\n+\n+    /**\n+     * Gets current generation of cache.\n+     *\n+     * @return\n+     */\n+    public long getCurrentGeneration() {\n+        return currentGeneration.get();", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkyNzgwMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445927800", "bodyText": "That will help with eviction of segments but not chunks.\nThis data is used only for finding first chunk to read from in case of a read. (and also helps with recently written last chunks so even if the segment is evicted out of memory it can still do quick look up for recently evicted segments.\nIt has a bit different use case and some specific logic related to chunks that is not covered by Cache Entries.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T01:53:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5ODk1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkyODUwNg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445928506", "bodyText": "Anyways we'll revisit this soon #4902 as a separate PR.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T01:56:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5ODk1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDE5MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445900191", "bodyText": "Is this the most appropriate data type for this? Could we use one of the following:\n\nTreeMap (you'll have to synchronize).\nSortedIndex (in Pravega Common). This is an AVL tree that uses less memory and is a bit faster than TreeMap.", "author": "andreipaduroiu", "createdAt": "2020-06-25T23:57:44Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.\n+ */\n+class ReadIndexCache {\n+\n+    /**\n+     * Max number of indexed segments to keep in cache.\n+     */\n+    private final int maxIndexedSegments;\n+\n+    /**\n+     * Max number of indexed chunks to keep in cache.\n+     */\n+    private final int maxIndexedChunks;\n+\n+    /**\n+     * Max number of indexed chunks to keep per segment in cache.\n+     */\n+    private final int maxIndexedChunksPerSegment;\n+\n+    /**\n+     * Current generation of cache entries.\n+     */\n+    private final AtomicLong currentGeneration = new AtomicLong();\n+\n+    /**\n+     * Lowest generation of cache entries.\n+     */\n+    private final AtomicLong oldestGeneration = new AtomicLong();\n+\n+    /**\n+     * Total number of chunks in the cache.\n+     */\n+    private final AtomicInteger totalChunkCount = new AtomicInteger();\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, SegmentReadIndex> segmentsToReadIndexMap = new ConcurrentHashMap<String, SegmentReadIndex>();\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param maxIndexedSegments\n+     * @param maxIndexedChunksPerSegment\n+     * @param maxIndexedChunks\n+     */\n+    public ReadIndexCache(int maxIndexedSegments, int maxIndexedChunksPerSegment, int maxIndexedChunks) {\n+        this.maxIndexedChunksPerSegment = maxIndexedChunksPerSegment;\n+        this.maxIndexedSegments = maxIndexedSegments;\n+        this.maxIndexedChunks = maxIndexedChunks;\n+    }\n+\n+    /**\n+     * Retrieves the read index for given segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @return Read index correpsonding to the given segment. A new empty index is created if it doesn't already exist.\n+     */\n+    private SegmentReadIndex getSegmentReadIndex(String streamSegmentName) {\n+        SegmentReadIndex readIndex;\n+        if (segmentsToReadIndexMap.containsKey(streamSegmentName)) {\n+            readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        } else {\n+            // Evict segments if required.\n+            if (maxIndexedSegments < segmentsToReadIndexMap.size() + 1 || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictSegmentsFromOldestGeneration();\n+            }\n+            val newReadIndex = SegmentReadIndex.builder()\n+                    .chunkIndex(new ConcurrentSkipListMap<Long, SegmentReadIndexEntry>())\n+                    .generation(currentGeneration.get())\n+                    .build();\n+            val oldReadIndex = segmentsToReadIndexMap.putIfAbsent(streamSegmentName, newReadIndex);\n+            readIndex = null != oldReadIndex ? oldReadIndex : newReadIndex;\n+        }\n+\n+        return readIndex;\n+    }\n+\n+    /**\n+     * Gets total number of chunks in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalChunksCount() {\n+        return totalChunkCount.get();\n+    }\n+\n+    /**\n+     * Gets total number of segments in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalSegmentCount() {\n+        return segmentsToReadIndexMap.size();\n+    }\n+\n+    /**\n+     * Gets oldest generation in cache.\n+     *\n+     * @return\n+     */\n+    public long getOldestGeneration() {\n+        return oldestGeneration.get();\n+    }\n+\n+    /**\n+     * Gets current generation of cache.\n+     *\n+     * @return\n+     */\n+    public long getCurrentGeneration() {\n+        return currentGeneration.get();\n+    }\n+\n+    /**\n+     * Adds a new index entry for a given chunk in index for the segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param chunkName         Name of the chunk.\n+     * @param startOffset       Start offset of the chunk.\n+     */\n+    public void addIndexEntry(String streamSegmentName, String chunkName, long startOffset) {\n+        if (null != chunkName) {\n+            val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+\n+            // Evict chunks if required.\n+            if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + 1\n+                    || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictChunks(streamSegmentName, 1);\n+            }\n+\n+            segmentReadIndex.chunkIndex.put(startOffset,\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(chunkName)\n+                            .generation(currentGeneration.get())\n+                            .build());\n+            segmentReadIndex.setGeneration(currentGeneration.get());\n+            totalChunkCount.incrementAndGet();\n+        }\n+    }\n+\n+    /**\n+     * Updates read index for given segment with new entries.\n+     *\n+     * @param streamSegmentName\n+     * @param newReadIndexEntries List of {@link ChunkNameOffsetPair} for new entries.\n+     */\n+    public void addIndexEntries(String streamSegmentName, List<ChunkNameOffsetPair> newReadIndexEntries) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        // Evict chunks if required.\n+        if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + newReadIndexEntries.size()\n+                || maxIndexedChunks < totalChunkCount.get() + newReadIndexEntries.size()) {\n+            evictChunks(streamSegmentName, newReadIndexEntries.size());\n+        }\n+\n+        for (val entry : newReadIndexEntries) {\n+            segmentReadIndex.chunkIndex.put(entry.getOffset(),\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(entry.getChunkName())\n+                            .generation(currentGeneration.get()).build());\n+            segmentReadIndex.setGeneration(currentGeneration.get());\n+\n+        }\n+        totalChunkCount.getAndAdd(newReadIndexEntries.size());\n+    }\n+\n+    /**\n+     * Removes the given segment from cache.\n+     *\n+     * @param streamSegmentName\n+     */\n+    public void remove(String streamSegmentName) {\n+        val readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        if (null != readIndex) {\n+            segmentsToReadIndexMap.remove(streamSegmentName);\n+            totalChunkCount.getAndAdd(-1 * readIndex.chunkIndex.size());\n+        }\n+    }\n+\n+    /**\n+     * Finds a chunk that is floor to the given offset.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset for which to search.\n+     * @return\n+     */\n+    public ChunkNameOffsetPair findFloor(String streamSegmentName, long offset) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        if (segmentReadIndex.chunkIndex.size() > 0) {\n+            val floorEntry = segmentReadIndex.chunkIndex.floorEntry(offset);\n+            if (null != floorEntry) {\n+                // mark with current generation\n+                segmentReadIndex.setGeneration(currentGeneration.get());\n+                floorEntry.getValue().setGeneration(currentGeneration.get());\n+                // return value.\n+                return new ChunkNameOffsetPair(floorEntry.getKey(), floorEntry.getValue().getChunkName());\n+            }\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Truncate the read index for given segment by removing all the chunks that are below given offset.\n+     *\n+     * @param streamSegmentName\n+     * @param startOffset\n+     */\n+    public void truncateReadIndex(String streamSegmentName, long startOffset) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        if (null != segmentReadIndex) {\n+            if (segmentReadIndex.chunkIndex.size() > 0) {\n+                val headMap = segmentReadIndex.chunkIndex.headMap(startOffset);\n+                if (null != headMap) {\n+                    int removed = 0;\n+                    ArrayList<Long> keysToRemove = new ArrayList<Long>();\n+                    keysToRemove.addAll(headMap.keySet());\n+                    for (val keyToRemove : keysToRemove) {\n+                        segmentReadIndex.chunkIndex.remove(keyToRemove);\n+                        removed++;\n+                    }\n+                    if (removed > 0) {\n+                        totalChunkCount.getAndAdd(-1 * removed);\n+                    }\n+                }\n+            }\n+            if (segmentReadIndex.chunkIndex.size() > 0) {\n+                segmentReadIndex.setGeneration(currentGeneration.get());\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Evicts segments from the cache.\n+     */\n+    private void evictSegmentsFromOldestGeneration() {\n+        val oldGen = oldestGeneration.get();\n+        currentGeneration.getAndIncrement();\n+\n+        val iterator = segmentsToReadIndexMap.entrySet().iterator();\n+        int total = totalChunkCount.get();\n+        int removed = 0;\n+        while (iterator.hasNext() && (segmentsToReadIndexMap.size() >= maxIndexedSegments || total >= maxIndexedChunks)) {\n+            val entry = iterator.next();\n+            if (entry.getValue().getGeneration() <= oldGen) {\n+                val size = entry.getValue().chunkIndex.size();\n+                removed += size;\n+                total -= size;\n+                iterator.remove();\n+            }\n+        }\n+        if (removed > 0) {\n+            oldestGeneration.compareAndSet(oldGen, oldGen + 1);\n+            totalChunkCount.getAndAdd(-1 * removed);\n+        }\n+    }\n+\n+    /**\n+     * Evicts chunks from the cache.\n+     */\n+    private void evictChunks(String streamSegmentName, long toRemoveCount) {\n+        val segmentReadIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        evictChunks(segmentReadIndex, toRemoveCount);\n+    }\n+\n+    /**\n+     * Evicts chunks from the cache.\n+     */\n+    private void evictChunks(SegmentReadIndex segmentReadIndex, long toRemoveCount) {\n+        // Increment generation.\n+        val previousGen = currentGeneration.getAndIncrement();\n+        val oldGen = oldestGeneration.get();\n+\n+        // Step 1 : Go through all entries once to record counts per each generation.\n+        TreeMap<Long, Integer> counts = new TreeMap<Long, Integer>();\n+        val iterator = segmentReadIndex.chunkIndex.entrySet().iterator();\n+        while (iterator.hasNext()) {\n+            val entry = iterator.next();\n+            long generation = entry.getValue().getGeneration();\n+            val cnt = counts.get(generation);\n+            if (null == cnt) {\n+                counts.put(generation, 1);\n+            } else {\n+                counts.put(generation, cnt + 1);\n+            }\n+        }\n+\n+        // Step 2 : Determine upto what generation to delete.\n+        long deletedUpToGen = 0;\n+        int runningCount = 0;\n+        for (val entry : counts.entrySet()) {\n+            runningCount += entry.getValue();\n+            deletedUpToGen = entry.getKey();\n+            if (runningCount >= toRemoveCount) {\n+                break;\n+            }\n+        }\n+\n+        // Step 3: Now remove keys.\n+        int removed = 0;\n+        val iterator2 = segmentReadIndex.chunkIndex.entrySet().iterator();\n+        while (iterator2.hasNext() && removed < toRemoveCount) {\n+            val entry = iterator2.next();\n+            val gen = entry.getValue().getGeneration();\n+            if (gen <= deletedUpToGen) {\n+                iterator2.remove();\n+                removed++;\n+                counts.put(gen, counts.get(gen) - 1);\n+            }\n+            if (removed == toRemoveCount) {\n+                break;\n+            }\n+        }\n+\n+        if (removed > 0) {\n+            long newOldGenvalue = previousGen + 1;\n+            for (val entry : counts.entrySet()) {\n+                if (entry.getValue() != 0) {\n+                    newOldGenvalue = entry.getKey();\n+                    break;\n+                }\n+            }\n+            if (newOldGenvalue != oldGen) {\n+                oldestGeneration.compareAndSet(oldGen, newOldGenvalue);\n+            }\n+            totalChunkCount.getAndAdd(-1 * removed);\n+        }\n+    }\n+\n+    /**\n+     * Per segment read index.\n+     * The index contains mapping from start offset to entries containing chunk names.\n+     * The underlying data structure uses ConcurrentSkipListMap.\n+     * The generation is used to cleanup unused entries.\n+     */\n+    @Builder\n+    @Data\n+    private static class SegmentReadIndex {\n+        private final ConcurrentSkipListMap<Long, SegmentReadIndexEntry> chunkIndex;", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkyMzg5OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445923899", "bodyText": "It is thread safe . It is log(n) and already provides floor() method. It it standard library.\nIt is implemented with less locking (lock free).  This data structure is used by lots of threads.\nI think it's good enough for this purpose.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T01:36:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDE5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDQzMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445900430", "bodyText": "This class is not thread safe yet you are modifying this generation field from various different threads. Make it an AtomicLong (with appropriate getters and setters) or synchronize it.", "author": "andreipaduroiu", "createdAt": "2020-06-25T23:58:34Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.\n+ */\n+class ReadIndexCache {\n+\n+    /**\n+     * Max number of indexed segments to keep in cache.\n+     */\n+    private final int maxIndexedSegments;\n+\n+    /**\n+     * Max number of indexed chunks to keep in cache.\n+     */\n+    private final int maxIndexedChunks;\n+\n+    /**\n+     * Max number of indexed chunks to keep per segment in cache.\n+     */\n+    private final int maxIndexedChunksPerSegment;\n+\n+    /**\n+     * Current generation of cache entries.\n+     */\n+    private final AtomicLong currentGeneration = new AtomicLong();\n+\n+    /**\n+     * Lowest generation of cache entries.\n+     */\n+    private final AtomicLong oldestGeneration = new AtomicLong();\n+\n+    /**\n+     * Total number of chunks in the cache.\n+     */\n+    private final AtomicInteger totalChunkCount = new AtomicInteger();\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, SegmentReadIndex> segmentsToReadIndexMap = new ConcurrentHashMap<String, SegmentReadIndex>();\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param maxIndexedSegments\n+     * @param maxIndexedChunksPerSegment\n+     * @param maxIndexedChunks\n+     */\n+    public ReadIndexCache(int maxIndexedSegments, int maxIndexedChunksPerSegment, int maxIndexedChunks) {\n+        this.maxIndexedChunksPerSegment = maxIndexedChunksPerSegment;\n+        this.maxIndexedSegments = maxIndexedSegments;\n+        this.maxIndexedChunks = maxIndexedChunks;\n+    }\n+\n+    /**\n+     * Retrieves the read index for given segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @return Read index correpsonding to the given segment. A new empty index is created if it doesn't already exist.\n+     */\n+    private SegmentReadIndex getSegmentReadIndex(String streamSegmentName) {\n+        SegmentReadIndex readIndex;\n+        if (segmentsToReadIndexMap.containsKey(streamSegmentName)) {\n+            readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        } else {\n+            // Evict segments if required.\n+            if (maxIndexedSegments < segmentsToReadIndexMap.size() + 1 || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictSegmentsFromOldestGeneration();\n+            }\n+            val newReadIndex = SegmentReadIndex.builder()\n+                    .chunkIndex(new ConcurrentSkipListMap<Long, SegmentReadIndexEntry>())\n+                    .generation(currentGeneration.get())\n+                    .build();\n+            val oldReadIndex = segmentsToReadIndexMap.putIfAbsent(streamSegmentName, newReadIndex);\n+            readIndex = null != oldReadIndex ? oldReadIndex : newReadIndex;\n+        }\n+\n+        return readIndex;\n+    }\n+\n+    /**\n+     * Gets total number of chunks in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalChunksCount() {\n+        return totalChunkCount.get();\n+    }\n+\n+    /**\n+     * Gets total number of segments in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalSegmentCount() {\n+        return segmentsToReadIndexMap.size();\n+    }\n+\n+    /**\n+     * Gets oldest generation in cache.\n+     *\n+     * @return\n+     */\n+    public long getOldestGeneration() {\n+        return oldestGeneration.get();\n+    }\n+\n+    /**\n+     * Gets current generation of cache.\n+     *\n+     * @return\n+     */\n+    public long getCurrentGeneration() {\n+        return currentGeneration.get();\n+    }\n+\n+    /**\n+     * Adds a new index entry for a given chunk in index for the segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param chunkName         Name of the chunk.\n+     * @param startOffset       Start offset of the chunk.\n+     */\n+    public void addIndexEntry(String streamSegmentName, String chunkName, long startOffset) {\n+        if (null != chunkName) {\n+            val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+\n+            // Evict chunks if required.\n+            if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + 1\n+                    || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictChunks(streamSegmentName, 1);\n+            }\n+\n+            segmentReadIndex.chunkIndex.put(startOffset,\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(chunkName)\n+                            .generation(currentGeneration.get())\n+                            .build());\n+            segmentReadIndex.setGeneration(currentGeneration.get());\n+            totalChunkCount.incrementAndGet();\n+        }\n+    }\n+\n+    /**\n+     * Updates read index for given segment with new entries.\n+     *\n+     * @param streamSegmentName\n+     * @param newReadIndexEntries List of {@link ChunkNameOffsetPair} for new entries.\n+     */\n+    public void addIndexEntries(String streamSegmentName, List<ChunkNameOffsetPair> newReadIndexEntries) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        // Evict chunks if required.\n+        if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + newReadIndexEntries.size()\n+                || maxIndexedChunks < totalChunkCount.get() + newReadIndexEntries.size()) {\n+            evictChunks(streamSegmentName, newReadIndexEntries.size());\n+        }\n+\n+        for (val entry : newReadIndexEntries) {\n+            segmentReadIndex.chunkIndex.put(entry.getOffset(),\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(entry.getChunkName())\n+                            .generation(currentGeneration.get()).build());\n+            segmentReadIndex.setGeneration(currentGeneration.get());\n+\n+        }\n+        totalChunkCount.getAndAdd(newReadIndexEntries.size());\n+    }\n+\n+    /**\n+     * Removes the given segment from cache.\n+     *\n+     * @param streamSegmentName\n+     */\n+    public void remove(String streamSegmentName) {\n+        val readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        if (null != readIndex) {\n+            segmentsToReadIndexMap.remove(streamSegmentName);\n+            totalChunkCount.getAndAdd(-1 * readIndex.chunkIndex.size());\n+        }\n+    }\n+\n+    /**\n+     * Finds a chunk that is floor to the given offset.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset for which to search.\n+     * @return\n+     */\n+    public ChunkNameOffsetPair findFloor(String streamSegmentName, long offset) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        if (segmentReadIndex.chunkIndex.size() > 0) {\n+            val floorEntry = segmentReadIndex.chunkIndex.floorEntry(offset);\n+            if (null != floorEntry) {\n+                // mark with current generation\n+                segmentReadIndex.setGeneration(currentGeneration.get());\n+                floorEntry.getValue().setGeneration(currentGeneration.get());\n+                // return value.\n+                return new ChunkNameOffsetPair(floorEntry.getKey(), floorEntry.getValue().getChunkName());\n+            }\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Truncate the read index for given segment by removing all the chunks that are below given offset.\n+     *\n+     * @param streamSegmentName\n+     * @param startOffset\n+     */\n+    public void truncateReadIndex(String streamSegmentName, long startOffset) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        if (null != segmentReadIndex) {\n+            if (segmentReadIndex.chunkIndex.size() > 0) {\n+                val headMap = segmentReadIndex.chunkIndex.headMap(startOffset);\n+                if (null != headMap) {\n+                    int removed = 0;\n+                    ArrayList<Long> keysToRemove = new ArrayList<Long>();\n+                    keysToRemove.addAll(headMap.keySet());\n+                    for (val keyToRemove : keysToRemove) {\n+                        segmentReadIndex.chunkIndex.remove(keyToRemove);\n+                        removed++;\n+                    }\n+                    if (removed > 0) {\n+                        totalChunkCount.getAndAdd(-1 * removed);\n+                    }\n+                }\n+            }\n+            if (segmentReadIndex.chunkIndex.size() > 0) {\n+                segmentReadIndex.setGeneration(currentGeneration.get());\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Evicts segments from the cache.\n+     */\n+    private void evictSegmentsFromOldestGeneration() {\n+        val oldGen = oldestGeneration.get();\n+        currentGeneration.getAndIncrement();\n+\n+        val iterator = segmentsToReadIndexMap.entrySet().iterator();\n+        int total = totalChunkCount.get();\n+        int removed = 0;\n+        while (iterator.hasNext() && (segmentsToReadIndexMap.size() >= maxIndexedSegments || total >= maxIndexedChunks)) {\n+            val entry = iterator.next();\n+            if (entry.getValue().getGeneration() <= oldGen) {\n+                val size = entry.getValue().chunkIndex.size();\n+                removed += size;\n+                total -= size;\n+                iterator.remove();\n+            }\n+        }\n+        if (removed > 0) {\n+            oldestGeneration.compareAndSet(oldGen, oldGen + 1);\n+            totalChunkCount.getAndAdd(-1 * removed);\n+        }\n+    }\n+\n+    /**\n+     * Evicts chunks from the cache.\n+     */\n+    private void evictChunks(String streamSegmentName, long toRemoveCount) {\n+        val segmentReadIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        evictChunks(segmentReadIndex, toRemoveCount);\n+    }\n+\n+    /**\n+     * Evicts chunks from the cache.\n+     */\n+    private void evictChunks(SegmentReadIndex segmentReadIndex, long toRemoveCount) {\n+        // Increment generation.\n+        val previousGen = currentGeneration.getAndIncrement();\n+        val oldGen = oldestGeneration.get();\n+\n+        // Step 1 : Go through all entries once to record counts per each generation.\n+        TreeMap<Long, Integer> counts = new TreeMap<Long, Integer>();\n+        val iterator = segmentReadIndex.chunkIndex.entrySet().iterator();\n+        while (iterator.hasNext()) {\n+            val entry = iterator.next();\n+            long generation = entry.getValue().getGeneration();\n+            val cnt = counts.get(generation);\n+            if (null == cnt) {\n+                counts.put(generation, 1);\n+            } else {\n+                counts.put(generation, cnt + 1);\n+            }\n+        }\n+\n+        // Step 2 : Determine upto what generation to delete.\n+        long deletedUpToGen = 0;\n+        int runningCount = 0;\n+        for (val entry : counts.entrySet()) {\n+            runningCount += entry.getValue();\n+            deletedUpToGen = entry.getKey();\n+            if (runningCount >= toRemoveCount) {\n+                break;\n+            }\n+        }\n+\n+        // Step 3: Now remove keys.\n+        int removed = 0;\n+        val iterator2 = segmentReadIndex.chunkIndex.entrySet().iterator();\n+        while (iterator2.hasNext() && removed < toRemoveCount) {\n+            val entry = iterator2.next();\n+            val gen = entry.getValue().getGeneration();\n+            if (gen <= deletedUpToGen) {\n+                iterator2.remove();\n+                removed++;\n+                counts.put(gen, counts.get(gen) - 1);\n+            }\n+            if (removed == toRemoveCount) {\n+                break;\n+            }\n+        }\n+\n+        if (removed > 0) {\n+            long newOldGenvalue = previousGen + 1;\n+            for (val entry : counts.entrySet()) {\n+                if (entry.getValue() != 0) {\n+                    newOldGenvalue = entry.getKey();\n+                    break;\n+                }\n+            }\n+            if (newOldGenvalue != oldGen) {\n+                oldestGeneration.compareAndSet(oldGen, newOldGenvalue);\n+            }\n+            totalChunkCount.getAndAdd(-1 * removed);\n+        }\n+    }\n+\n+    /**\n+     * Per segment read index.\n+     * The index contains mapping from start offset to entries containing chunk names.\n+     * The underlying data structure uses ConcurrentSkipListMap.\n+     * The generation is used to cleanup unused entries.\n+     */\n+    @Builder\n+    @Data\n+    private static class SegmentReadIndex {\n+        private final ConcurrentSkipListMap<Long, SegmentReadIndexEntry> chunkIndex;\n+        private long generation;\n+    }\n+\n+    /**\n+     * Entry for chunk in a segment read index.\n+     * The generation is used to cleanup unused entries.\n+     */\n+    @Builder\n+    @Data\n+    private static class SegmentReadIndexEntry {\n+        private final String chunkName;\n+        private long generation;", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDUxNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445900514", "bodyText": "Same with the class above this.", "author": "andreipaduroiu", "createdAt": "2020-06-25T23:58:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDQzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk2MjYxNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445962617", "bodyText": "This was on purpose.\nThe left side of the assignment was atomic long. What's the probability of thread getting suspended there and also has been like that for the long time for generation value to matter.\nIn worst case, we have early eviction of object. Okay to have that if its rarely , but otherwise  causes overhead.\nWell - that was the thinking.\nBut I fixed it now with atomic anyway.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T04:35:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDQzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAzMzQ2Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447033467", "bodyText": "2 threads accessing the same object concurrently is not the only thing you need to worry about in Java. If you set a value to a field, that value may be visible to the current thread, but may not be immediately visible to other threads, even if subsequent threads access the object after the original thread is done with it. You need to use one of the Atomic* classes or a mutex (i.e., synchronized) to ensure the memory location for that field has been properly set.", "author": "andreipaduroiu", "createdAt": "2020-06-29T14:53:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDQzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAwNTA4MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r448005080", "bodyText": "Tracking this issue here #4902 This will be done as a separate PR.", "author": "sachin-j-joshi", "createdAt": "2020-06-30T22:03:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDQzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDg1MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445900851", "bodyText": "These fields are mutable. Is this class thread safe? Is it passed around different threads?", "author": "andreipaduroiu", "createdAt": "2020-06-25T23:59:56Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/ChunkMetadata.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Represents chunk metadata.\n+ * Following metadata is stored.\n+ * <ul>\n+ * <li>Name of the chunk.</li>\n+ * <li>Length of the chunk.</li>\n+ * <li>Name of the next chunk in list.</li>\n+ * </ul>\n+ */\n+@Builder(toBuilder = true)\n+@Data\n+@EqualsAndHashCode(callSuper = true)\n+public class ChunkMetadata extends StorageMetadata {\n+    /**\n+     * Name of this chunk.\n+     */\n+    private final String name;\n+\n+    /**\n+     * Length of the chunk.\n+     */\n+    private long length;", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkyNDcxMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445924713", "bodyText": "It is modified by only one thread.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T01:40:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDg1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAzNDUxMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447034511", "bodyText": "Then mark it as @NotThreadSafe. Same with all your other classes that are not thread safe.\nSimilarly, mark your thread-safe classes with @ThreadSafe.", "author": "andreipaduroiu", "createdAt": "2020-06-29T14:55:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDg1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA3NzY2NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r449077664", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-07-02T15:19:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDg1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMTYxNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445901617", "bodyText": "Why does this have a setter?", "author": "andreipaduroiu", "createdAt": "2020-06-26T00:02:45Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,943 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import lombok.var;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static io.pravega.segmentstore.storage.metadata.StorageMetadata.fromNullableString;\n+import static io.pravega.segmentstore.storage.metadata.StorageMetadata.toNullableString;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This creates a circular dependency while reading or writing the data about these segments from the metadata segments.\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    /**\n+     * Serializer for {@link SystemJournalRecordBatch}.\n+     */\n+    private static final SystemJournalRecordBatch.SystemJournalRecordBatchSerializer BATCH_SERIALIZER = new SystemJournalRecordBatch.SystemJournalRecordBatchSerializer();\n+\n+    /**\n+     * Serializer for {@link SystemSnapshotRecord}.\n+     */\n+    private static final SystemSnapshotRecord.Serializer SYSTEM_SNAPSHOT_SERIALIZER = new SystemSnapshotRecord.Serializer();\n+\n+    private final Object lock = new Object();\n+\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    @Getter\n+    private final ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private final long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private final int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    @Setter\n+    private String systemSegmentsPrefix;", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk2MTQ3MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445961471", "bodyText": "It was for tests. Fixed tests", "author": "sachin-j-joshi", "createdAt": "2020-06-26T04:30:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMTYxNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAzNTAzNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447035034", "bodyText": "It still has a setter.", "author": "andreipaduroiu", "createdAt": "2020-06-29T14:55:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMTYxNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1MjUwMg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447052502", "bodyText": "No it is fixed 90c2dd4", "author": "sachin-j-joshi", "createdAt": "2020-06-29T15:20:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMTYxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMTc4OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445901789", "bodyText": "It's best if you keep this collection internal and add APIs to mutate it. Otherwise external classes will need to know the internals of this class which can lead to trouble.", "author": "andreipaduroiu", "createdAt": "2020-06-26T00:03:27Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,943 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import lombok.var;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static io.pravega.segmentstore.storage.metadata.StorageMetadata.fromNullableString;\n+import static io.pravega.segmentstore.storage.metadata.StorageMetadata.toNullableString;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This creates a circular dependency while reading or writing the data about these segments from the metadata segments.\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    /**\n+     * Serializer for {@link SystemJournalRecordBatch}.\n+     */\n+    private static final SystemJournalRecordBatch.SystemJournalRecordBatchSerializer BATCH_SERIALIZER = new SystemJournalRecordBatch.SystemJournalRecordBatchSerializer();\n+\n+    /**\n+     * Serializer for {@link SystemSnapshotRecord}.\n+     */\n+    private static final SystemSnapshotRecord.Serializer SYSTEM_SNAPSHOT_SERIALIZER = new SystemSnapshotRecord.Serializer();\n+\n+    private final Object lock = new Object();\n+\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    @Getter\n+    private final ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private final long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private final int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    @Setter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    @Setter\n+    private String[] systemSegments;", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk2MTYwNg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445961606", "bodyText": "it was for tests. Fixed tests", "author": "sachin-j-joshi", "createdAt": "2020-06-26T04:31:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMTc4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAzNTIxNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447035214", "bodyText": "It still is exposed.", "author": "andreipaduroiu", "createdAt": "2020-06-29T14:56:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMTc4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1MjM0Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447052347", "bodyText": "No it is fixed 90c2dd4", "author": "sachin-j-joshi", "createdAt": "2020-06-29T15:19:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMTc4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMTk4OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445901989", "bodyText": "Collections.singletonList", "author": "andreipaduroiu", "createdAt": "2020-06-26T00:04:12Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,943 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import lombok.var;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static io.pravega.segmentstore.storage.metadata.StorageMetadata.fromNullableString;\n+import static io.pravega.segmentstore.storage.metadata.StorageMetadata.toNullableString;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This creates a circular dependency while reading or writing the data about these segments from the metadata segments.\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    /**\n+     * Serializer for {@link SystemJournalRecordBatch}.\n+     */\n+    private static final SystemJournalRecordBatch.SystemJournalRecordBatchSerializer BATCH_SERIALIZER = new SystemJournalRecordBatch.SystemJournalRecordBatchSerializer();\n+\n+    /**\n+     * Serializer for {@link SystemSnapshotRecord}.\n+     */\n+    private static final SystemSnapshotRecord.Serializer SYSTEM_SNAPSHOT_SERIALIZER = new SystemSnapshotRecord.Serializer();\n+\n+    private final Object lock = new Object();\n+\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    @Getter\n+    private final ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private final long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private final int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    @Setter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    @Setter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Configuration {@link ChunkManagerConfig} for the {@link ChunkManager}.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    private final AtomicBoolean reentryGuard = new AtomicBoolean();\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId   Container id of the owner container.\n+     * @param epoch         Epoch of the current container instance.\n+     * @param chunkStorage  ChunkStorage instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, ChunkManagerConfig config) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param record Record to persist.\n+     * @throws ChunkStorageException Exception if any.\n+     */\n+    public void commitRecord(SystemJournalRecord record) throws ChunkStorageException {\n+        commitRecords(Arrays.asList(record));", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk2MTE0MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445961141", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T04:29:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMTk4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMjg1Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445902857", "bodyText": "Is this the most appropriate data structure?", "author": "andreipaduroiu", "createdAt": "2020-06-26T00:07:34Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -0,0 +1,635 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import javax.annotation.concurrent.GuardedBy;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements base metadata store that provides core functionality of metadata store by encapsulating underlying key value store.\n+ * Derived classes override {@link BaseMetadataStore#read(String)} and {@link BaseMetadataStore#writeAll(Collection)} to write to underlying storage.\n+ * The minimum requirement for underlying key-value store is to provide optimistic concurrency ( Eg. using versions numbers or etags.)\n+ *\n+ *\n+ * Within a segment store instance there should be only one instance that exclusively writes to the underlying key value store.\n+ * For distributed systems the single writer pattern must be enforced through external means. (Eg. for table segment based implementation\n+ * DurableLog's native fencing is used to establish ownership and single writer pattern.)\n+ *\n+ * This implementation provides following features that simplify metadata management.\n+ *\n+ * All access to and modifications to the metadata the {@link ChunkMetadataStore} must be done through a transaction.\n+ *\n+ * A transaction is created by calling {@link ChunkMetadataStore#beginTransaction()}\n+ *\n+ * Changes made to metadata inside a transaction are not visible until a transaction is committed using any overload of{@link MetadataTransaction#commit()}.\n+ * Transaction is aborted automatically unless committed or when {@link MetadataTransaction#abort()} is called.\n+ * Transactions are atomic - either all changes in the transaction are committed or none at all.\n+ * In addition, Transactions provide snaphot isolation which means that transaction fails if any of the metadata records read during the transactions are changed outside the transaction after they were read.\n+ *\n+ * Within a transaction you can perform following actions on per record basis.\n+ * <ul>\n+ * <li>{@link MetadataTransaction#get(String)} Retrieves metadata using for given key.</li>\n+ * <li>{@link MetadataTransaction#create(StorageMetadata)} Creates a new record.</li>\n+ * <li>{@link MetadataTransaction#delete(String)} Deletes records for given key.</li>\n+ * <li>{@link MetadataTransaction#update(StorageMetadata)} Updates the transaction local copy of the record.\n+ * For each record modified inside the transaction update must be called to mark the record as dirty.</li>\n+ * </ul>\n+ * <div>\n+ * <pre>\n+ *  // Start a transaction.\n+ * try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+ *      // Retrieve the data from transaction\n+ *      SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+ *\n+ *      // Modify retrieved record\n+ *      // seal if it is not already sealed.\n+ *      segmentMetadata.setSealed(true);\n+ *\n+ *      // put it back transaction\n+ *      txn.update(segmentMetadata);\n+ *\n+ *      // Commit\n+ *      txn.commit();\n+ *  } catch (StorageMetadataException ex) {\n+ *      // Handle Exceptions\n+ *  }\n+ *  </pre>\n+ * </div>\n+ *\n+ * Underlying implementation might buffer frequently or recently updated metadata keys to optimize read/write performance.\n+ * To further optimize it may provide \"lazy committing\" of changes where there is application specific way to recover from failures.(Eg. when only length of chunk is changed.)\n+ * In this case {@link MetadataTransaction#commit(boolean)} can be called.Note that otherwise for each commit the data is written to underlying key-value store.\n+ *\n+ * There are two special methods provided to handle metadata about data segments for the underlying key-value store. They are useful in avoiding circular references.\n+ * <ul>\n+ * <li>A record marked as pinned by calling {@link MetadataTransaction#markPinned(StorageMetadata)} is never written to underlying storage.</li>\n+ * <li>In addition transaction can be committed using {@link MetadataTransaction#commit(boolean, boolean)} to skip validation step that reads any recently evicted changes from underlying storage.</li>\n+ * </ul>\n+ */\n+@Slf4j\n+abstract public class BaseMetadataStore implements ChunkMetadataStore {\n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    private static final int MAX_ENTRIES_IN_TXN_BUFFER = 5000;\n+\n+    /**\n+     * List of pinned keys. These keys should not be read or written.\n+     */\n+    private final ConcurrentSkipListSet<String> pinnedKeys = new ConcurrentSkipListSet<>();", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkyMzgwNg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445923806", "bodyText": "Removed. It was not needed any more.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T01:35:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMjg1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMzI5MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445903291", "bodyText": "Is this class thread safe? If more than one thread touches it during its lifetime, even if not concurrently, it must be made thread safe.", "author": "andreipaduroiu", "createdAt": "2020-06-26T00:09:17Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -0,0 +1,635 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import javax.annotation.concurrent.GuardedBy;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements base metadata store that provides core functionality of metadata store by encapsulating underlying key value store.\n+ * Derived classes override {@link BaseMetadataStore#read(String)} and {@link BaseMetadataStore#writeAll(Collection)} to write to underlying storage.\n+ * The minimum requirement for underlying key-value store is to provide optimistic concurrency ( Eg. using versions numbers or etags.)\n+ *\n+ *\n+ * Within a segment store instance there should be only one instance that exclusively writes to the underlying key value store.\n+ * For distributed systems the single writer pattern must be enforced through external means. (Eg. for table segment based implementation\n+ * DurableLog's native fencing is used to establish ownership and single writer pattern.)\n+ *\n+ * This implementation provides following features that simplify metadata management.\n+ *\n+ * All access to and modifications to the metadata the {@link ChunkMetadataStore} must be done through a transaction.\n+ *\n+ * A transaction is created by calling {@link ChunkMetadataStore#beginTransaction()}\n+ *\n+ * Changes made to metadata inside a transaction are not visible until a transaction is committed using any overload of{@link MetadataTransaction#commit()}.\n+ * Transaction is aborted automatically unless committed or when {@link MetadataTransaction#abort()} is called.\n+ * Transactions are atomic - either all changes in the transaction are committed or none at all.\n+ * In addition, Transactions provide snaphot isolation which means that transaction fails if any of the metadata records read during the transactions are changed outside the transaction after they were read.\n+ *\n+ * Within a transaction you can perform following actions on per record basis.\n+ * <ul>\n+ * <li>{@link MetadataTransaction#get(String)} Retrieves metadata using for given key.</li>\n+ * <li>{@link MetadataTransaction#create(StorageMetadata)} Creates a new record.</li>\n+ * <li>{@link MetadataTransaction#delete(String)} Deletes records for given key.</li>\n+ * <li>{@link MetadataTransaction#update(StorageMetadata)} Updates the transaction local copy of the record.\n+ * For each record modified inside the transaction update must be called to mark the record as dirty.</li>\n+ * </ul>\n+ * <div>\n+ * <pre>\n+ *  // Start a transaction.\n+ * try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+ *      // Retrieve the data from transaction\n+ *      SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+ *\n+ *      // Modify retrieved record\n+ *      // seal if it is not already sealed.\n+ *      segmentMetadata.setSealed(true);\n+ *\n+ *      // put it back transaction\n+ *      txn.update(segmentMetadata);\n+ *\n+ *      // Commit\n+ *      txn.commit();\n+ *  } catch (StorageMetadataException ex) {\n+ *      // Handle Exceptions\n+ *  }\n+ *  </pre>\n+ * </div>\n+ *\n+ * Underlying implementation might buffer frequently or recently updated metadata keys to optimize read/write performance.\n+ * To further optimize it may provide \"lazy committing\" of changes where there is application specific way to recover from failures.(Eg. when only length of chunk is changed.)\n+ * In this case {@link MetadataTransaction#commit(boolean)} can be called.Note that otherwise for each commit the data is written to underlying key-value store.\n+ *\n+ * There are two special methods provided to handle metadata about data segments for the underlying key-value store. They are useful in avoiding circular references.\n+ * <ul>\n+ * <li>A record marked as pinned by calling {@link MetadataTransaction#markPinned(StorageMetadata)} is never written to underlying storage.</li>\n+ * <li>In addition transaction can be committed using {@link MetadataTransaction#commit(boolean, boolean)} to skip validation step that reads any recently evicted changes from underlying storage.</li>\n+ * </ul>\n+ */\n+@Slf4j\n+abstract public class BaseMetadataStore implements ChunkMetadataStore {\n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    private static final int MAX_ENTRIES_IN_TXN_BUFFER = 5000;\n+\n+    /**\n+     * List of pinned keys. These keys should not be read or written.\n+     */\n+    private final ConcurrentSkipListSet<String> pinnedKeys = new ConcurrentSkipListSet<>();\n+\n+    /**\n+     * Lock for synchronization.\n+     */\n+    private final Object lock = new Object();\n+\n+    /**\n+     * Indicates whether this instance is fenced or not.\n+     */\n+    private final AtomicBoolean fenced;\n+\n+    /**\n+     * Monotonically increasing number. Keeps track of versions independent of external persistence or transaction mechanism.\n+     */\n+    private final AtomicLong version;\n+\n+    /**\n+     * Buffer for reading and writing transaction data entries to underlying KV store.\n+     * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     */\n+    @GuardedBy(\"lock\")\n+    private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n+\n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n+\n+    /**\n+     * Constructs a BaseMetadataStore object.\n+     */\n+    public BaseMetadataStore() {\n+        version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n+        fenced = new AtomicBoolean(false);\n+        bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+    }\n+\n+    /**\n+     * Begins a new transaction.\n+     *\n+     * @return Returns a new instance of MetadataTransaction.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @Override\n+    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n+        // Each transaction gets a unique number which is monotinically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet());\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     *\n+     * @param txn       transaction to commit.\n+     * @param lazyWrite true if data can be written lazily.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n+        commit(txn, lazyWrite, false);\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     *\n+     * @param txn transaction to commit.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n+        commit(txn, false, false);\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     *\n+     * @param txn       transaction to commit.\n+     * @param lazyWrite true if data can be written lazily.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+        Preconditions.checkArgument(null != txn);\n+        if (fenced.get()) {\n+            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n+        }\n+\n+        Map<String, TransactionData> txnData = txn.getData();\n+\n+        ArrayList<String> modifiedKeys = new ArrayList<>();\n+        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+\n+        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+        // This step is kind of thread safe\n+        for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+            String key = entry.getKey();\n+            if (skipStoreCheck || entry.getValue().isPinned()) {\n+                log.trace(\"Skipping loading key from the store key = {}\", key);\n+            } else {\n+                // This check is safe to be outside the lock\n+                if (!bufferedTxnData.containsKey(key)) {\n+                    loadFromStore(key);\n+                }\n+            }\n+        }\n+\n+        // Step 2 : Check whether transaction is safe to commit.\n+        // This check needs to be atomic, with absolutely no possibility of re-entry\n+        synchronized (lock) {\n+            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                String key = entry.getKey();\n+                val transactionData = entry.getValue();\n+                Preconditions.checkState(null != transactionData.getKey());\n+\n+                // See if this entry was modified in this transaction.\n+                if (transactionData.getVersion() == txn.getVersion()) {\n+                    modifiedKeys.add(key);\n+                    transactionData.setPersisted(false);\n+                    modifiedValues.add(transactionData);\n+                }\n+                // make sure none of the keys used in this transaction have changed.\n+                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n+                if (null != dataFromBuffer) {\n+                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n+                        throw new StorageMetadataVersionMismatchException(\n+                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n+                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+                    }\n+\n+                    // Pin it if it is already pinned.\n+                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+\n+                    // Set the database object.\n+                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n+                }\n+            }\n+\n+            // Step 3: Commit externally.\n+            // This operation may call external storage.\n+            if (!lazyWrite || (bufferedTxnData.size() > maxEntriesInTxnBuffer)) {\n+                log.trace(\"Persisting all modified keys (except pinned)\");\n+                val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+                writeAll(toWriteList);\n+                log.trace(\"Done persisting all modified keys\");\n+\n+                // Mark written keys as persisted.\n+                for (val writtenData : toWriteList) {\n+                    writtenData.setPersisted(true);\n+                }\n+            }\n+\n+            // Execute external commit step.\n+            try {\n+                if (null != txn.getExternalCommitStep()) {\n+                    txn.getExternalCommitStep().call();\n+                }\n+            } catch (Exception e) {\n+                log.error(\"Exception during execution of external commit step\", e);\n+                throw new StorageMetadataException(\"Exception during execution of external commit step\", e);\n+            }\n+\n+            // If we reach here then it means transaction is safe to commit.\n+            // Step 4: Insert\n+            long committedVersion = version.incrementAndGet();\n+            HashMap<String, TransactionData> toAdd = new HashMap<String, TransactionData>();\n+            for (String key : modifiedKeys) {\n+                TransactionData data = txnData.get(key);\n+                data.setVersion(committedVersion);\n+                toAdd.put(key, data);\n+            }\n+            bufferedTxnData.putAll(toAdd);\n+        }\n+\n+        //  Step 5 : evict if required.\n+        if (bufferedTxnData.size() > maxEntriesInTxnBuffer) {\n+            bufferedTxnData.entrySet().removeIf(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned());\n+        }\n+\n+        //  Step 6: finally clear\n+        txnData.clear();\n+    }\n+\n+    /**\n+     * Aborts given transaction.\n+     *\n+     * @param txn transaction to abort.\n+     * @throws StorageMetadataException If there are any errors.\n+     */\n+    public void abort(MetadataTransaction txn) throws StorageMetadataException {\n+        // Do nothing\n+    }\n+\n+    /**\n+     * Retrieves the metadata for given key.\n+     *\n+     * @param txn Transaction.\n+     * @param key key to use to retrieve metadata.\n+     * @return Metadata for given key. Null if key was not found.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @Override\n+    public StorageMetadata get(MetadataTransaction txn, String key) throws StorageMetadataException {\n+        Preconditions.checkArgument(null != txn);\n+        TransactionData dataFromBuffer = null;\n+        if (null == key) {\n+            return null;\n+        }\n+\n+        Map<String, TransactionData> txnData = txn.getData();\n+        TransactionData data = txnData.get(key);\n+\n+        // Search in the buffer.\n+        if (null == data) {\n+            synchronized (lock) {\n+                dataFromBuffer = bufferedTxnData.get(key);\n+            }\n+            // If we did not find in buffer then load it from store\n+            if (null == dataFromBuffer) {\n+                // NOTE: This call to read MUST be outside the lock, it is most likely cause re-entry.\n+                loadFromStore(key);\n+                dataFromBuffer = bufferedTxnData.get(key);\n+                Preconditions.checkState(null != dataFromBuffer);\n+            }\n+\n+            if (null != dataFromBuffer && null != dataFromBuffer.getValue()) {\n+                // Make copy.\n+                data = dataFromBuffer.toBuilder()\n+                        .key(key)\n+                        .value(dataFromBuffer.getValue().deepCopy())\n+                        .build();\n+                txnData.put(key, data);\n+            }\n+        }\n+\n+        if (data != null) {\n+            return data.getValue();\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Loads value from store\n+     *\n+     * @param key Key to load\n+     * @return Value if found null otherwise.\n+     * @throws StorageMetadataException Any exceptions.\n+     */\n+    private TransactionData loadFromStore(String key) throws StorageMetadataException {\n+        // NOTE: This call to read MUST be outside the lock, it is most likely cause re-entry.\n+        log.trace(\"Loading key from the store key = {}\", key);\n+        TransactionData fromDb = read(key);\n+        Preconditions.checkState(null != fromDb);\n+        log.trace(\"Done Loading key from the store key = {}\", key);\n+\n+        TransactionData copyForBuffer = fromDb.toBuilder()\n+                .key(key)\n+                .build();\n+\n+        if (null != fromDb.getValue()) {\n+            Preconditions.checkState(0 != fromDb.getVersion(), \"Version is not initialized\");\n+            // Make sure it is a deep copy.\n+            copyForBuffer.setValue(fromDb.getValue().deepCopy());\n+        }\n+        // Put this value in bufferedTxnData buffer.\n+        synchronized (lock) {\n+            // If some other transaction beat us then use that value.\n+            TransactionData oldValue = bufferedTxnData.putIfAbsent(key, copyForBuffer);\n+            if (oldValue != null) {\n+                copyForBuffer = oldValue;\n+            }\n+        }\n+        return copyForBuffer;\n+    }\n+\n+    /**\n+     * Reads a metadata record for the given key.\n+     *\n+     * @param key Key for the metadata record.\n+     * @return Associated {@link io.pravega.segmentstore.storage.metadata.BaseMetadataStore.TransactionData}.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    abstract protected TransactionData read(String key) throws StorageMetadataException;\n+\n+    /**\n+     * Writes transaction data from a given list to the metadata store.\n+     *\n+     * @param dataList List of transaction data to write.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    abstract protected void writeAll(Collection<TransactionData> dataList) throws StorageMetadataException;\n+\n+    /**\n+     * Updates existing metadata.\n+     *\n+     * @param txn      Transaction.\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @Override\n+    public void update(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException {\n+        Preconditions.checkArgument(null != txn);\n+        Preconditions.checkArgument(null != metadata);\n+        Preconditions.checkArgument(null != metadata.getKey());\n+        Map<String, TransactionData> txnData = txn.getData();\n+\n+        String key = metadata.getKey();\n+\n+        TransactionData data = TransactionData.builder().key(key).build();\n+        TransactionData oldData = txnData.putIfAbsent(key, data);\n+        if (null != oldData) {\n+            data = oldData;\n+        }\n+        data.setValue(metadata);\n+        data.setPersisted(false);\n+        Preconditions.checkState(txn.getVersion() >= data.getVersion());\n+        data.setVersion(txn.getVersion());\n+    }\n+\n+    /**\n+     * Marks given record as pinned.\n+     *\n+     * @param txn      Transaction.\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @Override\n+    public void markPinned(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException {\n+        Preconditions.checkArgument(null != txn);\n+        Preconditions.checkArgument(null != metadata);\n+        Map<String, TransactionData> txnData = txn.getData();\n+        String key = metadata.getKey();\n+\n+        TransactionData data = TransactionData.builder().key(key).build();\n+        TransactionData oldData = txnData.putIfAbsent(key, data);\n+        if (null != oldData) {\n+            data = oldData;\n+        }\n+\n+        data.setValue(metadata);\n+        data.setPinned(true);\n+        data.setVersion(txn.getVersion());\n+\n+        pinnedKeys.add(metadata.getKey());\n+    }\n+\n+    /**\n+     * Creates a new metadata record.\n+     *\n+     * @param txn      Transaction.\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @Override\n+    public void create(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException {\n+        Preconditions.checkArgument(null != txn);\n+        Preconditions.checkArgument(null != metadata);\n+        Preconditions.checkArgument(null != metadata.getKey());\n+        Map<String, TransactionData> txnData = txn.getData();\n+        txnData.put(metadata.getKey(), TransactionData.builder()\n+                .key(metadata.getKey())\n+                .value(metadata)\n+                .version(txn.getVersion())\n+                .build());\n+    }\n+\n+    /**\n+     * Deletes a metadata record given the key.\n+     *\n+     * @param txn Transaction.\n+     * @param key key to use to retrieve metadata.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @Override\n+    public void delete(MetadataTransaction txn, String key) throws StorageMetadataException {\n+        Preconditions.checkArgument(null != txn);\n+        Preconditions.checkArgument(null != key);\n+        Map<String, TransactionData> txnData = txn.getData();\n+\n+        TransactionData data = TransactionData.builder().key(key).build();\n+        TransactionData oldData = txnData.putIfAbsent(key, data);\n+        if (null != oldData) {\n+            data = oldData;\n+        }\n+        data.setValue(null);\n+        data.setPersisted(false);\n+        data.setVersion(txn.getVersion());\n+    }\n+\n+    /**\n+     * {@link AutoCloseable#close()} implementation.\n+     */\n+    @Override\n+    public void close() throws Exception {\n+        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        bufferedTxnData.entrySet().stream().filter(entry -> !entry.getValue().isPersisted() && !entry.getValue().isPinned()).forEach(entry -> modifiedValues.add(entry.getValue()));\n+        if (modifiedValues.size() > 0) {\n+            writeAll(modifiedValues);\n+        }\n+    }\n+\n+    /**\n+     * Explicitly marks the store as fenced.\n+     * Once marked fenced no modifications to data should be allowed.\n+     */\n+    public void markFenced() {\n+        this.fenced.set(true);\n+    }\n+\n+    /**\n+     * Return whether record for the given key is pinned or not.\n+     *\n+     * @param key Key to check.\n+     * @return True if record for the given key is pinned, false otherwise.\n+     */\n+    protected boolean isPinnedKey(String key) {\n+        return pinnedKeys.contains(key);\n+    }\n+\n+    /**\n+     * Retrieves the current version number.\n+     *\n+     * @return current version number.\n+     */\n+    protected long getVersion() {\n+        return version.get();\n+    }\n+\n+    /**\n+     * Sets the current version number.\n+     *\n+     * @param version Version to set.\n+     */\n+    protected void setVersion(long version) {\n+        this.version.set(version);\n+    }\n+\n+    /**\n+     * Stores the transaction data.\n+     */\n+    @Builder(toBuilder = true)\n+    @Data\n+    public static class TransactionData implements Serializable {", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkyMDUzMg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445920532", "bodyText": "This class is thread safe. Instance of these objects are not shared between transactions.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T01:21:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMzI5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMzg3MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445903871", "bodyText": "Check out Guava's Strings class. I'm sure at least one of these methods is in there.", "author": "andreipaduroiu", "createdAt": "2020-06-26T00:11:35Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/StorageMetadata.java", "diffHunk": "@@ -0,0 +1,87 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkManager;\n+import io.pravega.segmentstore.storage.mocks.MockStorageMetadata;\n+import lombok.Data;\n+\n+import java.io.Serializable;\n+\n+/**\n+ * Storage Metadata.\n+ * All storage related metadata is stored in {@link ChunkMetadataStore} using set of key-value pairs.\n+ * The String value returned by {@link StorageMetadata#getKey()} is used as key.\n+ * Notable derived classes are {@link SegmentMetadata} and {@link ChunkMetadata} which form the core of metadata related\n+ * to {@link ChunkManager} functionality.\n+ */\n+@Data\n+public abstract class StorageMetadata implements Serializable {\n+\n+    /**\n+     * Retrieves the key associated with the metadata.\n+     *\n+     * @return key.\n+     */\n+    public abstract String getKey();\n+\n+    /**\n+     * Creates a deep copy of this instance.\n+     *\n+     * @return A deep copy of this instance.\n+     */\n+    public abstract StorageMetadata deepCopy();\n+\n+    /**\n+     * Helper method that converts empty string to null value.\n+     *\n+     * @param toConvert String to convert.\n+     * @return If toConvert is null then it returns empty string. Otherwise returns original string.\n+     */\n+    public static String toNullableString(String toConvert) {\n+        if (toConvert.length() == 0) {\n+            return null;\n+        }\n+        return toConvert;\n+    }\n+\n+    /**\n+     * Helper method that converts null value to empty string.\n+     *\n+     * @param toConvert String to convert.\n+     * @return If toConvert is null then it returns empty string. Otherwise returns original string.\n+     */\n+    public static String fromNullableString(String toConvert) {", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk0MDg4Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445940887", "bodyText": "Nice.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T02:50:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMzg3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwNDkxMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445904913", "bodyText": "Why not put it in _system/containers? At least that way we'll keep all container-related stuff together.", "author": "andreipaduroiu", "createdAt": "2020-06-26T00:15:42Z", "path": "shared/protocol/src/main/java/io/pravega/shared/NameUtils.java", "diffHunk": "@@ -68,11 +68,26 @@\n      */\n     private static final String EPOCH_DELIMITER = \".#epoch.\";\n \n+    /**\n+     * Format for chunk name with segment name , epoch and offset.\n+     */\n+    private static final String CHUNK_NAME_FORMAT_WITH_EPOCH_OFFSET = \"%s.E-%d-O-%d.%s\";\n+\n     /**\n      * Format for Container Metadata Segment name.\n      */\n     private static final String METADATA_SEGMENT_NAME_FORMAT = \"_system/containers/metadata_%d\";\n \n+    /**\n+     * Format for Storage Metadata Segment name.\n+     */\n+    private static final String STORAGE_METADATA_SEGMENT_NAME_FORMAT = \"_system/containers/storage_metadata_%d\";\n+\n+    /**\n+     * Format for Container System Journal file name.\n+     */\n+    private static final String SYSJOURNAL_NAME_FORMAT = \"_sysjournal.epoch%d.container%d.file%d\";", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkyMDY1Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445920653", "bodyText": "ok", "author": "sachin-j-joshi", "createdAt": "2020-06-26T01:22:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwNDkxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk2MTExNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445961117", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T04:28:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwNDkxMw=="}], "type": "inlineReview"}, {"oid": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "url": "https://github.com/pravega/pravega/commit/90c2dd483f8507bb02048504a166fe71ae6a82ae", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Cleanup.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-06-26T04:28:02Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjA5NzU0Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446097542", "bodyText": "I'm not sure I understand the implication of this evaluating always to true. Why is it correct to implement it this way independent of the underlying storage?", "author": "fpj", "createdAt": "2020-06-26T10:17:17Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage ChunkStorage instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage  ChunkStorage instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkManager and bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = new SystemJournal(containerId,\n+                epoch,\n+                chunkStorage,\n+                metadataStore,\n+                config);\n+\n+        // Now bootstrap\n+        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        this.systemJournal.bootstrap();\n+        log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                checkNotSealed(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // Validate that offset is correct.\n+                if ((segmentMetadata.getLength()) != offset) {\n+                    throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), offset);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                // Get the last chunk segmentMetadata for the segment.\n+                if (null != segmentMetadata.getLastChunk()) {\n+                    lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                }\n+\n+                while (bytesRemaining > 0) {\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        String previousLastChunkName = lastChunkMetadata == null ? null : lastChunkMetadata.getName();\n+\n+                        // update first and last chunks.\n+                        lastChunkMetadata = updateMetadataForChunkAddition(txn,\n+                                segmentMetadata,\n+                                newChunkName,\n+                                isFirstWriteAfterFailover,\n+                                lastChunkMetadata);\n+\n+                        // Record the creation of new chunk.\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    previousLastChunkName,\n+                                    newChunkName);\n+                            txn.markPinned(lastChunkMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        isFirstWriteAfterFailover = false;\n+                        didSegmentLayoutChange = true;\n+                        chunksAddedCount++;\n+\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\",\n+                                logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int writeSize = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+\n+                    // Write data to last chunk.\n+                    int bytesWritten = writeToChunk(txn,\n+                            segmentMetadata,\n+                            offset,\n+                            data,\n+                            chunkHandle,\n+                            lastChunkMetadata,\n+                            offsetToWriteAt,\n+                            writeSize);\n+\n+                    // Update the counts\n+                    bytesRemaining -= bytesWritten;\n+                    currentOffset += bytesWritten;\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Updates the segment metadata for the newly added chunk.\n+     */\n+    private ChunkMetadata updateMetadataForChunkAddition(MetadataTransaction txn,\n+                                                         SegmentMetadata segmentMetadata,\n+                                                         String newChunkName,\n+                                                         boolean isFirstWriteAfterFailover,\n+                                                         ChunkMetadata lastChunkMetadata) throws StorageMetadataException {\n+        ChunkMetadata newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        segmentMetadata.setLastChunk(newChunkName);\n+        if (lastChunkMetadata == null) {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+        } else {\n+            lastChunkMetadata.setNextChunk(newChunkName);\n+            txn.update(lastChunkMetadata);\n+        }\n+        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+        // Reset ownershipChanged flag after first write is done.\n+        if (isFirstWriteAfterFailover) {\n+            segmentMetadata.setOwnerEpoch(this.epoch);\n+            segmentMetadata.setOwnershipChanged(false);\n+            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, segmentMetadata.getName());\n+        }\n+        segmentMetadata.incrementChunkCount();\n+\n+        // Update the transaction.\n+        txn.update(newChunkMetadata);\n+        txn.update(segmentMetadata);\n+        return newChunkMetadata;\n+    }\n+\n+    /**\n+     * Write to chunk.\n+     */\n+    private int writeToChunk(MetadataTransaction txn,\n+                             SegmentMetadata segmentMetadata,\n+                             long offset,\n+                             InputStream data,\n+                             ChunkHandle chunkHandle,\n+                             ChunkMetadata chunkWrittenMetadata,\n+                             long offsetToWriteAt,\n+                             int bytesCount) throws IOException, StorageMetadataException, BadOffsetException {\n+        int bytesWritten;\n+        Preconditions.checkState(0 != bytesCount, \"Attempt to write zero bytes\");\n+        try {\n+\n+            // Finally write the data.\n+            try (BoundedInputStream bis = new BoundedInputStream(data, bytesCount)) {\n+                bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesCount, bis);\n+            }\n+\n+            // Update the metadata for segment and chunk.\n+            Preconditions.checkState(bytesWritten >= 0);\n+            segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+            chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+            txn.update(chunkWrittenMetadata);\n+            txn.update(segmentMetadata);\n+        } catch (IndexOutOfBoundsException e) {\n+            try {\n+                throw new BadOffsetException(segmentMetadata.getName(), chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+            } catch (ChunkStorageException cse) {\n+                log.error(\"{} write - Error while retrieving ChunkInfo for {}.\", logPrefix, chunkHandle.getChunkName());\n+                // The exact expected offset for the  operation does not matter, the StorageWriter will enter reconciliation loop anyway.\n+                throw new BadOffsetException(segmentMetadata.getName(), offset, offset);\n+            }\n+        }\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                        .segmentName(streamSegmentName)\n+                        .offset(offset)\n+                        .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                        .newChunkName(newChunkName)\n+                        .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+                targetSegmentMetadata.checkInvariants();\n+                checkNotSealed(targetSegmentName, targetSegmentMetadata);\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                checkSealed(sourceSegmentMetadata);\n+                checkOwnership(targetSegmentMetadata.getName(), targetSegmentMetadata);\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In the absence of defragmentation, the number of chunks for individual segments keeps on increasing.\n+     * When we have too many small chunks (say because many transactions with little data on some segments), the segment\n+     * is fragmented - this may impact both the read throughputand the performance of the metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (e.g., stock S3) and\n+     * each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If the underlying storage provides some facility to stitch together smaller chunk into larger chunks, then we do\n+     * actually want to exploit that, specially when the underlying implementation is only a metadata operation. We want\n+     * to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are typically\n+     * only metadata operations, reducing the overall cost of the merging them together. HDFS also supports merges,\n+     * whereas NFS has no concept of merging natively.\n+     *\n+     * As chunks become larger, append writes (read source completely and append-i.e., write- it back at the end of target)\n+     * become inefficient. Consequently, a native option for merging is desirable. We use such native merge capability\n+     * when available, and if not available, then we use appends.\n+     * </li>\n+     * <li>\n+     * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+     * We can then fine tune that background task to run optimally with low overhead.\n+     * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+     * </li>\n+     * <li>\n+     * <li>\n+     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+     * </li>\n+     * </ul>\n+     *\n+     * <div>\n+     * What controls whether we invoke concat or simulate through appends?\n+     * There are a few different capabilities that ChunkStorage needs to provide.\n+     * <ul>\n+     * <li>Does ChunkStorage support appending to existing chunks? For vanilla S3 compatible this would return false.\n+     * This is indicated by supportsAppend.</li>\n+     * <li>Does ChunkStorage support for concatenating chunks ? This is indicated by supportsConcat.\n+     * If this is true then concat operation will be invoked otherwise chunks will be appended.</li>\n+     * <li>There are some obvious constraints - For ChunkStorage support any concat functionality it must support either\n+     * append or concat.</li>\n+     * <li>Also when ChunkStorage supports both concat and append, ChunkManager will invoke appropriate method\n+     * depending on size of target and source chunks. (Eg. ECS)</li>\n+     * </ul>\n+     * </div>\n+     * <li>\n+     * What controls defrag?\n+     * There are two additional parameters that control when concat\n+     * <li>minSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, append is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n+     * <li>maxSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n+     * In short there is a size beyond which using append is not advisable. Conversely there is a size below which concat is not efficient.(minSizeLimitForConcat )\n+     * Then there is limit which concating does not make sense maxSizeLimitForConcat\n+     * </li>\n+     * <li>\n+     * What is the defrag algorithm\n+     * <pre>\n+     * While(segment.hasConcatableChunks()){\n+     *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n+     *     For (List<chunk> list : s){\n+     *        ConcatChunks (list);\n+     *     }\n+     * }\n+     * </pre>\n+     * </li>\n+     * </ul>\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @param startChunkName  Name of the first chunk to start defragmentation.\n+     * @param lastChunkName   Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, String startChunkName, String lastChunkName) throws StorageMetadataException, ChunkStorageException {\n+        // The algorithm is actually very simple.\n+        // It tries to concat all small chunks using appends first.\n+        // Then it tries to concat remaining chunks using concat if available.\n+        // To implement it using single loop we toggle between concat with append and concat modes. (Instead of two passes.)\n+        boolean useAppend = true;\n+        String targetChunkName = startChunkName;\n+\n+        // Iterate through chunk list\n+        while (null != targetChunkName && !targetChunkName.equals(lastChunkName)) {\n+            ChunkMetadata target = (ChunkMetadata) txn.get(targetChunkName);\n+\n+            ArrayList<ChunkInfo> chunksToConcat = new ArrayList<>();\n+            long targetSizeAfterConcat = target.getLength();\n+\n+            // Add target to the list of chunks\n+            chunksToConcat.add(new ChunkInfo(targetSizeAfterConcat, targetChunkName));\n+\n+            String nextChunkName = target.getNextChunk();\n+            ChunkMetadata next = null;\n+\n+            // Gather list of chunks that can be appended together.\n+            while (null != nextChunkName) {\n+                next = (ChunkMetadata) txn.get(nextChunkName);\n+\n+                if (useAppend && config.getMinSizeLimitForConcat() < next.getLength()) {\n+                    break;\n+                }\n+\n+                if (targetSizeAfterConcat + next.getLength() > segmentMetadata.getMaxRollinglength() || next.getLength() > config.getMaxSizeLimitForConcat()) {\n+                    break;\n+                }\n+\n+                chunksToConcat.add(new ChunkInfo(next.getLength(), nextChunkName));\n+                targetSizeAfterConcat += next.getLength();\n+\n+                nextChunkName = next.getNextChunk();\n+            }\n+            // Note - After above while loop is exited nextChunkName points to chunk next to last one to be concat.\n+            // Which means target should now point to it as next after concat is complete.\n+\n+            // If there are chunks that can be appended together then concat them.\n+            if (chunksToConcat.size() > 1) {\n+                // Concat\n+\n+                ConcatArgument[] concatArgs = new ConcatArgument[chunksToConcat.size()];\n+                for (int i = 0; i < chunksToConcat.size(); i++) {\n+                    concatArgs[i] = ConcatArgument.fromChunkInfo(chunksToConcat.get(i));\n+                }\n+\n+                if (!useAppend && chunkStorage.supportsConcat()) {\n+                    int length = chunkStorage.concat(concatArgs);\n+                } else {\n+                    concatUsingAppend(concatArgs);\n+                }\n+\n+                // Set the pointers\n+                target.setLength(targetSizeAfterConcat);\n+                target.setNextChunk(nextChunkName);\n+\n+                // If target is the last chunk after this then update metadata accordingly\n+                if (null == nextChunkName) {\n+                    segmentMetadata.setLastChunk(target.getName());\n+                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength() - target.getLength());\n+                }\n+\n+                // Update metadata for affected chunks.\n+                for (int i = 1; i < concatArgs.length; i++) {\n+                    txn.delete(concatArgs[i].getName());\n+                    segmentMetadata.decrementChunkCount();\n+                }\n+                txn.update(target);\n+                txn.update(segmentMetadata);\n+            }\n+\n+            // Move on to next place in list where we can concat if we are done with append based concats.\n+            if (!useAppend) {\n+                targetChunkName = nextChunkName;\n+            }\n+\n+            // Toggle\n+            useAppend = !useAppend;\n+        }\n+\n+        // Make sure no invariants are broken.\n+        segmentMetadata.checkInvariants();\n+    }\n+\n+    private void concatUsingAppend(ConcatArgument[] concatArgs) throws ChunkStorageException {\n+        long writeAtOffset = concatArgs[0].getLength();\n+        val writeHandle = ChunkHandle.writeHandle(concatArgs[0].getName());\n+        for (int i = 1; i < concatArgs.length; i++) {\n+            int readAtOffset = 0;\n+            val arg = concatArgs[i];\n+            int bytesToRead = Math.toIntExact(arg.getLength());\n+\n+            while (bytesToRead > 0) {\n+                byte[] buffer = new byte[Math.min(config.getMaxBufferSizeForChunkDataTransfer(), bytesToRead)];\n+                int size = chunkStorage.read(ChunkHandle.readHandle(arg.getName()), readAtOffset, buffer.length, buffer, 0);\n+                bytesToRead -= size;\n+                readAtOffset += size;\n+                writeAtOffset += chunkStorage.write(writeHandle, writeAtOffset, size, new ByteArrayInputStream(buffer, 0, size));\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> delete(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle);\n+            Timer timer = new Timer();\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Check preconditions\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.setActive(false);\n+\n+                // Delete chunks\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    // Delete underlying file.\n+                    chunksToDelete.add(currentChunkName);\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                    txn.delete(currentMetadata.getName());\n+                }\n+\n+                // Commit.\n+                txn.delete(streamSegmentName);\n+                txn.commit();\n+\n+                // Collect garbage.\n+                collectGarbage(chunksToDelete);\n+\n+                // Update the read index.\n+                readIndexCache.remove(streamSegmentName);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} delete - segment={}, latency={}.\", logPrefix, handle.getSegmentName(), elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"delete\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> truncate(SegmentHandle handle, long offset, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle, offset);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Check preconditions\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                checkNotSealed(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getLength() < offset || segmentMetadata.getStartOffset() > offset) {\n+                    throw new IllegalArgumentException(String.format(\"offset %d is outside of valid range [%d, %d) for segment %s\",\n+                            offset, segmentMetadata.getStartOffset(), segmentMetadata.getLength(), streamSegmentName));\n+                }\n+\n+                if (segmentMetadata.getStartOffset() == offset) {\n+                    // Nothing to do\n+                    return null;\n+                }\n+\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                long oldLength = segmentMetadata.getLength();\n+                long startOffset = segmentMetadata.getFirstChunkStartOffset();\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    Preconditions.checkState(null != currentMetadata, \"currentMetadata is null.\");\n+\n+                    // If for given chunk start <= offset < end  then we have found the chunk that will be the first chunk.\n+                    if ((startOffset <= offset) && (startOffset + currentMetadata.getLength() > offset)) {\n+                        break;\n+                    }\n+\n+                    startOffset += currentMetadata.getLength();\n+                    chunksToDelete.add(currentMetadata.getName());\n+                    segmentMetadata.decrementChunkCount();\n+\n+                    // move to next chunk\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                }\n+                segmentMetadata.setFirstChunk(currentChunkName);\n+                segmentMetadata.setStartOffset(offset);\n+                segmentMetadata.setFirstChunkStartOffset(startOffset);\n+                for (String toDelete : chunksToDelete) {\n+                    txn.delete(toDelete);\n+                    // Adjust last chunk if required.\n+                    if (toDelete.equals(segmentMetadata.getLastChunk())) {\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+                        segmentMetadata.setLastChunk(null);\n+                    }\n+                }\n+                txn.update(segmentMetadata);\n+\n+                // Check invariants.\n+                Preconditions.checkState(segmentMetadata.getLength() == oldLength, \"truncate should not change segment length\");\n+                segmentMetadata.checkInvariants();\n+\n+                // Commit system logs.\n+                if (isStorageSystemSegment(segmentMetadata)) {\n+                    val finalStartOffset = startOffset;\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecord(\n+                                SystemJournal.TruncationRecord.builder()\n+                                        .segmentName(streamSegmentName)\n+                                        .offset(offset)\n+                                        .firstChunkName(segmentMetadata.getFirstChunk())\n+                                        .startOffset(finalStartOffset)\n+                                        .build());\n+                        return null;\n+                    });\n+                }\n+\n+                // Finally commit.\n+                txn.commit(chunksToDelete.size() == 0); // if layout did not change then commit with lazyWrite.\n+\n+                collectGarbage(chunksToDelete);\n+\n+                // Update the read index by removing all entries below truncate offset.\n+                readIndexCache.truncateReadIndex(streamSegmentName, segmentMetadata.getStartOffset());\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} truncate - segment={}, offset={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"truncate\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return true;", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIyNzgxMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446227810", "bodyText": "Segment is made up of multiple consecutive chunks. ChunkManager has an ability to truncate head part of the segment by removing chunks which are completely below truncate offset. When truncation offset falls in the middle of a given chunk, we can't remove or truncate that chunk at front. In such cases we keep the entire chunk but make sure data below requested truncation is not returned after truncate.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T14:43:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjA5NzU0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk3MzgxOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446973818", "bodyText": "A couple of questions:\n1- If chunk manager is a layer of abstraction on top of real storage and it is always there, then does checking whether it supports truncation even makes sense? Is this a legacy check that we can remove with this work?\n2- Truncating at the granularity of chunks is not really supporting truncation. One potential undesirable behavior from a user perspective is that after a successful call to truncation, the space used by truncated data might not be entirely reclaimed.\nIs there any chance we can support real truncation (i.e., truncating chunks partially when the offset falls in the middle of chunk) for the underlying storage system that provide such functionality?", "author": "fpj", "createdAt": "2020-06-29T13:32:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjA5NzU0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA2OTY1OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447069658", "bodyText": "The public boolean supportsTruncation() on Storage interface is used in select few places in the existing code. (SegmentAttributeBTreeIndex, SegmentAggregator). Old code has slightly different abstractions and it is not possible to cleanly remove this method at this time. (Without making bigger change.)\nNone of the underlying storage (FileSystem,ExtendedS3,HDFS)  actually provide ability to truncate at the front of the chunk.", "author": "sachin-j-joshi", "createdAt": "2020-06-29T15:43:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjA5NzU0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAwNDU5Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r448004593", "bodyText": "Created issue to track this as per our conversation #4912.", "author": "sachin-j-joshi", "createdAt": "2020-06-30T22:01:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjA5NzU0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjA5OTIyOQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446099229", "bodyText": "It is confusing that ChunkManager implements Storage while ChunkStorage does not implement any interface. Isn't it more natural that ChunkStorage implements Storage rather than ChunkManager?", "author": "fpj", "createdAt": "2020-06-26T10:20:41Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorage.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import java.io.InputStream;\n+\n+/**\n+ * Defines an abstraction for Permanent Storage.\n+ * Note: not all operations defined here are needed.\n+ * <div>\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are\n+ * provided even though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent\n+ * (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent\n+ * operation performed.</li>\n+ * </ul>\n+ * There is no need to implement any special logic to handle concurrent access to the underlying objects/files.\n+ * </div>\n+ * <div>\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end).\n+ * This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkManager will invoke appropriate method depending\n+ * on size of target and source chunks. (Eg. ECS)\n+ * </div>\n+ * It is recommended that the implementations should extend {@link BaseChunkStorage}.\n+ */\n+public interface ChunkStorage extends AutoCloseable {", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIyMzM1Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446223357", "bodyText": "Storage is the interface that is already defined and used by rest of the segment store. This change does not aim to change that interface. This interface has an abstraction of segments and operates at that level of segments.\nChunkStorage is a new interface which operates at individual chunks (CRDL). We wanted this to be simple interface. Implementations do not need to worry about fencing or how  these chunks are put together to form a segment. Consequently the methods and their behavior is different. Chunk is therefore not same as segment.\nWe need something that translates calls at segment level (Storage) to calls on individual chunks (ChunkStorage) and also manages all these chunks, fencing, rolling etc etc . This is exactly what ChunkManager does. It is lot more than just adapter or wrapper.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T14:36:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjA5OTIyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk5NjAzNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446996035", "bodyText": "The problem isn't keeping the Storage interface as is, but instead that the abstractions don't quite match here, as I see it:\n\nChunkManager should be called ChunkStorage instead\nChunkStorage should be called something else that reflects the fact that it operates on chunks individually, e.g., ChunkHandler since it handles operations on chunks.", "author": "fpj", "createdAt": "2020-06-29T14:03:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjA5OTIyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAzMjMyNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447032324", "bodyText": "The original names were\n\nChunkStorageManager\nChunkStorageProvider\n\nAlternative names are\n\nChunkStorage\nChunkProvider , ChunkHandler (But Handler has connotation of handling event) ,", "author": "sachin-j-joshi", "createdAt": "2020-06-29T14:52:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjA5OTIyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwMDUyNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446100527", "bodyText": "Truncates... to be consistent with the other javadocs.", "author": "fpj", "createdAt": "2020-06-26T10:23:53Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.\n+ */\n+class ReadIndexCache {\n+\n+    /**\n+     * Max number of indexed segments to keep in cache.\n+     */\n+    private final int maxIndexedSegments;\n+\n+    /**\n+     * Max number of indexed chunks to keep in cache.\n+     */\n+    private final int maxIndexedChunks;\n+\n+    /**\n+     * Max number of indexed chunks to keep per segment in cache.\n+     */\n+    private final int maxIndexedChunksPerSegment;\n+\n+    /**\n+     * Current generation of cache entries.\n+     */\n+    private final AtomicLong currentGeneration = new AtomicLong();\n+\n+    /**\n+     * Lowest generation of cache entries.\n+     */\n+    private final AtomicLong oldestGeneration = new AtomicLong();\n+\n+    /**\n+     * Total number of chunks in the cache.\n+     */\n+    private final AtomicInteger totalChunkCount = new AtomicInteger();\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, SegmentReadIndex> segmentsToReadIndexMap = new ConcurrentHashMap<String, SegmentReadIndex>();\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param maxIndexedSegments\n+     * @param maxIndexedChunksPerSegment\n+     * @param maxIndexedChunks\n+     */\n+    public ReadIndexCache(int maxIndexedSegments, int maxIndexedChunksPerSegment, int maxIndexedChunks) {\n+        this.maxIndexedChunksPerSegment = maxIndexedChunksPerSegment;\n+        this.maxIndexedSegments = maxIndexedSegments;\n+        this.maxIndexedChunks = maxIndexedChunks;\n+    }\n+\n+    /**\n+     * Retrieves the read index for given segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @return Read index corresponding to the given segment. A new empty index is created if it doesn't already exist.\n+     */\n+    private SegmentReadIndex getSegmentReadIndex(String streamSegmentName) {\n+        SegmentReadIndex readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+\n+        if (null == readIndex) {\n+            // Evict segments if required.\n+            if (maxIndexedSegments < segmentsToReadIndexMap.size() + 1 || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictSegmentsFromOldestGeneration();\n+            }\n+            val newReadIndex = SegmentReadIndex.builder()\n+                    .chunkIndex(new ConcurrentSkipListMap<Long, SegmentReadIndexEntry>())\n+                    .generation(new AtomicLong(currentGeneration.get()))\n+                    .build();\n+            val oldReadIndex = segmentsToReadIndexMap.putIfAbsent(streamSegmentName, newReadIndex);\n+            readIndex = null != oldReadIndex ? oldReadIndex : newReadIndex;\n+        }\n+\n+        return readIndex;\n+    }\n+\n+    /**\n+     * Gets total number of chunks in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalChunksCount() {\n+        return totalChunkCount.get();\n+    }\n+\n+    /**\n+     * Gets total number of segments in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalSegmentCount() {\n+        return segmentsToReadIndexMap.size();\n+    }\n+\n+    /**\n+     * Gets oldest generation in cache.\n+     *\n+     * @return\n+     */\n+    public long getOldestGeneration() {\n+        return oldestGeneration.get();\n+    }\n+\n+    /**\n+     * Gets current generation of cache.\n+     *\n+     * @return\n+     */\n+    public long getCurrentGeneration() {\n+        return currentGeneration.get();\n+    }\n+\n+    /**\n+     * Adds a new index entry for a given chunk in index for the segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param chunkName         Name of the chunk.\n+     * @param startOffset       Start offset of the chunk.\n+     */\n+    public void addIndexEntry(String streamSegmentName, String chunkName, long startOffset) {\n+        if (null != chunkName) {\n+            val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+\n+            // Evict chunks if required.\n+            if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + 1\n+                    || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictChunks(streamSegmentName, 1);\n+            }\n+\n+            segmentReadIndex.chunkIndex.put(startOffset,\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(chunkName)\n+                            .generation(new AtomicLong(currentGeneration.get()))\n+                            .build());\n+            segmentReadIndex.generation.set(currentGeneration.get());\n+            totalChunkCount.incrementAndGet();\n+        }\n+    }\n+\n+    /**\n+     * Updates read index for given segment with new entries.\n+     *\n+     * @param streamSegmentName\n+     * @param newReadIndexEntries List of {@link ChunkNameOffsetPair} for new entries.\n+     */\n+    public void addIndexEntries(String streamSegmentName, List<ChunkNameOffsetPair> newReadIndexEntries) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        // Evict chunks if required.\n+        if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + newReadIndexEntries.size()\n+                || maxIndexedChunks < totalChunkCount.get() + newReadIndexEntries.size()) {\n+            evictChunks(streamSegmentName, newReadIndexEntries.size());\n+        }\n+\n+        for (val entry : newReadIndexEntries) {\n+            segmentReadIndex.chunkIndex.put(entry.getOffset(),\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(entry.getChunkName())\n+                            .generation(new AtomicLong(currentGeneration.get()))\n+                            .build());\n+            segmentReadIndex.generation.set(currentGeneration.get());\n+\n+        }\n+        totalChunkCount.getAndAdd(newReadIndexEntries.size());\n+    }\n+\n+    /**\n+     * Removes the given segment from cache.\n+     *\n+     * @param streamSegmentName\n+     */\n+    public void remove(String streamSegmentName) {\n+        val readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        if (null != readIndex) {\n+            segmentsToReadIndexMap.remove(streamSegmentName);\n+            totalChunkCount.getAndAdd(-1 * readIndex.chunkIndex.size());\n+        }\n+    }\n+\n+    /**\n+     * Finds a chunk that is floor to the given offset.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset for which to search.\n+     * @return\n+     */\n+    public ChunkNameOffsetPair findFloor(String streamSegmentName, long offset) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        if (segmentReadIndex.chunkIndex.size() > 0) {\n+            val floorEntry = segmentReadIndex.chunkIndex.floorEntry(offset);\n+            if (null != floorEntry) {\n+                // mark with current generation\n+                segmentReadIndex.generation.set(currentGeneration.get());\n+                floorEntry.getValue().generation.set(currentGeneration.get());\n+                // return value.\n+                return new ChunkNameOffsetPair(floorEntry.getKey(), floorEntry.getValue().getChunkName());\n+            }\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Truncate the read index for given segment by removing all the chunks that are below given offset.", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwMTI2OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446101268", "bodyText": "Shouldn't findFloor be a read operation with no side effect?", "author": "fpj", "createdAt": "2020-06-26T10:25:35Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.\n+ */\n+class ReadIndexCache {\n+\n+    /**\n+     * Max number of indexed segments to keep in cache.\n+     */\n+    private final int maxIndexedSegments;\n+\n+    /**\n+     * Max number of indexed chunks to keep in cache.\n+     */\n+    private final int maxIndexedChunks;\n+\n+    /**\n+     * Max number of indexed chunks to keep per segment in cache.\n+     */\n+    private final int maxIndexedChunksPerSegment;\n+\n+    /**\n+     * Current generation of cache entries.\n+     */\n+    private final AtomicLong currentGeneration = new AtomicLong();\n+\n+    /**\n+     * Lowest generation of cache entries.\n+     */\n+    private final AtomicLong oldestGeneration = new AtomicLong();\n+\n+    /**\n+     * Total number of chunks in the cache.\n+     */\n+    private final AtomicInteger totalChunkCount = new AtomicInteger();\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, SegmentReadIndex> segmentsToReadIndexMap = new ConcurrentHashMap<String, SegmentReadIndex>();\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param maxIndexedSegments\n+     * @param maxIndexedChunksPerSegment\n+     * @param maxIndexedChunks\n+     */\n+    public ReadIndexCache(int maxIndexedSegments, int maxIndexedChunksPerSegment, int maxIndexedChunks) {\n+        this.maxIndexedChunksPerSegment = maxIndexedChunksPerSegment;\n+        this.maxIndexedSegments = maxIndexedSegments;\n+        this.maxIndexedChunks = maxIndexedChunks;\n+    }\n+\n+    /**\n+     * Retrieves the read index for given segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @return Read index corresponding to the given segment. A new empty index is created if it doesn't already exist.\n+     */\n+    private SegmentReadIndex getSegmentReadIndex(String streamSegmentName) {\n+        SegmentReadIndex readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+\n+        if (null == readIndex) {\n+            // Evict segments if required.\n+            if (maxIndexedSegments < segmentsToReadIndexMap.size() + 1 || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictSegmentsFromOldestGeneration();\n+            }\n+            val newReadIndex = SegmentReadIndex.builder()\n+                    .chunkIndex(new ConcurrentSkipListMap<Long, SegmentReadIndexEntry>())\n+                    .generation(new AtomicLong(currentGeneration.get()))\n+                    .build();\n+            val oldReadIndex = segmentsToReadIndexMap.putIfAbsent(streamSegmentName, newReadIndex);\n+            readIndex = null != oldReadIndex ? oldReadIndex : newReadIndex;\n+        }\n+\n+        return readIndex;\n+    }\n+\n+    /**\n+     * Gets total number of chunks in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalChunksCount() {\n+        return totalChunkCount.get();\n+    }\n+\n+    /**\n+     * Gets total number of segments in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalSegmentCount() {\n+        return segmentsToReadIndexMap.size();\n+    }\n+\n+    /**\n+     * Gets oldest generation in cache.\n+     *\n+     * @return\n+     */\n+    public long getOldestGeneration() {\n+        return oldestGeneration.get();\n+    }\n+\n+    /**\n+     * Gets current generation of cache.\n+     *\n+     * @return\n+     */\n+    public long getCurrentGeneration() {\n+        return currentGeneration.get();\n+    }\n+\n+    /**\n+     * Adds a new index entry for a given chunk in index for the segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param chunkName         Name of the chunk.\n+     * @param startOffset       Start offset of the chunk.\n+     */\n+    public void addIndexEntry(String streamSegmentName, String chunkName, long startOffset) {\n+        if (null != chunkName) {\n+            val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+\n+            // Evict chunks if required.\n+            if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + 1\n+                    || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictChunks(streamSegmentName, 1);\n+            }\n+\n+            segmentReadIndex.chunkIndex.put(startOffset,\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(chunkName)\n+                            .generation(new AtomicLong(currentGeneration.get()))\n+                            .build());\n+            segmentReadIndex.generation.set(currentGeneration.get());\n+            totalChunkCount.incrementAndGet();\n+        }\n+    }\n+\n+    /**\n+     * Updates read index for given segment with new entries.\n+     *\n+     * @param streamSegmentName\n+     * @param newReadIndexEntries List of {@link ChunkNameOffsetPair} for new entries.\n+     */\n+    public void addIndexEntries(String streamSegmentName, List<ChunkNameOffsetPair> newReadIndexEntries) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        // Evict chunks if required.\n+        if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + newReadIndexEntries.size()\n+                || maxIndexedChunks < totalChunkCount.get() + newReadIndexEntries.size()) {\n+            evictChunks(streamSegmentName, newReadIndexEntries.size());\n+        }\n+\n+        for (val entry : newReadIndexEntries) {\n+            segmentReadIndex.chunkIndex.put(entry.getOffset(),\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(entry.getChunkName())\n+                            .generation(new AtomicLong(currentGeneration.get()))\n+                            .build());\n+            segmentReadIndex.generation.set(currentGeneration.get());\n+\n+        }\n+        totalChunkCount.getAndAdd(newReadIndexEntries.size());\n+    }\n+\n+    /**\n+     * Removes the given segment from cache.\n+     *\n+     * @param streamSegmentName\n+     */\n+    public void remove(String streamSegmentName) {\n+        val readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        if (null != readIndex) {\n+            segmentsToReadIndexMap.remove(streamSegmentName);\n+            totalChunkCount.getAndAdd(-1 * readIndex.chunkIndex.size());\n+        }\n+    }\n+\n+    /**\n+     * Finds a chunk that is floor to the given offset.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset for which to search.\n+     * @return\n+     */\n+    public ChunkNameOffsetPair findFloor(String streamSegmentName, long offset) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        if (segmentReadIndex.chunkIndex.size() > 0) {\n+            val floorEntry = segmentReadIndex.chunkIndex.floorEntry(offset);\n+            if (null != floorEntry) {\n+                // mark with current generation\n+                segmentReadIndex.generation.set(currentGeneration.get());", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIzMDQzMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446230433", "bodyText": "Assuming that client is reading sequentially, it is likely that the same chunk will be read in subsequent calls. So we need to keep that entry in cache. Because this was a cache hit we need to mark it with current generation.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T14:48:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwMTI2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwMTg1Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446101857", "bodyText": "... up to ...", "author": "fpj", "createdAt": "2020-06-26T10:26:56Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.\n+ */\n+class ReadIndexCache {\n+\n+    /**\n+     * Max number of indexed segments to keep in cache.\n+     */\n+    private final int maxIndexedSegments;\n+\n+    /**\n+     * Max number of indexed chunks to keep in cache.\n+     */\n+    private final int maxIndexedChunks;\n+\n+    /**\n+     * Max number of indexed chunks to keep per segment in cache.\n+     */\n+    private final int maxIndexedChunksPerSegment;\n+\n+    /**\n+     * Current generation of cache entries.\n+     */\n+    private final AtomicLong currentGeneration = new AtomicLong();\n+\n+    /**\n+     * Lowest generation of cache entries.\n+     */\n+    private final AtomicLong oldestGeneration = new AtomicLong();\n+\n+    /**\n+     * Total number of chunks in the cache.\n+     */\n+    private final AtomicInteger totalChunkCount = new AtomicInteger();\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, SegmentReadIndex> segmentsToReadIndexMap = new ConcurrentHashMap<String, SegmentReadIndex>();\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param maxIndexedSegments\n+     * @param maxIndexedChunksPerSegment\n+     * @param maxIndexedChunks\n+     */\n+    public ReadIndexCache(int maxIndexedSegments, int maxIndexedChunksPerSegment, int maxIndexedChunks) {\n+        this.maxIndexedChunksPerSegment = maxIndexedChunksPerSegment;\n+        this.maxIndexedSegments = maxIndexedSegments;\n+        this.maxIndexedChunks = maxIndexedChunks;\n+    }\n+\n+    /**\n+     * Retrieves the read index for given segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @return Read index corresponding to the given segment. A new empty index is created if it doesn't already exist.\n+     */\n+    private SegmentReadIndex getSegmentReadIndex(String streamSegmentName) {\n+        SegmentReadIndex readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+\n+        if (null == readIndex) {\n+            // Evict segments if required.\n+            if (maxIndexedSegments < segmentsToReadIndexMap.size() + 1 || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictSegmentsFromOldestGeneration();\n+            }\n+            val newReadIndex = SegmentReadIndex.builder()\n+                    .chunkIndex(new ConcurrentSkipListMap<Long, SegmentReadIndexEntry>())\n+                    .generation(new AtomicLong(currentGeneration.get()))\n+                    .build();\n+            val oldReadIndex = segmentsToReadIndexMap.putIfAbsent(streamSegmentName, newReadIndex);\n+            readIndex = null != oldReadIndex ? oldReadIndex : newReadIndex;\n+        }\n+\n+        return readIndex;\n+    }\n+\n+    /**\n+     * Gets total number of chunks in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalChunksCount() {\n+        return totalChunkCount.get();\n+    }\n+\n+    /**\n+     * Gets total number of segments in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalSegmentCount() {\n+        return segmentsToReadIndexMap.size();\n+    }\n+\n+    /**\n+     * Gets oldest generation in cache.\n+     *\n+     * @return\n+     */\n+    public long getOldestGeneration() {\n+        return oldestGeneration.get();\n+    }\n+\n+    /**\n+     * Gets current generation of cache.\n+     *\n+     * @return\n+     */\n+    public long getCurrentGeneration() {\n+        return currentGeneration.get();\n+    }\n+\n+    /**\n+     * Adds a new index entry for a given chunk in index for the segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param chunkName         Name of the chunk.\n+     * @param startOffset       Start offset of the chunk.\n+     */\n+    public void addIndexEntry(String streamSegmentName, String chunkName, long startOffset) {\n+        if (null != chunkName) {\n+            val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+\n+            // Evict chunks if required.\n+            if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + 1\n+                    || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictChunks(streamSegmentName, 1);\n+            }\n+\n+            segmentReadIndex.chunkIndex.put(startOffset,\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(chunkName)\n+                            .generation(new AtomicLong(currentGeneration.get()))\n+                            .build());\n+            segmentReadIndex.generation.set(currentGeneration.get());\n+            totalChunkCount.incrementAndGet();\n+        }\n+    }\n+\n+    /**\n+     * Updates read index for given segment with new entries.\n+     *\n+     * @param streamSegmentName\n+     * @param newReadIndexEntries List of {@link ChunkNameOffsetPair} for new entries.\n+     */\n+    public void addIndexEntries(String streamSegmentName, List<ChunkNameOffsetPair> newReadIndexEntries) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        // Evict chunks if required.\n+        if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + newReadIndexEntries.size()\n+                || maxIndexedChunks < totalChunkCount.get() + newReadIndexEntries.size()) {\n+            evictChunks(streamSegmentName, newReadIndexEntries.size());\n+        }\n+\n+        for (val entry : newReadIndexEntries) {\n+            segmentReadIndex.chunkIndex.put(entry.getOffset(),\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(entry.getChunkName())\n+                            .generation(new AtomicLong(currentGeneration.get()))\n+                            .build());\n+            segmentReadIndex.generation.set(currentGeneration.get());\n+\n+        }\n+        totalChunkCount.getAndAdd(newReadIndexEntries.size());\n+    }\n+\n+    /**\n+     * Removes the given segment from cache.\n+     *\n+     * @param streamSegmentName\n+     */\n+    public void remove(String streamSegmentName) {\n+        val readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        if (null != readIndex) {\n+            segmentsToReadIndexMap.remove(streamSegmentName);\n+            totalChunkCount.getAndAdd(-1 * readIndex.chunkIndex.size());\n+        }\n+    }\n+\n+    /**\n+     * Finds a chunk that is floor to the given offset.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset for which to search.\n+     * @return\n+     */\n+    public ChunkNameOffsetPair findFloor(String streamSegmentName, long offset) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        if (segmentReadIndex.chunkIndex.size() > 0) {\n+            val floorEntry = segmentReadIndex.chunkIndex.floorEntry(offset);\n+            if (null != floorEntry) {\n+                // mark with current generation\n+                segmentReadIndex.generation.set(currentGeneration.get());\n+                floorEntry.getValue().generation.set(currentGeneration.get());\n+                // return value.\n+                return new ChunkNameOffsetPair(floorEntry.getKey(), floorEntry.getValue().getChunkName());\n+            }\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Truncate the read index for given segment by removing all the chunks that are below given offset.\n+     *\n+     * @param streamSegmentName\n+     * @param startOffset\n+     */\n+    public void truncateReadIndex(String streamSegmentName, long startOffset) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        if (null != segmentReadIndex) {\n+            if (segmentReadIndex.chunkIndex.size() > 0) {\n+                val headMap = segmentReadIndex.chunkIndex.headMap(startOffset);\n+                if (null != headMap) {\n+                    int removed = 0;\n+                    ArrayList<Long> keysToRemove = new ArrayList<Long>();\n+                    keysToRemove.addAll(headMap.keySet());\n+                    for (val keyToRemove : keysToRemove) {\n+                        segmentReadIndex.chunkIndex.remove(keyToRemove);\n+                        removed++;\n+                    }\n+                    if (removed > 0) {\n+                        totalChunkCount.getAndAdd(-1 * removed);\n+                    }\n+                }\n+            }\n+            if (segmentReadIndex.chunkIndex.size() > 0) {\n+                segmentReadIndex.generation.set(currentGeneration.get());\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Evicts segments from the cache.\n+     */\n+    private void evictSegmentsFromOldestGeneration() {\n+        val oldGen = oldestGeneration.get();\n+        currentGeneration.getAndIncrement();\n+\n+        val iterator = segmentsToReadIndexMap.entrySet().iterator();\n+        int total = totalChunkCount.get();\n+        int removed = 0;\n+        while (iterator.hasNext() && (segmentsToReadIndexMap.size() >= maxIndexedSegments || total >= maxIndexedChunks)) {\n+            val entry = iterator.next();\n+            if (entry.getValue().generation.get() <= oldGen) {\n+                val size = entry.getValue().chunkIndex.size();\n+                removed += size;\n+                total -= size;\n+                iterator.remove();\n+            }\n+        }\n+        if (removed > 0) {\n+            oldestGeneration.compareAndSet(oldGen, oldGen + 1);\n+            totalChunkCount.getAndAdd(-1 * removed);\n+        }\n+    }\n+\n+    /**\n+     * Evicts chunks from the cache.\n+     */\n+    private void evictChunks(String streamSegmentName, long toRemoveCount) {\n+        val segmentReadIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        evictChunks(segmentReadIndex, toRemoveCount);\n+    }\n+\n+    /**\n+     * Evicts chunks from the cache.\n+     */\n+    private void evictChunks(SegmentReadIndex segmentReadIndex, long toRemoveCount) {\n+        // Increment generation.\n+        val previousGen = currentGeneration.getAndIncrement();\n+        val oldGen = oldestGeneration.get();\n+\n+        // Step 1 : Go through all entries once to record counts per each generation.\n+        TreeMap<Long, Integer> counts = new TreeMap<Long, Integer>();\n+        val iterator = segmentReadIndex.chunkIndex.entrySet().iterator();\n+        while (iterator.hasNext()) {\n+            val entry = iterator.next();\n+            long generation = entry.getValue().generation.get();\n+            val cnt = counts.get(generation);\n+            if (null == cnt) {\n+                counts.put(generation, 1);\n+            } else {\n+                counts.put(generation, cnt + 1);\n+            }\n+        }\n+\n+        // Step 2 : Determine upto what generation to delete.", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwMjQzNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446102435", "bodyText": "Do we need metrics for this read index cache?", "author": "fpj", "createdAt": "2020-06-26T10:28:08Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.\n+ */\n+class ReadIndexCache {", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIzMjY1NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446232655", "bodyText": "Yes.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T14:52:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwMjQzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3NTA2NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446275065", "bodyText": "I plan to work bit more on  ReadIndexCache\nI have created #4902 For tracking improvements in cache eviction logic for ReadIndexCache in ChunkManager layer. This will be done as separate PR after first set of major changes.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T16:06:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwMjQzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwNTE0NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446105145", "bodyText": "I does not need to be accurate for correctness, but if miscalculated, then can it have an impact to performance?", "author": "fpj", "createdAt": "2020-06-26T10:34:25Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIzNTYyOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446235628", "bodyText": "Yes. But I think intention is to prefer performance over accuracy.\nEg. If it improves average performance by avoiding need to synchronize we would prefer that option, even if every once in a while we evict an entry too early or keep it for bit longer than needed.\nIt really depends on extent of miscalculation and probability of it happening.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T14:56:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwNTE0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk3NzA0Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446977046", "bodyText": "I need to understand this better, I'll have a look at this again and get back.", "author": "fpj", "createdAt": "2020-06-29T13:37:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwNTE0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAyNjk0OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447026948", "bodyText": "Ok, I'm trying to understand what made you say best effort here. Is it the fact that upon eviction, we iterate over the elements of the map concurrently with other operations? If not, what's it?\nI also saw a resolved comment down saying that you'll revisit the implementation of the cache in the next iteration? It is not clear to me what the next iteration is, whether for this PR or a future one, so let me know your plans so that I have the right expectation.", "author": "fpj", "createdAt": "2020-06-29T14:45:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwNTE0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzk5ODE5Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447998196", "bodyText": "Resolving as per discussion offline. Tracking this here #4902", "author": "sachin-j-joshi", "createdAt": "2020-06-30T21:47:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwNTE0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwNzU3Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446107572", "bodyText": "Typo in snalpshot", "author": "fpj", "createdAt": "2020-06-26T10:40:27Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,940 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import lombok.var;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Strings.nullToEmpty;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This creates a circular dependency while reading or writing the data about these segments from the metadata segments.\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    /**\n+     * Serializer for {@link SystemJournalRecordBatch}.\n+     */\n+    private static final SystemJournalRecordBatch.SystemJournalRecordBatchSerializer BATCH_SERIALIZER = new SystemJournalRecordBatch.SystemJournalRecordBatchSerializer();\n+\n+    /**\n+     * Serializer for {@link SystemSnapshotRecord}.\n+     */\n+    private static final SystemSnapshotRecord.Serializer SYSTEM_SNAPSHOT_SERIALIZER = new SystemSnapshotRecord.Serializer();\n+\n+    private final Object lock = new Object();\n+\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    @Getter\n+    private final ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private final long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private final int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Configuration {@link ChunkManagerConfig} for the {@link ChunkManager}.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    private final AtomicBoolean reentryGuard = new AtomicBoolean();\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId   Container id of the owner container.\n+     * @param epoch         Epoch of the current container instance.\n+     * @param chunkStorage  ChunkStorage instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, ChunkManagerConfig config) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param record Record to persist.\n+     * @throws ChunkStorageException Exception if any.\n+     */\n+    public void commitRecord(SystemJournalRecord record) throws ChunkStorageException {\n+        commitRecords(Collections.singletonList(record));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws ChunkStorageException Exception in case of any error.\n+     */\n+    public void commitRecords(Collection<SystemJournalRecord> records) throws ChunkStorageException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+\n+        // Open the underlying chunk to write.\n+        ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+        SystemJournalRecordBatch batch = SystemJournalRecordBatch.builder().systemJournalRecords(records).build();\n+        ByteArraySegment bytes;\n+        try {\n+            bytes = BATCH_SERIALIZER.serialize(batch);\n+        } catch (IOException e) {\n+            throw new ChunkStorageException(getSystemJournalFileName(), \"Unable to serialize\", e);\n+        }\n+        // Persist\n+        synchronized (lock) {\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.getLength(),\n+                    new ByteArrayInputStream(bytes.array(), bytes.arrayOffset(), bytes.getLength()));\n+            Preconditions.checkState(bytesWritten == bytes.getLength());\n+            systemJournalOffset += bytesWritten;\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend() || config.isAppendsDisabled()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+        log.debug(\"SystemJournal[{}] Logging system log records - file={}, batch ={}.\", containerId, h.getChunkName(), batch);\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    public void bootstrap() throws Exception {\n+        Preconditions.checkState(!reentryGuard.getAndSet(true), \"bootstrap called multiple times.\");\n+        try (val txn = metadataStore.beginTransaction()) {\n+            // Keep track of offsets at which chunks were added to the system segments.\n+            val chunkStartOffsets = new HashMap<String, Long>();\n+\n+            // Keep track of offsets at which system segments were truncated.\n+            // We don't need to apply each truncate operation, only need to apply the final truncate offset.\n+            val finalTruncateOffsets = new HashMap<String, Long>();\n+            val finalFirstChunkStartsAtOffsets = new HashMap<String, Long>();\n+\n+            // Step 1: Create metadata records for system segments from latest snapshot.\n+            val epochToStart = applyLatestSnapshot(txn, chunkStartOffsets);\n+\n+            // Step 2: For each epoch, find the corresponding system journal files, process them and apply operations recorded.\n+            applySystemLogOperations(txn, epochToStart, chunkStartOffsets, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 3: Adjust the length of the last chunk.\n+            adjustLastChunkLengths(txn);\n+\n+            // Step 4: Apply the truncate offsets.\n+            applyFinalTruncateOffsets(txn, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 5: Validate and save a snapshot.\n+            validateAndSaveSnapshot(txn);\n+\n+            // Step 5: Finally commit all data.\n+            txn.commit(true, true);\n+        }\n+    }\n+\n+    /**\n+     * Find and apply latest snapshot.\n+     */\n+    private long applyLatestSnapshot(MetadataTransaction txn, HashMap<String, Long> chunkStartOffsets) throws Exception {\n+        long epochToCheck = epoch - 1;\n+        String snapshotFile = null;\n+        boolean found = false;\n+\n+        // Find latest epoch with snapshot.\n+        for (epochToCheck = epoch - 1; epochToCheck >= 0; epochToCheck--) {\n+            snapshotFile = getSystemJournalFileName(containerId, epochToCheck, 0);\n+            if (chunkStorage.exists(snapshotFile)) {\n+                // Read contents.\n+                byte[] contents = getContents(snapshotFile);\n+                SystemSnapshotRecord systemSnapshot = SYSTEM_SNAPSHOT_SERIALIZER.deserialize(contents);\n+                if (null != systemSnapshot) {\n+                    log.debug(\"SystemJournal[{}] Processing system log snalpshot {}.\", containerId, systemSnapshot);", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwNzcwOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446107708", "bodyText": "Typo in thier", "author": "fpj", "createdAt": "2020-06-26T10:40:49Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,940 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import lombok.var;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Strings.nullToEmpty;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This creates a circular dependency while reading or writing the data about these segments from the metadata segments.\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    /**\n+     * Serializer for {@link SystemJournalRecordBatch}.\n+     */\n+    private static final SystemJournalRecordBatch.SystemJournalRecordBatchSerializer BATCH_SERIALIZER = new SystemJournalRecordBatch.SystemJournalRecordBatchSerializer();\n+\n+    /**\n+     * Serializer for {@link SystemSnapshotRecord}.\n+     */\n+    private static final SystemSnapshotRecord.Serializer SYSTEM_SNAPSHOT_SERIALIZER = new SystemSnapshotRecord.Serializer();\n+\n+    private final Object lock = new Object();\n+\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    @Getter\n+    private final ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private final long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private final int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Configuration {@link ChunkManagerConfig} for the {@link ChunkManager}.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    private final AtomicBoolean reentryGuard = new AtomicBoolean();\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId   Container id of the owner container.\n+     * @param epoch         Epoch of the current container instance.\n+     * @param chunkStorage  ChunkStorage instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, ChunkManagerConfig config) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param record Record to persist.\n+     * @throws ChunkStorageException Exception if any.\n+     */\n+    public void commitRecord(SystemJournalRecord record) throws ChunkStorageException {\n+        commitRecords(Collections.singletonList(record));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws ChunkStorageException Exception in case of any error.\n+     */\n+    public void commitRecords(Collection<SystemJournalRecord> records) throws ChunkStorageException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+\n+        // Open the underlying chunk to write.\n+        ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+        SystemJournalRecordBatch batch = SystemJournalRecordBatch.builder().systemJournalRecords(records).build();\n+        ByteArraySegment bytes;\n+        try {\n+            bytes = BATCH_SERIALIZER.serialize(batch);\n+        } catch (IOException e) {\n+            throw new ChunkStorageException(getSystemJournalFileName(), \"Unable to serialize\", e);\n+        }\n+        // Persist\n+        synchronized (lock) {\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.getLength(),\n+                    new ByteArrayInputStream(bytes.array(), bytes.arrayOffset(), bytes.getLength()));\n+            Preconditions.checkState(bytesWritten == bytes.getLength());\n+            systemJournalOffset += bytesWritten;\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend() || config.isAppendsDisabled()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+        log.debug(\"SystemJournal[{}] Logging system log records - file={}, batch ={}.\", containerId, h.getChunkName(), batch);\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    public void bootstrap() throws Exception {\n+        Preconditions.checkState(!reentryGuard.getAndSet(true), \"bootstrap called multiple times.\");\n+        try (val txn = metadataStore.beginTransaction()) {\n+            // Keep track of offsets at which chunks were added to the system segments.\n+            val chunkStartOffsets = new HashMap<String, Long>();\n+\n+            // Keep track of offsets at which system segments were truncated.\n+            // We don't need to apply each truncate operation, only need to apply the final truncate offset.\n+            val finalTruncateOffsets = new HashMap<String, Long>();\n+            val finalFirstChunkStartsAtOffsets = new HashMap<String, Long>();\n+\n+            // Step 1: Create metadata records for system segments from latest snapshot.\n+            val epochToStart = applyLatestSnapshot(txn, chunkStartOffsets);\n+\n+            // Step 2: For each epoch, find the corresponding system journal files, process them and apply operations recorded.\n+            applySystemLogOperations(txn, epochToStart, chunkStartOffsets, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 3: Adjust the length of the last chunk.\n+            adjustLastChunkLengths(txn);\n+\n+            // Step 4: Apply the truncate offsets.\n+            applyFinalTruncateOffsets(txn, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 5: Validate and save a snapshot.\n+            validateAndSaveSnapshot(txn);\n+\n+            // Step 5: Finally commit all data.\n+            txn.commit(true, true);\n+        }\n+    }\n+\n+    /**\n+     * Find and apply latest snapshot.\n+     */\n+    private long applyLatestSnapshot(MetadataTransaction txn, HashMap<String, Long> chunkStartOffsets) throws Exception {\n+        long epochToCheck = epoch - 1;\n+        String snapshotFile = null;\n+        boolean found = false;\n+\n+        // Find latest epoch with snapshot.\n+        for (epochToCheck = epoch - 1; epochToCheck >= 0; epochToCheck--) {\n+            snapshotFile = getSystemJournalFileName(containerId, epochToCheck, 0);\n+            if (chunkStorage.exists(snapshotFile)) {\n+                // Read contents.\n+                byte[] contents = getContents(snapshotFile);\n+                SystemSnapshotRecord systemSnapshot = SYSTEM_SNAPSHOT_SERIALIZER.deserialize(contents);\n+                if (null != systemSnapshot) {\n+                    log.debug(\"SystemJournal[{}] Processing system log snalpshot {}.\", containerId, systemSnapshot);\n+                    // Initialize the segments and thier chunks.", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwODcwOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446108708", "bodyText": "Why are we calling it a file? It depends on the underlying storage we are using for chunks, no?", "author": "fpj", "createdAt": "2020-06-26T10:43:11Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,940 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import lombok.var;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Strings.nullToEmpty;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This creates a circular dependency while reading or writing the data about these segments from the metadata segments.\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    /**\n+     * Serializer for {@link SystemJournalRecordBatch}.\n+     */\n+    private static final SystemJournalRecordBatch.SystemJournalRecordBatchSerializer BATCH_SERIALIZER = new SystemJournalRecordBatch.SystemJournalRecordBatchSerializer();\n+\n+    /**\n+     * Serializer for {@link SystemSnapshotRecord}.\n+     */\n+    private static final SystemSnapshotRecord.Serializer SYSTEM_SNAPSHOT_SERIALIZER = new SystemSnapshotRecord.Serializer();\n+\n+    private final Object lock = new Object();\n+\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    @Getter\n+    private final ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private final long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private final int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Configuration {@link ChunkManagerConfig} for the {@link ChunkManager}.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    private final AtomicBoolean reentryGuard = new AtomicBoolean();\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId   Container id of the owner container.\n+     * @param epoch         Epoch of the current container instance.\n+     * @param chunkStorage  ChunkStorage instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, ChunkManagerConfig config) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param record Record to persist.\n+     * @throws ChunkStorageException Exception if any.\n+     */\n+    public void commitRecord(SystemJournalRecord record) throws ChunkStorageException {\n+        commitRecords(Collections.singletonList(record));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws ChunkStorageException Exception in case of any error.\n+     */\n+    public void commitRecords(Collection<SystemJournalRecord> records) throws ChunkStorageException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+\n+        // Open the underlying chunk to write.\n+        ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+        SystemJournalRecordBatch batch = SystemJournalRecordBatch.builder().systemJournalRecords(records).build();\n+        ByteArraySegment bytes;\n+        try {\n+            bytes = BATCH_SERIALIZER.serialize(batch);\n+        } catch (IOException e) {\n+            throw new ChunkStorageException(getSystemJournalFileName(), \"Unable to serialize\", e);\n+        }\n+        // Persist\n+        synchronized (lock) {\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.getLength(),\n+                    new ByteArrayInputStream(bytes.array(), bytes.arrayOffset(), bytes.getLength()));\n+            Preconditions.checkState(bytesWritten == bytes.getLength());\n+            systemJournalOffset += bytesWritten;\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend() || config.isAppendsDisabled()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+        log.debug(\"SystemJournal[{}] Logging system log records - file={}, batch ={}.\", containerId, h.getChunkName(), batch);\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    public void bootstrap() throws Exception {\n+        Preconditions.checkState(!reentryGuard.getAndSet(true), \"bootstrap called multiple times.\");\n+        try (val txn = metadataStore.beginTransaction()) {\n+            // Keep track of offsets at which chunks were added to the system segments.\n+            val chunkStartOffsets = new HashMap<String, Long>();\n+\n+            // Keep track of offsets at which system segments were truncated.\n+            // We don't need to apply each truncate operation, only need to apply the final truncate offset.\n+            val finalTruncateOffsets = new HashMap<String, Long>();\n+            val finalFirstChunkStartsAtOffsets = new HashMap<String, Long>();\n+\n+            // Step 1: Create metadata records for system segments from latest snapshot.\n+            val epochToStart = applyLatestSnapshot(txn, chunkStartOffsets);\n+\n+            // Step 2: For each epoch, find the corresponding system journal files, process them and apply operations recorded.\n+            applySystemLogOperations(txn, epochToStart, chunkStartOffsets, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 3: Adjust the length of the last chunk.\n+            adjustLastChunkLengths(txn);\n+\n+            // Step 4: Apply the truncate offsets.\n+            applyFinalTruncateOffsets(txn, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 5: Validate and save a snapshot.\n+            validateAndSaveSnapshot(txn);\n+\n+            // Step 5: Finally commit all data.\n+            txn.commit(true, true);\n+        }\n+    }\n+\n+    /**\n+     * Find and apply latest snapshot.\n+     */\n+    private long applyLatestSnapshot(MetadataTransaction txn, HashMap<String, Long> chunkStartOffsets) throws Exception {\n+        long epochToCheck = epoch - 1;\n+        String snapshotFile = null;\n+        boolean found = false;\n+\n+        // Find latest epoch with snapshot.\n+        for (epochToCheck = epoch - 1; epochToCheck >= 0; epochToCheck--) {\n+            snapshotFile = getSystemJournalFileName(containerId, epochToCheck, 0);\n+            if (chunkStorage.exists(snapshotFile)) {\n+                // Read contents.\n+                byte[] contents = getContents(snapshotFile);\n+                SystemSnapshotRecord systemSnapshot = SYSTEM_SNAPSHOT_SERIALIZER.deserialize(contents);\n+                if (null != systemSnapshot) {\n+                    log.debug(\"SystemJournal[{}] Processing system log snalpshot {}.\", containerId, systemSnapshot);\n+                    // Initialize the segments and thier chunks.\n+                    for (SegmentSnapshotRecord segmentSnapshot : systemSnapshot.segmentSnapshotRecords) {\n+                        // Update segment data.\n+                        segmentSnapshot.segmentMetadata.setActive(true)\n+                                .setOwnershipChanged(true)\n+                                .setStorageSystemSegment(true);\n+                        segmentSnapshot.segmentMetadata.setOwnerEpoch(epoch);\n+\n+                        // Add segment data.\n+                        txn.create(segmentSnapshot.segmentMetadata);\n+\n+                        // make sure that the record is marked pinned.\n+                        txn.markPinned(segmentSnapshot.segmentMetadata);\n+\n+                        // Add chunk metadata and keep track of start offsets for each chunk.\n+                        long offset = segmentSnapshot.segmentMetadata.getFirstChunkStartOffset();\n+                        for (ChunkMetadata metadata : segmentSnapshot.chunkMetadataCollection) {\n+                            txn.create(metadata);\n+\n+                            // make sure that the record is marked pinned.\n+                            txn.markPinned(metadata);\n+\n+                            chunkStartOffsets.put(metadata.getName(), offset);\n+                            offset += metadata.getLength();\n+                        }\n+                        found = true;\n+                    }\n+                    break;\n+                }\n+            }\n+        }\n+        if (!found) {\n+            for (String systemSegment : systemSegments) {\n+                SegmentMetadata segmentMetadata = SegmentMetadata.builder()\n+                        .name(systemSegment)\n+                        .ownerEpoch(epoch)\n+                        .maxRollinglength(config.getDefaultRollingPolicy().getMaxLength())\n+                        .build();\n+                segmentMetadata.setActive(true)\n+                        .setOwnershipChanged(true)\n+                        .setStorageSystemSegment(true);\n+                segmentMetadata.checkInvariants();\n+                txn.create(segmentMetadata);\n+                txn.markPinned(segmentMetadata);\n+            }\n+        }\n+        return epochToCheck;\n+    }\n+\n+    /**\n+     * Read contents from file.\n+     */\n+    private byte[] getContents(String snapshotFile) throws ChunkStorageException {\n+        val info = chunkStorage.getInfo(snapshotFile);\n+        val h = chunkStorage.openRead(snapshotFile);\n+        byte[] contents = new byte[Math.toIntExact(info.getLength())];\n+        long fromOffset = 0;\n+        int remaining = contents.length;\n+        while (remaining > 0) {\n+            int bytesRead = chunkStorage.read(h, fromOffset, remaining, contents, Math.toIntExact(fromOffset));\n+            remaining -= bytesRead;\n+            fromOffset += bytesRead;\n+        }\n+        return contents;\n+    }\n+\n+    /**\n+     * Process all systemLog entries to recreate the state of metadata storage system segments.\n+     */\n+    private void applySystemLogOperations(MetadataTransaction txn,\n+                                          long epochToStartScanning,\n+                                          HashMap<String, Long> chunkStartOffsets,\n+                                          HashMap<String, Long> finalTruncateOffsets,\n+                                          HashMap<String, Long> finalFirstChunkStartsAtOffsets) throws ChunkStorageException, IOException, StorageMetadataException {\n+        for (long epochToRecover = epochToStartScanning; epochToRecover < epoch; epochToRecover++) {\n+            // Start scan with file index 1.\n+            int fileIndexToRecover = 1;\n+            while (chunkStorage.exists(getSystemJournalFileName(containerId, epochToRecover, fileIndexToRecover))) {\n+                // Read contents.\n+                val systemLogName = getSystemJournalFileName(containerId, epochToRecover, fileIndexToRecover);\n+                byte[] contents = getContents(systemLogName);\n+                var input = new ByteArrayInputStream(contents);\n+\n+                // Apply record batches from the file.\n+                // Loop is exited with eventual EOFException.\n+                while (true) {\n+                    try {\n+                        val batch = BATCH_SERIALIZER.deserialize(input);\n+                        if (null != batch.getSystemJournalRecords()) {\n+                            for (var record : batch.getSystemJournalRecords()) {\n+                                log.debug(\"SystemJournal[{}] Processing system log record ={}.\", epoch, record);\n+                                // ChunkAddedRecord.\n+                                if (record instanceof ChunkAddedRecord) {\n+                                    val chunkAddedRecord = (ChunkAddedRecord) record;\n+                                    applyChunkAddition(txn, chunkStartOffsets,\n+                                            chunkAddedRecord.getSegmentName(),\n+                                            nullToEmpty(chunkAddedRecord.getOldChunkName()),\n+                                            chunkAddedRecord.getNewChunkName(),\n+                                            chunkAddedRecord.getOffset());\n+                                }\n+\n+                                // TruncationRecord.\n+                                if (record instanceof TruncationRecord) {\n+                                    val truncationRecord = (TruncationRecord) record;\n+                                    finalTruncateOffsets.put(truncationRecord.getSegmentName(), truncationRecord.getOffset());\n+                                    finalFirstChunkStartsAtOffsets.put(truncationRecord.getSegmentName(), truncationRecord.getStartOffset());\n+                                }\n+                            }\n+                        }\n+                    } catch (EOFException e) {\n+                        log.debug(\"SystemJournal[{}] Done processing file {}.\", containerId, systemLogName);\n+                        break;\n+                    }\n+                }\n+                // Move to next file.\n+                fileIndexToRecover++;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Adjusts the lengths of last chunks for each segment.\n+     */\n+    private void adjustLastChunkLengths(MetadataTransaction txn) throws StorageMetadataException, ChunkStorageException {\n+        for (String systemSegment : systemSegments) {\n+            SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(systemSegment);\n+            segmentMetadata.checkInvariants();\n+            // Update length of last chunk in metadata to what we actually find on LTS.\n+            if (null != segmentMetadata.getLastChunk()) {\n+                val chunkInfo = chunkStorage.getInfo(segmentMetadata.getLastChunk());\n+                long length = chunkInfo.getLength();\n+\n+                ChunkMetadata lastChunk = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                Preconditions.checkState(null != lastChunk);\n+                lastChunk.setLength(length);\n+                txn.update(lastChunk);\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + length);\n+            }\n+            Preconditions.checkState(segmentMetadata.isOwnershipChanged());\n+            segmentMetadata.checkInvariants();\n+            txn.update(segmentMetadata);\n+        }\n+    }\n+\n+    /**\n+     * Apply last effective truncate offsets.\n+     */\n+    private void applyFinalTruncateOffsets(MetadataTransaction txn, HashMap<String, Long> finalTruncateOffsets, HashMap<String, Long> finalFirstChunkStartsAtOffsets) throws StorageMetadataException {\n+        for (String systemSegment : systemSegments) {\n+            if (finalTruncateOffsets.containsKey(systemSegment)) {\n+                val truncateAt = finalTruncateOffsets.get(systemSegment);\n+                val firstChunkStartsAt = finalFirstChunkStartsAtOffsets.get(systemSegment);\n+                applyTruncate(txn, systemSegment, truncateAt, firstChunkStartsAt);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Apply chunk addition.\n+     */\n+    private void applyChunkAddition(MetadataTransaction txn, HashMap<String, Long> chunkStartOffsets, String segmentName, String oldChunkName, String newChunkName, long offset) throws StorageMetadataException {\n+        Preconditions.checkState(null != oldChunkName);\n+        Preconditions.checkState(null != newChunkName && !newChunkName.isEmpty());\n+\n+        SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(segmentName);\n+        segmentMetadata.checkInvariants();\n+\n+        // set length.\n+        segmentMetadata.setLength(offset);\n+\n+        val newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        txn.create(newChunkMetadata);\n+        txn.markPinned(newChunkMetadata);\n+\n+        chunkStartOffsets.put(newChunkName, offset);\n+        // Set first and last pointers.\n+        if (!oldChunkName.isEmpty()) {\n+            ChunkMetadata oldChunk = (ChunkMetadata) txn.get(oldChunkName);\n+            Preconditions.checkState(null != oldChunk);\n+\n+            // In case the old segment store was still writing some zombie chunks when ownership changed\n+            // then new offset may invalidate tail part of chunk list.\n+            // Note that chunk with oldChunkName is still valid, it is the chunks after this that become invalid.\n+            String toDelete = oldChunk.getNextChunk();\n+            while (toDelete != null) {\n+                ChunkMetadata chunkToDelete = (ChunkMetadata) txn.get(toDelete);\n+                txn.delete(toDelete);\n+                toDelete = chunkToDelete.getNextChunk();\n+                segmentMetadata.decrementChunkCount();\n+            }\n+\n+            // Set next chunk\n+            oldChunk.setNextChunk(newChunkName);\n+\n+            // Set length\n+            long oldLength = chunkStartOffsets.get(oldChunkName);\n+            oldChunk.setLength(offset - oldLength);\n+\n+            txn.update(oldChunk);\n+        } else {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+            segmentMetadata.setStartOffset(offset);\n+        }\n+        segmentMetadata.setLastChunk(newChunkName);\n+        segmentMetadata.setLastChunkStartOffset(offset);\n+        segmentMetadata.incrementChunkCount();\n+        segmentMetadata.checkInvariants();\n+        // Save the segment metadata.\n+        txn.update(segmentMetadata);\n+    }\n+\n+    private String getSystemJournalFileName() {", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIzNjc3Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446236772", "bodyText": "ok changing it to getSystemJournalChunkName.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T14:58:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwODcwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3NDA5OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446274098", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T16:04:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwODcwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwOTIwNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446109205", "bodyText": "Do these data classes need to be public?", "author": "fpj", "createdAt": "2020-06-26T10:44:34Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,940 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import lombok.var;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Strings.nullToEmpty;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This creates a circular dependency while reading or writing the data about these segments from the metadata segments.\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    /**\n+     * Serializer for {@link SystemJournalRecordBatch}.\n+     */\n+    private static final SystemJournalRecordBatch.SystemJournalRecordBatchSerializer BATCH_SERIALIZER = new SystemJournalRecordBatch.SystemJournalRecordBatchSerializer();\n+\n+    /**\n+     * Serializer for {@link SystemSnapshotRecord}.\n+     */\n+    private static final SystemSnapshotRecord.Serializer SYSTEM_SNAPSHOT_SERIALIZER = new SystemSnapshotRecord.Serializer();\n+\n+    private final Object lock = new Object();\n+\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    @Getter\n+    private final ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private final long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private final int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Configuration {@link ChunkManagerConfig} for the {@link ChunkManager}.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    private final AtomicBoolean reentryGuard = new AtomicBoolean();\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId   Container id of the owner container.\n+     * @param epoch         Epoch of the current container instance.\n+     * @param chunkStorage  ChunkStorage instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, ChunkManagerConfig config) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param record Record to persist.\n+     * @throws ChunkStorageException Exception if any.\n+     */\n+    public void commitRecord(SystemJournalRecord record) throws ChunkStorageException {\n+        commitRecords(Collections.singletonList(record));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws ChunkStorageException Exception in case of any error.\n+     */\n+    public void commitRecords(Collection<SystemJournalRecord> records) throws ChunkStorageException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+\n+        // Open the underlying chunk to write.\n+        ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+        SystemJournalRecordBatch batch = SystemJournalRecordBatch.builder().systemJournalRecords(records).build();\n+        ByteArraySegment bytes;\n+        try {\n+            bytes = BATCH_SERIALIZER.serialize(batch);\n+        } catch (IOException e) {\n+            throw new ChunkStorageException(getSystemJournalFileName(), \"Unable to serialize\", e);\n+        }\n+        // Persist\n+        synchronized (lock) {\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.getLength(),\n+                    new ByteArrayInputStream(bytes.array(), bytes.arrayOffset(), bytes.getLength()));\n+            Preconditions.checkState(bytesWritten == bytes.getLength());\n+            systemJournalOffset += bytesWritten;\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend() || config.isAppendsDisabled()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+        log.debug(\"SystemJournal[{}] Logging system log records - file={}, batch ={}.\", containerId, h.getChunkName(), batch);\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    public void bootstrap() throws Exception {\n+        Preconditions.checkState(!reentryGuard.getAndSet(true), \"bootstrap called multiple times.\");\n+        try (val txn = metadataStore.beginTransaction()) {\n+            // Keep track of offsets at which chunks were added to the system segments.\n+            val chunkStartOffsets = new HashMap<String, Long>();\n+\n+            // Keep track of offsets at which system segments were truncated.\n+            // We don't need to apply each truncate operation, only need to apply the final truncate offset.\n+            val finalTruncateOffsets = new HashMap<String, Long>();\n+            val finalFirstChunkStartsAtOffsets = new HashMap<String, Long>();\n+\n+            // Step 1: Create metadata records for system segments from latest snapshot.\n+            val epochToStart = applyLatestSnapshot(txn, chunkStartOffsets);\n+\n+            // Step 2: For each epoch, find the corresponding system journal files, process them and apply operations recorded.\n+            applySystemLogOperations(txn, epochToStart, chunkStartOffsets, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 3: Adjust the length of the last chunk.\n+            adjustLastChunkLengths(txn);\n+\n+            // Step 4: Apply the truncate offsets.\n+            applyFinalTruncateOffsets(txn, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 5: Validate and save a snapshot.\n+            validateAndSaveSnapshot(txn);\n+\n+            // Step 5: Finally commit all data.\n+            txn.commit(true, true);\n+        }\n+    }\n+\n+    /**\n+     * Find and apply latest snapshot.\n+     */\n+    private long applyLatestSnapshot(MetadataTransaction txn, HashMap<String, Long> chunkStartOffsets) throws Exception {\n+        long epochToCheck = epoch - 1;\n+        String snapshotFile = null;\n+        boolean found = false;\n+\n+        // Find latest epoch with snapshot.\n+        for (epochToCheck = epoch - 1; epochToCheck >= 0; epochToCheck--) {\n+            snapshotFile = getSystemJournalFileName(containerId, epochToCheck, 0);\n+            if (chunkStorage.exists(snapshotFile)) {\n+                // Read contents.\n+                byte[] contents = getContents(snapshotFile);\n+                SystemSnapshotRecord systemSnapshot = SYSTEM_SNAPSHOT_SERIALIZER.deserialize(contents);\n+                if (null != systemSnapshot) {\n+                    log.debug(\"SystemJournal[{}] Processing system log snalpshot {}.\", containerId, systemSnapshot);\n+                    // Initialize the segments and thier chunks.\n+                    for (SegmentSnapshotRecord segmentSnapshot : systemSnapshot.segmentSnapshotRecords) {\n+                        // Update segment data.\n+                        segmentSnapshot.segmentMetadata.setActive(true)\n+                                .setOwnershipChanged(true)\n+                                .setStorageSystemSegment(true);\n+                        segmentSnapshot.segmentMetadata.setOwnerEpoch(epoch);\n+\n+                        // Add segment data.\n+                        txn.create(segmentSnapshot.segmentMetadata);\n+\n+                        // make sure that the record is marked pinned.\n+                        txn.markPinned(segmentSnapshot.segmentMetadata);\n+\n+                        // Add chunk metadata and keep track of start offsets for each chunk.\n+                        long offset = segmentSnapshot.segmentMetadata.getFirstChunkStartOffset();\n+                        for (ChunkMetadata metadata : segmentSnapshot.chunkMetadataCollection) {\n+                            txn.create(metadata);\n+\n+                            // make sure that the record is marked pinned.\n+                            txn.markPinned(metadata);\n+\n+                            chunkStartOffsets.put(metadata.getName(), offset);\n+                            offset += metadata.getLength();\n+                        }\n+                        found = true;\n+                    }\n+                    break;\n+                }\n+            }\n+        }\n+        if (!found) {\n+            for (String systemSegment : systemSegments) {\n+                SegmentMetadata segmentMetadata = SegmentMetadata.builder()\n+                        .name(systemSegment)\n+                        .ownerEpoch(epoch)\n+                        .maxRollinglength(config.getDefaultRollingPolicy().getMaxLength())\n+                        .build();\n+                segmentMetadata.setActive(true)\n+                        .setOwnershipChanged(true)\n+                        .setStorageSystemSegment(true);\n+                segmentMetadata.checkInvariants();\n+                txn.create(segmentMetadata);\n+                txn.markPinned(segmentMetadata);\n+            }\n+        }\n+        return epochToCheck;\n+    }\n+\n+    /**\n+     * Read contents from file.\n+     */\n+    private byte[] getContents(String snapshotFile) throws ChunkStorageException {\n+        val info = chunkStorage.getInfo(snapshotFile);\n+        val h = chunkStorage.openRead(snapshotFile);\n+        byte[] contents = new byte[Math.toIntExact(info.getLength())];\n+        long fromOffset = 0;\n+        int remaining = contents.length;\n+        while (remaining > 0) {\n+            int bytesRead = chunkStorage.read(h, fromOffset, remaining, contents, Math.toIntExact(fromOffset));\n+            remaining -= bytesRead;\n+            fromOffset += bytesRead;\n+        }\n+        return contents;\n+    }\n+\n+    /**\n+     * Process all systemLog entries to recreate the state of metadata storage system segments.\n+     */\n+    private void applySystemLogOperations(MetadataTransaction txn,\n+                                          long epochToStartScanning,\n+                                          HashMap<String, Long> chunkStartOffsets,\n+                                          HashMap<String, Long> finalTruncateOffsets,\n+                                          HashMap<String, Long> finalFirstChunkStartsAtOffsets) throws ChunkStorageException, IOException, StorageMetadataException {\n+        for (long epochToRecover = epochToStartScanning; epochToRecover < epoch; epochToRecover++) {\n+            // Start scan with file index 1.\n+            int fileIndexToRecover = 1;\n+            while (chunkStorage.exists(getSystemJournalFileName(containerId, epochToRecover, fileIndexToRecover))) {\n+                // Read contents.\n+                val systemLogName = getSystemJournalFileName(containerId, epochToRecover, fileIndexToRecover);\n+                byte[] contents = getContents(systemLogName);\n+                var input = new ByteArrayInputStream(contents);\n+\n+                // Apply record batches from the file.\n+                // Loop is exited with eventual EOFException.\n+                while (true) {\n+                    try {\n+                        val batch = BATCH_SERIALIZER.deserialize(input);\n+                        if (null != batch.getSystemJournalRecords()) {\n+                            for (var record : batch.getSystemJournalRecords()) {\n+                                log.debug(\"SystemJournal[{}] Processing system log record ={}.\", epoch, record);\n+                                // ChunkAddedRecord.\n+                                if (record instanceof ChunkAddedRecord) {\n+                                    val chunkAddedRecord = (ChunkAddedRecord) record;\n+                                    applyChunkAddition(txn, chunkStartOffsets,\n+                                            chunkAddedRecord.getSegmentName(),\n+                                            nullToEmpty(chunkAddedRecord.getOldChunkName()),\n+                                            chunkAddedRecord.getNewChunkName(),\n+                                            chunkAddedRecord.getOffset());\n+                                }\n+\n+                                // TruncationRecord.\n+                                if (record instanceof TruncationRecord) {\n+                                    val truncationRecord = (TruncationRecord) record;\n+                                    finalTruncateOffsets.put(truncationRecord.getSegmentName(), truncationRecord.getOffset());\n+                                    finalFirstChunkStartsAtOffsets.put(truncationRecord.getSegmentName(), truncationRecord.getStartOffset());\n+                                }\n+                            }\n+                        }\n+                    } catch (EOFException e) {\n+                        log.debug(\"SystemJournal[{}] Done processing file {}.\", containerId, systemLogName);\n+                        break;\n+                    }\n+                }\n+                // Move to next file.\n+                fileIndexToRecover++;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Adjusts the lengths of last chunks for each segment.\n+     */\n+    private void adjustLastChunkLengths(MetadataTransaction txn) throws StorageMetadataException, ChunkStorageException {\n+        for (String systemSegment : systemSegments) {\n+            SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(systemSegment);\n+            segmentMetadata.checkInvariants();\n+            // Update length of last chunk in metadata to what we actually find on LTS.\n+            if (null != segmentMetadata.getLastChunk()) {\n+                val chunkInfo = chunkStorage.getInfo(segmentMetadata.getLastChunk());\n+                long length = chunkInfo.getLength();\n+\n+                ChunkMetadata lastChunk = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                Preconditions.checkState(null != lastChunk);\n+                lastChunk.setLength(length);\n+                txn.update(lastChunk);\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + length);\n+            }\n+            Preconditions.checkState(segmentMetadata.isOwnershipChanged());\n+            segmentMetadata.checkInvariants();\n+            txn.update(segmentMetadata);\n+        }\n+    }\n+\n+    /**\n+     * Apply last effective truncate offsets.\n+     */\n+    private void applyFinalTruncateOffsets(MetadataTransaction txn, HashMap<String, Long> finalTruncateOffsets, HashMap<String, Long> finalFirstChunkStartsAtOffsets) throws StorageMetadataException {\n+        for (String systemSegment : systemSegments) {\n+            if (finalTruncateOffsets.containsKey(systemSegment)) {\n+                val truncateAt = finalTruncateOffsets.get(systemSegment);\n+                val firstChunkStartsAt = finalFirstChunkStartsAtOffsets.get(systemSegment);\n+                applyTruncate(txn, systemSegment, truncateAt, firstChunkStartsAt);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Apply chunk addition.\n+     */\n+    private void applyChunkAddition(MetadataTransaction txn, HashMap<String, Long> chunkStartOffsets, String segmentName, String oldChunkName, String newChunkName, long offset) throws StorageMetadataException {\n+        Preconditions.checkState(null != oldChunkName);\n+        Preconditions.checkState(null != newChunkName && !newChunkName.isEmpty());\n+\n+        SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(segmentName);\n+        segmentMetadata.checkInvariants();\n+\n+        // set length.\n+        segmentMetadata.setLength(offset);\n+\n+        val newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        txn.create(newChunkMetadata);\n+        txn.markPinned(newChunkMetadata);\n+\n+        chunkStartOffsets.put(newChunkName, offset);\n+        // Set first and last pointers.\n+        if (!oldChunkName.isEmpty()) {\n+            ChunkMetadata oldChunk = (ChunkMetadata) txn.get(oldChunkName);\n+            Preconditions.checkState(null != oldChunk);\n+\n+            // In case the old segment store was still writing some zombie chunks when ownership changed\n+            // then new offset may invalidate tail part of chunk list.\n+            // Note that chunk with oldChunkName is still valid, it is the chunks after this that become invalid.\n+            String toDelete = oldChunk.getNextChunk();\n+            while (toDelete != null) {\n+                ChunkMetadata chunkToDelete = (ChunkMetadata) txn.get(toDelete);\n+                txn.delete(toDelete);\n+                toDelete = chunkToDelete.getNextChunk();\n+                segmentMetadata.decrementChunkCount();\n+            }\n+\n+            // Set next chunk\n+            oldChunk.setNextChunk(newChunkName);\n+\n+            // Set length\n+            long oldLength = chunkStartOffsets.get(oldChunkName);\n+            oldChunk.setLength(offset - oldLength);\n+\n+            txn.update(oldChunk);\n+        } else {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+            segmentMetadata.setStartOffset(offset);\n+        }\n+        segmentMetadata.setLastChunk(newChunkName);\n+        segmentMetadata.setLastChunkStartOffset(offset);\n+        segmentMetadata.incrementChunkCount();\n+        segmentMetadata.checkInvariants();\n+        // Save the segment metadata.\n+        txn.update(segmentMetadata);\n+    }\n+\n+    private String getSystemJournalFileName() {\n+        return getSystemJournalFileName(containerId, epoch, currentFileIndex);\n+    }\n+\n+    private String getSystemJournalFileName(int containerId, long epoch, long currentFileIndex) {\n+        return NameUtils.getSystemJournalFileName(containerId, epoch, currentFileIndex);\n+    }\n+\n+    private ChunkHandle getChunkHandleForSystemJournal() throws ChunkStorageException {\n+        ChunkHandle h;\n+        val systemLogName = getSystemJournalFileName();\n+        try {\n+            h = chunkStorage.openWrite(systemLogName);\n+        } catch (ChunkNotFoundException e) {\n+            h = chunkStorage.create(systemLogName);\n+        }\n+        return h;\n+    }\n+\n+    /**\n+     * Apply truncate action to the segment metadata.\n+     */\n+    private void applyTruncate(MetadataTransaction txn, String segmentName, long truncateAt, long firstChunkStartsAt) throws StorageMetadataException {\n+        SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(segmentName);\n+        segmentMetadata.checkInvariants();\n+        String currentChunkName = segmentMetadata.getFirstChunk();\n+        ChunkMetadata currentMetadata;\n+        long startOffset = segmentMetadata.getFirstChunkStartOffset();\n+        while (null != currentChunkName) {\n+            currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+            // If for given chunk start <= truncateAt < end  then we have found the chunk that will be the first chunk.\n+            if ((startOffset <= truncateAt) && (startOffset + currentMetadata.getLength() > truncateAt)) {\n+                break;\n+            }\n+\n+            startOffset += currentMetadata.getLength();\n+            // move to next chunk\n+            currentChunkName = currentMetadata.getNextChunk();\n+            txn.delete(currentMetadata.getName());\n+            segmentMetadata.decrementChunkCount();\n+        }\n+        Preconditions.checkState(firstChunkStartsAt == startOffset);\n+        segmentMetadata.setFirstChunk(currentChunkName);\n+        if (null == currentChunkName) {\n+            segmentMetadata.setLastChunk(null);\n+            segmentMetadata.setLastChunkStartOffset(firstChunkStartsAt);\n+        }\n+        segmentMetadata.setStartOffset(truncateAt);\n+        segmentMetadata.setFirstChunkStartOffset(firstChunkStartsAt);\n+        segmentMetadata.checkInvariants();\n+\n+    }\n+\n+    public void validateAndSaveSnapshot(MetadataTransaction txn) throws Exception {\n+        SystemSnapshotRecord systemSnapshot = SystemSnapshotRecord.builder()\n+                .epoch(epoch)\n+                .segmentSnapshotRecords(new ArrayList<>())\n+                .build();\n+\n+        for (String systemSegment : systemSegments) {\n+            // Find segment metadata.\n+            SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(systemSegment);\n+            segmentMetadata.checkInvariants();\n+\n+            SegmentSnapshotRecord segmentSnapshot = SegmentSnapshotRecord.builder()\n+                    .segmentMetadata(segmentMetadata)\n+                    .chunkMetadataCollection(new ArrayList<>())\n+                    .build();\n+\n+            // Enumerate all chunks.\n+            String currentChunkName = segmentMetadata.getFirstChunk();\n+            ChunkMetadata currentMetadata = null;\n+            long dataSize = 0;\n+            long chunkCount = 0;\n+            while (null != currentChunkName) {\n+                currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+\n+                val chunkInfo = chunkStorage.getInfo(currentChunkName);\n+                dataSize += currentMetadata.getLength();\n+                chunkCount++;\n+                Preconditions.checkState(chunkInfo.getLength() >= currentMetadata.getLength(),\n+                        \"Wrong chunk length chunkInfo=%d, currentMetadata=%d.\", chunkInfo.getLength(), currentMetadata.getLength());\n+\n+                segmentSnapshot.chunkMetadataCollection.add(currentMetadata);\n+                // move to next chunk\n+                currentChunkName = currentMetadata.getNextChunk();\n+            }\n+\n+            // Validate\n+            Preconditions.checkState(chunkCount == segmentMetadata.getChunkCount(), \"Wrong chunk count.\");\n+            Preconditions.checkState(dataSize == segmentMetadata.getLength() - segmentMetadata.getFirstChunkStartOffset(), \"Data size does not match dataSize.\");\n+\n+            // Add to the system snapshot.\n+            systemSnapshot.segmentSnapshotRecords.add(segmentSnapshot);\n+        }\n+\n+        // Write snapshot\n+        val snapshotFile = getSystemJournalFileName(containerId, epoch, 0);\n+        ChunkHandle h = chunkStorage.create(snapshotFile);\n+        ByteArraySegment bytes;\n+        try {\n+            bytes = SYSTEM_SNAPSHOT_SERIALIZER.serialize(systemSnapshot);\n+        } catch (IOException e) {\n+            throw new ChunkStorageException(getSystemJournalFileName(), \"Unable to serialize\", e);\n+        }\n+        // Persist\n+        synchronized (lock) {\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.getLength(),\n+                    new ByteArrayInputStream(bytes.array(), bytes.arrayOffset(), bytes.getLength()));\n+            Preconditions.checkState(bytesWritten == bytes.getLength());\n+        }\n+        currentFileIndex++;\n+    }\n+\n+    /**\n+     * Indicates whether given segment is a system segment.\n+     *\n+     * @param segmentName Name of the sgement to check.\n+     * @return True if given segment is a system segment.\n+     */\n+    public boolean isStorageSystemSegment(String segmentName) {\n+        if (segmentName.startsWith(systemSegmentsPrefix)) {\n+            for (String systemSegment : systemSegments) {\n+                if (segmentName.equals(systemSegment)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        return false;\n+    }\n+\n+    /**\n+     * Gets the names of the critical storage segments.\n+     *\n+     * @param containerId Container if of the owner container.\n+     * @return Array of names of the critical storage segments.\n+     */\n+    public static String[] getChunkStorageSystemSegments(int containerId) {\n+        return new String[]{\n+                NameUtils.getStorageMetadataSegmentName(containerId),\n+                NameUtils.getAttributeSegmentName(NameUtils.getStorageMetadataSegmentName(containerId)),\n+                NameUtils.getMetadataSegmentName(containerId),\n+                NameUtils.getAttributeSegmentName(NameUtils.getMetadataSegmentName(containerId))\n+        };\n+    }\n+\n+    /**\n+     * Represents a system journal record.\n+     */\n+    @Data\n+    public static class SystemJournalRecord {", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIzNzI0Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446237246", "bodyText": "not really.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T14:59:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwOTIwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3Mzk3MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446273970", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T16:04:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwOTIwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMDkxMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446110913", "bodyText": "For readability and organization, should we move this up in the class definition? to be the first after the constructor?", "author": "fpj", "createdAt": "2020-06-26T10:48:52Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,940 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import lombok.var;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Strings.nullToEmpty;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This creates a circular dependency while reading or writing the data about these segments from the metadata segments.\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    /**\n+     * Serializer for {@link SystemJournalRecordBatch}.\n+     */\n+    private static final SystemJournalRecordBatch.SystemJournalRecordBatchSerializer BATCH_SERIALIZER = new SystemJournalRecordBatch.SystemJournalRecordBatchSerializer();\n+\n+    /**\n+     * Serializer for {@link SystemSnapshotRecord}.\n+     */\n+    private static final SystemSnapshotRecord.Serializer SYSTEM_SNAPSHOT_SERIALIZER = new SystemSnapshotRecord.Serializer();\n+\n+    private final Object lock = new Object();\n+\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    @Getter\n+    private final ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private final long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private final int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Configuration {@link ChunkManagerConfig} for the {@link ChunkManager}.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    private final AtomicBoolean reentryGuard = new AtomicBoolean();\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId   Container id of the owner container.\n+     * @param epoch         Epoch of the current container instance.\n+     * @param chunkStorage  ChunkStorage instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, ChunkManagerConfig config) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param record Record to persist.\n+     * @throws ChunkStorageException Exception if any.\n+     */\n+    public void commitRecord(SystemJournalRecord record) throws ChunkStorageException {\n+        commitRecords(Collections.singletonList(record));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws ChunkStorageException Exception in case of any error.\n+     */\n+    public void commitRecords(Collection<SystemJournalRecord> records) throws ChunkStorageException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+\n+        // Open the underlying chunk to write.\n+        ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+        SystemJournalRecordBatch batch = SystemJournalRecordBatch.builder().systemJournalRecords(records).build();\n+        ByteArraySegment bytes;\n+        try {\n+            bytes = BATCH_SERIALIZER.serialize(batch);\n+        } catch (IOException e) {\n+            throw new ChunkStorageException(getSystemJournalFileName(), \"Unable to serialize\", e);\n+        }\n+        // Persist\n+        synchronized (lock) {\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.getLength(),\n+                    new ByteArrayInputStream(bytes.array(), bytes.arrayOffset(), bytes.getLength()));\n+            Preconditions.checkState(bytesWritten == bytes.getLength());\n+            systemJournalOffset += bytesWritten;\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend() || config.isAppendsDisabled()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+        log.debug(\"SystemJournal[{}] Logging system log records - file={}, batch ={}.\", containerId, h.getChunkName(), batch);\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    public void bootstrap() throws Exception {", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI2ODA5Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446268092", "bodyText": "It is already immediately after constructor.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T15:54:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMDkxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk4MjUwMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446982501", "bodyText": "No, this is after three methods after the constructor: initialize, commitRecord, commitRecords.", "author": "fpj", "createdAt": "2020-06-29T13:44:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMDkxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAyNDAxMg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447024012", "bodyText": "Ah okay. I was confused about ChunkManager class where boostrap is immediately after constructor.", "author": "sachin-j-joshi", "createdAt": "2020-06-29T14:41:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMDkxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzIxOTg1MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447219850", "bodyText": "updated", "author": "sachin-j-joshi", "createdAt": "2020-06-29T20:01:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMDkxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMjIzNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446112237", "bodyText": "Do I understand right that this system journal has two primary public calls: commitRecord and commitRecords? The records as declared below in the class can be of four types.\nAlso, is the utilization of the system journal in a follow-up PR?", "author": "fpj", "createdAt": "2020-06-26T10:52:10Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,940 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import lombok.var;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Strings.nullToEmpty;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This creates a circular dependency while reading or writing the data about these segments from the metadata segments.\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    /**\n+     * Serializer for {@link SystemJournalRecordBatch}.\n+     */\n+    private static final SystemJournalRecordBatch.SystemJournalRecordBatchSerializer BATCH_SERIALIZER = new SystemJournalRecordBatch.SystemJournalRecordBatchSerializer();\n+\n+    /**\n+     * Serializer for {@link SystemSnapshotRecord}.\n+     */\n+    private static final SystemSnapshotRecord.Serializer SYSTEM_SNAPSHOT_SERIALIZER = new SystemSnapshotRecord.Serializer();\n+\n+    private final Object lock = new Object();\n+\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    @Getter\n+    private final ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private final long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private final int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Configuration {@link ChunkManagerConfig} for the {@link ChunkManager}.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    private final AtomicBoolean reentryGuard = new AtomicBoolean();\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId   Container id of the owner container.\n+     * @param epoch         Epoch of the current container instance.\n+     * @param chunkStorage  ChunkStorage instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, ChunkManagerConfig config) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param record Record to persist.\n+     * @throws ChunkStorageException Exception if any.\n+     */\n+    public void commitRecord(SystemJournalRecord record) throws ChunkStorageException {", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEzNjE0Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446136146", "bodyText": "Also, is the utilization of the system journal in a follow-up PR?\n\nnever mind about this comment, I had ChunkManager marked as viewed and a simple search did not show the system journal being used.", "author": "fpj", "createdAt": "2020-06-26T11:50:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMjIzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIzMTI2OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446231269", "bodyText": "yes bootstrap is called from ChunkManager.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T14:50:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMjIzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMjg5NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446112894", "bodyText": "Maybe list what it is precisely that is recorded in this journal. The comment says that it records layout changes, and there are four types of records that can be recorded.", "author": "fpj", "createdAt": "2020-06-26T10:53:55Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,940 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import lombok.var;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Strings.nullToEmpty;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3Mzc5MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446273790", "bodyText": "updated", "author": "sachin-j-joshi", "createdAt": "2020-06-26T16:04:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMjg5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTA4Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446115083", "bodyText": "Change it back, please.", "author": "fpj", "createdAt": "2020-06-26T10:59:21Z", "path": "segmentstore/storage/src/test/java/io/pravega/segmentstore/storage/mocks/InMemoryStorageTests.java", "diffHunk": "@@ -5,7 +5,7 @@\n  * you may not use this file except in compliance with the License.\n  * You may obtain a copy of the License at\n  *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n+ * http://www.apache.org/licenses/LICENSE-2.0", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIzMTgyMg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446231822", "bodyText": "oops..formatting file messed it up.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T14:50:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTA4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3MzY1MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446273651", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T16:03:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTA4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyOTU0Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446129542", "bodyText": "What pattern have we been using for exceptions? I'm wondering whether we should group the subtypes in the same class definition like in KeeperException:\nhttps://zookeeper.apache.org/doc/r3.6.0/apidocs/zookeeper-server/index.html?org/apache/zookeeper/KeeperException.html\nI don't necessarily want to deviate from how we have been doing it elsewhere in the code.", "author": "fpj", "createdAt": "2020-06-26T11:33:49Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/StorageMetadataException.java", "diffHunk": "@@ -0,0 +1,34 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+/**\n+ * Exception related to storage metadata operations.\n+ */\n+public class StorageMetadataException extends Exception {", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIzMjI5Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446232292", "bodyText": "We are following current convention in the code.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T14:51:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyOTU0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAxNjQ2Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447016466", "bodyText": "Can you justify the use of ConcurrentHashMap vs. other data structure implementations, e.g., LinkedHashMap? I'm thinking that it would avoid the whole management of generations and a full iteration over the map upon eviction.", "author": "fpj", "createdAt": "2020-06-29T14:31:12Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.\n+ */\n+class ReadIndexCache {\n+\n+    /**\n+     * Max number of indexed segments to keep in cache.\n+     */\n+    private final int maxIndexedSegments;\n+\n+    /**\n+     * Max number of indexed chunks to keep in cache.\n+     */\n+    private final int maxIndexedChunks;\n+\n+    /**\n+     * Max number of indexed chunks to keep per segment in cache.\n+     */\n+    private final int maxIndexedChunksPerSegment;\n+\n+    /**\n+     * Current generation of cache entries.\n+     */\n+    private final AtomicLong currentGeneration = new AtomicLong();\n+\n+    /**\n+     * Lowest generation of cache entries.\n+     */\n+    private final AtomicLong oldestGeneration = new AtomicLong();\n+\n+    /**\n+     * Total number of chunks in the cache.\n+     */\n+    private final AtomicInteger totalChunkCount = new AtomicInteger();\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, SegmentReadIndex> segmentsToReadIndexMap = new ConcurrentHashMap<String, SegmentReadIndex>();", "originalCommit": "2c3e4b6cbd0a6aa7519b1c7d46c3f288e0a466d0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA0MDMzOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447040338", "bodyText": "Conceptually - as entries are found and used, they need to be moved to the end of list to keep LRU order.\nLinkedHashMap keeps order by of insertion order, which is different than what we want.", "author": "sachin-j-joshi", "createdAt": "2020-06-29T15:03:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAxNjQ2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzk5ODAxNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447998015", "bodyText": "Resolving as per discussion offline. Tracking this here #4902", "author": "sachin-j-joshi", "createdAt": "2020-06-30T21:46:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAxNjQ2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAxODY2MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447018661", "bodyText": "I'm not sure I follow why we need to update the generation upon truncation.", "author": "fpj", "createdAt": "2020-06-29T14:34:09Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.\n+ */\n+class ReadIndexCache {\n+\n+    /**\n+     * Max number of indexed segments to keep in cache.\n+     */\n+    private final int maxIndexedSegments;\n+\n+    /**\n+     * Max number of indexed chunks to keep in cache.\n+     */\n+    private final int maxIndexedChunks;\n+\n+    /**\n+     * Max number of indexed chunks to keep per segment in cache.\n+     */\n+    private final int maxIndexedChunksPerSegment;\n+\n+    /**\n+     * Current generation of cache entries.\n+     */\n+    private final AtomicLong currentGeneration = new AtomicLong();\n+\n+    /**\n+     * Lowest generation of cache entries.\n+     */\n+    private final AtomicLong oldestGeneration = new AtomicLong();\n+\n+    /**\n+     * Total number of chunks in the cache.\n+     */\n+    private final AtomicInteger totalChunkCount = new AtomicInteger();\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, SegmentReadIndex> segmentsToReadIndexMap = new ConcurrentHashMap<String, SegmentReadIndex>();\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param maxIndexedSegments\n+     * @param maxIndexedChunksPerSegment\n+     * @param maxIndexedChunks\n+     */\n+    public ReadIndexCache(int maxIndexedSegments, int maxIndexedChunksPerSegment, int maxIndexedChunks) {\n+        this.maxIndexedChunksPerSegment = maxIndexedChunksPerSegment;\n+        this.maxIndexedSegments = maxIndexedSegments;\n+        this.maxIndexedChunks = maxIndexedChunks;\n+    }\n+\n+    /**\n+     * Retrieves the read index for given segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @return Read index corresponding to the given segment. A new empty index is created if it doesn't already exist.\n+     */\n+    private SegmentReadIndex getSegmentReadIndex(String streamSegmentName) {\n+        SegmentReadIndex readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+\n+        if (null == readIndex) {\n+            // Evict segments if required.\n+            if (maxIndexedSegments < segmentsToReadIndexMap.size() + 1 || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictSegmentsFromOldestGeneration();\n+            }\n+            val newReadIndex = SegmentReadIndex.builder()\n+                    .chunkIndex(new ConcurrentSkipListMap<Long, SegmentReadIndexEntry>())\n+                    .generation(new AtomicLong(currentGeneration.get()))\n+                    .build();\n+            val oldReadIndex = segmentsToReadIndexMap.putIfAbsent(streamSegmentName, newReadIndex);\n+            readIndex = null != oldReadIndex ? oldReadIndex : newReadIndex;\n+        }\n+\n+        return readIndex;\n+    }\n+\n+    /**\n+     * Gets total number of chunks in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalChunksCount() {\n+        return totalChunkCount.get();\n+    }\n+\n+    /**\n+     * Gets total number of segments in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalSegmentCount() {\n+        return segmentsToReadIndexMap.size();\n+    }\n+\n+    /**\n+     * Gets oldest generation in cache.\n+     *\n+     * @return\n+     */\n+    public long getOldestGeneration() {\n+        return oldestGeneration.get();\n+    }\n+\n+    /**\n+     * Gets current generation of cache.\n+     *\n+     * @return\n+     */\n+    public long getCurrentGeneration() {\n+        return currentGeneration.get();\n+    }\n+\n+    /**\n+     * Adds a new index entry for a given chunk in index for the segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param chunkName         Name of the chunk.\n+     * @param startOffset       Start offset of the chunk.\n+     */\n+    public void addIndexEntry(String streamSegmentName, String chunkName, long startOffset) {\n+        if (null != chunkName) {\n+            val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+\n+            // Evict chunks if required.\n+            if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + 1\n+                    || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictChunks(streamSegmentName, 1);\n+            }\n+\n+            segmentReadIndex.chunkIndex.put(startOffset,\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(chunkName)\n+                            .generation(new AtomicLong(currentGeneration.get()))\n+                            .build());\n+            segmentReadIndex.generation.set(currentGeneration.get());\n+            totalChunkCount.incrementAndGet();\n+        }\n+    }\n+\n+    /**\n+     * Updates read index for given segment with new entries.\n+     *\n+     * @param streamSegmentName\n+     * @param newReadIndexEntries List of {@link ChunkNameOffsetPair} for new entries.\n+     */\n+    public void addIndexEntries(String streamSegmentName, List<ChunkNameOffsetPair> newReadIndexEntries) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        // Evict chunks if required.\n+        if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + newReadIndexEntries.size()\n+                || maxIndexedChunks < totalChunkCount.get() + newReadIndexEntries.size()) {\n+            evictChunks(streamSegmentName, newReadIndexEntries.size());\n+        }\n+\n+        for (val entry : newReadIndexEntries) {\n+            segmentReadIndex.chunkIndex.put(entry.getOffset(),\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(entry.getChunkName())\n+                            .generation(new AtomicLong(currentGeneration.get()))\n+                            .build());\n+            segmentReadIndex.generation.set(currentGeneration.get());\n+\n+        }\n+        totalChunkCount.getAndAdd(newReadIndexEntries.size());\n+    }\n+\n+    /**\n+     * Removes the given segment from cache.\n+     *\n+     * @param streamSegmentName\n+     */\n+    public void remove(String streamSegmentName) {\n+        val readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        if (null != readIndex) {\n+            segmentsToReadIndexMap.remove(streamSegmentName);\n+            totalChunkCount.getAndAdd(-1 * readIndex.chunkIndex.size());\n+        }\n+    }\n+\n+    /**\n+     * Finds a chunk that is floor to the given offset.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset for which to search.\n+     * @return\n+     */\n+    public ChunkNameOffsetPair findFloor(String streamSegmentName, long offset) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        if (segmentReadIndex.chunkIndex.size() > 0) {\n+            val floorEntry = segmentReadIndex.chunkIndex.floorEntry(offset);\n+            if (null != floorEntry) {\n+                // mark with current generation\n+                segmentReadIndex.generation.set(currentGeneration.get());\n+                floorEntry.getValue().generation.set(currentGeneration.get());\n+                // return value.\n+                return new ChunkNameOffsetPair(floorEntry.getKey(), floorEntry.getValue().getChunkName());\n+            }\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Truncates the read index for given segment by removing all the chunks that are below given offset.\n+     *\n+     * @param streamSegmentName\n+     * @param startOffset\n+     */\n+    public void truncateReadIndex(String streamSegmentName, long startOffset) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        if (null != segmentReadIndex) {\n+            if (segmentReadIndex.chunkIndex.size() > 0) {\n+                val headMap = segmentReadIndex.chunkIndex.headMap(startOffset);\n+                if (null != headMap) {\n+                    int removed = 0;\n+                    ArrayList<Long> keysToRemove = new ArrayList<Long>();\n+                    keysToRemove.addAll(headMap.keySet());\n+                    for (val keyToRemove : keysToRemove) {\n+                        segmentReadIndex.chunkIndex.remove(keyToRemove);\n+                        removed++;\n+                    }\n+                    if (removed > 0) {\n+                        totalChunkCount.getAndAdd(-1 * removed);\n+                    }\n+                }\n+            }\n+            if (segmentReadIndex.chunkIndex.size() > 0) {\n+                segmentReadIndex.generation.set(currentGeneration.get());", "originalCommit": "2c3e4b6cbd0a6aa7519b1c7d46c3f288e0a466d0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU2NTc2MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447565760", "bodyText": "Nit: are we using these tags in the rest of documentation? If not, it would be great to keep the same format.", "author": "RaulGracia", "createdAt": "2020-06-30T10:02:22Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,563 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ * <div>", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU2NzcwMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447567701", "bodyText": "Nit: I don't remember any place in the code in which we describe in the Javadoc the places where are a class is tested. One of the reasons may be that this information can get quickly outdated as there is a new test class, or a rename or rellocation of one of the existing test classes. This forces the developer to be aware of updating the documentation of a class based on a work related to test, which looks error-prone.", "author": "RaulGracia", "createdAt": "2020-06-30T10:05:53Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,563 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ * <div>\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ * </div>\n+ * <div>\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkManager will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ * </div>\n+ *\n+ * <div>\n+ * The implementations in this repository are tested using following test suites.", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc0ODI0MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447748240", "bodyText": "This is given as a guidance that  derived classes should be tested using this set of tests. This is helpful reminder for anyone implementing this class.\nWe don't have a separate compatibility test suite as of now.", "author": "sachin-j-joshi", "createdAt": "2020-06-30T14:54:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU2NzcwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODg5NTU2Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r448895566", "bodyText": "I think that any developer is able to locate where a given class is being tested, right? I'm not saying that this information is not useful, what I'm saying is that, as far as I'm aware, nowhere else in the Javadoc we enumerate the test classes related to the class, so this is a comment about consistency. Also, I suspect that this may introduce additional maintenance burden, as we need to keep this part of the Javadoc updated for any changes related to test classes.", "author": "RaulGracia", "createdAt": "2020-07-02T10:09:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU2NzcwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTAxMzEwMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r449013103", "bodyText": "This is intentional.", "author": "sachin-j-joshi", "createdAt": "2020-07-02T13:45:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU2NzcwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU2OTEyNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447569125", "bodyText": "Nit: this is not adding much information.", "author": "RaulGracia", "createdAt": "2020-06-30T10:08:21Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,563 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ * <div>\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ * </div>\n+ * <div>\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkManager will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ * </div>\n+ *\n+ * <div>\n+ * The implementations in this repository are tested using following test suites.\n+ * <ul>\n+ * <li>SimpleStorageTests</li>\n+ * <li>ChunkManagerRollingTests</li>\n+ * <li>ChunkStorageProviderTests</li>\n+ * <li>SystemJournalTests</li>\n+ * </ul>\n+ * </div>\n+ */\n+@Slf4j\n+public abstract class BaseChunkStorage implements ChunkStorage {\n+\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Constructor.\n+     */\n+    public BaseChunkStorage() {\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public boolean exists(String chunkName) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+\n+        // Call concrete implementation.\n+        boolean retValue = checkExists(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Creates a new chunk.\n+     *\n+     * @param chunkName Name of the chunk to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public ChunkHandle create(String chunkName) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"create\", chunkName);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doCreate(chunkName);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageMetrics.CREATE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageMetrics.CREATE_COUNT.inc();\n+\n+        log.debug(\"Create - chunk={}, latency={}.\", chunkName, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"create\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Deletes a chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to delete.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public void delete(ChunkHandle handle) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle.getChunkName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        doDelete(handle);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageMetrics.DELETE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageMetrics.DELETE_COUNT.inc();\n+\n+        log.debug(\"Delete - chunk={}, latency={}.\", handle.getChunkName(), elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"delete\", traceId, handle.getChunkName());\n+\n+    }\n+\n+    /**\n+     * Opens chunk for Read.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkHandle A readable handle for the given chunk.\n+     * @throws ChunkStorageException    Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openRead\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenRead(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openRead\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Opens chunk for Write (or modifications).\n+     *\n+     * @param chunkName String name of the chunk to write to or modify.\n+     * @return ChunkHandle A writable handle for the given chunk.\n+     * @throws ChunkStorageException    Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenWrite(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openWrite\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Retrieves the ChunkInfo for given name.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkInfo Information about the given chunk.\n+     * @throws ChunkStorageException    Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkInfo getInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkNotNull(chunkName);\n+        long traceId = LoggerHelpers.traceEnter(log, \"getInfo\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkInfo info = doGetInfo(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"getInfo\", traceId, chunkName);\n+\n+        return info;\n+    }\n+\n+    /**\n+     * Reads a range of bytes from the underlying chunk.\n+     *\n+     * @param handle       ChunkHandle of the chunk to read from.\n+     * @param fromOffset   Offset in the chunk from which to start reading.\n+     * @param length       Number of bytes to read.\n+     * @param buffer       Byte buffer to which data is copied.\n+     * @param bufferOffset Offset in the buffer at which to start copying read data.\n+     * @return int Number of bytes read.\n+     * @throws ChunkStorageException     Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException  If argument is invalid.\n+     * @throws IndexOutOfBoundsException If the index is out of bounds or offset is not a valid offset in the underlying file/object.\n+     */\n+    @Override\n+    final public int read(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle\");\n+        Preconditions.checkArgument(null != buffer, \"buffer\");\n+        Preconditions.checkArgument(fromOffset >= 0, \"fromOffset must be non-negative\");\n+        Preconditions.checkArgument(length >= 0 && length <= buffer.length, \"length\");\n+        Preconditions.checkElementIndex(bufferOffset, buffer.length, \"bufferOffset\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"read\", handle.getChunkName(), fromOffset, bufferOffset, length);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int bytesRead = doRead(handle, fromOffset, length, buffer, bufferOffset);\n+\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageMetrics.READ_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageMetrics.READ_BYTES.add(bytesRead);\n+\n+        log.debug(\"Read - chunk={}, offset={}, bytesRead={}, latency={}.\", handle.getChunkName(), fromOffset, length, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"read\", traceId, bytesRead);\n+\n+        return bytesRead;\n+    }\n+\n+    /**\n+     * Writes the given data to the underlying chunk.\n+     *\n+     * <ul>\n+     * <li>It is expected that in cases where it can not overwrite the existing data at given offset, the implementation should throw IndexOutOfBoundsException.</li>\n+     * For storage where underlying files/objects are immutable once written, the implementation should return false on {@link ChunkStorage#supportsAppend()}.\n+     * <li>In such cases only valid offset is 0.</li>\n+     * <li>For storages where underlying files/objects can only be appended but not overwritten, it must match actual current length of underlying file/object.</li>\n+     * <li>In all cases the offset can not be greater that actual current length of underlying file/object. </li>\n+     * </ul>\n+     * @param handle ChunkHandle of the chunk to write to.\n+     * @param offset Offset in the chunk to start writing.\n+     * @param length Number of bytes to write.\n+     * @param data   An InputStream representing the data to write.\n+     * @return int Number of bytes written.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public int write(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        Preconditions.checkArgument(null != data, \"data must not be null\");\n+        Preconditions.checkArgument(offset >= 0, \"offset must be non-negative\");\n+        Preconditions.checkArgument(length >= 0, \"length must be non-negative\");\n+        if (!supportsAppend()) {\n+            Preconditions.checkArgument(offset == 0, \"offset must be 0 because storage does not support appends.\");\n+        }\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"write\", handle.getChunkName(), offset, length);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int bytesWritten = doWrite(handle, offset, length, data);\n+\n+        Duration elapsed = timer.getElapsed();\n+\n+        ChunkStorageMetrics.WRITE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageMetrics.WRITE_BYTES.add(bytesWritten);\n+\n+        log.debug(\"Write - chunk={}, offset={}, bytesWritten={}, latency={}.\", handle.getChunkName(), offset, length, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"read\", traceId, bytesWritten);\n+\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Concatenates two or more chunks. The first chunk is concatenated to.\n+     *\n+     * @param chunks Array of ConcatArgument objects containing info about existing chunks to be concatenated together.\n+     *               The chunks must be concatenated in the same sequence the arguments are provided.\n+     * @return int Number of bytes concatenated.\n+     * @throws ChunkStorageException         Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public int concat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        checkConcatArgs(chunks);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"concat\", chunks[0].getName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int retValue = doConcat(chunks);\n+\n+        Duration elapsed = timer.getElapsed();\n+        log.debug(\"concat - target={}, latency={}.\", chunks[0].getName(), elapsed.toMillis());\n+\n+        ChunkStorageMetrics.CONCAT_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageMetrics.CONCAT_BYTES.add(retValue);\n+        ChunkStorageMetrics.CONCAT_COUNT.inc();\n+        ChunkStorageMetrics.LARGE_CONCAT_COUNT.inc();\n+\n+        LoggerHelpers.traceLeave(log, \"concat\", traceId, chunks[0].getName());\n+\n+        return retValue;\n+    }\n+\n+    private void checkConcatArgs(ConcatArgument[] chunks) {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunks, \"chunks must not be null\");\n+        Preconditions.checkArgument(chunks.length >= 2, \"There must be at least two chunks\");\n+\n+        Preconditions.checkArgument(null != chunks[0], \"target chunk must not be null\");\n+        Preconditions.checkArgument(chunks[0].getLength() >= 0, \"target chunk lenth must be non negative.\");\n+\n+        for (int i = 1; i < chunks.length; i++) {\n+            Preconditions.checkArgument(null != chunks[i], \"source chunk must not be null\");\n+            Preconditions.checkArgument(chunks[i].getLength() >= 0, \"source chunk lenth must be non negative.\");\n+            Preconditions.checkArgument(!chunks[i].getName().equals(chunks[0].getName()), \"source chunk is same as target\");\n+            Preconditions.checkArgument(!chunks[i].getName().equals(chunks[i - 1].getName()), \"duplicate chunk found\");\n+        }\n+    }\n+\n+    /**\n+     * Truncates a given chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to truncate.\n+     * @param offset Offset to truncate to.\n+     * @return True if the object was truncated, false otherwise.\n+     * @throws ChunkStorageException         Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public boolean truncate(ChunkHandle handle, long offset) throws ChunkStorageException, UnsupportedOperationException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        Preconditions.checkArgument(offset > 0, \"handle must not be readonly\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle.getChunkName());\n+\n+        // Call concrete implementation.\n+        boolean retValue = doTruncate(handle, offset);\n+\n+        LoggerHelpers.traceLeave(log, \"truncate\", traceId, handle.getChunkName());\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Sets readonly attribute for the chunk.\n+     *\n+     * @param handle     ChunkHandle of the chunk.\n+     * @param isReadonly True if chunk is set to be readonly.\n+     * @return True if the operation was successful, false otherwise.\n+     * @throws ChunkStorageException         Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public boolean setReadOnly(ChunkHandle handle, boolean isReadonly) throws ChunkStorageException, UnsupportedOperationException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"setReadOnly\", handle.getChunkName());\n+\n+        // Call concrete implementation.\n+        boolean retValue = doSetReadOnly(handle, isReadonly);\n+\n+        LoggerHelpers.traceLeave(log, \"setReadOnly\", traceId, handle.getChunkName());\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Closes.", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU3MjUzMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447572530", "bodyText": "Do you know how long this list can get in the worst case?", "author": "RaulGracia", "createdAt": "2020-06-30T10:14:25Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU4ODczMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447588730", "bodyText": "Also, I see that you use this list only under a synchronized block. In this case, maybe you should add the annotation GuardedBy here to make it more explicit and create awareness that this list needs to be safely accessed.", "author": "RaulGracia", "createdAt": "2020-06-30T10:44:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU3MjUzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzczMzQ1Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447733457", "bodyText": "There is no theoretical upper bound.\n#4903 We plan to revisit this.", "author": "sachin-j-joshi", "createdAt": "2020-06-30T14:35:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU3MjUzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU3Nzc3OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447577778", "bodyText": "While I may be missing some deep details here, I have doubt that we need another cache implementation in the code. As this PR is very large and we need to reduce potential sources of issues, I really recommend to evaluate if we can use here a Guava cache as the Controller is already doing: \n  \n    \n      pravega/controller/src/main/java/io/pravega/controller/store/stream/AbstractStreamMetadataStore.java\n    \n    \n         Line 75\n      in\n      de7f435\n    \n    \n    \n    \n\n        \n          \n           private final LoadingCache<String, Scope> scopeCache; \n        \n    \n  \n\n\nThe more code in this PR, the slower the reviews are and the higher the uncertainty, so please consider this option to make this PR more lightweight in case that the Guava cache fulfills the main requirements that led you to write your own cache. If it is not the case, I also recommend to move this class to common, so others can use this data structure if it brings a functionality that does not exist yet in the current code/dependencies.", "author": "RaulGracia", "createdAt": "2020-06-30T10:23:46Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzczMDE5Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447730192", "bodyText": "That's the plan #4902 . However this change is getting bigger and it is better to do it as a separate PR.", "author": "sachin-j-joshi", "createdAt": "2020-06-30T14:31:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU3Nzc3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU4MzAwMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447583001", "bodyText": "Can this delay significantly a write operation?", "author": "RaulGracia", "createdAt": "2020-06-30T10:33:16Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage ChunkStorage instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage  ChunkStorage instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkManager and bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = new SystemJournal(containerId,\n+                epoch,\n+                chunkStorage,\n+                metadataStore,\n+                config);\n+\n+        // Now bootstrap\n+        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        this.systemJournal.bootstrap();\n+        log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                checkNotSealed(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // Validate that offset is correct.\n+                if ((segmentMetadata.getLength()) != offset) {\n+                    throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), offset);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                // Get the last chunk segmentMetadata for the segment.\n+                if (null != segmentMetadata.getLastChunk()) {\n+                    lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                }\n+\n+                while (bytesRemaining > 0) {\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        String previousLastChunkName = lastChunkMetadata == null ? null : lastChunkMetadata.getName();\n+\n+                        // update first and last chunks.\n+                        lastChunkMetadata = updateMetadataForChunkAddition(txn,\n+                                segmentMetadata,\n+                                newChunkName,\n+                                isFirstWriteAfterFailover,\n+                                lastChunkMetadata);\n+\n+                        // Record the creation of new chunk.\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    previousLastChunkName,\n+                                    newChunkName);\n+                            txn.markPinned(lastChunkMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        isFirstWriteAfterFailover = false;\n+                        didSegmentLayoutChange = true;\n+                        chunksAddedCount++;\n+\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\",\n+                                logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int writeSize = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+\n+                    // Write data to last chunk.\n+                    int bytesWritten = writeToChunk(txn,\n+                            segmentMetadata,\n+                            offset,\n+                            data,\n+                            chunkHandle,\n+                            lastChunkMetadata,\n+                            offsetToWriteAt,\n+                            writeSize);\n+\n+                    // Update the counts\n+                    bytesRemaining -= bytesWritten;\n+                    currentOffset += bytesWritten;\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzcyMDExMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447720111", "bodyText": "This is called when write operation has already failed.\nThe GC will be implemented on background thread in near future #4903", "author": "sachin-j-joshi", "createdAt": "2020-06-30T14:19:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU4MzAwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU4NzIzNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447587234", "bodyText": "If this check fails it will throw an IllegalStateException, but I don't see a handling for this in the catch block. What would happen in this case?", "author": "RaulGracia", "createdAt": "2020-06-30T10:41:17Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage ChunkStorage instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage  ChunkStorage instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkManager and bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = new SystemJournal(containerId,\n+                epoch,\n+                chunkStorage,\n+                metadataStore,\n+                config);\n+\n+        // Now bootstrap\n+        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        this.systemJournal.bootstrap();\n+        log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                checkNotSealed(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // Validate that offset is correct.\n+                if ((segmentMetadata.getLength()) != offset) {\n+                    throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), offset);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                // Get the last chunk segmentMetadata for the segment.\n+                if (null != segmentMetadata.getLastChunk()) {\n+                    lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                }\n+\n+                while (bytesRemaining > 0) {\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        String previousLastChunkName = lastChunkMetadata == null ? null : lastChunkMetadata.getName();\n+\n+                        // update first and last chunks.\n+                        lastChunkMetadata = updateMetadataForChunkAddition(txn,\n+                                segmentMetadata,\n+                                newChunkName,\n+                                isFirstWriteAfterFailover,\n+                                lastChunkMetadata);\n+\n+                        // Record the creation of new chunk.\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    previousLastChunkName,\n+                                    newChunkName);\n+                            txn.markPinned(lastChunkMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        isFirstWriteAfterFailover = false;\n+                        didSegmentLayoutChange = true;\n+                        chunksAddedCount++;\n+\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\",\n+                                logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int writeSize = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+\n+                    // Write data to last chunk.\n+                    int bytesWritten = writeToChunk(txn,\n+                            segmentMetadata,\n+                            offset,\n+                            data,\n+                            chunkHandle,\n+                            lastChunkMetadata,\n+                            offsetToWriteAt,\n+                            writeSize);\n+\n+                    // Update the counts\n+                    bytesRemaining -= bytesWritten;\n+                    currentOffset += bytesWritten;\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Updates the segment metadata for the newly added chunk.\n+     */\n+    private ChunkMetadata updateMetadataForChunkAddition(MetadataTransaction txn,\n+                                                         SegmentMetadata segmentMetadata,\n+                                                         String newChunkName,\n+                                                         boolean isFirstWriteAfterFailover,\n+                                                         ChunkMetadata lastChunkMetadata) throws StorageMetadataException {\n+        ChunkMetadata newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        segmentMetadata.setLastChunk(newChunkName);\n+        if (lastChunkMetadata == null) {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+        } else {\n+            lastChunkMetadata.setNextChunk(newChunkName);\n+            txn.update(lastChunkMetadata);\n+        }\n+        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+        // Reset ownershipChanged flag after first write is done.\n+        if (isFirstWriteAfterFailover) {\n+            segmentMetadata.setOwnerEpoch(this.epoch);\n+            segmentMetadata.setOwnershipChanged(false);\n+            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, segmentMetadata.getName());\n+        }\n+        segmentMetadata.incrementChunkCount();\n+\n+        // Update the transaction.\n+        txn.update(newChunkMetadata);\n+        txn.update(segmentMetadata);\n+        return newChunkMetadata;\n+    }\n+\n+    /**\n+     * Write to chunk.\n+     */\n+    private int writeToChunk(MetadataTransaction txn,\n+                             SegmentMetadata segmentMetadata,\n+                             long offset,\n+                             InputStream data,\n+                             ChunkHandle chunkHandle,\n+                             ChunkMetadata chunkWrittenMetadata,\n+                             long offsetToWriteAt,\n+                             int bytesCount) throws IOException, StorageMetadataException, BadOffsetException {\n+        int bytesWritten;\n+        Preconditions.checkState(0 != bytesCount, \"Attempt to write zero bytes\");\n+        try {\n+\n+            // Finally write the data.\n+            try (BoundedInputStream bis = new BoundedInputStream(data, bytesCount)) {\n+                bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesCount, bis);\n+            }\n+\n+            // Update the metadata for segment and chunk.\n+            Preconditions.checkState(bytesWritten >= 0);", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzczNDk1Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447734956", "bodyText": "This is an extra ordinary situation. This exception needs to be bubbled back to the caller.  An error will be logged in SS by calling code.", "author": "sachin-j-joshi", "createdAt": "2020-06-30T14:37:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU4NzIzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU4Nzc4MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447587780", "bodyText": "Nit: systemJournal != null seems more readable.", "author": "RaulGracia", "createdAt": "2020-06-30T10:42:24Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage ChunkStorage instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage  ChunkStorage instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkManager and bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = new SystemJournal(containerId,\n+                epoch,\n+                chunkStorage,\n+                metadataStore,\n+                config);\n+\n+        // Now bootstrap\n+        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        this.systemJournal.bootstrap();\n+        log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                checkNotSealed(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // Validate that offset is correct.\n+                if ((segmentMetadata.getLength()) != offset) {\n+                    throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), offset);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                // Get the last chunk segmentMetadata for the segment.\n+                if (null != segmentMetadata.getLastChunk()) {\n+                    lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                }\n+\n+                while (bytesRemaining > 0) {\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        String previousLastChunkName = lastChunkMetadata == null ? null : lastChunkMetadata.getName();\n+\n+                        // update first and last chunks.\n+                        lastChunkMetadata = updateMetadataForChunkAddition(txn,\n+                                segmentMetadata,\n+                                newChunkName,\n+                                isFirstWriteAfterFailover,\n+                                lastChunkMetadata);\n+\n+                        // Record the creation of new chunk.\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    previousLastChunkName,\n+                                    newChunkName);\n+                            txn.markPinned(lastChunkMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        isFirstWriteAfterFailover = false;\n+                        didSegmentLayoutChange = true;\n+                        chunksAddedCount++;\n+\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\",\n+                                logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int writeSize = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+\n+                    // Write data to last chunk.\n+                    int bytesWritten = writeToChunk(txn,\n+                            segmentMetadata,\n+                            offset,\n+                            data,\n+                            chunkHandle,\n+                            lastChunkMetadata,\n+                            offsetToWriteAt,\n+                            writeSize);\n+\n+                    // Update the counts\n+                    bytesRemaining -= bytesWritten;\n+                    currentOffset += bytesWritten;\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Updates the segment metadata for the newly added chunk.\n+     */\n+    private ChunkMetadata updateMetadataForChunkAddition(MetadataTransaction txn,\n+                                                         SegmentMetadata segmentMetadata,\n+                                                         String newChunkName,\n+                                                         boolean isFirstWriteAfterFailover,\n+                                                         ChunkMetadata lastChunkMetadata) throws StorageMetadataException {\n+        ChunkMetadata newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        segmentMetadata.setLastChunk(newChunkName);\n+        if (lastChunkMetadata == null) {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+        } else {\n+            lastChunkMetadata.setNextChunk(newChunkName);\n+            txn.update(lastChunkMetadata);\n+        }\n+        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+        // Reset ownershipChanged flag after first write is done.\n+        if (isFirstWriteAfterFailover) {\n+            segmentMetadata.setOwnerEpoch(this.epoch);\n+            segmentMetadata.setOwnershipChanged(false);\n+            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, segmentMetadata.getName());\n+        }\n+        segmentMetadata.incrementChunkCount();\n+\n+        // Update the transaction.\n+        txn.update(newChunkMetadata);\n+        txn.update(segmentMetadata);\n+        return newChunkMetadata;\n+    }\n+\n+    /**\n+     * Write to chunk.\n+     */\n+    private int writeToChunk(MetadataTransaction txn,\n+                             SegmentMetadata segmentMetadata,\n+                             long offset,\n+                             InputStream data,\n+                             ChunkHandle chunkHandle,\n+                             ChunkMetadata chunkWrittenMetadata,\n+                             long offsetToWriteAt,\n+                             int bytesCount) throws IOException, StorageMetadataException, BadOffsetException {\n+        int bytesWritten;\n+        Preconditions.checkState(0 != bytesCount, \"Attempt to write zero bytes\");\n+        try {\n+\n+            // Finally write the data.\n+            try (BoundedInputStream bis = new BoundedInputStream(data, bytesCount)) {\n+                bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesCount, bis);\n+            }\n+\n+            // Update the metadata for segment and chunk.\n+            Preconditions.checkState(bytesWritten >= 0);\n+            segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+            chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+            txn.update(chunkWrittenMetadata);\n+            txn.update(segmentMetadata);\n+        } catch (IndexOutOfBoundsException e) {\n+            try {\n+                throw new BadOffsetException(segmentMetadata.getName(), chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+            } catch (ChunkStorageException cse) {\n+                log.error(\"{} write - Error while retrieving ChunkInfo for {}.\", logPrefix, chunkHandle.getChunkName());\n+                // The exact expected offset for the  operation does not matter, the StorageWriter will enter reconciliation loop anyway.\n+                throw new BadOffsetException(segmentMetadata.getName(), offset, offset);\n+            }\n+        }\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzcyODI3MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447728270", "bodyText": "I'm an old C/C++ programmer , some habits die hard :).\nLittle fun background .\nIn those language any non-zero value is considered true and assignment operator returns value. So if by mistake you wrote if (systemJournal = NULL) ... that is a valid code and you had unexpectedly assigned  null to systemJournal. Doing it if (NULL = systemJournal )  prevents assignment to const and catches syntax errors.", "author": "sachin-j-joshi", "createdAt": "2020-06-30T14:29:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU4Nzc4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzk5NzA5Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447997093", "bodyText": "skipping...forgot but will fix in general in future", "author": "sachin-j-joshi", "createdAt": "2020-06-30T21:44:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU4Nzc4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MTM2MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447591360", "bodyText": "We really need to evaluate the performance of this method. The reason is the following: if by some reason this is slow, given the amount of computations inside this method, it will greatly limit the throughput of the whole system, specially for scenarios with 1 or few segments in which parallelism is limited. So, could you please deploy this branch of Pravega, create a 1-segment Stream and see the throughput when writing large events?", "author": "RaulGracia", "createdAt": "2020-06-30T10:48:52Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage ChunkStorage instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage  ChunkStorage instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkManager and bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = new SystemJournal(containerId,\n+                epoch,\n+                chunkStorage,\n+                metadataStore,\n+                config);\n+\n+        // Now bootstrap\n+        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        this.systemJournal.bootstrap();\n+        log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzczOTkzNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447739937", "bodyText": "For \"happy path\" of appending to existing chunk this method is fast. It directly writes to the existing chunk. The metadata is not written to underlying store in this case. In system tests most writes are complete under 3 ms. (haven't run a perf run but don't expect a vastly different result. But will do it shortly.)\nIn case of need to add new chunk there is small extra overhead of creating a new chunk, updating and storing metadata.\nFor storage system segments, it is necessary to journal this change in system journal.", "author": "sachin-j-joshi", "createdAt": "2020-06-30T14:44:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MTM2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MjgwNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447592805", "bodyText": "Using targetLastChunk != null && shouldDefrag() will execute fewer times shouldDefrag() in the case it contains some more complex logic.", "author": "RaulGracia", "createdAt": "2020-06-30T10:51:34Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage ChunkStorage instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage  ChunkStorage instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkManager and bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = new SystemJournal(containerId,\n+                epoch,\n+                chunkStorage,\n+                metadataStore,\n+                config);\n+\n+        // Now bootstrap\n+        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        this.systemJournal.bootstrap();\n+        log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                checkNotSealed(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // Validate that offset is correct.\n+                if ((segmentMetadata.getLength()) != offset) {\n+                    throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), offset);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                // Get the last chunk segmentMetadata for the segment.\n+                if (null != segmentMetadata.getLastChunk()) {\n+                    lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                }\n+\n+                while (bytesRemaining > 0) {\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        String previousLastChunkName = lastChunkMetadata == null ? null : lastChunkMetadata.getName();\n+\n+                        // update first and last chunks.\n+                        lastChunkMetadata = updateMetadataForChunkAddition(txn,\n+                                segmentMetadata,\n+                                newChunkName,\n+                                isFirstWriteAfterFailover,\n+                                lastChunkMetadata);\n+\n+                        // Record the creation of new chunk.\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    previousLastChunkName,\n+                                    newChunkName);\n+                            txn.markPinned(lastChunkMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        isFirstWriteAfterFailover = false;\n+                        didSegmentLayoutChange = true;\n+                        chunksAddedCount++;\n+\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\",\n+                                logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int writeSize = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+\n+                    // Write data to last chunk.\n+                    int bytesWritten = writeToChunk(txn,\n+                            segmentMetadata,\n+                            offset,\n+                            data,\n+                            chunkHandle,\n+                            lastChunkMetadata,\n+                            offsetToWriteAt,\n+                            writeSize);\n+\n+                    // Update the counts\n+                    bytesRemaining -= bytesWritten;\n+                    currentOffset += bytesWritten;\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Updates the segment metadata for the newly added chunk.\n+     */\n+    private ChunkMetadata updateMetadataForChunkAddition(MetadataTransaction txn,\n+                                                         SegmentMetadata segmentMetadata,\n+                                                         String newChunkName,\n+                                                         boolean isFirstWriteAfterFailover,\n+                                                         ChunkMetadata lastChunkMetadata) throws StorageMetadataException {\n+        ChunkMetadata newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        segmentMetadata.setLastChunk(newChunkName);\n+        if (lastChunkMetadata == null) {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+        } else {\n+            lastChunkMetadata.setNextChunk(newChunkName);\n+            txn.update(lastChunkMetadata);\n+        }\n+        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+        // Reset ownershipChanged flag after first write is done.\n+        if (isFirstWriteAfterFailover) {\n+            segmentMetadata.setOwnerEpoch(this.epoch);\n+            segmentMetadata.setOwnershipChanged(false);\n+            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, segmentMetadata.getName());\n+        }\n+        segmentMetadata.incrementChunkCount();\n+\n+        // Update the transaction.\n+        txn.update(newChunkMetadata);\n+        txn.update(segmentMetadata);\n+        return newChunkMetadata;\n+    }\n+\n+    /**\n+     * Write to chunk.\n+     */\n+    private int writeToChunk(MetadataTransaction txn,\n+                             SegmentMetadata segmentMetadata,\n+                             long offset,\n+                             InputStream data,\n+                             ChunkHandle chunkHandle,\n+                             ChunkMetadata chunkWrittenMetadata,\n+                             long offsetToWriteAt,\n+                             int bytesCount) throws IOException, StorageMetadataException, BadOffsetException {\n+        int bytesWritten;\n+        Preconditions.checkState(0 != bytesCount, \"Attempt to write zero bytes\");\n+        try {\n+\n+            // Finally write the data.\n+            try (BoundedInputStream bis = new BoundedInputStream(data, bytesCount)) {\n+                bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesCount, bis);\n+            }\n+\n+            // Update the metadata for segment and chunk.\n+            Preconditions.checkState(bytesWritten >= 0);\n+            segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+            chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+            txn.update(chunkWrittenMetadata);\n+            txn.update(segmentMetadata);\n+        } catch (IndexOutOfBoundsException e) {\n+            try {\n+                throw new BadOffsetException(segmentMetadata.getName(), chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+            } catch (ChunkStorageException cse) {\n+                log.error(\"{} write - Error while retrieving ChunkInfo for {}.\", logPrefix, chunkHandle.getChunkName());\n+                // The exact expected offset for the  operation does not matter, the StorageWriter will enter reconciliation loop anyway.\n+                throw new BadOffsetException(segmentMetadata.getName(), offset, offset);\n+            }\n+        }\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                        .segmentName(streamSegmentName)\n+                        .offset(offset)\n+                        .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                        .newChunkName(newChunkName)\n+                        .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+                targetSegmentMetadata.checkInvariants();\n+                checkNotSealed(targetSegmentName, targetSegmentMetadata);\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                checkSealed(sourceSegmentMetadata);\n+                checkOwnership(targetSegmentMetadata.getName(), targetSegmentMetadata);\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc0NTk1Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447745957", "bodyText": "null == targetLastChunk means it is an empty segment.", "author": "sachin-j-joshi", "createdAt": "2020-06-30T14:51:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MjgwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MzM5NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447593395", "bodyText": "Why this is left after the logging? This is still part of the logic, right? Logging and traceLeave() should be the last part of the execution in most of the cases.", "author": "RaulGracia", "createdAt": "2020-06-30T10:52:39Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage ChunkStorage instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage  ChunkStorage instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkManager and bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = new SystemJournal(containerId,\n+                epoch,\n+                chunkStorage,\n+                metadataStore,\n+                config);\n+\n+        // Now bootstrap\n+        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        this.systemJournal.bootstrap();\n+        log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                checkNotSealed(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // Validate that offset is correct.\n+                if ((segmentMetadata.getLength()) != offset) {\n+                    throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), offset);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                // Get the last chunk segmentMetadata for the segment.\n+                if (null != segmentMetadata.getLastChunk()) {\n+                    lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                }\n+\n+                while (bytesRemaining > 0) {\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        String previousLastChunkName = lastChunkMetadata == null ? null : lastChunkMetadata.getName();\n+\n+                        // update first and last chunks.\n+                        lastChunkMetadata = updateMetadataForChunkAddition(txn,\n+                                segmentMetadata,\n+                                newChunkName,\n+                                isFirstWriteAfterFailover,\n+                                lastChunkMetadata);\n+\n+                        // Record the creation of new chunk.\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    previousLastChunkName,\n+                                    newChunkName);\n+                            txn.markPinned(lastChunkMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        isFirstWriteAfterFailover = false;\n+                        didSegmentLayoutChange = true;\n+                        chunksAddedCount++;\n+\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\",\n+                                logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int writeSize = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+\n+                    // Write data to last chunk.\n+                    int bytesWritten = writeToChunk(txn,\n+                            segmentMetadata,\n+                            offset,\n+                            data,\n+                            chunkHandle,\n+                            lastChunkMetadata,\n+                            offsetToWriteAt,\n+                            writeSize);\n+\n+                    // Update the counts\n+                    bytesRemaining -= bytesWritten;\n+                    currentOffset += bytesWritten;\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Updates the segment metadata for the newly added chunk.\n+     */\n+    private ChunkMetadata updateMetadataForChunkAddition(MetadataTransaction txn,\n+                                                         SegmentMetadata segmentMetadata,\n+                                                         String newChunkName,\n+                                                         boolean isFirstWriteAfterFailover,\n+                                                         ChunkMetadata lastChunkMetadata) throws StorageMetadataException {\n+        ChunkMetadata newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        segmentMetadata.setLastChunk(newChunkName);\n+        if (lastChunkMetadata == null) {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+        } else {\n+            lastChunkMetadata.setNextChunk(newChunkName);\n+            txn.update(lastChunkMetadata);\n+        }\n+        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+        // Reset ownershipChanged flag after first write is done.\n+        if (isFirstWriteAfterFailover) {\n+            segmentMetadata.setOwnerEpoch(this.epoch);\n+            segmentMetadata.setOwnershipChanged(false);\n+            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, segmentMetadata.getName());\n+        }\n+        segmentMetadata.incrementChunkCount();\n+\n+        // Update the transaction.\n+        txn.update(newChunkMetadata);\n+        txn.update(segmentMetadata);\n+        return newChunkMetadata;\n+    }\n+\n+    /**\n+     * Write to chunk.\n+     */\n+    private int writeToChunk(MetadataTransaction txn,\n+                             SegmentMetadata segmentMetadata,\n+                             long offset,\n+                             InputStream data,\n+                             ChunkHandle chunkHandle,\n+                             ChunkMetadata chunkWrittenMetadata,\n+                             long offsetToWriteAt,\n+                             int bytesCount) throws IOException, StorageMetadataException, BadOffsetException {\n+        int bytesWritten;\n+        Preconditions.checkState(0 != bytesCount, \"Attempt to write zero bytes\");\n+        try {\n+\n+            // Finally write the data.\n+            try (BoundedInputStream bis = new BoundedInputStream(data, bytesCount)) {\n+                bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesCount, bis);\n+            }\n+\n+            // Update the metadata for segment and chunk.\n+            Preconditions.checkState(bytesWritten >= 0);\n+            segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+            chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+            txn.update(chunkWrittenMetadata);\n+            txn.update(segmentMetadata);\n+        } catch (IndexOutOfBoundsException e) {\n+            try {\n+                throw new BadOffsetException(segmentMetadata.getName(), chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+            } catch (ChunkStorageException cse) {\n+                log.error(\"{} write - Error while retrieving ChunkInfo for {}.\", logPrefix, chunkHandle.getChunkName());\n+                // The exact expected offset for the  operation does not matter, the StorageWriter will enter reconciliation loop anyway.\n+                throw new BadOffsetException(segmentMetadata.getName(), offset, offset);\n+            }\n+        }\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                        .segmentName(streamSegmentName)\n+                        .offset(offset)\n+                        .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                        .newChunkName(newChunkName)\n+                        .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+                targetSegmentMetadata.checkInvariants();\n+                checkNotSealed(targetSegmentName, targetSegmentMetadata);\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                checkSealed(sourceSegmentMetadata);\n+                checkOwnership(targetSegmentMetadata.getName(), targetSegmentMetadata);\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzcxODk0OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447718948", "bodyText": "good point. will fix.", "author": "sachin-j-joshi", "createdAt": "2020-06-30T14:17:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MzM5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzk5NjgzMg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447996832", "bodyText": "fixed 1fd6faf", "author": "sachin-j-joshi", "createdAt": "2020-06-30T21:43:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MzM5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5Mzg0Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447593842", "bodyText": "Typo: throughputand -> throughput and", "author": "RaulGracia", "createdAt": "2020-06-30T10:53:27Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage ChunkStorage instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage  ChunkStorage instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkManager and bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = new SystemJournal(containerId,\n+                epoch,\n+                chunkStorage,\n+                metadataStore,\n+                config);\n+\n+        // Now bootstrap\n+        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        this.systemJournal.bootstrap();\n+        log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                checkNotSealed(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // Validate that offset is correct.\n+                if ((segmentMetadata.getLength()) != offset) {\n+                    throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), offset);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                // Get the last chunk segmentMetadata for the segment.\n+                if (null != segmentMetadata.getLastChunk()) {\n+                    lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                }\n+\n+                while (bytesRemaining > 0) {\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        String previousLastChunkName = lastChunkMetadata == null ? null : lastChunkMetadata.getName();\n+\n+                        // update first and last chunks.\n+                        lastChunkMetadata = updateMetadataForChunkAddition(txn,\n+                                segmentMetadata,\n+                                newChunkName,\n+                                isFirstWriteAfterFailover,\n+                                lastChunkMetadata);\n+\n+                        // Record the creation of new chunk.\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    previousLastChunkName,\n+                                    newChunkName);\n+                            txn.markPinned(lastChunkMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        isFirstWriteAfterFailover = false;\n+                        didSegmentLayoutChange = true;\n+                        chunksAddedCount++;\n+\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\",\n+                                logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int writeSize = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+\n+                    // Write data to last chunk.\n+                    int bytesWritten = writeToChunk(txn,\n+                            segmentMetadata,\n+                            offset,\n+                            data,\n+                            chunkHandle,\n+                            lastChunkMetadata,\n+                            offsetToWriteAt,\n+                            writeSize);\n+\n+                    // Update the counts\n+                    bytesRemaining -= bytesWritten;\n+                    currentOffset += bytesWritten;\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Updates the segment metadata for the newly added chunk.\n+     */\n+    private ChunkMetadata updateMetadataForChunkAddition(MetadataTransaction txn,\n+                                                         SegmentMetadata segmentMetadata,\n+                                                         String newChunkName,\n+                                                         boolean isFirstWriteAfterFailover,\n+                                                         ChunkMetadata lastChunkMetadata) throws StorageMetadataException {\n+        ChunkMetadata newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        segmentMetadata.setLastChunk(newChunkName);\n+        if (lastChunkMetadata == null) {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+        } else {\n+            lastChunkMetadata.setNextChunk(newChunkName);\n+            txn.update(lastChunkMetadata);\n+        }\n+        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+        // Reset ownershipChanged flag after first write is done.\n+        if (isFirstWriteAfterFailover) {\n+            segmentMetadata.setOwnerEpoch(this.epoch);\n+            segmentMetadata.setOwnershipChanged(false);\n+            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, segmentMetadata.getName());\n+        }\n+        segmentMetadata.incrementChunkCount();\n+\n+        // Update the transaction.\n+        txn.update(newChunkMetadata);\n+        txn.update(segmentMetadata);\n+        return newChunkMetadata;\n+    }\n+\n+    /**\n+     * Write to chunk.\n+     */\n+    private int writeToChunk(MetadataTransaction txn,\n+                             SegmentMetadata segmentMetadata,\n+                             long offset,\n+                             InputStream data,\n+                             ChunkHandle chunkHandle,\n+                             ChunkMetadata chunkWrittenMetadata,\n+                             long offsetToWriteAt,\n+                             int bytesCount) throws IOException, StorageMetadataException, BadOffsetException {\n+        int bytesWritten;\n+        Preconditions.checkState(0 != bytesCount, \"Attempt to write zero bytes\");\n+        try {\n+\n+            // Finally write the data.\n+            try (BoundedInputStream bis = new BoundedInputStream(data, bytesCount)) {\n+                bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesCount, bis);\n+            }\n+\n+            // Update the metadata for segment and chunk.\n+            Preconditions.checkState(bytesWritten >= 0);\n+            segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+            chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+            txn.update(chunkWrittenMetadata);\n+            txn.update(segmentMetadata);\n+        } catch (IndexOutOfBoundsException e) {\n+            try {\n+                throw new BadOffsetException(segmentMetadata.getName(), chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+            } catch (ChunkStorageException cse) {\n+                log.error(\"{} write - Error while retrieving ChunkInfo for {}.\", logPrefix, chunkHandle.getChunkName());\n+                // The exact expected offset for the  operation does not matter, the StorageWriter will enter reconciliation loop anyway.\n+                throw new BadOffsetException(segmentMetadata.getName(), offset, offset);\n+            }\n+        }\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                        .segmentName(streamSegmentName)\n+                        .offset(offset)\n+                        .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                        .newChunkName(newChunkName)\n+                        .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+                targetSegmentMetadata.checkInvariants();\n+                checkNotSealed(targetSegmentName, targetSegmentMetadata);\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                checkSealed(sourceSegmentMetadata);\n+                checkOwnership(targetSegmentMetadata.getName(), targetSegmentMetadata);\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In the absence of defragmentation, the number of chunks for individual segments keeps on increasing.\n+     * When we have too many small chunks (say because many transactions with little data on some segments), the segment\n+     * is fragmented - this may impact both the read throughputand the performance of the metadata store.", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzk5NjY1OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447996659", "bodyText": "fixed. 1fd6faf", "author": "sachin-j-joshi", "createdAt": "2020-06-30T21:43:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5Mzg0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5NDI1Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447594252", "bodyText": "You can remove -i.e., write-", "author": "RaulGracia", "createdAt": "2020-06-30T10:54:18Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage ChunkStorage instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage  ChunkStorage instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkManager and bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = new SystemJournal(containerId,\n+                epoch,\n+                chunkStorage,\n+                metadataStore,\n+                config);\n+\n+        // Now bootstrap\n+        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        this.systemJournal.bootstrap();\n+        log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                checkNotSealed(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // Validate that offset is correct.\n+                if ((segmentMetadata.getLength()) != offset) {\n+                    throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), offset);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                // Get the last chunk segmentMetadata for the segment.\n+                if (null != segmentMetadata.getLastChunk()) {\n+                    lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                }\n+\n+                while (bytesRemaining > 0) {\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        String previousLastChunkName = lastChunkMetadata == null ? null : lastChunkMetadata.getName();\n+\n+                        // update first and last chunks.\n+                        lastChunkMetadata = updateMetadataForChunkAddition(txn,\n+                                segmentMetadata,\n+                                newChunkName,\n+                                isFirstWriteAfterFailover,\n+                                lastChunkMetadata);\n+\n+                        // Record the creation of new chunk.\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    previousLastChunkName,\n+                                    newChunkName);\n+                            txn.markPinned(lastChunkMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        isFirstWriteAfterFailover = false;\n+                        didSegmentLayoutChange = true;\n+                        chunksAddedCount++;\n+\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\",\n+                                logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int writeSize = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+\n+                    // Write data to last chunk.\n+                    int bytesWritten = writeToChunk(txn,\n+                            segmentMetadata,\n+                            offset,\n+                            data,\n+                            chunkHandle,\n+                            lastChunkMetadata,\n+                            offsetToWriteAt,\n+                            writeSize);\n+\n+                    // Update the counts\n+                    bytesRemaining -= bytesWritten;\n+                    currentOffset += bytesWritten;\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Updates the segment metadata for the newly added chunk.\n+     */\n+    private ChunkMetadata updateMetadataForChunkAddition(MetadataTransaction txn,\n+                                                         SegmentMetadata segmentMetadata,\n+                                                         String newChunkName,\n+                                                         boolean isFirstWriteAfterFailover,\n+                                                         ChunkMetadata lastChunkMetadata) throws StorageMetadataException {\n+        ChunkMetadata newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        segmentMetadata.setLastChunk(newChunkName);\n+        if (lastChunkMetadata == null) {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+        } else {\n+            lastChunkMetadata.setNextChunk(newChunkName);\n+            txn.update(lastChunkMetadata);\n+        }\n+        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+        // Reset ownershipChanged flag after first write is done.\n+        if (isFirstWriteAfterFailover) {\n+            segmentMetadata.setOwnerEpoch(this.epoch);\n+            segmentMetadata.setOwnershipChanged(false);\n+            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, segmentMetadata.getName());\n+        }\n+        segmentMetadata.incrementChunkCount();\n+\n+        // Update the transaction.\n+        txn.update(newChunkMetadata);\n+        txn.update(segmentMetadata);\n+        return newChunkMetadata;\n+    }\n+\n+    /**\n+     * Write to chunk.\n+     */\n+    private int writeToChunk(MetadataTransaction txn,\n+                             SegmentMetadata segmentMetadata,\n+                             long offset,\n+                             InputStream data,\n+                             ChunkHandle chunkHandle,\n+                             ChunkMetadata chunkWrittenMetadata,\n+                             long offsetToWriteAt,\n+                             int bytesCount) throws IOException, StorageMetadataException, BadOffsetException {\n+        int bytesWritten;\n+        Preconditions.checkState(0 != bytesCount, \"Attempt to write zero bytes\");\n+        try {\n+\n+            // Finally write the data.\n+            try (BoundedInputStream bis = new BoundedInputStream(data, bytesCount)) {\n+                bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesCount, bis);\n+            }\n+\n+            // Update the metadata for segment and chunk.\n+            Preconditions.checkState(bytesWritten >= 0);\n+            segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+            chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+            txn.update(chunkWrittenMetadata);\n+            txn.update(segmentMetadata);\n+        } catch (IndexOutOfBoundsException e) {\n+            try {\n+                throw new BadOffsetException(segmentMetadata.getName(), chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+            } catch (ChunkStorageException cse) {\n+                log.error(\"{} write - Error while retrieving ChunkInfo for {}.\", logPrefix, chunkHandle.getChunkName());\n+                // The exact expected offset for the  operation does not matter, the StorageWriter will enter reconciliation loop anyway.\n+                throw new BadOffsetException(segmentMetadata.getName(), offset, offset);\n+            }\n+        }\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                        .segmentName(streamSegmentName)\n+                        .offset(offset)\n+                        .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                        .newChunkName(newChunkName)\n+                        .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+                targetSegmentMetadata.checkInvariants();\n+                checkNotSealed(targetSegmentName, targetSegmentMetadata);\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                checkSealed(sourceSegmentMetadata);\n+                checkOwnership(targetSegmentMetadata.getName(), targetSegmentMetadata);\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In the absence of defragmentation, the number of chunks for individual segments keeps on increasing.\n+     * When we have too many small chunks (say because many transactions with little data on some segments), the segment\n+     * is fragmented - this may impact both the read throughputand the performance of the metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (e.g., stock S3) and\n+     * each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If the underlying storage provides some facility to stitch together smaller chunk into larger chunks, then we do\n+     * actually want to exploit that, specially when the underlying implementation is only a metadata operation. We want\n+     * to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are typically\n+     * only metadata operations, reducing the overall cost of the merging them together. HDFS also supports merges,\n+     * whereas NFS has no concept of merging natively.\n+     *\n+     * As chunks become larger, append writes (read source completely and append-i.e., write- it back at the end of target)", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzk5NTk3Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447995972", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-06-30T21:42:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5NDI1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5NTE4Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447595187", "bodyText": "While I understand in theory Consequently, a native option for merging is desirable may be true, I'm not so sure that based on our real experience with this option relying on this feature is a great idea. We can end up in a situation in which, if the native concat is not efficient in the system at hand, we can get real problems that we cannot solve because we are relying on logic implemented by third parties.", "author": "RaulGracia", "createdAt": "2020-06-30T10:56:09Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage ChunkStorage instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage  ChunkStorage instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkManager and bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = new SystemJournal(containerId,\n+                epoch,\n+                chunkStorage,\n+                metadataStore,\n+                config);\n+\n+        // Now bootstrap\n+        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        this.systemJournal.bootstrap();\n+        log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                checkNotSealed(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // Validate that offset is correct.\n+                if ((segmentMetadata.getLength()) != offset) {\n+                    throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), offset);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                // Get the last chunk segmentMetadata for the segment.\n+                if (null != segmentMetadata.getLastChunk()) {\n+                    lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                }\n+\n+                while (bytesRemaining > 0) {\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        String previousLastChunkName = lastChunkMetadata == null ? null : lastChunkMetadata.getName();\n+\n+                        // update first and last chunks.\n+                        lastChunkMetadata = updateMetadataForChunkAddition(txn,\n+                                segmentMetadata,\n+                                newChunkName,\n+                                isFirstWriteAfterFailover,\n+                                lastChunkMetadata);\n+\n+                        // Record the creation of new chunk.\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    previousLastChunkName,\n+                                    newChunkName);\n+                            txn.markPinned(lastChunkMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        isFirstWriteAfterFailover = false;\n+                        didSegmentLayoutChange = true;\n+                        chunksAddedCount++;\n+\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\",\n+                                logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int writeSize = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+\n+                    // Write data to last chunk.\n+                    int bytesWritten = writeToChunk(txn,\n+                            segmentMetadata,\n+                            offset,\n+                            data,\n+                            chunkHandle,\n+                            lastChunkMetadata,\n+                            offsetToWriteAt,\n+                            writeSize);\n+\n+                    // Update the counts\n+                    bytesRemaining -= bytesWritten;\n+                    currentOffset += bytesWritten;\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Updates the segment metadata for the newly added chunk.\n+     */\n+    private ChunkMetadata updateMetadataForChunkAddition(MetadataTransaction txn,\n+                                                         SegmentMetadata segmentMetadata,\n+                                                         String newChunkName,\n+                                                         boolean isFirstWriteAfterFailover,\n+                                                         ChunkMetadata lastChunkMetadata) throws StorageMetadataException {\n+        ChunkMetadata newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        segmentMetadata.setLastChunk(newChunkName);\n+        if (lastChunkMetadata == null) {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+        } else {\n+            lastChunkMetadata.setNextChunk(newChunkName);\n+            txn.update(lastChunkMetadata);\n+        }\n+        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+        // Reset ownershipChanged flag after first write is done.\n+        if (isFirstWriteAfterFailover) {\n+            segmentMetadata.setOwnerEpoch(this.epoch);\n+            segmentMetadata.setOwnershipChanged(false);\n+            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, segmentMetadata.getName());\n+        }\n+        segmentMetadata.incrementChunkCount();\n+\n+        // Update the transaction.\n+        txn.update(newChunkMetadata);\n+        txn.update(segmentMetadata);\n+        return newChunkMetadata;\n+    }\n+\n+    /**\n+     * Write to chunk.\n+     */\n+    private int writeToChunk(MetadataTransaction txn,\n+                             SegmentMetadata segmentMetadata,\n+                             long offset,\n+                             InputStream data,\n+                             ChunkHandle chunkHandle,\n+                             ChunkMetadata chunkWrittenMetadata,\n+                             long offsetToWriteAt,\n+                             int bytesCount) throws IOException, StorageMetadataException, BadOffsetException {\n+        int bytesWritten;\n+        Preconditions.checkState(0 != bytesCount, \"Attempt to write zero bytes\");\n+        try {\n+\n+            // Finally write the data.\n+            try (BoundedInputStream bis = new BoundedInputStream(data, bytesCount)) {\n+                bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesCount, bis);\n+            }\n+\n+            // Update the metadata for segment and chunk.\n+            Preconditions.checkState(bytesWritten >= 0);\n+            segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+            chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+            txn.update(chunkWrittenMetadata);\n+            txn.update(segmentMetadata);\n+        } catch (IndexOutOfBoundsException e) {\n+            try {\n+                throw new BadOffsetException(segmentMetadata.getName(), chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+            } catch (ChunkStorageException cse) {\n+                log.error(\"{} write - Error while retrieving ChunkInfo for {}.\", logPrefix, chunkHandle.getChunkName());\n+                // The exact expected offset for the  operation does not matter, the StorageWriter will enter reconciliation loop anyway.\n+                throw new BadOffsetException(segmentMetadata.getName(), offset, offset);\n+            }\n+        }\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                        .segmentName(streamSegmentName)\n+                        .offset(offset)\n+                        .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                        .newChunkName(newChunkName)\n+                        .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+                targetSegmentMetadata.checkInvariants();\n+                checkNotSealed(targetSegmentName, targetSegmentMetadata);\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                checkSealed(sourceSegmentMetadata);\n+                checkOwnership(targetSegmentMetadata.getName(), targetSegmentMetadata);\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In the absence of defragmentation, the number of chunks for individual segments keeps on increasing.\n+     * When we have too many small chunks (say because many transactions with little data on some segments), the segment\n+     * is fragmented - this may impact both the read throughputand the performance of the metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (e.g., stock S3) and\n+     * each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If the underlying storage provides some facility to stitch together smaller chunk into larger chunks, then we do\n+     * actually want to exploit that, specially when the underlying implementation is only a metadata operation. We want\n+     * to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are typically\n+     * only metadata operations, reducing the overall cost of the merging them together. HDFS also supports merges,\n+     * whereas NFS has no concept of merging natively.\n+     *\n+     * As chunks become larger, append writes (read source completely and append-i.e., write- it back at the end of target)\n+     * become inefficient. Consequently, a native option for merging is desirable. We use such native merge capability", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzcxNjgzOQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447716839", "bodyText": "In the long term the storage optimizer will be running on the background thread.\nNote that currently defrag is called only with concat.\nThere are two settings  minSizeLimitForConcat and maxSizeLimitForConcat to control when concat is invoked (vs data is copied using appends). We'll measure and tune how often and how much to defrag.", "author": "sachin-j-joshi", "createdAt": "2020-06-30T14:14:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5NTE4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5ODY1NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447598654", "bodyText": "If I'm not mistaken, you are executing this logic as part of concat() and, in addition to that, you are executing collectGarbage() as part of write(). These are the two most performance critical operations, as they will dictate the write throughput and the transaction throughput to Tier 2, respectively. Please, consider if these 2 tasks should be executed on a regular basis by an independent service instead of adding this burden to the regular write activity of Tier 2.", "author": "RaulGracia", "createdAt": "2020-06-30T11:03:09Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage ChunkStorage instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage  ChunkStorage instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkManager and bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = new SystemJournal(containerId,\n+                epoch,\n+                chunkStorage,\n+                metadataStore,\n+                config);\n+\n+        // Now bootstrap\n+        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        this.systemJournal.bootstrap();\n+        log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                checkNotSealed(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // Validate that offset is correct.\n+                if ((segmentMetadata.getLength()) != offset) {\n+                    throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), offset);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                // Get the last chunk segmentMetadata for the segment.\n+                if (null != segmentMetadata.getLastChunk()) {\n+                    lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                }\n+\n+                while (bytesRemaining > 0) {\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        String previousLastChunkName = lastChunkMetadata == null ? null : lastChunkMetadata.getName();\n+\n+                        // update first and last chunks.\n+                        lastChunkMetadata = updateMetadataForChunkAddition(txn,\n+                                segmentMetadata,\n+                                newChunkName,\n+                                isFirstWriteAfterFailover,\n+                                lastChunkMetadata);\n+\n+                        // Record the creation of new chunk.\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    previousLastChunkName,\n+                                    newChunkName);\n+                            txn.markPinned(lastChunkMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        isFirstWriteAfterFailover = false;\n+                        didSegmentLayoutChange = true;\n+                        chunksAddedCount++;\n+\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\",\n+                                logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int writeSize = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+\n+                    // Write data to last chunk.\n+                    int bytesWritten = writeToChunk(txn,\n+                            segmentMetadata,\n+                            offset,\n+                            data,\n+                            chunkHandle,\n+                            lastChunkMetadata,\n+                            offsetToWriteAt,\n+                            writeSize);\n+\n+                    // Update the counts\n+                    bytesRemaining -= bytesWritten;\n+                    currentOffset += bytesWritten;\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Updates the segment metadata for the newly added chunk.\n+     */\n+    private ChunkMetadata updateMetadataForChunkAddition(MetadataTransaction txn,\n+                                                         SegmentMetadata segmentMetadata,\n+                                                         String newChunkName,\n+                                                         boolean isFirstWriteAfterFailover,\n+                                                         ChunkMetadata lastChunkMetadata) throws StorageMetadataException {\n+        ChunkMetadata newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        segmentMetadata.setLastChunk(newChunkName);\n+        if (lastChunkMetadata == null) {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+        } else {\n+            lastChunkMetadata.setNextChunk(newChunkName);\n+            txn.update(lastChunkMetadata);\n+        }\n+        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+        // Reset ownershipChanged flag after first write is done.\n+        if (isFirstWriteAfterFailover) {\n+            segmentMetadata.setOwnerEpoch(this.epoch);\n+            segmentMetadata.setOwnershipChanged(false);\n+            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, segmentMetadata.getName());\n+        }\n+        segmentMetadata.incrementChunkCount();\n+\n+        // Update the transaction.\n+        txn.update(newChunkMetadata);\n+        txn.update(segmentMetadata);\n+        return newChunkMetadata;\n+    }\n+\n+    /**\n+     * Write to chunk.\n+     */\n+    private int writeToChunk(MetadataTransaction txn,\n+                             SegmentMetadata segmentMetadata,\n+                             long offset,\n+                             InputStream data,\n+                             ChunkHandle chunkHandle,\n+                             ChunkMetadata chunkWrittenMetadata,\n+                             long offsetToWriteAt,\n+                             int bytesCount) throws IOException, StorageMetadataException, BadOffsetException {\n+        int bytesWritten;\n+        Preconditions.checkState(0 != bytesCount, \"Attempt to write zero bytes\");\n+        try {\n+\n+            // Finally write the data.\n+            try (BoundedInputStream bis = new BoundedInputStream(data, bytesCount)) {\n+                bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesCount, bis);\n+            }\n+\n+            // Update the metadata for segment and chunk.\n+            Preconditions.checkState(bytesWritten >= 0);\n+            segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+            chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+            txn.update(chunkWrittenMetadata);\n+            txn.update(segmentMetadata);\n+        } catch (IndexOutOfBoundsException e) {\n+            try {\n+                throw new BadOffsetException(segmentMetadata.getName(), chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+            } catch (ChunkStorageException cse) {\n+                log.error(\"{} write - Error while retrieving ChunkInfo for {}.\", logPrefix, chunkHandle.getChunkName());\n+                // The exact expected offset for the  operation does not matter, the StorageWriter will enter reconciliation loop anyway.\n+                throw new BadOffsetException(segmentMetadata.getName(), offset, offset);\n+            }\n+        }\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                        .segmentName(streamSegmentName)\n+                        .offset(offset)\n+                        .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                        .newChunkName(newChunkName)\n+                        .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+                targetSegmentMetadata.checkInvariants();\n+                checkNotSealed(targetSegmentName, targetSegmentMetadata);\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                checkSealed(sourceSegmentMetadata);\n+                checkOwnership(targetSegmentMetadata.getName(), targetSegmentMetadata);\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In the absence of defragmentation, the number of chunks for individual segments keeps on increasing.\n+     * When we have too many small chunks (say because many transactions with little data on some segments), the segment\n+     * is fragmented - this may impact both the read throughputand the performance of the metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (e.g., stock S3) and\n+     * each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If the underlying storage provides some facility to stitch together smaller chunk into larger chunks, then we do\n+     * actually want to exploit that, specially when the underlying implementation is only a metadata operation. We want\n+     * to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are typically\n+     * only metadata operations, reducing the overall cost of the merging them together. HDFS also supports merges,\n+     * whereas NFS has no concept of merging natively.\n+     *\n+     * As chunks become larger, append writes (read source completely and append-i.e., write- it back at the end of target)\n+     * become inefficient. Consequently, a native option for merging is desirable. We use such native merge capability\n+     * when available, and if not available, then we use appends.\n+     * </li>\n+     * <li>\n+     * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+     * We can then fine tune that background task to run optimally with low overhead.\n+     * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+     * </li>\n+     * <li>\n+     * <li>\n+     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+     * </li>\n+     * </ul>\n+     *\n+     * <div>\n+     * What controls whether we invoke concat or simulate through appends?\n+     * There are a few different capabilities that ChunkStorage needs to provide.\n+     * <ul>\n+     * <li>Does ChunkStorage support appending to existing chunks? For vanilla S3 compatible this would return false.\n+     * This is indicated by supportsAppend.</li>\n+     * <li>Does ChunkStorage support for concatenating chunks ? This is indicated by supportsConcat.\n+     * If this is true then concat operation will be invoked otherwise chunks will be appended.</li>\n+     * <li>There are some obvious constraints - For ChunkStorage support any concat functionality it must support either\n+     * append or concat.</li>\n+     * <li>Also when ChunkStorage supports both concat and append, ChunkManager will invoke appropriate method\n+     * depending on size of target and source chunks. (Eg. ECS)</li>\n+     * </ul>\n+     * </div>\n+     * <li>\n+     * What controls defrag?\n+     * There are two additional parameters that control when concat\n+     * <li>minSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, append is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n+     * <li>maxSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n+     * In short there is a size beyond which using append is not advisable. Conversely there is a size below which concat is not efficient.(minSizeLimitForConcat )\n+     * Then there is limit which concating does not make sense maxSizeLimitForConcat\n+     * </li>\n+     * <li>\n+     * What is the defrag algorithm\n+     * <pre>\n+     * While(segment.hasConcatableChunks()){", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzcwODIyNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447708224", "bodyText": "There is a planned StorageOptimiser component that is planned but not part of this set of PRs. https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2#components\n#4903", "author": "sachin-j-joshi", "createdAt": "2020-06-30T14:03:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5ODY1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc3MTA2MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447771060", "bodyText": "typo\nThis is \"max chunk length\", not max segment length. Segments can have a Long.MAX_VALUE length.", "author": "andreipaduroiu", "createdAt": "2020-06-30T15:24:15Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/SegmentRollingPolicy.java", "diffHunk": "@@ -10,22 +10,23 @@\n package io.pravega.segmentstore.storage;\n \n import com.google.common.base.Preconditions;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n import lombok.Getter;\n \n /**\n  * A generic rolling policy that can be applied to any Storage unit.\n  */\n public final class SegmentRollingPolicy {\n     /**\n-     * The max allowed value for 61 bit signed number which is 2,305,843,009,213,693,952.\n+     * The max allowed value for segment size.\n      */\n-    public static final long MAX_62_BIT_SIGNED_NUMBER = 1L << 61;\n+    public static final long MAX_SGEMENT_LENGTH = RevisionDataOutput.COMPACT_LONG_MAX;", "originalCommit": "681f95cc731842023fbbac1281109e01aa25a304", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzk5NjA1NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447996055", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-06-30T21:42:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc3MTA2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzk5NjQ5NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447996494", "bodyText": "1fd6faf", "author": "sachin-j-joshi", "createdAt": "2020-06-30T21:43:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc3MTA2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE1OTAxMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r448159010", "bodyText": "Don't we need to rename this to ChunkedSegmentStorageConfig?", "author": "fpj", "createdAt": "2020-07-01T07:07:44Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManagerConfig.java", "diffHunk": "@@ -16,13 +16,13 @@\n import lombok.NonNull;\n \n /**\n- * Configuration for {@link ChunkManager}.\n+ * Configuration for {@link ChunkedSegmentStorage}.\n  */\n @AllArgsConstructor\n @Builder(toBuilder = true)\n public class ChunkManagerConfig {\n     /**\n-     * Default configuration for {@link ChunkManager}.\n+     * Default configuration for {@link ChunkedSegmentStorage}.\n      */\n     public static final ChunkManagerConfig DEFAULT_CONFIG = ChunkManagerConfig.builder()", "originalCommit": "989fc1726ae61852ee5867c384ab5b788704b64b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQwMTQ2NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r448401464", "bodyText": "Sorry missed that.\nFixed.\neda9341", "author": "sachin-j-joshi", "createdAt": "2020-07-01T14:26:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE1OTAxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODg3MDg0NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r448870844", "bodyText": "Please do not leave TODOs in the code without an associated issue:\nhttps://github.com/pravega/pravega/wiki/Contributing#todos", "author": "fpj", "createdAt": "2020-07-02T09:26:34Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/mocks/InMemoryChunkStorage.java", "diffHunk": "@@ -0,0 +1,295 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.mocks;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.Builder;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/**\n+ * In-Memory mock for ChunkStorage. Contents is destroyed when object is garbage collected.\n+ */\n+@Slf4j\n+public class InMemoryChunkStorage extends AbstractInMemoryChunkStorage {\n+    private final ConcurrentHashMap<String, InMemoryChunk> chunks = new ConcurrentHashMap<String, InMemoryChunk>();\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Preconditions.checkNotNull(chunkName);\n+        InMemoryChunk chunk = chunks.get(chunkName);\n+        if (null == chunk) {\n+            throw new ChunkNotFoundException(chunkName, \"InMemoryChunkStorage::doGetInfo\");\n+        }\n+        return ChunkInfo.builder()\n+                .length(chunk.getLength())\n+                .name(chunkName)\n+                .build();\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Preconditions.checkNotNull(chunkName);\n+        if (null != chunks.putIfAbsent(chunkName, new InMemoryChunk(chunkName))) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"InMemoryChunkStorage::doCreate\");\n+        }\n+        return new ChunkHandle(chunkName, false);\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        return chunks.containsKey(chunkName);\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        InMemoryChunk chunk = getInMemoryChunk(handle);\n+        if (null == chunk) {\n+            throw new ChunkNotFoundException(handle.getChunkName(), \"InMemoryChunkStorage::doDelete\");\n+        }\n+        if (chunk.isReadOnly) {\n+            throw new ChunkStorageException(handle.getChunkName(), \"chunk is readonly\");\n+        }\n+        chunks.remove(handle.getChunkName());\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (chunks.containsKey(chunkName)) {\n+            return new ChunkHandle(chunkName, true);\n+        }\n+        throw new ChunkNotFoundException(chunkName, \"InMemoryChunkStorage::doOpenRead\");\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (chunks.containsKey(chunkName)) {\n+            return new ChunkHandle(chunkName, false);\n+        }\n+        throw new ChunkNotFoundException(chunkName, \"InMemoryChunkStorage::doOpenWrite\");\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        InMemoryChunk chunk = getInMemoryChunk(handle);\n+        if (fromOffset >= chunk.getLength()) {\n+            throw new ChunkStorageException(handle.getChunkName(), \"Attempt to read at wrong offset\");\n+        }\n+        if (length == 0) {\n+            throw new ChunkStorageException(handle.getChunkName(), \"Attempt to read 0 bytes\");\n+        }\n+\n+        // This is implemented this way to simulate possibility of partial read.\n+        InMemoryChunkData matchingData = null;\n+\n+        // TODO : This is OK for now. This is just test code, but need binary search here.", "originalCommit": "18e63a2df449f4e4249d5ed2fc71557351d3842f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA3ODU5OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r449078599", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-07-02T15:20:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODg3MDg0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODg5NzU1MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r448897550", "bodyText": "Some classes in the Javadoc are mentioned in plain text, whereas others are using {@link xxx} . I think that the latter is the right one.", "author": "RaulGracia", "createdAt": "2020-07-02T10:13:30Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -0,0 +1,1310 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+@Beta\n+public class ChunkedSegmentStorage implements Storage {\n+    /**\n+     * Configuration options for this ChunkedSegmentStorage instance.\n+     */\n+    @Getter\n+    private final ChunkedSegmentStorageConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkedSegmentStorage#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkedSegmentStorage#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkedSegmentStorage#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkedSegmentStorage class.", "originalCommit": "18e63a2df449f4e4249d5ed2fc71557351d3842f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA3NjYwMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r449076603", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-07-02T15:17:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODg5NzU1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODkxMDY1NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r448910655", "bodyText": "Nit: batch = extra space", "author": "RaulGracia", "createdAt": "2020-07-02T10:38:58Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,945 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import lombok.var;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Strings.nullToEmpty;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This creates a circular dependency while reading or writing the data about these segments from the metadata segments.\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks {@link ChunkAddedRecord} and truncation of segments {@link TruncationRecord}.\n+ * In addition to these two records, log also contains system snapshot records {@link SystemSnapshotRecord} which contains the state\n+ * of each storage system segments ({@link SegmentSnapshotRecord}) after replaying all available logs at the time of snapshots.\n+ * These snapshot records help avoid replaying entire log evey time. Each container instance records snapshot immediately after bootstrap.\n+ * To avoid data corruption, each instance writes to its own distinct log file/object.\n+ * The bootstrap algorithm also correctly ignores invalid log entries written by running instance which is no longer owner of the given container.\n+ * To prevent applying partial changes resulting from unexpected crash, the log records are written as {@link SystemJournalRecordBatch}.\n+ * In such cases either a full batch is read and applied completely or no records in the batch are applied.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    /**\n+     * Serializer for {@link SystemJournalRecordBatch}.\n+     */\n+    private static final SystemJournalRecordBatch.SystemJournalRecordBatchSerializer BATCH_SERIALIZER = new SystemJournalRecordBatch.SystemJournalRecordBatchSerializer();\n+\n+    /**\n+     * Serializer for {@link SystemSnapshotRecord}.\n+     */\n+    private static final SystemSnapshotRecord.Serializer SYSTEM_SNAPSHOT_SERIALIZER = new SystemSnapshotRecord.Serializer();\n+\n+    private final Object lock = new Object();\n+\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    @Getter\n+    private final ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private final long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private final int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    private final String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    private final String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Configuration {@link ChunkedSegmentStorageConfig} for the {@link ChunkedSegmentStorage}.\n+     */\n+    @Getter\n+    private final ChunkedSegmentStorageConfig config;\n+\n+    private final AtomicBoolean reentryGuard = new AtomicBoolean();\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId   Container id of the owner container.\n+     * @param epoch         Epoch of the current container instance.\n+     * @param chunkStorage  ChunkStorage instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param config        Configuration options for this ChunkedSegmentStorage instance.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, ChunkedSegmentStorageConfig config) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalChunkName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalChunkName());\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    public void bootstrap() throws Exception {\n+        Preconditions.checkState(!reentryGuard.getAndSet(true), \"bootstrap called multiple times.\");\n+        try (val txn = metadataStore.beginTransaction()) {\n+            // Keep track of offsets at which chunks were added to the system segments.\n+            val chunkStartOffsets = new HashMap<String, Long>();\n+\n+            // Keep track of offsets at which system segments were truncated.\n+            // We don't need to apply each truncate operation, only need to apply the final truncate offset.\n+            val finalTruncateOffsets = new HashMap<String, Long>();\n+            val finalFirstChunkStartsAtOffsets = new HashMap<String, Long>();\n+\n+            // Step 1: Create metadata records for system segments from latest snapshot.\n+            val epochToStart = applyLatestSnapshot(txn, chunkStartOffsets);\n+\n+            // Step 2: For each epoch, find the corresponding system journal files, process them and apply operations recorded.\n+            applySystemLogOperations(txn, epochToStart, chunkStartOffsets, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 3: Adjust the length of the last chunk.\n+            adjustLastChunkLengths(txn);\n+\n+            // Step 4: Apply the truncate offsets.\n+            applyFinalTruncateOffsets(txn, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 5: Validate and save a snapshot.\n+            validateAndSaveSnapshot(txn);\n+\n+            // Step 5: Finally commit all data.\n+            txn.commit(true, true);\n+        }\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param record Record to persist.\n+     * @throws ChunkStorageException Exception if any.\n+     */\n+    public void commitRecord(SystemJournalRecord record) throws ChunkStorageException {\n+        commitRecords(Collections.singletonList(record));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws ChunkStorageException Exception in case of any error.\n+     */\n+    public void commitRecords(Collection<SystemJournalRecord> records) throws ChunkStorageException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+\n+        // Open the underlying chunk to write.\n+        ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+        SystemJournalRecordBatch batch = SystemJournalRecordBatch.builder().systemJournalRecords(records).build();\n+        ByteArraySegment bytes;\n+        try {\n+            bytes = BATCH_SERIALIZER.serialize(batch);\n+        } catch (IOException e) {\n+            throw new ChunkStorageException(getSystemJournalChunkName(), \"Unable to serialize\", e);\n+        }\n+        // Persist\n+        synchronized (lock) {\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.getLength(),\n+                    new ByteArrayInputStream(bytes.array(), bytes.arrayOffset(), bytes.getLength()));\n+            Preconditions.checkState(bytesWritten == bytes.getLength());\n+            systemJournalOffset += bytesWritten;\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend() || config.isAppendsDisabled()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+        log.debug(\"SystemJournal[{}] Logging system log records - file={}, batch ={}.\", containerId, h.getChunkName(), batch);", "originalCommit": "18e63a2df449f4e4249d5ed2fc71557351d3842f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "504d605ea10dec901777041ef5964e2fe145b391", "url": "https://github.com/pravega/pravega/commit/504d605ea10dec901777041ef5964e2fe145b391", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Core functionality.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "3cdf67f5e1665461649746cbefe730247c2904ea", "url": "https://github.com/pravega/pravega/commit/3cdf67f5e1665461649746cbefe730247c2904ea", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Timing chunk lookup for reads, using debug friendly chunk names.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "5d58a22f8812915bebe13a03928ac518f6c9f8de", "url": "https://github.com/pravega/pravega/commit/5d58a22f8812915bebe13a03928ac518f6c9f8de", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Serialize with VersionedSerializer.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "3a310cef434d43a6fbbbeaf7604957ae04b386d3", "url": "https://github.com/pravega/pravega/commit/3a310cef434d43a6fbbbeaf7604957ae04b386d3", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Address review comments.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "1aa25b5a5ddd007e35f5761db478b314538cc848", "url": "https://github.com/pravega/pravega/commit/1aa25b5a5ddd007e35f5761db478b314538cc848", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Use VersionedSerializer and add snapshot records in SystemJournalLog.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "be55302044c89300557e8b67d0b7716fe4d5251a", "url": "https://github.com/pravega/pravega/commit/be55302044c89300557e8b67d0b7716fe4d5251a", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Formatting.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "21804f7bb90e6d46e545f670ed6a4231bcc2ee34", "url": "https://github.com/pravega/pravega/commit/21804f7bb90e6d46e545f670ed6a4231bcc2ee34", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Renaming and other review changes.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "c1247972baba6bc197a9e2f80178917603fadac8", "url": "https://github.com/pravega/pravega/commit/c1247972baba6bc197a9e2f80178917603fadac8", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Missing comments.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "587c483043b5b5ba7c3e317438277d83e4e2f420", "url": "https://github.com/pravega/pravega/commit/587c483043b5b5ba7c3e317438277d83e4e2f420", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - More cleanup.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "1e29944475db6539459206028f14130fa390f65a", "url": "https://github.com/pravega/pravega/commit/1e29944475db6539459206028f14130fa390f65a", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Cleanup.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "43357b2125ffdebe6f01b8f36a692b5b1f33df6d", "url": "https://github.com/pravega/pravega/commit/43357b2125ffdebe6f01b8f36a692b5b1f33df6d", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - More cleanup.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "d86cf60326c95782afa1847c345fe4962802a776", "url": "https://github.com/pravega/pravega/commit/d86cf60326c95782afa1847c345fe4962802a776", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Remove unnecessary asserts for pinned records.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "c5f833b56a03f14cdb789a92b6a19f6d57fd587e", "url": "https://github.com/pravega/pravega/commit/c5f833b56a03f14cdb789a92b6a19f6d57fd587e", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Address review comments.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "8d82ca4d4a6ee29e28485f30cd34c0a9ec0c2bb7", "url": "https://github.com/pravega/pravega/commit/8d82ca4d4a6ee29e28485f30cd34c0a9ec0c2bb7", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Address review comments.\nUse RevisionDataOutput.COMPACT_LONG_MAX as max segment length.\nMake fields in SystemJournal final.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "5a21fd216be6897d4f98c3ef25675bf1df6b4f72", "url": "https://github.com/pravega/pravega/commit/5a21fd216be6897d4f98c3ef25675bf1df6b4f72", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Address review comments.\nFix typos.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "433231dce580163cff6d56989fd9fa63617c52e4", "url": "https://github.com/pravega/pravega/commit/433231dce580163cff6d56989fd9fa63617c52e4", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Address review comments.\nRename ChunkManager to ChunkedSegmentStorage.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "1231efcf568485bfe2e15168ce9866b98e964f39", "url": "https://github.com/pravega/pravega/commit/1231efcf568485bfe2e15168ce9866b98e964f39", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Address review comments.\nRename ChunkManagerConfig to ChunkedSegmentStorageConfig.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "d622132b086f09b86028dba5cbf4980ce070b9d3", "url": "https://github.com/pravega/pravega/commit/d622132b086f09b86028dba5cbf4980ce070b9d3", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Add @Beta annotation.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "152739c667387c371d71a6bbd833b87d70eafd7d", "url": "https://github.com/pravega/pravega/commit/152739c667387c371d71a6bbd833b87d70eafd7d", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Fix review comments.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "152739c667387c371d71a6bbd833b87d70eafd7d", "url": "https://github.com/pravega/pravega/commit/152739c667387c371d71a6bbd833b87d70eafd7d", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Fix review comments.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "forcePushed"}]}