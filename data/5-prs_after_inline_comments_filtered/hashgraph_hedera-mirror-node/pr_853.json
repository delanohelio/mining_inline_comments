{"pr_number": 853, "pr_title": "Concurrent inserts", "pr_createdAt": "2020-06-29T21:15:52Z", "pr_url": "https://github.com/hashgraph/hedera-mirror-node/pull/853", "timeline": [{"oid": "c58c5f3e4289d4dac7908533f7a39ebb95bc89ac", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/c58c5f3e4289d4dac7908533f7a39ebb95bc89ac", "message": "add tests\n\nSigned-off-by: Apekshit Sharma <apekshit.sharma@hedera.com>", "committedDate": "2020-06-29T21:16:18Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg4ODEwNw==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r447888107", "bodyText": "You should expose this ms duration as a metric for elastic. i.e duration to copy for each table", "author": "Nana-EC", "createdAt": "2020-06-30T18:18:12Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/PgCopy.java", "diffHunk": "@@ -0,0 +1,105 @@\n+package com.hedera.mirror.importer.parser.record.entity.sql;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import com.fasterxml.jackson.databind.ObjectWriter;\n+import com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+import com.google.common.base.CaseFormat;\n+import com.google.common.base.Stopwatch;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.io.StringReader;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.util.List;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+import lombok.extern.log4j.Log4j2;\n+import org.apache.commons.io.output.StringBuilderWriter;\n+import org.postgresql.copy.CopyManager;\n+import org.postgresql.jdbc.PgConnection;\n+\n+import com.hedera.mirror.importer.exception.ParserException;\n+\n+// TODO: unit tests\n+/**\n+ * Stateless writer to insert rows into Postgres table using COPY.\n+ * @param <T> domain object\n+ */\n+@Log4j2\n+public class PgCopy<T> implements Closeable {\n+    private final Connection connection;\n+    private final String tableName;\n+    private final String columnsCsv;\n+    private final ObjectWriter writer;\n+    private final CopyManager copyManager;\n+\n+    public PgCopy(Connection connection, Class<T> tClass) throws SQLException {\n+        this.connection = connection;\n+        this.copyManager = connection.unwrap(PgConnection.class).getCopyAPI();\n+        this.tableName = CaseFormat.UPPER_CAMEL.to(CaseFormat.LOWER_UNDERSCORE, tClass.getSimpleName());\n+        var mapper = new CsvMapper();\n+        var schema = mapper.schemaFor(tClass);\n+        this.writer = mapper.writer(schema);\n+        this.columnsCsv = Lists.newArrayList(schema.iterator()).stream()\n+                .map(CsvSchema.Column::getName)\n+                .map(name -> CaseFormat.UPPER_CAMEL.to(CaseFormat.LOWER_UNDERSCORE, name))\n+                .collect(Collectors.joining(\", \"));\n+    }\n+\n+    public void copy(List<T> items) {\n+        if (items.size() == 0) {\n+            return;\n+        }\n+        try {\n+            Stopwatch stopwatch = Stopwatch.createStarted();\n+            long rowsCount = copyManager.copyIn(\n+                    String.format(\"COPY %s(%s) FROM STDIN WITH CSV\", tableName, columnsCsv),\n+                    new StringReader(getCsvData(items)));\n+            log.info(\"Copied {} rows to {} table in {}ms\",", "originalCommit": "c58c5f3e4289d4dac7908533f7a39ebb95bc89ac", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTMxNDg5Ng==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r449314896", "bodyText": "Good idea. Added 2 metrics.", "author": "apeksharma", "createdAt": "2020-07-02T23:57:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzg4ODEwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQxMzk3OA==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r448413978", "bodyText": "nit: remove or uncomment as needed", "author": "Nana-EC", "createdAt": "2020-07-01T14:44:11Z", "path": "hedera-mirror-datagenerator/src/main/java/com/hedera/datagenerator/DataGenerator.java", "diffHunk": "@@ -22,6 +23,8 @@\n import org.springframework.boot.SpringApplication;\n import org.springframework.boot.autoconfigure.SpringBootApplication;\n \n+//@ConfigurationPropertiesScan(basePackages = \"com.hedera.datagenerator\", basePackageClasses = {SqlProperties.class})", "originalCommit": "e58d7877254ea868aa2be7edf2a9835febdb02ad", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTMxNDA0OA==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r449314048", "bodyText": "done.", "author": "apeksharma", "createdAt": "2020-07-02T23:54:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQxMzk3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQxNjg2Mw==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r448416863", "bodyText": "nit: same comment on line 108. I think it's the flush that actually causes the writing to db no?", "author": "Nana-EC", "createdAt": "2020-07-01T14:48:10Z", "path": "hedera-mirror-datagenerator/src/main/java/com/hedera/datagenerator/domain/DomainDriver.java", "diffHunk": "@@ -97,13 +102,19 @@ public void run(ApplicationArguments args) {\n         }\n         log.info(\"Generated {} transactions in {}\", numTransactionsGenerated, stopwatch);\n         new EntityGenerator().generateAndWriteEntities(entityManager, domainWriter);\n+        log.info(\"Writing data to db\");\n+        sqlEntityListener.onEnd(new RecordFile(0L, 1L, 1L, \"\", 0L, 1L, \"\", \"\", 0)); // writes data to db", "originalCommit": "e58d7877254ea868aa2be7edf2a9835febdb02ad", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTMxNjI1MA==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r449316250", "bodyText": "sqlEntityListener.onEnd() writes the entities (txn, ctl, topic message, etc).\nSince SEL (scope: Mirror importer) doesn't have writers for Entities and AccountBalance, we need DomainWriter (scope limited to datagenerator.\nIn future, you can move onEnd inside while loop above and accurately measure raw write throughput of SqlEntityListnener for any kind of workload. For eg\n\nCrypto transfers only, 5 transfers per txn, or 10 transfers per txn\nHCS messages with custom message sizes", "author": "apeksharma", "createdAt": "2020-07-03T00:03:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQxNjg2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQyMDc4OA==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r448420788", "bodyText": "q: Why the need to create this extra domain class in datagenarator instead of using com.hedera.mirror.importer.domain.AccountBalance?", "author": "Nana-EC", "createdAt": "2020-07-01T14:53:54Z", "path": "hedera-mirror-datagenerator/src/main/java/com/hedera/datagenerator/domain/AccountBalance.java", "diffHunk": "@@ -0,0 +1,36 @@\n+package com.hedera.datagenerator.domain;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import lombok.Data;\n+import lombok.RequiredArgsConstructor;\n+\n+@Data\n+@RequiredArgsConstructor\n+public class AccountBalance {", "originalCommit": "e58d7877254ea868aa2be7edf2a9835febdb02ad", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTMxNDQxOA==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r449314418", "bodyText": "Since the other one using EmbeddedId and won't work with ObjectMapper, at least not easily. Am not sure if there's a roundabout way. This is simpler and faster.", "author": "apeksharma", "createdAt": "2020-07-02T23:55:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQyMDc4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQzMTcyMQ==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r448431721", "bodyText": "q: shouldn't we at least have an error log highlighting something went wrong?", "author": "Nana-EC", "createdAt": "2020-07-01T15:10:20Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -48,352 +49,163 @@\n import com.hedera.mirror.importer.exception.ParserException;\n import com.hedera.mirror.importer.exception.ParserSQLException;\n import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+import com.hedera.mirror.importer.parser.record.ConditionalOnRecordParser;\n import com.hedera.mirror.importer.parser.record.RecordStreamFileListener;\n-import com.hedera.mirror.importer.parser.record.entity.ConditionOnEntityRecordParser;\n import com.hedera.mirror.importer.parser.record.entity.EntityListener;\n+import com.hedera.mirror.importer.repository.EntityRepository;\n import com.hedera.mirror.importer.repository.RecordFileRepository;\n \n @Log4j2\n @Named\n-@RequiredArgsConstructor\n-@ConditionOnEntityRecordParser\n-public class SqlEntityListener implements EntityListener, RecordStreamFileListener {\n-    private final SqlProperties properties;\n+@ConditionalOnRecordParser\n+public class SqlEntityListener implements EntityListener, RecordStreamFileListener, Closeable {\n     private final DataSource dataSource;\n+    private final ExecutorService executorService;\n     private final RecordFileRepository recordFileRepository;\n-    private long batch_count = 0;\n-    // Keeps track of entityIds seen in the current file being processed. This is for optimizing inserts into\n-    // t_entities table so that insertion of node and treasury ids are not tried for every transaction.\n-    private Collection<EntityId> entityIds;\n-    private PreparedStatement sqlInsertTransaction;\n-    private PreparedStatement sqlInsertEntityId;\n-    private PreparedStatement sqlInsertTransferList;\n-    private PreparedStatement sqlInsertNonFeeTransfers;\n-    private PreparedStatement sqlInsertFileData;\n-    private PreparedStatement sqlInsertContractResult;\n-    private PreparedStatement sqlInsertLiveHashes;\n-    private PreparedStatement sqlInsertTopicMessage;\n-    private Connection connection;\n+    private final EntityRepository entityRepository;\n+    // Keeps track of entityIds seen so far. This is for optimizing inserts into t_entities table so that insertion of\n+    // node and treasury ids are not tried for every transaction.\n+    private final Collection<EntityId> seenEntityIds = new HashSet<>();\n+    // init connections, schemas, writers, etc once per process\n+    private final PgCopy<Transaction> transactionPgCopy;\n+    private final PgCopy<CryptoTransfer> cryptoTransferPgCopy;\n+    private final PgCopy<NonFeeTransfer> nonFeeTransferPgCopy;\n+    private final PgCopy<FileData> fileDataPgCopy;\n+    private final PgCopy<ContractResult> contractResultPgCopy;\n+    private final PgCopy<LiveHash> liveHashPgCopy;\n+    private final PgCopy<TopicMessage> topicMessagePgCopy;\n+\n+    private List<Transaction> transactions;\n+    private List<CryptoTransfer> cryptoTransfers;\n+    private List<NonFeeTransfer> nonFeeTransfers;\n+    private List<FileData> fileData;\n+    private List<ContractResult> contractResults;\n+    private List<LiveHash> liveHashes;\n+    private List<TopicMessage> topicMessages;\n+    private List<EntityId> entityIds;\n+\n+    public SqlEntityListener(SqlProperties properties, DataSource dataSource,\n+                             RecordFileRepository recordFileRepository, EntityRepository entityRepository) {\n+        this.dataSource = dataSource;\n+        this.recordFileRepository = recordFileRepository;\n+        this.entityRepository = entityRepository;\n+        this.executorService = Executors.newFixedThreadPool(properties.getThreads());\n+        try {\n+            transactionPgCopy = new PgCopy<>(getConnection(), Transaction.class);\n+            cryptoTransferPgCopy = new PgCopy<>(getConnection(), CryptoTransfer.class);\n+            nonFeeTransferPgCopy = new PgCopy<>(getConnection(), NonFeeTransfer.class);\n+            fileDataPgCopy = new PgCopy<>(getConnection(), FileData.class);\n+            contractResultPgCopy = new PgCopy<>(getConnection(), ContractResult.class);\n+            liveHashPgCopy = new PgCopy<>(getConnection(), LiveHash.class);\n+            topicMessagePgCopy = new PgCopy<>(getConnection(), TopicMessage.class);\n+        } catch (SQLException e) {\n+            throw new ParserException(\"Error setting up postgres copier\", e);\n+        }\n+    }\n \n     @Override\n     public void onStart(StreamFileData streamFileData) {\n         String fileName = streamFileData.getFilename();\n-        entityIds = new HashSet<>();\n         if (recordFileRepository.findByName(fileName).size() > 0) {\n             throw new DuplicateFileException(\"File already exists in the database: \" + fileName);\n         }\n-        try {\n-            initConnectionAndStatements();\n-        } catch (Exception e) {\n-            throw new ParserException(\"Error setting up connection and statements\", e);\n-        }\n+        transactions = new ArrayList<>();\n+        cryptoTransfers = new ArrayList<>();\n+        nonFeeTransfers = new ArrayList<>();\n+        fileData = new ArrayList<>();\n+        contractResults = new ArrayList<>();\n+        liveHashes = new ArrayList<>();\n+        entityIds = new ArrayList<>();\n+        topicMessages = new ArrayList<>();\n     }\n \n     @Override\n     public void onEnd(RecordFile recordFile) {\n-        executeBatches();\n         try {\n-            connection.commit();\n-            recordFileRepository.save(recordFile);\n-            closeConnectionAndStatements();\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(e);\n-        }\n+            CompletableFuture.allOf(\n+                    CompletableFuture.runAsync(() -> transactionPgCopy.copy(transactions), executorService),\n+                    CompletableFuture.runAsync(() -> cryptoTransferPgCopy.copy(cryptoTransfers), executorService),\n+                    CompletableFuture.runAsync(() -> nonFeeTransferPgCopy.copy(nonFeeTransfers), executorService),\n+                    CompletableFuture.runAsync(() -> fileDataPgCopy.copy(fileData), executorService),\n+                    CompletableFuture.runAsync(() -> contractResultPgCopy.copy(contractResults), executorService),\n+                    CompletableFuture.runAsync(() -> liveHashPgCopy.copy(liveHashes), executorService),\n+                    CompletableFuture.runAsync(() -> topicMessagePgCopy.copy(topicMessages), executorService),\n+                    CompletableFuture.runAsync(() ->\n+                        entityIds.forEach(entityRepository::insertEntityIdDoNothingOnConflict), executorService)\n+            ).get();\n+        } catch (InterruptedException | ExecutionException e) {\n+            log.error(e);\n+            throw new ParserException(e);\n+        }\n+        recordFileRepository.save(recordFile);\n     }\n \n     @Override\n     public void onError() {\n-        try {\n-            if (connection != null) {\n-                connection.rollback();\n-                closeConnectionAndStatements();\n-            }\n-        } catch (SQLException e) {\n-            log.error(\"Exception while rolling transaction back\", e);\n-        }\n-    }\n-\n-    private void initConnectionAndStatements() throws ParserSQLException {\n-        try {\n-            connection = dataSource.getConnection();\n-            connection.setAutoCommit(false); // do not auto-commit\n-            connection.setClientInfo(\"ApplicationName\", getClass().getSimpleName());\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(\"Error setting up connection to database\", e);\n-        }\n-        try {\n-            sqlInsertTransaction = connection.prepareStatement(\"INSERT INTO transaction\"\n-                    + \" (node_account_id, memo, valid_start_ns, type, payer_account_id, result, \" +\n-                    \"consensus_ns, entity_id, charged_tx_fee, initial_balance, valid_duration_seconds, max_fee, \" +\n-                    \"transaction_hash, transaction_bytes)\"\n-                    + \" VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\");\n-\n-            sqlInsertEntityId = connection.prepareStatement(\"INSERT INTO t_entities \" +\n-                    \"(id, entity_shard, entity_realm, entity_num, fk_entity_type_id) \" +\n-                    \"VALUES (?, ?, ?, ?, ?) \" +\n-                    \"ON CONFLICT DO NOTHING\");\n-\n-            sqlInsertTransferList = connection.prepareStatement(\"INSERT INTO crypto_transfer\"\n-                    + \" (consensus_timestamp, amount, entity_id)\"\n-                    + \" VALUES (?, ?, ?)\");\n-\n-            sqlInsertNonFeeTransfers = connection.prepareStatement(\"insert into non_fee_transfer\"\n-                    + \" (consensus_timestamp, amount, entity_id)\"\n-                    + \" values (?, ?, ?)\");\n-\n-            sqlInsertFileData = connection.prepareStatement(\"INSERT INTO t_file_data\"\n-                    + \" (consensus_timestamp, file_data)\"\n-                    + \" VALUES (?, ?)\");\n-\n-            sqlInsertContractResult = connection.prepareStatement(\"INSERT INTO t_contract_result\"\n-                    + \" (consensus_timestamp, function_params, gas_supplied, call_result, gas_used)\"\n-                    + \" VALUES (?, ?, ?, ?, ?)\");\n-\n-            sqlInsertLiveHashes = connection.prepareStatement(\"INSERT INTO t_livehashes\"\n-                    + \" (consensus_timestamp, livehash)\"\n-                    + \" VALUES (?, ?)\");\n-\n-            sqlInsertTopicMessage = connection.prepareStatement(\"insert into topic_message\"\n-                    + \" (consensus_timestamp, realm_num, topic_num, message, running_hash, sequence_number\" +\n-                    \", running_hash_version)\"\n-                    + \" values (?, ?, ?, ?, ?, ?, ?)\");\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(\"Unable to prepare SQL statements\", e);\n-        }\n-    }\n-\n-    private void closeConnectionAndStatements() {\n-        try {\n-            sqlInsertTransaction.close();\n-            sqlInsertEntityId.close();\n-            sqlInsertTransferList.close();\n-            sqlInsertNonFeeTransfers.close();\n-            sqlInsertFileData.close();\n-            sqlInsertContractResult.close();\n-            sqlInsertLiveHashes.close();\n-            sqlInsertTopicMessage.close();\n-\n-            connection.close();\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(\"Error closing connection\", e);\n-        }\n+        // no error handling", "originalCommit": "e58d7877254ea868aa2be7edf2a9835febdb02ad", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTMxNDgxOQ==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r449314819", "bodyText": "RecordFileParser logs error before calling this method", "author": "apeksharma", "createdAt": "2020-07-02T23:57:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQzMTcyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTI4NDY0MA==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r449284640", "bodyText": "Use writeValueAsString() as it's more efficient and cleaner.", "author": "steven-sheehy", "createdAt": "2020-07-02T21:59:52Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/PgCopy.java", "diffHunk": "@@ -0,0 +1,106 @@\n+package com.hedera.mirror.importer.parser.record.entity.sql;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import com.fasterxml.jackson.databind.ObjectWriter;\n+import com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+import com.google.common.base.CaseFormat;\n+import com.google.common.base.Stopwatch;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.io.StringReader;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.util.List;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+import lombok.extern.log4j.Log4j2;\n+import org.apache.commons.io.output.StringBuilderWriter;\n+import org.postgresql.PGConnection;\n+import org.postgresql.copy.CopyManager;\n+\n+import com.hedera.mirror.importer.exception.ParserException;\n+\n+/**\n+ * Stateless writer to insert rows into Postgres table using COPY.\n+ * @param <T> domain object\n+ */\n+@Log4j2\n+public class PgCopy<T> implements Closeable {\n+    private final Connection connection;\n+    private final String tableName;\n+    private final String columnsCsv;\n+    private final ObjectWriter writer;\n+    private final CopyManager copyManager;\n+\n+    public PgCopy(Connection connection, Class<T> tClass) throws SQLException {\n+        this.connection = connection;\n+        this.copyManager = connection.unwrap(PGConnection.class).getCopyAPI();\n+        this.tableName = CaseFormat.UPPER_CAMEL.to(CaseFormat.LOWER_UNDERSCORE, tClass.getSimpleName());\n+        var mapper = new CsvMapper();\n+        var schema = mapper.schemaFor(tClass);\n+        this.writer = mapper.writer(schema);\n+        this.columnsCsv = Lists.newArrayList(schema.iterator()).stream()\n+                .map(CsvSchema.Column::getName)\n+                .map(name -> CaseFormat.UPPER_CAMEL.to(CaseFormat.LOWER_UNDERSCORE, name))\n+                .collect(Collectors.joining(\", \"));\n+    }\n+\n+    public void copy(List<T> items) {\n+        if (items == null || items.size() == 0) {\n+            return;\n+        }\n+        try {\n+            Stopwatch stopwatch = Stopwatch.createStarted();\n+            log.debug(\"Copying {} rows to {} table\", items.size(), tableName);\n+            long rowsCount = copyManager.copyIn(\n+                    String.format(\"COPY %s(%s) FROM STDIN WITH CSV\", tableName, columnsCsv),\n+                    new StringReader(getCsvData(items)));\n+            log.info(\"Copied {} rows to {} table in {}ms\",\n+                    rowsCount, tableName, stopwatch.elapsed(TimeUnit.MILLISECONDS));\n+        } catch (IOException | SQLException e) {\n+            log.error(e);\n+            throw new ParserException(e);\n+        }\n+    }\n+\n+    @Override\n+    public void close() {\n+        try {\n+            connection.close();\n+        } catch (SQLException e) {\n+            log.error(\"Exception closing connection\", e);\n+        }\n+    }\n+\n+    private String getCsvData(List<T> items) throws IOException {\n+        Stopwatch stopwatch = Stopwatch.createStarted();\n+        var stringBuilderWriter = new StringBuilderWriter();\n+        writer.writeValues(stringBuilderWriter).writeAll(items);", "originalCommit": "e58d7877254ea868aa2be7edf2a9835febdb02ad", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTI4NTk4NQ==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r449285985", "bodyText": "Why do we need to store the connection and copymanager and manually close? It's not like we're using transactions anymore between the various tables so they could each use a separate connection (or same, point is I don't think it matters anymore they're the same).", "author": "steven-sheehy", "createdAt": "2020-07-02T22:04:25Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/PgCopy.java", "diffHunk": "@@ -0,0 +1,106 @@\n+package com.hedera.mirror.importer.parser.record.entity.sql;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import com.fasterxml.jackson.databind.ObjectWriter;\n+import com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+import com.google.common.base.CaseFormat;\n+import com.google.common.base.Stopwatch;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.io.StringReader;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.util.List;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+import lombok.extern.log4j.Log4j2;\n+import org.apache.commons.io.output.StringBuilderWriter;\n+import org.postgresql.PGConnection;\n+import org.postgresql.copy.CopyManager;\n+\n+import com.hedera.mirror.importer.exception.ParserException;\n+\n+/**\n+ * Stateless writer to insert rows into Postgres table using COPY.\n+ * @param <T> domain object\n+ */\n+@Log4j2\n+public class PgCopy<T> implements Closeable {\n+    private final Connection connection;\n+    private final String tableName;\n+    private final String columnsCsv;\n+    private final ObjectWriter writer;\n+    private final CopyManager copyManager;\n+\n+    public PgCopy(Connection connection, Class<T> tClass) throws SQLException {\n+        this.connection = connection;\n+        this.copyManager = connection.unwrap(PGConnection.class).getCopyAPI();", "originalCommit": "e58d7877254ea868aa2be7edf2a9835febdb02ad", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYyNDc0MQ==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r454624741", "bodyText": "Adopted dataSource injection and allowing PgCopyclasses to request connection", "author": "Nana-EC", "createdAt": "2020-07-14T20:28:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTI4NTk4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTI4NzU1NQ==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r449287555", "bodyText": "We may want to add the buffer size and make the buffer size a property", "author": "steven-sheehy", "createdAt": "2020-07-02T22:09:49Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/PgCopy.java", "diffHunk": "@@ -0,0 +1,106 @@\n+package com.hedera.mirror.importer.parser.record.entity.sql;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import com.fasterxml.jackson.databind.ObjectWriter;\n+import com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+import com.google.common.base.CaseFormat;\n+import com.google.common.base.Stopwatch;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.io.StringReader;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.util.List;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+import lombok.extern.log4j.Log4j2;\n+import org.apache.commons.io.output.StringBuilderWriter;\n+import org.postgresql.PGConnection;\n+import org.postgresql.copy.CopyManager;\n+\n+import com.hedera.mirror.importer.exception.ParserException;\n+\n+/**\n+ * Stateless writer to insert rows into Postgres table using COPY.\n+ * @param <T> domain object\n+ */\n+@Log4j2\n+public class PgCopy<T> implements Closeable {\n+    private final Connection connection;\n+    private final String tableName;\n+    private final String columnsCsv;\n+    private final ObjectWriter writer;\n+    private final CopyManager copyManager;\n+\n+    public PgCopy(Connection connection, Class<T> tClass) throws SQLException {\n+        this.connection = connection;\n+        this.copyManager = connection.unwrap(PGConnection.class).getCopyAPI();\n+        this.tableName = CaseFormat.UPPER_CAMEL.to(CaseFormat.LOWER_UNDERSCORE, tClass.getSimpleName());\n+        var mapper = new CsvMapper();\n+        var schema = mapper.schemaFor(tClass);\n+        this.writer = mapper.writer(schema);\n+        this.columnsCsv = Lists.newArrayList(schema.iterator()).stream()\n+                .map(CsvSchema.Column::getName)\n+                .map(name -> CaseFormat.UPPER_CAMEL.to(CaseFormat.LOWER_UNDERSCORE, name))\n+                .collect(Collectors.joining(\", \"));\n+    }\n+\n+    public void copy(List<T> items) {\n+        if (items == null || items.size() == 0) {\n+            return;\n+        }\n+        try {\n+            Stopwatch stopwatch = Stopwatch.createStarted();\n+            log.debug(\"Copying {} rows to {} table\", items.size(), tableName);\n+            long rowsCount = copyManager.copyIn(", "originalCommit": "e58d7877254ea868aa2be7edf2a9835febdb02ad", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzkwOTEzMQ==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r453909131", "bodyText": "Added buffer size but unsure about adding it here since the size will differ on the last buffer batch", "author": "Nana-EC", "createdAt": "2020-07-13T20:22:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTI4NzU1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA3NDEyNg==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r456074126", "bodyText": "What do you mean? It's just a buffer, it doesn't vary. The real bytes within the buffer vary but not the size of the buffer.", "author": "steven-sheehy", "createdAt": "2020-07-16T20:58:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTI4NzU1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM1OTcyNw==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r449359727", "bodyText": "Would prefer shorter insertEntityId for this and custom method since it's not like we'll have an insertEntityIdDoSomethingElseOnConflict in the future.", "author": "steven-sheehy", "createdAt": "2020-07-03T03:36:41Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/repository/EntityRepository.java", "diffHunk": "@@ -21,21 +21,30 @@\n  */\n \n import java.util.Optional;\n+import javax.transaction.Transactional;\n import org.springframework.cache.annotation.CacheConfig;\n import org.springframework.cache.annotation.CachePut;\n import org.springframework.cache.annotation.Cacheable;\n+import org.springframework.data.jpa.repository.Modifying;\n+import org.springframework.data.jpa.repository.Query;\n import org.springframework.data.repository.PagingAndSortingRepository;\n \n import com.hedera.mirror.importer.config.CacheConfiguration;\n import com.hedera.mirror.importer.domain.Entities;\n \n @CacheConfig(cacheNames = \"entities\", cacheManager = CacheConfiguration.EXPIRE_AFTER_30M)\n-public interface EntityRepository extends PagingAndSortingRepository<Entities, Long> {\n+public interface EntityRepository extends PagingAndSortingRepository<Entities, Long>, EntityRepositoryCustom {\n \n     @Cacheable(key = \"{#p0}\", sync = true)\n     Optional<Entities> findById(long id);\n \n     @CachePut(key = \"{#p0.entityShard, #p0.entityRealm, #p0.entityNum}\")\n     @Override\n     <S extends Entities> S save(S entity);\n+\n+    @Modifying\n+    @Query(value = \"insert into t_entities (id, entity_shard, entity_realm, entity_num, fk_entity_type_id) \" +\n+            \"values (?1, ?2, ?3, ?4, ?5) on conflict do nothing\", nativeQuery = true)\n+    @Transactional\n+    void insertEntityIdDoNothingOnConflict(long id, long shard, long realm, long num, long type);", "originalCommit": "c29400bbb5fea504df2c7f2008b6272e44bad81e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg4ODk1OQ==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r453888959", "bodyText": "Updated", "author": "Nana-EC", "createdAt": "2020-07-13T19:43:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM1OTcyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTY2NDkyNw==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r449664927", "bodyText": "We should consider keeping the batching we had previously but adapting it for pgcopy. Reasons are:\na) To avoid having to load the entire file into memory + keeping all domain objects from file in memory + keeping a csv representation of objects in memory\nb) Reducing end to end latency for an individual transaction\nWe could even keep the default batch size at Integer.MAX_VALUE for now effectively keeping the current functionality but having the option to tweak it to see it's effect on memory and latency during performance testing I think is important.", "author": "steven-sheehy", "createdAt": "2020-07-03T17:20:52Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -48,352 +52,171 @@\n import com.hedera.mirror.importer.exception.ParserException;\n import com.hedera.mirror.importer.exception.ParserSQLException;\n import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+import com.hedera.mirror.importer.parser.record.ConditionalOnRecordParser;\n import com.hedera.mirror.importer.parser.record.RecordStreamFileListener;\n-import com.hedera.mirror.importer.parser.record.entity.ConditionOnEntityRecordParser;\n import com.hedera.mirror.importer.parser.record.entity.EntityListener;\n+import com.hedera.mirror.importer.repository.EntityRepository;\n import com.hedera.mirror.importer.repository.RecordFileRepository;\n \n @Log4j2\n @Named\n-@RequiredArgsConstructor\n-@ConditionOnEntityRecordParser\n-public class SqlEntityListener implements EntityListener, RecordStreamFileListener {\n-    private final SqlProperties properties;\n+@ConditionalOnRecordParser\n+public class SqlEntityListener implements EntityListener, RecordStreamFileListener, Closeable {\n     private final DataSource dataSource;\n+    private final ExecutorService executorService;\n     private final RecordFileRepository recordFileRepository;\n-    private long batch_count = 0;\n-    // Keeps track of entityIds seen in the current file being processed. This is for optimizing inserts into\n-    // t_entities table so that insertion of node and treasury ids are not tried for every transaction.\n-    private Collection<EntityId> entityIds;\n-    private PreparedStatement sqlInsertTransaction;\n-    private PreparedStatement sqlInsertEntityId;\n-    private PreparedStatement sqlInsertTransferList;\n-    private PreparedStatement sqlInsertNonFeeTransfers;\n-    private PreparedStatement sqlInsertFileData;\n-    private PreparedStatement sqlInsertContractResult;\n-    private PreparedStatement sqlInsertLiveHashes;\n-    private PreparedStatement sqlInsertTopicMessage;\n-    private Connection connection;\n+    private final EntityRepository entityRepository;\n+    // Keeps track of entityIds seen so far. This is for optimizing inserts into t_entities table so that insertion of\n+    // node and treasury ids are not tried for every transaction.\n+    private final Collection<EntityId> seenEntityIds = new HashSet<>();\n+    // init connections, schemas, writers, etc once per process\n+    private final PgCopy<Transaction> transactionPgCopy;\n+    private final PgCopy<CryptoTransfer> cryptoTransferPgCopy;\n+    private final PgCopy<NonFeeTransfer> nonFeeTransferPgCopy;\n+    private final PgCopy<FileData> fileDataPgCopy;\n+    private final PgCopy<ContractResult> contractResultPgCopy;\n+    private final PgCopy<LiveHash> liveHashPgCopy;\n+    private final PgCopy<TopicMessage> topicMessagePgCopy;\n+\n+    private final Timer insertDuration;\n+\n+    private List<Transaction> transactions;\n+    private List<CryptoTransfer> cryptoTransfers;\n+    private List<NonFeeTransfer> nonFeeTransfers;\n+    private List<FileData> fileData;\n+    private List<ContractResult> contractResults;\n+    private List<LiveHash> liveHashes;\n+    private List<TopicMessage> topicMessages;\n+    private List<EntityId> entityIds;\n+\n+    public SqlEntityListener(SqlProperties properties, DataSource dataSource,\n+                             RecordFileRepository recordFileRepository, EntityRepository entityRepository,\n+                             MeterRegistry meterRegistry) {\n+        this.dataSource = dataSource;\n+        this.recordFileRepository = recordFileRepository;\n+        this.entityRepository = entityRepository;\n+        this.executorService = Executors.newFixedThreadPool(properties.getThreads());\n+        try {\n+            transactionPgCopy = new PgCopy<>(getConnection(), Transaction.class, meterRegistry);\n+            cryptoTransferPgCopy = new PgCopy<>(getConnection(), CryptoTransfer.class, meterRegistry);\n+            nonFeeTransferPgCopy = new PgCopy<>(getConnection(), NonFeeTransfer.class, meterRegistry);\n+            fileDataPgCopy = new PgCopy<>(getConnection(), FileData.class, meterRegistry);\n+            contractResultPgCopy = new PgCopy<>(getConnection(), ContractResult.class, meterRegistry);\n+            liveHashPgCopy = new PgCopy<>(getConnection(), LiveHash.class, meterRegistry);\n+            topicMessagePgCopy = new PgCopy<>(getConnection(), TopicMessage.class, meterRegistry);\n+        } catch (SQLException e) {\n+            throw new ParserException(\"Error setting up postgres copier\", e);\n+        }\n+        insertDuration = Timer.builder(\"hedera.mirror.importer.parser.record.entity.sql.insert\")\n+                .description(\"Time to insert all entities into database\")\n+                .register(meterRegistry);\n+    }\n \n     @Override\n     public void onStart(StreamFileData streamFileData) {\n         String fileName = streamFileData.getFilename();\n-        entityIds = new HashSet<>();\n         if (recordFileRepository.findByName(fileName).size() > 0) {\n             throw new DuplicateFileException(\"File already exists in the database: \" + fileName);\n         }\n-        try {\n-            initConnectionAndStatements();\n-        } catch (Exception e) {\n-            throw new ParserException(\"Error setting up connection and statements\", e);\n-        }\n+        transactions = new ArrayList<>();\n+        cryptoTransfers = new ArrayList<>();\n+        nonFeeTransfers = new ArrayList<>();\n+        fileData = new ArrayList<>();\n+        contractResults = new ArrayList<>();\n+        liveHashes = new ArrayList<>();\n+        entityIds = new ArrayList<>();\n+        topicMessages = new ArrayList<>();\n     }\n \n     @Override\n     public void onEnd(RecordFile recordFile) {\n-        executeBatches();\n+        Stopwatch stopwatch = Stopwatch.createStarted();\n         try {\n-            connection.commit();\n-            recordFileRepository.save(recordFile);\n-            closeConnectionAndStatements();\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(e);\n-        }\n+            CompletableFuture.allOf(", "originalCommit": "c29400bbb5fea504df2c7f2008b6272e44bad81e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "a95540a5a09d83dd87aaef62e961f498efce463d", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/a95540a5a09d83dd87aaef62e961f498efce463d", "message": "Use COPY to insert into db\n\nSigned-off-by: Apekshit Sharma <apekshit.sharma@hedera.com>", "committedDate": "2020-07-13T19:41:55Z", "type": "commit"}, {"oid": "8b07d0ce3760791501dfc9a75fcb1a39dbd26047", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/8b07d0ce3760791501dfc9a75fcb1a39dbd26047", "message": "datagenerator changes\n\nSigned-off-by: Apekshit Sharma <apekshit.sharma@hedera.com>", "committedDate": "2020-07-13T19:41:55Z", "type": "commit"}, {"oid": "7dd6230c5afc5a08c3a79e68acdfe35df043913a", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/7dd6230c5afc5a08c3a79e68acdfe35df043913a", "message": "add tests\n\nSigned-off-by: Apekshit Sharma <apekshit.sharma@hedera.com>", "committedDate": "2020-07-13T19:41:55Z", "type": "commit"}, {"oid": "e7521106fef699404377acd5fb04a957e198df29", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/e7521106fef699404377acd5fb04a957e198df29", "message": "add tests\n\nSigned-off-by: Apekshit Sharma <apekshit.sharma@hedera.com>", "committedDate": "2020-07-13T19:41:55Z", "type": "commit"}, {"oid": "52b6b0703100d6c4ba002120bca31adfd5e83569", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/52b6b0703100d6c4ba002120bca31adfd5e83569", "message": "make datagerator work by java -jar\n\nSigned-off-by: Apekshit Sharma <apekshit.sharma@hedera.com>", "committedDate": "2020-07-13T19:41:56Z", "type": "commit"}, {"oid": "20db45a6a1ee6c939f260d5859592f550f30e4e5", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/20db45a6a1ee6c939f260d5859592f550f30e4e5", "message": "address review comments\n\nSigned-off-by: Apekshit Sharma <apekshit.sharma@hedera.com>", "committedDate": "2020-07-13T19:41:56Z", "type": "commit"}, {"oid": "2171ef777d7c65f65b3c1187181201e1e22fe343", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/2171ef777d7c65f65b3c1187181201e1e22fe343", "message": "Add batching and address comments\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>", "committedDate": "2020-07-13T19:41:56Z", "type": "commit"}, {"oid": "2171ef777d7c65f65b3c1187181201e1e22fe343", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/2171ef777d7c65f65b3c1187181201e1e22fe343", "message": "Add batching and address comments\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>", "committedDate": "2020-07-13T19:41:56Z", "type": "forcePushed"}, {"oid": "43b94cf46ad571edf444be0d76c695a334189532", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/43b94cf46ad571edf444be0d76c695a334189532", "message": "Update entity caching on parse\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>", "committedDate": "2020-07-14T03:01:31Z", "type": "commit"}, {"oid": "bf02cf1bcf2c664e642c3d187cebf38cc479739e", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/bf02cf1bcf2c664e642c3d187cebf38cc479739e", "message": "Updated seen entity caching logic and tests\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>", "committedDate": "2020-07-14T20:08:18Z", "type": "commit"}, {"oid": "c73c65377426b428869922be432c32bac41a7ee7", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/c73c65377426b428869922be432c32bac41a7ee7", "message": "Updated pgcopy write call\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>", "committedDate": "2020-07-14T21:22:26Z", "type": "commit"}, {"oid": "59b4c4aa2e41f1ed57eaac0e340e62362a2e4615", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/59b4c4aa2e41f1ed57eaac0e340e62362a2e4615", "message": "Fix pubsub dependency issues on tests\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>", "committedDate": "2020-07-15T04:58:41Z", "type": "commit"}, {"oid": "6a90ddf2e9c6a79ef3d8129e6b9836521c6544fb", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/6a90ddf2e9c6a79ef3d8129e6b9836521c6544fb", "message": "Merge branch 'master' into concurrent_inserts", "committedDate": "2020-07-16T18:33:55Z", "type": "commit"}, {"oid": "6bed7d988bcf20f5c40f2a4683fc3dca04bb873b", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/6bed7d988bcf20f5c40f2a4683fc3dca04bb873b", "message": "Resolve merge conflict with NPE and IO exceptions\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>", "committedDate": "2020-07-16T18:39:57Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA3Mjg0Ng==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r456072846", "bodyText": "You can't store this. It's the same effect as the last approach. If the connection goes down this copymanager will not reconnect. Just get it dynamically each time. The connection pool will make it efficient.", "author": "steven-sheehy", "createdAt": "2020-07-16T20:56:12Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/PgCopy.java", "diffHunk": "@@ -0,0 +1,115 @@\n+package com.hedera.mirror.importer.parser.record.entity.sql;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import com.fasterxml.jackson.databind.ObjectWriter;\n+import com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+import com.google.common.base.CaseFormat;\n+import com.google.common.base.Stopwatch;\n+import com.google.common.collect.Lists;\n+import io.micrometer.core.instrument.MeterRegistry;\n+import io.micrometer.core.instrument.Timer;\n+import java.io.IOException;\n+import java.io.StringReader;\n+import java.sql.SQLException;\n+import java.util.List;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+import javax.sql.DataSource;\n+import lombok.extern.log4j.Log4j2;\n+import org.postgresql.PGConnection;\n+import org.postgresql.copy.CopyManager;\n+\n+import com.hedera.mirror.importer.exception.ParserException;\n+\n+/**\n+ * Stateless writer to insert rows into Postgres table using COPY.\n+ *\n+ * @param <T> domain object\n+ */\n+@Log4j2\n+public class PgCopy<T> {\n+    private final DataSource dataSource;\n+    private final String tableName;\n+    private final String columnsCsv;\n+    private final ObjectWriter writer;\n+    private final Timer buildCsvDurationMetric;\n+    private final Timer copyDurationMetric;\n+    private CopyManager copyManager;", "originalCommit": "6bed7d988bcf20f5c40f2a4683fc3dca04bb873b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY2MjgwMg==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r457662802", "bodyText": "Will update MirrorImporterConfiguration to create the CopyManager bean for dynamic injection into SqlEntityListener which creates PgCopy objects", "author": "Nana-EC", "createdAt": "2020-07-20T20:09:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA3Mjg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY3NjEwMQ==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r457676101", "bodyText": "How will that address my concern? You need to create the copymanager every time inside PgCopy.copy()", "author": "steven-sheehy", "createdAt": "2020-07-20T20:35:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA3Mjg0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzc5MDA3Ng==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r457790076", "bodyText": "Moved copy manager creation to PgCopy.copy()", "author": "Nana-EC", "createdAt": "2020-07-21T01:56:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA3Mjg0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA3OTExNA==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r456079114", "bodyText": "You just changed it from base64 encoded to hex. gRPC API is expecting base64", "author": "steven-sheehy", "createdAt": "2020-07-16T21:08:49Z", "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/domain/TopicMessageTest.java", "diffHunk": "@@ -50,10 +50,10 @@ void toJson() throws Exception {\n                 \"\\\"chunk_num\\\":1,\" +\n                 \"\\\"chunk_total\\\":2,\" +\n                 \"\\\"consensus_timestamp\\\":1594401417000000000,\" +\n-                \"\\\"message\\\":\\\"AQID\\\",\" +\n+                \"\\\"message\\\":\\\"\\\\\\\\x010203\\\",\" +", "originalCommit": "6bed7d988bcf20f5c40f2a4683fc3dca04bb873b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgyMjc1Ng==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r457822756", "bodyText": "Reverted", "author": "Nana-EC", "createdAt": "2020-07-21T04:02:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA3OTExNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA4MDU2NQ==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r456080565", "bodyText": "We shouldn't have two stopwatches to record csv generation. Should move csv metric into buildCsv", "author": "steven-sheehy", "createdAt": "2020-07-16T21:11:45Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/PgCopy.java", "diffHunk": "@@ -0,0 +1,115 @@\n+package com.hedera.mirror.importer.parser.record.entity.sql;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import com.fasterxml.jackson.databind.ObjectWriter;\n+import com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+import com.google.common.base.CaseFormat;\n+import com.google.common.base.Stopwatch;\n+import com.google.common.collect.Lists;\n+import io.micrometer.core.instrument.MeterRegistry;\n+import io.micrometer.core.instrument.Timer;\n+import java.io.IOException;\n+import java.io.StringReader;\n+import java.sql.SQLException;\n+import java.util.List;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+import javax.sql.DataSource;\n+import lombok.extern.log4j.Log4j2;\n+import org.postgresql.PGConnection;\n+import org.postgresql.copy.CopyManager;\n+\n+import com.hedera.mirror.importer.exception.ParserException;\n+\n+/**\n+ * Stateless writer to insert rows into Postgres table using COPY.\n+ *\n+ * @param <T> domain object\n+ */\n+@Log4j2\n+public class PgCopy<T> {\n+    private final DataSource dataSource;\n+    private final String tableName;\n+    private final String columnsCsv;\n+    private final ObjectWriter writer;\n+    private final Timer buildCsvDurationMetric;\n+    private final Timer copyDurationMetric;\n+    private CopyManager copyManager;\n+\n+    public PgCopy(DataSource dataSource, Class<T> tClass, MeterRegistry meterRegistry) {\n+        this.dataSource = dataSource;\n+        tableName = CaseFormat.UPPER_CAMEL.to(CaseFormat.LOWER_UNDERSCORE, tClass.getSimpleName());\n+        var mapper = new CsvMapper();\n+        var schema = mapper.schemaFor(tClass);\n+        writer = mapper.writer(schema);\n+        columnsCsv = Lists.newArrayList(schema.iterator()).stream()\n+                .map(CsvSchema.Column::getName)\n+                .map(name -> CaseFormat.UPPER_CAMEL.to(CaseFormat.LOWER_UNDERSCORE, name))\n+                .collect(Collectors.joining(\", \"));\n+        buildCsvDurationMetric = Timer.builder(\"hedera.mirror.importer.parser.record.entity.sql.pgcopy.csv\")\n+                .description(\"Time to build csv string\")\n+                .tag(\"table\", tableName)\n+                .register(meterRegistry);\n+        copyDurationMetric = Timer.builder(\"hedera.mirror.importer.parser.record.entity.sql.pgcopy.copy\")\n+                .description(\"Time to insert transactions into table\")\n+                .tag(\"table\", tableName)\n+                .register(meterRegistry);\n+    }\n+\n+    public void copy(List<T> items) {\n+\n+        if (items == null || items.size() == 0) {\n+            return;\n+        }\n+        try {\n+            if (copyManager == null) {\n+                copyManager = dataSource.getConnection().unwrap(PGConnection.class).getCopyAPI();\n+            }\n+\n+            Stopwatch stopwatch = Stopwatch.createStarted();\n+            var csv = buildCsv(items);\n+            var csvDuration = stopwatch.elapsed();", "originalCommit": "6bed7d988bcf20f5c40f2a4683fc3dca04bb873b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY3MjE1MQ==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r457672151", "bodyText": "Good point. Will add an extra metric and capture build, insert and copy = build + insert", "author": "Nana-EC", "createdAt": "2020-07-20T20:27:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA4MDU2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEwMTgwMA==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r456101800", "bodyText": "batchCount please", "author": "steven-sheehy", "createdAt": "2020-07-16T21:57:51Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -47,375 +50,175 @@\n import com.hedera.mirror.importer.exception.DuplicateFileException;\n import com.hedera.mirror.importer.exception.ImporterException;\n import com.hedera.mirror.importer.exception.ParserException;\n-import com.hedera.mirror.importer.exception.ParserSQLException;\n import com.hedera.mirror.importer.parser.domain.StreamFileData;\n import com.hedera.mirror.importer.parser.record.RecordStreamFileListener;\n import com.hedera.mirror.importer.parser.record.entity.ConditionOnEntityRecordParser;\n import com.hedera.mirror.importer.parser.record.entity.EntityListener;\n+import com.hedera.mirror.importer.repository.EntityRepository;\n import com.hedera.mirror.importer.repository.RecordFileRepository;\n \n @Log4j2\n @Named\n-@RequiredArgsConstructor\n @ConditionOnEntityRecordParser\n-public class SqlEntityListener implements EntityListener, RecordStreamFileListener {\n-    private final SqlProperties properties;\n+public class SqlEntityListener implements EntityListener, RecordStreamFileListener, Closeable {\n     private final DataSource dataSource;\n+    private final ExecutorService executorService;\n     private final RecordFileRepository recordFileRepository;\n-    private long batch_count = 0;\n-    // Keeps track of entityIds seen in the current file being processed. This is for optimizing inserts into\n-    // t_entities table so that insertion of node and treasury ids are not tried for every transaction.\n-    private Collection<EntityId> entityIds;\n-    private PreparedStatement sqlInsertTransaction;\n-    private PreparedStatement sqlInsertEntityId;\n-    private PreparedStatement sqlInsertTransferList;\n-    private PreparedStatement sqlInsertNonFeeTransfers;\n-    private PreparedStatement sqlInsertFileData;\n-    private PreparedStatement sqlInsertContractResult;\n-    private PreparedStatement sqlInsertLiveHashes;\n-    private PreparedStatement sqlInsertTopicMessage;\n-    private Connection connection;\n+    private final EntityRepository entityRepository;\n+    private final SqlProperties sqlProperties;\n+    private final CacheManager cacheManager;\n+    private long batch_count;\n+\n+    // Keeps track of entityIds seen so far. This is for optimizing inserts into t_entities table so that insertion of\n+    // node and treasury ids are not tried for every transaction.\n+    private final Collection<EntityId> seenEntityIds = new HashSet<>();\n+    // init connections, schemas, writers, etc once per process\n+    private final PgCopy<Transaction> transactionPgCopy;\n+    private final PgCopy<CryptoTransfer> cryptoTransferPgCopy;\n+    private final PgCopy<NonFeeTransfer> nonFeeTransferPgCopy;\n+    private final PgCopy<FileData> fileDataPgCopy;\n+    private final PgCopy<ContractResult> contractResultPgCopy;\n+    private final PgCopy<LiveHash> liveHashPgCopy;\n+    private final PgCopy<TopicMessage> topicMessagePgCopy;\n+\n+    private final Timer insertDuration;\n+\n+    private List<Transaction> transactions;\n+    private List<CryptoTransfer> cryptoTransfers;\n+    private List<NonFeeTransfer> nonFeeTransfers;\n+    private List<FileData> fileData;\n+    private List<ContractResult> contractResults;\n+    private List<LiveHash> liveHashes;\n+    private List<TopicMessage> topicMessages;\n+    private List<EntityId> entityIds;\n+    private final Cache entityCache;\n+\n+    public SqlEntityListener(SqlProperties properties, DataSource dataSource,\n+                             RecordFileRepository recordFileRepository, EntityRepository entityRepository,\n+                             MeterRegistry meterRegistry, CacheManager cacheManager) {\n+        this.dataSource = dataSource;\n+        this.recordFileRepository = recordFileRepository;\n+        this.entityRepository = entityRepository;\n+        sqlProperties = properties;\n+        executorService = Executors.newFixedThreadPool(properties.getThreads());\n+        this.cacheManager = cacheManager;\n+\n+        transactionPgCopy = new PgCopy<>(dataSource, Transaction.class, meterRegistry);\n+        cryptoTransferPgCopy = new PgCopy<>(dataSource, CryptoTransfer.class, meterRegistry);\n+        nonFeeTransferPgCopy = new PgCopy<>(dataSource, NonFeeTransfer.class, meterRegistry);\n+        fileDataPgCopy = new PgCopy<>(dataSource, FileData.class, meterRegistry);\n+        contractResultPgCopy = new PgCopy<>(dataSource, ContractResult.class, meterRegistry);\n+        liveHashPgCopy = new PgCopy<>(dataSource, LiveHash.class, meterRegistry);\n+        topicMessagePgCopy = new PgCopy<>(dataSource, TopicMessage.class, meterRegistry);\n+\n+        insertDuration = Timer.builder(\"hedera.mirror.importer.parser.record.entity.sql.insert\")\n+                .description(\"Time to insert all entities into database\")\n+                .register(meterRegistry);\n+\n+        entityCache = cacheManager.getCache(\"seen_entities\");\n+    }\n \n     @Override\n     public void onStart(StreamFileData streamFileData) {\n         String fileName = streamFileData.getFilename();\n-        entityIds = new HashSet<>();\n         if (recordFileRepository.findByName(fileName).size() > 0) {\n             throw new DuplicateFileException(\"File already exists in the database: \" + fileName);\n         }\n-        try {\n-            initConnectionAndStatements();\n-        } catch (Exception e) {\n-            throw new ParserException(\"Error setting up connection and statements\", e);\n-        }\n+        transactions = new ArrayList<>();\n+        cryptoTransfers = new ArrayList<>();\n+        nonFeeTransfers = new ArrayList<>();\n+        fileData = new ArrayList<>();\n+        contractResults = new ArrayList<>();\n+        liveHashes = new ArrayList<>();\n+        entityIds = new ArrayList<>();\n+        topicMessages = new ArrayList<>();\n+        batch_count = 0;", "originalCommit": "6bed7d988bcf20f5c40f2a4683fc3dca04bb873b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEwMjMxMA==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r456102310", "bodyText": "This should be a HashSet with the other recommended changes", "author": "steven-sheehy", "createdAt": "2020-07-16T21:59:01Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -47,375 +50,175 @@\n import com.hedera.mirror.importer.exception.DuplicateFileException;\n import com.hedera.mirror.importer.exception.ImporterException;\n import com.hedera.mirror.importer.exception.ParserException;\n-import com.hedera.mirror.importer.exception.ParserSQLException;\n import com.hedera.mirror.importer.parser.domain.StreamFileData;\n import com.hedera.mirror.importer.parser.record.RecordStreamFileListener;\n import com.hedera.mirror.importer.parser.record.entity.ConditionOnEntityRecordParser;\n import com.hedera.mirror.importer.parser.record.entity.EntityListener;\n+import com.hedera.mirror.importer.repository.EntityRepository;\n import com.hedera.mirror.importer.repository.RecordFileRepository;\n \n @Log4j2\n @Named\n-@RequiredArgsConstructor\n @ConditionOnEntityRecordParser\n-public class SqlEntityListener implements EntityListener, RecordStreamFileListener {\n-    private final SqlProperties properties;\n+public class SqlEntityListener implements EntityListener, RecordStreamFileListener, Closeable {\n     private final DataSource dataSource;\n+    private final ExecutorService executorService;\n     private final RecordFileRepository recordFileRepository;\n-    private long batch_count = 0;\n-    // Keeps track of entityIds seen in the current file being processed. This is for optimizing inserts into\n-    // t_entities table so that insertion of node and treasury ids are not tried for every transaction.\n-    private Collection<EntityId> entityIds;\n-    private PreparedStatement sqlInsertTransaction;\n-    private PreparedStatement sqlInsertEntityId;\n-    private PreparedStatement sqlInsertTransferList;\n-    private PreparedStatement sqlInsertNonFeeTransfers;\n-    private PreparedStatement sqlInsertFileData;\n-    private PreparedStatement sqlInsertContractResult;\n-    private PreparedStatement sqlInsertLiveHashes;\n-    private PreparedStatement sqlInsertTopicMessage;\n-    private Connection connection;\n+    private final EntityRepository entityRepository;\n+    private final SqlProperties sqlProperties;\n+    private final CacheManager cacheManager;\n+    private long batch_count;\n+\n+    // Keeps track of entityIds seen so far. This is for optimizing inserts into t_entities table so that insertion of\n+    // node and treasury ids are not tried for every transaction.\n+    private final Collection<EntityId> seenEntityIds = new HashSet<>();\n+    // init connections, schemas, writers, etc once per process\n+    private final PgCopy<Transaction> transactionPgCopy;\n+    private final PgCopy<CryptoTransfer> cryptoTransferPgCopy;\n+    private final PgCopy<NonFeeTransfer> nonFeeTransferPgCopy;\n+    private final PgCopy<FileData> fileDataPgCopy;\n+    private final PgCopy<ContractResult> contractResultPgCopy;\n+    private final PgCopy<LiveHash> liveHashPgCopy;\n+    private final PgCopy<TopicMessage> topicMessagePgCopy;\n+\n+    private final Timer insertDuration;\n+\n+    private List<Transaction> transactions;\n+    private List<CryptoTransfer> cryptoTransfers;\n+    private List<NonFeeTransfer> nonFeeTransfers;\n+    private List<FileData> fileData;\n+    private List<ContractResult> contractResults;\n+    private List<LiveHash> liveHashes;\n+    private List<TopicMessage> topicMessages;\n+    private List<EntityId> entityIds;\n+    private final Cache entityCache;\n+\n+    public SqlEntityListener(SqlProperties properties, DataSource dataSource,\n+                             RecordFileRepository recordFileRepository, EntityRepository entityRepository,\n+                             MeterRegistry meterRegistry, CacheManager cacheManager) {\n+        this.dataSource = dataSource;\n+        this.recordFileRepository = recordFileRepository;\n+        this.entityRepository = entityRepository;\n+        sqlProperties = properties;\n+        executorService = Executors.newFixedThreadPool(properties.getThreads());\n+        this.cacheManager = cacheManager;\n+\n+        transactionPgCopy = new PgCopy<>(dataSource, Transaction.class, meterRegistry);\n+        cryptoTransferPgCopy = new PgCopy<>(dataSource, CryptoTransfer.class, meterRegistry);\n+        nonFeeTransferPgCopy = new PgCopy<>(dataSource, NonFeeTransfer.class, meterRegistry);\n+        fileDataPgCopy = new PgCopy<>(dataSource, FileData.class, meterRegistry);\n+        contractResultPgCopy = new PgCopy<>(dataSource, ContractResult.class, meterRegistry);\n+        liveHashPgCopy = new PgCopy<>(dataSource, LiveHash.class, meterRegistry);\n+        topicMessagePgCopy = new PgCopy<>(dataSource, TopicMessage.class, meterRegistry);\n+\n+        insertDuration = Timer.builder(\"hedera.mirror.importer.parser.record.entity.sql.insert\")\n+                .description(\"Time to insert all entities into database\")\n+                .register(meterRegistry);\n+\n+        entityCache = cacheManager.getCache(\"seen_entities\");\n+    }\n \n     @Override\n     public void onStart(StreamFileData streamFileData) {\n         String fileName = streamFileData.getFilename();\n-        entityIds = new HashSet<>();\n         if (recordFileRepository.findByName(fileName).size() > 0) {\n             throw new DuplicateFileException(\"File already exists in the database: \" + fileName);\n         }\n-        try {\n-            initConnectionAndStatements();\n-        } catch (Exception e) {\n-            throw new ParserException(\"Error setting up connection and statements\", e);\n-        }\n+        transactions = new ArrayList<>();\n+        cryptoTransfers = new ArrayList<>();\n+        nonFeeTransfers = new ArrayList<>();\n+        fileData = new ArrayList<>();\n+        contractResults = new ArrayList<>();\n+        liveHashes = new ArrayList<>();\n+        entityIds = new ArrayList<>();", "originalCommit": "6bed7d988bcf20f5c40f2a4683fc3dca04bb873b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEwMjU4MA==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r456102580", "bodyText": "Shouldn't log error twice", "author": "steven-sheehy", "createdAt": "2020-07-16T21:59:36Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -47,375 +50,175 @@\n import com.hedera.mirror.importer.exception.DuplicateFileException;\n import com.hedera.mirror.importer.exception.ImporterException;\n import com.hedera.mirror.importer.exception.ParserException;\n-import com.hedera.mirror.importer.exception.ParserSQLException;\n import com.hedera.mirror.importer.parser.domain.StreamFileData;\n import com.hedera.mirror.importer.parser.record.RecordStreamFileListener;\n import com.hedera.mirror.importer.parser.record.entity.ConditionOnEntityRecordParser;\n import com.hedera.mirror.importer.parser.record.entity.EntityListener;\n+import com.hedera.mirror.importer.repository.EntityRepository;\n import com.hedera.mirror.importer.repository.RecordFileRepository;\n \n @Log4j2\n @Named\n-@RequiredArgsConstructor\n @ConditionOnEntityRecordParser\n-public class SqlEntityListener implements EntityListener, RecordStreamFileListener {\n-    private final SqlProperties properties;\n+public class SqlEntityListener implements EntityListener, RecordStreamFileListener, Closeable {\n     private final DataSource dataSource;\n+    private final ExecutorService executorService;\n     private final RecordFileRepository recordFileRepository;\n-    private long batch_count = 0;\n-    // Keeps track of entityIds seen in the current file being processed. This is for optimizing inserts into\n-    // t_entities table so that insertion of node and treasury ids are not tried for every transaction.\n-    private Collection<EntityId> entityIds;\n-    private PreparedStatement sqlInsertTransaction;\n-    private PreparedStatement sqlInsertEntityId;\n-    private PreparedStatement sqlInsertTransferList;\n-    private PreparedStatement sqlInsertNonFeeTransfers;\n-    private PreparedStatement sqlInsertFileData;\n-    private PreparedStatement sqlInsertContractResult;\n-    private PreparedStatement sqlInsertLiveHashes;\n-    private PreparedStatement sqlInsertTopicMessage;\n-    private Connection connection;\n+    private final EntityRepository entityRepository;\n+    private final SqlProperties sqlProperties;\n+    private final CacheManager cacheManager;\n+    private long batch_count;\n+\n+    // Keeps track of entityIds seen so far. This is for optimizing inserts into t_entities table so that insertion of\n+    // node and treasury ids are not tried for every transaction.\n+    private final Collection<EntityId> seenEntityIds = new HashSet<>();\n+    // init connections, schemas, writers, etc once per process\n+    private final PgCopy<Transaction> transactionPgCopy;\n+    private final PgCopy<CryptoTransfer> cryptoTransferPgCopy;\n+    private final PgCopy<NonFeeTransfer> nonFeeTransferPgCopy;\n+    private final PgCopy<FileData> fileDataPgCopy;\n+    private final PgCopy<ContractResult> contractResultPgCopy;\n+    private final PgCopy<LiveHash> liveHashPgCopy;\n+    private final PgCopy<TopicMessage> topicMessagePgCopy;\n+\n+    private final Timer insertDuration;\n+\n+    private List<Transaction> transactions;\n+    private List<CryptoTransfer> cryptoTransfers;\n+    private List<NonFeeTransfer> nonFeeTransfers;\n+    private List<FileData> fileData;\n+    private List<ContractResult> contractResults;\n+    private List<LiveHash> liveHashes;\n+    private List<TopicMessage> topicMessages;\n+    private List<EntityId> entityIds;\n+    private final Cache entityCache;\n+\n+    public SqlEntityListener(SqlProperties properties, DataSource dataSource,\n+                             RecordFileRepository recordFileRepository, EntityRepository entityRepository,\n+                             MeterRegistry meterRegistry, CacheManager cacheManager) {\n+        this.dataSource = dataSource;\n+        this.recordFileRepository = recordFileRepository;\n+        this.entityRepository = entityRepository;\n+        sqlProperties = properties;\n+        executorService = Executors.newFixedThreadPool(properties.getThreads());\n+        this.cacheManager = cacheManager;\n+\n+        transactionPgCopy = new PgCopy<>(dataSource, Transaction.class, meterRegistry);\n+        cryptoTransferPgCopy = new PgCopy<>(dataSource, CryptoTransfer.class, meterRegistry);\n+        nonFeeTransferPgCopy = new PgCopy<>(dataSource, NonFeeTransfer.class, meterRegistry);\n+        fileDataPgCopy = new PgCopy<>(dataSource, FileData.class, meterRegistry);\n+        contractResultPgCopy = new PgCopy<>(dataSource, ContractResult.class, meterRegistry);\n+        liveHashPgCopy = new PgCopy<>(dataSource, LiveHash.class, meterRegistry);\n+        topicMessagePgCopy = new PgCopy<>(dataSource, TopicMessage.class, meterRegistry);\n+\n+        insertDuration = Timer.builder(\"hedera.mirror.importer.parser.record.entity.sql.insert\")\n+                .description(\"Time to insert all entities into database\")\n+                .register(meterRegistry);\n+\n+        entityCache = cacheManager.getCache(\"seen_entities\");\n+    }\n \n     @Override\n     public void onStart(StreamFileData streamFileData) {\n         String fileName = streamFileData.getFilename();\n-        entityIds = new HashSet<>();\n         if (recordFileRepository.findByName(fileName).size() > 0) {\n             throw new DuplicateFileException(\"File already exists in the database: \" + fileName);\n         }\n-        try {\n-            initConnectionAndStatements();\n-        } catch (Exception e) {\n-            throw new ParserException(\"Error setting up connection and statements\", e);\n-        }\n+        transactions = new ArrayList<>();\n+        cryptoTransfers = new ArrayList<>();\n+        nonFeeTransfers = new ArrayList<>();\n+        fileData = new ArrayList<>();\n+        contractResults = new ArrayList<>();\n+        liveHashes = new ArrayList<>();\n+        entityIds = new ArrayList<>();\n+        topicMessages = new ArrayList<>();\n+        batch_count = 0;\n     }\n \n     @Override\n     public void onEnd(RecordFile recordFile) {\n         executeBatches();\n-        try {\n-            connection.commit();\n-            recordFileRepository.save(recordFile);\n-            closeConnectionAndStatements();\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(e);\n-        }\n+        recordFileRepository.save(recordFile);\n     }\n \n     @Override\n     public void onError() {\n-        try {\n-            if (connection != null) {\n-                connection.rollback();\n-                closeConnectionAndStatements();\n-            }\n-        } catch (SQLException e) {\n-            log.error(\"Exception while rolling transaction back\", e);\n-        }\n+        // no error handling\n     }\n \n-    private void initConnectionAndStatements() throws ParserSQLException {\n-        try {\n-            connection = dataSource.getConnection();\n-            connection.setAutoCommit(false); // do not auto-commit\n-            connection.setClientInfo(\"ApplicationName\", getClass().getSimpleName());\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(\"Error setting up connection to database\", e);\n-        }\n-        try {\n-            sqlInsertTransaction = connection.prepareStatement(\"INSERT INTO transaction\"\n-                    + \" (node_account_id, memo, valid_start_ns, type, payer_account_id, result, \" +\n-                    \"consensus_ns, entity_id, charged_tx_fee, initial_balance, valid_duration_seconds, max_fee, \" +\n-                    \"transaction_hash, transaction_bytes)\"\n-                    + \" VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\");\n-\n-            sqlInsertEntityId = connection.prepareStatement(\"INSERT INTO t_entities \" +\n-                    \"(id, entity_shard, entity_realm, entity_num, fk_entity_type_id) \" +\n-                    \"VALUES (?, ?, ?, ?, ?) \" +\n-                    \"ON CONFLICT DO NOTHING\");\n-\n-            sqlInsertTransferList = connection.prepareStatement(\"INSERT INTO crypto_transfer\"\n-                    + \" (consensus_timestamp, amount, entity_id)\"\n-                    + \" VALUES (?, ?, ?)\");\n-\n-            sqlInsertNonFeeTransfers = connection.prepareStatement(\"insert into non_fee_transfer\"\n-                    + \" (consensus_timestamp, amount, entity_id)\"\n-                    + \" values (?, ?, ?)\");\n-\n-            sqlInsertFileData = connection.prepareStatement(\"INSERT INTO t_file_data\"\n-                    + \" (consensus_timestamp, file_data)\"\n-                    + \" VALUES (?, ?)\");\n-\n-            sqlInsertContractResult = connection.prepareStatement(\"INSERT INTO t_contract_result\"\n-                    + \" (consensus_timestamp, function_params, gas_supplied, call_result, gas_used)\"\n-                    + \" VALUES (?, ?, ?, ?, ?)\");\n-\n-            sqlInsertLiveHashes = connection.prepareStatement(\"INSERT INTO t_livehashes\"\n-                    + \" (consensus_timestamp, livehash)\"\n-                    + \" VALUES (?, ?)\");\n-\n-            sqlInsertTopicMessage = connection.prepareStatement(\"insert into topic_message\"\n-                    + \" (consensus_timestamp, realm_num, topic_num, message, running_hash, sequence_number\" +\n-                    \", running_hash_version, chunk_num, chunk_total, payer_account_id, valid_start_timestamp)\"\n-                    + \" values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\");\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(\"Unable to prepare SQL statements\", e);\n-        }\n-    }\n-\n-    private void closeConnectionAndStatements() {\n-        try {\n-            sqlInsertTransaction.close();\n-            sqlInsertEntityId.close();\n-            sqlInsertTransferList.close();\n-            sqlInsertNonFeeTransfers.close();\n-            sqlInsertFileData.close();\n-            sqlInsertContractResult.close();\n-            sqlInsertLiveHashes.close();\n-            sqlInsertTopicMessage.close();\n-\n-            connection.close();\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(\"Error closing connection\", e);\n-        }\n+    @Override\n+    public void close() {\n+        executorService.shutdown();\n     }\n \n     private void executeBatches() {\n-        try {\n-            var transactions = executeBatch(sqlInsertTransaction);\n-            var entityIds = executeBatch(sqlInsertEntityId);\n-            var transferLists = executeBatch(sqlInsertTransferList);\n-            var nonFeeTransfers = executeBatch(sqlInsertNonFeeTransfers);\n-            var fileData = executeBatch(sqlInsertFileData);\n-            var contractResult = executeBatch(sqlInsertContractResult);\n-            var liveHashes = executeBatch(sqlInsertLiveHashes);\n-            var topicMessages = executeBatch(sqlInsertTopicMessage);\n-            log.info(\"Inserted transactions: {}, entities: {}, transfer lists: {}, files: {}, contracts: {}, \" +\n-                            \"claims: {}, topic messages: {}, non-fee transfers: {}\",\n-                    transactions, entityIds, transferLists, fileData, contractResult, liveHashes, topicMessages,\n-                    nonFeeTransfers);\n-        } catch (SQLException e) {\n-            log.error(\"Error committing sql insert batch \", e);\n-            throw new ParserSQLException(e);\n-        }\n-        batch_count = 0;\n-    }\n-\n-    private ExecuteBatchResult executeBatch(PreparedStatement ps) throws SQLException {\n         Stopwatch stopwatch = Stopwatch.createStarted();\n-        var executeResult = ps.executeBatch();\n-        return new ExecuteBatchResult(executeResult.length, stopwatch.elapsed(TimeUnit.MILLISECONDS));\n+        try {\n+            CompletableFuture.allOf(\n+                    CompletableFuture.runAsync(() -> transactionPgCopy.copy(transactions), executorService),\n+                    CompletableFuture.runAsync(() -> cryptoTransferPgCopy.copy(cryptoTransfers), executorService),\n+                    CompletableFuture.runAsync(() -> nonFeeTransferPgCopy.copy(nonFeeTransfers), executorService),\n+                    CompletableFuture.runAsync(() -> fileDataPgCopy.copy(fileData), executorService),\n+                    CompletableFuture.runAsync(() -> contractResultPgCopy.copy(contractResults), executorService),\n+                    CompletableFuture.runAsync(() -> liveHashPgCopy.copy(liveHashes), executorService),\n+                    CompletableFuture.runAsync(() -> topicMessagePgCopy.copy(topicMessages), executorService),\n+                    CompletableFuture\n+                            .runAsync(() -> entityIds.forEach(entityRepository::insertEntityId), executorService)\n+            ).get();\n+        } catch (InterruptedException | ExecutionException e) {\n+            log.error(e);", "originalCommit": "6bed7d988bcf20f5c40f2a4683fc3dca04bb873b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcyMzE3NA==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r457723174", "bodyText": "Removing log, left throw", "author": "Nana-EC", "createdAt": "2020-07-20T22:17:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEwMjU4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEwNzIzOQ==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r456107239", "bodyText": "I don't think this logic is right. If the database is down it will get added to the cache before the insert and then never get inserted unless the app is restarted. This code should simply entityIds.add(entityId). The cache update should be in executeBatch", "author": "steven-sheehy", "createdAt": "2020-07-16T22:10:51Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -47,375 +50,175 @@\n import com.hedera.mirror.importer.exception.DuplicateFileException;\n import com.hedera.mirror.importer.exception.ImporterException;\n import com.hedera.mirror.importer.exception.ParserException;\n-import com.hedera.mirror.importer.exception.ParserSQLException;\n import com.hedera.mirror.importer.parser.domain.StreamFileData;\n import com.hedera.mirror.importer.parser.record.RecordStreamFileListener;\n import com.hedera.mirror.importer.parser.record.entity.ConditionOnEntityRecordParser;\n import com.hedera.mirror.importer.parser.record.entity.EntityListener;\n+import com.hedera.mirror.importer.repository.EntityRepository;\n import com.hedera.mirror.importer.repository.RecordFileRepository;\n \n @Log4j2\n @Named\n-@RequiredArgsConstructor\n @ConditionOnEntityRecordParser\n-public class SqlEntityListener implements EntityListener, RecordStreamFileListener {\n-    private final SqlProperties properties;\n+public class SqlEntityListener implements EntityListener, RecordStreamFileListener, Closeable {\n     private final DataSource dataSource;\n+    private final ExecutorService executorService;\n     private final RecordFileRepository recordFileRepository;\n-    private long batch_count = 0;\n-    // Keeps track of entityIds seen in the current file being processed. This is for optimizing inserts into\n-    // t_entities table so that insertion of node and treasury ids are not tried for every transaction.\n-    private Collection<EntityId> entityIds;\n-    private PreparedStatement sqlInsertTransaction;\n-    private PreparedStatement sqlInsertEntityId;\n-    private PreparedStatement sqlInsertTransferList;\n-    private PreparedStatement sqlInsertNonFeeTransfers;\n-    private PreparedStatement sqlInsertFileData;\n-    private PreparedStatement sqlInsertContractResult;\n-    private PreparedStatement sqlInsertLiveHashes;\n-    private PreparedStatement sqlInsertTopicMessage;\n-    private Connection connection;\n+    private final EntityRepository entityRepository;\n+    private final SqlProperties sqlProperties;\n+    private final CacheManager cacheManager;\n+    private long batch_count;\n+\n+    // Keeps track of entityIds seen so far. This is for optimizing inserts into t_entities table so that insertion of\n+    // node and treasury ids are not tried for every transaction.\n+    private final Collection<EntityId> seenEntityIds = new HashSet<>();\n+    // init connections, schemas, writers, etc once per process\n+    private final PgCopy<Transaction> transactionPgCopy;\n+    private final PgCopy<CryptoTransfer> cryptoTransferPgCopy;\n+    private final PgCopy<NonFeeTransfer> nonFeeTransferPgCopy;\n+    private final PgCopy<FileData> fileDataPgCopy;\n+    private final PgCopy<ContractResult> contractResultPgCopy;\n+    private final PgCopy<LiveHash> liveHashPgCopy;\n+    private final PgCopy<TopicMessage> topicMessagePgCopy;\n+\n+    private final Timer insertDuration;\n+\n+    private List<Transaction> transactions;\n+    private List<CryptoTransfer> cryptoTransfers;\n+    private List<NonFeeTransfer> nonFeeTransfers;\n+    private List<FileData> fileData;\n+    private List<ContractResult> contractResults;\n+    private List<LiveHash> liveHashes;\n+    private List<TopicMessage> topicMessages;\n+    private List<EntityId> entityIds;\n+    private final Cache entityCache;\n+\n+    public SqlEntityListener(SqlProperties properties, DataSource dataSource,\n+                             RecordFileRepository recordFileRepository, EntityRepository entityRepository,\n+                             MeterRegistry meterRegistry, CacheManager cacheManager) {\n+        this.dataSource = dataSource;\n+        this.recordFileRepository = recordFileRepository;\n+        this.entityRepository = entityRepository;\n+        sqlProperties = properties;\n+        executorService = Executors.newFixedThreadPool(properties.getThreads());\n+        this.cacheManager = cacheManager;\n+\n+        transactionPgCopy = new PgCopy<>(dataSource, Transaction.class, meterRegistry);\n+        cryptoTransferPgCopy = new PgCopy<>(dataSource, CryptoTransfer.class, meterRegistry);\n+        nonFeeTransferPgCopy = new PgCopy<>(dataSource, NonFeeTransfer.class, meterRegistry);\n+        fileDataPgCopy = new PgCopy<>(dataSource, FileData.class, meterRegistry);\n+        contractResultPgCopy = new PgCopy<>(dataSource, ContractResult.class, meterRegistry);\n+        liveHashPgCopy = new PgCopy<>(dataSource, LiveHash.class, meterRegistry);\n+        topicMessagePgCopy = new PgCopy<>(dataSource, TopicMessage.class, meterRegistry);\n+\n+        insertDuration = Timer.builder(\"hedera.mirror.importer.parser.record.entity.sql.insert\")\n+                .description(\"Time to insert all entities into database\")\n+                .register(meterRegistry);\n+\n+        entityCache = cacheManager.getCache(\"seen_entities\");\n+    }\n \n     @Override\n     public void onStart(StreamFileData streamFileData) {\n         String fileName = streamFileData.getFilename();\n-        entityIds = new HashSet<>();\n         if (recordFileRepository.findByName(fileName).size() > 0) {\n             throw new DuplicateFileException(\"File already exists in the database: \" + fileName);\n         }\n-        try {\n-            initConnectionAndStatements();\n-        } catch (Exception e) {\n-            throw new ParserException(\"Error setting up connection and statements\", e);\n-        }\n+        transactions = new ArrayList<>();\n+        cryptoTransfers = new ArrayList<>();\n+        nonFeeTransfers = new ArrayList<>();\n+        fileData = new ArrayList<>();\n+        contractResults = new ArrayList<>();\n+        liveHashes = new ArrayList<>();\n+        entityIds = new ArrayList<>();\n+        topicMessages = new ArrayList<>();\n+        batch_count = 0;\n     }\n \n     @Override\n     public void onEnd(RecordFile recordFile) {\n         executeBatches();\n-        try {\n-            connection.commit();\n-            recordFileRepository.save(recordFile);\n-            closeConnectionAndStatements();\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(e);\n-        }\n+        recordFileRepository.save(recordFile);\n     }\n \n     @Override\n     public void onError() {\n-        try {\n-            if (connection != null) {\n-                connection.rollback();\n-                closeConnectionAndStatements();\n-            }\n-        } catch (SQLException e) {\n-            log.error(\"Exception while rolling transaction back\", e);\n-        }\n+        // no error handling\n     }\n \n-    private void initConnectionAndStatements() throws ParserSQLException {\n-        try {\n-            connection = dataSource.getConnection();\n-            connection.setAutoCommit(false); // do not auto-commit\n-            connection.setClientInfo(\"ApplicationName\", getClass().getSimpleName());\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(\"Error setting up connection to database\", e);\n-        }\n-        try {\n-            sqlInsertTransaction = connection.prepareStatement(\"INSERT INTO transaction\"\n-                    + \" (node_account_id, memo, valid_start_ns, type, payer_account_id, result, \" +\n-                    \"consensus_ns, entity_id, charged_tx_fee, initial_balance, valid_duration_seconds, max_fee, \" +\n-                    \"transaction_hash, transaction_bytes)\"\n-                    + \" VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\");\n-\n-            sqlInsertEntityId = connection.prepareStatement(\"INSERT INTO t_entities \" +\n-                    \"(id, entity_shard, entity_realm, entity_num, fk_entity_type_id) \" +\n-                    \"VALUES (?, ?, ?, ?, ?) \" +\n-                    \"ON CONFLICT DO NOTHING\");\n-\n-            sqlInsertTransferList = connection.prepareStatement(\"INSERT INTO crypto_transfer\"\n-                    + \" (consensus_timestamp, amount, entity_id)\"\n-                    + \" VALUES (?, ?, ?)\");\n-\n-            sqlInsertNonFeeTransfers = connection.prepareStatement(\"insert into non_fee_transfer\"\n-                    + \" (consensus_timestamp, amount, entity_id)\"\n-                    + \" values (?, ?, ?)\");\n-\n-            sqlInsertFileData = connection.prepareStatement(\"INSERT INTO t_file_data\"\n-                    + \" (consensus_timestamp, file_data)\"\n-                    + \" VALUES (?, ?)\");\n-\n-            sqlInsertContractResult = connection.prepareStatement(\"INSERT INTO t_contract_result\"\n-                    + \" (consensus_timestamp, function_params, gas_supplied, call_result, gas_used)\"\n-                    + \" VALUES (?, ?, ?, ?, ?)\");\n-\n-            sqlInsertLiveHashes = connection.prepareStatement(\"INSERT INTO t_livehashes\"\n-                    + \" (consensus_timestamp, livehash)\"\n-                    + \" VALUES (?, ?)\");\n-\n-            sqlInsertTopicMessage = connection.prepareStatement(\"insert into topic_message\"\n-                    + \" (consensus_timestamp, realm_num, topic_num, message, running_hash, sequence_number\" +\n-                    \", running_hash_version, chunk_num, chunk_total, payer_account_id, valid_start_timestamp)\"\n-                    + \" values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\");\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(\"Unable to prepare SQL statements\", e);\n-        }\n-    }\n-\n-    private void closeConnectionAndStatements() {\n-        try {\n-            sqlInsertTransaction.close();\n-            sqlInsertEntityId.close();\n-            sqlInsertTransferList.close();\n-            sqlInsertNonFeeTransfers.close();\n-            sqlInsertFileData.close();\n-            sqlInsertContractResult.close();\n-            sqlInsertLiveHashes.close();\n-            sqlInsertTopicMessage.close();\n-\n-            connection.close();\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(\"Error closing connection\", e);\n-        }\n+    @Override\n+    public void close() {\n+        executorService.shutdown();\n     }\n \n     private void executeBatches() {\n-        try {\n-            var transactions = executeBatch(sqlInsertTransaction);\n-            var entityIds = executeBatch(sqlInsertEntityId);\n-            var transferLists = executeBatch(sqlInsertTransferList);\n-            var nonFeeTransfers = executeBatch(sqlInsertNonFeeTransfers);\n-            var fileData = executeBatch(sqlInsertFileData);\n-            var contractResult = executeBatch(sqlInsertContractResult);\n-            var liveHashes = executeBatch(sqlInsertLiveHashes);\n-            var topicMessages = executeBatch(sqlInsertTopicMessage);\n-            log.info(\"Inserted transactions: {}, entities: {}, transfer lists: {}, files: {}, contracts: {}, \" +\n-                            \"claims: {}, topic messages: {}, non-fee transfers: {}\",\n-                    transactions, entityIds, transferLists, fileData, contractResult, liveHashes, topicMessages,\n-                    nonFeeTransfers);\n-        } catch (SQLException e) {\n-            log.error(\"Error committing sql insert batch \", e);\n-            throw new ParserSQLException(e);\n-        }\n-        batch_count = 0;\n-    }\n-\n-    private ExecuteBatchResult executeBatch(PreparedStatement ps) throws SQLException {\n         Stopwatch stopwatch = Stopwatch.createStarted();\n-        var executeResult = ps.executeBatch();\n-        return new ExecuteBatchResult(executeResult.length, stopwatch.elapsed(TimeUnit.MILLISECONDS));\n+        try {\n+            CompletableFuture.allOf(\n+                    CompletableFuture.runAsync(() -> transactionPgCopy.copy(transactions), executorService),\n+                    CompletableFuture.runAsync(() -> cryptoTransferPgCopy.copy(cryptoTransfers), executorService),\n+                    CompletableFuture.runAsync(() -> nonFeeTransferPgCopy.copy(nonFeeTransfers), executorService),\n+                    CompletableFuture.runAsync(() -> fileDataPgCopy.copy(fileData), executorService),\n+                    CompletableFuture.runAsync(() -> contractResultPgCopy.copy(contractResults), executorService),\n+                    CompletableFuture.runAsync(() -> liveHashPgCopy.copy(liveHashes), executorService),\n+                    CompletableFuture.runAsync(() -> topicMessagePgCopy.copy(topicMessages), executorService),\n+                    CompletableFuture\n+                            .runAsync(() -> entityIds.forEach(entityRepository::insertEntityId), executorService)\n+            ).get();\n+        } catch (InterruptedException | ExecutionException e) {\n+            log.error(e);\n+            throw new ParserException(e);\n+        }\n+        insertDuration.record(stopwatch.elapsed());\n     }\n \n     @Override\n     public void onTransaction(Transaction transaction) throws ImporterException {\n-        try {\n-            // Temporary until we convert SQL statements to repository invocations\n-            if (transaction.getEntityId() != null) {\n-                sqlInsertTransaction.setLong(F_TRANSACTION.ENTITY_ID.ordinal(), transaction.getEntityId().getId());\n-            } else {\n-                sqlInsertTransaction.setObject(F_TRANSACTION.ENTITY_ID.ordinal(), null);\n-            }\n-            sqlInsertTransaction.setLong(F_TRANSACTION.NODE_ACCOUNT_ID.ordinal(),\n-                    transaction.getNodeAccountId().getId());\n-            sqlInsertTransaction.setBytes(F_TRANSACTION.MEMO.ordinal(), transaction.getMemo());\n-            sqlInsertTransaction.setLong(F_TRANSACTION.VALID_START_NS.ordinal(), transaction.getValidStartNs());\n-            sqlInsertTransaction.setInt(F_TRANSACTION.TYPE.ordinal(), transaction.getType());\n-            sqlInsertTransaction.setLong(F_TRANSACTION.VALID_DURATION_SECONDS.ordinal(),\n-                    transaction.getValidDurationSeconds());\n-            sqlInsertTransaction.setLong(F_TRANSACTION.PAYER_ACCOUNT_ID.ordinal(),\n-                    transaction.getPayerAccountId().getId());\n-            sqlInsertTransaction.setLong(F_TRANSACTION.RESULT.ordinal(), transaction.getResult());\n-            sqlInsertTransaction.setLong(F_TRANSACTION.CONSENSUS_NS.ordinal(), transaction.getConsensusNs());\n-            sqlInsertTransaction.setLong(F_TRANSACTION.CHARGED_TX_FEE.ordinal(), transaction.getChargedTxFee());\n-            sqlInsertTransaction.setLong(F_TRANSACTION.MAX_FEE.ordinal(), transaction.getMaxFee());\n-            sqlInsertTransaction.setBytes(F_TRANSACTION.TRANSACTION_HASH.ordinal(), transaction.getTransactionHash());\n-            sqlInsertTransaction.setBytes(F_TRANSACTION.TRANSACTION_BYTES.ordinal(), transaction.getTransactionBytes());\n-            sqlInsertTransaction.setLong(F_TRANSACTION.INITIAL_BALANCE.ordinal(), transaction.getInitialBalance());\n-            sqlInsertTransaction.addBatch();\n-\n-            if (batch_count == properties.getBatchSize() - 1) {\n-                // execute any remaining batches\n-                executeBatches();\n-            } else {\n-                batch_count += 1;\n-            }\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(e);\n+        transactions.add(transaction);\n+        if (batch_count == sqlProperties.getBatchSize() - 1) {\n+            // execute any remaining batches\n+            executeBatches();\n+        } else {\n+            batch_count += 1;\n         }\n     }\n \n     @Override\n     public void onEntityId(EntityId entityId) throws ImporterException {\n-        if (entityIds.contains(entityId)) {\n-            return;\n-        }\n-        try {\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ID.ordinal(), entityId.getId());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ENTITY_SHARD.ordinal(), entityId.getShardNum());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ENTITY_REALM.ordinal(), entityId.getRealmNum());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ENTITY_NUM.ordinal(), entityId.getEntityNum());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.TYPE.ordinal(), entityId.getType());\n-            sqlInsertEntityId.addBatch();\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(e);\n+        // add entities not found in cache to list of entities to be persisted\n+        if (entityCache.get(entityId.getId()) == null) {\n+            entityIds.add(entityId);\n+            entityCache.put(entityId.getId(), entityId);", "originalCommit": "6bed7d988bcf20f5c40f2a4683fc3dca04bb873b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjEwODE4Nw==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r456108187", "bodyText": "CompletableFuture.runAsync(() -> entityIds.forEach(e -> {\n  if (entityCache.get(entity.getId()) == null) {\n    entityRepository.insertEntityId(e);\n    entityCache.put(e.getId(), null);\n  }}, executorService);\nNote the null value since we don't need the full entityId to be stored in memory for the life of the app.", "author": "steven-sheehy", "createdAt": "2020-07-16T22:13:22Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -47,375 +50,175 @@\n import com.hedera.mirror.importer.exception.DuplicateFileException;\n import com.hedera.mirror.importer.exception.ImporterException;\n import com.hedera.mirror.importer.exception.ParserException;\n-import com.hedera.mirror.importer.exception.ParserSQLException;\n import com.hedera.mirror.importer.parser.domain.StreamFileData;\n import com.hedera.mirror.importer.parser.record.RecordStreamFileListener;\n import com.hedera.mirror.importer.parser.record.entity.ConditionOnEntityRecordParser;\n import com.hedera.mirror.importer.parser.record.entity.EntityListener;\n+import com.hedera.mirror.importer.repository.EntityRepository;\n import com.hedera.mirror.importer.repository.RecordFileRepository;\n \n @Log4j2\n @Named\n-@RequiredArgsConstructor\n @ConditionOnEntityRecordParser\n-public class SqlEntityListener implements EntityListener, RecordStreamFileListener {\n-    private final SqlProperties properties;\n+public class SqlEntityListener implements EntityListener, RecordStreamFileListener, Closeable {\n     private final DataSource dataSource;\n+    private final ExecutorService executorService;\n     private final RecordFileRepository recordFileRepository;\n-    private long batch_count = 0;\n-    // Keeps track of entityIds seen in the current file being processed. This is for optimizing inserts into\n-    // t_entities table so that insertion of node and treasury ids are not tried for every transaction.\n-    private Collection<EntityId> entityIds;\n-    private PreparedStatement sqlInsertTransaction;\n-    private PreparedStatement sqlInsertEntityId;\n-    private PreparedStatement sqlInsertTransferList;\n-    private PreparedStatement sqlInsertNonFeeTransfers;\n-    private PreparedStatement sqlInsertFileData;\n-    private PreparedStatement sqlInsertContractResult;\n-    private PreparedStatement sqlInsertLiveHashes;\n-    private PreparedStatement sqlInsertTopicMessage;\n-    private Connection connection;\n+    private final EntityRepository entityRepository;\n+    private final SqlProperties sqlProperties;\n+    private final CacheManager cacheManager;\n+    private long batch_count;\n+\n+    // Keeps track of entityIds seen so far. This is for optimizing inserts into t_entities table so that insertion of\n+    // node and treasury ids are not tried for every transaction.\n+    private final Collection<EntityId> seenEntityIds = new HashSet<>();\n+    // init connections, schemas, writers, etc once per process\n+    private final PgCopy<Transaction> transactionPgCopy;\n+    private final PgCopy<CryptoTransfer> cryptoTransferPgCopy;\n+    private final PgCopy<NonFeeTransfer> nonFeeTransferPgCopy;\n+    private final PgCopy<FileData> fileDataPgCopy;\n+    private final PgCopy<ContractResult> contractResultPgCopy;\n+    private final PgCopy<LiveHash> liveHashPgCopy;\n+    private final PgCopy<TopicMessage> topicMessagePgCopy;\n+\n+    private final Timer insertDuration;\n+\n+    private List<Transaction> transactions;\n+    private List<CryptoTransfer> cryptoTransfers;\n+    private List<NonFeeTransfer> nonFeeTransfers;\n+    private List<FileData> fileData;\n+    private List<ContractResult> contractResults;\n+    private List<LiveHash> liveHashes;\n+    private List<TopicMessage> topicMessages;\n+    private List<EntityId> entityIds;\n+    private final Cache entityCache;\n+\n+    public SqlEntityListener(SqlProperties properties, DataSource dataSource,\n+                             RecordFileRepository recordFileRepository, EntityRepository entityRepository,\n+                             MeterRegistry meterRegistry, CacheManager cacheManager) {\n+        this.dataSource = dataSource;\n+        this.recordFileRepository = recordFileRepository;\n+        this.entityRepository = entityRepository;\n+        sqlProperties = properties;\n+        executorService = Executors.newFixedThreadPool(properties.getThreads());\n+        this.cacheManager = cacheManager;\n+\n+        transactionPgCopy = new PgCopy<>(dataSource, Transaction.class, meterRegistry);\n+        cryptoTransferPgCopy = new PgCopy<>(dataSource, CryptoTransfer.class, meterRegistry);\n+        nonFeeTransferPgCopy = new PgCopy<>(dataSource, NonFeeTransfer.class, meterRegistry);\n+        fileDataPgCopy = new PgCopy<>(dataSource, FileData.class, meterRegistry);\n+        contractResultPgCopy = new PgCopy<>(dataSource, ContractResult.class, meterRegistry);\n+        liveHashPgCopy = new PgCopy<>(dataSource, LiveHash.class, meterRegistry);\n+        topicMessagePgCopy = new PgCopy<>(dataSource, TopicMessage.class, meterRegistry);\n+\n+        insertDuration = Timer.builder(\"hedera.mirror.importer.parser.record.entity.sql.insert\")\n+                .description(\"Time to insert all entities into database\")\n+                .register(meterRegistry);\n+\n+        entityCache = cacheManager.getCache(\"seen_entities\");\n+    }\n \n     @Override\n     public void onStart(StreamFileData streamFileData) {\n         String fileName = streamFileData.getFilename();\n-        entityIds = new HashSet<>();\n         if (recordFileRepository.findByName(fileName).size() > 0) {\n             throw new DuplicateFileException(\"File already exists in the database: \" + fileName);\n         }\n-        try {\n-            initConnectionAndStatements();\n-        } catch (Exception e) {\n-            throw new ParserException(\"Error setting up connection and statements\", e);\n-        }\n+        transactions = new ArrayList<>();\n+        cryptoTransfers = new ArrayList<>();\n+        nonFeeTransfers = new ArrayList<>();\n+        fileData = new ArrayList<>();\n+        contractResults = new ArrayList<>();\n+        liveHashes = new ArrayList<>();\n+        entityIds = new ArrayList<>();\n+        topicMessages = new ArrayList<>();\n+        batch_count = 0;\n     }\n \n     @Override\n     public void onEnd(RecordFile recordFile) {\n         executeBatches();\n-        try {\n-            connection.commit();\n-            recordFileRepository.save(recordFile);\n-            closeConnectionAndStatements();\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(e);\n-        }\n+        recordFileRepository.save(recordFile);\n     }\n \n     @Override\n     public void onError() {\n-        try {\n-            if (connection != null) {\n-                connection.rollback();\n-                closeConnectionAndStatements();\n-            }\n-        } catch (SQLException e) {\n-            log.error(\"Exception while rolling transaction back\", e);\n-        }\n+        // no error handling\n     }\n \n-    private void initConnectionAndStatements() throws ParserSQLException {\n-        try {\n-            connection = dataSource.getConnection();\n-            connection.setAutoCommit(false); // do not auto-commit\n-            connection.setClientInfo(\"ApplicationName\", getClass().getSimpleName());\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(\"Error setting up connection to database\", e);\n-        }\n-        try {\n-            sqlInsertTransaction = connection.prepareStatement(\"INSERT INTO transaction\"\n-                    + \" (node_account_id, memo, valid_start_ns, type, payer_account_id, result, \" +\n-                    \"consensus_ns, entity_id, charged_tx_fee, initial_balance, valid_duration_seconds, max_fee, \" +\n-                    \"transaction_hash, transaction_bytes)\"\n-                    + \" VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\");\n-\n-            sqlInsertEntityId = connection.prepareStatement(\"INSERT INTO t_entities \" +\n-                    \"(id, entity_shard, entity_realm, entity_num, fk_entity_type_id) \" +\n-                    \"VALUES (?, ?, ?, ?, ?) \" +\n-                    \"ON CONFLICT DO NOTHING\");\n-\n-            sqlInsertTransferList = connection.prepareStatement(\"INSERT INTO crypto_transfer\"\n-                    + \" (consensus_timestamp, amount, entity_id)\"\n-                    + \" VALUES (?, ?, ?)\");\n-\n-            sqlInsertNonFeeTransfers = connection.prepareStatement(\"insert into non_fee_transfer\"\n-                    + \" (consensus_timestamp, amount, entity_id)\"\n-                    + \" values (?, ?, ?)\");\n-\n-            sqlInsertFileData = connection.prepareStatement(\"INSERT INTO t_file_data\"\n-                    + \" (consensus_timestamp, file_data)\"\n-                    + \" VALUES (?, ?)\");\n-\n-            sqlInsertContractResult = connection.prepareStatement(\"INSERT INTO t_contract_result\"\n-                    + \" (consensus_timestamp, function_params, gas_supplied, call_result, gas_used)\"\n-                    + \" VALUES (?, ?, ?, ?, ?)\");\n-\n-            sqlInsertLiveHashes = connection.prepareStatement(\"INSERT INTO t_livehashes\"\n-                    + \" (consensus_timestamp, livehash)\"\n-                    + \" VALUES (?, ?)\");\n-\n-            sqlInsertTopicMessage = connection.prepareStatement(\"insert into topic_message\"\n-                    + \" (consensus_timestamp, realm_num, topic_num, message, running_hash, sequence_number\" +\n-                    \", running_hash_version, chunk_num, chunk_total, payer_account_id, valid_start_timestamp)\"\n-                    + \" values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\");\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(\"Unable to prepare SQL statements\", e);\n-        }\n-    }\n-\n-    private void closeConnectionAndStatements() {\n-        try {\n-            sqlInsertTransaction.close();\n-            sqlInsertEntityId.close();\n-            sqlInsertTransferList.close();\n-            sqlInsertNonFeeTransfers.close();\n-            sqlInsertFileData.close();\n-            sqlInsertContractResult.close();\n-            sqlInsertLiveHashes.close();\n-            sqlInsertTopicMessage.close();\n-\n-            connection.close();\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(\"Error closing connection\", e);\n-        }\n+    @Override\n+    public void close() {\n+        executorService.shutdown();\n     }\n \n     private void executeBatches() {\n-        try {\n-            var transactions = executeBatch(sqlInsertTransaction);\n-            var entityIds = executeBatch(sqlInsertEntityId);\n-            var transferLists = executeBatch(sqlInsertTransferList);\n-            var nonFeeTransfers = executeBatch(sqlInsertNonFeeTransfers);\n-            var fileData = executeBatch(sqlInsertFileData);\n-            var contractResult = executeBatch(sqlInsertContractResult);\n-            var liveHashes = executeBatch(sqlInsertLiveHashes);\n-            var topicMessages = executeBatch(sqlInsertTopicMessage);\n-            log.info(\"Inserted transactions: {}, entities: {}, transfer lists: {}, files: {}, contracts: {}, \" +\n-                            \"claims: {}, topic messages: {}, non-fee transfers: {}\",\n-                    transactions, entityIds, transferLists, fileData, contractResult, liveHashes, topicMessages,\n-                    nonFeeTransfers);\n-        } catch (SQLException e) {\n-            log.error(\"Error committing sql insert batch \", e);\n-            throw new ParserSQLException(e);\n-        }\n-        batch_count = 0;\n-    }\n-\n-    private ExecuteBatchResult executeBatch(PreparedStatement ps) throws SQLException {\n         Stopwatch stopwatch = Stopwatch.createStarted();\n-        var executeResult = ps.executeBatch();\n-        return new ExecuteBatchResult(executeResult.length, stopwatch.elapsed(TimeUnit.MILLISECONDS));\n+        try {\n+            CompletableFuture.allOf(\n+                    CompletableFuture.runAsync(() -> transactionPgCopy.copy(transactions), executorService),\n+                    CompletableFuture.runAsync(() -> cryptoTransferPgCopy.copy(cryptoTransfers), executorService),\n+                    CompletableFuture.runAsync(() -> nonFeeTransferPgCopy.copy(nonFeeTransfers), executorService),\n+                    CompletableFuture.runAsync(() -> fileDataPgCopy.copy(fileData), executorService),\n+                    CompletableFuture.runAsync(() -> contractResultPgCopy.copy(contractResults), executorService),\n+                    CompletableFuture.runAsync(() -> liveHashPgCopy.copy(liveHashes), executorService),\n+                    CompletableFuture.runAsync(() -> topicMessagePgCopy.copy(topicMessages), executorService),\n+                    CompletableFuture\n+                            .runAsync(() -> entityIds.forEach(entityRepository::insertEntityId), executorService)", "originalCommit": "6bed7d988bcf20f5c40f2a4683fc3dca04bb873b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjExMDAzMA==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r456110030", "bodyText": "You're not specifying which CacheManager. The primary CacheManager will expire entries after 5m, which is not what we want. You would need to create and specify a CacheManager with @Qualifier that does not expire and has no max size (or a really large size).", "author": "steven-sheehy", "createdAt": "2020-07-16T22:18:20Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -47,375 +50,175 @@\n import com.hedera.mirror.importer.exception.DuplicateFileException;\n import com.hedera.mirror.importer.exception.ImporterException;\n import com.hedera.mirror.importer.exception.ParserException;\n-import com.hedera.mirror.importer.exception.ParserSQLException;\n import com.hedera.mirror.importer.parser.domain.StreamFileData;\n import com.hedera.mirror.importer.parser.record.RecordStreamFileListener;\n import com.hedera.mirror.importer.parser.record.entity.ConditionOnEntityRecordParser;\n import com.hedera.mirror.importer.parser.record.entity.EntityListener;\n+import com.hedera.mirror.importer.repository.EntityRepository;\n import com.hedera.mirror.importer.repository.RecordFileRepository;\n \n @Log4j2\n @Named\n-@RequiredArgsConstructor\n @ConditionOnEntityRecordParser\n-public class SqlEntityListener implements EntityListener, RecordStreamFileListener {\n-    private final SqlProperties properties;\n+public class SqlEntityListener implements EntityListener, RecordStreamFileListener, Closeable {\n     private final DataSource dataSource;\n+    private final ExecutorService executorService;\n     private final RecordFileRepository recordFileRepository;\n-    private long batch_count = 0;\n-    // Keeps track of entityIds seen in the current file being processed. This is for optimizing inserts into\n-    // t_entities table so that insertion of node and treasury ids are not tried for every transaction.\n-    private Collection<EntityId> entityIds;\n-    private PreparedStatement sqlInsertTransaction;\n-    private PreparedStatement sqlInsertEntityId;\n-    private PreparedStatement sqlInsertTransferList;\n-    private PreparedStatement sqlInsertNonFeeTransfers;\n-    private PreparedStatement sqlInsertFileData;\n-    private PreparedStatement sqlInsertContractResult;\n-    private PreparedStatement sqlInsertLiveHashes;\n-    private PreparedStatement sqlInsertTopicMessage;\n-    private Connection connection;\n+    private final EntityRepository entityRepository;\n+    private final SqlProperties sqlProperties;\n+    private final CacheManager cacheManager;\n+    private long batch_count;\n+\n+    // Keeps track of entityIds seen so far. This is for optimizing inserts into t_entities table so that insertion of\n+    // node and treasury ids are not tried for every transaction.\n+    private final Collection<EntityId> seenEntityIds = new HashSet<>();\n+    // init connections, schemas, writers, etc once per process\n+    private final PgCopy<Transaction> transactionPgCopy;\n+    private final PgCopy<CryptoTransfer> cryptoTransferPgCopy;\n+    private final PgCopy<NonFeeTransfer> nonFeeTransferPgCopy;\n+    private final PgCopy<FileData> fileDataPgCopy;\n+    private final PgCopy<ContractResult> contractResultPgCopy;\n+    private final PgCopy<LiveHash> liveHashPgCopy;\n+    private final PgCopy<TopicMessage> topicMessagePgCopy;\n+\n+    private final Timer insertDuration;\n+\n+    private List<Transaction> transactions;\n+    private List<CryptoTransfer> cryptoTransfers;\n+    private List<NonFeeTransfer> nonFeeTransfers;\n+    private List<FileData> fileData;\n+    private List<ContractResult> contractResults;\n+    private List<LiveHash> liveHashes;\n+    private List<TopicMessage> topicMessages;\n+    private List<EntityId> entityIds;\n+    private final Cache entityCache;\n+\n+    public SqlEntityListener(SqlProperties properties, DataSource dataSource,\n+                             RecordFileRepository recordFileRepository, EntityRepository entityRepository,\n+                             MeterRegistry meterRegistry, CacheManager cacheManager) {", "originalCommit": "6bed7d988bcf20f5c40f2a4683fc3dca04bb873b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcyMTAyNw==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r457721027", "bodyText": "Creating a sessionCache for non expiring 1 trillion cache size", "author": "Nana-EC", "createdAt": "2020-07-20T22:12:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjExMDAzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcyMzUxNw==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r457723517", "bodyText": "Maybe not 1 trillion as that's 58TB. Just to be safe maybe 2M or so (120MB) LRU cache. If it goes over 2M somehow in our life then the risk is only that we do occasional extra inserts that are ignored with on conflict do nothing.", "author": "steven-sheehy", "createdAt": "2020-07-20T22:18:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjExMDAzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjExMjE3MQ==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r456112171", "bodyText": "gRPC expects this in base64 and standard is generally base64, not hex encoded. Instead of hardcoding this here, recommend configuring at objectmapper layer and not domain layer:\n        ObjectMapper objectMapper = new ObjectMapper();\n        SimpleModule module = new SimpleModule();\n        module.addSerializer(byte[].class, new ByteArrayToHexSerializer());\n        objectMapper.registerModule(module);", "author": "steven-sheehy", "createdAt": "2020-07-16T22:24:28Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/domain/TopicMessage.java", "diffHunk": "@@ -40,6 +42,7 @@\n     @Id\n     private long consensusTimestamp;\n \n+    @JsonSerialize(using = ByteArraySerializer.class)", "originalCommit": "6bed7d988bcf20f5c40f2a4683fc3dca04bb873b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcyMDg0NQ==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r457720845", "bodyText": "I agree on the base64 logic. Not sure on the original hex choice. Can simply change the serialize method to base64 encode.\nWill verify this is not a limitation", "author": "Nana-EC", "createdAt": "2020-07-20T22:11:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjExMjE3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE4MDA5MA==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r458180090", "bodyText": "Default base64 JSONserialize seems to work fine.\nHad to add a converter to take care of decoding and also encoding for repository to be consistent.\nThere's a thought to say we could just go with base64 encoding during serialize and ByteArrayBase64Converter for all the byte[] columns.", "author": "Nana-EC", "createdAt": "2020-07-21T15:19:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjExMjE3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg5ODMxMw==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r458898313", "bodyText": "Circling back on this as the better solution. Registering the module.", "author": "Nana-EC", "createdAt": "2020-07-22T15:53:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjExMjE3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjExMjQ1Ng==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r456112456", "bodyText": "ByteArrayToHexSerializer to clarify target encoding", "author": "steven-sheehy", "createdAt": "2020-07-16T22:25:14Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/converter/ByteArraySerializer.java", "diffHunk": "@@ -0,0 +1,38 @@\n+package com.hedera.mirror.importer.converter;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.JsonSerializer;\n+import com.fasterxml.jackson.databind.SerializerProvider;\n+import java.io.IOException;\n+import javax.inject.Named;\n+import org.apache.commons.codec.binary.Hex;\n+\n+@Named\n+public class ByteArraySerializer extends JsonSerializer<byte[]> {", "originalCommit": "6bed7d988bcf20f5c40f2a4683fc3dca04bb873b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcyMDg3Mg==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r457720872", "bodyText": "Based on switching to base64 I can just make this ByteArrayToBase64Serializer", "author": "Nana-EC", "createdAt": "2020-07-20T22:11:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjExMjQ1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcyNDAxOQ==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r457724019", "bodyText": "You won't need a custom serializer if using base64. Just remove the serializer and annotation in that case.", "author": "steven-sheehy", "createdAt": "2020-07-20T22:19:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjExMjQ1Ng=="}], "type": "inlineReview"}, {"oid": "5c68f7a4b8484f77040dc9cfa09540cf23f2d320", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/5c68f7a4b8484f77040dc9cfa09540cf23f2d320", "message": "Address comments and fixed tests\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>", "committedDate": "2020-07-21T01:57:37Z", "type": "commit"}, {"oid": "44b182755b02254e6178f139bd0335804eadb0f3", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/44b182755b02254e6178f139bd0335804eadb0f3", "message": "Merge events and pg_notify in\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>", "committedDate": "2020-07-21T03:24:39Z", "type": "commit"}, {"oid": "d749e7441a0bcdf20fa79f6faba6cdda9518f5f5", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/d749e7441a0bcdf20fa79f6faba6cdda9518f5f5", "message": "Added ByteArrayBase64Converter. Fixed topic message base 64 decoding for tests\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>", "committedDate": "2020-07-21T15:07:28Z", "type": "commit"}, {"oid": "f332cda9e5e0efe9a2d3a8bfe6a817e6ebf3776d", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/f332cda9e5e0efe9a2d3a8bfe6a817e6ebf3776d", "message": "Fix data generator build issue\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>", "committedDate": "2020-07-21T15:15:42Z", "type": "commit"}, {"oid": "7c32fca0d8e69d3065efd56fafdace36af421416", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/7c32fca0d8e69d3065efd56fafdace36af421416", "message": "Renamed migration script version to avoid conflict with record file table change\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>", "committedDate": "2020-07-21T15:23:58Z", "type": "commit"}, {"oid": "85d14a668eed5bd7deb162bc6cfe69279b8a580c", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/85d14a668eed5bd7deb162bc6cfe69279b8a580c", "message": "Add buffersize to log copyIn and log\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>", "committedDate": "2020-07-21T17:26:20Z", "type": "commit"}, {"oid": "1fa404475e9ad067cdf0d8953c538d10e91059fd", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/1fa404475e9ad067cdf0d8953c538d10e91059fd", "message": "Merge record file table update\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>", "committedDate": "2020-07-21T17:32:52Z", "type": "commit"}, {"oid": "3e89d4147c94b87ed2d7d2034e57b66133d1a48b", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/3e89d4147c94b87ed2d7d2034e57b66133d1a48b", "message": "Removed ByteArrayToHexSerializer default to base64 encoding for serialization\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>", "committedDate": "2020-07-21T21:07:08Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMxMzU5Mw==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r458313593", "bodyText": "Looks like this can be removed?", "author": "steven-sheehy", "createdAt": "2020-07-21T18:45:59Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/config/MirrorImporterConfiguration.java", "diffHunk": "@@ -62,6 +63,7 @@\n     private final MirrorProperties mirrorProperties;\n     private final CommonDownloaderProperties downloaderProperties;\n     private final MetricsExecutionInterceptor metricsExecutionInterceptor;\n+    private final DataSource dataSource;", "originalCommit": "1fa404475e9ad067cdf0d8953c538d10e91059fd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMxNDk5Mg==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r458314992", "bodyText": "This annotation and ByteArrayBase64Converter  should be removed. We don't need to convert byte[] to base64 for JPA as we store raw bytes into the bytea type field. If that data is somehow getting stored as base64 encoded bytes that is very wrong.", "author": "steven-sheehy", "createdAt": "2020-07-21T18:48:33Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/domain/TopicMessage.java", "diffHunk": "@@ -40,6 +41,7 @@\n     @Id\n     private long consensusTimestamp;\n \n+    @Convert(converter = ByteArrayBase64Converter.class)", "originalCommit": "1fa404475e9ad067cdf0d8953c538d10e91059fd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODkwMTEyMQ==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r458901121", "bodyText": "Removed this and applied the bytearraytohex mapper.", "author": "Nana-EC", "createdAt": "2020-07-22T15:57:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMxNDk5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMxODg2OA==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r458318868", "bodyText": "Technically this is not a cache but a CacheManager which can generate many caches. I wouldn't consider it a session either. Would prefer a more generic name like String NEVER_EXPIRE_LARGE = cacheManagerNeverExpireLarge to allow for reuse.", "author": "steven-sheehy", "createdAt": "2020-07-21T18:55:32Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/config/CacheConfiguration.java", "diffHunk": "@@ -42,6 +42,7 @@\n     public static final String EXPIRE_AFTER_5M = \"cacheManagerExpireAfter5m\";\n     public static final String EXPIRE_AFTER_30M = \"cacheManagerExpireAfter30m\";\n     public static final String TINY_LRU_CACHE = \"tinyLruCache\";\n+    public static final String SESSION_CACHE = \"sessionCache\";", "originalCommit": "1fa404475e9ad067cdf0d8953c538d10e91059fd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTA0OTA2Ng==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r459049066", "bodyText": "Applied your naming", "author": "Nana-EC", "createdAt": "2020-07-22T20:01:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMxODg2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2NjE4MA==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r458366180", "bodyText": "These metric names are too long. Prometheus will usually add stuff to the end like _total_seconds making them even longer. I don't think the intent is to match them to the package name like it is with properties. We definitely haven't been doing so in the past. Also would be good to have a non-pgcopy specific insert metric in case we switch to repository or something else later. Suggest hedera.mirror.parse.insert and hedera.mirror.parse.csv.", "author": "steven-sheehy", "createdAt": "2020-07-21T20:24:52Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/PgCopy.java", "diffHunk": "@@ -0,0 +1,123 @@\n+package com.hedera.mirror.importer.parser.record.entity.sql;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import com.fasterxml.jackson.databind.ObjectWriter;\n+import com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+import com.google.common.base.CaseFormat;\n+import com.google.common.base.Stopwatch;\n+import com.google.common.collect.Lists;\n+import io.micrometer.core.instrument.MeterRegistry;\n+import io.micrometer.core.instrument.Timer;\n+import java.io.IOException;\n+import java.io.StringReader;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.util.HashSet;\n+import java.util.stream.Collectors;\n+import javax.sql.DataSource;\n+import lombok.extern.log4j.Log4j2;\n+import org.postgresql.PGConnection;\n+import org.postgresql.copy.CopyManager;\n+\n+import com.hedera.mirror.importer.exception.ParserException;\n+\n+/**\n+ * Stateless writer to insert rows into Postgres table using COPY.\n+ *\n+ * @param <T> domain object\n+ */\n+@Log4j2\n+public class PgCopy<T> {\n+    private final DataSource dataSource;\n+    private final String tableName;\n+    private final String columnsCsv;\n+    private final ObjectWriter writer;\n+    private final Timer buildCsvDurationMetric;\n+    private final Timer copyDurationMetric;\n+    private final Timer insertDurationMetric;\n+    private final int bufferSize;\n+\n+    public PgCopy(DataSource dataSource, Class<T> tClass, MeterRegistry meterRegistry, int bufferSize) {\n+        this.dataSource = dataSource;\n+        this.bufferSize = bufferSize;\n+        tableName = CaseFormat.UPPER_CAMEL.to(CaseFormat.LOWER_UNDERSCORE, tClass.getSimpleName());\n+        var mapper = new CsvMapper();\n+        var schema = mapper.schemaFor(tClass);\n+        writer = mapper.writer(schema);\n+        columnsCsv = Lists.newArrayList(schema.iterator()).stream()\n+                .map(CsvSchema.Column::getName)\n+                .map(name -> CaseFormat.UPPER_CAMEL.to(CaseFormat.LOWER_UNDERSCORE, name))\n+                .collect(Collectors.joining(\", \"));\n+        buildCsvDurationMetric = Timer.builder(\"hedera.mirror.importer.parser.record.entity.sql.pgcopy.csv\")", "originalCommit": "1fa404475e9ad067cdf0d8953c538d10e91059fd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTA0OTE5NA==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r459049194", "bodyText": "Adjusted", "author": "Nana-EC", "createdAt": "2020-07-22T20:01:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2NjE4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQxMDQyNg==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r458410426", "bodyText": "Shouldn't log error twice", "author": "steven-sheehy", "createdAt": "2020-07-21T21:52:51Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/PgCopy.java", "diffHunk": "@@ -0,0 +1,123 @@\n+package com.hedera.mirror.importer.parser.record.entity.sql;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import com.fasterxml.jackson.databind.ObjectWriter;\n+import com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+import com.google.common.base.CaseFormat;\n+import com.google.common.base.Stopwatch;\n+import com.google.common.collect.Lists;\n+import io.micrometer.core.instrument.MeterRegistry;\n+import io.micrometer.core.instrument.Timer;\n+import java.io.IOException;\n+import java.io.StringReader;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.util.HashSet;\n+import java.util.stream.Collectors;\n+import javax.sql.DataSource;\n+import lombok.extern.log4j.Log4j2;\n+import org.postgresql.PGConnection;\n+import org.postgresql.copy.CopyManager;\n+\n+import com.hedera.mirror.importer.exception.ParserException;\n+\n+/**\n+ * Stateless writer to insert rows into Postgres table using COPY.\n+ *\n+ * @param <T> domain object\n+ */\n+@Log4j2\n+public class PgCopy<T> {\n+    private final DataSource dataSource;\n+    private final String tableName;\n+    private final String columnsCsv;\n+    private final ObjectWriter writer;\n+    private final Timer buildCsvDurationMetric;\n+    private final Timer copyDurationMetric;\n+    private final Timer insertDurationMetric;\n+    private final int bufferSize;\n+\n+    public PgCopy(DataSource dataSource, Class<T> tClass, MeterRegistry meterRegistry, int bufferSize) {\n+        this.dataSource = dataSource;\n+        this.bufferSize = bufferSize;\n+        tableName = CaseFormat.UPPER_CAMEL.to(CaseFormat.LOWER_UNDERSCORE, tClass.getSimpleName());\n+        var mapper = new CsvMapper();\n+        var schema = mapper.schemaFor(tClass);\n+        writer = mapper.writer(schema);\n+        columnsCsv = Lists.newArrayList(schema.iterator()).stream()\n+                .map(CsvSchema.Column::getName)\n+                .map(name -> CaseFormat.UPPER_CAMEL.to(CaseFormat.LOWER_UNDERSCORE, name))\n+                .collect(Collectors.joining(\", \"));\n+        buildCsvDurationMetric = Timer.builder(\"hedera.mirror.importer.parser.record.entity.sql.pgcopy.csv\")\n+                .description(\"Time to build csv string\")\n+                .tag(\"table\", tableName)\n+                .register(meterRegistry);\n+        insertDurationMetric = Timer.builder(\"hedera.mirror.importer.parser.record.entity.sql.pgcopy.insert\")\n+                .description(\"Time to insert transactions into table\")\n+                .tag(\"table\", tableName)\n+                .register(meterRegistry);\n+        copyDurationMetric = Timer.builder(\"hedera.mirror.importer.parser.record.entity.sql.pgcopy.copy\")\n+                .description(\"Time to copy transactions (build and insert) into table\")\n+                .tag(\"table\", tableName)\n+                .register(meterRegistry);\n+    }\n+\n+    public void copy(HashSet<T> items) {\n+\n+        if (items == null || items.size() == 0) {\n+            return;\n+        }\n+        try (Connection connection = dataSource.getConnection()) {\n+            Stopwatch stopwatch = Stopwatch.createStarted();\n+            var csv = buildCsv(items);\n+            var csvBuildDuration = stopwatch.elapsed();\n+\n+            log.debug(\"Copying {} rows from buffer of size {} to {} table.\", items.size(), bufferSize, tableName);\n+            CopyManager copyManager = connection.unwrap(PGConnection.class).getCopyAPI();\n+            long rowsCount = copyManager.copyIn(\n+                    String.format(\"COPY %s(%s) FROM STDIN WITH CSV\", tableName, columnsCsv),\n+                    new StringReader(csv),\n+                    bufferSize);\n+\n+            var copyDuration = stopwatch.elapsed();\n+            insertDurationMetric.record(copyDuration.minus(csvBuildDuration));\n+            copyDurationMetric.record(stopwatch.elapsed().minus(copyDuration));\n+            log.info(\"Copied {} rows to {} table in {}ms\",\n+                    rowsCount, tableName, copyDuration.toMillis());\n+        } catch (IOException | SQLException e) {\n+            log.error(e);", "originalCommit": "3e89d4147c94b87ed2d7d2034e57b66133d1a48b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQxMTI1MQ==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r458411251", "bodyText": "We already track csv + insert times separately and don't need to track them combined. We can just use metric queries to sum them together if need be.", "author": "steven-sheehy", "createdAt": "2020-07-21T21:54:48Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/PgCopy.java", "diffHunk": "@@ -0,0 +1,123 @@\n+package com.hedera.mirror.importer.parser.record.entity.sql;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import com.fasterxml.jackson.databind.ObjectWriter;\n+import com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+import com.google.common.base.CaseFormat;\n+import com.google.common.base.Stopwatch;\n+import com.google.common.collect.Lists;\n+import io.micrometer.core.instrument.MeterRegistry;\n+import io.micrometer.core.instrument.Timer;\n+import java.io.IOException;\n+import java.io.StringReader;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.util.HashSet;\n+import java.util.stream.Collectors;\n+import javax.sql.DataSource;\n+import lombok.extern.log4j.Log4j2;\n+import org.postgresql.PGConnection;\n+import org.postgresql.copy.CopyManager;\n+\n+import com.hedera.mirror.importer.exception.ParserException;\n+\n+/**\n+ * Stateless writer to insert rows into Postgres table using COPY.\n+ *\n+ * @param <T> domain object\n+ */\n+@Log4j2\n+public class PgCopy<T> {\n+    private final DataSource dataSource;\n+    private final String tableName;\n+    private final String columnsCsv;\n+    private final ObjectWriter writer;\n+    private final Timer buildCsvDurationMetric;\n+    private final Timer copyDurationMetric;\n+    private final Timer insertDurationMetric;\n+    private final int bufferSize;\n+\n+    public PgCopy(DataSource dataSource, Class<T> tClass, MeterRegistry meterRegistry, int bufferSize) {\n+        this.dataSource = dataSource;\n+        this.bufferSize = bufferSize;\n+        tableName = CaseFormat.UPPER_CAMEL.to(CaseFormat.LOWER_UNDERSCORE, tClass.getSimpleName());\n+        var mapper = new CsvMapper();\n+        var schema = mapper.schemaFor(tClass);\n+        writer = mapper.writer(schema);\n+        columnsCsv = Lists.newArrayList(schema.iterator()).stream()\n+                .map(CsvSchema.Column::getName)\n+                .map(name -> CaseFormat.UPPER_CAMEL.to(CaseFormat.LOWER_UNDERSCORE, name))\n+                .collect(Collectors.joining(\", \"));\n+        buildCsvDurationMetric = Timer.builder(\"hedera.mirror.importer.parser.record.entity.sql.pgcopy.csv\")\n+                .description(\"Time to build csv string\")\n+                .tag(\"table\", tableName)\n+                .register(meterRegistry);\n+        insertDurationMetric = Timer.builder(\"hedera.mirror.importer.parser.record.entity.sql.pgcopy.insert\")\n+                .description(\"Time to insert transactions into table\")\n+                .tag(\"table\", tableName)\n+                .register(meterRegistry);\n+        copyDurationMetric = Timer.builder(\"hedera.mirror.importer.parser.record.entity.sql.pgcopy.copy\")", "originalCommit": "3e89d4147c94b87ed2d7d2034e57b66133d1a48b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQxMjUzMA==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r458412530", "bodyText": "Formatted wrong. Not sure what number you're going for. Min should allow lower like 1. Also need to update docs.", "author": "steven-sheehy", "createdAt": "2020-07-21T21:57:36Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlProperties.java", "diffHunk": "@@ -24,14 +24,17 @@\n import lombok.Data;\n import org.springframework.boot.context.properties.ConfigurationProperties;\n \n+import com.hedera.mirror.importer.parser.record.entity.ConditionOnEntityRecordParser;\n+\n @Data\n+@ConditionOnEntityRecordParser\n @ConfigurationProperties(\"hedera.mirror.importer.parser.record.entity.sql\")\n public class SqlProperties {\n-    /**\n-     * PreparedStatement.executeBatch() is called after every batchSize number of transactions from record stream file.\n-     */\n     @Min(1)\n-    private int batchSize = 2000;\n+    private int threads = 10;\n+\n+    @Min(2000)\n+    private int batchSize = 1_00_000_000;", "originalCommit": "3e89d4147c94b87ed2d7d2034e57b66133d1a48b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODkwNTE4Ng==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r458905186", "bodyText": "100_000_000. Higher numbers eventually cause an OOM heap space exception. I'll actually add this as the max for now also. Future testing can find the fine tuned max", "author": "Nana-EC", "createdAt": "2020-07-22T16:03:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQxMjUzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQxMzI3NQ==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r458413275", "bodyText": "Any idea why this is changing? Usually that indicates a bug with the refactoring", "author": "steven-sheehy", "createdAt": "2020-07-21T21:59:15Z", "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/parser/record/entity/EntityRecordItemListenerCryptoTest.java", "diffHunk": "@@ -513,7 +513,7 @@ void cryptoTransferWithPersistence() throws Exception {\n         assertAll(\n                 () -> assertEquals(1, transactionRepository.count())\n                 , () -> assertEquals(5, entityRepository.count())\n-                , () -> assertEquals(6, cryptoTransferRepository.count())\n+                , () -> assertEquals(5, cryptoTransferRepository.count())", "originalCommit": "3e89d4147c94b87ed2d7d2034e57b66133d1a48b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODkwNDA2Mw==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r458904063", "bodyText": "This was indeed a bug and will be reverted", "author": "Nana-EC", "createdAt": "2020-07-22T16:02:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQxMzI3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQxNjMwNA==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r458416304", "bodyText": "I'm not sure this Qualifier translates to the generated constructor. Have you verified it's not using the primary cachemanager?", "author": "steven-sheehy", "createdAt": "2020-07-21T22:06:11Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -55,52 +63,115 @@\n import com.hedera.mirror.importer.parser.record.RecordStreamFileListener;\n import com.hedera.mirror.importer.parser.record.entity.ConditionOnEntityRecordParser;\n import com.hedera.mirror.importer.parser.record.entity.EntityListener;\n+import com.hedera.mirror.importer.repository.EntityRepository;\n import com.hedera.mirror.importer.repository.RecordFileRepository;\n \n @Log4j2\n @Named\n-@RequiredArgsConstructor\n @ConditionOnEntityRecordParser\n-public class SqlEntityListener implements EntityListener, RecordStreamFileListener {\n+\n+public class SqlEntityListener implements EntityListener, RecordStreamFileListener, Closeable {\n \n     static final ObjectMapper OBJECT_MAPPER = new ObjectMapper()\n             .setPropertyNamingStrategy(PropertyNamingStrategy.SNAKE_CASE);\n \n-    private final SqlProperties properties;\n     private final DataSource dataSource;\n+    private final ExecutorService executorService;\n     private final RecordFileRepository recordFileRepository;\n-    private long batch_count = 0;\n-    // Keeps track of entityIds seen in the current file being processed. This is for optimizing inserts into\n-    // t_entities table so that insertion of node and treasury ids are not tried for every transaction.\n-    private Collection<EntityId> entityIds;\n-    private PreparedStatement sqlInsertTransaction;\n-    private PreparedStatement sqlInsertEntityId;\n-    private PreparedStatement sqlInsertTransferList;\n-    private PreparedStatement sqlInsertNonFeeTransfers;\n-    private PreparedStatement sqlInsertFileData;\n-    private PreparedStatement sqlInsertContractResult;\n-    private PreparedStatement sqlInsertLiveHashes;\n-    private PreparedStatement sqlInsertTopicMessage;\n+    private final EntityRepository entityRepository;\n+    private final SqlProperties sqlProperties;\n+\n+    @Qualifier(value = \"sessionCache\")", "originalCommit": "3e89d4147c94b87ed2d7d2034e57b66133d1a48b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTEzMDMxOA==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r459130318", "bodyText": "Had to move qualifier to constructor and update Lombok config", "author": "Nana-EC", "createdAt": "2020-07-22T22:58:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQxNjMwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQxNzgyNg==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r458417826", "bodyText": "Only entityIds should be a HashSet. Everything else should be an ArrayList. The default equals/hashCode implemented by Lombok will cause issues with sets.", "author": "steven-sheehy", "createdAt": "2020-07-21T22:09:48Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -55,52 +63,115 @@\n import com.hedera.mirror.importer.parser.record.RecordStreamFileListener;\n import com.hedera.mirror.importer.parser.record.entity.ConditionOnEntityRecordParser;\n import com.hedera.mirror.importer.parser.record.entity.EntityListener;\n+import com.hedera.mirror.importer.repository.EntityRepository;\n import com.hedera.mirror.importer.repository.RecordFileRepository;\n \n @Log4j2\n @Named\n-@RequiredArgsConstructor\n @ConditionOnEntityRecordParser\n-public class SqlEntityListener implements EntityListener, RecordStreamFileListener {\n+\n+public class SqlEntityListener implements EntityListener, RecordStreamFileListener, Closeable {\n \n     static final ObjectMapper OBJECT_MAPPER = new ObjectMapper()\n             .setPropertyNamingStrategy(PropertyNamingStrategy.SNAKE_CASE);\n \n-    private final SqlProperties properties;\n     private final DataSource dataSource;\n+    private final ExecutorService executorService;\n     private final RecordFileRepository recordFileRepository;\n-    private long batch_count = 0;\n-    // Keeps track of entityIds seen in the current file being processed. This is for optimizing inserts into\n-    // t_entities table so that insertion of node and treasury ids are not tried for every transaction.\n-    private Collection<EntityId> entityIds;\n-    private PreparedStatement sqlInsertTransaction;\n-    private PreparedStatement sqlInsertEntityId;\n-    private PreparedStatement sqlInsertTransferList;\n-    private PreparedStatement sqlInsertNonFeeTransfers;\n-    private PreparedStatement sqlInsertFileData;\n-    private PreparedStatement sqlInsertContractResult;\n-    private PreparedStatement sqlInsertLiveHashes;\n-    private PreparedStatement sqlInsertTopicMessage;\n+    private final EntityRepository entityRepository;\n+    private final SqlProperties sqlProperties;\n+\n+    @Qualifier(value = \"sessionCache\")\n+    private final CacheManager cacheManager;\n+    private long batchCount;\n+\n+    // Keeps track of entityIds seen so far. This is for optimizing inserts into t_entities table so that insertion of\n+    // node and treasury ids are not tried for every transaction.\n+    private final Collection<EntityId> seenEntityIds = new HashSet<>();\n+    // init connections, schemas, writers, etc once per process\n+    private final PgCopy<Transaction> transactionPgCopy;\n+    private final PgCopy<CryptoTransfer> cryptoTransferPgCopy;\n+    private final PgCopy<NonFeeTransfer> nonFeeTransferPgCopy;\n+    private final PgCopy<FileData> fileDataPgCopy;\n+    private final PgCopy<ContractResult> contractResultPgCopy;\n+    private final PgCopy<LiveHash> liveHashPgCopy;\n+    private final PgCopy<TopicMessage> topicMessagePgCopy;\n+\n+    private final Timer insertDuration;\n+\n+    private HashSet<Transaction> transactions;\n+    private HashSet<CryptoTransfer> cryptoTransfers;\n+    private HashSet<NonFeeTransfer> nonFeeTransfers;\n+    private HashSet<FileData> fileData;\n+    private HashSet<ContractResult> contractResults;\n+    private HashSet<LiveHash> liveHashes;\n+    private HashSet<TopicMessage> topicMessages;\n+    private HashSet<EntityId> entityIds;\n+    private final Cache entityCache;\n+\n     private PreparedStatement sqlNotifyTopicMessage;\n     private Connection connection;\n \n+    public SqlEntityListener(SqlProperties properties, DataSource dataSource,\n+                             RecordFileRepository recordFileRepository, EntityRepository entityRepository,\n+                             MeterRegistry meterRegistry, CacheManager cacheManager) {\n+        this.dataSource = dataSource;\n+        this.recordFileRepository = recordFileRepository;\n+        this.entityRepository = entityRepository;\n+        sqlProperties = properties;\n+        executorService = Executors.newFixedThreadPool(properties.getThreads());\n+        this.cacheManager = cacheManager;\n+\n+        transactionPgCopy = new PgCopy<>(dataSource, Transaction.class, meterRegistry, sqlProperties.getBatchSize());\n+        cryptoTransferPgCopy = new PgCopy<>(dataSource, CryptoTransfer.class, meterRegistry, sqlProperties\n+                .getBatchSize());\n+        nonFeeTransferPgCopy = new PgCopy<>(dataSource, NonFeeTransfer.class, meterRegistry, sqlProperties\n+                .getBatchSize());\n+        fileDataPgCopy = new PgCopy<>(dataSource, FileData.class, meterRegistry, sqlProperties.getBatchSize());\n+        contractResultPgCopy = new PgCopy<>(dataSource, ContractResult.class, meterRegistry, sqlProperties\n+                .getBatchSize());\n+        liveHashPgCopy = new PgCopy<>(dataSource, LiveHash.class, meterRegistry, sqlProperties.getBatchSize());\n+        topicMessagePgCopy = new PgCopy<>(dataSource, TopicMessage.class, meterRegistry, sqlProperties.getBatchSize());\n+\n+        insertDuration = Timer.builder(\"hedera.mirror.importer.parser.record.entity.sql.insert\")\n+                .description(\"Time to insert all entities into database\")\n+                .register(meterRegistry);\n+\n+        entityCache = cacheManager.getCache(\"seen_entities\");\n+    }\n+\n     @Override\n     public void onStart(StreamFileData streamFileData) {\n         String fileName = FilenameUtils.getName(streamFileData.getFilename());\n         entityIds = new HashSet<>();\n         if (recordFileRepository.findByName(fileName).size() > 0) {\n             throw new DuplicateFileException(\"File already exists in the database: \" + fileName);\n         }\n+        transactions = new HashSet<>();", "originalCommit": "3e89d4147c94b87ed2d7d2034e57b66133d1a48b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQxODU0MA==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r458418540", "bodyText": "Using the Dependency Inversion Principle we should use Collection as the declaration interface for all these entities and not depend upon concrete implementations.", "author": "steven-sheehy", "createdAt": "2020-07-21T22:11:19Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -55,52 +63,115 @@\n import com.hedera.mirror.importer.parser.record.RecordStreamFileListener;\n import com.hedera.mirror.importer.parser.record.entity.ConditionOnEntityRecordParser;\n import com.hedera.mirror.importer.parser.record.entity.EntityListener;\n+import com.hedera.mirror.importer.repository.EntityRepository;\n import com.hedera.mirror.importer.repository.RecordFileRepository;\n \n @Log4j2\n @Named\n-@RequiredArgsConstructor\n @ConditionOnEntityRecordParser\n-public class SqlEntityListener implements EntityListener, RecordStreamFileListener {\n+\n+public class SqlEntityListener implements EntityListener, RecordStreamFileListener, Closeable {\n \n     static final ObjectMapper OBJECT_MAPPER = new ObjectMapper()\n             .setPropertyNamingStrategy(PropertyNamingStrategy.SNAKE_CASE);\n \n-    private final SqlProperties properties;\n     private final DataSource dataSource;\n+    private final ExecutorService executorService;\n     private final RecordFileRepository recordFileRepository;\n-    private long batch_count = 0;\n-    // Keeps track of entityIds seen in the current file being processed. This is for optimizing inserts into\n-    // t_entities table so that insertion of node and treasury ids are not tried for every transaction.\n-    private Collection<EntityId> entityIds;\n-    private PreparedStatement sqlInsertTransaction;\n-    private PreparedStatement sqlInsertEntityId;\n-    private PreparedStatement sqlInsertTransferList;\n-    private PreparedStatement sqlInsertNonFeeTransfers;\n-    private PreparedStatement sqlInsertFileData;\n-    private PreparedStatement sqlInsertContractResult;\n-    private PreparedStatement sqlInsertLiveHashes;\n-    private PreparedStatement sqlInsertTopicMessage;\n+    private final EntityRepository entityRepository;\n+    private final SqlProperties sqlProperties;\n+\n+    @Qualifier(value = \"sessionCache\")\n+    private final CacheManager cacheManager;\n+    private long batchCount;\n+\n+    // Keeps track of entityIds seen so far. This is for optimizing inserts into t_entities table so that insertion of\n+    // node and treasury ids are not tried for every transaction.\n+    private final Collection<EntityId> seenEntityIds = new HashSet<>();\n+    // init connections, schemas, writers, etc once per process\n+    private final PgCopy<Transaction> transactionPgCopy;\n+    private final PgCopy<CryptoTransfer> cryptoTransferPgCopy;\n+    private final PgCopy<NonFeeTransfer> nonFeeTransferPgCopy;\n+    private final PgCopy<FileData> fileDataPgCopy;\n+    private final PgCopy<ContractResult> contractResultPgCopy;\n+    private final PgCopy<LiveHash> liveHashPgCopy;\n+    private final PgCopy<TopicMessage> topicMessagePgCopy;\n+\n+    private final Timer insertDuration;\n+\n+    private HashSet<Transaction> transactions;", "originalCommit": "3e89d4147c94b87ed2d7d2034e57b66133d1a48b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQxOTU0OA==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r458419548", "bodyText": "We don't need this metric if we have the per table metric. Metrics can be queried either with or without the tag to show specific or all tables.", "author": "steven-sheehy", "createdAt": "2020-07-21T22:13:40Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -55,52 +63,115 @@\n import com.hedera.mirror.importer.parser.record.RecordStreamFileListener;\n import com.hedera.mirror.importer.parser.record.entity.ConditionOnEntityRecordParser;\n import com.hedera.mirror.importer.parser.record.entity.EntityListener;\n+import com.hedera.mirror.importer.repository.EntityRepository;\n import com.hedera.mirror.importer.repository.RecordFileRepository;\n \n @Log4j2\n @Named\n-@RequiredArgsConstructor\n @ConditionOnEntityRecordParser\n-public class SqlEntityListener implements EntityListener, RecordStreamFileListener {\n+\n+public class SqlEntityListener implements EntityListener, RecordStreamFileListener, Closeable {\n \n     static final ObjectMapper OBJECT_MAPPER = new ObjectMapper()\n             .setPropertyNamingStrategy(PropertyNamingStrategy.SNAKE_CASE);\n \n-    private final SqlProperties properties;\n     private final DataSource dataSource;\n+    private final ExecutorService executorService;\n     private final RecordFileRepository recordFileRepository;\n-    private long batch_count = 0;\n-    // Keeps track of entityIds seen in the current file being processed. This is for optimizing inserts into\n-    // t_entities table so that insertion of node and treasury ids are not tried for every transaction.\n-    private Collection<EntityId> entityIds;\n-    private PreparedStatement sqlInsertTransaction;\n-    private PreparedStatement sqlInsertEntityId;\n-    private PreparedStatement sqlInsertTransferList;\n-    private PreparedStatement sqlInsertNonFeeTransfers;\n-    private PreparedStatement sqlInsertFileData;\n-    private PreparedStatement sqlInsertContractResult;\n-    private PreparedStatement sqlInsertLiveHashes;\n-    private PreparedStatement sqlInsertTopicMessage;\n+    private final EntityRepository entityRepository;\n+    private final SqlProperties sqlProperties;\n+\n+    @Qualifier(value = \"sessionCache\")\n+    private final CacheManager cacheManager;\n+    private long batchCount;\n+\n+    // Keeps track of entityIds seen so far. This is for optimizing inserts into t_entities table so that insertion of\n+    // node and treasury ids are not tried for every transaction.\n+    private final Collection<EntityId> seenEntityIds = new HashSet<>();\n+    // init connections, schemas, writers, etc once per process\n+    private final PgCopy<Transaction> transactionPgCopy;\n+    private final PgCopy<CryptoTransfer> cryptoTransferPgCopy;\n+    private final PgCopy<NonFeeTransfer> nonFeeTransferPgCopy;\n+    private final PgCopy<FileData> fileDataPgCopy;\n+    private final PgCopy<ContractResult> contractResultPgCopy;\n+    private final PgCopy<LiveHash> liveHashPgCopy;\n+    private final PgCopy<TopicMessage> topicMessagePgCopy;\n+\n+    private final Timer insertDuration;\n+\n+    private HashSet<Transaction> transactions;\n+    private HashSet<CryptoTransfer> cryptoTransfers;\n+    private HashSet<NonFeeTransfer> nonFeeTransfers;\n+    private HashSet<FileData> fileData;\n+    private HashSet<ContractResult> contractResults;\n+    private HashSet<LiveHash> liveHashes;\n+    private HashSet<TopicMessage> topicMessages;\n+    private HashSet<EntityId> entityIds;\n+    private final Cache entityCache;\n+\n     private PreparedStatement sqlNotifyTopicMessage;\n     private Connection connection;\n \n+    public SqlEntityListener(SqlProperties properties, DataSource dataSource,\n+                             RecordFileRepository recordFileRepository, EntityRepository entityRepository,\n+                             MeterRegistry meterRegistry, CacheManager cacheManager) {\n+        this.dataSource = dataSource;\n+        this.recordFileRepository = recordFileRepository;\n+        this.entityRepository = entityRepository;\n+        sqlProperties = properties;\n+        executorService = Executors.newFixedThreadPool(properties.getThreads());\n+        this.cacheManager = cacheManager;\n+\n+        transactionPgCopy = new PgCopy<>(dataSource, Transaction.class, meterRegistry, sqlProperties.getBatchSize());\n+        cryptoTransferPgCopy = new PgCopy<>(dataSource, CryptoTransfer.class, meterRegistry, sqlProperties\n+                .getBatchSize());\n+        nonFeeTransferPgCopy = new PgCopy<>(dataSource, NonFeeTransfer.class, meterRegistry, sqlProperties\n+                .getBatchSize());\n+        fileDataPgCopy = new PgCopy<>(dataSource, FileData.class, meterRegistry, sqlProperties.getBatchSize());\n+        contractResultPgCopy = new PgCopy<>(dataSource, ContractResult.class, meterRegistry, sqlProperties\n+                .getBatchSize());\n+        liveHashPgCopy = new PgCopy<>(dataSource, LiveHash.class, meterRegistry, sqlProperties.getBatchSize());\n+        topicMessagePgCopy = new PgCopy<>(dataSource, TopicMessage.class, meterRegistry, sqlProperties.getBatchSize());\n+\n+        insertDuration = Timer.builder(\"hedera.mirror.importer.parser.record.entity.sql.insert\")", "originalCommit": "3e89d4147c94b87ed2d7d2034e57b66133d1a48b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQyMTc1Mw==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r458421753", "bodyText": "Since you manually get connection for pg_notify and turn off auto commit, you will need to close it and rollback appropriately. Or you can look into moving pg_notify to topicMessageRepository.notify(TopicMessage).", "author": "steven-sheehy", "createdAt": "2020-07-21T22:19:18Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -112,77 +183,23 @@ public void onEnd(RecordFile recordFile) {\n \n     @Override\n     public void onError() {\n-        try {\n-            if (connection != null) {\n-                connection.rollback();\n-                closeConnectionAndStatements();\n-            }\n-        } catch (SQLException e) {\n-            log.error(\"Exception while rolling transaction back\", e);\n-        }\n+        // no error handling", "originalCommit": "3e89d4147c94b87ed2d7d2034e57b66133d1a48b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTA1MDI4Ng==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r459050286", "bodyText": "Went with manual rollback", "author": "Nana-EC", "createdAt": "2020-07-22T20:03:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQyMTc1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQyMjYzNw==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r458422637", "bodyText": "Remove and re-throw", "author": "steven-sheehy", "createdAt": "2020-07-21T22:21:27Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -191,155 +208,72 @@ private void closeConnectionAndStatements() {\n     }\n \n     private void executeBatches() {\n+        Stopwatch stopwatch = Stopwatch.createStarted();\n         try {\n-            var transactions = executeBatch(sqlInsertTransaction);\n-            var entityIds = executeBatch(sqlInsertEntityId);\n-            var transferLists = executeBatch(sqlInsertTransferList);\n-            var nonFeeTransfers = executeBatch(sqlInsertNonFeeTransfers);\n-            var fileData = executeBatch(sqlInsertFileData);\n-            var contractResult = executeBatch(sqlInsertContractResult);\n-            var liveHashes = executeBatch(sqlInsertLiveHashes);\n-            var topicMessages = executeBatch(sqlInsertTopicMessage);\n-            var topicMessageNotifications = executeBatch(sqlNotifyTopicMessage);\n-            log.info(\"Inserted transactions: {}, entities: {}, transfer lists: {}, files: {}, contracts: {}, \" +\n-                            \"claims: {}, topic messages: {}, topic notifications: {}, non-fee transfers: {}\",\n-                    transactions, entityIds, transferLists, fileData, contractResult, liveHashes, topicMessages,\n-                    topicMessageNotifications, nonFeeTransfers);\n-        } catch (SQLException e) {\n-            log.error(\"Error committing sql insert batch \", e);\n-            throw new ParserSQLException(e);\n+            CompletableFuture.allOf(\n+                    CompletableFuture.runAsync(() -> transactionPgCopy.copy(transactions), executorService),\n+                    CompletableFuture.runAsync(() -> cryptoTransferPgCopy.copy(cryptoTransfers), executorService),\n+                    CompletableFuture.runAsync(() -> nonFeeTransferPgCopy.copy(nonFeeTransfers), executorService),\n+                    CompletableFuture.runAsync(() -> fileDataPgCopy.copy(fileData), executorService),\n+                    CompletableFuture.runAsync(() -> contractResultPgCopy.copy(contractResults), executorService),\n+                    CompletableFuture.runAsync(() -> liveHashPgCopy.copy(liveHashes), executorService),\n+                    CompletableFuture.runAsync(() -> topicMessagePgCopy.copy(topicMessages), executorService),\n+                    CompletableFuture\n+                            .runAsync(() -> entityIds.forEach(entityId -> {\n+                                if (entityCache.get(entityId.getId()) == null) {\n+                                    entityRepository.insertEntityId(entityId);\n+                                    entityCache.put(entityId.getId(), null);\n+                                }\n+                            }), executorService)\n+                    ,\n+                    CompletableFuture.runAsync(() -> {\n+                        try {\n+                            var topicMessageNotifications = executeBatch(sqlNotifyTopicMessage);\n+                            log.info(\"Inserted {} topic notifications\", topicMessageNotifications);\n+                        } catch (SQLException e) {\n+                            e.printStackTrace();", "originalCommit": "3e89d4147c94b87ed2d7d2034e57b66133d1a48b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQyMzA1Ng==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/853#discussion_r458423056", "bodyText": "The risk is here we continually re-send notifications if a parsing error, but since we filter for dupes on grpc guess that's ok.", "author": "steven-sheehy", "createdAt": "2020-07-21T22:22:28Z", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -191,155 +208,72 @@ private void closeConnectionAndStatements() {\n     }\n \n     private void executeBatches() {\n+        Stopwatch stopwatch = Stopwatch.createStarted();\n         try {\n-            var transactions = executeBatch(sqlInsertTransaction);\n-            var entityIds = executeBatch(sqlInsertEntityId);\n-            var transferLists = executeBatch(sqlInsertTransferList);\n-            var nonFeeTransfers = executeBatch(sqlInsertNonFeeTransfers);\n-            var fileData = executeBatch(sqlInsertFileData);\n-            var contractResult = executeBatch(sqlInsertContractResult);\n-            var liveHashes = executeBatch(sqlInsertLiveHashes);\n-            var topicMessages = executeBatch(sqlInsertTopicMessage);\n-            var topicMessageNotifications = executeBatch(sqlNotifyTopicMessage);\n-            log.info(\"Inserted transactions: {}, entities: {}, transfer lists: {}, files: {}, contracts: {}, \" +\n-                            \"claims: {}, topic messages: {}, topic notifications: {}, non-fee transfers: {}\",\n-                    transactions, entityIds, transferLists, fileData, contractResult, liveHashes, topicMessages,\n-                    topicMessageNotifications, nonFeeTransfers);\n-        } catch (SQLException e) {\n-            log.error(\"Error committing sql insert batch \", e);\n-            throw new ParserSQLException(e);\n+            CompletableFuture.allOf(\n+                    CompletableFuture.runAsync(() -> transactionPgCopy.copy(transactions), executorService),\n+                    CompletableFuture.runAsync(() -> cryptoTransferPgCopy.copy(cryptoTransfers), executorService),\n+                    CompletableFuture.runAsync(() -> nonFeeTransferPgCopy.copy(nonFeeTransfers), executorService),\n+                    CompletableFuture.runAsync(() -> fileDataPgCopy.copy(fileData), executorService),\n+                    CompletableFuture.runAsync(() -> contractResultPgCopy.copy(contractResults), executorService),\n+                    CompletableFuture.runAsync(() -> liveHashPgCopy.copy(liveHashes), executorService),\n+                    CompletableFuture.runAsync(() -> topicMessagePgCopy.copy(topicMessages), executorService),\n+                    CompletableFuture\n+                            .runAsync(() -> entityIds.forEach(entityId -> {\n+                                if (entityCache.get(entityId.getId()) == null) {\n+                                    entityRepository.insertEntityId(entityId);\n+                                    entityCache.put(entityId.getId(), null);\n+                                }\n+                            }), executorService)\n+                    ,\n+                    CompletableFuture.runAsync(() -> {\n+                        try {\n+                            var topicMessageNotifications = executeBatch(sqlNotifyTopicMessage);", "originalCommit": "3e89d4147c94b87ed2d7d2034e57b66133d1a48b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "d238394bc53e2a7b7df00eeac5b02c7f7966bcfa", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/d238394bc53e2a7b7df00eeac5b02c7f7966bcfa", "message": "Addressed comments on serialization\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>", "committedDate": "2020-07-22T19:59:34Z", "type": "commit"}, {"oid": "4f8137eb6755f8d26a4d3801a417316a51cc5ed8", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/4f8137eb6755f8d26a4d3801a417316a51cc5ed8", "message": "Signed-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>\nMerge large pg_notify load in", "committedDate": "2020-07-22T20:09:34Z", "type": "commit"}, {"oid": "bd1cb97851bf1f582c256d0e2d710699fe334da5", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/bd1cb97851bf1f582c256d0e2d710699fe334da5", "message": "Added rollback support for concurrent pgcopy inserts\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>", "committedDate": "2020-07-22T22:32:50Z", "type": "commit"}, {"oid": "2a8b97e9802a21e81a125f175e6fbfcab2610705", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/2a8b97e9802a21e81a125f175e6fbfcab2610705", "message": "Fixed minor log\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>", "committedDate": "2020-07-22T22:57:57Z", "type": "commit"}, {"oid": "3c8f7d6222ae2b54cc9b7112ad8299221a0af7cf", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/3c8f7d6222ae2b54cc9b7112ad8299221a0af7cf", "message": "Reverted entity persistence to prepared statement to preserve rollback coverage\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>", "committedDate": "2020-07-23T19:14:25Z", "type": "commit"}, {"oid": "8b0cd3a9dadf8481d7efa63aaaeb529cdd695793", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/8b0cd3a9dadf8481d7efa63aaaeb529cdd695793", "message": "A little cleanup\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>", "committedDate": "2020-07-23T19:32:50Z", "type": "commit"}]}