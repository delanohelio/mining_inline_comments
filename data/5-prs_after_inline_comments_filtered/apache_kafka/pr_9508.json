{"pr_number": 9508, "pr_title": "KAFKA-10648: Add Prefix Scan support to State Stores", "pr_createdAt": "2020-10-27T05:18:06Z", "pr_url": "https://github.com/apache/kafka/pull/9508", "timeline": [{"oid": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "url": "https://github.com/apache/kafka/commit/0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "message": "KAFKA-10648: Add Prefix Scan support to State Stores", "committedDate": "2020-12-01T13:03:55Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQxMTcwMQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r544411701", "bodyText": "This is not strictly necessary since you test the mock result you provide which has nothing to do with the code under test.", "author": "cadonna", "createdAt": "2020-12-16T15:53:05Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,23 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        expect(inner.prefixScan(KEY, stringSerializer))\n+                .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));\n+        init();\n+\n+        final KeyValueIterator<String, String> iterator = metered.prefixScan(KEY, stringSerializer);\n+        assertThat(iterator.next().value, equalTo(VALUE));\n+        assertFalse(iterator.hasNext());", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDUwMTQ1NQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r544501455", "bodyText": "Please change prefixScan to prefix-scan. For metrics name we do not use camel case. See put-all or put-if-absent for examples.", "author": "cadonna", "createdAt": "2020-12-16T17:50:51Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java", "diffHunk": "@@ -110,6 +110,14 @@ private StateStoreMetrics() {}\n     private static final String RANGE_AVG_LATENCY_DESCRIPTION = AVG_LATENCY_DESCRIPTION_PREFIX + RANGE_DESCRIPTION;\n     private static final String RANGE_MAX_LATENCY_DESCRIPTION = MAX_LATENCY_DESCRIPTION_PREFIX + RANGE_DESCRIPTION;\n \n+    private static final String PREFIX_SCAN = \"prefixScan\";\n+    private static final String PREFIX_SCAN_DESCRIPTION = \"calls to prefixScan\";", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDUwNDc5MQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r544504791", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return new MeteredKeyValueIterator(\n          \n          \n            \n                        wrapped().prefixScan(prefix, prefixKeySerializer),\n          \n          \n            \n                        prefixScanSensor\n          \n          \n            \n                    );\n          \n          \n            \n                    return new MeteredKeyValueIterator(wrapped().prefixScan(prefix, prefixKeySerializer), prefixScanSensor);", "author": "cadonna", "createdAt": "2020-12-16T17:55:34Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java", "diffHunk": "@@ -229,6 +232,15 @@ public V delete(final K key) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<K, V> prefixScan(final P prefix, final PS prefixKeySerializer) {\n+\n+        return new MeteredKeyValueIterator(\n+            wrapped().prefixScan(prefix, prefixKeySerializer),\n+            prefixScanSensor\n+        );", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDUwNTc1OA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r544505758", "bodyText": "nit: Please use 4 instead of 8 spaces indentation.", "author": "cadonna", "createdAt": "2020-12-16T17:56:59Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,23 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        expect(inner.prefixScan(KEY, stringSerializer))\n+                .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDUxNzAxMg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r544517012", "bodyText": "Sorry, if I haven't noticed it before. Could you move this method up before approximateNumEntries() to have all operations in one block?", "author": "cadonna", "createdAt": "2020-12-16T18:13:55Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -627,6 +645,11 @@ public void addToBatch(final byte[] key,\n         public void close() {\n             columnFamily.close();\n         }\n+\n+        @Override\n+        public KeyValueIterator<Bytes, byte[]> prefixScan(final Bytes prefix) {\n+            return new RocksDBPrefixIterator(name, db.newIterator(columnFamily), openIterators, prefix);\n+        }", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAzMzk5Ng==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545033996", "bodyText": "nit: I would just add parameter toInclusive to the existing method instead of creating an overload.", "author": "cadonna", "createdAt": "2020-12-17T11:56:54Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java", "diffHunk": "@@ -284,6 +284,10 @@ public boolean isEmpty() {\n         return keySetIterator(cache.navigableKeySet().subSet(from, true, to, true), true);\n     }\n \n+    synchronized Iterator<Bytes> keyRange(final Bytes from, final Bytes to, final boolean toInclusive) {\n+        return keySetIterator(cache.navigableKeySet().subSet(from, true, to, toInclusive), true);\n+    }", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545040031", "bodyText": "I actually do not completely understand why we need a specific iterator for the prefix scan. We could just as good extend RocksDBRangeIterator to consider or not consider the end result of the range. We can do that because those iterator implementations are internal and the public API does not care which iterator is used as long as it implements interface KeyValueIterator and the behavior is correct. Extending and re-using RocksDBRangeIterator would lead to less code to maintain. Note, I agree that we need the public method prefixScan(), but what the implementation uses internally is not relevant for the KIP as long as it is correct. Did I miss something that imposes the implementation for a separate iterator?", "author": "cadonna", "createdAt": "2020-12-17T12:07:11Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBPrefixIterator.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.state.internals;\n+\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.rocksdb.RocksIterator;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Set;\n+\n+class RocksDBPrefixIterator extends RocksDbIterator {", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk1ODk3MQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545958971", "bodyText": "Well the only reason i chose to add a separate iterator is that for prefix scan there is no end key which could be known upfront. We could pass a null end key but that's prohibited in RocksDBRangeIterator constructor and i don't think we should be changing that condition or we could pass in the same value or from and to and add a condition in makeNext() for prefix scan. i thought having a separate iterator might be cleaner. You have some other approach in mind?", "author": "vamossagar12", "createdAt": "2020-12-18T16:51:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk4NjIwNA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545986204", "bodyText": "Could you not use the same technique as you use in the in-memory key-value state store and the cache?\n        final Bytes from = Bytes.wrap(prefixKeySerializer.serialize(null, prefix));\n        final Bytes to = Bytes.increment(from);", "author": "cadonna", "createdAt": "2020-12-18T17:39:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjY0MzI3Mg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r546643272", "bodyText": "I tested with that approach. It worked for all cases apart from the abcd/abce case that you mentioned below. So, we will need a way to signal to the Iterator that for the case of range iteration for prefixScan, it should ignore the last key. I suppose we will have to add another flag similar to the ones used in in memory key-value store.\nBTW, the other reason why I created a separate prefix iterator was that there was an old unused prefix iterator, So, I thought of repurposing it :)", "author": "vamossagar12", "createdAt": "2020-12-21T11:02:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzQ4MzQxOQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r553483419", "bodyText": "hey @cadonna , did you get a chance to look at the comment?", "author": "vamossagar12", "createdAt": "2021-01-07T17:45:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTEyNDY3NQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r555124675", "bodyText": "Sorry for the late reply but I was on holidays.\n\nI suppose we will have to add another flag similar to the ones used in in memory key-value store.\n\nThis is what I meant with \"extend RocksDBRangeIterator\".\nI think less code is easier maintainable. In addition, I think the code is easier to follow if cache, in-memory, and RocksDB use the same technique for the prefix scan unless we have a technique with better performance that is specific to one of those components. So I would be in favor of just using RocksDBRangeIterator.", "author": "cadonna", "createdAt": "2021-01-11T15:25:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTEzNDg2Ng==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r555134866", "bodyText": "Thanks for the reply. Sure, I would make the relevant changes.", "author": "vamossagar12", "createdAt": "2021-01-11T15:39:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTE0MDg3Mg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r555140872", "bodyText": "Awesome!", "author": "cadonna", "createdAt": "2021-01-11T15:48:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjQ5Nzk2NA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r556497964", "bodyText": "As discussed, I have extended RocksDBRangeIterator to include the toInclusive flag.\n1 point to note here is that with this change, RocksDBPrefixIterator is no longer needed so I have removed it and it's relevant test case.\nAlso, the unit tests for prefix scan, i have a separate PR 9717 where i am writing the unit tests for RangeIterator. I plan to add the prefix scan cases there. would that be ok?", "author": "vamossagar12", "createdAt": "2021-01-13T12:53:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0NDQwMg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545044402", "bodyText": "If you consider my comment in NamedCache, you could also just call range() here. Or -- if you want to be more descriptive -- having prefixScan() and range() in this class calling a private overload of range() with a flag that excludes or includes the end of the range. That would deduplicate code.", "author": "cadonna", "createdAt": "2020-12-17T12:14:50Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java", "diffHunk": "@@ -201,6 +201,14 @@ public MemoryLRUCacheBytesIterator reverseAll(final String namespace) {\n         return new MemoryLRUCacheBytesIterator(cache.reverseAllKeys(), cache);\n     }\n \n+    public MemoryLRUCacheBytesIterator prefixScan(final String namespace, final Bytes from, final Bytes to) {\n+        final NamedCache cache = getCache(namespace);\n+        if (cache == null) {\n+            return new MemoryLRUCacheBytesIterator(Collections.emptyIterator(), new NamedCache(namespace, this.metrics));\n+        }\n+        return new MemoryLRUCacheBytesIterator(cache.keyRange(from, to, false), cache);", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1MzI5OQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545053299", "bodyText": "For new sensors like this, we only need to consider built-in metrics version LATEST. Hence, you should not call throughputAndLatencySensor(), but only call the parts that are relevant for LATEST and not for FROM_0100_TO_24. You also need to adapt the corresponding unit test for that. See also KIP-444 for more details.", "author": "cadonna", "createdAt": "2020-12-17T12:30:03Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java", "diffHunk": "@@ -307,6 +315,26 @@ public static Sensor rangeSensor(final String threadId,\n         );\n     }\n \n+    public static Sensor prefixScanSensor(final String threadId,\n+                                     final String taskId,\n+                                     final String storeType,\n+                                     final String storeName,\n+                                     final StreamsMetricsImpl streamsMetrics) {\n+        return throughputAndLatencySensor(", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1NTc5NA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545055794", "bodyText": "In all the tests for the prefix scan you should also verify boundary conditions, e.g., if you have a prefix abcd, you should verify that abce is not matched since this is the first key that should not be matched.", "author": "cadonna", "createdAt": "2020-12-17T12:34:18Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/CachingInMemoryKeyValueStoreTest.java", "diffHunk": "@@ -359,6 +361,34 @@ public void shouldReverseIterateOverRange() {\n         ), results);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjUwMzE0OA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r556503148", "bodyText": "i have added wherever the boundary conditions were not getting tested.", "author": "vamossagar12", "createdAt": "2021-01-13T13:02:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1NTc5NA=="}], "type": "inlineReview"}, {"oid": "1210e2475f23648bbcdd338fc9a7beb38541c0c4", "url": "https://github.com/apache/kafka/commit/1210e2475f23648bbcdd338fc9a7beb38541c0c4", "message": "KAFKA-10648: Add Prefix Scan support to State Stores", "committedDate": "2021-01-13T12:47:23Z", "type": "forcePushed"}, {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "url": "https://github.com/apache/kafka/commit/822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "message": "KAFKA-10648: Add Prefix Scan support to State Stores", "committedDate": "2021-01-13T13:01:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIwMTEwOQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558201109", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return new MeteredKeyValueIterator(wrapped().prefixScan(prefix,\n          \n          \n            \n                        prefixKeySerializer), prefixScanSensor);\n          \n          \n            \n                    return new MeteredKeyValueIterator(wrapped().prefixScan(prefix, prefixKeySerializer), prefixScanSensor);", "author": "cadonna", "createdAt": "2021-01-15T10:21:52Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java", "diffHunk": "@@ -229,6 +232,13 @@ public V delete(final K key) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<K, V> prefixScan(final P prefix, final PS prefixKeySerializer) {\n+\n+        return new MeteredKeyValueIterator(wrapped().prefixScan(prefix,\n+            prefixKeySerializer), prefixScanSensor);", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIxNzEyMQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558217121", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        return new RocksDBRangeIterator(\n          \n          \n            \n                            name,\n          \n          \n            \n                            db.newIterator(columnFamily),\n          \n          \n            \n                            openIterators,\n          \n          \n            \n                            prefix,\n          \n          \n            \n                            to,\n          \n          \n            \n                            true,\n          \n          \n            \n                            false);\n          \n          \n            \n                        return new RocksDBRangeIterator(\n          \n          \n            \n                            name,\n          \n          \n            \n                            db.newIterator(columnFamily),\n          \n          \n            \n                            openIterators,\n          \n          \n            \n                            prefix,\n          \n          \n            \n                            to,\n          \n          \n            \n                            true,\n          \n          \n            \n                            false\n          \n          \n            \n                        );", "author": "cadonna", "createdAt": "2021-01-15T10:33:38Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -594,6 +616,20 @@ public void prepareBatch(final List<KeyValue<Bytes, byte[]>> entries,\n             return new RocksDbIterator(name, innerIterWithTimestamp, openIterators, forward);\n         }\n \n+        @Override\n+        public KeyValueIterator<Bytes, byte[]> prefixScan(final Bytes prefix) {\n+            final Bytes to = Bytes.increment(prefix);\n+            return new RocksDBRangeIterator(\n+                name,\n+                db.newIterator(columnFamily),\n+                openIterators,\n+                prefix,\n+                to,\n+                true,\n+                false);", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIxNzM1NQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558217355", "bodyText": "Please remove empty line.", "author": "cadonna", "createdAt": "2021-01-15T10:33:50Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -594,6 +616,20 @@ public void prepareBatch(final List<KeyValue<Bytes, byte[]>> entries,\n             return new RocksDbIterator(name, innerIterWithTimestamp, openIterators, forward);\n         }\n \n+        @Override\n+        public KeyValueIterator<Bytes, byte[]> prefixScan(final Bytes prefix) {\n+            final Bytes to = Bytes.increment(prefix);\n+            return new RocksDBRangeIterator(\n+                name,\n+                db.newIterator(columnFamily),\n+                openIterators,\n+                prefix,\n+                to,\n+                true,\n+                false);\n+", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyMDE3OQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558220179", "bodyText": "I guess you forgot to remove this. \ud83d\ude42", "author": "cadonna", "createdAt": "2021-01-15T10:37:23Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -308,6 +309,24 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix,\n+                                                                                    final PS prefixKeySerializer) {\n+        Objects.requireNonNull(prefix, \"prefix cannot be null\");\n+        Objects.requireNonNull(prefixKeySerializer, \"prefixKeySerializer cannot be null\");\n+\n+        validateStoreOpen();\n+        final Bytes prefixBytes = Bytes.wrap(prefixKeySerializer.serialize(null, prefix));\n+\n+        /*final Bytes from = Bytes.wrap(prefixKeySerializer.serialize(null, prefix));\n+        final Bytes to = Bytes.increment(from);*/", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyMTk3NA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558221974", "bodyText": "Please remove empty line.", "author": "cadonna", "createdAt": "2021-01-15T10:40:29Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,130 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k1\")),\n+            stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+            stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k2\")),\n+            stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+            stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k3\")),\n+            stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abc\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abcd\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abce\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"prefix\", stringSerializer);\n+        final List<String> valuesWithPrefix = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+        assertThat(numberOfKeysReturned, is(3));\n+        assertThat(valuesWithPrefix.get(0), is(\"f\"));\n+        assertThat(valuesWithPrefix.get(1), is(\"d\"));\n+        assertThat(valuesWithPrefix.get(2), is(\"b\"));\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefixAsabcd = rocksDBStore.prefixScan(\"abcd\", stringSerializer);\n+        numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefixAsabcd.hasNext()) {\n+            keysWithPrefixAsabcd.next().key.get();\n+            numberOfKeysReturned++;\n+        }\n+\n+        assertThat(numberOfKeysReturned, is(1));\n+\n+    }\n+\n+    @Test\n+    public void shouldReturnUUIDsWithStringPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        final Serializer<UUID> uuidSerializer = Serdes.UUID().serializer();\n+        final UUID uuid1 = UUID.randomUUID();\n+        final UUID uuid2 = UUID.randomUUID();\n+        final String prefix = uuid1.toString().substring(0, 4);\n+        entries.add(new KeyValue<>(\n+            new Bytes(uuidSerializer.serialize(null, uuid1)),\n+            stringSerializer.serialize(null, \"a\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(uuidSerializer.serialize(null, uuid2)),\n+            stringSerializer.serialize(null, \"b\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(prefix, stringSerializer);\n+        final List<String> valuesWithPrefix = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+\n+        assertThat(numberOfKeysReturned, is(1));\n+        assertThat(valuesWithPrefix.get(0), is(\"a\"));\n+    }\n+\n+    @Test\n+    public void shouldReturnNoKeys() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"a\")),\n+            stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"b\")),\n+            stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"c\")),\n+            stringSerializer.serialize(null, \"e\")));\n+", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyMjA2Nw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558222067", "bodyText": "Please remove empty line.", "author": "cadonna", "createdAt": "2021-01-15T10:40:39Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,130 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k1\")),\n+            stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+            stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k2\")),\n+            stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+            stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k3\")),\n+            stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abc\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abcd\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abce\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"prefix\", stringSerializer);\n+        final List<String> valuesWithPrefix = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+        assertThat(numberOfKeysReturned, is(3));\n+        assertThat(valuesWithPrefix.get(0), is(\"f\"));\n+        assertThat(valuesWithPrefix.get(1), is(\"d\"));\n+        assertThat(valuesWithPrefix.get(2), is(\"b\"));\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefixAsabcd = rocksDBStore.prefixScan(\"abcd\", stringSerializer);\n+        numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefixAsabcd.hasNext()) {\n+            keysWithPrefixAsabcd.next().key.get();\n+            numberOfKeysReturned++;\n+        }\n+\n+        assertThat(numberOfKeysReturned, is(1));\n+\n+    }\n+\n+    @Test\n+    public void shouldReturnUUIDsWithStringPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        final Serializer<UUID> uuidSerializer = Serdes.UUID().serializer();\n+        final UUID uuid1 = UUID.randomUUID();\n+        final UUID uuid2 = UUID.randomUUID();\n+        final String prefix = uuid1.toString().substring(0, 4);\n+        entries.add(new KeyValue<>(\n+            new Bytes(uuidSerializer.serialize(null, uuid1)),\n+            stringSerializer.serialize(null, \"a\")));\n+", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyMzAwMg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558223002", "bodyText": "Could you split up this unit test into two, one for prefix prefix and one for prefix abcd? The former tests the general case whereas the latter tests the corner case where the the end of the range should not be returned.", "author": "cadonna", "createdAt": "2021-01-15T10:42:29Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,130 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyMzcwOA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558223708", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            true);\n          \n          \n            \n                            true\n          \n          \n            \n                        );", "author": "cadonna", "createdAt": "2021-01-15T10:43:50Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -580,7 +601,8 @@ public void prepareBatch(final List<KeyValue<Bytes, byte[]>> entries,\n                 openIterators,\n                 from,\n                 to,\n-                forward);\n+                forward,\n+                true);", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyNTAxNA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558225014", "bodyText": "I think here IntelliJ's formatting confused you. \ud83d\ude42\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            to,\n          \n          \n            \n                    true,\n          \n          \n            \n                  false);\n          \n          \n            \n                            to,\n          \n          \n            \n                            true,\n          \n          \n            \n                            false\n          \n          \n            \n                        );", "author": "cadonna", "createdAt": "2021-01-15T10:46:08Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStore.java", "diffHunk": "@@ -218,6 +219,19 @@ public void prepareBatch(final List<KeyValue<Bytes, byte[]>> entries,\n             return new RocksDBDualCFIterator(name, innerIterWithTimestamp, innerIterNoTimestamp, forward);\n         }\n \n+        @Override\n+        public KeyValueIterator<Bytes, byte[]> prefixScan(final Bytes prefix) {\n+            final Bytes to = Bytes.increment(prefix);\n+            return new RocksDBDualCFRangeIterator(\n+                name,\n+                db.newIterator(newColumnFamily),\n+                db.newIterator(oldColumnFamily),\n+                prefix,\n+                to,\n+        true,\n+      false);", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyNjg4Mw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558226883", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    try (final KeyValueIterator<Bytes, byte[]> it =\n          \n          \n            \n                                 rocksDBStore.prefixScan(\"key1\", stringSerializer)) {\n          \n          \n            \n                    try (final KeyValueIterator<Bytes, byte[]> it = rocksDBStore.prefixScan(\"key1\", stringSerializer)) {", "author": "cadonna", "createdAt": "2021-01-15T10:49:21Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStoreTest.java", "diffHunk": "@@ -337,6 +341,22 @@ private void iteratorsShouldNotMigrateData() {\n             }\n             assertFalse(it.hasNext());\n         }\n+\n+        try (final KeyValueIterator<Bytes, byte[]> it =\n+                     rocksDBStore.prefixScan(\"key1\", stringSerializer)) {", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIzMTk0NQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558231945", "bodyText": "Please remove empty line.", "author": "cadonna", "createdAt": "2021-01-15T10:58:55Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java", "diffHunk": "@@ -307,6 +315,34 @@ public static Sensor rangeSensor(final String threadId,\n         );\n     }\n \n+    public static Sensor prefixScanSensor(final String taskId,\n+                                          final String storeType,\n+                                          final String storeName,\n+                                          final StreamsMetricsImpl streamsMetrics) {\n+\n+        final String latencyMetricName = PREFIX_SCAN + LATENCY_SUFFIX;\n+        final Map<String, String> tagMap = streamsMetrics.storeLevelTagMap(taskId, storeType, storeName);\n+\n+        final Sensor sensor = streamsMetrics.storeLevelSensor(taskId, storeName, PREFIX_SCAN, RecordingLevel.DEBUG);\n+        addInvocationRateToSensor(\n+            sensor,\n+            STATE_STORE_LEVEL_GROUP,\n+            tagMap,\n+            PREFIX_SCAN,\n+            PREFIX_SCAN_RATE_DESCRIPTION\n+        );\n+", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIzMjIyMA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558232220", "bodyText": "This description is not needed since there is no total metric.", "author": "cadonna", "createdAt": "2021-01-15T10:59:24Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java", "diffHunk": "@@ -110,6 +110,14 @@ private StateStoreMetrics() {}\n     private static final String RANGE_AVG_LATENCY_DESCRIPTION = AVG_LATENCY_DESCRIPTION_PREFIX + RANGE_DESCRIPTION;\n     private static final String RANGE_MAX_LATENCY_DESCRIPTION = MAX_LATENCY_DESCRIPTION_PREFIX + RANGE_DESCRIPTION;\n \n+    private static final String PREFIX_SCAN = \"prefix-scan\";\n+    private static final String PREFIX_SCAN_DESCRIPTION = \"calls to prefix-scan\";\n+    private static final String PREFIX_SCAN_TOTAL_DESCRIPTION = TOTAL_DESCRIPTION + PREFIX_SCAN_DESCRIPTION;", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIzNDYzNw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558234637", "bodyText": "I do not think you need to put an entry if you use mocks.", "author": "cadonna", "createdAt": "2021-01-15T11:04:08Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,25 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        inner.put(eq(Bytes.increment(KEY_BYTES)), aryEq(VALUE_BYTES));\n+        expectLastCall();\n+        expect(inner.prefixScan(KEY, stringSerializer))\n+            .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));\n+        init();\n+\n+        metered.put(Bytes.increment(KEY_BYTES).toString(), VALUE);", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIzNDg3NA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558234874", "bodyText": "This line is not needed in this case. A method call without a return value is expected on the mock if you simply call the method on the mock in the replay phase.", "author": "cadonna", "createdAt": "2021-01-15T11:04:32Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,25 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        inner.put(eq(Bytes.increment(KEY_BYTES)), aryEq(VALUE_BYTES));\n+        expectLastCall();", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwNDAwMg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558304002", "bodyText": "I think we do not need this method. We can just call range() in CachingKeyValueStore.", "author": "cadonna", "createdAt": "2021-01-15T13:25:15Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java", "diffHunk": "@@ -218,6 +222,10 @@ public MemoryLRUCacheBytesIterator reverseAll(final String namespace) {\n         return new MemoryLRUCacheBytesIterator(cache.reverseAllKeys(), cache);\n     }\n \n+    public MemoryLRUCacheBytesIterator prefixScan(final String namespace, final Bytes from, final Bytes to) {", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwNzIyOA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558307228", "bodyText": "If we remove prefixScan(), you can remove this test, but we need a test for range() that excludes the end limit.", "author": "cadonna", "createdAt": "2021-01-15T13:30:37Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/ThreadCacheTest.java", "diffHunk": "@@ -285,6 +285,25 @@ public void shouldPeekAndIterateOverRange() {\n         assertEquals(5, bytesIndex);\n     }\n \n+    @Test\n+    public void testPrefixScan() {", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwNzc1Ng==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558307756", "bodyText": "Please remove empty line.", "author": "cadonna", "createdAt": "2021-01-15T13:31:32Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java", "diffHunk": "@@ -204,6 +204,42 @@ public void shouldGetRangeSensor() {\n         );\n     }\n \n+    @Test\n+    public void shouldGetPrefixScanSensor() {\n+        final String metricName = \"prefix-scan\";\n+        final String descriptionOfRate = \"The average number of calls to prefix-scan per second\";\n+        final String descriptionOfAvg = \"The average latency of calls to prefix-scan\";\n+        final String descriptionOfMax = \"The maximum latency of calls to prefix-scan\";\n+\n+        expect(streamsMetrics.storeLevelSensor(TASK_ID, STORE_NAME, metricName, RecordingLevel.DEBUG))\n+            .andReturn(expectedSensor);\n+        expect(streamsMetrics.storeLevelTagMap(TASK_ID, STORE_TYPE, STORE_NAME)).andReturn(storeTagMap);\n+\n+        StreamsMetricsImpl.addInvocationRateToSensor(\n+            expectedSensor,\n+            STORE_LEVEL_GROUP,\n+            storeTagMap,\n+            metricName,\n+            descriptionOfRate\n+        );\n+", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwNzc5MQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558307791", "bodyText": "Please remove empty line.", "author": "cadonna", "createdAt": "2021-01-15T13:31:37Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java", "diffHunk": "@@ -204,6 +204,42 @@ public void shouldGetRangeSensor() {\n         );\n     }\n \n+    @Test\n+    public void shouldGetPrefixScanSensor() {\n+        final String metricName = \"prefix-scan\";\n+        final String descriptionOfRate = \"The average number of calls to prefix-scan per second\";\n+        final String descriptionOfAvg = \"The average latency of calls to prefix-scan\";\n+        final String descriptionOfMax = \"The maximum latency of calls to prefix-scan\";\n+\n+        expect(streamsMetrics.storeLevelSensor(TASK_ID, STORE_NAME, metricName, RecordingLevel.DEBUG))\n+            .andReturn(expectedSensor);\n+        expect(streamsMetrics.storeLevelTagMap(TASK_ID, STORE_TYPE, STORE_NAME)).andReturn(storeTagMap);\n+", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwNzkyMw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558307923", "bodyText": "Please remove empty line.", "author": "cadonna", "createdAt": "2021-01-15T13:31:56Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java", "diffHunk": "@@ -204,6 +204,42 @@ public void shouldGetRangeSensor() {\n         );\n     }\n \n+    @Test\n+    public void shouldGetPrefixScanSensor() {\n+        final String metricName = \"prefix-scan\";\n+        final String descriptionOfRate = \"The average number of calls to prefix-scan per second\";\n+        final String descriptionOfAvg = \"The average latency of calls to prefix-scan\";\n+        final String descriptionOfMax = \"The maximum latency of calls to prefix-scan\";\n+", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "17be91a37214bf77430c65d9300a5120e4348df9", "url": "https://github.com/apache/kafka/commit/17be91a37214bf77430c65d9300a5120e4348df9", "message": "KAFKA-10648: Add Prefix Scan support to State Stores", "committedDate": "2021-01-20T12:27:29Z", "type": "commit"}, {"oid": "17be91a37214bf77430c65d9300a5120e4348df9", "url": "https://github.com/apache/kafka/commit/17be91a37214bf77430c65d9300a5120e4348df9", "message": "KAFKA-10648: Add Prefix Scan support to State Stores", "committedDate": "2021-01-20T12:27:29Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDM5NDQ3Ng==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564394476", "bodyText": "This method needs unit testing. Try to use a mock for the cache in the test.", "author": "cadonna", "createdAt": "2021-01-26T10:11:07Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/CachingKeyValueStore.java", "diffHunk": "@@ -291,6 +292,16 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         return new MergedSortedCacheKeyValueBytesStoreIterator(cacheIterator, storeIterator, true);\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjIzNDAzNA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r566234034", "bodyText": "Similar comment as below. Unit tests are in CachingInMemoryKeyValueStoreTest which already extends AbstractKeyValueStoreTest and creates an in memory cache store.", "author": "vamossagar12", "createdAt": "2021-01-28T16:31:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDM5NDQ3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzQ3ODg3MQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r567478871", "bodyText": "Oh, I see. I missed those. Sorry! That is fine then, although I think unit tests with mocks would be better.", "author": "cadonna", "createdAt": "2021-01-31T20:21:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDM5NDQ3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDE1NzcyNw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r570157727", "bodyText": "Actually, I had created another ticket to streamline tests for CachingKVStore: https://issues.apache.org/jira/browse/KAFKA-10788. @rohitrmd  had volunteered to take this up.", "author": "vamossagar12", "createdAt": "2021-02-04T11:41:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDM5NDQ3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwNTMwOA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564405308", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());\n          \n          \n            \n                    final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(hi.toString(), new StringSerializer());\n          \n      \n    \n    \n  \n\nIn such a way, you can reuse variable hi and there. Similar is true for my suggestions below.", "author": "cadonna", "createdAt": "2021-01-26T10:27:55Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java", "diffHunk": "@@ -196,6 +201,26 @@ public void shouldReturnValueOnGetWhenExists() {\n         assertThat(store.get(hello), equalTo(world));\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        store.put(hi, there);\n+        store.put(Bytes.increment(hi), world);\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwNjI2OA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564406268", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final List<String> keys = new ArrayList<>();\n          \n          \n            \n                    final List<String> values = new ArrayList<>();\n          \n          \n            \n                    final List<Bytes> keys = new ArrayList<>();\n          \n          \n            \n                    final List<Bytes> values = new ArrayList<>();", "author": "cadonna", "createdAt": "2021-01-26T10:29:29Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java", "diffHunk": "@@ -196,6 +201,26 @@ public void shouldReturnValueOnGetWhenExists() {\n         assertThat(store.get(hello), equalTo(world));\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        store.put(hi, there);\n+        store.put(Bytes.increment(hi), world);\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());\n+        final List<String> keys = new ArrayList<>();\n+        final List<String> values = new ArrayList<>();", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwNjUxMg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564406512", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        keys.add(next.key.toString());\n          \n          \n            \n                        values.add(new String(next.value));\n          \n          \n            \n                        keys.add(next.key);\n          \n          \n            \n                        values.add(Bytes.wrap(next.value));", "author": "cadonna", "createdAt": "2021-01-26T10:29:53Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java", "diffHunk": "@@ -196,6 +201,26 @@ public void shouldReturnValueOnGetWhenExists() {\n         assertThat(store.get(hello), equalTo(world));\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        store.put(hi, there);\n+        store.put(Bytes.increment(hi), world);\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());\n+        final List<String> keys = new ArrayList<>();\n+        final List<String> values = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            keys.add(next.key.toString());\n+            values.add(new String(next.value));", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwNjg4Mw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564406883", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    assertThat(keys, is(Collections.singletonList(\"hi\")));\n          \n          \n            \n                    assertThat(values, is(Collections.singletonList(\"there\")));\n          \n          \n            \n                    assertThat(keys, is(Collections.singletonList(hi)));\n          \n          \n            \n                    assertThat(values, is(Collections.singletonList(Bytes.wrap(there))));", "author": "cadonna", "createdAt": "2021-01-26T10:30:22Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java", "diffHunk": "@@ -196,6 +201,26 @@ public void shouldReturnValueOnGetWhenExists() {\n         assertThat(store.get(hello), equalTo(world));\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        store.put(hi, there);\n+        store.put(Bytes.increment(hi), world);\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());\n+        final List<String> keys = new ArrayList<>();\n+        final List<String> values = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            keys.add(next.key.toString());\n+            values.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+        assertThat(numberOfKeysReturned, is(1));\n+        assertThat(keys, is(Collections.singletonList(\"hi\")));\n+        assertThat(values, is(Collections.singletonList(\"there\")));", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwOTkzNw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564409937", "bodyText": "@vamossagar12 I can still not find the unit test for this method.", "author": "cadonna", "createdAt": "2021-01-26T10:35:35Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStore.java", "diffHunk": "@@ -103,6 +105,20 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjIzMzAzOQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r566233039", "bodyText": "For this, do you want me to add the test cases here?https://github.com/apache/kafka/blob/17be91a37214bf77430c65d9300a5120e4348df9/streams/src/test/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStoreTest.java\nThere are tests in CachingInMemoryKeyValueStoreTest, which is where the tests for other methods like range etc have been added.", "author": "vamossagar12", "createdAt": "2021-01-28T16:30:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwOTkzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzQ3OTU0Mw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r567479543", "bodyText": "I think those tests never call the prefixScan() on the underlying in-memory state store because all entries fit into the cache. You would need to add another test that flushes the cache before you call prefixScan(). I would prefer a test that directly tests the in-memory store without any cache in between.", "author": "cadonna", "createdAt": "2021-01-31T20:27:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwOTkzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDE1MzEyMg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r570153122", "bodyText": "Here is the new ticket: https://issues.apache.org/jira/browse/KAFKA-12289 and the PR for the ticket:\n#10052", "author": "vamossagar12", "createdAt": "2021-02-04T11:33:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwOTkzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQxNTY4Mw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564415683", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final KafkaMetric metric = metrics.metric(new MetricName(\"prefix-scan-rate\", STORE_LEVEL_GROUP, \"\", tags));\n          \n          \n            \n                    final KafkaMetric metric = metric(\"prefix-scan-rate\");", "author": "cadonna", "createdAt": "2021-01-26T10:44:39Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,22 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        expect(inner.prefixScan(KEY, stringSerializer))\n+            .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));\n+        init();\n+\n+        final KeyValueIterator<String, String> iterator = metered.prefixScan(KEY, stringSerializer);\n+        assertThat(iterator.next().value, equalTo(VALUE));\n+        iterator.close();\n+\n+        final KafkaMetric metric = metrics.metric(new MetricName(\"prefix-scan-rate\", STORE_LEVEL_GROUP, \"\", tags));", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQyMTkzNg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564421936", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final Sensor sensor =\n          \n          \n            \n                            StateStoreMetrics.prefixScanSensor(TASK_ID, STORE_TYPE, STORE_NAME, streamsMetrics);\n          \n          \n            \n                    final Sensor sensor = StateStoreMetrics.prefixScanSensor(TASK_ID, STORE_TYPE, STORE_NAME, streamsMetrics);", "author": "cadonna", "createdAt": "2021-01-26T10:55:02Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java", "diffHunk": "@@ -204,6 +204,39 @@ public void shouldGetRangeSensor() {\n         );\n     }\n \n+    @Test\n+    public void shouldGetPrefixScanSensor() {\n+        final String metricName = \"prefix-scan\";\n+        final String descriptionOfRate = \"The average number of calls to prefix-scan per second\";\n+        final String descriptionOfAvg = \"The average latency of calls to prefix-scan\";\n+        final String descriptionOfMax = \"The maximum latency of calls to prefix-scan\";\n+        expect(streamsMetrics.storeLevelSensor(TASK_ID, STORE_NAME, metricName, RecordingLevel.DEBUG))\n+            .andReturn(expectedSensor);\n+        expect(streamsMetrics.storeLevelTagMap(TASK_ID, STORE_TYPE, STORE_NAME)).andReturn(storeTagMap);\n+        StreamsMetricsImpl.addInvocationRateToSensor(\n+            expectedSensor,\n+            STORE_LEVEL_GROUP,\n+            storeTagMap,\n+            metricName,\n+            descriptionOfRate\n+        );\n+        StreamsMetricsImpl.addAvgAndMaxToSensor(\n+            expectedSensor,\n+            STORE_LEVEL_GROUP,\n+            storeTagMap,\n+            latencyMetricName(metricName),\n+            descriptionOfAvg,\n+            descriptionOfMax\n+        );\n+        replay(StreamsMetricsImpl.class, streamsMetrics);\n+\n+        final Sensor sensor =\n+                StateStoreMetrics.prefixScanSensor(TASK_ID, STORE_TYPE, STORE_NAME, streamsMetrics);", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "4a41206daa0ffbc7516059d29a7ddda109f64b5e", "url": "https://github.com/apache/kafka/commit/4a41206daa0ffbc7516059d29a7ddda109f64b5e", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>", "committedDate": "2021-01-28T16:16:52Z", "type": "commit"}, {"oid": "33be9113c6225063a1af489c5eca62f7645250ab", "url": "https://github.com/apache/kafka/commit/33be9113c6225063a1af489c5eca62f7645250ab", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>", "committedDate": "2021-01-28T16:17:12Z", "type": "commit"}, {"oid": "25980a0b3e6fdedf2fe707f78591dd5c9ba840c9", "url": "https://github.com/apache/kafka/commit/25980a0b3e6fdedf2fe707f78591dd5c9ba840c9", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>", "committedDate": "2021-01-28T16:17:21Z", "type": "commit"}, {"oid": "a2ea51336e4ea2010f1d93dd87d4b1526281cadb", "url": "https://github.com/apache/kafka/commit/a2ea51336e4ea2010f1d93dd87d4b1526281cadb", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>", "committedDate": "2021-01-28T16:17:32Z", "type": "commit"}, {"oid": "dddad17ad5102e937150bd7115c215b92807e734", "url": "https://github.com/apache/kafka/commit/dddad17ad5102e937150bd7115c215b92807e734", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>", "committedDate": "2021-01-28T16:17:49Z", "type": "commit"}, {"oid": "d2479a41c4d90e44c7dacb8028368cfa4a846cbb", "url": "https://github.com/apache/kafka/commit/d2479a41c4d90e44c7dacb8028368cfa4a846cbb", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>", "committedDate": "2021-01-28T16:18:01Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODUwODA3OQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r568508079", "bodyText": "To get rid of the test failure, you need to change this:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final KafkaMetric metric = metric(\"prefix-scan-rate\");\n          \n          \n            \n                    final KafkaMetric metric = metrics.metric(new MetricName(\"prefix-scan-rate\", STORE_LEVEL_GROUP, \"\", tags));\n          \n      \n    \n    \n  \n\nSorry, the failure of the test is my bad. I missed the issue with the different metrics versions when I requested to change this in a previous review.", "author": "cadonna", "createdAt": "2021-02-02T10:55:13Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,22 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        expect(inner.prefixScan(KEY, stringSerializer))\n+            .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));\n+        init();\n+\n+        final KeyValueIterator<String, String> iterator = metered.prefixScan(KEY, stringSerializer);\n+        assertThat(iterator.next().value, equalTo(VALUE));\n+        iterator.close();\n+\n+        final KafkaMetric metric = metric(\"prefix-scan-rate\");", "originalCommit": "d2479a41c4d90e44c7dacb8028368cfa4a846cbb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "8eca3c9c2852172896001178f8e7a115fd392aeb", "url": "https://github.com/apache/kafka/commit/8eca3c9c2852172896001178f8e7a115fd392aeb", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>", "committedDate": "2021-02-03T10:49:01Z", "type": "commit"}]}