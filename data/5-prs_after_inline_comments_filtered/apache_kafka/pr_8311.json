{"pr_number": 8311, "pr_title": "KAFKA-9434: automated protocol for alterReplicaLogDirs", "pr_createdAt": "2020-03-18T11:13:14Z", "pr_url": "https://github.com/apache/kafka/pull/8311", "timeline": [{"oid": "c2183171394262974cc77a39980c666da0970fdc", "url": "https://github.com/apache/kafka/commit/c2183171394262974cc77a39980c666da0970fdc", "message": "wip, need to fix KafkaApis/ReplicaManager", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "9a338902805a8a50a4edcc33e2ff984cf6b0920d", "url": "https://github.com/apache/kafka/commit/9a338902805a8a50a4edcc33e2ff984cf6b0920d", "message": "wip", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "21f7375d0f429e0ea9d8991c0b2820a296164ed6", "url": "https://github.com/apache/kafka/commit/21f7375d0f429e0ea9d8991c0b2820a296164ed6", "message": "Minor improvements", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "a42774804167c66163288431283465f0c8aeb67e", "url": "https://github.com/apache/kafka/commit/a42774804167c66163288431283465f0c8aeb67e", "message": "Review comments", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "f9069aa905f76d7efbce12126eed71b7d51d815c", "url": "https://github.com/apache/kafka/commit/f9069aa905f76d7efbce12126eed71b7d51d815c", "message": "review comments", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "d3fd28c7b5aeb837488fd5e7670a2d30d354875b", "url": "https://github.com/apache/kafka/commit/d3fd28c7b5aeb837488fd5e7670a2d30d354875b", "message": "Add test for KafkaAdminClient.alterReplicaLogDirs", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "ff5e4576fc5265de0aab24c6205981c94286ec11", "url": "https://github.com/apache/kafka/commit/ff5e4576fc5265de0aab24c6205981c94286ec11", "message": "Add test for KafkaAdminClient.alterReplicaLogDirs", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "a4bcdc04a580271a6db46afa4d077b9551e2f1f2", "url": "https://github.com/apache/kafka/commit/a4bcdc04a580271a6db46afa4d077b9551e2f1f2", "message": "Add test for AlterLogDirs RPCs to RequestResponseTests", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "e8893789d5d64419b3c55eefc0561af543b0de29", "url": "https://github.com/apache/kafka/commit/e8893789d5d64419b3c55eefc0561af543b0de29", "message": "Suggested code by @dajac", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "01a2980254d8b5ffccceb35e25ef40fe0b09f86f", "url": "https://github.com/apache/kafka/commit/01a2980254d8b5ffccceb35e25ef40fe0b09f86f", "message": "Add KafkaApisTest.testAlterReplicaLogDirs()", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "eaf80dc6b2fcc218da0d7c6b2933089b0aa5a6d1", "url": "https://github.com/apache/kafka/commit/eaf80dc6b2fcc218da0d7c6b2933089b0aa5a6d1", "message": "Fix test", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "0fc3bc37ad3406190015ce3f66673c4bf372a6f3", "url": "https://github.com/apache/kafka/commit/0fc3bc37ad3406190015ce3f66673c4bf372a6f3", "message": "review comments", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "1dffaa9fef1346c056fd57f7d427a81aead15d3e", "url": "https://github.com/apache/kafka/commit/1dffaa9fef1346c056fd57f7d427a81aead15d3e", "message": "Review comments; change handling of unexpected responses", "committedDate": "2020-04-27T13:08:54Z", "type": "commit"}, {"oid": "1dffaa9fef1346c056fd57f7d427a81aead15d3e", "url": "https://github.com/apache/kafka/commit/1dffaa9fef1346c056fd57f7d427a81aead15d3e", "message": "Review comments; change handling of unexpected responses", "committedDate": "2020-04-27T13:08:54Z", "type": "forcePushed"}, {"oid": "c82455ffe5b1f97c548ab9cb966ff78300f33b07", "url": "https://github.com/apache/kafka/commit/c82455ffe5b1f97c548ab9cb966ff78300f33b07", "message": "Review comments 2", "committedDate": "2020-04-27T13:47:54Z", "type": "commit"}, {"oid": "4c5d1e69c288cbf6aecb9864c0f859502e2ca301", "url": "https://github.com/apache/kafka/commit/4c5d1e69c288cbf6aecb9864c0f859502e2ca301", "message": "Exception on partial response", "committedDate": "2020-05-04T17:13:25Z", "type": "commit"}, {"oid": "e0fc9bf5e68896bd034ea8e1bc863db313ce7e98", "url": "https://github.com/apache/kafka/commit/e0fc9bf5e68896bd034ea8e1bc863db313ce7e98", "message": "Add extra test", "committedDate": "2020-05-05T16:09:37Z", "type": "commit"}, {"oid": "103a123ff12524aed9c45235a851c9df882ceb32", "url": "https://github.com/apache/kafka/commit/103a123ff12524aed9c45235a851c9df882ceb32", "message": "Two more tests", "committedDate": "2020-05-05T17:11:35Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYxNTkzOA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r420615938", "bodyText": "nit: Can we align it with the new above?", "author": "dajac", "createdAt": "2020-05-06T08:13:07Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -477,6 +484,27 @@ public void testCreateTopics() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testCreateTopicsPartialResponse() throws Exception {\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n+            env.kafkaClient().prepareResponse(body -> body instanceof CreateTopicsRequest,\n+                    prepareCreateTopicsResponse(\"myTopic\", Errors.NONE));\n+            CreateTopicsResult topicsResult = env.adminClient().createTopics(\n+                    asList(new NewTopic(\"myTopic\", Collections.singletonMap(0, asList(0, 1, 2))),\n+                            new NewTopic(\"myTopic2\", Collections.singletonMap(0, asList(0, 1, 2)))),", "originalCommit": "103a123ff12524aed9c45235a851c9df882ceb32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYxNzQwOQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r420617409", "bodyText": "nit: indentation is a bit inconsistent here.", "author": "dajac", "createdAt": "2020-05-06T08:15:47Z", "path": "clients/src/test/java/org/apache/kafka/common/requests/AlterReplicaLogDirsRequestTest.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.requests;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.LogDirNotFoundException;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData.AlterReplicaLogDir;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData.AlterReplicaLogDirCollection;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData.AlterReplicaLogDirTopic;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData.AlterReplicaLogDirTopicCollection;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData.AlterReplicaLogDirTopicResult;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.junit.Test;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.singletonList;\n+import static org.junit.Assert.assertEquals;\n+\n+public class AlterReplicaLogDirsRequestTest {\n+\n+    @Test\n+    public void testErrorResponse() {\n+        AlterReplicaLogDirsRequestData data = new AlterReplicaLogDirsRequestData()\n+                .setDirs(new AlterReplicaLogDirCollection(\n+                        singletonList(new AlterReplicaLogDir()\n+                                .setPath(\"/data0\")\n+                                .setTopics(new AlterReplicaLogDirTopicCollection(\n+                                        singletonList(new AlterReplicaLogDirTopic()\n+                                                .setName(\"topic\")\n+                                                .setPartitions(asList(0, 1, 2))).iterator()))).iterator()));\n+        AlterReplicaLogDirsResponse errorResponse = new AlterReplicaLogDirsRequest.Builder(data).build()\n+                .getErrorResponse(123, new LogDirNotFoundException(\"/data0\"));\n+        assertEquals(1, errorResponse.data().results().size());\n+        AlterReplicaLogDirTopicResult topicResponse = errorResponse.data().results().get(0);\n+        assertEquals(\"topic\", topicResponse.topicName());\n+        assertEquals(3, topicResponse.partitions().size());\n+        for (int i = 0; i < 3; i++) {\n+            assertEquals(i, topicResponse.partitions().get(i).partitionIndex());\n+            assertEquals(Errors.LOG_DIR_NOT_FOUND.code(), topicResponse.partitions().get(i).errorCode());\n+        }\n+    }\n+\n+    @Test\n+    public void testPartitionDir() {\n+        AlterReplicaLogDirsRequestData data = new AlterReplicaLogDirsRequestData()\n+                .setDirs(new AlterReplicaLogDirCollection(\n+                        asList(new AlterReplicaLogDir()\n+                                .setPath(\"/data0\")\n+                                .setTopics(new AlterReplicaLogDirTopicCollection(\n+                                        asList(new AlterReplicaLogDirTopic()\n+                                                .setName(\"topic\")\n+                                                .setPartitions(asList(0, 1)),\n+                                                new AlterReplicaLogDirTopic()\n+                                                        .setName(\"topic2\")\n+                                                        .setPartitions(asList(7))).iterator())),", "originalCommit": "103a123ff12524aed9c45235a851c9df882ceb32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYxNzk5Ng==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r420617996", "bodyText": "nit: Indentation of these two lines look weird.", "author": "dajac", "createdAt": "2020-05-06T08:16:54Z", "path": "clients/src/test/java/org/apache/kafka/common/requests/AlterReplicaLogDirsRequestTest.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.requests;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.LogDirNotFoundException;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData.AlterReplicaLogDir;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData.AlterReplicaLogDirCollection;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData.AlterReplicaLogDirTopic;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsRequestData.AlterReplicaLogDirTopicCollection;\n+import org.apache.kafka.common.message.AlterReplicaLogDirsResponseData.AlterReplicaLogDirTopicResult;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.junit.Test;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.singletonList;\n+import static org.junit.Assert.assertEquals;\n+\n+public class AlterReplicaLogDirsRequestTest {\n+\n+    @Test\n+    public void testErrorResponse() {\n+        AlterReplicaLogDirsRequestData data = new AlterReplicaLogDirsRequestData()\n+                .setDirs(new AlterReplicaLogDirCollection(\n+                        singletonList(new AlterReplicaLogDir()\n+                                .setPath(\"/data0\")\n+                                .setTopics(new AlterReplicaLogDirTopicCollection(\n+                                        singletonList(new AlterReplicaLogDirTopic()\n+                                                .setName(\"topic\")\n+                                                .setPartitions(asList(0, 1, 2))).iterator()))).iterator()));\n+        AlterReplicaLogDirsResponse errorResponse = new AlterReplicaLogDirsRequest.Builder(data).build()\n+                .getErrorResponse(123, new LogDirNotFoundException(\"/data0\"));\n+        assertEquals(1, errorResponse.data().results().size());\n+        AlterReplicaLogDirTopicResult topicResponse = errorResponse.data().results().get(0);\n+        assertEquals(\"topic\", topicResponse.topicName());\n+        assertEquals(3, topicResponse.partitions().size());\n+        for (int i = 0; i < 3; i++) {\n+            assertEquals(i, topicResponse.partitions().get(i).partitionIndex());\n+            assertEquals(Errors.LOG_DIR_NOT_FOUND.code(), topicResponse.partitions().get(i).errorCode());\n+        }\n+    }\n+\n+    @Test\n+    public void testPartitionDir() {\n+        AlterReplicaLogDirsRequestData data = new AlterReplicaLogDirsRequestData()\n+                .setDirs(new AlterReplicaLogDirCollection(\n+                        asList(new AlterReplicaLogDir()\n+                                .setPath(\"/data0\")\n+                                .setTopics(new AlterReplicaLogDirTopicCollection(\n+                                        asList(new AlterReplicaLogDirTopic()\n+                                                .setName(\"topic\")\n+                                                .setPartitions(asList(0, 1)),\n+                                                new AlterReplicaLogDirTopic()\n+                                                        .setName(\"topic2\")\n+                                                        .setPartitions(asList(7))).iterator())),\n+                                new AlterReplicaLogDir()\n+                                        .setPath(\"/data1\")\n+                                        .setTopics(new AlterReplicaLogDirTopicCollection(\n+                                                asList(new AlterReplicaLogDirTopic()\n+                                                                .setName(\"topic3\")\n+                                                                .setPartitions(asList(12))).iterator()))).iterator()));", "originalCommit": "103a123ff12524aed9c45235a851c9df882ceb32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYxODY0MA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r420618640", "bodyText": "nit: i would bring the second part on the first line.", "author": "dajac", "createdAt": "2020-05-06T08:18:14Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -2234,18 +2258,29 @@ public AlterReplicaLogDirsResult alterReplicaLogDirs(Map<TopicPartitionReplica,\n                 @Override\n                 public void handleResponse(AbstractResponse abstractResponse) {\n                     AlterReplicaLogDirsResponse response = (AlterReplicaLogDirsResponse) abstractResponse;\n-                    for (Map.Entry<TopicPartition, Errors> responseEntry: response.responses().entrySet()) {\n-                        TopicPartition tp = responseEntry.getKey();\n-                        Errors error = responseEntry.getValue();\n-                        TopicPartitionReplica replica = new TopicPartitionReplica(tp.topic(), tp.partition(), brokerId);\n-                        KafkaFutureImpl<Void> future = futures.get(replica);\n-                        if (future == null) {\n-                            handleFailure(new IllegalStateException(\n-                                \"The partition \" + tp + \" in the response from broker \" + brokerId + \" is not in the request\"));\n-                        } else if (error == Errors.NONE) {\n-                            future.complete(null);\n-                        } else {\n-                            future.completeExceptionally(error.exception());\n+                    for (AlterReplicaLogDirTopicResult topicResult: response.data().results()) {\n+                        for (AlterReplicaLogDirPartitionResult partitionResult: topicResult.partitions()) {\n+                            TopicPartitionReplica replica = new TopicPartitionReplica(topicResult.topicName(), partitionResult.partitionIndex(), brokerId);\n+                            KafkaFutureImpl<Void> future = futures.get(replica);\n+                            if (future == null) {\n+                                log.warn(\"The partition {} in the response from broker {}} is not in the request\",\n+                                        new TopicPartition(topicResult.topicName(), partitionResult.partitionIndex()),\n+                                        brokerId);\n+                            } else if (partitionResult.errorCode() == Errors.NONE.code()) {\n+                                future.complete(null);\n+                            } else {\n+                                future.completeExceptionally(Errors.forCode(partitionResult.errorCode()).exception());\n+                            }\n+                        }\n+                    }\n+                    // The server should send back a response for every replica. But do a sanity check anyway.\n+                    for (Map.Entry<TopicPartitionReplica, KafkaFutureImpl<Void>> entry : futures.entrySet()) {\n+                        TopicPartitionReplica replica = entry.getKey();\n+                        KafkaFutureImpl<Void> future = entry.getValue();\n+                        if (!future.isDone()\n+                                && replica.brokerId() == brokerId) {", "originalCommit": "103a123ff12524aed9c45235a851c9df882ceb32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYyMTYwNA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r420621604", "bodyText": "nit: . at the end to stay consistent with previous line.", "author": "dajac", "createdAt": "2020-05-06T08:23:43Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -1402,6 +1407,15 @@ int numPendingCalls() {\n         return runnable.pendingCalls.size();\n     }\n \n+    /**\n+     * Fail the given future when a response handler expected a result for an entity but no result was present.\n+     * @param future The future to fail.\n+     * @param message The message to fail the future with", "originalCommit": "103a123ff12524aed9c45235a851c9df882ceb32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYyNDI4MQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r420624281", "bodyText": "I am not entirely convinced by the name. At the end, the method complete a future with the given message. There is nothing specific to handling partial responses. I would rename it to something more generic or simply remove it as we don't gain much with it.", "author": "dajac", "createdAt": "2020-05-06T08:28:31Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -1402,6 +1407,15 @@ int numPendingCalls() {\n         return runnable.pendingCalls.size();\n     }\n \n+    /**\n+     * Fail the given future when a response handler expected a result for an entity but no result was present.\n+     * @param future The future to fail.\n+     * @param message The message to fail the future with\n+     */\n+    private void partialResponse(KafkaFutureImpl<?> future, String message) {", "originalCommit": "103a123ff12524aed9c45235a851c9df882ceb32", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDY0NDc5NQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r420644795", "bodyText": "I agree it's not doing much. What value it has is in trying to handle these cases in a consistent way, and being able to more easily discover/reason about the call sites. Maybe something like invalidBrokerResponse() would be a better name? But if you don't like that I'm happy to remove it.", "author": "tombentley", "createdAt": "2020-05-06T09:04:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYyNDI4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDY2NDMzMQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r420664331", "bodyText": "I see your point. I am definitely for reusing logic as much as possible for those cases. What about going an extra step and do something like this then (code not tested)? Just an idea...\npublic static <K, V> void completeUnrealizedFutures(\n            Map<K, KafkaFutureImpl<V>> futures,\n            Function<K, Boolean> filter,\n            Function<K, String> messageFormatter) {\n        for (Map.Entry<K, KafkaFutureImpl<V>> entry : futures.entrySet()) {\n            K key = entry.getKey();\n            KafkaFutureImpl<V> future = entry.getValue();\n            if (!future.isDone() && filter.apply(key)) {\n                future.completeExceptionally(new ApiException(messageFormatter.apply(key)));\n            }\n        }\n    }\n\n    public static  <K, V> void completeUnrealizedFutures(\n            Map<K, KafkaFutureImpl<V>> futures,\n            unction<K, String> messageFormatter) {\n        completeUnrealizedFutures(futures, (key) -> true, messageFormatter);\n    }\n\nIt would allow the following:\ncompleteUnrealizedFutures(topicFutures,\n     replica -> replica.brokerId() == brokerId,\n     topic -> \"The response from broker \" + brokerId + \" did not contain a result for replica \" + replica);\n\ncompleteUnrealizedFutures(topicFutures,\n     topic -> \"The server response did not contain a reference to node \" + topic);", "author": "dajac", "createdAt": "2020-05-06T09:39:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYyNDI4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDg2MTg2NA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r420861864", "bodyText": "That works for me, especially if this is something you're going to make more use of for the other response handlers in the admin client.", "author": "tombentley", "createdAt": "2020-05-06T15:00:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYyNDI4MQ=="}], "type": "inlineReview"}, {"oid": "0ebdc39ffae3c6cb1cea27cfcbc95f41c989746d", "url": "https://github.com/apache/kafka/commit/0ebdc39ffae3c6cb1cea27cfcbc95f41c989746d", "message": "Code review", "committedDate": "2020-05-06T09:02:00Z", "type": "commit"}, {"oid": "3d1aa65a267132bc7bdd00244525a288dd863dd4", "url": "https://github.com/apache/kafka/commit/3d1aa65a267132bc7bdd00244525a288dd863dd4", "message": "completeUnrealizedFutures", "committedDate": "2020-05-06T13:51:35Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjA5MDA2Mg==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r422090062", "bodyText": "nit: we can bring this back on the first line.", "author": "dajac", "createdAt": "2020-05-08T11:25:00Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -1479,13 +1512,9 @@ public void handleResponse(AbstractResponse abstractResponse) {\n                     }\n                 }\n                 // The server should send back a response for every topic. But do a sanity check anyway.\n-                for (Map.Entry<String, KafkaFutureImpl<TopicMetadataAndConfig>> entry : topicFutures.entrySet()) {\n-                    KafkaFutureImpl<TopicMetadataAndConfig> future = entry.getValue();\n-                    if (!future.isDone()) {\n-                        future.completeExceptionally(new ApiException(\"The server response did not \" +\n-                            \"contain a reference to node \" + entry.getKey()));\n-                    }\n-                }\n+                completeUnrealizedFutures(topicFutures,\n+                    topic -> \"The controller response \" +\n+                            \"did not contain a result for topic \" + topic);", "originalCommit": "3d1aa65a267132bc7bdd00244525a288dd863dd4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjA5MDA4NA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r422090084", "bodyText": "nit: we can bring this back on the first line.", "author": "dajac", "createdAt": "2020-05-08T11:25:05Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -1552,13 +1581,9 @@ void handleResponse(AbstractResponse abstractResponse) {\n                     }\n                 }\n                 // The server should send back a response for every topic. But do a sanity check anyway.\n-                for (Map.Entry<String, KafkaFutureImpl<Void>> entry : topicFutures.entrySet()) {\n-                    KafkaFutureImpl<Void> future = entry.getValue();\n-                    if (!future.isDone()) {\n-                        future.completeExceptionally(new ApiException(\"The server response did not \" +\n-                            \"contain a reference to node \" + entry.getKey()));\n-                    }\n-                }\n+                completeUnrealizedFutures(topicFutures,\n+                    topic -> \"The controller response \" +\n+                            \"did not contain a result for topic \" + topic);", "originalCommit": "3d1aa65a267132bc7bdd00244525a288dd863dd4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjA5MDM1Ng==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r422090356", "bodyText": "nit: We could break this long line.", "author": "dajac", "createdAt": "2020-05-08T11:25:48Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -2234,20 +2269,26 @@ public AlterReplicaLogDirsResult alterReplicaLogDirs(Map<TopicPartitionReplica,\n                 @Override\n                 public void handleResponse(AbstractResponse abstractResponse) {\n                     AlterReplicaLogDirsResponse response = (AlterReplicaLogDirsResponse) abstractResponse;\n-                    for (Map.Entry<TopicPartition, Errors> responseEntry: response.responses().entrySet()) {\n-                        TopicPartition tp = responseEntry.getKey();\n-                        Errors error = responseEntry.getValue();\n-                        TopicPartitionReplica replica = new TopicPartitionReplica(tp.topic(), tp.partition(), brokerId);\n-                        KafkaFutureImpl<Void> future = futures.get(replica);\n-                        if (future == null) {\n-                            handleFailure(new IllegalStateException(\n-                                \"The partition \" + tp + \" in the response from broker \" + brokerId + \" is not in the request\"));\n-                        } else if (error == Errors.NONE) {\n-                            future.complete(null);\n-                        } else {\n-                            future.completeExceptionally(error.exception());\n+                    for (AlterReplicaLogDirTopicResult topicResult: response.data().results()) {\n+                        for (AlterReplicaLogDirPartitionResult partitionResult: topicResult.partitions()) {\n+                            TopicPartitionReplica replica = new TopicPartitionReplica(topicResult.topicName(), partitionResult.partitionIndex(), brokerId);", "originalCommit": "3d1aa65a267132bc7bdd00244525a288dd863dd4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "6a794a656ad42f65dfff083a45a616513be72cd3", "url": "https://github.com/apache/kafka/commit/6a794a656ad42f65dfff083a45a616513be72cd3", "message": "Review comments + improvement", "committedDate": "2020-05-11T09:50:15Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTExOTMxNQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r435119315", "bodyText": "We can use assertThrows() here. Same below", "author": "mimaison", "createdAt": "2020-06-04T09:31:03Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -477,6 +484,27 @@ public void testCreateTopics() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testCreateTopicsPartialResponse() throws Exception {\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n+            env.kafkaClient().prepareResponse(body -> body instanceof CreateTopicsRequest,\n+                    prepareCreateTopicsResponse(\"myTopic\", Errors.NONE));\n+            CreateTopicsResult topicsResult = env.adminClient().createTopics(\n+                    asList(new NewTopic(\"myTopic\", Collections.singletonMap(0, asList(0, 1, 2))),\n+                           new NewTopic(\"myTopic2\", Collections.singletonMap(0, asList(0, 1, 2)))),\n+                    new CreateTopicsOptions().timeoutMs(10000));\n+            topicsResult.values().get(\"myTopic\").get();\n+            try {\n+                topicsResult.values().get(\"myTopic2\").get();\n+                fail(\"Expected an exception.\");\n+            } catch (ExecutionException e) {", "originalCommit": "6a794a656ad42f65dfff083a45a616513be72cd3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTE2MjE3NA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r435162174", "bodyText": "I don't think we can, at least not if we want to check that topicsResult.values().get(\"myTopic2\").get() throws an ExecutionException wrapping an ApiException. assertThrows() would only let us assert the outer exception type.", "author": "tombentley", "createdAt": "2020-06-04T10:46:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTExOTMxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTIxODU2MA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r435218560", "bodyText": "Sorry, I meant TestUtils.assertFutureThrows()", "author": "mimaison", "createdAt": "2020-06-04T12:35:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTExOTMxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTIyMjIwNA==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r435222204", "bodyText": "Ah, thanks! Now fixed.", "author": "tombentley", "createdAt": "2020-06-04T12:42:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTExOTMxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTEyMjg2Nw==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r435122867", "bodyText": "There's an extra }", "author": "mimaison", "createdAt": "2020-06-04T09:37:03Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -2234,20 +2251,27 @@ public AlterReplicaLogDirsResult alterReplicaLogDirs(Map<TopicPartitionReplica,\n                 @Override\n                 public void handleResponse(AbstractResponse abstractResponse) {\n                     AlterReplicaLogDirsResponse response = (AlterReplicaLogDirsResponse) abstractResponse;\n-                    for (Map.Entry<TopicPartition, Errors> responseEntry: response.responses().entrySet()) {\n-                        TopicPartition tp = responseEntry.getKey();\n-                        Errors error = responseEntry.getValue();\n-                        TopicPartitionReplica replica = new TopicPartitionReplica(tp.topic(), tp.partition(), brokerId);\n-                        KafkaFutureImpl<Void> future = futures.get(replica);\n-                        if (future == null) {\n-                            handleFailure(new IllegalStateException(\n-                                \"The partition \" + tp + \" in the response from broker \" + brokerId + \" is not in the request\"));\n-                        } else if (error == Errors.NONE) {\n-                            future.complete(null);\n-                        } else {\n-                            future.completeExceptionally(error.exception());\n+                    for (AlterReplicaLogDirTopicResult topicResult: response.data().results()) {\n+                        for (AlterReplicaLogDirPartitionResult partitionResult: topicResult.partitions()) {\n+                            TopicPartitionReplica replica = new TopicPartitionReplica(\n+                                    topicResult.topicName(), partitionResult.partitionIndex(), brokerId);\n+                            KafkaFutureImpl<Void> future = futures.get(replica);\n+                            if (future == null) {\n+                                log.warn(\"The partition {} in the response from broker {}} is not in the request\",", "originalCommit": "6a794a656ad42f65dfff083a45a616513be72cd3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "7b6f3c01f88055ac453f8a1288e9f3daf5c666dd", "url": "https://github.com/apache/kafka/commit/7b6f3c01f88055ac453f8a1288e9f3daf5c666dd", "message": "review comment", "committedDate": "2020-06-04T10:45:44Z", "type": "commit"}, {"oid": "66f796c147536847db2b90d88da648277c2e9767", "url": "https://github.com/apache/kafka/commit/66f796c147536847db2b90d88da648277c2e9767", "message": "TestUtils.assertFutureThrows", "committedDate": "2020-06-04T12:41:29Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI0MzI4Nw==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r435243287", "bodyText": "We can use TestUtils.assertFutureThrows() here too", "author": "mimaison", "createdAt": "2020-06-04T13:15:11Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -570,6 +592,27 @@ public void testDeleteTopics() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testDeleteTopicsPartialResponse() throws Exception {\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n+\n+            env.kafkaClient().prepareResponse(body -> body instanceof DeleteTopicsRequest,\n+                    prepareDeleteTopicsResponse(\"myTopic\", Errors.NONE));\n+            Map<String, KafkaFuture<Void>> values = env.adminClient().deleteTopics(asList(\"myTopic\", \"myOtherTopic\"),\n+                    new DeleteTopicsOptions()).values();\n+            values.get(\"myTopic\").get();\n+\n+            try {", "originalCommit": "66f796c147536847db2b90d88da648277c2e9767", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI0MzQ2NQ==", "url": "https://github.com/apache/kafka/pull/8311#discussion_r435243465", "bodyText": "we can use TestUtils.assertFutureThrows() here too", "author": "mimaison", "createdAt": "2020-06-04T13:15:26Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3382,6 +3425,90 @@ public void testAlterClientQuotas() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testAlterReplicaLogDirsSuccess() throws Exception {\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            createAlterLogDirsResponse(env, env.cluster().nodeById(0), Errors.NONE, 0);\n+            createAlterLogDirsResponse(env, env.cluster().nodeById(1), Errors.NONE, 0);\n+\n+            TopicPartitionReplica tpr0 = new TopicPartitionReplica(\"topic\", 0, 0);\n+            TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 0, 1);\n+\n+            Map<TopicPartitionReplica, String> logDirs = new HashMap<>();\n+            logDirs.put(tpr0, \"/data0\");\n+            logDirs.put(tpr1, \"/data1\");\n+            AlterReplicaLogDirsResult result = env.adminClient().alterReplicaLogDirs(logDirs);\n+            assertNull(result.values().get(tpr0).get());\n+            assertNull(result.values().get(tpr1).get());\n+        }\n+    }\n+\n+    @Test\n+    public void testAlterReplicaLogDirsLogDirNotFound() throws Exception {\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            createAlterLogDirsResponse(env, env.cluster().nodeById(0), Errors.NONE, 0);\n+            createAlterLogDirsResponse(env, env.cluster().nodeById(1), Errors.LOG_DIR_NOT_FOUND, 0);\n+\n+            TopicPartitionReplica tpr0 = new TopicPartitionReplica(\"topic\", 0, 0);\n+            TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 0, 1);\n+\n+            Map<TopicPartitionReplica, String> logDirs = new HashMap<>();\n+            logDirs.put(tpr0, \"/data0\");\n+            logDirs.put(tpr1, \"/data1\");\n+            AlterReplicaLogDirsResult result = env.adminClient().alterReplicaLogDirs(logDirs);\n+            assertNull(result.values().get(tpr0).get());\n+            TestUtils.assertFutureError(result.values().get(tpr1), LogDirNotFoundException.class);\n+        }\n+    }\n+\n+    @Test\n+    public void testAlterReplicaLogDirsUnrequested() throws Exception {\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            createAlterLogDirsResponse(env, env.cluster().nodeById(0), Errors.NONE, 1, 2);\n+\n+            TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 1, 0);\n+\n+            Map<TopicPartitionReplica, String> logDirs = new HashMap<>();\n+            logDirs.put(tpr1, \"/data1\");\n+            AlterReplicaLogDirsResult result = env.adminClient().alterReplicaLogDirs(logDirs);\n+            assertNull(result.values().get(tpr1).get());\n+        }\n+    }\n+\n+    @Test\n+    public void testAlterReplicaLogDirsPartialResponse() throws Exception {\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            createAlterLogDirsResponse(env, env.cluster().nodeById(0), Errors.NONE, 1);\n+\n+            TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 1, 0);\n+            TopicPartitionReplica tpr2 = new TopicPartitionReplica(\"topic\", 2, 0);\n+\n+            Map<TopicPartitionReplica, String> logDirs = new HashMap<>();\n+            logDirs.put(tpr1, \"/data1\");\n+            logDirs.put(tpr2, \"/data1\");\n+            AlterReplicaLogDirsResult result = env.adminClient().alterReplicaLogDirs(logDirs);\n+            assertNull(result.values().get(tpr1).get());\n+            try {", "originalCommit": "66f796c147536847db2b90d88da648277c2e9767", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "2af415b22d9671ee7aff320f104d975b084ea450", "url": "https://github.com/apache/kafka/commit/2af415b22d9671ee7aff320f104d975b084ea450", "message": "More TestUtils.assertFutureThrows", "committedDate": "2020-06-04T13:26:12Z", "type": "commit"}]}