{"pr_number": 9615, "pr_title": "KAFKA-10500: Add thread option", "pr_createdAt": "2020-11-18T18:54:55Z", "pr_url": "https://github.com/apache/kafka/pull/9615", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTQ2MTgwNA==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r529461804", "bodyText": "Please adjust indentation:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            mkMap(\n          \n          \n            \n                                    mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n          \n          \n            \n                                    mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId),\n          \n          \n            \n                                    mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()),\n          \n          \n            \n                                    mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2),\n          \n          \n            \n                                    mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class),\n          \n          \n            \n                                    mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class)\n          \n          \n            \n                            )\n          \n          \n            \n                        mkMap(\n          \n          \n            \n                            mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n          \n          \n            \n                            mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId),\n          \n          \n            \n                            mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()),\n          \n          \n            \n                            mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2),\n          \n          \n            \n                            mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class),\n          \n          \n            \n                            mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class)\n          \n          \n            \n                        )", "author": "cadonna", "createdAt": "2020-11-24T11:07:26Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/AdjustStreamThreadCountTest.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.Properties;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.purgeLocalStreamsState;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@Category(IntegrationTest.class)\n+public class AdjustStreamThreadCountTest {\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    private static String inputTopic;\n+    private static StreamsBuilder builder;\n+    private static Properties properties;\n+    private static String appId = \"\";\n+\n+    @Before\n+    public void setup() {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        appId = \"appId_\" + testId;\n+        inputTopic = \"input\" + testId;\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, inputTopic);\n+\n+        builder  = new StreamsBuilder();\n+        builder.stream(inputTopic);\n+\n+        properties  = mkObjectProperties(\n+                mkMap(\n+                        mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                        mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId),\n+                        mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()),\n+                        mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2),\n+                        mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class),\n+                        mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class)\n+                )", "originalCommit": "2d44bce5251dbb94c8751739b75424f9e043c0ba", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTQ2MTk3NQ==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r529461975", "bodyText": "wrong indentation", "author": "cadonna", "createdAt": "2020-11-24T11:07:45Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/AdjustStreamThreadCountTest.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.Properties;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.purgeLocalStreamsState;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@Category(IntegrationTest.class)\n+public class AdjustStreamThreadCountTest {\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    private static String inputTopic;\n+    private static StreamsBuilder builder;\n+    private static Properties properties;\n+    private static String appId = \"\";\n+\n+    @Before\n+    public void setup() {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        appId = \"appId_\" + testId;\n+        inputTopic = \"input\" + testId;\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, inputTopic);\n+\n+        builder  = new StreamsBuilder();\n+        builder.stream(inputTopic);\n+\n+        properties  = mkObjectProperties(\n+                mkMap(\n+                        mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                        mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId),\n+                        mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()),\n+                        mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2),\n+                        mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class),\n+                        mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class)\n+                )\n+        );\n+    }\n+\n+    @After\n+    public void teardown() throws IOException {\n+        purgeLocalStreamsState(properties);\n+    }\n+\n+    private void produceMessages(final long timestamp, final String streamOneInput, final String msg) {\n+        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n+                streamOneInput,\n+                Collections.singletonList(new KeyValue<>(\"1\", msg)),\n+                TestUtils.producerConfig(\n+                        CLUSTER.bootstrapServers(),\n+                        StringSerializer.class,\n+                        StringSerializer.class,\n+                        new Properties()),\n+                timestamp);", "originalCommit": "2d44bce5251dbb94c8751739b75424f9e043c0ba", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTQ2NDE4Mw==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r529464183", "bodyText": "We should wait for the new stream thread with a timeout, otherwise we risk that this test may become flaky.", "author": "cadonna", "createdAt": "2020-11-24T11:11:22Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/AdjustStreamThreadCountTest.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.Properties;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.purgeLocalStreamsState;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@Category(IntegrationTest.class)\n+public class AdjustStreamThreadCountTest {\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    private static String inputTopic;\n+    private static StreamsBuilder builder;\n+    private static Properties properties;\n+    private static String appId = \"\";\n+\n+    @Before\n+    public void setup() {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        appId = \"appId_\" + testId;\n+        inputTopic = \"input\" + testId;\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, inputTopic);\n+\n+        builder  = new StreamsBuilder();\n+        builder.stream(inputTopic);\n+\n+        properties  = mkObjectProperties(\n+                mkMap(\n+                        mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                        mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId),\n+                        mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()),\n+                        mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2),\n+                        mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class),\n+                        mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class)\n+                )\n+        );\n+    }\n+\n+    @After\n+    public void teardown() throws IOException {\n+        purgeLocalStreamsState(properties);\n+    }\n+\n+    private void produceMessages(final long timestamp, final String streamOneInput, final String msg) {\n+        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n+                streamOneInput,\n+                Collections.singletonList(new KeyValue<>(\"1\", msg)),\n+                TestUtils.producerConfig(\n+                        CLUSTER.bootstrapServers(),\n+                        StringSerializer.class,\n+                        StringSerializer.class,\n+                        new Properties()),\n+                timestamp);\n+    }\n+\n+    @Test\n+    public void shouldAddStreamThread() throws Exception {\n+        try (final KafkaStreams kafkaStreams = new KafkaStreams(builder.build(), properties)) {\n+            StreamsTestUtils.startKafkaStreamsAndWaitForRunningState(kafkaStreams);\n+            final int oldThreadCount = kafkaStreams.localThreadsMetadata().size();\n+\n+            kafkaStreams.addStreamThread();\n+            produceMessages(0L, inputTopic, \"A\");\n+\n+            assertThat(kafkaStreams.localThreadsMetadata().size(), equalTo(oldThreadCount + 1));", "originalCommit": "2d44bce5251dbb94c8751739b75424f9e043c0ba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTcxNjE2OQ==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r529716169", "bodyText": "we can wait for it to be added to the thread meta data. I assume that is what you mean", "author": "wcarlson5", "createdAt": "2020-11-24T16:36:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTQ2NDE4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTQ2NDcxMw==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r529464713", "bodyText": "Do we need this line?", "author": "cadonna", "createdAt": "2020-11-24T11:12:09Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/AdjustStreamThreadCountTest.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.Properties;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.purgeLocalStreamsState;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@Category(IntegrationTest.class)\n+public class AdjustStreamThreadCountTest {\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    private static String inputTopic;\n+    private static StreamsBuilder builder;\n+    private static Properties properties;\n+    private static String appId = \"\";\n+\n+    @Before\n+    public void setup() {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        appId = \"appId_\" + testId;\n+        inputTopic = \"input\" + testId;\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, inputTopic);\n+\n+        builder  = new StreamsBuilder();\n+        builder.stream(inputTopic);\n+\n+        properties  = mkObjectProperties(\n+                mkMap(\n+                        mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                        mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId),\n+                        mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()),\n+                        mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2),\n+                        mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class),\n+                        mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class)\n+                )\n+        );\n+    }\n+\n+    @After\n+    public void teardown() throws IOException {\n+        purgeLocalStreamsState(properties);\n+    }\n+\n+    private void produceMessages(final long timestamp, final String streamOneInput, final String msg) {\n+        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n+                streamOneInput,\n+                Collections.singletonList(new KeyValue<>(\"1\", msg)),\n+                TestUtils.producerConfig(\n+                        CLUSTER.bootstrapServers(),\n+                        StringSerializer.class,\n+                        StringSerializer.class,\n+                        new Properties()),\n+                timestamp);\n+    }\n+\n+    @Test\n+    public void shouldAddStreamThread() throws Exception {\n+        try (final KafkaStreams kafkaStreams = new KafkaStreams(builder.build(), properties)) {\n+            StreamsTestUtils.startKafkaStreamsAndWaitForRunningState(kafkaStreams);\n+            final int oldThreadCount = kafkaStreams.localThreadsMetadata().size();\n+\n+            kafkaStreams.addStreamThread();\n+            produceMessages(0L, inputTopic, \"A\");", "originalCommit": "2d44bce5251dbb94c8751739b75424f9e043c0ba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTcwOTU5NQ==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r529709595", "bodyText": "no, we don't", "author": "wcarlson5", "createdAt": "2020-11-24T16:27:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTQ2NDcxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTUxODQwMw==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r529518403", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        synchronized (stateLock) {\n          \n          \n            \n                            if (isRunningOrRebalancing()) {\n          \n          \n            \n                                streamThread.start();\n          \n          \n            \n                            } else {\n          \n          \n            \n                                return Optional.empty();\n          \n          \n            \n                            }\n          \n          \n            \n                        }\n          \n          \n            \n                        return Optional.of(streamThread.getName());\n          \n          \n            \n                        synchronized (stateLock) {\n          \n          \n            \n                            if (isRunningOrRebalancing()) {\n          \n          \n            \n                                streamThread.start();\n          \n          \n            \n                                return Optional.of(streamThread.getName());\n          \n          \n            \n                            } else {\n          \n          \n            \n                                return Optional.empty();\n          \n          \n            \n                            }\n          \n          \n            \n                        }", "author": "cadonna", "createdAt": "2020-11-24T12:46:49Z", "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,19 +885,88 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        if (isRunningOrRebalancing()) {\n+            final int threadIdx = getNextThreadIndex();\n+            final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+            resizeThreadCache(cacheSizePerThread);\n+            final StreamThread streamThread = createStreamThread(cacheSizePerThread, threadIdx);\n+            streamThread.setStateListener(streamStateListener);\n+            synchronized (stateLock) {\n+                if (isRunningOrRebalancing()) {\n+                    streamThread.start();\n+                } else {\n+                    return Optional.empty();\n+                }\n+            }\n+            return Optional.of(streamThread.getName());", "originalCommit": "2d44bce5251dbb94c8751739b75424f9e043c0ba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTcxMzc2NA==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r529713764", "bodyText": "sure", "author": "wcarlson5", "createdAt": "2020-11-24T16:32:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTUxODQwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTg1ODQwMQ==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r529858401", "bodyText": "nit: If not all parameters fit on one line, we put each of them on a new line. Additionally we put also the closing parenthesis on a new line.\nnit: Lines should -- if possible -- not exceed 120 characters.\nnit: I like when tests are visually structured with one block for setup, the call under test, and one block for verifications.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        StreamsTestUtils.startKafkaStreamsAndWaitForRunningState(kafkaStreams);\n          \n          \n            \n                        final int oldThreadCount = kafkaStreams.localThreadsMetadata().size();\n          \n          \n            \n            \n          \n          \n            \n                        final Optional<String> name = kafkaStreams.addStreamThread();\n          \n          \n            \n                        assertThat(name, CoreMatchers.not(Optional.empty()));\n          \n          \n            \n                        TestUtils.waitForCondition(() -> kafkaStreams.localThreadsMetadata().stream().sequential().map(ThreadMetadata::threadName).anyMatch(t -> t.equals(name.orElse(\"\"))),\n          \n          \n            \n                            \"Wait for the thread to be added\");\n          \n          \n            \n                        assertThat(kafkaStreams.localThreadsMetadata().size(), equalTo(oldThreadCount + 1));\n          \n          \n            \n                        StreamsTestUtils.startKafkaStreamsAndWaitForRunningState(kafkaStreams);\n          \n          \n            \n                        final int oldThreadCount = kafkaStreams.localThreadsMetadata().size();\n          \n          \n            \n            \n          \n          \n            \n                        final Optional<String> name = kafkaStreams.addStreamThread();\n          \n          \n            \n                        \n          \n          \n            \n                        assertThat(name, CoreMatchers.not(Optional.empty()));\n          \n          \n            \n                        TestUtils.waitForCondition(\n          \n          \n            \n                            () -> kafkaStreams.localThreadsMetadata().stream().sequential()\n          \n          \n            \n                                .map(ThreadMetadata::threadName).anyMatch(t -> t.equals(name.orElse(\"\"))), \n          \n          \n            \n                            \"Wait for the thread to be added\"\n          \n          \n            \n                        );\n          \n          \n            \n                        assertThat(kafkaStreams.localThreadsMetadata().size(), equalTo(oldThreadCount + 1));", "author": "cadonna", "createdAt": "2020-11-24T20:28:53Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/AdjustStreamThreadCountTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.processor.ThreadMetadata;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.hamcrest.CoreMatchers;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.purgeLocalStreamsState;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@Category(IntegrationTest.class)\n+public class AdjustStreamThreadCountTest {\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    private static String inputTopic;\n+    private static StreamsBuilder builder;\n+    private static Properties properties;\n+    private static String appId = \"\";\n+\n+    @Before\n+    public void setup() {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        appId = \"appId_\" + testId;\n+        inputTopic = \"input\" + testId;\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, inputTopic);\n+\n+        builder  = new StreamsBuilder();\n+        builder.stream(inputTopic);\n+\n+        properties  = mkObjectProperties(\n+            mkMap(\n+                mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId),\n+                mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()),\n+                mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2),\n+                mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class),\n+                mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class)\n+            )\n+        );\n+    }\n+\n+    @After\n+    public void teardown() throws IOException {\n+        purgeLocalStreamsState(properties);\n+    }\n+\n+    @Test\n+    public void shouldAddStreamThread() throws Exception {\n+        try (final KafkaStreams kafkaStreams = new KafkaStreams(builder.build(), properties)) {\n+            StreamsTestUtils.startKafkaStreamsAndWaitForRunningState(kafkaStreams);\n+            final int oldThreadCount = kafkaStreams.localThreadsMetadata().size();\n+\n+            final Optional<String> name = kafkaStreams.addStreamThread();\n+            assertThat(name, CoreMatchers.not(Optional.empty()));\n+            TestUtils.waitForCondition(() -> kafkaStreams.localThreadsMetadata().stream().sequential().map(ThreadMetadata::threadName).anyMatch(t -> t.equals(name.orElse(\"\"))),\n+                \"Wait for the thread to be added\");\n+            assertThat(kafkaStreams.localThreadsMetadata().size(), equalTo(oldThreadCount + 1));", "originalCommit": "70d500aae259184538feb662047f75897bb89b4d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyNjIzMg==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533026232", "bodyText": "nit. remove unnecessary this.", "author": "mjsax", "createdAt": "2020-12-01T02:10:09Z", "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -398,6 +407,7 @@ public void setUncaughtExceptionHandler(final StreamsUncaughtExceptionHandler st\n         final Consumer<Throwable> handler = exception -> handleStreamsUncaughtException(exception, streamsUncaughtExceptionHandler);\n         synchronized (stateLock) {\n             if (state == State.CREATED) {\n+                this.streamsUncaughtExceptionHandler = handler;", "originalCommit": "e2130ad1193ea2de05814b1218f33f216a438b6c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU2ODYxMQ==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533568611", "bodyText": "the this. is necessary. the parameter is the same name", "author": "wcarlson5", "createdAt": "2020-12-01T16:55:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyNjIzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYyNjMyOA==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533626328", "bodyText": "Ah. Was missing that as you assign handler that is not a StreamsUncaughtExceptionHandler but a Consumer<Throwable>...", "author": "mjsax", "createdAt": "2020-12-01T18:21:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyNjIzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyODE1Ng==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533028156", "bodyText": "Nit: can we change the loop to int = 1; i <= numStreamThreads and just pass in i here?", "author": "mjsax", "createdAt": "2020-12-01T02:16:02Z", "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -846,43 +856,24 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n                 time,\n                 globalThreadId,\n                 delegatingStateRestoreListener,\n-                this::defaultStreamsUncaughtExceptionHandler\n+                streamsUncaughtExceptionHandler\n             );\n             globalThreadState = globalStreamThread.state();\n         }\n \n         // use client id instead of thread client id since this admin client may be shared among threads\n         adminClient = clientSupplier.getAdmin(config.getAdminConfigs(ClientUtils.getSharedAdminClientId(clientId)));\n \n-        final Map<Long, StreamThread.State> threadState = new HashMap<>(numStreamThreads);\n-        final ArrayList<StreamThreadStateStoreProvider> storeProviders = new ArrayList<>();\n+        threadState = new HashMap<>(numStreamThreads);\n+        storeProviders = new ArrayList<>();\n         for (int i = 0; i < numStreamThreads; i++) {\n-            final StreamThread streamThread = StreamThread.create(\n-                internalTopologyBuilder,\n-                config,\n-                clientSupplier,\n-                adminClient,\n-                processId,\n-                clientId,\n-                streamsMetrics,\n-                time,\n-                streamsMetadataState,\n-                cacheSizePerThread,\n-                stateDirectory,\n-                delegatingStateRestoreListener,\n-                i + 1,\n-                KafkaStreams.this::closeToError,\n-                this::defaultStreamsUncaughtExceptionHandler\n-            );\n-            threads.add(streamThread);\n-            threadState.put(streamThread.getId(), streamThread.state());\n-            storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+            createStreamThread(cacheSizePerThread, i + 1);", "originalCommit": "e2130ad1193ea2de05814b1218f33f216a438b6c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU3NDE3Mg==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533574172", "bodyText": "I think so", "author": "wcarlson5", "createdAt": "2020-12-01T17:02:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyODE1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyODQ3NA==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533028474", "bodyText": "Add missing <p> tag", "author": "mjsax", "createdAt": "2020-12-01T02:17:06Z", "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,19 +885,88 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *", "originalCommit": "e2130ad1193ea2de05814b1218f33f216a438b6c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyODUyNg==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533028526", "bodyText": "As above.", "author": "mjsax", "createdAt": "2020-12-01T02:17:15Z", "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,19 +885,88 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *", "originalCommit": "e2130ad1193ea2de05814b1218f33f216a438b6c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyODcwOA==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533028708", "bodyText": "Should we link to StreamConfig instead?", "author": "mjsax", "createdAt": "2020-12-01T02:17:54Z", "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,19 +885,88 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.", "originalCommit": "e2130ad1193ea2de05814b1218f33f216a438b6c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU3ODQ4Ng==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533578486", "bodyText": "I think that makes more sense.", "author": "wcarlson5", "createdAt": "2020-12-01T17:08:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyODcwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyOTQ4NA==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533029484", "bodyText": "Can we create the StreamStateListener before we call createStreamThread and do setStateListener within createStreamThread ? If yes, we also don't need to call setStateListener within addStreamThread", "author": "mjsax", "createdAt": "2020-12-01T02:20:24Z", "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -846,43 +856,24 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n                 time,\n                 globalThreadId,\n                 delegatingStateRestoreListener,\n-                this::defaultStreamsUncaughtExceptionHandler\n+                streamsUncaughtExceptionHandler\n             );\n             globalThreadState = globalStreamThread.state();\n         }\n \n         // use client id instead of thread client id since this admin client may be shared among threads\n         adminClient = clientSupplier.getAdmin(config.getAdminConfigs(ClientUtils.getSharedAdminClientId(clientId)));\n \n-        final Map<Long, StreamThread.State> threadState = new HashMap<>(numStreamThreads);\n-        final ArrayList<StreamThreadStateStoreProvider> storeProviders = new ArrayList<>();\n+        threadState = new HashMap<>(numStreamThreads);\n+        storeProviders = new ArrayList<>();\n         for (int i = 0; i < numStreamThreads; i++) {\n-            final StreamThread streamThread = StreamThread.create(\n-                internalTopologyBuilder,\n-                config,\n-                clientSupplier,\n-                adminClient,\n-                processId,\n-                clientId,\n-                streamsMetrics,\n-                time,\n-                streamsMetadataState,\n-                cacheSizePerThread,\n-                stateDirectory,\n-                delegatingStateRestoreListener,\n-                i + 1,\n-                KafkaStreams.this::closeToError,\n-                this::defaultStreamsUncaughtExceptionHandler\n-            );\n-            threads.add(streamThread);\n-            threadState.put(streamThread.getId(), streamThread.state());\n-            storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+            createStreamThread(cacheSizePerThread, i + 1);\n         }\n \n         ClientMetrics.addNumAliveStreamThreadMetric(streamsMetrics, (metricsConfig, now) ->\n             Math.toIntExact(threads.stream().filter(thread -> thread.state().isAlive()).count()));\n \n-        final StreamStateListener streamStateListener = new StreamStateListener(threadState, globalThreadState);\n+        streamStateListener = new StreamStateListener(threadState, globalThreadState);", "originalCommit": "e2130ad1193ea2de05814b1218f33f216a438b6c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU3NjczNg==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533576736", "bodyText": "good idea. I don't know why the SteamStateListener is created after the stream threads are made but it seems to work.", "author": "wcarlson5", "createdAt": "2020-12-01T17:06:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyOTQ4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYyNzE0Mw==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533627143", "bodyText": "Maybe we had some cyclic dependency at some point in the past? Not sure.", "author": "mjsax", "createdAt": "2020-12-01T18:23:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyOTQ4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyOTk0Mg==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533029942", "bodyText": "Just to clarify for myself: if we don't start() the thread, no harm is done creating it? Or would we need to do some cleanup even if we don't start the thread?", "author": "mjsax", "createdAt": "2020-12-01T02:21:50Z", "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,19 +885,88 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        if (isRunningOrRebalancing()) {\n+            final int threadIdx = getNextThreadIndex();\n+            final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+            resizeThreadCache(cacheSizePerThread);\n+            final StreamThread streamThread = createStreamThread(cacheSizePerThread, threadIdx);\n+            streamThread.setStateListener(streamStateListener);\n+            synchronized (stateLock) {\n+                if (isRunningOrRebalancing()) {\n+                    streamThread.start();", "originalCommit": "e2130ad1193ea2de05814b1218f33f216a438b6c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU4Mjc2MA==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533582760", "bodyText": "I think I explain this above. But we can remove from the thread list.", "author": "wcarlson5", "createdAt": "2020-12-01T17:15:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyOTk0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYzNzQ1MQ==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533637451", "bodyText": "Maybe. We should ensure that we do proper cleanup for all cases.\nWhat make we wonder: I don't see any code (except the remove you added below) that would remove a StreamThread from the list? Will this be done in a different PR?", "author": "mjsax", "createdAt": "2020-12-01T18:39:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyOTk0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY1NjczMg==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533656732", "bodyText": "There will be two more cases of remove. In the replace thread option and in the remove thread option.\nI'm not really convinced it is necessary but I don't see a problem with re-resizing the cache if we do not start the thread. I don't think there will be any side affects as the client should be shutting down, but if we resize there should be a little extra info in the state and store providers but it would not get used", "author": "wcarlson5", "createdAt": "2020-12-01T19:11:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyOTk0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY3NTQyOQ==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533675429", "bodyText": "What about checking for the state and do the clean-up only if the state is not PENDING_SHUTDOWN and not ERROR and not NOT_RUNNING? In this way we are safe for future changes that break our assumption on state transitions and we make sure not to do unnecessary stuff when we are shutting down.", "author": "cadonna", "createdAt": "2020-12-01T19:43:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyOTk0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY3Njk3Ng==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533676976", "bodyText": "From running or rebalancing aren't those the only states we can get to?", "author": "wcarlson5", "createdAt": "2020-12-01T19:46:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyOTk0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY4NzAzMQ==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533687031", "bodyText": "Yes, currently this assumption is correct, but if the state transitions change in future, we would be safe if we do the cleanup.\nOn a second thought, we are probably not 100% safe because if a transition from NOT_RUNNING to RUNNING is added (or any other transition that goes from the above mentioned states to RUNNING or REBALANCING), we would still not do the clean up.", "author": "cadonna", "createdAt": "2020-12-01T20:03:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyOTk0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzcwNzY0MA==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533707640", "bodyText": "I think when a state transition is changed or add is when these changes should be made. Removing from the thread list is low cost as is increasing the size of the cache, so it won't be expensive to make these changes for all cases.\nI think the two good options we have is that we can move the cache resize and create thread into the stateLock or we can undo the changes we made if we have to abort starting the new thread.", "author": "wcarlson5", "createdAt": "2020-12-01T20:41:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyOTk0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDI2Mzc0Ng==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r534263746", "bodyText": "See my other comment #9615 (comment)", "author": "cadonna", "createdAt": "2020-12-02T15:36:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyOTk0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMDY0OA==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533030648", "bodyText": "Don't we need to get the stateLock as an outer most guard (and not check isRunningOrRebalancing() twice)? It seems weird to create a thread but later not start it and throw it away -- especially because we resize the caches (but also don't undo the resizing)?", "author": "mjsax", "createdAt": "2020-12-01T02:24:21Z", "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,19 +885,88 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        if (isRunningOrRebalancing()) {\n+            final int threadIdx = getNextThreadIndex();\n+            final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+            resizeThreadCache(cacheSizePerThread);\n+            final StreamThread streamThread = createStreamThread(cacheSizePerThread, threadIdx);\n+            streamThread.setStateListener(streamStateListener);\n+            synchronized (stateLock) {", "originalCommit": "e2130ad1193ea2de05814b1218f33f216a438b6c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU4NDk1Ng==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533584956", "bodyText": "If it is not running or rebalancing we after it was already running or rebalancing on line 930 we know the client has stopped. The number of threads will be reset to the config and everything will be rebuilt anyways, so changing the cache size should not matter.\nThe state lock is so that in between the second check and starting the thread the state does not change to pending shutdown or something else. I don't think its necessary to guard the whole method as the cache should be thrown out if it's not being started.", "author": "wcarlson5", "createdAt": "2020-12-01T17:18:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMDY0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYzNDA0OQ==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533634049", "bodyText": "I see. So we exploit that possible state transitions are limited. Thanks for explaining. Makes sense.", "author": "mjsax", "createdAt": "2020-12-01T18:34:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMDY0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMDkxNA==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533030914", "bodyText": "Why do we compute the names from scratch, but not incrementally maintain them as member variable?", "author": "mjsax", "createdAt": "2020-12-01T02:25:11Z", "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,19 +885,88 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        if (isRunningOrRebalancing()) {\n+            final int threadIdx = getNextThreadIndex();\n+            final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+            resizeThreadCache(cacheSizePerThread);\n+            final StreamThread streamThread = createStreamThread(cacheSizePerThread, threadIdx);\n+            streamThread.setStateListener(streamStateListener);\n+            synchronized (stateLock) {\n+                if (isRunningOrRebalancing()) {\n+                    streamThread.start();\n+                    return Optional.of(streamThread.getName());\n+                } else {\n+                    return Optional.empty();\n+                }\n+            }\n+        } else {\n+            return Optional.empty();\n+        }\n+    }\n+\n+    private int getNextThreadIndex() {\n+        final HashSet<String> names = new HashSet<>();", "originalCommit": "e2130ad1193ea2de05814b1218f33f216a438b6c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU4NzcxOQ==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533587719", "bodyText": "As threads are removed we want to reuse those names, so incrementing would not work for us. Maybe there is away to store a next name, but then the logic would have to be spread out in a few places and I prefer to just compute a few names.", "author": "wcarlson5", "createdAt": "2020-12-01T17:22:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMDkxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYzMTUzMQ==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533631531", "bodyText": "That is not what I meant. But it might not matter much anyway.\nWhile we need to loop over all used names in L951 below to reuse, we don't need to compute names from scratch but would just modify names each time we add/remove a thread. But it's not perf-critical so re-doing the computation is fine, too.", "author": "mjsax", "createdAt": "2020-12-01T18:30:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMDkxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY1Nzk4OA==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533657988", "bodyText": "I'll remove a few of the unnecessary + operations then", "author": "wcarlson5", "createdAt": "2020-12-01T19:13:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMDkxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMTIyOA==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533031228", "bodyText": "Why this check?", "author": "mjsax", "createdAt": "2020-12-01T02:26:12Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java", "diffHunk": "@@ -76,6 +76,9 @@ public void resize(final long newCacheSizeBytes) {\n         final boolean shrink = newCacheSizeBytes < maxCacheSizeBytes;\n         maxCacheSizeBytes = newCacheSizeBytes;\n         if (shrink) {\n+            if (caches.values().isEmpty()) {", "originalCommit": "e2130ad1193ea2de05814b1218f33f216a438b6c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU4ODcxMg==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533588712", "bodyText": "Apparently CircularIterators throw an error if they are made on empty lists. And if there are no caches to resize we don't need to bother with the rest of this.", "author": "wcarlson5", "createdAt": "2020-12-01T17:23:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMTIyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYzMjMzMg==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533632332", "bodyText": "I see -- so it's a bug fix on the side (not directly related to this RP) -- the original PR that added this method should have added this check.", "author": "mjsax", "createdAt": "2020-12-01T18:31:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMTIyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY1NzY1OQ==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533657659", "bodyText": "yeah, I didn't realize this was a problem, but when I added more test coverage it showed up", "author": "wcarlson5", "createdAt": "2020-12-01T19:13:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMTIyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMTY1Mg==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533031652", "bodyText": "shouldAddThreadWhenRunning", "author": "mjsax", "createdAt": "2020-12-01T02:27:33Z", "path": "streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java", "diffHunk": "@@ -588,6 +592,25 @@ public void testCloseIsIdempotent() {\n             closeCount, MockMetricsReporter.CLOSE_COUNT.get());\n     }\n \n+    @Test\n+    public void shouldAddThread() throws InterruptedException {", "originalCommit": "e2130ad1193ea2de05814b1218f33f216a438b6c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMTkzNA==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533031934", "bodyText": "shouldNotAddThreadWhenCreated\nShould we also close() KafkaStreams and check that adding a thread is not possible and/or even put the client into ERROR state and test?", "author": "mjsax", "createdAt": "2020-12-01T02:28:21Z", "path": "streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java", "diffHunk": "@@ -588,6 +592,25 @@ public void testCloseIsIdempotent() {\n             closeCount, MockMetricsReporter.CLOSE_COUNT.get());\n     }\n \n+    @Test\n+    public void shouldAddThread() throws InterruptedException {\n+        props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 1);\n+        final KafkaStreams streams = new KafkaStreams(getBuilderWithSource().build(), props, supplier, time);\n+        streams.start();\n+        final int oldSize = streams.threads.size();\n+        TestUtils.waitForCondition(() -> streams.state() == KafkaStreams.State.RUNNING, 15L, \"wait until running\");\n+        assertThat(streams.addStreamThread(), equalTo(Optional.of(\"newThread\")));\n+        assertThat(streams.threads.size(), equalTo(oldSize + 1));\n+    }\n+\n+    @Test\n+    public void shouldNotAddThread() {", "originalCommit": "e2130ad1193ea2de05814b1218f33f216a438b6c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU5MTMyNA==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533591324", "bodyText": "Good idea I'll add some tests.", "author": "wcarlson5", "createdAt": "2020-12-01T17:27:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMTkzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYyODM0Mg==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533628342", "bodyText": "i + 1 -> i\nWondering why this does not result in a test failure? (Or does it; Jenkins is still running.) -- Maybe we want to add a small test that verifies that we name threads correctly.", "author": "mjsax", "createdAt": "2020-12-01T18:24:56Z", "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -846,18 +856,37 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n                 time,\n                 globalThreadId,\n                 delegatingStateRestoreListener,\n-                this::defaultStreamsUncaughtExceptionHandler\n+                streamsUncaughtExceptionHandler\n             );\n             globalThreadState = globalStreamThread.state();\n         }\n \n         // use client id instead of thread client id since this admin client may be shared among threads\n         adminClient = clientSupplier.getAdmin(config.getAdminConfigs(ClientUtils.getSharedAdminClientId(clientId)));\n \n-        final Map<Long, StreamThread.State> threadState = new HashMap<>(numStreamThreads);\n-        final ArrayList<StreamThreadStateStoreProvider> storeProviders = new ArrayList<>();\n-        for (int i = 0; i < numStreamThreads; i++) {\n-            final StreamThread streamThread = StreamThread.create(\n+        threadState = new HashMap<>(numStreamThreads);\n+        storeProviders = new ArrayList<>();\n+        streamStateListener = new StreamStateListener(threadState, globalThreadState);\n+        if (hasGlobalTopology) {\n+            globalStreamThread.setStateListener(streamStateListener);\n+        }\n+        for (int i = 1; i <= numStreamThreads; i++) {\n+            createStreamThread(cacheSizePerThread, i + 1);", "originalCommit": "4cd470a4a5883481a5bd79d9b992a717470e5413", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY1MzE4MA==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533653180", "bodyText": "good catch. I don't think we make sure the thread index starts at 1. But let me fix that", "author": "wcarlson5", "createdAt": "2020-12-01T19:05:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYyODM0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg3ODc2Nw==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533878767", "bodyText": "We should also shutdown the thread if it doesn't get started, otherwise me may leak (consumer or producer) clients. But I'm actually not sure why we don't just do everything (resize cache, create thread) inside the synchronized block? I'm guessing it would deadlock due to locking on the statelock but can't we just synchronize on something else that wouldn't interfere with the StreamThread creation?", "author": "ableegoldman", "createdAt": "2020-12-02T03:51:04Z", "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -870,43 +899,73 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n                 cacheSizePerThread,\n                 stateDirectory,\n                 delegatingStateRestoreListener,\n-                i + 1,\n+                threadIdx,\n                 KafkaStreams.this::closeToError,\n-                this::defaultStreamsUncaughtExceptionHandler\n-            );\n-            threads.add(streamThread);\n-            threadState.put(streamThread.getId(), streamThread.state());\n-            storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n-        }\n+                streamsUncaughtExceptionHandler\n+        );\n+        streamThread.setStateListener(streamStateListener);\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n \n-        ClientMetrics.addNumAliveStreamThreadMetric(streamsMetrics, (metricsConfig, now) ->\n-            Math.toIntExact(threads.stream().filter(thread -> thread.state().isAlive()).count()));\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     * <p>\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@link StreamsConfig#CACHE_MAX_BYTES_BUFFERING_CONFIG}.\n+     * <p>\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        if (isRunningOrRebalancing()) {\n+            final int threadIdx = getNextThreadIndex();\n+            final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+            resizeThreadCache(cacheSizePerThread);\n+            final StreamThread streamThread = createStreamThread(cacheSizePerThread, threadIdx);\n+            synchronized (stateLock) {\n+                if (isRunningOrRebalancing()) {\n+                    streamThread.start();\n+                    return Optional.of(streamThread.getName());\n+                } else {\n+                    threads.remove(streamThread);", "originalCommit": "85f5ca44aaf6a9f7b3bd4e5ba098e8a67d2219d9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDI2MTg0Mw==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r534261843", "bodyText": "Good point about the shutdown of the stream thread!\nActually, I did not want to have everything in the synchronized block because I thought blocking the client state more than needed was not a good idea. I thought decreasing the size of the cache might be costly if the evicted records are forwarded downstream.\nNow that you mention to synchronize on a separate lock, I noticed that we probably need to put resize, start, and cleanup in the same synchronized block. The reason is that if two threads call addStreamThread() one after the other and the later thread passes\nfinal long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n\nbefore the earlier thread adds the new stream thread to threads in createStreamThread(), the later thread would compute the wrong cache size.\nSo, I am in favor of having a separate lock that just synchronizes the threads calling addStreamThread(). Maybe we can simply synchronize the whole method (which means to synchronize with start() and close()).\nStill a minor issue seems to be the synchronization betweenisRunningOrRebalancing() and streamThread.start(). If between these two calls the Streams client transits to ERROR (the global stream thread died) an IllegalStateException would be thrown from the StreamStateListener because the Streams client would try to transit from ERROR to REBALANCING. But I guess that would also happen if the Streams client transits to ERROR before the new stream thread transits to PARTITION_ASSIGNED and calls the StreamStateListener that would transit the Streams client to REBALANCING. So it needs to be fixed somewhere else.\nDid I miss something?", "author": "cadonna", "createdAt": "2020-12-02T15:34:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg3ODc2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDMxMzg0NQ==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r534313845", "bodyText": "Unfortunately I don't think we can shutdown a thread until we have started it.\nI don't think there should be a dead lock by just using the state lock around most of the method. It as mostly about cost. However I think that its is probably the safest way as it solves our problem about needing to remove a thread we have just created and we won't potentially waste time resizing the cache as such.\nIf we synchronize on a new lock we end up with all the problems we were trying to solve earlier anyways with the possible state changes and having to clean up un-started threads", "author": "wcarlson5", "createdAt": "2020-12-02T16:39:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg3ODc2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDMxOTUwMA==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r534319500", "bodyText": "Unfortunately I don't think we can shutdown a thread until we have started it.\n\nHave a look at \n  \n    \n      kafka/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java\n    \n    \n        Lines 976 to 983\n      in\n      aeeb7b2\n    \n    \n    \n    \n\n        \n          \n           public void shutdown() { \n        \n\n        \n          \n               log.info(\"Informed to shut down\"); \n        \n\n        \n          \n               final State oldState = setState(State.PENDING_SHUTDOWN); \n        \n\n        \n          \n               if (oldState == State.CREATED) { \n        \n\n        \n          \n                   // The thread may not have been started. Take responsibility for shutting down \n        \n\n        \n          \n                   completeShutdown(true); \n        \n\n        \n          \n               } \n        \n\n        \n          \n           }", "author": "cadonna", "createdAt": "2020-12-02T16:46:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg3ODc2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDMzOTI3NQ==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r534339275", "bodyText": "Oh okay, when I shutdown unstarted threads in a test I got a java.lang.IllegalStateException Unexpected state transition. But it looks like that is the client.\nI added a new lock for the add threads, and shutdown the thread. I think this address the problem you found with concurrent resizes. As well as @ableegoldman 's concerns", "author": "wcarlson5", "createdAt": "2020-12-02T17:13:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg3ODc2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg4MDE1NA==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533880154", "bodyText": "Can we also assert that the state gets to RUNNING after the new thread has joined", "author": "ableegoldman", "createdAt": "2020-12-02T03:56:01Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/AdjustStreamThreadCountTest.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.processor.ThreadMetadata;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.hamcrest.CoreMatchers;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.purgeLocalStreamsState;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@Category(IntegrationTest.class)\n+public class AdjustStreamThreadCountTest {\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    private static String inputTopic;\n+    private static StreamsBuilder builder;\n+    private static Properties properties;\n+    private static String appId = \"\";\n+\n+    @Before\n+    public void setup() {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        appId = \"appId_\" + testId;\n+        inputTopic = \"input\" + testId;\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, inputTopic);\n+\n+        builder  = new StreamsBuilder();\n+        builder.stream(inputTopic);\n+\n+        properties  = mkObjectProperties(\n+            mkMap(\n+                mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId),\n+                mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()),\n+                mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2),\n+                mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class),\n+                mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class)\n+            )\n+        );\n+    }\n+\n+    @After\n+    public void teardown() throws IOException {\n+        purgeLocalStreamsState(properties);\n+    }\n+\n+    @Test\n+    public void shouldAddStreamThread() throws Exception {\n+        try (final KafkaStreams kafkaStreams = new KafkaStreams(builder.build(), properties)) {\n+            StreamsTestUtils.startKafkaStreamsAndWaitForRunningState(kafkaStreams);\n+            final int oldThreadCount = kafkaStreams.localThreadsMetadata().size();\n+\n+            final Optional<String> name = kafkaStreams.addStreamThread();\n+\n+            assertThat(name, CoreMatchers.not(Optional.empty()));\n+            TestUtils.waitForCondition(\n+                () -> kafkaStreams.localThreadsMetadata().stream().sequential()\n+                        .map(ThreadMetadata::threadName).anyMatch(t -> t.equals(name.orElse(\"\"))),\n+                \"Wait for the thread to be added\"\n+            );\n+            assertThat(kafkaStreams.localThreadsMetadata().size(), equalTo(oldThreadCount + 1));\n+        }", "originalCommit": "85f5ca44aaf6a9f7b3bd4e5ba098e8a67d2219d9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM4MzkwOQ==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r535383909", "bodyText": "nit: If it happens that you need to push another commit, could you fix the indentation here? Sorry that I haven't noticed this before.", "author": "cadonna", "createdAt": "2020-12-03T16:26:24Z", "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -870,43 +900,75 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n                 cacheSizePerThread,\n                 stateDirectory,\n                 delegatingStateRestoreListener,\n-                i + 1,\n+                threadIdx,\n                 KafkaStreams.this::closeToError,\n-                this::defaultStreamsUncaughtExceptionHandler\n-            );\n-            threads.add(streamThread);\n-            threadState.put(streamThread.getId(), streamThread.state());\n-            storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n-        }\n+                streamsUncaughtExceptionHandler", "originalCommit": "6f6e589e0024878ebfd8b4657e8a04df5278bb90", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ3MTE0Ng==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r535471146", "bodyText": "ah good catch. the diff makes that hard to see as it was actually moved to a new method.", "author": "wcarlson5", "createdAt": "2020-12-03T18:17:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM4MzkwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM5NjE0OQ==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r535396149", "bodyText": "Sorry to bother you again with the synchronization on the stateLock, but could you explain why we still need it after we synchronize on newThread?", "author": "cadonna", "createdAt": "2020-12-03T16:41:36Z", "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -870,43 +900,75 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n                 cacheSizePerThread,\n                 stateDirectory,\n                 delegatingStateRestoreListener,\n-                i + 1,\n+                threadIdx,\n                 KafkaStreams.this::closeToError,\n-                this::defaultStreamsUncaughtExceptionHandler\n-            );\n-            threads.add(streamThread);\n-            threadState.put(streamThread.getId(), streamThread.state());\n-            storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n-        }\n+                streamsUncaughtExceptionHandler\n+        );\n+        streamThread.setStateListener(streamStateListener);\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n \n-        ClientMetrics.addNumAliveStreamThreadMetric(streamsMetrics, (metricsConfig, now) ->\n-            Math.toIntExact(threads.stream().filter(thread -> thread.state().isAlive()).count()));\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     * <p>\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@link StreamsConfig#CACHE_MAX_BYTES_BUFFERING_CONFIG}.\n+     * <p>\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        synchronized (newThread) {\n+            if (isRunningOrRebalancing()) {\n+                final int threadIdx = getNextThreadIndex();\n+                final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+                resizeThreadCache(cacheSizePerThread);\n+                final StreamThread streamThread = createStreamThread(cacheSizePerThread, threadIdx);\n+                synchronized (stateLock) {", "originalCommit": "6f6e589e0024878ebfd8b4657e8a04df5278bb90", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ2ODU3NA==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r535468574", "bodyText": "Well newThread only syncs the addThread method. There is still the race condition between the second check of is running and starting the thread. It seems like a bad idea to leave that open as it could cause thread state changes when there shouldn't be. Starting the thread is relatively low cost so this shouldn't have much impact perf wise.", "author": "wcarlson5", "createdAt": "2020-12-03T18:14:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM5NjE0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTUwMzQ1Mw==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r535503453", "bodyText": "Expanding on this, the problem in the shutdown thread. When the join only waits for alive threads, and to be alive the thread needs to be started.\nSo if in between the check and the start thread another thread transitions the state to NOT_RUNNING the thread will not join in the shutdown thread. Then when it continues it will start as it passed the check and we will have a thread running after the client is shutdown.\nThis would be extremely though race condition to find or reproduce so best to just avoid it.", "author": "wcarlson5", "createdAt": "2020-12-03T19:04:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM5NjE0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQwNzk5Mg==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r535407992", "bodyText": "Shouldn't that be int i = 1; i <= threads.size(); i++? Otherwise, we would look up *-StreamThread-0\" and we would not look up \"*-StreamThread-\" + threads.size().\nCould you add some tests that check the correct naming as @mjsax suggested?", "author": "cadonna", "createdAt": "2020-12-03T16:54:05Z", "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -870,43 +900,75 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n                 cacheSizePerThread,\n                 stateDirectory,\n                 delegatingStateRestoreListener,\n-                i + 1,\n+                threadIdx,\n                 KafkaStreams.this::closeToError,\n-                this::defaultStreamsUncaughtExceptionHandler\n-            );\n-            threads.add(streamThread);\n-            threadState.put(streamThread.getId(), streamThread.state());\n-            storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n-        }\n+                streamsUncaughtExceptionHandler\n+        );\n+        streamThread.setStateListener(streamStateListener);\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n \n-        ClientMetrics.addNumAliveStreamThreadMetric(streamsMetrics, (metricsConfig, now) ->\n-            Math.toIntExact(threads.stream().filter(thread -> thread.state().isAlive()).count()));\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     * <p>\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@link StreamsConfig#CACHE_MAX_BYTES_BUFFERING_CONFIG}.\n+     * <p>\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        synchronized (newThread) {\n+            if (isRunningOrRebalancing()) {\n+                final int threadIdx = getNextThreadIndex();\n+                final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+                resizeThreadCache(cacheSizePerThread);\n+                final StreamThread streamThread = createStreamThread(cacheSizePerThread, threadIdx);\n+                synchronized (stateLock) {\n+                    if (isRunningOrRebalancing()) {\n+                        streamThread.start();\n+                        return Optional.of(streamThread.getName());\n+                    } else {\n+                        streamThread.shutdown();\n+                        threads.remove(streamThread);\n+                        resizeThreadCache(getCacheSizePerThread(threads.size()));\n+                        return Optional.empty();\n+                    }\n+                }\n+            }\n+        }\n+        return Optional.empty();\n+    }\n \n-        final StreamStateListener streamStateListener = new StreamStateListener(threadState, globalThreadState);\n-        if (hasGlobalTopology) {\n-            globalStreamThread.setStateListener(streamStateListener);\n+    private int getNextThreadIndex() {\n+        final HashSet<String> names = new HashSet<>();\n+        for (final StreamThread streamThread: threads) {\n+            names.add(streamThread.getName());\n         }\n-        for (final StreamThread thread : threads) {\n-            thread.setStateListener(streamStateListener);\n+        final String baseName = clientId + \"-StreamThread-\";\n+        for (int i = 0; i < threads.size(); i++) {", "originalCommit": "6f6e589e0024878ebfd8b4657e8a04df5278bb90", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ3NzIyOQ==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r535477229", "bodyText": "Sure, I missed that suggestion", "author": "wcarlson5", "createdAt": "2020-12-03T18:24:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQwNzk5Mg=="}], "type": "inlineReview"}, {"oid": "dfbd9f47becff83cde4092e46ba180a9f6c49eb0", "url": "https://github.com/apache/kafka/commit/dfbd9f47becff83cde4092e46ba180a9f6c49eb0", "message": "ThreadCache Resizes", "committedDate": "2020-12-03T19:09:25Z", "type": "commit"}, {"oid": "dc4fcb8572b7dd4bb9e99a0879a4ea0a0ad28524", "url": "https://github.com/apache/kafka/commit/dc4fcb8572b7dd4bb9e99a0879a4ea0a0ad28524", "message": "comments", "committedDate": "2020-12-03T19:09:25Z", "type": "commit"}, {"oid": "5b2df7e44c52b54943fb09bd860215f803a7147e", "url": "https://github.com/apache/kafka/commit/5b2df7e44c52b54943fb09bd860215f803a7147e", "message": "improved names and checks", "committedDate": "2020-12-03T19:09:25Z", "type": "commit"}, {"oid": "82f307c87ea2484c6bf75f0f376dd620255fb246", "url": "https://github.com/apache/kafka/commit/82f307c87ea2484c6bf75f0f376dd620255fb246", "message": "make helper for cache size", "committedDate": "2020-12-03T19:09:25Z", "type": "commit"}, {"oid": "65f237dd9336ef3bacae8516b7d1b402242199f8", "url": "https://github.com/apache/kafka/commit/65f237dd9336ef3bacae8516b7d1b402242199f8", "message": "remove redundant check", "committedDate": "2020-12-03T19:09:25Z", "type": "commit"}, {"oid": "c0a94e3c006e044b96f3c00682fab938efdbea85", "url": "https://github.com/apache/kafka/commit/c0a94e3c006e044b96f3c00682fab938efdbea85", "message": "add thread", "committedDate": "2020-12-03T19:09:25Z", "type": "commit"}, {"oid": "ad38800664d81cbe0c57fabd8f6be4c980e28482", "url": "https://github.com/apache/kafka/commit/ad38800664d81cbe0c57fabd8f6be4c980e28482", "message": "add thread tests", "committedDate": "2020-12-03T19:09:25Z", "type": "commit"}, {"oid": "d43bb5e1eb2f0519cfa11e27198dc3531ca54115", "url": "https://github.com/apache/kafka/commit/d43bb5e1eb2f0519cfa11e27198dc3531ca54115", "message": "remove extra methods", "committedDate": "2020-12-03T19:09:25Z", "type": "commit"}, {"oid": "d8793d6115eec1445d15b03cb8bd202e098add8e", "url": "https://github.com/apache/kafka/commit/d8793d6115eec1445d15b03cb8bd202e098add8e", "message": "add line", "committedDate": "2020-12-03T19:09:25Z", "type": "commit"}, {"oid": "9f77626c0185e474f491cdbcb4327525817439c1", "url": "https://github.com/apache/kafka/commit/9f77626c0185e474f491cdbcb4327525817439c1", "message": "fix tests", "committedDate": "2020-12-03T19:09:25Z", "type": "commit"}, {"oid": "c42551a49580dd658f141a5b294a30df66eb4d79", "url": "https://github.com/apache/kafka/commit/c42551a49580dd658f141a5b294a30df66eb4d79", "message": "add start and gets name properly", "committedDate": "2020-12-03T19:09:25Z", "type": "commit"}, {"oid": "acd65f2cf50c3066e8b6841f4df660a577d31ddd", "url": "https://github.com/apache/kafka/commit/acd65f2cf50c3066e8b6841f4df660a577d31ddd", "message": "need to add an int test", "committedDate": "2020-12-03T19:09:25Z", "type": "commit"}, {"oid": "8fdaca11be2ac8c30179a9043a236e9a78913579", "url": "https://github.com/apache/kafka/commit/8fdaca11be2ac8c30179a9043a236e9a78913579", "message": "added int test", "committedDate": "2020-12-03T19:09:25Z", "type": "commit"}, {"oid": "a698a6826c5eecce1e6123fecabbebb4503197eb", "url": "https://github.com/apache/kafka/commit/a698a6826c5eecce1e6123fecabbebb4503197eb", "message": "address comments", "committedDate": "2020-12-03T19:09:25Z", "type": "commit"}, {"oid": "df25f11e53608e91c29573a199656fd7ee7cf36b", "url": "https://github.com/apache/kafka/commit/df25f11e53608e91c29573a199656fd7ee7cf36b", "message": "reduce line size", "committedDate": "2020-12-03T19:09:26Z", "type": "commit"}, {"oid": "6d394a3bf204905931c42d273a8b34bab9023c41", "url": "https://github.com/apache/kafka/commit/6d394a3bf204905931c42d273a8b34bab9023c41", "message": "address comments", "committedDate": "2020-12-03T19:09:26Z", "type": "commit"}, {"oid": "d818144143e5faaf61916c14c6b862b7a818709d", "url": "https://github.com/apache/kafka/commit/d818144143e5faaf61916c14c6b862b7a818709d", "message": "address comments pt 2", "committedDate": "2020-12-03T19:09:26Z", "type": "commit"}, {"oid": "99649b9faf95282e0c3a0f8c533e9cc99191b20f", "url": "https://github.com/apache/kafka/commit/99649b9faf95282e0c3a0f8c533e9cc99191b20f", "message": "wait for running in test", "committedDate": "2020-12-03T19:09:26Z", "type": "commit"}, {"oid": "17d8ca4ff8e91736de49ff8499ad57cd4486bfd4", "url": "https://github.com/apache/kafka/commit/17d8ca4ff8e91736de49ff8499ad57cd4486bfd4", "message": "shutdown thread", "committedDate": "2020-12-03T19:09:26Z", "type": "commit"}, {"oid": "af3e5674f77037796801afcd445e126c1aa7f6b0", "url": "https://github.com/apache/kafka/commit/af3e5674f77037796801afcd445e126c1aa7f6b0", "message": "Add tests for thread names", "committedDate": "2020-12-03T19:09:26Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI4NTUwNQ==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r536285505", "bodyText": "Why do we need this new lock-object? Would it not be simpler to just reuse stateLock ?", "author": "mjsax", "createdAt": "2020-12-04T18:11:50Z", "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -845,67 +856,118 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n                 time,\n                 globalThreadId,\n                 delegatingStateRestoreListener,\n-                this::defaultStreamsUncaughtExceptionHandler\n+                streamsUncaughtExceptionHandler\n             );\n             globalThreadState = globalStreamThread.state();\n         }\n \n         // use client id instead of thread client id since this admin client may be shared among threads\n         adminClient = clientSupplier.getAdmin(config.getAdminConfigs(ClientUtils.getSharedAdminClientId(clientId)));\n \n-        final Map<Long, StreamThread.State> threadState = new HashMap<>(numStreamThreads);\n-        final ArrayList<StreamThreadStateStoreProvider> storeProviders = new ArrayList<>();\n-        for (int i = 0; i < numStreamThreads; i++) {\n-            final StreamThread streamThread = StreamThread.create(\n-                internalTopologyBuilder,\n-                config,\n-                clientSupplier,\n-                adminClient,\n-                processId,\n-                clientId,\n-                streamsMetrics,\n-                time,\n-                streamsMetadataState,\n-                cacheSizePerThread,\n-                stateDirectory,\n-                delegatingStateRestoreListener,\n-                i + 1,\n-                KafkaStreams.this::closeToError,\n-                this::defaultStreamsUncaughtExceptionHandler\n-            );\n-            threads.add(streamThread);\n-            threadState.put(streamThread.getId(), streamThread.state());\n-            storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n-        }\n-\n-        ClientMetrics.addNumAliveStreamThreadMetric(streamsMetrics, (metricsConfig, now) ->\n-            Math.toIntExact(threads.stream().filter(thread -> thread.state().isAlive()).count()));\n-\n-        final StreamStateListener streamStateListener = new StreamStateListener(threadState, globalThreadState);\n+        threadState = new HashMap<>(numStreamThreads);\n+        storeProviders = new ArrayList<>();\n+        streamStateListener = new StreamStateListener(threadState, globalThreadState);\n         if (hasGlobalTopology) {\n             globalStreamThread.setStateListener(streamStateListener);\n         }\n-        for (final StreamThread thread : threads) {\n-            thread.setStateListener(streamStateListener);\n+        for (int i = 1; i <= numStreamThreads; i++) {\n+            createStreamThread(cacheSizePerThread, i);\n         }\n \n+        ClientMetrics.addNumAliveStreamThreadMetric(streamsMetrics, (metricsConfig, now) ->\n+            Math.toIntExact(threads.stream().filter(thread -> thread.state().isAlive()).count()));\n+\n         final GlobalStateStoreProvider globalStateStoreProvider = new GlobalStateStoreProvider(internalTopologyBuilder.globalStateStores());\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+            internalTopologyBuilder,\n+            config,\n+            clientSupplier,\n+            adminClient,\n+            processId,\n+            clientId,\n+            streamsMetrics,\n+            time,\n+            streamsMetadataState,\n+            cacheSizePerThread,\n+            stateDirectory,\n+            delegatingStateRestoreListener,\n+            threadIdx,\n+            KafkaStreams.this::closeToError,\n+            streamsUncaughtExceptionHandler\n+        );\n+        streamThread.setStateListener(streamStateListener);\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     * <p>\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@link StreamsConfig#CACHE_MAX_BYTES_BUFFERING_CONFIG}.\n+     * <p>\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        synchronized (changeThreadCount) {", "originalCommit": "af3e5674f77037796801afcd445e126c1aa7f6b0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjMxODczNw==", "url": "https://github.com/apache/kafka/pull/9615#discussion_r536318737", "bodyText": "We thought it would be better use a separate lock because it is serving a different purpose. It will also use used in remove thread. It might be simpler to reuse the statelock but I don\u2019t think that would be cleaner. We are really locking on the thread cache access and the thread indexes", "author": "wcarlson5", "createdAt": "2020-12-04T19:11:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI4NTUwNQ=="}], "type": "inlineReview"}]}