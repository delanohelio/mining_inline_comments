{"pr_number": 8451, "pr_title": "KAFKA-9835; Protect `FileRecords.slice` from concurrent write", "pr_createdAt": "2020-04-08T16:02:08Z", "pr_url": "https://github.com/apache/kafka/pull/8451", "timeline": [{"oid": "a362b45b814a5712861d7eca66f1453361aa1bff", "url": "https://github.com/apache/kafka/commit/a362b45b814a5712861d7eca66f1453361aa1bff", "message": "KAFKA-9835; Protect `FileRecords.slice` from concurrent write", "committedDate": "2020-04-08T15:58:54Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY0NjUzNg==", "url": "https://github.com/apache/kafka/pull/8451#discussion_r405646536", "bodyText": "Was the boundary check wrong? You changed >= to >.", "author": "ijuma", "createdAt": "2020-04-08T16:16:17Z", "path": "clients/src/main/java/org/apache/kafka/common/record/FileRecords.java", "diffHunk": "@@ -135,17 +135,20 @@ public void readInto(ByteBuffer buffer, int position) throws IOException {\n      * @return A sliced wrapper on this message set limited based on the given position and size\n      */\n     public FileRecords slice(int position, int size) throws IOException {\n+        // Cache current size in case concurrent write changes it\n+        int currentSizeInBytes = sizeInBytes();\n+\n         if (position < 0)\n             throw new IllegalArgumentException(\"Invalid position: \" + position + \" in read from \" + this);\n-        if (position > sizeInBytes() - start)\n+        if (position > currentSizeInBytes - start)\n             throw new IllegalArgumentException(\"Slice from position \" + position + \" exceeds end position of \" + this);\n         if (size < 0)\n             throw new IllegalArgumentException(\"Invalid size: \" + size + \" in read from \" + this);\n \n         int end = this.start + position + size;\n         // handle integer overflow or if end is beyond the end of the file\n-        if (end < 0 || end >= start + sizeInBytes())\n-            end = start + sizeInBytes();\n+        if (end < 0 || end > start + currentSizeInBytes)", "originalCommit": "a362b45b814a5712861d7eca66f1453361aa1bff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY1MTU0OQ==", "url": "https://github.com/apache/kafka/pull/8451#discussion_r405651549", "bodyText": "The common case that we would see this is when the limit matches the file end exactly. So changing this to > would also have fixed this problem for that case. I decided to remove it here though just because the equality check is redundant (if end == start + currentSizeInBytes, then there's no need to update it).", "author": "hachikuji", "createdAt": "2020-04-08T16:23:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY0NjUzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY1NzIxNA==", "url": "https://github.com/apache/kafka/pull/8451#discussion_r405657214", "bodyText": "Makes sense.", "author": "ijuma", "createdAt": "2020-04-08T16:32:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY0NjUzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY1ODAzMg==", "url": "https://github.com/apache/kafka/pull/8451#discussion_r405658032", "bodyText": "How often does this fail?", "author": "ijuma", "createdAt": "2020-04-08T16:33:34Z", "path": "clients/src/test/java/org/apache/kafka/common/record/FileRecordsTest.java", "diffHunk": "@@ -118,6 +121,36 @@ public void testIterationOverPartialAndTruncation() throws IOException {\n         testPartialWrite(6, fileRecords);\n     }\n \n+    @Test\n+    public void testSliceSizeLimitWithConcurrentWrite() throws Exception {\n+        FileRecords log = FileRecords.open(tempFile());\n+        ExecutorService executor = Executors.newFixedThreadPool(2);\n+        int maxSizeInBytes = 16384;\n+\n+        try {\n+            Future<Object> readerCompletion = executor.submit(() -> {\n+                while (log.sizeInBytes() < maxSizeInBytes) {\n+                    int currentSize = log.sizeInBytes();\n+                    FileRecords slice = log.slice(0, currentSize);\n+                    assertEquals(currentSize, slice.sizeInBytes());\n+                }\n+                return null;\n+            });\n+\n+            Future<Object> writerCompletion = executor.submit(() -> {\n+                while (log.sizeInBytes() < maxSizeInBytes) {\n+                    append(log, values);\n+                }\n+                return null;\n+            });\n+\n+            writerCompletion.get();\n+            readerCompletion.get();\n+        } finally {\n+            executor.shutdownNow();", "originalCommit": "a362b45b814a5712861d7eca66f1453361aa1bff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY1OTQzOQ==", "url": "https://github.com/apache/kafka/pull/8451#discussion_r405659439", "bodyText": "It fails consistently for me without the fix.", "author": "hachikuji", "createdAt": "2020-04-08T16:35:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY1ODAzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY5NzQ3Nw==", "url": "https://github.com/apache/kafka/pull/8451#discussion_r405697477", "bodyText": "Nice", "author": "ijuma", "createdAt": "2020-04-08T17:36:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY1ODAzMg=="}], "type": "inlineReview"}]}