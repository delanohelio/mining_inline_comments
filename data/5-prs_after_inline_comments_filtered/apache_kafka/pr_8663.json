{"pr_number": 8663, "pr_title": "KAFKA-9985: Sink connector may exhaust broker when writing in DLQ", "pr_createdAt": "2020-05-13T02:58:53Z", "pr_url": "https://github.com/apache/kafka/pull/8663", "timeline": [{"oid": "9806bd8a6ff94689223f2bf761d1cc92455c3659", "url": "https://github.com/apache/kafka/commit/9806bd8a6ff94689223f2bf761d1cc92455c3659", "message": "Validate topic name against DQL topic in Sink connector config", "committedDate": "2020-05-13T02:55:55Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU5MDQxOQ==", "url": "https://github.com/apache/kafka/pull/8663#discussion_r424590419", "bodyText": "I think we can use a utility method provided by the ConfigDef class here:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            String[] topics = props.get(SinkTask.TOPICS_CONFIG).split(\",\");\n          \n          \n            \n                            Arrays.setAll(topics, i -> topics[i].trim());\n          \n          \n            \n                            if (Arrays.asList(topics).contains(dlqTopic)) {\n          \n          \n            \n                            List<String> topics = (List<String>) ConfigDef.parseType(SinkTask.TOPICS_CONFIG, props.get(SinkTask.TOPICS_CONFIG), ConfigDef.Type.LIST);\n          \n          \n            \n                            if (topics.contains(dlqTopic)) {", "author": "C0urante", "createdAt": "2020-05-13T16:57:16Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SinkConnectorConfig.java", "diffHunk": "@@ -96,6 +99,26 @@ public static void validate(Map<String, String> props) {\n             throw new ConfigException(\"Must configure one of \" +\n                 SinkTask.TOPICS_CONFIG + \" or \" + SinkTask.TOPICS_REGEX_CONFIG);\n         }\n+\n+        if (hasDlqTopicConfig) {\n+            String dlqTopic = props.get(DLQ_TOPIC_NAME_CONFIG).trim();\n+            if (hasTopicsConfig) {\n+                String[] topics = props.get(SinkTask.TOPICS_CONFIG).split(\",\");\n+                Arrays.setAll(topics, i -> topics[i].trim());\n+                if (Arrays.asList(topics).contains(dlqTopic)) {", "originalCommit": "9806bd8a6ff94689223f2bf761d1cc92455c3659", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDYwMTIxNA==", "url": "https://github.com/apache/kafka/pull/8663#discussion_r424601214", "bodyText": "I agree but we have to take into account the following:\n\nThat source code is also in WorkerSinkTask class.\nConfigDef.parseType does not trim the split. I mean, the result of splitting \"topic1,topic2          \" is not the same that \" topic1        ,topic2\".\n\nI could include the changes you comment in this class and also in the WorkerSinkTask to be aligned. On the other hand, I would also fix the trim thing in ConfigDef.parseType(or maybe in another PR).\nWhat do you think?", "author": "mmolimar", "createdAt": "2020-05-13T17:15:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU5MDQxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDYwODkxNA==", "url": "https://github.com/apache/kafka/pull/8663#discussion_r424608914", "bodyText": "I think ConfigDef::parseType does do the trimming: \n  \n    \n      kafka/clients/src/main/java/org/apache/kafka/common/config/ConfigDef.java\n    \n    \n         Line 705\n      in\n      9bc96d5\n    \n    \n    \n    \n\n        \n          \n           return Arrays.asList(COMMA_WITH_WHITESPACE.split(trimmed, -1)); \n        \n    \n  \n\n\nChanging the WorkerSinkTask to align seems fine, although if we're reusing this logic all over the place it might make sense to just add a utility method like parseTopicsList(Map<String, Object>) to something like the SinkConnectorConfig class that grabs the value for SinkTask.TOPICS_CONFIG, passes it to ConfigDef::parseType, and returns the result. That way we can just use that whenever we need to parse a topics list from a sink connector config.", "author": "C0urante", "createdAt": "2020-05-13T17:27:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU5MDQxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDYxMTMxMA==", "url": "https://github.com/apache/kafka/pull/8663#discussion_r424611310", "bodyText": "ConfigDef.parseType does a trim but for the whole string, not the splits. This would be the issue in case you have spaces in each splitted topic name.", "author": "mmolimar", "createdAt": "2020-05-13T17:31:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU5MDQxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDYxMjg5OA==", "url": "https://github.com/apache/kafka/pull/8663#discussion_r424612898", "bodyText": "I think it does do trimming for the splits, based on the regex that's used for the splitting. I tried out a few local examples and they seemed to confirm this; parsing something like \" \\t foo \\t, \\tbar\\t , b a z \" returned the list [\"foo\", \"bar\", \"b a z\"] for me.", "author": "C0urante", "createdAt": "2020-05-13T17:34:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU5MDQxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDYxNzMwMA==", "url": "https://github.com/apache/kafka/pull/8663#discussion_r424617300", "bodyText": "Sorry about that. You're totally right.", "author": "mmolimar", "createdAt": "2020-05-13T17:41:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU5MDQxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDYxOTU2Mg==", "url": "https://github.com/apache/kafka/pull/8663#discussion_r424619562", "bodyText": "Haha, no worries! Does adding this logic to a reusable place like the SinkConnectorConfig sound good to you?", "author": "C0urante", "createdAt": "2020-05-13T17:45:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU5MDQxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDYyODQ4Mg==", "url": "https://github.com/apache/kafka/pull/8663#discussion_r424628482", "bodyText": "Sure!", "author": "mmolimar", "createdAt": "2020-05-13T18:00:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU5MDQxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDU5NDcwNQ==", "url": "https://github.com/apache/kafka/pull/8663#discussion_r424594705", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                throw new ConfigException(DLQ_TOPIC_NAME_CONFIG + \" has a topic name which matches in \" +\n          \n          \n            \n                                    SinkTask.TOPICS_REGEX_CONFIG);\n          \n          \n            \n                                throw new ConfigException(DLQ_TOPIC_NAME_CONFIG + \" has a topic name which matches the regex in \" +\n          \n          \n            \n                                    SinkTask.TOPICS_REGEX_CONFIG);", "author": "C0urante", "createdAt": "2020-05-13T17:04:05Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SinkConnectorConfig.java", "diffHunk": "@@ -96,6 +99,26 @@ public static void validate(Map<String, String> props) {\n             throw new ConfigException(\"Must configure one of \" +\n                 SinkTask.TOPICS_CONFIG + \" or \" + SinkTask.TOPICS_REGEX_CONFIG);\n         }\n+\n+        if (hasDlqTopicConfig) {\n+            String dlqTopic = props.get(DLQ_TOPIC_NAME_CONFIG).trim();\n+            if (hasTopicsConfig) {\n+                String[] topics = props.get(SinkTask.TOPICS_CONFIG).split(\",\");\n+                Arrays.setAll(topics, i -> topics[i].trim());\n+                if (Arrays.asList(topics).contains(dlqTopic)) {\n+                    throw new ConfigException(DLQ_TOPIC_NAME_CONFIG + \" has a topic name which is already in \" +\n+                        SinkTask.TOPICS_CONFIG);\n+                }\n+            }\n+            if (hasTopicsRegexConfig) {\n+                String topicsRegexStr = props.get(SinkTask.TOPICS_REGEX_CONFIG);\n+                Pattern pattern = Pattern.compile(topicsRegexStr);\n+                if (pattern.matcher(dlqTopic).matches()) {\n+                    throw new ConfigException(DLQ_TOPIC_NAME_CONFIG + \" has a topic name which matches in \" +\n+                        SinkTask.TOPICS_REGEX_CONFIG);", "originalCommit": "9806bd8a6ff94689223f2bf761d1cc92455c3659", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "d9f55aed36097b74b0953b180e8d4dec882cd04b", "url": "https://github.com/apache/kafka/commit/d9f55aed36097b74b0953b180e8d4dec882cd04b", "message": "Adding parseTopicsList method", "committedDate": "2020-05-13T18:30:40Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY1NjI4NA==", "url": "https://github.com/apache/kafka/pull/8663#discussion_r424656284", "bodyText": "Is this necessary? Doesn't seem like it actually breaks anything if the user specifies duplicate topics. And if we do want to enforce constraints about non-empty topics and/or no duplicated topics, we can do that with a Validator for that property when it's defined in the ConfigDef.", "author": "C0urante", "createdAt": "2020-05-13T18:46:41Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SinkConnectorConfig.java", "diffHunk": "@@ -108,6 +132,20 @@ public static boolean hasTopicsRegexConfig(Map<String, String> props) {\n         return topicsRegexStr != null && !topicsRegexStr.trim().isEmpty();\n     }\n \n+    public static boolean hasDlqTopicConfig(Map<String, String> props) {\n+        String dqlTopicStr = props.get(DLQ_TOPIC_NAME_CONFIG);\n+        return dqlTopicStr != null && !dqlTopicStr.trim().isEmpty();\n+    }\n+\n+    public static List<String> parseTopicsList(Map<String, String> props) {\n+        List<String> topics = (List<String>) ConfigDef.parseType(TOPICS_CONFIG, props.get(TOPICS_CONFIG), Type.LIST);\n+        return topics\n+                .stream()\n+                .filter(topic -> !topic.isEmpty())\n+                .distinct()", "originalCommit": "d9f55aed36097b74b0953b180e8d4dec882cd04b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY1OTM0OA==", "url": "https://github.com/apache/kafka/pull/8663#discussion_r424659348", "bodyText": "Actually it isn't. Just for the shake of having a \"cleaned\" list of topics. For instance, when logging topics in which the connector is going to subscribe...", "author": "mmolimar", "createdAt": "2020-05-13T18:51:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY1NjI4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY2Nzg2NQ==", "url": "https://github.com/apache/kafka/pull/8663#discussion_r424667865", "bodyText": "\ud83d\udc4d fair enough", "author": "C0urante", "createdAt": "2020-05-13T19:06:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY1NjI4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg1OTUzMw==", "url": "https://github.com/apache/kafka/pull/8663#discussion_r424859533", "bodyText": "How about returning a Set instead of List?\nreturn topics\n                .stream()\n                .filter(topic -> !topic.isEmpty())\n                .collect(Collectors.toSet());", "author": "chia7712", "createdAt": "2020-05-14T04:08:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY1NjI4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTExNTQ3NQ==", "url": "https://github.com/apache/kafka/pull/8663#discussion_r425115475", "bodyText": "That what I thought but we would also have a topic name with an empty string.", "author": "mmolimar", "createdAt": "2020-05-14T12:58:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY1NjI4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg2MDI1NA==", "url": "https://github.com/apache/kafka/pull/8663#discussion_r424860254", "bodyText": "Should we log the topic name for this exception? For example, has a topic name (xxx) which", "author": "chia7712", "createdAt": "2020-05-14T04:11:29Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SinkConnectorConfig.java", "diffHunk": "@@ -96,6 +101,25 @@ public static void validate(Map<String, String> props) {\n             throw new ConfigException(\"Must configure one of \" +\n                 SinkTask.TOPICS_CONFIG + \" or \" + SinkTask.TOPICS_REGEX_CONFIG);\n         }\n+\n+        if (hasDlqTopicConfig) {\n+            String dlqTopic = props.get(DLQ_TOPIC_NAME_CONFIG).trim();\n+            if (hasTopicsConfig) {\n+                List<String> topics = parseTopicsList(props);\n+                if (topics.contains(dlqTopic)) {\n+                    throw new ConfigException(DLQ_TOPIC_NAME_CONFIG + \" has a topic name which is already in \" +", "originalCommit": "d9f55aed36097b74b0953b180e8d4dec882cd04b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTEzNTM0MQ==", "url": "https://github.com/apache/kafka/pull/8663#discussion_r425135341", "bodyText": "There will be just one topic in the DQL topic config. We could add it but I'm not sure if it's explicitly necessary.", "author": "mmolimar", "createdAt": "2020-05-14T13:27:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDg2MDI1NA=="}], "type": "inlineReview"}, {"oid": "5d990ca8c20525437a0cbb2bdd7f4316b1bfc43b", "url": "https://github.com/apache/kafka/commit/5d990ca8c20525437a0cbb2bdd7f4316b1bfc43b", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into KAFKA-9985", "committedDate": "2020-05-22T19:13:48Z", "type": "commit"}, {"oid": "199f47a1e292c0d0ca9e9b4d6a752e0844737cde", "url": "https://github.com/apache/kafka/commit/199f47a1e292c0d0ca9e9b4d6a752e0844737cde", "message": "Suppress warning", "committedDate": "2020-05-22T19:48:31Z", "type": "commit"}, {"oid": "93ca5b5aae5f4ba816aaa4956e9914e8e9ae61ef", "url": "https://github.com/apache/kafka/commit/93ca5b5aae5f4ba816aaa4956e9914e8e9ae61ef", "message": "KAFKA-9985: Minor changes to improve readability of exception messages", "committedDate": "2020-06-10T22:32:36Z", "type": "commit"}]}