{"pr_number": 8088, "pr_title": "KAFKA-9535: Update metadata upon retrying partitions for ListOffset", "pr_createdAt": "2020-02-11T06:51:28Z", "pr_url": "https://github.com/apache/kafka/pull/8088", "timeline": [{"oid": "cc84e6f2a996a743e74d4e904cda4c1b86c4915a", "url": "https://github.com/apache/kafka/commit/cc84e6f2a996a743e74d4e904cda4c1b86c4915a", "message": "update metadata upon FENCED_LEADER_EPOCH", "committedDate": "2020-02-12T21:39:01Z", "type": "commit"}, {"oid": "32553454bb54d9db69030dcce992989ddcf2780c", "url": "https://github.com/apache/kafka/commit/32553454bb54d9db69030dcce992989ddcf2780c", "message": "blindly trigger metadata update for retry partitions", "committedDate": "2020-02-12T21:39:01Z", "type": "commit"}, {"oid": "d731718803825bd8a65833594a26e3b49531ea68", "url": "https://github.com/apache/kafka/commit/d731718803825bd8a65833594a26e3b49531ea68", "message": "side cleanup", "committedDate": "2020-02-12T21:39:01Z", "type": "commit"}, {"oid": "100bfd51ede28e59d3b675a7cf463d39e811f686", "url": "https://github.com/apache/kafka/commit/100bfd51ede28e59d3b675a7cf463d39e811f686", "message": "expand test", "committedDate": "2020-02-12T21:39:01Z", "type": "commit"}, {"oid": "616c245c474ec83acb72e2f5eb92e9e49174c91a", "url": "https://github.com/apache/kafka/commit/616c245c474ec83acb72e2f5eb92e9e49174c91a", "message": "consolidate", "committedDate": "2020-02-12T21:39:01Z", "type": "commit"}, {"oid": "7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967", "url": "https://github.com/apache/kafka/commit/7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967", "message": "add leader change", "committedDate": "2020-02-12T21:39:01Z", "type": "commit"}, {"oid": "7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967", "url": "https://github.com/apache/kafka/commit/7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967", "message": "add leader change", "committedDate": "2020-02-12T21:39:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY4ODczMw==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r378688733", "bodyText": "This test seems to pass with the original logic. I'm wondering if we need to let the offset request take two partitions. One of them can succeed and the other can fail due to the provided error so that we are handling the partial failure case.", "author": "hachikuji", "createdAt": "2020-02-13T07:26:18Z", "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -2421,6 +2413,38 @@ public void testGetOffsetsFencedLeaderEpoch() {\n         assertEquals(0L, metadata.timeToNextUpdate(time.milliseconds()));\n     }\n \n+    @Test\n+    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {", "originalCommit": "7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODk5NzU4MA==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r378997580", "bodyText": "I double checked, the reason was because the second try will not match the newLeader so that it actually disconnects and ask for a metadata update. I have injected a fatal error if the client reconnects to the original leader.", "author": "abbccdda", "createdAt": "2020-02-13T17:07:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY4ODczMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTA2ODcyOA==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r379068728", "bodyText": "Ok, maybe we need a separate test for the partial failure case? I am interested in verifying 1) that metadata update gets triggered after a partial failure, and 2) the retry does not request partitions that were fetched successfully.", "author": "hachikuji", "createdAt": "2020-02-13T19:23:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY4ODczMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTEyMTU5NA==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r379121594", "bodyText": "For 1), as long as it's partition level error, it's a partial failure. For 2) I could check to see if there is a way.", "author": "abbccdda", "createdAt": "2020-02-13T21:13:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY4ODczMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY4ODk1MQ==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r378688951", "bodyText": "Not really sure this has value if the test case expects the leader change correctly.", "author": "hachikuji", "createdAt": "2020-02-13T07:26:55Z", "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -2421,6 +2413,38 @@ public void testGetOffsetsFencedLeaderEpoch() {\n         assertEquals(0L, metadata.timeToNextUpdate(time.milliseconds()));\n     }\n \n+    @Test\n+    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {\n+        buildFetcher();\n+        List<Errors> retriableErrors = Arrays.asList(Errors.NOT_LEADER_FOR_PARTITION,\n+            Errors.REPLICA_NOT_AVAILABLE, Errors.KAFKA_STORAGE_ERROR, Errors.OFFSET_NOT_AVAILABLE,\n+            Errors.LEADER_NOT_AVAILABLE, Errors.FENCED_LEADER_EPOCH, Errors.UNKNOWN_LEADER_EPOCH);\n+\n+        for (Errors retriableError : retriableErrors) {\n+            subscriptions.assignFromUser(singleton(tp1));\n+            client.updateMetadata(initialUpdateResponse);\n+\n+            Node originalLeader = metadata.fetch().leaderFor(tp1);\n+\n+            final int updatedNodeSize = 3;\n+            MetadataResponse updatedMetadata = TestUtils.metadataUpdateWith(\"dummy\", 3, Collections.emptyMap(), singletonMap(topicName, 4), tp -> 3);\n+\n+            client.prepareMetadataUpdate(updatedMetadata);\n+            client.prepareResponseFrom(listOffsetResponse(tp1, retriableError, ListOffsetRequest.LATEST_TIMESTAMP, -1L), originalLeader);\n+\n+            final long timestamp = 1L;\n+            Node newLeader = new Node(1, \"localhost\", 1970);\n+            assertNotEquals(originalLeader, newLeader);\n+\n+            client.prepareResponseFrom(listOffsetResponse(tp1, Errors.NONE, timestamp, 5L), newLeader);\n+            Map<TopicPartition, OffsetAndTimestamp> offsetAndTimestampMap =\n+                fetcher.offsetsForTimes(Collections.singletonMap(tp1, timestamp), time.timer(Integer.MAX_VALUE));\n+\n+            assertEquals(Collections.singletonMap(tp1, new OffsetAndTimestamp(5L, timestamp)), offsetAndTimestampMap);\n+            assertEquals(updatedNodeSize, metadata.fetch().nodes().size());", "originalCommit": "7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODk5ODgxMg==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r378998812", "bodyText": "Removed", "author": "abbccdda", "createdAt": "2020-02-13T17:09:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY4ODk1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY4OTUzMg==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r378689532", "bodyText": "Why construct this directly? Shouldn't we get it from the metadata update?", "author": "hachikuji", "createdAt": "2020-02-13T07:28:47Z", "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -2421,6 +2413,38 @@ public void testGetOffsetsFencedLeaderEpoch() {\n         assertEquals(0L, metadata.timeToNextUpdate(time.milliseconds()));\n     }\n \n+    @Test\n+    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {\n+        buildFetcher();\n+        List<Errors> retriableErrors = Arrays.asList(Errors.NOT_LEADER_FOR_PARTITION,\n+            Errors.REPLICA_NOT_AVAILABLE, Errors.KAFKA_STORAGE_ERROR, Errors.OFFSET_NOT_AVAILABLE,\n+            Errors.LEADER_NOT_AVAILABLE, Errors.FENCED_LEADER_EPOCH, Errors.UNKNOWN_LEADER_EPOCH);\n+\n+        for (Errors retriableError : retriableErrors) {\n+            subscriptions.assignFromUser(singleton(tp1));\n+            client.updateMetadata(initialUpdateResponse);\n+\n+            Node originalLeader = metadata.fetch().leaderFor(tp1);\n+\n+            final int updatedNodeSize = 3;\n+            MetadataResponse updatedMetadata = TestUtils.metadataUpdateWith(\"dummy\", 3, Collections.emptyMap(), singletonMap(topicName, 4), tp -> 3);\n+\n+            client.prepareMetadataUpdate(updatedMetadata);\n+            client.prepareResponseFrom(listOffsetResponse(tp1, retriableError, ListOffsetRequest.LATEST_TIMESTAMP, -1L), originalLeader);\n+\n+            final long timestamp = 1L;\n+            Node newLeader = new Node(1, \"localhost\", 1970);", "originalCommit": "7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODk5Nzk1Mg==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r378997952", "bodyText": "Unfortunately the metadata update happens within the offsetsForTimes call, so we could not capture it beforehand.", "author": "abbccdda", "createdAt": "2020-02-13T17:08:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY4OTUzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTA2NzU1OQ==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r379067559", "bodyText": "What I'm saying is that the metadata update contains the node we are looking for. So why do we need to build the Node object directly? e.g. where does 1970 come from?", "author": "hachikuji", "createdAt": "2020-02-13T19:21:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY4OTUzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY5MDk0OA==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r378690948", "bodyText": "Can we add an assertion like the following to ensure that both requests were sent?\n            assertFalse(client.hasPendingResponses());", "author": "hachikuji", "createdAt": "2020-02-13T07:32:23Z", "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -2421,6 +2413,38 @@ public void testGetOffsetsFencedLeaderEpoch() {\n         assertEquals(0L, metadata.timeToNextUpdate(time.milliseconds()));\n     }\n \n+    @Test\n+    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {\n+        buildFetcher();\n+        List<Errors> retriableErrors = Arrays.asList(Errors.NOT_LEADER_FOR_PARTITION,\n+            Errors.REPLICA_NOT_AVAILABLE, Errors.KAFKA_STORAGE_ERROR, Errors.OFFSET_NOT_AVAILABLE,\n+            Errors.LEADER_NOT_AVAILABLE, Errors.FENCED_LEADER_EPOCH, Errors.UNKNOWN_LEADER_EPOCH);\n+\n+        for (Errors retriableError : retriableErrors) {\n+            subscriptions.assignFromUser(singleton(tp1));\n+            client.updateMetadata(initialUpdateResponse);\n+\n+            Node originalLeader = metadata.fetch().leaderFor(tp1);\n+\n+            final int updatedNodeSize = 3;\n+            MetadataResponse updatedMetadata = TestUtils.metadataUpdateWith(\"dummy\", 3, Collections.emptyMap(), singletonMap(topicName, 4), tp -> 3);\n+\n+            client.prepareMetadataUpdate(updatedMetadata);\n+            client.prepareResponseFrom(listOffsetResponse(tp1, retriableError, ListOffsetRequest.LATEST_TIMESTAMP, -1L), originalLeader);\n+\n+            final long timestamp = 1L;\n+            Node newLeader = new Node(1, \"localhost\", 1970);\n+            assertNotEquals(originalLeader, newLeader);\n+\n+            client.prepareResponseFrom(listOffsetResponse(tp1, Errors.NONE, timestamp, 5L), newLeader);\n+            Map<TopicPartition, OffsetAndTimestamp> offsetAndTimestampMap =\n+                fetcher.offsetsForTimes(Collections.singletonMap(tp1, timestamp), time.timer(Integer.MAX_VALUE));\n+\n+            assertEquals(Collections.singletonMap(tp1, new OffsetAndTimestamp(5L, timestamp)), offsetAndTimestampMap);\n+            assertEquals(updatedNodeSize, metadata.fetch().nodes().size());\n+        }", "originalCommit": "7b8869d4fc4ebaad315fec7fd6a2cd33c6ee3967", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTAwMzIxNg==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r379003216", "bodyText": "Add check of pending responses.", "author": "abbccdda", "createdAt": "2020-02-13T17:17:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODY5MDk0OA=="}], "type": "inlineReview"}, {"oid": "772b1a9c7971b03bdc02df20bc18ff8f07b7f95d", "url": "https://github.com/apache/kafka/commit/772b1a9c7971b03bdc02df20bc18ff8f07b7f95d", "message": "improve test", "committedDate": "2020-02-13T17:20:53Z", "type": "commit"}, {"oid": "772b1a9c7971b03bdc02df20bc18ff8f07b7f95d", "url": "https://github.com/apache/kafka/commit/772b1a9c7971b03bdc02df20bc18ff8f07b7f95d", "message": "improve test", "committedDate": "2020-02-13T17:20:53Z", "type": "forcePushed"}, {"oid": "9fdf4b794c06e8788905bd79b72d3818306f68c9", "url": "https://github.com/apache/kafka/commit/9fdf4b794c06e8788905bd79b72d3818306f68c9", "message": "request matcher", "committedDate": "2020-02-13T22:44:26Z", "type": "commit"}, {"oid": "9fdf4b794c06e8788905bd79b72d3818306f68c9", "url": "https://github.com/apache/kafka/commit/9fdf4b794c06e8788905bd79b72d3818306f68c9", "message": "request matcher", "committedDate": "2020-02-13T22:44:26Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI2Mzc2Mg==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r379263762", "bodyText": "This is an interesting idea, but it seems good enough to verify the fetched offsets. The only way we could get 5L is fetching against the new leader.", "author": "hachikuji", "createdAt": "2020-02-14T06:06:26Z", "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -2421,6 +2413,93 @@ public void testGetOffsetsFencedLeaderEpoch() {\n         assertEquals(0L, metadata.timeToNextUpdate(time.milliseconds()));\n     }\n \n+    @Test\n+    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {\n+        List<Errors> retriableErrors = Arrays.asList(Errors.NOT_LEADER_FOR_PARTITION,\n+            Errors.REPLICA_NOT_AVAILABLE, Errors.KAFKA_STORAGE_ERROR, Errors.OFFSET_NOT_AVAILABLE,\n+            Errors.LEADER_NOT_AVAILABLE, Errors.FENCED_LEADER_EPOCH, Errors.UNKNOWN_LEADER_EPOCH);\n+\n+        final int newLeaderEpoch = 3;\n+        MetadataResponse updatedMetadata = TestUtils.metadataUpdateWith(\"dummy\", 3,\n+            singletonMap(topicName, Errors.NONE), singletonMap(topicName, 4), tp -> newLeaderEpoch);\n+        LogContext dummyContext = new LogContext();\n+        ConsumerMetadata dummyMetadata = new ConsumerMetadata(0, Long.MAX_VALUE, false, false,\n+            new SubscriptionState(dummyContext, OffsetResetStrategy.EARLIEST),\n+            dummyContext, new ClusterResourceListeners());\n+        dummyMetadata.updateWithCurrentRequestVersion(updatedMetadata, false, 0L);\n+\n+        Node newLeader = dummyMetadata.fetch().leaderFor(tp1);\n+\n+        for (Errors retriableError : retriableErrors) {\n+            buildFetcher();\n+\n+            subscriptions.assignFromUser(Utils.mkSet(tp0, tp1));\n+            client.updateMetadata(initialUpdateResponse);\n+\n+            Node originalLeader = metadata.fetch().leaderFor(tp1);\n+            assertNotEquals(originalLeader, newLeader);\n+\n+            final long fetchTimestamp = 10L;\n+            Map<TopicPartition, ListOffsetResponse.PartitionData> allPartitionData = new HashMap<>();\n+            allPartitionData.put(tp0, new ListOffsetResponse.PartitionData(\n+                Errors.NONE, fetchTimestamp, 4L, Optional.empty()));\n+            allPartitionData.put(tp1, new ListOffsetResponse.PartitionData(\n+                retriableError, ListOffsetRequest.LATEST_TIMESTAMP, -1L, Optional.empty()));\n+\n+            client.prepareResponseFrom(body -> {\n+                boolean isListOffsetRequest = body instanceof ListOffsetRequest;\n+                if (isListOffsetRequest) {\n+                    ListOffsetRequest request = (ListOffsetRequest) body;\n+                    Map<TopicPartition, ListOffsetRequest.PartitionData> expectedTopicPartitions = new HashMap<>();\n+                    expectedTopicPartitions.put(tp0, new ListOffsetRequest.PartitionData(\n+                        fetchTimestamp, Optional.empty()));\n+                    expectedTopicPartitions.put(tp1, new ListOffsetRequest.PartitionData(\n+                        fetchTimestamp, Optional.empty()));\n+\n+                    return request.partitionTimestamps().equals(expectedTopicPartitions);\n+                } else {\n+                    return false;\n+                }\n+            }, new ListOffsetResponse(allPartitionData), originalLeader);\n+\n+            client.prepareMetadataUpdate(updatedMetadata);\n+\n+            // If the metadata wasn't updated before retrying, the fetcher would consult the original leader and hit a fatal exception.", "originalCommit": "9fdf4b794c06e8788905bd79b72d3818306f68c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5Njc2Mg==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r379296762", "bodyText": "In fact, it isn't. We shall hit a DisconnectedException first, and the fetcher would refresh the metadata before a third attempt. So a fatal exception is needed.", "author": "abbccdda", "createdAt": "2020-02-14T08:12:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI2Mzc2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU3NDgyMg==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r379574822", "bodyText": "Ok. How about we use the NOT_LEADER error since that is the case we are trying to simulate?", "author": "hachikuji", "createdAt": "2020-02-14T18:17:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI2Mzc2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU4NzAwOQ==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r379587009", "bodyText": "Just did", "author": "abbccdda", "createdAt": "2020-02-14T18:45:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI2Mzc2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI2NzU1Mg==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r379267552", "bodyText": "MetadataResponse allows us to get the Cluster directly, so we can do something simpler:\n        Node oldLeader = initialUpdateResponse.cluster().leaderFor(tp1);\n        Node newLeader = updatedMetadata.cluster().leaderFor(tp1);\n        assertNotEquals(oldLeader, newLeader);\n\nSince the metadata doesn't change, we can just do this check once.", "author": "hachikuji", "createdAt": "2020-02-14T06:24:33Z", "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -2421,6 +2413,93 @@ public void testGetOffsetsFencedLeaderEpoch() {\n         assertEquals(0L, metadata.timeToNextUpdate(time.milliseconds()));\n     }\n \n+    @Test\n+    public void testGetOffsetByTimeWithPartitionsRetryCouldTriggerMetadataUpdate() {\n+        List<Errors> retriableErrors = Arrays.asList(Errors.NOT_LEADER_FOR_PARTITION,\n+            Errors.REPLICA_NOT_AVAILABLE, Errors.KAFKA_STORAGE_ERROR, Errors.OFFSET_NOT_AVAILABLE,\n+            Errors.LEADER_NOT_AVAILABLE, Errors.FENCED_LEADER_EPOCH, Errors.UNKNOWN_LEADER_EPOCH);\n+\n+        final int newLeaderEpoch = 3;\n+        MetadataResponse updatedMetadata = TestUtils.metadataUpdateWith(\"dummy\", 3,\n+            singletonMap(topicName, Errors.NONE), singletonMap(topicName, 4), tp -> newLeaderEpoch);\n+        LogContext dummyContext = new LogContext();\n+        ConsumerMetadata dummyMetadata = new ConsumerMetadata(0, Long.MAX_VALUE, false, false,", "originalCommit": "9fdf4b794c06e8788905bd79b72d3818306f68c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI5NjAwNg==", "url": "https://github.com/apache/kafka/pull/8088#discussion_r379296006", "bodyText": "Sounds good", "author": "abbccdda", "createdAt": "2020-02-14T08:10:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTI2NzU1Mg=="}], "type": "inlineReview"}, {"oid": "1500e239dc91b80a9dd653c321d40fda812a4a25", "url": "https://github.com/apache/kafka/commit/1500e239dc91b80a9dd653c321d40fda812a4a25", "message": "old/new leader put out", "committedDate": "2020-02-14T08:08:41Z", "type": "commit"}, {"oid": "78820ca0b3d6bde44409634dae6a78900838870f", "url": "https://github.com/apache/kafka/commit/78820ca0b3d6bde44409634dae6a78900838870f", "message": "not leader", "committedDate": "2020-02-14T18:32:16Z", "type": "commit"}]}