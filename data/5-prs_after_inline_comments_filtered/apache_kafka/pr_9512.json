{"pr_number": 9512, "pr_title": "KAFKA-10394: generate snapshot", "pr_createdAt": "2020-10-27T17:12:43Z", "pr_url": "https://github.com/apache/kafka/pull/9512", "timeline": [{"oid": "91bce7e8964d0f5a80a521ffeb67c1033417a4d1", "url": "https://github.com/apache/kafka/commit/91bce7e8964d0f5a80a521ffeb67c1033417a4d1", "message": "KAFKA-10394: Generate Kafka Snapshots", "committedDate": "2020-10-28T17:07:53Z", "type": "commit"}, {"oid": "455e36221edb91f90b0ad7c05042e44b0754f512", "url": "https://github.com/apache/kafka/commit/455e36221edb91f90b0ad7c05042e44b0754f512", "message": "Implement a buffered snapshot writer", "committedDate": "2020-10-29T17:40:36Z", "type": "commit"}, {"oid": "455e36221edb91f90b0ad7c05042e44b0754f512", "url": "https://github.com/apache/kafka/commit/455e36221edb91f90b0ad7c05042e44b0754f512", "message": "Implement a buffered snapshot writer", "committedDate": "2020-10-29T17:40:36Z", "type": "forcePushed"}, {"oid": "00ea9fdd864efb45bd6987c874662a51641d6487", "url": "https://github.com/apache/kafka/commit/00ea9fdd864efb45bd6987c874662a51641d6487", "message": "Simplify some tests", "committedDate": "2020-10-29T18:36:32Z", "type": "commit"}, {"oid": "5ac347d0733cad6ef0454249cf1f8835f8d4fb9c", "url": "https://github.com/apache/kafka/commit/5ac347d0733cad6ef0454249cf1f8835f8d4fb9c", "message": "Support appending bytes directly to the snapshot", "committedDate": "2020-10-31T20:52:52Z", "type": "commit"}, {"oid": "5ac347d0733cad6ef0454249cf1f8835f8d4fb9c", "url": "https://github.com/apache/kafka/commit/5ac347d0733cad6ef0454249cf1f8835f8d4fb9c", "message": "Support appending bytes directly to the snapshot", "committedDate": "2020-10-31T20:52:52Z", "type": "forcePushed"}, {"oid": "a3cc586bd56e28686b54a5f719956ca271e9dc9e", "url": "https://github.com/apache/kafka/commit/a3cc586bd56e28686b54a5f719956ca271e9dc9e", "message": "Rename snapshot writing to BatchedSnapshotWriter", "committedDate": "2020-10-31T21:01:29Z", "type": "commit"}, {"oid": "288b19f4ec11f9ced3327ac1c371927245e06882", "url": "https://github.com/apache/kafka/commit/288b19f4ec11f9ced3327ac1c371927245e06882", "message": "Merge remote-tracking branch 'upstream/trunk' into kafka-10394-generate-snapshot", "committedDate": "2020-11-03T18:34:09Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk4ODgwNg==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r516988806", "bodyText": "It would be nice if we can figure out how to consolidate this and BatchReader. It seems like it should be doable.", "author": "hachikuji", "createdAt": "2020-11-03T22:19:26Z", "path": "raft/src/main/java/org/apache/kafka/snapshot/SnapshotReader.java", "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.snapshot;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.raft.OffsetAndEpoch;\n+\n+// TODO: Write documentation for this type and all of the methods\n+public interface SnapshotReader extends Closeable, Iterable<RecordBatch> {", "originalCommit": "288b19f4ec11f9ced3327ac1c371927245e06882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzcwMzA3OA==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r517703078", "bodyText": "This comment applies to some of your other observations. At high-level there are 4 use cases that we need to design and implement. Two use cases are for the Raft implementation. Two use cases are for the state machine.\nRaft implementation\nThese types are internal to the raft implementation and don't have to be exposed to the state machine.\nLeader Use Case\nThe leader needs to be able to send a part of the snapshot over the network. Something like this\ninterface SnapshotReader extends Closeable {\n    long transferTo(long position, long maxBytes, WritableChannel channel);\n    int read(ByteBuffer buffer, long position);\n}\nFollower Use Case\nThe followers need to be able to copy bytes from the network and validate the snapshot on disk when fetching is done.\ninterface SnapshotWriter extends Closeable {\n    void append(ByteBuffer buffer);\n    void validate();\n    void freeze();\n}\nState machine implementation\nThese types are exposed to the state machine.\nLoad Snapshot\nThe state machine needs to be able to load/scan the entire snapshot. The state machine can use close to tell the raft client that it finished loading the snapshot. This will be implemented in a future PR but it could look like this:\ninterface BatchedSnapshotReader<T> extends Iterable<Iterable<T>>, Closeable {\n}\nGenerate Snapshot\nThe state machine needs to be able to generate a snapshot by appending records/values and marking the snapshot as immutable (freeze) when it is done.\ninterface BatchdSnapshotWriter<T> extends Closeable {\n   void append(Iterable<T> records);\n   void freeze();\n}\nNotes\nSnapshotWriter and SnapshotReader need to be interfaces because we will have a real implementation and a mocked implementation for testing. These two types are internal to raft and are not exposed to the state machine.\nBatchedSnapshotReader and BatchedSnapshotWriter can depend on SnapshotWriter and SnapshotReade to reuse some code but this is not strictly required. These two type don't have to be interfaces if they delegate the IO to SnapshotWriter and SnapshotReader.\nWhat do you think @hachikuji?", "author": "jsancio", "createdAt": "2020-11-05T00:03:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk4ODgwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg4MjYwNA==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r518882604", "bodyText": "Ok, I think I see what you're saying. I guess I was not expecting that we would need a separate low-level interface when we already have FileRecords and MemoryRecords. I think Records already gives us a way to read from arbitrary positions:\n    long writeTo(GatheringByteChannel channel, long position, int length) throws IOException;\nBut there is no Records implementation currently that allows unaligned writes. We only have this:\n    public int append(MemoryRecords records) throws IOException;\nPerhaps it would be possible to extend Records to provide what we need instead of creating a new interface?\nAs far as the naming, I wonder if we can reserve the nicer SnapshotXXX names for the state machine. Really what I would like is a common type that can be used by both handleSnapshot and handleCommit since both callbacks just need to provide a way to read through a set of records. I was thinking it could be BatchReader, but it doesn't have to be if there is something better.\n(By the way, it's not too clear to me why we need freeze when we already have close.)", "author": "hachikuji", "createdAt": "2020-11-06T17:00:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk4ODgwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkxNjM5Mg==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r518916392", "bodyText": "Perhaps it would be possible to extend Records to provide what we need instead of creating a new interface?\n\nI think you are right that we don't need SnapshotReader as Records provides all of the functionality we need. SnapshotReader was added so that we didn't expose all of the mutating APIs in FileRecords. Snapshot are supposed to be immutable once frozen.\nI think we still want SnapshotWriter or something similar as it provides 1. raw writes of bytes and 2. freeze which optionally marks the snapshot as immutable if the validation passes.\nFor 1., I don't think we should add raw writes to Records or FileRecords as in essence we are exposing an unsafe API and the user needs to make sure that they are writing the correct data.\nFor 2., I think we can get away from introducing a new type/interface and instead add that functionality as a static method in Snapshots.java. Unittest (mock tests) maybe difficult with this code organization.\n\nAs far as the naming, I wonder if we can reserve the nicer SnapshotXXX names for the state machine.\n\nYes.I'll use the prettier name for the type exposed to the state machine.\n\nReally what I would like is a common type that can be used by both handleSnapshot and handleCommit since both callbacks just need to provide a way to read through a set of records. I was thinking it could be BatchReader, but it doesn't have to be if there is something better.\n\nI think this should be possible. I haven't implemented this part so I don't have all of the details. I think the requirement for the type sent through handleSnapshot are a subset of the requirements for the type sent through handleCommit. In particular, the Raft Client (ListenerContext) only needs to know when the snapshot has been read fully (e.g. close). The Raft Client doesn't need so to know what is the \"last seen offset + 1\" as it already knows the SnapshotId sent through handleSnapshot.\n\n(By the way, it's not too clear to me why we need freeze when we already have close.)\n\nI wanted to allow the user to abort a snapshot. If close is called without calling freeze then the partial snapshot is deleted and not made immutable.", "author": "jsancio", "createdAt": "2020-11-06T18:02:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk4ODgwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTIxMzY3MQ==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r519213671", "bodyText": "@hachikuji and I discussed this offline. The latest PR should reflect our discussion.\nThe interfaces RawSnapshotWriter and RawSnapshotReader are expected to change after we address https://issues.apache.org/jira/browse/KAFKA-10694", "author": "jsancio", "createdAt": "2020-11-07T20:09:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk4ODgwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk4OTM4MA==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r516989380", "bodyText": "So I guess we need this in order to be able to serve FetchSnapshot requests. I am wondering if it would be better to pull it into a separate object. Maybe we can have something like this:\nclass SnapshotReader extends BatchReader;\n\nclass Snapshot extends Iterable<Batch> {\n  public SnapshotReader iterator();\n  public int read(ByteBuffer buffer, long position) throws IOException;\n}", "author": "hachikuji", "createdAt": "2020-11-03T22:20:48Z", "path": "raft/src/main/java/org/apache/kafka/snapshot/SnapshotReader.java", "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.snapshot;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.raft.OffsetAndEpoch;\n+\n+// TODO: Write documentation for this type and all of the methods\n+public interface SnapshotReader extends Closeable, Iterable<RecordBatch> {\n+\n+    public OffsetAndEpoch snapshotId();\n+\n+    public long sizeInBytes();\n+\n+    public Iterator<RecordBatch> iterator();\n+\n+    public int read(ByteBuffer buffer, long position) throws IOException;", "originalCommit": "288b19f4ec11f9ced3327ac1c371927245e06882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzcwMzYyNQ==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r517703625", "bodyText": "I think I address this in this comment #9512 (comment). If you agree let's move the conversation there.", "author": "jsancio", "createdAt": "2020-11-05T00:04:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk4OTM4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk5NDc3NA==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r516994774", "bodyText": "The types surprised me a bit. I would have expected this to extend SnapshotWriter. I'm wondering why we don't push the type <T> up to SnapshotWriter and try to keep Records out of the interfaces?", "author": "hachikuji", "createdAt": "2020-11-03T22:33:30Z", "path": "raft/src/main/java/org/apache/kafka/snapshot/BatchedSnapshotWriter.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.snapshot;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.kafka.common.memory.MemoryPool;\n+import org.apache.kafka.common.record.CompressionType;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.raft.OffsetAndEpoch;\n+import org.apache.kafka.raft.RecordSerde;\n+import org.apache.kafka.raft.internals.BatchAccumulator.CompletedBatch;\n+import org.apache.kafka.raft.internals.BatchAccumulator;\n+\n+// TODO: Write documentation for this type and all of the methods\n+final public class BatchedSnapshotWriter<T> implements Closeable {", "originalCommit": "288b19f4ec11f9ced3327ac1c371927245e06882", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzcwMzk1Mw==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r517703953", "bodyText": "I cover some of the motivation for this here: #9512 (comment).\nLet's move the conversation there if you agree.", "author": "jsancio", "createdAt": "2020-11-05T00:06:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk5NDc3NA=="}], "type": "inlineReview"}, {"oid": "b05419777b17bff90e9a0c3bef8c373cab38e7ee", "url": "https://github.com/apache/kafka/commit/b05419777b17bff90e9a0c3bef8c373cab38e7ee", "message": "Move snapshot implementations to raft project", "committedDate": "2020-11-05T21:58:07Z", "type": "commit"}, {"oid": "5c85536b893e65609bb2433749852f1cdc3779b4", "url": "https://github.com/apache/kafka/commit/5c85536b893e65609bb2433749852f1cdc3779b4", "message": "Rename the snapshot classes and interfaces", "committedDate": "2020-11-06T21:37:55Z", "type": "commit"}, {"oid": "303c300a3761bce08ae4605158c2a65414b7923f", "url": "https://github.com/apache/kafka/commit/303c300a3761bce08ae4605158c2a65414b7923f", "message": "Improve documentation", "committedDate": "2020-11-07T19:41:41Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTIxMTM5MA==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r519211390", "bodyText": "Changes to this file are unrelated to this PR. Made them here so that docsJar would succeed.", "author": "jsancio", "createdAt": "2020-11-07T19:44:17Z", "path": "raft/src/main/java/org/apache/kafka/raft/QuorumState.java", "diffHunk": "@@ -32,25 +32,25 @@\n  * This class is responsible for managing the current state of this node and ensuring only\n  * valid state transitions.\n  *\n- * Unattached =>\n+ * Unattached transitions to:", "originalCommit": "303c300a3761bce08ae4605158c2a65414b7923f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTIxMTU4Ng==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r519211586", "bodyText": "As @hachikuji and I discussed offline, it is very like that this method will change to:\npublic void append(MemoryRecords records) throws IOException;\nAfter we implement https://issues.apache.org/jira/browse/KAFKA-10694", "author": "jsancio", "createdAt": "2020-11-07T19:46:11Z", "path": "raft/src/main/java/org/apache/kafka/snapshot/RawSnapshotWriter.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.snapshot;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import org.apache.kafka.raft.OffsetAndEpoch;\n+\n+/**\n+ * Interface for writing snapshot as a sequence of records.\n+ */\n+public interface RawSnapshotWriter extends Closeable {\n+    /**\n+     * Returns the end offset and epoch for the snapshot.\n+     */\n+    public OffsetAndEpoch snapshotId();\n+\n+    /**\n+     * Returns the number of bytes for the snapshot.\n+     *\n+     * @throws IOException for any IO error while reading the size\n+     */\n+    public long sizeInBytes() throws IOException;\n+\n+    /**\n+     * Fully appends the buffer to the snapshot.\n+     *\n+     * If the method returns without an exception the given buffer was fully writing the\n+     * snapshot.\n+     *\n+     * @param buffer the buffer to append\n+     * @throws IOException for any IO error during append\n+     */\n+    public void append(ByteBuffer buffer) throws IOException;", "originalCommit": "303c300a3761bce08ae4605158c2a65414b7923f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTIxMTcyMQ==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r519211721", "bodyText": "As @hachikuji and I discussed offline, it is very like that this method will change to:\npublic BaseRecords slice(long position) throws IOException;\nAfter we implement https://issues.apache.org/jira/browse/KAFKA-10694", "author": "jsancio", "createdAt": "2020-11-07T19:47:45Z", "path": "raft/src/main/java/org/apache/kafka/snapshot/RawSnapshotReader.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.snapshot;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.raft.OffsetAndEpoch;\n+\n+/**\n+ * Interface for reading snapshots as a sequence of records.\n+ */\n+public interface RawSnapshotReader extends Closeable, Iterable<RecordBatch> {\n+    /**\n+     * Returns the end offset and epoch for the snapshot.\n+     */\n+    public OffsetAndEpoch snapshotId();\n+\n+    /**\n+     * Returns the number of bytes for the snapshot.\n+     *\n+     * @throws IOException for any IO error while reading the size\n+     */\n+    public long sizeInBytes() throws IOException;\n+\n+    /**\n+     * Reads bytes from position into the given buffer.\n+     *\n+     * It is not guarantee that the given buffer will be filled.\n+     *\n+     * @param buffer byte buffer to put the read files\n+     * @param position the starting position in the snapshot to read\n+     * @return the number of bytes read\n+     * @throws IOException for any IO error while reading the snapshot\n+     */\n+    public int read(ByteBuffer buffer, long position) throws IOException;", "originalCommit": "303c300a3761bce08ae4605158c2a65414b7923f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "db591686f308776d5d7d515df89f5a12c3d6d841", "url": "https://github.com/apache/kafka/commit/db591686f308776d5d7d515df89f5a12c3d6d841", "message": "Change the epoch formatter", "committedDate": "2020-11-07T19:59:17Z", "type": "commit"}, {"oid": "408d0b7968f79100834058034721573335ba0f21", "url": "https://github.com/apache/kafka/commit/408d0b7968f79100834058034721573335ba0f21", "message": "Merge remote-tracking branch 'upstream/trunk' into kafka-10394-generate-snapshot", "committedDate": "2020-11-07T20:32:21Z", "type": "commit"}, {"oid": "9c7b8e45679eab960fb634a91eafe0b0f5d28e9e", "url": "https://github.com/apache/kafka/commit/9c7b8e45679eab960fb634a91eafe0b0f5d28e9e", "message": "Merge remote-tracking branch 'upstream/trunk' into kafka-10394-generate-snapshot", "committedDate": "2020-11-23T17:46:07Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzcyOTcyMw==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r533729723", "bodyText": "Do we assume the offset is below the high watermark?", "author": "hachikuji", "createdAt": "2020-12-01T21:22:35Z", "path": "raft/src/main/java/org/apache/kafka/raft/RaftClient.java", "diffHunk": "@@ -100,4 +102,15 @@ default void handleResign() {}\n      */\n     CompletableFuture<Void> shutdown(int timeoutMs);\n \n+    /**\n+     * Create a writable snapshot file for a given offset and epoch.\n+     *\n+     * The RaftClient assumes that the snapshot return will contain the records up to but\n+     * not including the end offset in the snapshot id. See {@link SnapshotWriter} for\n+     * details on how to use this object.\n+     *\n+     * @param snapshotId the end offset and epoch that identifies the snapshot", "originalCommit": "9c7b8e45679eab960fb634a91eafe0b0f5d28e9e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzgyODMyOQ==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r533828329", "bodyText": "Is there a specific reason why you are asking this? We don't currently check for this. I will add a check for this and we can relax this later if we need to.", "author": "jsancio", "createdAt": "2020-12-02T01:10:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzcyOTcyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ4NTA4OA==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r534485088", "bodyText": "Just curious. Thought it might be worth mentioning the expectation in the javadoc.", "author": "hachikuji", "createdAt": "2020-12-02T21:13:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzcyOTcyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ4OTc1MQ==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r534489751", "bodyText": "Yeah. I'll create a Jira to validate the snapshot id. We should perform at least the following validations:\n\nit is less than the high-watermark.\nit is a valid epoch and end offset based on the log. I think we can use the leader epoch cache to check this.", "author": "jsancio", "createdAt": "2020-12-02T21:22:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzcyOTcyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDU5MjYyNA==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r534592624", "bodyText": "Create https://issues.apache.org/jira/browse/KAFKA-10800", "author": "jsancio", "createdAt": "2020-12-03T01:14:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzcyOTcyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzczMjc4OQ==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r533732789", "bodyText": "This suffix is used for producer state snapshots already. Maybe we could use .snap or something like that.", "author": "hachikuji", "createdAt": "2020-12-01T21:28:37Z", "path": "raft/src/main/java/org/apache/kafka/snapshot/Snapshots.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.snapshot;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.text.NumberFormat;\n+import org.apache.kafka.raft.OffsetAndEpoch;\n+\n+final class Snapshots {\n+    private static final String SNAPSHOT_DIR = \"snapshots\";\n+    private static final String SUFFIX =  \".snapshot\";", "originalCommit": "9c7b8e45679eab960fb634a91eafe0b0f5d28e9e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzgyOTU4Mw==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r533829583", "bodyText": "Good catch. I mentioned using .checkpoint in KIP-630. I forgot to change it here. I'll change it to that but let me know if you have a preference.", "author": "jsancio", "createdAt": "2020-12-02T01:13:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzczMjc4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ4NTI5Mw==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r534485293", "bodyText": "Sounds fine to me.", "author": "hachikuji", "createdAt": "2020-12-02T21:14:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzczMjc4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzc0MDk1MA==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r533740950", "bodyText": "Do we also need an api to list snapshots?", "author": "hachikuji", "createdAt": "2020-12-01T21:44:15Z", "path": "raft/src/main/java/org/apache/kafka/raft/ReplicatedLog.java", "diffHunk": "@@ -149,6 +152,29 @@ default OptionalLong truncateToEndOffset(OffsetAndEpoch endOffset) {\n         return OptionalLong.of(truncationOffset);\n     }\n \n+    /**\n+     * Create a writable snapshot for the given snapshot id.\n+     *\n+     * See {@link RawSnapshotWriter} for details on how to use this object.\n+     *\n+     * @param snapshotId the end offset and epoch that identifies the snapshot\n+     * @return a writable snapshot\n+     */\n+    RawSnapshotWriter createSnapshot(OffsetAndEpoch snapshotId) throws IOException;", "originalCommit": "9c7b8e45679eab960fb634a91eafe0b0f5d28e9e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzgzNTg1Nw==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r533835857", "bodyText": "Yeah. I think we will need that when we implement deleting snapshot. Do you mind if I add this later?\nAlso, I think we are going to need a readLatestSnapshot() when the state machine (controller or broker) needs to load the latest valid snapshot. I was planning to add this later when the case was clear to me.", "author": "jsancio", "createdAt": "2020-12-02T01:30:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzc0MDk1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzc0MjU4MA==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r533742580", "bodyText": "Is there a tangible benefit to separating snapshots into a new directory? Currently the log directory is a flat structure. I'm wondering if we should stick with convention.", "author": "hachikuji", "createdAt": "2020-12-01T21:47:21Z", "path": "raft/src/main/java/org/apache/kafka/snapshot/Snapshots.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.snapshot;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.text.NumberFormat;\n+import org.apache.kafka.raft.OffsetAndEpoch;\n+\n+final class Snapshots {\n+    private static final String SNAPSHOT_DIR = \"snapshots\";", "originalCommit": "9c7b8e45679eab960fb634a91eafe0b0f5d28e9e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzgzMzQyOA==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r533833428", "bodyText": "I think this depends on if we need to scan the snapshot directory. Unfortunately, I don't have a concrete answer at the moment. When we implement the changes to the rest of the raft client. Log truncation, updating the start offset and LEO, we may need to scan the snapshot/checkpoint folder to determine the greatest log start offset and LEO. @lbradstreet suggested storing them in a different directory as part of the KIP-630 review process as Kafka already have a few files in the partition log directory.", "author": "jsancio", "createdAt": "2020-12-02T01:23:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzc0MjU4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ4NjAwMg==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r534486002", "bodyText": "Not sure if the snapshots will have much of an impact on this. It's not like we'll be storing more than a couple of them.", "author": "hachikuji", "createdAt": "2020-12-02T21:15:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzc0MjU4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ4NzY4MQ==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r534487681", "bodyText": "My preference is probably to keep things flat for now until we figure out what we want the long-term structure to look like. I guess it comes down to the implementation, but intuitively, the only time we'd need to scan would be on startup and we have to do that anyway.", "author": "hachikuji", "createdAt": "2020-12-02T21:18:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzc0MjU4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDU4NzM0NA==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r534587344", "bodyText": "Done. Removed the snapshots directory.", "author": "jsancio", "createdAt": "2020-12-03T01:00:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzc0MjU4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDU5MDYyMA==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r534590620", "bodyText": "@jsancio @hachikuji we saw DeleteRecords calls became pretty expensive and tied up request handler threads when made on big partition directories and it scanned the directory to find deletable snapshot files. If we keep references to these snapshots in memory after log open this won't be a big deal to place them in the same directory. That's what we ended up doing for the producer state snapshots and it worked well.", "author": "lbradstreet", "createdAt": "2020-12-03T01:09:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzc0MjU4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzc0NjQ3NQ==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r533746475", "bodyText": "Maybe we can just a better name for path since it makes this code look suspicious.", "author": "hachikuji", "createdAt": "2020-12-01T21:54:44Z", "path": "raft/src/main/java/org/apache/kafka/snapshot/FileRawSnapshotWriter.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.snapshot;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardCopyOption;\n+import java.nio.file.StandardOpenOption;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.raft.OffsetAndEpoch;\n+\n+public final class FileRawSnapshotWriter implements RawSnapshotWriter {\n+    private final Path path;\n+    private final FileChannel channel;\n+    private final OffsetAndEpoch snapshotId;\n+    private boolean frozen = false;\n+\n+    private FileRawSnapshotWriter(\n+        Path path,\n+        FileChannel channel,\n+        OffsetAndEpoch snapshotId\n+    ) {\n+        this.path = path;\n+        this.channel = channel;\n+        this.snapshotId = snapshotId;\n+    }\n+\n+    @Override\n+    public OffsetAndEpoch snapshotId() {\n+        return snapshotId;\n+    }\n+\n+    @Override\n+    public long sizeInBytes() throws IOException {\n+        return channel.size();\n+    }\n+\n+    @Override\n+    public void append(ByteBuffer buffer) throws IOException {\n+        if (frozen) {\n+            throw new IllegalStateException(\n+                String.format(\"Append not supported. Snapshot is already frozen: id = %s; path = %s\", snapshotId, path)\n+            );\n+        }\n+\n+        Utils.writeFully(channel, buffer);\n+    }\n+\n+    @Override\n+    public boolean isFrozen() {\n+        return frozen;\n+    }\n+\n+    @Override\n+    public void freeze() throws IOException {\n+        channel.close();\n+        frozen = true;\n+\n+        // Set readonly and ignore the result\n+        if (!path.toFile().setReadOnly()) {\n+            throw new IOException(String.format(\"Unable to set file %s as read-only\", path));\n+        }\n+\n+        Path destination = Snapshots.moveRename(path, snapshotId);\n+        Files.move(path, destination, StandardCopyOption.ATOMIC_MOVE);\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        channel.close();\n+        Files.deleteIfExists(path);", "originalCommit": "9c7b8e45679eab960fb634a91eafe0b0f5d28e9e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzc1MDA4OA==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r533750088", "bodyText": "nit: I think this would be more natural:\nif (accumulator.needsDrain(time.milliseconds())) {\n  appendBatches(accumulator.drain());\n}", "author": "hachikuji", "createdAt": "2020-12-01T22:01:27Z", "path": "raft/src/main/java/org/apache/kafka/snapshot/SnapshotWriter.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.snapshot;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.kafka.common.memory.MemoryPool;\n+import org.apache.kafka.common.record.CompressionType;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.raft.OffsetAndEpoch;\n+import org.apache.kafka.raft.RecordSerde;\n+import org.apache.kafka.raft.internals.BatchAccumulator.CompletedBatch;\n+import org.apache.kafka.raft.internals.BatchAccumulator;\n+\n+/**\n+ * A type for writing a snapshot fora given end offset and epoch.\n+ *\n+ * A snapshot writer can be used to append objects until freeze is called. When freeze is\n+ * called the snapshot is validated and marked as immutable. After freeze is called any\n+ * append will fail with an exception.\n+ *\n+ * It is assumed that the content of the snapshot represents all of the records for the\n+ * topic partition from offset 0 up to but not including the end offset in the snapshot\n+ * id.\n+ *\n+ * @see org.apache.kafka.raft.RaftClient#createSnapshot(OffsetAndEpoch)\n+ */\n+final public class SnapshotWriter<T> implements Closeable {\n+    final private RawSnapshotWriter snapshot;\n+    final private BatchAccumulator<T> accumulator;\n+    final private Time time;\n+\n+    /**\n+     * Initializes a new instance of the class.\n+     *\n+     * @param snapshot the low level snapshot writer\n+     * @param maxBatchSize the maximum size in byte for a batch\n+     * @param memoryPool the memory pool for buffer allocation\n+     * @param time the clock implementation\n+     * @param compressionType the compression algorithm to use\n+     * @param serde the record serialization and deserialization implementation\n+     */\n+    public SnapshotWriter(\n+        RawSnapshotWriter snapshot,\n+        int maxBatchSize,\n+        MemoryPool memoryPool,\n+        Time time,\n+        CompressionType compressionType,\n+        RecordSerde<T> serde\n+    ) {\n+        this.snapshot = snapshot;\n+        this.time = time;\n+\n+        this.accumulator = new BatchAccumulator<>(\n+            snapshot.snapshotId().epoch,\n+            0,\n+            Integer.MAX_VALUE,\n+            maxBatchSize,\n+            memoryPool,\n+            time,\n+            compressionType,\n+            serde\n+        );\n+    }\n+\n+    /**\n+     * Returns the end offset and epoch for the snapshot.\n+     */\n+    public OffsetAndEpoch snapshotId() {\n+        return snapshot.snapshotId();\n+    }\n+\n+    /**\n+     * Returns true if the snapshot has been frozen, otherwise false is returned.\n+     *\n+     * Modification to the snapshot are not allowed once it is frozen.\n+     */\n+    public boolean isFrozen() {\n+        return snapshot.isFrozen();\n+    }\n+\n+    /**\n+     * Appends a list of values to the snapshot.\n+     *\n+     * The list of record passed are guaranteed to get written together.\n+     *\n+     * @param records the list of records to append to the snapshot\n+     * @throws IOException for any IO error while appending\n+     * @throws IllegalStateException if append is called when isFrozen is true\n+     */\n+    public void append(List<T> records) throws IOException {\n+        if (snapshot.isFrozen()) {\n+            String message = String.format(\n+                \"Append not supported. Snapshot is already frozen: id = {}.\",\n+                snapshot.snapshotId()\n+            );\n+\n+            throw new IllegalStateException(message);\n+        }\n+\n+        accumulator.append(snapshot.snapshotId().epoch, records);\n+\n+        if (!accumulator.needsDrain(time.milliseconds())) {", "originalCommit": "9c7b8e45679eab960fb634a91eafe0b0f5d28e9e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzc1MjAxOA==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r533752018", "bodyText": "nit: kind of annoying to wrap the iterator just to avoid the generic warning. An alternative might be to use <? extends RecordBatch> in the interface.", "author": "hachikuji", "createdAt": "2020-12-01T22:05:08Z", "path": "raft/src/main/java/org/apache/kafka/snapshot/FileRawSnapshotReader.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.snapshot;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.Path;\n+import java.util.Iterator;\n+import org.apache.kafka.common.record.FileLogInputStream.FileChannelRecordBatch;\n+import org.apache.kafka.common.record.FileRecords;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.common.utils.AbstractIterator;\n+import org.apache.kafka.raft.OffsetAndEpoch;\n+\n+public final class FileRawSnapshotReader implements RawSnapshotReader {\n+    private final FileRecords fileRecords;\n+    private final OffsetAndEpoch snapshotId;\n+\n+    private FileRawSnapshotReader(FileRecords fileRecords, OffsetAndEpoch snapshotId) {\n+        this.fileRecords = fileRecords;\n+        this.snapshotId = snapshotId;\n+    }\n+\n+    @Override\n+    public OffsetAndEpoch snapshotId() {\n+        return snapshotId;\n+    }\n+\n+    @Override\n+    public long sizeInBytes() {\n+        return fileRecords.sizeInBytes();\n+    }\n+\n+    @Override\n+    public Iterator<RecordBatch> iterator() {\n+        return new Iterator<RecordBatch>() {", "originalCommit": "9c7b8e45679eab960fb634a91eafe0b0f5d28e9e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDU3MjI5Ng==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r534572296", "bodyText": "I am not sure if we can do that. The interface RawSnapshotReader extends Iterable<RecordBatch>. Iterable defines that method as Iterator<T> iterator(). As you point out, they should have defined it as Iterator<? extends T>  iterator() since Iterator is covariant.", "author": "jsancio", "createdAt": "2020-12-03T00:20:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzc1MjAxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYwNjgxNw==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r534606817", "bodyText": "In that case, I would probably just do the cast to avoid the useless wrapper. Maybe we could create a helper like this in Utils:\n    @SuppressWarnings(\"unchecked\")\n    private <S, T extends S> Iterator<S> covariantCast(Iterator<T> iterator) {\n        return (Iterator<S>) iterator;\n    }", "author": "hachikuji", "createdAt": "2020-12-03T01:52:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzc1MjAxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzc1NDEwNg==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r533754106", "bodyText": "Wonder if we should consider using Utils.atomicMoveWithFallback?", "author": "hachikuji", "createdAt": "2020-12-01T22:09:13Z", "path": "raft/src/main/java/org/apache/kafka/snapshot/FileRawSnapshotWriter.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.snapshot;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardCopyOption;\n+import java.nio.file.StandardOpenOption;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.raft.OffsetAndEpoch;\n+\n+public final class FileRawSnapshotWriter implements RawSnapshotWriter {\n+    private final Path path;\n+    private final FileChannel channel;\n+    private final OffsetAndEpoch snapshotId;\n+    private boolean frozen = false;\n+\n+    private FileRawSnapshotWriter(\n+        Path path,\n+        FileChannel channel,\n+        OffsetAndEpoch snapshotId\n+    ) {\n+        this.path = path;\n+        this.channel = channel;\n+        this.snapshotId = snapshotId;\n+    }\n+\n+    @Override\n+    public OffsetAndEpoch snapshotId() {\n+        return snapshotId;\n+    }\n+\n+    @Override\n+    public long sizeInBytes() throws IOException {\n+        return channel.size();\n+    }\n+\n+    @Override\n+    public void append(ByteBuffer buffer) throws IOException {\n+        if (frozen) {\n+            throw new IllegalStateException(\n+                String.format(\"Append not supported. Snapshot is already frozen: id = %s; path = %s\", snapshotId, path)\n+            );\n+        }\n+\n+        Utils.writeFully(channel, buffer);\n+    }\n+\n+    @Override\n+    public boolean isFrozen() {\n+        return frozen;\n+    }\n+\n+    @Override\n+    public void freeze() throws IOException {\n+        channel.close();\n+        frozen = true;\n+\n+        // Set readonly and ignore the result\n+        if (!path.toFile().setReadOnly()) {\n+            throw new IOException(String.format(\"Unable to set file %s as read-only\", path));\n+        }\n+\n+        Path destination = Snapshots.moveRename(path, snapshotId);\n+        Files.move(path, destination, StandardCopyOption.ATOMIC_MOVE);", "originalCommit": "9c7b8e45679eab960fb634a91eafe0b0f5d28e9e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDU3NzY1OQ==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r534577659", "bodyText": "I prefer to keep it this way. The atomic move (rename) should always succeed because we guarantee it is within the same directory. I think I prefer for Kafka to throw an exception than for the raft client or state machine to see a partial snapshot file because it performed a file copy instead of a file rename.", "author": "jsancio", "createdAt": "2020-12-03T00:35:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzc1NDEwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzc1NjY1OQ==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r533756659", "bodyText": "We could probably use ByteBufferOutputStream which already handles expansion.", "author": "hachikuji", "createdAt": "2020-12-01T22:14:12Z", "path": "raft/src/test/java/org/apache/kafka/raft/MockLog.java", "diffHunk": "@@ -472,4 +489,110 @@ private EpochStartOffset(int epoch, long startOffset) {\n         }\n     }\n \n+    final class MockRawSnapshotWriter implements RawSnapshotWriter {\n+        private final OffsetAndEpoch snapshotId;\n+        private ByteBuffer data;", "originalCommit": "9c7b8e45679eab960fb634a91eafe0b0f5d28e9e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDU4MTI4Nw==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r534581287", "bodyText": "Cool. Yeah, let's use that. So many util goodies.", "author": "jsancio", "createdAt": "2020-12-03T00:45:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzc1NjY1OQ=="}], "type": "inlineReview"}, {"oid": "10f1d5cea71910398979c368842b34ac964646bb", "url": "https://github.com/apache/kafka/commit/10f1d5cea71910398979c368842b34ac964646bb", "message": "Store snapshots directly in the partition directory", "committedDate": "2020-12-03T00:58:46Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDU5OTA4Nw==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r534599087", "bodyText": "Would it be useful to call this in a finally?", "author": "hachikuji", "createdAt": "2020-12-03T01:31:18Z", "path": "raft/src/main/java/org/apache/kafka/snapshot/FileRawSnapshotWriter.java", "diffHunk": "@@ -74,18 +74,19 @@ public void freeze() throws IOException {\n         frozen = true;\n \n         // Set readonly and ignore the result\n-        if (!path.toFile().setReadOnly()) {\n-            throw new IOException(String.format(\"Unable to set file %s as read-only\", path));\n+        if (!tempSnapshotPath.toFile().setReadOnly()) {\n+            throw new IOException(String.format(\"Unable to set file (%s) as read-only\", tempSnapshotPath));\n         }\n \n-        Path destination = Snapshots.moveRename(path, snapshotId);\n-        Files.move(path, destination, StandardCopyOption.ATOMIC_MOVE);\n+        Path destination = Snapshots.moveRename(tempSnapshotPath, snapshotId);\n+        Files.move(tempSnapshotPath, destination, StandardCopyOption.ATOMIC_MOVE);\n     }\n \n     @Override\n     public void close() throws IOException {\n         channel.close();\n-        Files.deleteIfExists(path);\n+        // This is a noop if freeze was called before calling close\n+        Files.deleteIfExists(tempSnapshotPath);", "originalCommit": "10f1d5cea71910398979c368842b34ac964646bb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ4MjI2Mg==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r535482262", "bodyText": "Yes!", "author": "jsancio", "createdAt": "2020-12-03T18:31:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDU5OTA4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYwNzI2NA==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r534607264", "bodyText": "nit: can just do buildRecords(ByteBuffer.wrap(randomBytes(bufferSize)))", "author": "hachikuji", "createdAt": "2020-12-03T01:53:20Z", "path": "raft/src/test/java/org/apache/kafka/snapshot/FileRawSnapshotTest.java", "diffHunk": "@@ -0,0 +1,289 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.snapshot;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.stream.IntStream;\n+import org.apache.kafka.common.record.BufferSupplier.GrowableBufferSupplier;\n+import org.apache.kafka.common.record.CompressionType;\n+import org.apache.kafka.common.record.MemoryRecords;\n+import org.apache.kafka.common.record.RecordBatch;\n+import org.apache.kafka.common.record.SimpleRecord;\n+import org.apache.kafka.common.record.Record;\n+import org.apache.kafka.raft.OffsetAndEpoch;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.jupiter.api.Test;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+public final class FileRawSnapshotTest {\n+    @Test\n+    public void testWritingSnapshot() throws IOException {\n+        Path tempDir = TestUtils.tempDirectory().toPath();\n+        OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(10L, 3);\n+        int bufferSize = 256;\n+        int batches = 10;\n+        int expectedSize = 0;\n+\n+        try (FileRawSnapshotWriter snapshot = FileRawSnapshotWriter.create(tempDir, offsetAndEpoch)) {\n+            assertEquals(0, snapshot.sizeInBytes());\n+\n+            MemoryRecords records = buildRecords(new ByteBuffer[] {ByteBuffer.wrap(randomBytes(bufferSize))});", "originalCommit": "10f1d5cea71910398979c368842b34ac964646bb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYwOTE3Nw==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r534609177", "bodyText": "It might be useful to review the history behind Utils.atomicMoveWithFallback. It's not clear to me why this case is different from some of the other situations that it is used.", "author": "hachikuji", "createdAt": "2020-12-03T01:58:32Z", "path": "raft/src/main/java/org/apache/kafka/snapshot/FileRawSnapshotWriter.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.snapshot;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardCopyOption;\n+import java.nio.file.StandardOpenOption;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.raft.OffsetAndEpoch;\n+\n+public final class FileRawSnapshotWriter implements RawSnapshotWriter {\n+    private final Path tempSnapshotPath;\n+    private final FileChannel channel;\n+    private final OffsetAndEpoch snapshotId;\n+    private boolean frozen = false;\n+\n+    private FileRawSnapshotWriter(\n+        Path tempSnapshotPath,\n+        FileChannel channel,\n+        OffsetAndEpoch snapshotId\n+    ) {\n+        this.tempSnapshotPath = tempSnapshotPath;\n+        this.channel = channel;\n+        this.snapshotId = snapshotId;\n+    }\n+\n+    @Override\n+    public OffsetAndEpoch snapshotId() {\n+        return snapshotId;\n+    }\n+\n+    @Override\n+    public long sizeInBytes() throws IOException {\n+        return channel.size();\n+    }\n+\n+    @Override\n+    public void append(ByteBuffer buffer) throws IOException {\n+        if (frozen) {\n+            throw new IllegalStateException(\n+                String.format(\"Append not supported. Snapshot is already frozen: id = %s; temp path = %s\", snapshotId, tempSnapshotPath)\n+            );\n+        }\n+\n+        Utils.writeFully(channel, buffer);\n+    }\n+\n+    @Override\n+    public boolean isFrozen() {\n+        return frozen;\n+    }\n+\n+    @Override\n+    public void freeze() throws IOException {\n+        channel.close();\n+        frozen = true;\n+\n+        // Set readonly and ignore the result\n+        if (!tempSnapshotPath.toFile().setReadOnly()) {\n+            throw new IOException(String.format(\"Unable to set file (%s) as read-only\", tempSnapshotPath));\n+        }\n+\n+        Path destination = Snapshots.moveRename(tempSnapshotPath, snapshotId);\n+        Files.move(tempSnapshotPath, destination, StandardCopyOption.ATOMIC_MOVE);", "originalCommit": "10f1d5cea71910398979c368842b34ac964646bb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ3NDA0MA==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r535474040", "bodyText": "I investigated the issue a bit. It look like this utility was introduce to replace these lines in OffsetCheckpoint.scala:\n// swap new offset checkpoint file with previous one\nif(!temp.renameTo(file)) {\n    // renameTo() fails on Windows if the destination file exists.\n    file.delete()\n    if(!temp.renameTo(file))\n        throw new IOException(...)\n}\nLooking at the JDK implementation in Windows, ATOMIC_MOVE should work in Windows if the target file exists: https://github.com/openjdk/jdk/blob/master/src/java.base/windows/classes/sun/nio/fs/WindowsFileCopy.java#L298-L312\nIn other words, I think we can keep the code as is in this PR.\ncc @ijuma", "author": "jsancio", "createdAt": "2020-12-03T18:20:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYwOTE3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU3MjAzMw==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r535572033", "bodyText": "It's not only Windows, NFS also has some restrictions. Why don't you want to use the utility method?", "author": "ijuma", "createdAt": "2020-12-03T20:28:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYwOTE3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU3NDE4NA==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r535574184", "bodyText": "Also, I would not rely on reading the code to assume one way or another. You'd want to test it too.", "author": "ijuma", "createdAt": "2020-12-03T20:30:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYwOTE3Nw=="}], "type": "inlineReview"}, {"oid": "aa500b9839322903770011696849e44064292bd1", "url": "https://github.com/apache/kafka/commit/aa500b9839322903770011696849e44064292bd1", "message": "Always attempt to delete the temporary snapshot file", "committedDate": "2020-12-03T18:31:10Z", "type": "commit"}, {"oid": "f853c50e61e179daf239a46fd8b77bbae6eeefca", "url": "https://github.com/apache/kafka/commit/f853c50e61e179daf239a46fd8b77bbae6eeefca", "message": "Use `atomicMoveWithFallback`", "committedDate": "2020-12-07T19:19:59Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc1NzQ4Nw==", "url": "https://github.com/apache/kafka/pull/9512#discussion_r537757487", "bodyText": "nit: we cab use covariantCast?", "author": "hachikuji", "createdAt": "2020-12-07T19:08:19Z", "path": "raft/src/test/java/org/apache/kafka/raft/MockLog.java", "diffHunk": "@@ -472,4 +490,106 @@ private EpochStartOffset(int epoch, long startOffset) {\n         }\n     }\n \n+    final class MockRawSnapshotWriter implements RawSnapshotWriter {\n+        private final OffsetAndEpoch snapshotId;\n+        private ByteBufferOutputStream data;\n+        private boolean frozen;\n+\n+        public MockRawSnapshotWriter(OffsetAndEpoch snapshotId) {\n+            this.snapshotId = snapshotId;\n+            this.data = new ByteBufferOutputStream(0);\n+            this.frozen = false;\n+        }\n+\n+        @Override\n+        public OffsetAndEpoch snapshotId() {\n+            return snapshotId;\n+        }\n+\n+        @Override\n+        public long sizeInBytes() {\n+            return data.position();\n+        }\n+\n+        @Override\n+        public void append(ByteBuffer buffer) {\n+            if (frozen) {\n+                throw new RuntimeException(\"Snapshot is already frozen \" + snapshotId);\n+            }\n+\n+            data.write(buffer);\n+        }\n+\n+        @Override\n+        public boolean isFrozen() {\n+            return frozen;\n+        }\n+\n+        @Override\n+        public void freeze() {\n+            if (frozen) {\n+                throw new RuntimeException(\"Snapshot is already frozen \" + snapshotId);\n+            }\n+\n+            frozen = true;\n+            ByteBuffer buffer = data.buffer();\n+            buffer.flip();\n+\n+            snapshots.putIfAbsent(snapshotId, new MockRawSnapshotReader(snapshotId, buffer));\n+        }\n+\n+        @Override\n+        public void close() {}\n+    }\n+\n+    final static class MockRawSnapshotReader implements RawSnapshotReader {\n+        private final OffsetAndEpoch snapshotId;\n+        private final MemoryRecords data;\n+\n+        MockRawSnapshotReader(OffsetAndEpoch snapshotId, ByteBuffer data) {\n+            this.snapshotId = snapshotId;\n+            this.data = MemoryRecords.readableRecords(data);\n+        }\n+\n+        @Override\n+        public OffsetAndEpoch snapshotId() {\n+            return snapshotId;\n+        }\n+\n+        @Override\n+        public long sizeInBytes() {\n+            return data.sizeInBytes();\n+        }\n+\n+        @Override\n+        public Iterator<RecordBatch> iterator() {\n+            return new Iterator<RecordBatch>() {", "originalCommit": "aa500b9839322903770011696849e44064292bd1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}