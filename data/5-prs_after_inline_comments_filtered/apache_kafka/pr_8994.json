{"pr_number": 8994, "pr_title": "KAFKA-10247: Correctly reset state when task is corrupted", "pr_createdAt": "2020-07-08T17:43:12Z", "pr_url": "https://github.com/apache/kafka/pull/8994", "timeline": [{"oid": "15661551a76cbe1d52422c4ed539262bf139a4de", "url": "https://github.com/apache/kafka/commit/15661551a76cbe1d52422c4ed539262bf139a4de", "message": "KAFKA-10247: Skip processing if task isn't running", "committedDate": "2020-07-10T02:06:53Z", "type": "commit"}, {"oid": "00ef72788fdb643bc7c8f8b1f58f3cbd0ee66258", "url": "https://github.com/apache/kafka/commit/00ef72788fdb643bc7c8f8b1f58f3cbd0ee66258", "message": "rebase on trunk", "committedDate": "2020-07-10T02:12:01Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjYyMDkwNQ==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452620905", "bodyText": "Nit: As reset policy is set on a per topic basis, it's sufficient to list the topic names -- it does not add value if we list the partitions, because all assigned partitions would be affected anyway.", "author": "mjsax", "createdAt": "2020-07-10T04:52:22Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -768,9 +770,30 @@ void runOnce() {\n \n     private void resetInvalidOffsets(final InvalidOffsetException e) {\n         final Set<TopicPartition> partitions = e.partitions();\n+        final Set<TopicPartition> notReset = resetOffsets(partitions);\n+        if (!notReset.isEmpty()) {\n+            final String notResetString =\n+                notReset.stream()\n+                        .map(tp -> \"topic \" + tp.topic() + \"(partition \" + tp.partition() + \")\")\n+                        .collect(Collectors.joining(\",\"));", "originalCommit": "00ef72788fdb643bc7c8f8b1f58f3cbd0ee66258", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjg4OTY0Mw==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452889643", "bodyText": "Interesting; good point. I just copied over the original message. Do you think I should drop the partition?", "author": "vvcephei", "createdAt": "2020-07-10T14:45:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjYyMDkwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjkzOTMyMw==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452939323", "bodyText": "I think it would be good. Seems the original message should not have contained the partition to begin with. Even if for the original one, it's less severe because we only consider a single topic-partition in the error message.\nFor this new case, we get multiple topic-partitions and the error message might become quite large? Also, if multiple task of the same sub-topology are assigned, the same topic name might appear multiple times for different partitions? Might be best to only add a Set or topic names to the error message?", "author": "mjsax", "createdAt": "2020-07-10T16:12:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjYyMDkwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk0ODYwOA==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452948608", "bodyText": "Ok, I've done this.", "author": "vvcephei", "createdAt": "2020-07-10T16:30:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjYyMDkwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjYyMjE2MA==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452622160", "bodyText": "I think we should fail for this case, because if user configures \"none\" they request that we fail if we loose track of the offset.", "author": "mjsax", "createdAt": "2020-07-10T04:58:03Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -193,6 +196,26 @@ private void closeAndRevive(final Map<Task, Collection<TopicPartition>> taskWith\n                 log.error(\"Error suspending corrupted task {} \", task.id(), swallow);\n             }\n             task.closeDirty();\n+            if (task.isActive()) {\n+                // Pause so we won't poll any more records for this task until it has been re-initialized\n+                // Note, closeDirty already clears the partitiongroup for the task.\n+                final Set<TopicPartition> currentAssignment = mainConsumer().assignment();\n+                final Set<TopicPartition> assignedToPauseAndReset =\n+                    Utils.intersection(HashSet::new, currentAssignment, task.inputPartitions());\n+\n+                mainConsumer().pause(assignedToPauseAndReset);\n+                final Map<TopicPartition, OffsetAndMetadata> committed = mainConsumer().committed(assignedToPauseAndReset);\n+                for (final Map.Entry<TopicPartition, OffsetAndMetadata> committedEntry : committed.entrySet()) {\n+                    final OffsetAndMetadata offsetAndMetadata = committedEntry.getValue();\n+                    if (offsetAndMetadata != null) {\n+                        mainConsumer().seek(committedEntry.getKey(), offsetAndMetadata);\n+                        assignedToPauseAndReset.remove(committedEntry.getKey());\n+                    }\n+                }\n+                final Set<TopicPartition> remainder = resetter.apply(assignedToPauseAndReset);\n+                // If anything didn't have a configured policy, reset to beginning\n+                mainConsumer().seekToBeginning(remainder);", "originalCommit": "00ef72788fdb643bc7c8f8b1f58f3cbd0ee66258", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk0OTIzMA==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452949230", "bodyText": "Thanks. I was confused by the behaviour of the StreamThreadTest. I think there was something funny in the StreamThread factory, which just never surfaced before. I'll call it out for your review.", "author": "vvcephei", "createdAt": "2020-07-10T16:31:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjYyMjE2MA=="}], "type": "inlineReview"}, {"oid": "60ed4e27134758750c0fd24adbfba52010577327", "url": "https://github.com/apache/kafka/commit/60ed4e27134758750c0fd24adbfba52010577327", "message": "cr comments", "committedDate": "2020-07-10T16:29:37Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1MTI4Ng==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452951286", "bodyText": "I had to change this to get the StreamThread test to actually use the configured ConsumerConfig.AUTO_OFFSET_RESET_CONFIG := earliest\nReading the conditional, it doesn't make any sense to me, but it's been in the codebase for a long time, so I'm doubting myself. It seems to say that we will only use the provided client configuration if there is an override, but it seems like it should have been \"if there is not an override\".\nRegardless, the \"originalReset\" is only used as a fallback after we apply the builder reset patterns, so I don't see why we should leave it null in any case.", "author": "vvcephei", "createdAt": "2020-07-10T16:35:24Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -359,11 +359,8 @@ public static StreamThread create(final InternalTopologyBuilder builder,\n         consumerConfigs.put(StreamsConfig.InternalConfig.ASSIGNMENT_ERROR_CODE, assignmentErrorCode);\n         final AtomicLong nextScheduledRebalanceMs = new AtomicLong(Long.MAX_VALUE);\n         consumerConfigs.put(StreamsConfig.InternalConfig.NEXT_SCHEDULED_REBALANCE_MS, nextScheduledRebalanceMs);\n-        String originalReset = null;\n-        if (!builder.latestResetTopicsPattern().pattern().equals(\"\") || !builder.earliestResetTopicsPattern().pattern().equals(\"\")) {\n-            originalReset = (String) consumerConfigs.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG);\n-            consumerConfigs.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"none\");\n-        }", "originalCommit": "60ed4e27134758750c0fd24adbfba52010577327", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk4NDc0Mw==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452984743", "bodyText": "This makes my head hurt..", "author": "ableegoldman", "createdAt": "2020-07-10T17:44:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1MTI4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk5NTA0MA==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452995040", "bodyText": "If users specify an in-code overwrite via Consumed the user might do this only for some topics. For all other topics, the configures reset policy should be used. Hence, we just \"remember\" the default config in originalReset and set the consumer config to \"none\".\nIf there is no in-code overwrite, we don't need to remember the original strategy because all topics use the same strategy and thus we rely on the consumer anyway to do the reset for us.\nWith this change, we now always set the reset policy to \"none\". What is still possible of course, but it implies, we never rely on the consumer any longer to do any reset and we always to it manually. (This might be \"cleaner\" as it might be easier to reason about.) -- But the old code was correct, too.", "author": "mjsax", "createdAt": "2020-07-10T18:06:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1MTI4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk5NTc3MQ==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452995771", "bodyText": "I guess the main reason for this old design was, that user see their config when ConsumerConfig is logged. -- Now, we would always log \"none\" independent what the users sets, and this might be confusing.\nI think we should update the docs accordingly and call it out.", "author": "mjsax", "createdAt": "2020-07-10T18:07:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1MTI4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA4NjczNA==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453086734", "bodyText": "Aha! Thanks, @mjsax. This makes sense. I hadn't considered that the consumer never throws InvalidOffsetException if there's a valid policy configured, so the prior logic only needs the fallback when there's an override.\nNow that we also use the resetting logic for our own \"manual\" reset when handling TaskCorruptedException, we do need to capture the Consumer config value, but we can still enforce that if there are any overrides in the topology, we set the consumer to \"none\". Just updated the code to do this.", "author": "vvcephei", "createdAt": "2020-07-10T21:32:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1MTI4Ng=="}], "type": "inlineReview"}, {"oid": "568dceb5f61c71428432d23d947c5f9b29fb7bfb", "url": "https://github.com/apache/kafka/commit/568dceb5f61c71428432d23d947c5f9b29fb7bfb", "message": "cr comments", "committedDate": "2020-07-10T16:39:49Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk5NjU2NA==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r452996564", "bodyText": "This method is a one-liner now and is only called in a single place IIRC. Maybe better to remove the method and embed the call?", "author": "mjsax", "createdAt": "2020-07-10T18:09:18Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -767,38 +766,65 @@ void runOnce() {\n     }\n \n     private void resetInvalidOffsets(final InvalidOffsetException e) {", "originalCommit": "568dceb5f61c71428432d23d947c5f9b29fb7bfb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA4NzQ5Mw==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453087493", "bodyText": "Good catch! I'll inline it.", "author": "vvcephei", "createdAt": "2020-07-10T21:34:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk5NjU2NA=="}], "type": "inlineReview"}, {"oid": "a876630e98c9efea68a849fcc9759eac2ee488a4", "url": "https://github.com/apache/kafka/commit/a876630e98c9efea68a849fcc9759eac2ee488a4", "message": "CR comment", "committedDate": "2020-07-10T21:21:46Z", "type": "commit"}, {"oid": "f20a4710d4620bad771d7bdac486601b6e4cb7d3", "url": "https://github.com/apache/kafka/commit/f20a4710d4620bad771d7bdac486601b6e4cb7d3", "message": "partially revert originalReset logic", "committedDate": "2020-07-10T21:29:39Z", "type": "commit"}, {"oid": "5ae9c240d2856a40169ca6e6362cadc7415b47df", "url": "https://github.com/apache/kafka/commit/5ae9c240d2856a40169ca6e6362cadc7415b47df", "message": "inline the reset functions", "committedDate": "2020-07-10T21:40:57Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwNTgwNQ==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453105805", "bodyText": "We we need this isEmpty check? What happens is we blindly pass an empty set into seekToBeginning? -- Similar for seekToEnd() below?", "author": "mjsax", "createdAt": "2020-07-10T22:31:31Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -760,44 +763,62 @@ void runOnce() {\n         try {\n             records = mainConsumer.poll(pollTime);\n         } catch (final InvalidOffsetException e) {\n-            resetInvalidOffsets(e);\n+            resetOffsets(e.partitions(), e);\n         }\n \n         return records;\n     }\n \n-    private void resetInvalidOffsets(final InvalidOffsetException e) {\n-        final Set<TopicPartition> partitions = e.partitions();\n+    private void resetOffsets(final Set<TopicPartition> partitions, final Exception cause) {\n         final Set<String> loggedTopics = new HashSet<>();\n         final Set<TopicPartition> seekToBeginning = new HashSet<>();\n         final Set<TopicPartition> seekToEnd = new HashSet<>();\n+        final Set<TopicPartition> notReset = new HashSet<>();\n \n         for (final TopicPartition partition : partitions) {\n             if (builder.earliestResetTopicsPattern().matcher(partition.topic()).matches()) {\n                 addToResetList(partition, seekToBeginning, \"Setting topic '{}' to consume from {} offset\", \"earliest\", loggedTopics);\n             } else if (builder.latestResetTopicsPattern().matcher(partition.topic()).matches()) {\n                 addToResetList(partition, seekToEnd, \"Setting topic '{}' to consume from {} offset\", \"latest\", loggedTopics);\n             } else {\n-                if (originalReset == null || (!originalReset.equals(\"earliest\") && !originalReset.equals(\"latest\"))) {\n-                    final String errorMessage = \"No valid committed offset found for input topic %s (partition %s) and no valid reset policy configured.\" +\n-                        \" You need to set configuration parameter \\\"auto.offset.reset\\\" or specify a topic specific reset \" +\n-                        \"policy via StreamsBuilder#stream(..., Consumed.with(Topology.AutoOffsetReset)) or StreamsBuilder#table(..., Consumed.with(Topology.AutoOffsetReset))\";\n-                    throw new StreamsException(String.format(errorMessage, partition.topic(), partition.partition()), e);\n-                }\n-\n-                if (originalReset.equals(\"earliest\")) {\n+                if (\"earliest\".equals(originalReset)) {\n                     addToResetList(partition, seekToBeginning, \"No custom setting defined for topic '{}' using original config '{}' for offset reset\", \"earliest\", loggedTopics);\n-                } else { // can only be \"latest\"\n+                } else if (\"latest\".equals(originalReset)) {\n                     addToResetList(partition, seekToEnd, \"No custom setting defined for topic '{}' using original config '{}' for offset reset\", \"latest\", loggedTopics);\n+                } else {\n+                    notReset.add(partition);\n                 }\n             }\n         }\n \n-        if (!seekToBeginning.isEmpty()) {\n-            mainConsumer.seekToBeginning(seekToBeginning);\n-        }\n-        if (!seekToEnd.isEmpty()) {\n-            mainConsumer.seekToEnd(seekToEnd);\n+        if (notReset.isEmpty()) {\n+            if (!seekToBeginning.isEmpty()) {", "originalCommit": "5ae9c240d2856a40169ca6e6362cadc7415b47df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwNzcwMA==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453107700", "bodyText": "Huh, I didn't wonder that before, but ... dear god. From the javadoc on KafkaConsumer:\n\nIf no partitions are provided, seek to the first offset for all of the currently assigned partitions.\n\nWhat a dangerous API!", "author": "vvcephei", "createdAt": "2020-07-10T22:39:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwNTgwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyMTU1MQ==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453121551", "bodyText": "Just verified that we never call it unguarded by a non-empty check.", "author": "vvcephei", "createdAt": "2020-07-10T23:35:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwNTgwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzMTY3MQ==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453131671", "bodyText": "Awesome! Thanks for double checking!", "author": "mjsax", "createdAt": "2020-07-11T00:33:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwNTgwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwNjYwNg==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453106606", "bodyText": "For my own education: when does the actual resume() happen (and are we sure those partitions are resumed later?)", "author": "mjsax", "createdAt": "2020-07-10T22:34:35Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -193,6 +195,34 @@ private void closeAndRevive(final Map<Task, Collection<TopicPartition>> taskWith\n                 log.error(\"Error suspending corrupted task {} \", task.id(), swallow);\n             }\n             task.closeDirty();\n+            if (task.isActive()) {\n+                // Pause so we won't poll any more records for this task until it has been re-initialized\n+                // Note, closeDirty already clears the partitiongroup for the task.\n+                final Set<TopicPartition> currentAssignment = mainConsumer().assignment();\n+                final Set<TopicPartition> taskInputPartitions = task.inputPartitions();\n+                final Set<TopicPartition> assignedToPauseAndReset =\n+                    intersection(HashSet::new, currentAssignment, taskInputPartitions);\n+                if (!assignedToPauseAndReset.equals(taskInputPartitions)) {\n+                    log.warn(\n+                        \"Expected the current consumer assignment {} to contain the input partitions {}. \" +\n+                            \"Will proceed to recover.\",\n+                        currentAssignment,\n+                        taskInputPartitions\n+                    );\n+                }\n+\n+                mainConsumer().pause(assignedToPauseAndReset);", "originalCommit": "5ae9c240d2856a40169ca6e6362cadc7415b47df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwODAzMA==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453108030", "bodyText": "Good question!\nIt goes through the \"restoration\" phase, at the end of which, we resume the main consumer's partitions.\nIn TaskManager:\n        if (allRunning) {\n            // we can call resume multiple times since it is idempotent.\n            mainConsumer.resume(mainConsumer.assignment());\n        }", "author": "vvcephei", "createdAt": "2020-07-10T22:40:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwNjYwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwOTc0MQ==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453109741", "bodyText": "Do we need the sink?", "author": "mjsax", "createdAt": "2020-07-10T22:47:18Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java", "diffHunk": "@@ -1177,6 +1180,109 @@ public void shouldNotCloseTaskAndRemoveFromTaskManagerIfProducerGotFencedInCommi\n         assertEquals(1, thread.activeTasks().size());\n     }\n \n+    @Test\n+    public void shouldReinitializeRevivedTasksInAnyState() {\n+        final StreamThread thread = createStreamThread(CLIENT_ID, new StreamsConfig(configProps(false)), false);\n+\n+        final String storeName = \"store\";\n+        final String storeChangelog = \"stream-thread-test-store-changelog\";\n+        final TopicPartition storeChangelogTopicPartition = new TopicPartition(storeChangelog, 1);\n+\n+        internalTopologyBuilder.addSource(null, \"name\", null, null, null, topic1);\n+        final AtomicBoolean shouldThrow = new AtomicBoolean(false);\n+        final AtomicBoolean processed = new AtomicBoolean(false);\n+        internalTopologyBuilder.addProcessor(\"proc\", new ProcessorSupplier<Object, Object>() {\n+            @Override\n+            public Processor<Object, Object> get() {\n+                return new Processor<Object, Object>() {\n+                    private ProcessorContext context;\n+\n+                    @Override\n+                    public void init(final ProcessorContext context) {\n+                        this.context = context;\n+                    }\n+\n+                    @Override\n+                    public void process(final Object key, final Object value) {\n+                        if (shouldThrow.get()) {\n+                            throw new TaskCorruptedException(singletonMap(task1, new HashSet<TopicPartition>(singleton(storeChangelogTopicPartition))));\n+                        } else {\n+                            processed.set(true);\n+                        }\n+                    }\n+\n+                    @Override\n+                    public void close() {\n+\n+                    }\n+                };\n+            }\n+        }, \"name\");\n+        internalTopologyBuilder.addStateStore(\n+            Stores.keyValueStoreBuilder(\n+                Stores.persistentKeyValueStore(storeName),\n+                Serdes.String(),\n+                Serdes.String()\n+            ),\n+            \"proc\"\n+        );\n+        internalTopologyBuilder.addSink(\"out\", \"output\", null, null, null, \"proc\");", "originalCommit": "5ae9c240d2856a40169ca6e6362cadc7415b47df", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyMTA5Mw==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453121093", "bodyText": "Uh, no. It was just copied with the rest of the test. Sorry about that.", "author": "vvcephei", "createdAt": "2020-07-10T23:33:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwOTc0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzE1NTYyNA==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453155624", "bodyText": "fixed this, since I needed more commits anyway.", "author": "vvcephei", "createdAt": "2020-07-11T04:54:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEwOTc0MQ=="}], "type": "inlineReview"}, {"oid": "a378948c89335bc5bd91f9101275776671080041", "url": "https://github.com/apache/kafka/commit/a378948c89335bc5bd91f9101275776671080041", "message": "fix race condition", "committedDate": "2020-07-11T04:37:08Z", "type": "commit"}, {"oid": "7a8fbcc37a3f9881be49852b8a300b046a90a920", "url": "https://github.com/apache/kafka/commit/7a8fbcc37a3f9881be49852b8a300b046a90a920", "message": "remove unnecessary node from test", "committedDate": "2020-07-11T04:41:10Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzE1NTk0OQ==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453155949", "bodyText": "The flaky test was related to us going into this block from other states. I finally got a clue when one of the tests failed on \"invalid transition from PARTITIONS_REVOKED to RUNNING\". I'm not sure how, exactly, but I think the shutdown test that failed on ConcurrentModificationException was also related, probably due to the test invoking the handleAssignment/Revocation/Lost methods from a different thread (which can normally never happen).\nAnyway, my prior code only intended to add the self transition, but failed to make sure we were actually in a self-transition. It's fixed now.", "author": "vvcephei", "createdAt": "2020-07-11T04:58:02Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -648,7 +651,9 @@ void runOnce() {\n \n         // only try to initialize the assigned tasks\n         // if the state is still in PARTITION_ASSIGNED after the poll call\n-        if (state == State.PARTITIONS_ASSIGNED) {\n+        if (state == State.PARTITIONS_ASSIGNED\n+            || state == State.RUNNING && taskManager.needsInitializationOrRestoration()) {", "originalCommit": "7a8fbcc37a3f9881be49852b8a300b046a90a920", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIyMjUyMw==", "url": "https://github.com/apache/kafka/pull/8994#discussion_r453222523", "bodyText": "Thanks. The fix makes sense to me.", "author": "mjsax", "createdAt": "2020-07-11T18:38:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzE1NTk0OQ=="}], "type": "inlineReview"}]}