{"pr_number": 8720, "pr_title": "KAFKA-9971: Error Reporting in Sink Connectors (KIP-610)", "pr_createdAt": "2020-05-23T01:00:41Z", "pr_url": "https://github.com/apache/kafka/pull/8720", "timeline": [{"oid": "26034508ea5c799768484c70df2a06fe0f8c53c9", "url": "https://github.com/apache/kafka/commit/26034508ea5c799768484c70df2a06fe0f8c53c9", "message": "KAFKA-9971: Error Reporting in Sink Connectors\n\nSigned-off-by: Aakash Shah <ashah@confluent.io>", "committedDate": "2020-05-27T19:26:35Z", "type": "commit"}, {"oid": "1134e990335001d732678c4def54738a4d71473d", "url": "https://github.com/apache/kafka/commit/1134e990335001d732678c4def54738a4d71473d", "message": "addressed comments\n\nSigned-off-by: Aakash Shah <ashah@confluent.io>", "committedDate": "2020-05-27T19:26:35Z", "type": "commit"}, {"oid": "cd043161ad3a430f4c341fb86b4a4f31aded0e5b", "url": "https://github.com/apache/kafka/commit/cd043161ad3a430f4c341fb86b4a4f31aded0e5b", "message": "addressed more comments\n\nSigned-off-by: Aakash Shah <ashah@confluent.io>", "committedDate": "2020-05-27T19:26:35Z", "type": "commit"}, {"oid": "b91a8f989aa3dcfccf9d3498717c0517b013c677", "url": "https://github.com/apache/kafka/commit/b91a8f989aa3dcfccf9d3498717c0517b013c677", "message": "addressed some more comments", "committedDate": "2020-05-27T19:26:35Z", "type": "commit"}, {"oid": "b91a8f989aa3dcfccf9d3498717c0517b013c677", "url": "https://github.com/apache/kafka/commit/b91a8f989aa3dcfccf9d3498717c0517b013c677", "message": "addressed some more comments", "committedDate": "2020-05-27T19:26:35Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM5NTU3OQ==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431395579", "bodyText": "We should not make this method a default method, since both implementations of the interface define this method.", "author": "rhauch", "createdAt": "2020-05-27T19:34:37Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java", "diffHunk": "@@ -16,17 +16,25 @@\n  */\n package org.apache.kafka.connect.runtime.errors;\n \n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Future;\n+\n /**\n  * Report an error using the information contained in the {@link ProcessingContext}.\n  */\n public interface ErrorReporter extends AutoCloseable {\n \n     /**\n-     * Report an error.\n+     * Report an error and return the producer future.\n      *\n      * @param context the processing context (cannot be null).\n+     * @return future result from the producer sending a record to Kafka.\n      */\n-    void report(ProcessingContext context);\n+    default Future<RecordMetadata> report(ProcessingContext context) {\n+        return CompletableFuture.completedFuture(null);\n+    }", "originalCommit": "b91a8f989aa3dcfccf9d3498717c0517b013c677", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM5NzA1NA==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431397054", "bodyText": "What about creating:\n    private static final Future<RecordMetadata> COMPLETED = CompletableFuture.completedFuture(null);\n\nand then returning that instance in all of these places. Since it's already completed, immutable, and we don't allow cancellation, it should be fine to reuse in this LogReporter.", "author": "rhauch", "createdAt": "2020-05-27T19:37:27Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java", "diffHunk": "@@ -50,17 +53,18 @@ public LogReporter(ConnectorTaskId id, ConnectorConfig connConfig, ErrorHandling\n      * @param context the processing context.\n      */\n     @Override\n-    public void report(ProcessingContext context) {\n+    public Future<RecordMetadata> report(ProcessingContext context) {\n         if (!connConfig.enableErrorLog()) {\n-            return;\n+            return CompletableFuture.completedFuture(null);\n         }\n \n         if (!context.failed()) {\n-            return;\n+            return CompletableFuture.completedFuture(null);\n         }\n \n         log.error(message(context), context.error());\n         errorHandlingMetrics.recordErrorLogged();\n+        return CompletableFuture.completedFuture(null);", "originalCommit": "b91a8f989aa3dcfccf9d3498717c0517b013c677", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQwMDc4Nw==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431400787", "bodyText": "How about clarifying this a bit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        String topic = record.topic();\n          \n          \n            \n                        // Generate a new consumer record from the modified sink record. We prefer\n          \n          \n            \n                        // to send the original consumer record (pre-transformed) to the DLQ, \n          \n          \n            \n                        // but in this case we don't have one and send the potentially transformed\n          \n          \n            \n                        // record instead\n          \n          \n            \n                        String topic = record.topic();", "author": "rhauch", "createdAt": "2020-05-27T19:44:44Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private RetryWithToleranceOperator retryWithToleranceOperator;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    final LinkedList<Future<Void>> futures;\n+\n+    public WorkerErrantRecordReporter(\n+        RetryWithToleranceOperator retryWithToleranceOperator,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        this.retryWithToleranceOperator = retryWithToleranceOperator;\n+        this.keyConverter = keyConverter;\n+        this.valueConverter = valueConverter;\n+        this.headerConverter = headerConverter;\n+        this.futures = new LinkedList<>();\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+        ConsumerRecord<byte[], byte[]> consumerRecord;\n+\n+        // Most of the records will be an internal sink record, but the task could potentially\n+        // report modified or new records, so handle both cases\n+        if (record instanceof InternalSinkRecord) {\n+            consumerRecord = ((InternalSinkRecord) record).originalRecord();\n+        } else {\n+            String topic = record.topic();", "originalCommit": "b91a8f989aa3dcfccf9d3498717c0517b013c677", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQwMjEyMw==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431402123", "bodyText": "We should use the length of the key and value in the record:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(),\n          \n          \n            \n                            record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, -1,\n          \n          \n            \n                            -1, key, value, headers);\n          \n          \n            \n                        int keyLength = key != null ? key.length : -1;\n          \n          \n            \n                        int valLength = value != null ? value.length : -1;\n          \n          \n            \n                        consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(),\n          \n          \n            \n                            record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, keyLength,\n          \n          \n            \n                            valLength, key, value, headers);", "author": "rhauch", "createdAt": "2020-05-27T19:47:20Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private RetryWithToleranceOperator retryWithToleranceOperator;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    final LinkedList<Future<Void>> futures;\n+\n+    public WorkerErrantRecordReporter(\n+        RetryWithToleranceOperator retryWithToleranceOperator,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        this.retryWithToleranceOperator = retryWithToleranceOperator;\n+        this.keyConverter = keyConverter;\n+        this.valueConverter = valueConverter;\n+        this.headerConverter = headerConverter;\n+        this.futures = new LinkedList<>();\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+        ConsumerRecord<byte[], byte[]> consumerRecord;\n+\n+        // Most of the records will be an internal sink record, but the task could potentially\n+        // report modified or new records, so handle both cases\n+        if (record instanceof InternalSinkRecord) {\n+            consumerRecord = ((InternalSinkRecord) record).originalRecord();\n+        } else {\n+            String topic = record.topic();\n+            byte[] key = keyConverter.fromConnectData(topic, record.keySchema(), record.key());\n+            byte[] value = valueConverter.fromConnectData(topic,\n+                record.valueSchema(), record.value());\n+\n+            RecordHeaders headers = new RecordHeaders();\n+            if (record.headers() != null) {\n+                for (Header header : record.headers()) {\n+                    String headerKey = header.key();\n+                    byte[] rawHeader = headerConverter.fromConnectHeader(topic, headerKey,\n+                        header.schema(), header.value());\n+                    headers.add(headerKey, rawHeader);\n+                }\n+            }\n+\n+            consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(),\n+                record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, -1,\n+                -1, key, value, headers);", "originalCommit": "b91a8f989aa3dcfccf9d3498717c0517b013c677", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQwNzg1NA==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431407854", "bodyText": "Once again, please add trace log messages before an after this line.", "author": "rhauch", "createdAt": "2020-05-27T19:58:44Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java", "diffHunk": "@@ -360,6 +364,10 @@ private void doCommit(Map<TopicPartition, OffsetAndMetadata> offsets, boolean cl\n     }\n \n     private void commitOffsets(long now, boolean closing) {\n+        if (workerErrantRecordReporter != null) {\n+            workerErrantRecordReporter.awaitAllFutures();", "originalCommit": "b91a8f989aa3dcfccf9d3498717c0517b013c677", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "ab1af2bd15f4a05974ec55bcae34ad39ece1aad3", "url": "https://github.com/apache/kafka/commit/ab1af2bd15f4a05974ec55bcae34ad39ece1aad3", "message": "KAFKA-9971: Added trace logging", "committedDate": "2020-05-27T20:02:55Z", "type": "commit"}, {"oid": "0e40408c23c1086dddf01d741a5e27a86fc2ab9a", "url": "https://github.com/apache/kafka/commit/0e40408c23c1086dddf01d741a5e27a86fc2ab9a", "message": "more comments", "committedDate": "2020-05-27T21:24:31Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ3NDExNg==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431474116", "bodyText": "Hmm, let's just have this delegate to the super method. It's internal, so we need not include the original record details.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return \"InternalSinkRecord{\" +\n          \n          \n            \n                            \"consumerRecord=\" + originalRecord.toString() +\n          \n          \n            \n                            \"} \" + super.toString();\n          \n          \n            \n                    return super.toString();", "author": "rhauch", "createdAt": "2020-05-27T22:16:37Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+public class InternalSinkRecord extends SinkRecord {\n+\n+    private final ConsumerRecord<byte[], byte[]> originalRecord;\n+\n+    public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, SinkRecord record) {\n+        super(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(),\n+            record.valueSchema(), record.value(), record.kafkaOffset(), record.timestamp(),\n+            record.timestampType(), record.headers());\n+        this.originalRecord = originalRecord;\n+    }\n+\n+    public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic,\n+                              int partition, Schema keySchema, Object key, Schema valueSchema,\n+                              Object value, long kafkaOffset, Long timestamp,\n+                              TimestampType timestampType, Iterable<Header> headers) {\n+        super(topic, partition, keySchema, key, valueSchema, value, kafkaOffset, timestamp, timestampType, headers);\n+        this.originalRecord = originalRecord;\n+    }\n+\n+    @Override\n+    public SinkRecord newRecord(String topic, Integer kafkaPartition, Schema keySchema, Object key,\n+                                Schema valueSchema, Object value, Long timestamp,\n+                                Iterable<Header> headers) {\n+        return new InternalSinkRecord(originalRecord, topic, kafkaPartition, keySchema, key,\n+            valueSchema, value, kafkaOffset(), timestamp, timestampType(), headers());\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        return super.equals(o);\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return super.hashCode();\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"InternalSinkRecord{\" +\n+                \"consumerRecord=\" + originalRecord.toString() +\n+                \"} \" + super.toString();", "originalCommit": "0e40408c23c1086dddf01d741a5e27a86fc2ab9a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ3NDI5Nw==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431474297", "bodyText": "Let's make this protected.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic,\n          \n          \n            \n                protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic,", "author": "rhauch", "createdAt": "2020-05-27T22:17:06Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+public class InternalSinkRecord extends SinkRecord {\n+\n+    private final ConsumerRecord<byte[], byte[]> originalRecord;\n+\n+    public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, SinkRecord record) {\n+        super(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(),\n+            record.valueSchema(), record.value(), record.kafkaOffset(), record.timestamp(),\n+            record.timestampType(), record.headers());\n+        this.originalRecord = originalRecord;\n+    }\n+\n+    public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic,", "originalCommit": "0e40408c23c1086dddf01d741a5e27a86fc2ab9a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ3NTg0Ng==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431475846", "bodyText": "These can be final.", "author": "rhauch", "createdAt": "2020-05-27T22:21:14Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private RetryWithToleranceOperator retryWithToleranceOperator;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private HeaderConverter headerConverter;", "originalCommit": "0e40408c23c1086dddf01d741a5e27a86fc2ab9a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ3NjI3MA==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431476270", "bodyText": "Nit: new line is unnecessary, and there's a misspelling:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            log.error(\"Encountered an error while awaiting an errant record future's \" +\n          \n          \n            \n                                \"completition.\");\n          \n          \n            \n                            log.error(\"Encountered an error while awaiting an errant record future's completion.\");", "author": "rhauch", "createdAt": "2020-05-27T22:22:19Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private RetryWithToleranceOperator retryWithToleranceOperator;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    final LinkedList<Future<Void>> futures;\n+\n+    public WorkerErrantRecordReporter(\n+        RetryWithToleranceOperator retryWithToleranceOperator,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        this.retryWithToleranceOperator = retryWithToleranceOperator;\n+        this.keyConverter = keyConverter;\n+        this.valueConverter = valueConverter;\n+        this.headerConverter = headerConverter;\n+        this.futures = new LinkedList<>();\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+        ConsumerRecord<byte[], byte[]> consumerRecord;\n+\n+        // Most of the records will be an internal sink record, but the task could potentially\n+        // report modified or new records, so handle both cases\n+        if (record instanceof InternalSinkRecord) {\n+            consumerRecord = ((InternalSinkRecord) record).originalRecord();\n+        } else {\n+            // Generate a new consumer record from the modified sink record. We prefer\n+            // to send the original consumer record (pre-transformed) to the DLQ,\n+            // but in this case we don't have one and send the potentially transformed\n+            // record instead\n+            String topic = record.topic();\n+            byte[] key = keyConverter.fromConnectData(topic, record.keySchema(), record.key());\n+            byte[] value = valueConverter.fromConnectData(topic,\n+                record.valueSchema(), record.value());\n+\n+            RecordHeaders headers = new RecordHeaders();\n+            if (record.headers() != null) {\n+                for (Header header : record.headers()) {\n+                    String headerKey = header.key();\n+                    byte[] rawHeader = headerConverter.fromConnectHeader(topic, headerKey,\n+                        header.schema(), header.value());\n+                    headers.add(headerKey, rawHeader);\n+                }\n+            }\n+\n+            int keyLength = key != null ? key.length : -1;\n+            int valLength = value != null ? value.length : -1;\n+\n+            consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(),\n+                record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, keyLength,\n+                valLength, key, value, headers);\n+        }\n+\n+        Future<Void> future = retryWithToleranceOperator.executeFailed(Stage.TASK_PUT,\n+            SinkTask.class, consumerRecord, error);\n+\n+        if (!future.isDone()) {\n+            futures.add(future);\n+        }\n+        return future;\n+    }\n+\n+    /**\n+     * Gets all futures returned by the sink records sent to Kafka by the errant\n+     * record reporter. This function is intended to be used to block on all the errant record\n+     * futures.\n+     */\n+    public void awaitAllFutures() {\n+        Future<?> future = null;\n+        while ((future = futures.poll()) != null) {\n+            try {\n+                future.get();\n+            } catch (InterruptedException | ExecutionException e) {\n+                log.error(\"Encountered an error while awaiting an errant record future's \" +\n+                    \"completition.\");", "originalCommit": "0e40408c23c1086dddf01d741a5e27a86fc2ab9a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "126b04c568381dc108db87da27ddd4862a5e0d2b", "url": "https://github.com/apache/kafka/commit/126b04c568381dc108db87da27ddd4862a5e0d2b", "message": "more comments again", "committedDate": "2020-05-27T22:58:05Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUxMjAxOQ==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431512019", "bodyText": "nit: indentation is a bit off here.", "author": "kkonstantine", "createdAt": "2020-05-28T00:10:55Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+public class InternalSinkRecord extends SinkRecord {\n+\n+    private final ConsumerRecord<byte[], byte[]> originalRecord;\n+\n+    public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, SinkRecord record) {\n+        super(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(),\n+            record.valueSchema(), record.value(), record.kafkaOffset(), record.timestamp(),\n+            record.timestampType(), record.headers());\n+        this.originalRecord = originalRecord;\n+    }\n+\n+    protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic,\n+                              int partition, Schema keySchema, Object key, Schema valueSchema,\n+                              Object value, long kafkaOffset, Long timestamp,\n+                              TimestampType timestampType, Iterable<Header> headers) {", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUxNDAxNQ==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431514015", "bodyText": "these overrides don't seem to add much.", "author": "kkonstantine", "createdAt": "2020-05-28T00:17:44Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+public class InternalSinkRecord extends SinkRecord {\n+\n+    private final ConsumerRecord<byte[], byte[]> originalRecord;\n+\n+    public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, SinkRecord record) {\n+        super(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(),\n+            record.valueSchema(), record.value(), record.kafkaOffset(), record.timestamp(),\n+            record.timestampType(), record.headers());\n+        this.originalRecord = originalRecord;\n+    }\n+\n+    protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic,\n+                              int partition, Schema keySchema, Object key, Schema valueSchema,\n+                              Object value, long kafkaOffset, Long timestamp,\n+                              TimestampType timestampType, Iterable<Header> headers) {\n+        super(topic, partition, keySchema, key, valueSchema, value, kafkaOffset, timestamp, timestampType, headers);\n+        this.originalRecord = originalRecord;\n+    }\n+\n+    @Override\n+    public SinkRecord newRecord(String topic, Integer kafkaPartition, Schema keySchema, Object key,\n+                                Schema valueSchema, Object value, Long timestamp,\n+                                Iterable<Header> headers) {\n+        return new InternalSinkRecord(originalRecord, topic, kafkaPartition, keySchema, key,\n+            valueSchema, value, kafkaOffset(), timestamp, timestampType(), headers());\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        return super.equals(o);\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return super.hashCode();\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return super.toString();\n+    }", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU0NDg0Mg==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431544842", "bodyText": "IIUC, spotbugs complained if these were not here.", "author": "rhauch", "createdAt": "2020-05-28T02:17:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUxNDAxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU1NzcxNw==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431557717", "bodyText": "never mind then. I'll leave this to AI.", "author": "kkonstantine", "createdAt": "2020-05-28T03:11:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUxNDAxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUxNDMzNg==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431514336", "bodyText": "nit: I think we always keep = with the left operand.", "author": "kkonstantine", "createdAt": "2020-05-28T00:18:41Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java", "diffHunk": "@@ -553,13 +554,16 @@ private WorkerTask buildWorkerTask(ClusterConfigState configState,\n             log.info(\"Initializing: {}\", transformationChain);\n             SinkConnectorConfig sinkConfig = new SinkConnectorConfig(plugins, connConfig.originalsStrings());\n             retryWithToleranceOperator.reporters(sinkTaskReporters(id, sinkConfig, errorHandlingMetrics, connectorClass));\n+            WorkerErrantRecordReporter workerErrantRecordReporter\n+                = createWorkerErrantRecordReporter(sinkConfig, retryWithToleranceOperator,", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUxNjcyNA==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431516724", "bodyText": "should be final right?", "author": "kkonstantine", "createdAt": "2020-05-28T00:27:30Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java", "diffHunk": "@@ -94,6 +95,7 @@\n     private int commitFailures;\n     private boolean pausedForRedelivery;\n     private boolean committing;\n+    private WorkerErrantRecordReporter workerErrantRecordReporter;", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUyMjUzNA==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431522534", "bodyText": "let's add protected here to be symmetric to what other fields that are accessed by the context have as scope", "author": "kkonstantine", "createdAt": "2020-05-28T00:49:31Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java", "diffHunk": "@@ -518,6 +535,10 @@ private Headers convertHeadersFor(ConsumerRecord<byte[], byte[]> record) {\n         return result;\n     }\n \n+    WorkerErrantRecordReporter workerErrantRecordReporter() {", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUyMjkwOQ==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431522909", "bodyText": "typo. Not sure how you want to say it.", "author": "kkonstantine", "createdAt": "2020-05-28T00:50:57Z", "path": "connect/api/src/main/java/org/apache/kafka/connect/sink/ErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.sink;\n+\n+import java.util.concurrent.Future;\n+import org.apache.kafka.connect.errors.ConnectException;\n+\n+/**\n+ * Component that the sink task can use as it {@link SinkTask#put(java.util.Collection)}.", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUyMzYxOA==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431523618", "bodyText": "A class javadoc would be helpful, in order to understand why this is introduced.", "author": "kkonstantine", "createdAt": "2020-05-28T00:53:45Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+public class InternalSinkRecord extends SinkRecord {", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUzNDgyNw==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431534827", "bodyText": "nit: it was correct before", "author": "kkonstantine", "createdAt": "2020-05-28T01:38:01Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java", "diffHunk": "@@ -111,41 +115,41 @@ public static DeadLetterQueueReporter createAndSetup(Map<String, Object> adminPr\n         this.connConfig = connConfig;\n         this.connectorTaskId = id;\n         this.errorHandlingMetrics = errorHandlingMetrics;\n+        this.dlqTopicName = connConfig.dlqTopicName().trim();\n     }\n \n     /**\n-     * Write the raw records into a Kafka topic.\n+     * Write the raw records into a Kafka topic and return the producer future.\n      *\n      * @param context processing context containing the raw record at {@link ProcessingContext#consumerRecord()}.\n+     * @return the future associated with the writing of this record; never null\n      */\n-    public void report(ProcessingContext context) {\n-        final String dlqTopicName = connConfig.dlqTopicName();\n+    public Future<RecordMetadata> report(ProcessingContext context) {\n         if (dlqTopicName.isEmpty()) {\n-            return;\n+            return CompletableFuture.completedFuture(null);\n         }\n-\n         errorHandlingMetrics.recordDeadLetterQueueProduceRequest();\n \n         ConsumerRecord<byte[], byte[]> originalMessage = context.consumerRecord();\n         if (originalMessage == null) {\n             errorHandlingMetrics.recordDeadLetterQueueProduceFailed();\n-            return;\n+            return CompletableFuture.completedFuture(null);\n         }\n \n         ProducerRecord<byte[], byte[]> producerRecord;\n         if (originalMessage.timestamp() == RecordBatch.NO_TIMESTAMP) {\n             producerRecord = new ProducerRecord<>(dlqTopicName, null,\n-                    originalMessage.key(), originalMessage.value(), originalMessage.headers());", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUzNDg0Mw==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431534843", "bodyText": "nit: it was correct before", "author": "kkonstantine", "createdAt": "2020-05-28T01:38:07Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java", "diffHunk": "@@ -111,41 +115,41 @@ public static DeadLetterQueueReporter createAndSetup(Map<String, Object> adminPr\n         this.connConfig = connConfig;\n         this.connectorTaskId = id;\n         this.errorHandlingMetrics = errorHandlingMetrics;\n+        this.dlqTopicName = connConfig.dlqTopicName().trim();\n     }\n \n     /**\n-     * Write the raw records into a Kafka topic.\n+     * Write the raw records into a Kafka topic and return the producer future.\n      *\n      * @param context processing context containing the raw record at {@link ProcessingContext#consumerRecord()}.\n+     * @return the future associated with the writing of this record; never null\n      */\n-    public void report(ProcessingContext context) {\n-        final String dlqTopicName = connConfig.dlqTopicName();\n+    public Future<RecordMetadata> report(ProcessingContext context) {\n         if (dlqTopicName.isEmpty()) {\n-            return;\n+            return CompletableFuture.completedFuture(null);\n         }\n-\n         errorHandlingMetrics.recordDeadLetterQueueProduceRequest();\n \n         ConsumerRecord<byte[], byte[]> originalMessage = context.consumerRecord();\n         if (originalMessage == null) {\n             errorHandlingMetrics.recordDeadLetterQueueProduceFailed();\n-            return;\n+            return CompletableFuture.completedFuture(null);\n         }\n \n         ProducerRecord<byte[], byte[]> producerRecord;\n         if (originalMessage.timestamp() == RecordBatch.NO_TIMESTAMP) {\n             producerRecord = new ProducerRecord<>(dlqTopicName, null,\n-                    originalMessage.key(), originalMessage.value(), originalMessage.headers());\n+                originalMessage.key(), originalMessage.value(), originalMessage.headers());\n         } else {\n             producerRecord = new ProducerRecord<>(dlqTopicName, null, originalMessage.timestamp(),\n-                    originalMessage.key(), originalMessage.value(), originalMessage.headers());", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUzNTQ2Nw==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431535467", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                final LinkedList<Future<Void>> futures;\n          \n          \n            \n                protected final List<Future<Void>> futures;", "author": "kkonstantine", "createdAt": "2020-05-28T01:40:39Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private final RetryWithToleranceOperator retryWithToleranceOperator;\n+    private final Converter keyConverter;\n+    private final Converter valueConverter;\n+    private final HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    final LinkedList<Future<Void>> futures;", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU2NDY3MQ==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431564671", "bodyText": "Unfortunately, this doesn't let for poll() to be recognized.", "author": "aakashnshah", "createdAt": "2020-05-28T03:42:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUzNTQ2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTYxMjczNw==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431612737", "bodyText": "Is that for tests? Anyway, we can revisit in a cleanup in the future.", "author": "kkonstantine", "createdAt": "2020-05-28T06:39:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUzNTQ2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUzODgzMg==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431538832", "bodyText": "Suggestion (can't add because of the deleted line):\n        List<Future<RecordMetadata>> futures = reporters.stream()\n                .map(r -> r.report(this))\n                .filter(Future::isDone)\n                .collect(Collectors.toCollection(LinkedList::new));", "author": "kkonstantine", "createdAt": "2020-05-28T01:53:49Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ProcessingContext.java", "diffHunk": "@@ -132,11 +138,25 @@ public void currentContext(Stage stage, Class<?> klass) {\n \n     /**\n      * Report errors. Should be called only if an error was encountered while executing the operation.\n+     *\n+     * @return a errant record future that potentially aggregates the producer futures\n      */\n-    public void report() {\n+    public Future<Void> report() {\n+        if (reporters.size() == 1) {\n+            return new ErrantRecordFuture(Collections.singletonList(reporters.iterator().next().report(this)));\n+        }\n+\n+        List<Future<RecordMetadata>> futures = new LinkedList<>();", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUzOTQxNA==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431539414", "bodyText": "nit: initialization is not required", "author": "kkonstantine", "createdAt": "2020-05-28T01:55:58Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private final RetryWithToleranceOperator retryWithToleranceOperator;\n+    private final Converter keyConverter;\n+    private final Converter valueConverter;\n+    private final HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    final LinkedList<Future<Void>> futures;\n+\n+    public WorkerErrantRecordReporter(\n+        RetryWithToleranceOperator retryWithToleranceOperator,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        this.retryWithToleranceOperator = retryWithToleranceOperator;\n+        this.keyConverter = keyConverter;\n+        this.valueConverter = valueConverter;\n+        this.headerConverter = headerConverter;\n+        this.futures = new LinkedList<>();\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+        ConsumerRecord<byte[], byte[]> consumerRecord;\n+\n+        // Most of the records will be an internal sink record, but the task could potentially\n+        // report modified or new records, so handle both cases\n+        if (record instanceof InternalSinkRecord) {\n+            consumerRecord = ((InternalSinkRecord) record).originalRecord();\n+        } else {\n+            // Generate a new consumer record from the modified sink record. We prefer\n+            // to send the original consumer record (pre-transformed) to the DLQ,\n+            // but in this case we don't have one and send the potentially transformed\n+            // record instead\n+            String topic = record.topic();\n+            byte[] key = keyConverter.fromConnectData(topic, record.keySchema(), record.key());\n+            byte[] value = valueConverter.fromConnectData(topic,\n+                record.valueSchema(), record.value());\n+\n+            RecordHeaders headers = new RecordHeaders();\n+            if (record.headers() != null) {\n+                for (Header header : record.headers()) {\n+                    String headerKey = header.key();\n+                    byte[] rawHeader = headerConverter.fromConnectHeader(topic, headerKey,\n+                        header.schema(), header.value());\n+                    headers.add(headerKey, rawHeader);\n+                }\n+            }\n+\n+            int keyLength = key != null ? key.length : -1;\n+            int valLength = value != null ? value.length : -1;\n+\n+            consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(),\n+                record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, keyLength,\n+                valLength, key, value, headers);\n+        }\n+\n+        Future<Void> future = retryWithToleranceOperator.executeFailed(Stage.TASK_PUT,\n+            SinkTask.class, consumerRecord, error);\n+\n+        if (!future.isDone()) {\n+            futures.add(future);\n+        }\n+        return future;\n+    }\n+\n+    /**\n+     * Gets all futures returned by the sink records sent to Kafka by the errant\n+     * record reporter. This function is intended to be used to block on all the errant record\n+     * futures.\n+     */\n+    public void awaitAllFutures() {\n+        Future<?> future = null;", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU0MDI3Ng==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431540276", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        for (Future<RecordMetadata> future: futures) {\n          \n          \n            \n                            if (!future.isDone()) {\n          \n          \n            \n                                return false;\n          \n          \n            \n                            }\n          \n          \n            \n                        }\n          \n          \n            \n                        return true;\n          \n          \n            \n                        return futures.stream().allMatch(Future::isDone);", "author": "kkonstantine", "createdAt": "2020-05-28T01:59:32Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private final RetryWithToleranceOperator retryWithToleranceOperator;\n+    private final Converter keyConverter;\n+    private final Converter valueConverter;\n+    private final HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    final LinkedList<Future<Void>> futures;\n+\n+    public WorkerErrantRecordReporter(\n+        RetryWithToleranceOperator retryWithToleranceOperator,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        this.retryWithToleranceOperator = retryWithToleranceOperator;\n+        this.keyConverter = keyConverter;\n+        this.valueConverter = valueConverter;\n+        this.headerConverter = headerConverter;\n+        this.futures = new LinkedList<>();\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+        ConsumerRecord<byte[], byte[]> consumerRecord;\n+\n+        // Most of the records will be an internal sink record, but the task could potentially\n+        // report modified or new records, so handle both cases\n+        if (record instanceof InternalSinkRecord) {\n+            consumerRecord = ((InternalSinkRecord) record).originalRecord();\n+        } else {\n+            // Generate a new consumer record from the modified sink record. We prefer\n+            // to send the original consumer record (pre-transformed) to the DLQ,\n+            // but in this case we don't have one and send the potentially transformed\n+            // record instead\n+            String topic = record.topic();\n+            byte[] key = keyConverter.fromConnectData(topic, record.keySchema(), record.key());\n+            byte[] value = valueConverter.fromConnectData(topic,\n+                record.valueSchema(), record.value());\n+\n+            RecordHeaders headers = new RecordHeaders();\n+            if (record.headers() != null) {\n+                for (Header header : record.headers()) {\n+                    String headerKey = header.key();\n+                    byte[] rawHeader = headerConverter.fromConnectHeader(topic, headerKey,\n+                        header.schema(), header.value());\n+                    headers.add(headerKey, rawHeader);\n+                }\n+            }\n+\n+            int keyLength = key != null ? key.length : -1;\n+            int valLength = value != null ? value.length : -1;\n+\n+            consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(),\n+                record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, keyLength,\n+                valLength, key, value, headers);\n+        }\n+\n+        Future<Void> future = retryWithToleranceOperator.executeFailed(Stage.TASK_PUT,\n+            SinkTask.class, consumerRecord, error);\n+\n+        if (!future.isDone()) {\n+            futures.add(future);\n+        }\n+        return future;\n+    }\n+\n+    /**\n+     * Gets all futures returned by the sink records sent to Kafka by the errant\n+     * record reporter. This function is intended to be used to block on all the errant record\n+     * futures.\n+     */\n+    public void awaitAllFutures() {\n+        Future<?> future = null;\n+        while ((future = futures.poll()) != null) {\n+            try {\n+                future.get();\n+            } catch (InterruptedException | ExecutionException e) {\n+                log.error(\"Encountered an error while awaiting an errant record future's completion.\");\n+                throw new ConnectException(e);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Wrapper class to aggregate producer futures and abstract away the record metadata from the\n+     * Connect user.\n+     */\n+    public static class ErrantRecordFuture implements Future<Void> {\n+\n+        private final List<Future<RecordMetadata>> futures;\n+\n+        public ErrantRecordFuture(List<Future<RecordMetadata>> producerFutures) {\n+            futures = producerFutures;\n+        }\n+\n+        public boolean cancel(boolean mayInterruptIfRunning) {\n+            throw new UnsupportedOperationException(\"Reporting an errant record cannot be cancelled.\");\n+        }\n+\n+        public boolean isCancelled() {\n+            return false;\n+        }\n+\n+        public boolean isDone() {\n+            for (Future<RecordMetadata> future: futures) {\n+                if (!future.isDone()) {\n+                    return false;\n+                }\n+            }\n+            return true;", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU0MDk3MA==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431540970", "bodyText": "nit: extra blank line\n\n  \n    \n      \n        Suggested change", "author": "kkonstantine", "createdAt": "2020-05-28T02:02:04Z", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/ErrantRecordSinkConnector.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.connect.integration;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.connect.connector.Task;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class ErrantRecordSinkConnector extends MonitorableSinkConnector {\n+\n+    @Override\n+    public Class<? extends Task> taskClass() {\n+        return ErrantRecordSinkTask.class;\n+    }\n+\n+    public static class ErrantRecordSinkTask extends MonitorableSinkTask {\n+        private ErrantRecordReporter reporter;\n+\n+        public ErrantRecordSinkTask() {\n+            super();\n+        }\n+\n+        @Override\n+        public void start(Map<String, String> props) {\n+            super.start(props);\n+            reporter = context.errantRecordReporter();\n+        }\n+\n+        @Override\n+        public void put(Collection<SinkRecord> records) {\n+", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU0MTI5MQ==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431541291", "bodyText": "Suggested change", "author": "kkonstantine", "createdAt": "2020-05-28T02:03:22Z", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/ExampleConnectIntegrationTest.java", "diffHunk": "@@ -219,6 +223,72 @@ public void testSourceConnector() throws Exception {\n         connect.deleteConnector(CONNECTOR_NAME);\n     }\n \n+    @Test\n+    public void testErrantRecordReporter() throws Exception {\n+        connect.kafka().createTopic(DLQ_TOPIC, 1);\n+        // create test topic\n+        connect.kafka().createTopic(\"test-topic\", NUM_TOPIC_PARTITIONS);\n+\n+        // setup up props for the sink connector\n+        Map<String, String> props = new HashMap<>();\n+        props.put(CONNECTOR_CLASS_CONFIG, ERRANT_RECORD_SINK_CONNECTOR_CLASS_NAME);\n+        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));\n+        props.put(TOPICS_CONFIG, \"test-topic\");\n+        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());\n+        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());\n+        props.put(DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC);\n+\n+        // expect all records to be consumed by the connector\n+        connectorHandle.expectedRecords(NUM_RECORDS_PRODUCED);\n+\n+        // expect all records to be consumed by the connector\n+        connectorHandle.expectedCommits(NUM_RECORDS_PRODUCED);\n+\n+        // validate the intended connector configuration, a config that errors\n+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(ERRANT_RECORD_SINK_CONNECTOR_CLASS_NAME, props, 1,\n+            \"Validating connector configuration produced an unexpected number or errors.\");\n+\n+        // add missing configuration to make the config valid\n+        props.put(\"name\", CONNECTOR_NAME);\n+\n+        // validate the intended connector configuration, a valid config\n+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(ERRANT_RECORD_SINK_CONNECTOR_CLASS_NAME, props, 0,\n+            \"Validating connector configuration produced an unexpected number or errors.\");\n+\n+        // start a sink connector\n+        connect.configureConnector(CONNECTOR_NAME, props);\n+\n+        waitForCondition(this::checkForPartitionAssignment,\n+            CONNECTOR_SETUP_DURATION_MS,\n+            \"Connector tasks were not assigned a partition each.\");\n+\n+        // produce some messages into source topic partitions\n+        for (int i = 0; i < NUM_RECORDS_PRODUCED; i++) {\n+            connect.kafka().produce(\"test-topic\", i % NUM_TOPIC_PARTITIONS, \"key\", \"simple-message-value-\" + i);\n+        }\n+\n+        // consume all records from the source topic or fail, to ensure that they were correctly produced.\n+        assertEquals(\"Unexpected number of records consumed\", NUM_RECORDS_PRODUCED,\n+            connect.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"test-topic\").count());\n+\n+        // wait for the connector tasks to consume all records.\n+        connectorHandle.awaitRecords(RECORD_TRANSFER_DURATION_MS);\n+\n+        // wait for the connector tasks to commit all records.\n+        connectorHandle.awaitCommits(RECORD_TRANSFER_DURATION_MS);\n+\n+        // consume all records from the dlq topic or fail, to ensure that they were correctly produced\n+        int recordNum = connect.kafka().consume(\n+            NUM_RECORDS_PRODUCED,\n+            RECORD_TRANSFER_DURATION_MS,\n+            DLQ_TOPIC\n+        ).count();\n+\n+        // delete connector\n+        connect.deleteConnector(CONNECTOR_NAME);\n+", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "1c4442fffdc3b163458bf42b7af29183062c636f", "url": "https://github.com/apache/kafka/commit/1c4442fffdc3b163458bf42b7af29183062c636f", "message": "addressed some more comments", "committedDate": "2020-05-28T03:55:43Z", "type": "commit"}]}