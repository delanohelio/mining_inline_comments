{"pr_number": 8514, "pr_title": "MINOR: Further reduce runtime for metrics integration tests", "pr_createdAt": "2020-04-19T00:24:01Z", "pr_url": "https://github.com/apache/kafka/pull/8514", "timeline": [{"oid": "27fefff2508444c2d58e94e3b936660d62b340e0", "url": "https://github.com/apache/kafka/commit/27fefff2508444c2d58e94e3b936660d62b340e0", "message": "reduce runtime further for metrics integration tests", "committedDate": "2020-04-19T00:16:10Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDk2NDcyNw==", "url": "https://github.com/apache/kafka/pull/8514#discussion_r410964727", "bodyText": "Why do we write into STREAM_INPUT_TWO with 3 calls instead of just one call passing in all 3 records at once?", "author": "mjsax", "createdAt": "2020-04-19T18:39:20Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/RocksDBMetricsIntegrationTest.java", "diffHunk": "@@ -214,70 +176,55 @@ private StreamsBuilder builderForSegmentedStateStore() {\n                     .withRetention(WINDOW_SIZE))\n             .toStream()\n             .map((key, value) -> KeyValue.pair(value, value))\n-            .to(STREAM_OUTPUT, Produced.with(Serdes.Long(), Serdes.Long()));\n+            .to(STREAM_OUTPUT_TWO, Produced.with(Serdes.Long(), Serdes.Long()));\n         return builder;\n     }\n \n     private void cleanUpStateRunVerifyAndClose(final StreamsBuilder builder,\n                                                final Properties streamsConfiguration,\n-                                               final Class outputKeyDeserializer,\n-                                               final Class outputValueDeserializer,\n-                                               final MetricsVerifier metricsVerifier,\n-                                               final String metricsScope) throws Exception {\n+                                               final MetricsVerifier metricsVerifier) throws Exception {\n         final KafkaStreams kafkaStreams = new KafkaStreams(builder.build(), streamsConfiguration);\n         kafkaStreams.cleanUp();\n         produceRecords();\n \n         StreamsTestUtils.startKafkaStreamsAndWaitForRunningState(kafkaStreams, TIMEOUT);\n \n-        IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(\n-            TestUtils.consumerConfig(\n-                CLUSTER.bootstrapServers(),\n-                \"consumerApp\",\n-                outputKeyDeserializer,\n-                outputValueDeserializer,\n-                new Properties()\n-            ),\n-            STREAM_OUTPUT,\n-            1\n-        );\n-        metricsVerifier.verify(kafkaStreams, metricsScope);\n+        metricsVerifier.verify(kafkaStreams, \"rocksdb-state-id\");\n+        metricsVerifier.verify(kafkaStreams, \"rocksdb-window-state-id\");\n         kafkaStreams.close();\n     }\n \n     private void produceRecords() throws Exception {\n         final MockTime mockTime = new MockTime(WINDOW_SIZE.toMillis());\n+        final Properties prop = TestUtils.producerConfig(\n+            CLUSTER.bootstrapServers(),\n+            IntegerSerializer.class,\n+            StringSerializer.class,\n+            new Properties()\n+        );\n+        // non-segmented store do not need records with different timestamps\n         IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n-            STREAM_INPUT,\n-            Collections.singletonList(new KeyValue<>(1, \"A\")),\n-            TestUtils.producerConfig(\n-                CLUSTER.bootstrapServers(),\n-                IntegerSerializer.class,\n-                StringSerializer.class,\n-                new Properties()\n-            ),\n+            STREAM_INPUT_ONE,\n+            Utils.mkSet(new KeyValue<>(1, \"A\"), new KeyValue<>(1, \"B\"), new KeyValue<>(1, \"C\")),\n+            prop,\n             mockTime.milliseconds()\n         );\n         IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n-            STREAM_INPUT,\n-            Collections.singletonList(new KeyValue<>(1, \"B\")),\n-            TestUtils.producerConfig(\n-                CLUSTER.bootstrapServers(),\n-                IntegerSerializer.class,\n-                StringSerializer.class,\n-                new Properties()\n-            ),\n+            STREAM_INPUT_TWO,\n+            Collections.singleton(new KeyValue<>(1, \"A\")),", "originalCommit": "27fefff2508444c2d58e94e3b936660d62b340e0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDk3NTI5Mw==", "url": "https://github.com/apache/kafka/pull/8514#discussion_r410975293", "bodyText": "Note we use a mocktime with auto tick WINDOW_SIZE.toMillis(): and each time its milliseconds() is called it would auto-advance, we need the produced records with those advanced timestamps.", "author": "guozhangwang", "createdAt": "2020-04-19T19:36:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDk2NDcyNw=="}], "type": "inlineReview"}, {"oid": "50765792c168aca3bdb2a604f6f0172794998e7b", "url": "https://github.com/apache/kafka/commit/50765792c168aca3bdb2a604f6f0172794998e7b", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into KMinor-reduce-metrics-integration-runtime", "committedDate": "2020-04-19T19:38:03Z", "type": "commit"}, {"oid": "897ff76770290ecca0b2344a7ffb9a975f0309e8", "url": "https://github.com/apache/kafka/commit/897ff76770290ecca0b2344a7ffb9a975f0309e8", "message": "fix checkstyle", "committedDate": "2020-04-19T19:39:46Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTMwNDQ1Mg==", "url": "https://github.com/apache/kafka/pull/8514#discussion_r411304452", "bodyText": "Thank you! I forgot to delete this in my refactoring.", "author": "cadonna", "createdAt": "2020-04-20T11:32:39Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/RocksDBMetricsIntegrationTest.java", "diffHunk": "@@ -319,18 +266,6 @@ private void checkMetricByName(final List<Metric> listMetric,\n         }\n     }\n \n-    private void verifyThatBytesWrittenTotalIncreases(final KafkaStreams kafkaStreams,\n-                                                      final String metricsScope) throws InterruptedException {\n-        final List<Metric> metric = getRocksDBMetrics(kafkaStreams, metricsScope).stream()\n-            .filter(m -> BYTES_WRITTEN_TOTAL.equals(m.metricName().name()))\n-            .collect(Collectors.toList());\n-        TestUtils.waitForCondition(\n-            () -> (double) metric.get(0).metricValue() > 0,\n-            TIMEOUT,\n-            () -> \"RocksDB metric bytes.written.total did not increase in \" + TIMEOUT + \" ms\"\n-        );\n-    }\n-", "originalCommit": "897ff76770290ecca0b2344a7ffb9a975f0309e8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}