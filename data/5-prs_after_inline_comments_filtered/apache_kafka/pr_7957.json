{"pr_number": 7957, "pr_title": "KAFKA-8768: DeleteRecords request/response automated protocol", "pr_createdAt": "2020-01-14T13:55:40Z", "pr_url": "https://github.com/apache/kafka/pull/7957", "timeline": [{"oid": "71b6b461894f520a610dfc59b763908e16ddc361", "url": "https://github.com/apache/kafka/commit/71b6b461894f520a610dfc59b763908e16ddc361", "message": "KAFKA-8768: Replace DeleteRecords request/response with automated protocol\n\nAlso add version 2 to make use of flexible versions, per KIP-482.", "committedDate": "2020-02-25T10:49:03Z", "type": "commit"}, {"oid": "88617744e55ad81566df79789012f31b1dd12f7b", "url": "https://github.com/apache/kafka/commit/88617744e55ad81566df79789012f31b1dd12f7b", "message": "Fix comments", "committedDate": "2020-02-25T11:34:40Z", "type": "commit"}, {"oid": "88617744e55ad81566df79789012f31b1dd12f7b", "url": "https://github.com/apache/kafka/commit/88617744e55ad81566df79789012f31b1dd12f7b", "message": "Fix comments", "committedDate": "2020-02-25T11:34:40Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjIwNTM3NA==", "url": "https://github.com/apache/kafka/pull/7957#discussion_r392205374", "bodyText": "I think we can computeIfAbsent() here. It will replace containsKey, put and get.", "author": "mimaison", "createdAt": "2020-03-13T12:46:08Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -2469,19 +2474,30 @@ void handleResponse(AbstractResponse abstractResponse) {\n                 Cluster cluster = response.cluster();\n \n                 // Group topic partitions by leader\n-                Map<Node, Map<TopicPartition, Long>> leaders = new HashMap<>();\n+                Map<Node, Map<String, DeleteRecordsTopic>> leaders = new HashMap<>();\n                 for (Map.Entry<TopicPartition, RecordsToDelete> entry: recordsToDelete.entrySet()) {\n-                    KafkaFutureImpl<DeletedRecords> future = futures.get(entry.getKey());\n+                    TopicPartition topicPartition = entry.getKey();\n+                    KafkaFutureImpl<DeletedRecords> future = futures.get(topicPartition);\n \n                     // Fail partitions with topic errors\n-                    Errors topicError = errors.get(entry.getKey().topic());\n-                    if (errors.containsKey(entry.getKey().topic())) {\n+                    Errors topicError = errors.get(topicPartition.topic());\n+                    if (errors.containsKey(topicPartition.topic())) {\n                         future.completeExceptionally(topicError.exception());\n                     } else {\n-                        Node node = cluster.leaderFor(entry.getKey());\n+                        Node node = cluster.leaderFor(topicPartition);\n                         if (node != null) {\n-                            leaders.computeIfAbsent(node, key -> new HashMap<>()).put(entry.getKey(),\n-                                entry.getValue().beforeOffset());\n+                            if (!leaders.containsKey(node))", "originalCommit": "88617744e55ad81566df79789012f31b1dd12f7b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjIwNTg0NQ==", "url": "https://github.com/apache/kafka/pull/7957#discussion_r392205845", "bodyText": "Same here, I think computeIfAbsent would simplify the logic", "author": "mimaison", "createdAt": "2020-03-13T12:47:03Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -2469,19 +2474,30 @@ void handleResponse(AbstractResponse abstractResponse) {\n                 Cluster cluster = response.cluster();\n \n                 // Group topic partitions by leader\n-                Map<Node, Map<TopicPartition, Long>> leaders = new HashMap<>();\n+                Map<Node, Map<String, DeleteRecordsTopic>> leaders = new HashMap<>();\n                 for (Map.Entry<TopicPartition, RecordsToDelete> entry: recordsToDelete.entrySet()) {\n-                    KafkaFutureImpl<DeletedRecords> future = futures.get(entry.getKey());\n+                    TopicPartition topicPartition = entry.getKey();\n+                    KafkaFutureImpl<DeletedRecords> future = futures.get(topicPartition);\n \n                     // Fail partitions with topic errors\n-                    Errors topicError = errors.get(entry.getKey().topic());\n-                    if (errors.containsKey(entry.getKey().topic())) {\n+                    Errors topicError = errors.get(topicPartition.topic());\n+                    if (errors.containsKey(topicPartition.topic())) {\n                         future.completeExceptionally(topicError.exception());\n                     } else {\n-                        Node node = cluster.leaderFor(entry.getKey());\n+                        Node node = cluster.leaderFor(topicPartition);\n                         if (node != null) {\n-                            leaders.computeIfAbsent(node, key -> new HashMap<>()).put(entry.getKey(),\n-                                entry.getValue().beforeOffset());\n+                            if (!leaders.containsKey(node))\n+                                leaders.put(node, new HashMap<>());\n+                            Map<String, DeleteRecordsTopic> deletionsForLeader = leaders.get(node);\n+                            DeleteRecordsTopic deleteRecords = deletionsForLeader.get(topicPartition.topic());", "originalCommit": "88617744e55ad81566df79789012f31b1dd12f7b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjIwNzI4Nw==", "url": "https://github.com/apache/kafka/pull/7957#discussion_r392207287", "bodyText": "Can we return that directly without creating a variable?", "author": "mimaison", "createdAt": "2020-03-13T12:50:22Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -2490,36 +2506,45 @@ void handleResponse(AbstractResponse abstractResponse) {\n \n                 final long deleteRecordsCallTimeMs = time.milliseconds();\n \n-                for (final Map.Entry<Node, Map<TopicPartition, Long>> entry : leaders.entrySet()) {\n-                    final Map<TopicPartition, Long> partitionDeleteOffsets = entry.getValue();\n+                for (final Map.Entry<Node, Map<String, DeleteRecordsTopic>> entry : leaders.entrySet()) {\n+                    final Map<String, DeleteRecordsTopic> partitionDeleteOffsets = entry.getValue();\n                     final int brokerId = entry.getKey().id();\n \n                     runnable.call(new Call(\"deleteRecords\", deadline,\n                             new ConstantNodeIdProvider(brokerId)) {\n \n                         @Override\n                         DeleteRecordsRequest.Builder createRequest(int timeoutMs) {\n-                            return new DeleteRecordsRequest.Builder(timeoutMs, partitionDeleteOffsets);\n+                            return new DeleteRecordsRequest.Builder(new DeleteRecordsRequestData()\n+                                    .setTimeoutMs(timeoutMs)\n+                                    .setTopics(new ArrayList<>(partitionDeleteOffsets.values())));\n                         }\n \n                         @Override\n                         void handleResponse(AbstractResponse abstractResponse) {\n                             DeleteRecordsResponse response = (DeleteRecordsResponse) abstractResponse;\n-                            for (Map.Entry<TopicPartition, DeleteRecordsResponse.PartitionResponse> result: response.responses().entrySet()) {\n-\n-                                KafkaFutureImpl<DeletedRecords> future = futures.get(result.getKey());\n-                                if (result.getValue().error == Errors.NONE) {\n-                                    future.complete(new DeletedRecords(result.getValue().lowWatermark));\n-                                } else {\n-                                    future.completeExceptionally(result.getValue().error.exception());\n+                            for (DeleteRecordsTopicResult topicResult: response.data().topics()) {\n+                                for (DeleteRecordsResponseData.DeleteRecordsPartitionResult partitionResult : topicResult.partitions()) {\n+                                    KafkaFutureImpl<DeletedRecords> future = futures.get(new TopicPartition(topicResult.name(), partitionResult.partitionIndex()));\n+                                    if (partitionResult.errorCode() == Errors.NONE.code()) {\n+                                        future.complete(new DeletedRecords(partitionResult.lowWatermark()));\n+                                    } else {\n+                                        future.completeExceptionally(Errors.forCode(partitionResult.errorCode()).exception());\n+                                    }\n                                 }\n                             }\n                         }\n \n                         @Override\n                         void handleFailure(Throwable throwable) {\n                             Stream<KafkaFutureImpl<DeletedRecords>> callFutures =\n-                                    partitionDeleteOffsets.keySet().stream().map(futures::get);\n+                                    partitionDeleteOffsets.values().stream().flatMap(\n+                                        recordsToDelete1 -> {\n+                                            Stream<TopicPartition> topicPartitionStream = recordsToDelete1.partitions().stream().map(partitionsToDelete ->", "originalCommit": "88617744e55ad81566df79789012f31b1dd12f7b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjIwODQyOA==", "url": "https://github.com/apache/kafka/pull/7957#discussion_r392208428", "bodyText": "Let's keep the L", "author": "mimaison", "createdAt": "2020-03-13T12:52:52Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/DeleteRecordsResponse.java", "diffHunk": "@@ -17,64 +17,19 @@\n \n package org.apache.kafka.common.requests;\n \n-import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.message.DeleteRecordsResponseData;\n import org.apache.kafka.common.protocol.ApiKeys;\n import org.apache.kafka.common.protocol.Errors;\n-import org.apache.kafka.common.protocol.types.ArrayOf;\n-import org.apache.kafka.common.protocol.types.Field;\n-import org.apache.kafka.common.protocol.types.Schema;\n import org.apache.kafka.common.protocol.types.Struct;\n-import org.apache.kafka.common.utils.CollectionUtils;\n \n import java.nio.ByteBuffer;\n-import java.util.ArrayList;\n import java.util.HashMap;\n-import java.util.List;\n import java.util.Map;\n \n-import static org.apache.kafka.common.protocol.CommonFields.ERROR_CODE;\n-import static org.apache.kafka.common.protocol.CommonFields.PARTITION_ID;\n-import static org.apache.kafka.common.protocol.CommonFields.THROTTLE_TIME_MS;\n-import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;\n-import static org.apache.kafka.common.protocol.types.Type.INT64;\n-\n public class DeleteRecordsResponse extends AbstractResponse {\n \n-    public static final long INVALID_LOW_WATERMARK = -1L;\n-\n-    // request level key names\n-    private static final String TOPICS_KEY_NAME = \"topics\";\n-\n-    // topic level key names\n-    private static final String PARTITIONS_KEY_NAME = \"partitions\";\n-\n-    // partition level key names\n-    private static final String LOW_WATERMARK_KEY_NAME = \"low_watermark\";\n-\n-    private static final Schema DELETE_RECORDS_RESPONSE_PARTITION_V0 = new Schema(\n-            PARTITION_ID,\n-            new Field(LOW_WATERMARK_KEY_NAME, INT64, \"Smallest available offset of all live replicas\"),\n-            ERROR_CODE);\n-\n-    private static final Schema DELETE_RECORDS_RESPONSE_TOPIC_V0 = new Schema(\n-            TOPIC_NAME,\n-            new Field(PARTITIONS_KEY_NAME, new ArrayOf(DELETE_RECORDS_RESPONSE_PARTITION_V0)));\n-\n-    private static final Schema DELETE_RECORDS_RESPONSE_V0 = new Schema(\n-            THROTTLE_TIME_MS,\n-            new Field(TOPICS_KEY_NAME, new ArrayOf(DELETE_RECORDS_RESPONSE_TOPIC_V0)));\n-\n-    /**\n-     * The version number is bumped to indicate that on quota violation brokers send out responses before throttling.\n-     */\n-    private static final Schema DELETE_RECORDS_RESPONSE_V1 = DELETE_RECORDS_RESPONSE_V0;\n-\n-    public static Schema[] schemaVersions() {\n-        return new Schema[]{DELETE_RECORDS_RESPONSE_V0, DELETE_RECORDS_RESPONSE_V1};\n-    }\n-\n-    private final int throttleTimeMs;\n-    private final Map<TopicPartition, PartitionResponse> responses;\n+    public static final long INVALID_LOW_WATERMARK = -1;", "originalCommit": "88617744e55ad81566df79789012f31b1dd12f7b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "6b037108c621fc4cdc76ad2c64d376fc1bde6d97", "url": "https://github.com/apache/kafka/commit/6b037108c621fc4cdc76ad2c64d376fc1bde6d97", "message": "Review comments", "committedDate": "2020-03-13T13:59:49Z", "type": "commit"}]}