{"pr_number": 8065, "pr_title": "KAKFA-9503: Fix TopologyTestDriver output order", "pr_createdAt": "2020-02-07T23:23:58Z", "pr_url": "https://github.com/apache/kafka/pull/8065", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1OTIyOA==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r376859228", "bodyText": "Not sure why you flip this condition?", "author": "mjsax", "createdAt": "2020-02-10T03:53:05Z", "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -469,48 +465,76 @@ private void pipeRecord(final String topicName,\n             validateSourceTopicNameRegexPattern(topicName);\n         }\n         final TopicPartition topicPartition = getTopicPartition(topicName);\n-        if (topicPartition != null) {\n-            final long offset = offsetsByTopicPartition.get(topicPartition).incrementAndGet() - 1;\n-            task.addRecords(topicPartition, Collections.singleton(new ConsumerRecord<>(\n-                    topicName,\n-                    topicPartition.partition(),\n-                    offset,\n-                    timestamp,\n-                    TimestampType.CREATE_TIME,\n-                    (long) ConsumerRecord.NULL_CHECKSUM,\n-                    key == null ? ConsumerRecord.NULL_SIZE : key.length,\n-                    value == null ? ConsumerRecord.NULL_SIZE : value.length,\n-                    key,\n-                    value,\n-                    headers)));\n-\n-            // Process the record ...\n-            task.process(mockWallClockTime.milliseconds());\n-            task.maybePunctuateStreamTime();\n-            task.commit();\n-            captureOutputRecords();\n+\n+        if (topicPartition == null) {", "originalCommit": "5d18a92ac4b7d62a3f6fd1ab3e37bdc3ca3586eb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzE2NDMzNA==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377164334", "bodyText": "Just to improve readability.", "author": "vvcephei", "createdAt": "2020-02-10T16:16:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1OTIyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIyMjE0MQ==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377222141", "bodyText": "Personally I think it reduced readability \ud83d\ude1b", "author": "mjsax", "createdAt": "2020-02-10T17:55:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg1OTIyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg2MDE2MA==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r376860160", "bodyText": "nit: add a blank line before this one to make it easier to read", "author": "mjsax", "createdAt": "2020-02-10T03:59:32Z", "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -547,9 +571,12 @@ private void captureOutputRecords() {\n \n             // Forward back into the topology if the produced record is to an internal or a source topic ...\n             final String outputTopicName = record.topic();\n-            if (internalTopics.contains(outputTopicName) || processorTopology.sourceTopics().contains(outputTopicName)\n-                || globalPartitionsByTopic.containsKey(outputTopicName)) {\n-                pipeRecord(record);\n+            if (internalTopics.contains(outputTopicName)\n+                || processorTopology.sourceTopics().contains(outputTopicName)) {\n+                final TopicPartition topicPartition = getTopicPartition(record.topic());", "originalCommit": "5d18a92ac4b7d62a3f6fd1ab3e37bdc3ca3586eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg2MTQ3Ng==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r376861476", "bodyText": "Should we move this check out of this method to the caller? It's only called twice and one caller does this check outside already.", "author": "mjsax", "createdAt": "2020-02-10T04:08:42Z", "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -469,48 +465,76 @@ private void pipeRecord(final String topicName,\n             validateSourceTopicNameRegexPattern(topicName);\n         }\n         final TopicPartition topicPartition = getTopicPartition(topicName);\n-        if (topicPartition != null) {\n-            final long offset = offsetsByTopicPartition.get(topicPartition).incrementAndGet() - 1;\n-            task.addRecords(topicPartition, Collections.singleton(new ConsumerRecord<>(\n-                    topicName,\n-                    topicPartition.partition(),\n-                    offset,\n-                    timestamp,\n-                    TimestampType.CREATE_TIME,\n-                    (long) ConsumerRecord.NULL_CHECKSUM,\n-                    key == null ? ConsumerRecord.NULL_SIZE : key.length,\n-                    value == null ? ConsumerRecord.NULL_SIZE : value.length,\n-                    key,\n-                    value,\n-                    headers)));\n-\n-            // Process the record ...\n-            task.process(mockWallClockTime.milliseconds());\n-            task.maybePunctuateStreamTime();\n-            task.commit();\n-            captureOutputRecords();\n+\n+        if (topicPartition == null) {\n+            processGlobalRecord(topicName, timestamp, key, value, headers);\n         } else {\n-            final TopicPartition globalTopicPartition = globalPartitionsByTopic.get(topicName);\n-            if (globalTopicPartition == null) {\n-                throw new IllegalArgumentException(\"Unknown topic: \" + topicName);\n+            enqueueTaskRecord(topicName, topicPartition, timestamp, key, value, headers);\n+            processAllProcessableRecords();\n+        }\n+    }\n+\n+    private void enqueueTaskRecord(final String topicName,\n+                                   final TopicPartition topicPartition,\n+                                   final Long timestamp,\n+                                   final byte[] key,\n+                                   final byte[] value,\n+                                   final Headers headers) {\n+        if (!internalTopologyBuilder.sourceTopicNames().isEmpty()) {\n+            validateSourceTopicNameRegexPattern(topicName);\n+        }", "originalCommit": "5d18a92ac4b7d62a3f6fd1ab3e37bdc3ca3586eb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg2MTc2OA==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r376861768", "bodyText": "Should we add the same check for topicName, timestamp, and headers ?", "author": "mjsax", "createdAt": "2020-02-10T04:10:14Z", "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -469,48 +465,76 @@ private void pipeRecord(final String topicName,\n             validateSourceTopicNameRegexPattern(topicName);\n         }\n         final TopicPartition topicPartition = getTopicPartition(topicName);\n-        if (topicPartition != null) {\n-            final long offset = offsetsByTopicPartition.get(topicPartition).incrementAndGet() - 1;\n-            task.addRecords(topicPartition, Collections.singleton(new ConsumerRecord<>(\n-                    topicName,\n-                    topicPartition.partition(),\n-                    offset,\n-                    timestamp,\n-                    TimestampType.CREATE_TIME,\n-                    (long) ConsumerRecord.NULL_CHECKSUM,\n-                    key == null ? ConsumerRecord.NULL_SIZE : key.length,\n-                    value == null ? ConsumerRecord.NULL_SIZE : value.length,\n-                    key,\n-                    value,\n-                    headers)));\n-\n-            // Process the record ...\n-            task.process(mockWallClockTime.milliseconds());\n-            task.maybePunctuateStreamTime();\n-            task.commit();\n-            captureOutputRecords();\n+\n+        if (topicPartition == null) {\n+            processGlobalRecord(topicName, timestamp, key, value, headers);\n         } else {\n-            final TopicPartition globalTopicPartition = globalPartitionsByTopic.get(topicName);\n-            if (globalTopicPartition == null) {\n-                throw new IllegalArgumentException(\"Unknown topic: \" + topicName);\n+            enqueueTaskRecord(topicName, topicPartition, timestamp, key, value, headers);\n+            processAllProcessableRecords();\n+        }\n+    }\n+\n+    private void enqueueTaskRecord(final String topicName,\n+                                   final TopicPartition topicPartition,\n+                                   final Long timestamp,\n+                                   final byte[] key,\n+                                   final byte[] value,\n+                                   final Headers headers) {\n+        if (!internalTopologyBuilder.sourceTopicNames().isEmpty()) {\n+            validateSourceTopicNameRegexPattern(topicName);\n+        }\n+        Objects.requireNonNull(topicPartition);", "originalCommit": "5d18a92ac4b7d62a3f6fd1ab3e37bdc3ca3586eb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzE2NjQzMQ==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377166431", "bodyText": "I can; I only added this check because it was never possible to go into this logic before with a null topicPartition (because it was inside a condition block that topicPartition != null), but now that it's a separate method, future (buggy) code changes could result in passing a null here. We'd just get an NPE later on, but I wanted to check it up front on a line of its own to decrease debugging time.", "author": "vvcephei", "createdAt": "2020-02-10T16:19:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg2MTc2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg2MjA2OQ==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r376862069", "bodyText": "There is already StreamTask#numBuffered() -- should we reuse this one instead of adding a new method?", "author": "mjsax", "createdAt": "2020-02-10T04:11:49Z", "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -469,48 +465,76 @@ private void pipeRecord(final String topicName,\n             validateSourceTopicNameRegexPattern(topicName);\n         }\n         final TopicPartition topicPartition = getTopicPartition(topicName);\n-        if (topicPartition != null) {\n-            final long offset = offsetsByTopicPartition.get(topicPartition).incrementAndGet() - 1;\n-            task.addRecords(topicPartition, Collections.singleton(new ConsumerRecord<>(\n-                    topicName,\n-                    topicPartition.partition(),\n-                    offset,\n-                    timestamp,\n-                    TimestampType.CREATE_TIME,\n-                    (long) ConsumerRecord.NULL_CHECKSUM,\n-                    key == null ? ConsumerRecord.NULL_SIZE : key.length,\n-                    value == null ? ConsumerRecord.NULL_SIZE : value.length,\n-                    key,\n-                    value,\n-                    headers)));\n-\n-            // Process the record ...\n-            task.process(mockWallClockTime.milliseconds());\n-            task.maybePunctuateStreamTime();\n-            task.commit();\n-            captureOutputRecords();\n+\n+        if (topicPartition == null) {\n+            processGlobalRecord(topicName, timestamp, key, value, headers);\n         } else {\n-            final TopicPartition globalTopicPartition = globalPartitionsByTopic.get(topicName);\n-            if (globalTopicPartition == null) {\n-                throw new IllegalArgumentException(\"Unknown topic: \" + topicName);\n+            enqueueTaskRecord(topicName, topicPartition, timestamp, key, value, headers);\n+            processAllProcessableRecords();\n+        }\n+    }\n+\n+    private void enqueueTaskRecord(final String topicName,\n+                                   final TopicPartition topicPartition,\n+                                   final Long timestamp,\n+                                   final byte[] key,\n+                                   final byte[] value,\n+                                   final Headers headers) {\n+        if (!internalTopologyBuilder.sourceTopicNames().isEmpty()) {\n+            validateSourceTopicNameRegexPattern(topicName);\n+        }\n+        Objects.requireNonNull(topicPartition);\n+\n+        final long offset = offsetsByTopicPartition.get(topicPartition).incrementAndGet() - 1;\n+        task.addRecords(topicPartition, Collections.singleton(new ConsumerRecord<>(\n+            topicName,\n+            topicPartition.partition(),\n+            offset,\n+            timestamp,\n+            TimestampType.CREATE_TIME,\n+            (long) ConsumerRecord.NULL_CHECKSUM,\n+            key == null ? ConsumerRecord.NULL_SIZE : key.length,\n+            value == null ? ConsumerRecord.NULL_SIZE : value.length,\n+            key,\n+            value,\n+            headers)));\n+    }\n+\n+    private void processAllProcessableRecords() {\n+        // If the topology only has global tasks, then `task` would be null.\n+        // For this method, it just means there's nothing to do.\n+        if (task != null) {\n+            while (task.hasRecordsQueued()) {", "originalCommit": "5d18a92ac4b7d62a3f6fd1ab3e37bdc3ca3586eb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzE2NzQ4NQ==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377167485", "bodyText": "That one is private. I considered it, but it seemed silly to expose integer numBuffered() when the only thing we'd do with it is while (numBuffered() > 0). If we have another reason to expose the integer in the future, we can merge the use cases later on.", "author": "vvcephei", "createdAt": "2020-02-10T16:21:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg2MjA2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg2MzIyNQ==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r376863225", "bodyText": "Why do we not use a List?", "author": "mjsax", "createdAt": "2020-02-10T04:20:41Z", "path": "streams/test-utils/src/test/java/org/apache/kafka/streams/TopologyTestDriverTest.java", "diffHunk": "@@ -1522,4 +1525,52 @@ public void shouldCreateStateDirectoryForStatefulTopology() {\n         final TaskId taskId = new TaskId(0, 0);\n         assertTrue(new File(appDir, taskId.toString()).exists());\n     }\n+\n+    @Test\n+    public void shouldEnqueueLaterOutputsAfterEarlierOnes() {\n+        final Properties properties = new Properties();\n+        properties.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, \"dummy\");\n+        properties.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummy\");\n+\n+        final Topology topology = new Topology();\n+        topology.addSource(\"source\", new StringDeserializer(), new StringDeserializer(), \"input\");\n+        topology.addProcessor(\n+            \"recursiveProcessor\",\n+            () -> new AbstractProcessor<String, String>() {\n+                @Override\n+                public void process(final String key, final String value) {\n+                    if (!value.startsWith(\"recurse-\")) {\n+                        context().forward(key, \"recurse-\" + value, To.child(\"recursiveSink\"));\n+                    }\n+                    context().forward(key, value, To.child(\"sink\"));\n+                }\n+            },\n+            \"source\"\n+        );\n+        topology.addSink(\"recursiveSink\", \"input\", new StringSerializer(), new StringSerializer(), \"recursiveProcessor\");\n+        topology.addSink(\"sink\", \"output\", new StringSerializer(), new StringSerializer(), \"recursiveProcessor\");\n+\n+        try (final TopologyTestDriver topologyTestDriver = new TopologyTestDriver(topology, properties)) {\n+            final TestInputTopic<String, String> in = topologyTestDriver.createInputTopic(\"input\", new StringSerializer(), new StringSerializer());\n+            final TestOutputTopic<String, String> out = topologyTestDriver.createOutputTopic(\"output\", new StringDeserializer(), new StringDeserializer());\n+\n+            // given the topology above, we expect to see the output _first_ echo the input\n+            // and _then_ print it with \"recurse-\" prepended.\n+\n+            in.pipeInput(\"A\", \"alpha\");\n+            final Map<String, String> table = out.readKeyValuesToMap();", "originalCommit": "5d18a92ac4b7d62a3f6fd1ab3e37bdc3ca3586eb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzE2NzkxMw==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377167913", "bodyText": "I also verified the list later on (as I see you noticed ;) ).", "author": "vvcephei", "createdAt": "2020-02-10T16:22:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg2MzIyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIyMzAxNw==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377223017", "bodyText": "But the list-test is a superset of the map test? Seems to be redundant to me.", "author": "mjsax", "createdAt": "2020-02-10T17:57:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg2MzIyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg2MzMwOA==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r376863308", "bodyText": "Why do we test the same thing twice?", "author": "mjsax", "createdAt": "2020-02-10T04:21:17Z", "path": "streams/test-utils/src/test/java/org/apache/kafka/streams/TopologyTestDriverTest.java", "diffHunk": "@@ -1522,4 +1525,52 @@ public void shouldCreateStateDirectoryForStatefulTopology() {\n         final TaskId taskId = new TaskId(0, 0);\n         assertTrue(new File(appDir, taskId.toString()).exists());\n     }\n+\n+    @Test\n+    public void shouldEnqueueLaterOutputsAfterEarlierOnes() {\n+        final Properties properties = new Properties();\n+        properties.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, \"dummy\");\n+        properties.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummy\");\n+\n+        final Topology topology = new Topology();\n+        topology.addSource(\"source\", new StringDeserializer(), new StringDeserializer(), \"input\");\n+        topology.addProcessor(\n+            \"recursiveProcessor\",\n+            () -> new AbstractProcessor<String, String>() {\n+                @Override\n+                public void process(final String key, final String value) {\n+                    if (!value.startsWith(\"recurse-\")) {\n+                        context().forward(key, \"recurse-\" + value, To.child(\"recursiveSink\"));\n+                    }\n+                    context().forward(key, value, To.child(\"sink\"));\n+                }\n+            },\n+            \"source\"\n+        );\n+        topology.addSink(\"recursiveSink\", \"input\", new StringSerializer(), new StringSerializer(), \"recursiveProcessor\");\n+        topology.addSink(\"sink\", \"output\", new StringSerializer(), new StringSerializer(), \"recursiveProcessor\");\n+\n+        try (final TopologyTestDriver topologyTestDriver = new TopologyTestDriver(topology, properties)) {\n+            final TestInputTopic<String, String> in = topologyTestDriver.createInputTopic(\"input\", new StringSerializer(), new StringSerializer());\n+            final TestOutputTopic<String, String> out = topologyTestDriver.createOutputTopic(\"output\", new StringDeserializer(), new StringDeserializer());\n+\n+            // given the topology above, we expect to see the output _first_ echo the input\n+            // and _then_ print it with \"recurse-\" prepended.\n+\n+            in.pipeInput(\"A\", \"alpha\");\n+            final Map<String, String> table = out.readKeyValuesToMap();\n+            assertThat(table, is(Collections.singletonMap(\"A\", \"recurse-alpha\")));\n+\n+            in.pipeInput(\"B\", \"beta\");", "originalCommit": "5d18a92ac4b7d62a3f6fd1ab3e37bdc3ca3586eb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzE3MTExNA==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377171114", "bodyText": "It's not exactly the same.\nWe do want the \"final\" output to be the correct end-state of the operation, which the map test verifies very succinctly. Since this is a table operation, verifying the result as a map is most appropriate.\nBut we also don't want anything extra in the output topic, which the second test verifies.\nIt's true that the map test is redundant with the last element in the list test; it's just telling the narrative of what we expect.", "author": "vvcephei", "createdAt": "2020-02-10T16:27:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg2MzMwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIwNzgxNQ==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377207815", "bodyText": "I've added comments to make this narrative more apparent.", "author": "vvcephei", "createdAt": "2020-02-10T17:28:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg2MzMwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIyMzgxNQ==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377223815", "bodyText": "Since this is a table operation\n\nThe test uses the PAPI -- those semantic cannot be inferred -- I am fine with leaving the redundant code if you like it better that way -- but it does not improve test coverage and is confusing IMHO.", "author": "mjsax", "createdAt": "2020-02-10T17:59:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg2MzMwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM2ODg0Ng==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377368846", "bodyText": "Ah! I was still thinking about the foreign-key join test. You're right.", "author": "vvcephei", "createdAt": "2020-02-10T23:00:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Njg2MzMwOA=="}], "type": "inlineReview"}, {"oid": "feccf8219f65c5a24c570518c43467fc3b975a3c", "url": "https://github.com/apache/kafka/commit/feccf8219f65c5a24c570518c43467fc3b975a3c", "message": "KAFKA-9503: Fix TopologyTestDriver output order", "committedDate": "2020-02-10T23:09:21Z", "type": "commit"}, {"oid": "0ae1c3f35e5fc38999a8ad6a4786536231e2dc32", "url": "https://github.com/apache/kafka/commit/0ae1c3f35e5fc38999a8ad6a4786536231e2dc32", "message": "code review feedback and an extra fix", "committedDate": "2020-02-11T04:13:38Z", "type": "commit"}, {"oid": "6e0eff5c2be10be25c5eca053fa40e6eb44c4322", "url": "https://github.com/apache/kafka/commit/6e0eff5c2be10be25c5eca053fa40e6eb44c4322", "message": "porting cleanup from tdd-fix", "committedDate": "2020-02-11T04:20:18Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0NjA3Nw==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377446077", "bodyText": "Exposed this again for use in TopologyTestDriver", "author": "vvcephei", "createdAt": "2020-02-11T04:55:54Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -444,7 +444,7 @@ private void close(final boolean clean) {\n      * An active task is processable if its buffer contains data for all of its input\n      * source topic partitions, or if it is enforced to be processable\n      */\n-    private boolean isProcessable(final long wallClockTime) {\n+    public boolean isProcessable(final long wallClockTime) {", "originalCommit": "6e0eff5c2be10be25c5eca053fa40e6eb44c4322", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0NjIyMw==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377446223", "bodyText": "Here's the loop condition I added to let the loop terminate when task idling limits our ability to process enqueued records.", "author": "vvcephei", "createdAt": "2020-02-11T04:56:51Z", "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -456,62 +456,88 @@ public void pipeInput(final ConsumerRecord<byte[], byte[]> consumerRecord) {\n             consumerRecord.headers());\n     }\n \n-    private void pipeRecord(final ProducerRecord<byte[], byte[]> record) {\n-        pipeRecord(record.topic(), record.timestamp(), record.key(), record.value(), record.headers());\n-    }\n-\n     private void pipeRecord(final String topicName,\n-                            final Long timestamp,\n+                            final long timestamp,\n                             final byte[] key,\n                             final byte[] value,\n                             final Headers headers) {\n+        final TopicPartition inputTopicOrPatternPartition = getInputTopicOrPatternPartition(topicName);\n+        final TopicPartition globalInputTopicPartition = globalPartitionsByInputTopic.get(topicName);\n \n-        if (!internalTopologyBuilder.sourceTopicNames().isEmpty()) {\n-            validateSourceTopicNameRegexPattern(topicName);\n+        if (inputTopicOrPatternPartition == null && globalInputTopicPartition == null) {\n+            throw new IllegalArgumentException(\"Unknown topic: \" + topicName);\n         }\n-        final TopicPartition topicPartition = getTopicPartition(topicName);\n-        if (topicPartition != null) {\n-            final long offset = offsetsByTopicPartition.get(topicPartition).incrementAndGet() - 1;\n-            task.addRecords(topicPartition, Collections.singleton(new ConsumerRecord<>(\n-                    topicName,\n-                    topicPartition.partition(),\n-                    offset,\n-                    timestamp,\n-                    TimestampType.CREATE_TIME,\n-                    (long) ConsumerRecord.NULL_CHECKSUM,\n-                    key == null ? ConsumerRecord.NULL_SIZE : key.length,\n-                    value == null ? ConsumerRecord.NULL_SIZE : value.length,\n-                    key,\n-                    value,\n-                    headers)));\n-\n-            // Process the record ...\n-            task.process(mockWallClockTime.milliseconds());\n-            task.maybePunctuateStreamTime();\n-            task.commit();\n-            captureOutputRecords();\n-        } else {\n-            final TopicPartition globalTopicPartition = globalPartitionsByTopic.get(topicName);\n-            if (globalTopicPartition == null) {\n-                throw new IllegalArgumentException(\"Unknown topic: \" + topicName);\n+\n+        if (inputTopicOrPatternPartition != null) {\n+            enqueueTaskRecord(topicName, inputTopicOrPatternPartition, timestamp, key, value, headers);\n+            completeAllProcessableWork();\n+        }\n+\n+        if (globalInputTopicPartition != null) {\n+            processGlobalRecord(globalInputTopicPartition, timestamp, key, value, headers);\n+        }\n+    }\n+\n+    private void enqueueTaskRecord(final String inputTopic,\n+                                   final TopicPartition topicOrPatternPartition,\n+                                   final long timestamp,\n+                                   final byte[] key,\n+                                   final byte[] value,\n+                                   final Headers headers) {\n+        task.addRecords(topicOrPatternPartition, Collections.singleton(new ConsumerRecord<>(\n+            inputTopic,\n+            topicOrPatternPartition.partition(),\n+            offsetsByTopicOrPatternPartition.get(topicOrPatternPartition).incrementAndGet() - 1,\n+            timestamp,\n+            TimestampType.CREATE_TIME,\n+            (long) ConsumerRecord.NULL_CHECKSUM,\n+            key == null ? ConsumerRecord.NULL_SIZE : key.length,\n+            value == null ? ConsumerRecord.NULL_SIZE : value.length,\n+            key,\n+            value,\n+            headers)));\n+    }\n+\n+    private void completeAllProcessableWork() {\n+        // for internally triggered processing (like wall-clock punctuations),\n+        // we might have buffered some records to internal topics that need to\n+        // be piped back in to kick-start the processing loop. This is idempotent\n+        // and therefore harmless in the case where all we've done is enqueued an\n+        // input record from the user.\n+        captureOutputsAndReEnqueueInternalResults();\n+\n+        // If the topology only has global tasks, then `task` would be null.\n+        // For this method, it just means there's nothing to do.\n+        if (task != null) {\n+            while (task.hasRecordsQueued() && task.isProcessable(mockWallClockTime.milliseconds())) {", "originalCommit": "6e0eff5c2be10be25c5eca053fa40e6eb44c4322", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQ0NjQ5OA==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377446498", "bodyText": "Here's the new test I added to make sure TopologyTestDriver works properly with task idling.", "author": "vvcephei", "createdAt": "2020-02-11T04:58:51Z", "path": "streams/test-utils/src/test/java/org/apache/kafka/streams/TopologyTestDriverTest.java", "diffHunk": "@@ -1522,4 +1526,174 @@ public void shouldCreateStateDirectoryForStatefulTopology() {\n         final TaskId taskId = new TaskId(0, 0);\n         assertTrue(new File(appDir, taskId.toString()).exists());\n     }\n+\n+    @Test\n+    public void shouldEnqueueLaterOutputsAfterEarlierOnes() {\n+        final Properties properties = new Properties();\n+        properties.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, \"dummy\");\n+        properties.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummy\");\n+\n+        final Topology topology = new Topology();\n+        topology.addSource(\"source\", new StringDeserializer(), new StringDeserializer(), \"input\");\n+        topology.addProcessor(\n+            \"recursiveProcessor\",\n+            () -> new AbstractProcessor<String, String>() {\n+                @Override\n+                public void process(final String key, final String value) {\n+                    if (!value.startsWith(\"recurse-\")) {\n+                        context().forward(key, \"recurse-\" + value, To.child(\"recursiveSink\"));\n+                    }\n+                    context().forward(key, value, To.child(\"sink\"));\n+                }\n+            },\n+            \"source\"\n+        );\n+        topology.addSink(\"recursiveSink\", \"input\", new StringSerializer(), new StringSerializer(), \"recursiveProcessor\");\n+        topology.addSink(\"sink\", \"output\", new StringSerializer(), new StringSerializer(), \"recursiveProcessor\");\n+\n+        try (final TopologyTestDriver topologyTestDriver = new TopologyTestDriver(topology, properties)) {\n+            final TestInputTopic<String, String> in = topologyTestDriver.createInputTopic(\"input\", new StringSerializer(), new StringSerializer());\n+            final TestOutputTopic<String, String> out = topologyTestDriver.createOutputTopic(\"output\", new StringDeserializer(), new StringDeserializer());\n+\n+            // given the topology above, we expect to see the output _first_ echo the input\n+            // and _then_ print it with \"recurse-\" prepended.\n+\n+            in.pipeInput(\"B\", \"beta\");\n+            final List<KeyValue<String, String>> events = out.readKeyValuesToList();\n+            assertThat(\n+                events,\n+                is(Arrays.asList(\n+                    new KeyValue<>(\"B\", \"beta\"),\n+                    new KeyValue<>(\"B\", \"recurse-beta\")\n+                ))\n+            );\n+\n+        }\n+    }\n+\n+    @Test\n+    public void shouldApplyGlobalUpdatesCorrectlyInRecursiveTopologies() {\n+        final Properties properties = new Properties();\n+        properties.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, \"dummy\");\n+        properties.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummy\");\n+\n+        final Topology topology = new Topology();\n+        topology.addSource(\"source\", new StringDeserializer(), new StringDeserializer(), \"input\");\n+        topology.addGlobalStore(\n+            Stores.keyValueStoreBuilder(Stores.inMemoryKeyValueStore(\"globule-store\"), Serdes.String(), Serdes.String()).withLoggingDisabled(),\n+            \"globuleSource\",\n+            new StringDeserializer(),\n+            new StringDeserializer(),\n+            \"globule-topic\",\n+            \"globuleProcessor\",\n+            () -> new Processor<String, String>() {\n+                private KeyValueStore<String, String> stateStore;\n+\n+                @SuppressWarnings(\"unchecked\")\n+                @Override\n+                public void init(final ProcessorContext context) {\n+                    stateStore = (KeyValueStore<String, String>) context.getStateStore(\"globule-store\");\n+                }\n+\n+                @Override\n+                public void process(final String key, final String value) {\n+                    stateStore.put(key, value);\n+                }\n+\n+                @Override\n+                public void close() {\n+\n+                }\n+            }\n+        );\n+        topology.addProcessor(\n+            \"recursiveProcessor\",\n+            () -> new AbstractProcessor<String, String>() {\n+                @Override\n+                public void process(final String key, final String value) {\n+                    if (!value.startsWith(\"recurse-\")) {\n+                        context().forward(key, \"recurse-\" + value, To.child(\"recursiveSink\"));\n+                    }\n+                    context().forward(key, value, To.child(\"sink\"));\n+                    context().forward(key, value, To.child(\"globuleSink\"));\n+                }\n+            },\n+            \"source\"\n+        );\n+        topology.addSink(\"recursiveSink\", \"input\", new StringSerializer(), new StringSerializer(), \"recursiveProcessor\");\n+        topology.addSink(\"sink\", \"output\", new StringSerializer(), new StringSerializer(), \"recursiveProcessor\");\n+        topology.addSink(\"globuleSink\", \"globule-topic\", new StringSerializer(), new StringSerializer(), \"recursiveProcessor\");\n+\n+        try (final TopologyTestDriver topologyTestDriver = new TopologyTestDriver(topology, properties)) {\n+            final TestInputTopic<String, String> in = topologyTestDriver.createInputTopic(\"input\", new StringSerializer(), new StringSerializer());\n+            final TestOutputTopic<String, String> globalTopic = topologyTestDriver.createOutputTopic(\"globule-topic\", new StringDeserializer(), new StringDeserializer());\n+\n+            in.pipeInput(\"A\", \"alpha\");\n+\n+            // expect the global store to correctly reflect the last update\n+            final KeyValueStore<String, String> keyValueStore = topologyTestDriver.getKeyValueStore(\"globule-store\");\n+            assertThat(keyValueStore, notNullValue());\n+            assertThat(keyValueStore.get(\"A\"), is(\"recurse-alpha\"));\n+\n+            // and also just make sure the test really sent both events to the topic.\n+            final List<KeyValue<String, String>> events = globalTopic.readKeyValuesToList();\n+            assertThat(\n+                events,\n+                is(Arrays.asList(\n+                    new KeyValue<>(\"A\", \"alpha\"),\n+                    new KeyValue<>(\"A\", \"recurse-alpha\")\n+                ))\n+            );\n+        }\n+    }\n+\n+    @Test\n+    public void shouldRespectTaskIdling() {\n+        final Properties properties = new Properties();\n+        properties.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, \"dummy\");\n+        properties.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"dummy\");\n+\n+        // This is the key to this test. Wall-clock time doesn't advance automatically in TopologyTestDriver,\n+        // so with an idle time specified, TTD can't just expect all enqueued records to be processable.\n+        properties.setProperty(StreamsConfig.MAX_TASK_IDLE_MS_CONFIG, \"1000\");\n+\n+        final Topology topology = new Topology();\n+        topology.addSource(\"source1\", new StringDeserializer(), new StringDeserializer(), \"input1\");\n+        topology.addSource(\"source2\", new StringDeserializer(), new StringDeserializer(), \"input2\");\n+        topology.addSink(\"sink\", \"output\", new StringSerializer(), new StringSerializer(), \"source1\", \"source2\");\n+\n+        try (final TopologyTestDriver topologyTestDriver = new TopologyTestDriver(topology, properties)) {\n+            final TestInputTopic<String, String> in1 = topologyTestDriver.createInputTopic(\"input1\", new StringSerializer(), new StringSerializer());\n+            final TestInputTopic<String, String> in2 = topologyTestDriver.createInputTopic(\"input2\", new StringSerializer(), new StringSerializer());\n+            final TestOutputTopic<String, String> out = topologyTestDriver.createOutputTopic(\"output\", new StringDeserializer(), new StringDeserializer());\n+\n+            in1.pipeInput(\"A\", \"alpha\");\n+            topologyTestDriver.advanceWallClockTime(Duration.ofMillis(1));\n+\n+            // only one input has records, and it's only been one ms\n+            assertThat(out.readKeyValuesToList(), is(Collections.emptyList()));\n+\n+            in2.pipeInput(\"B\", \"beta\");\n+\n+            // because both topics have records, we can process (even though it's only been one ms)\n+            // but after processing A (the earlier record), we now only have one input queued, so\n+            // task idling takes effect again\n+            assertThat(\n+                out.readKeyValuesToList(),\n+                is(Collections.singletonList(\n+                    new KeyValue<>(\"A\", \"alpha\")\n+                ))\n+            );\n+\n+            topologyTestDriver.advanceWallClockTime(Duration.ofSeconds(1));\n+\n+            // now that one second has elapsed, the idle time has expired, and we can process B\n+            assertThat(\n+                out.readKeyValuesToList(),\n+                is(Collections.singletonList(\n+                    new KeyValue<>(\"B\", \"beta\")\n+                ))\n+            );", "originalCommit": "6e0eff5c2be10be25c5eca053fa40e6eb44c4322", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc5NDM0Mg==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377794342", "bodyText": "I think we should add a INFO (or WARN) log statement for the case that there is buffered data that cannot be processed yet and inform the use that they need to advance wall-clock time manually.\nWe might also have an INFO log when the TTD is created if we detect that task.idle is non-zero?\nHow should we handle close()? I think we should internally advance wall-clock time to ensure we drain all output? If we leave it to the user the might not be able to drain all records if there is a loop in the dataflow (what would required to advance wall-clock time after each new enqueue?). We would also have a test for this case if we implement it that way.", "author": "mjsax", "createdAt": "2020-02-11T17:45:20Z", "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -456,62 +456,88 @@ public void pipeInput(final ConsumerRecord<byte[], byte[]> consumerRecord) {\n             consumerRecord.headers());\n     }\n \n-    private void pipeRecord(final ProducerRecord<byte[], byte[]> record) {\n-        pipeRecord(record.topic(), record.timestamp(), record.key(), record.value(), record.headers());\n-    }\n-\n     private void pipeRecord(final String topicName,\n-                            final Long timestamp,\n+                            final long timestamp,\n                             final byte[] key,\n                             final byte[] value,\n                             final Headers headers) {\n+        final TopicPartition inputTopicOrPatternPartition = getInputTopicOrPatternPartition(topicName);\n+        final TopicPartition globalInputTopicPartition = globalPartitionsByInputTopic.get(topicName);\n \n-        if (!internalTopologyBuilder.sourceTopicNames().isEmpty()) {\n-            validateSourceTopicNameRegexPattern(topicName);\n+        if (inputTopicOrPatternPartition == null && globalInputTopicPartition == null) {\n+            throw new IllegalArgumentException(\"Unknown topic: \" + topicName);\n         }\n-        final TopicPartition topicPartition = getTopicPartition(topicName);\n-        if (topicPartition != null) {\n-            final long offset = offsetsByTopicPartition.get(topicPartition).incrementAndGet() - 1;\n-            task.addRecords(topicPartition, Collections.singleton(new ConsumerRecord<>(\n-                    topicName,\n-                    topicPartition.partition(),\n-                    offset,\n-                    timestamp,\n-                    TimestampType.CREATE_TIME,\n-                    (long) ConsumerRecord.NULL_CHECKSUM,\n-                    key == null ? ConsumerRecord.NULL_SIZE : key.length,\n-                    value == null ? ConsumerRecord.NULL_SIZE : value.length,\n-                    key,\n-                    value,\n-                    headers)));\n-\n-            // Process the record ...\n-            task.process(mockWallClockTime.milliseconds());\n-            task.maybePunctuateStreamTime();\n-            task.commit();\n-            captureOutputRecords();\n-        } else {\n-            final TopicPartition globalTopicPartition = globalPartitionsByTopic.get(topicName);\n-            if (globalTopicPartition == null) {\n-                throw new IllegalArgumentException(\"Unknown topic: \" + topicName);\n+\n+        if (inputTopicOrPatternPartition != null) {\n+            enqueueTaskRecord(topicName, inputTopicOrPatternPartition, timestamp, key, value, headers);\n+            completeAllProcessableWork();\n+        }\n+\n+        if (globalInputTopicPartition != null) {\n+            processGlobalRecord(globalInputTopicPartition, timestamp, key, value, headers);\n+        }\n+    }\n+\n+    private void enqueueTaskRecord(final String inputTopic,\n+                                   final TopicPartition topicOrPatternPartition,\n+                                   final long timestamp,\n+                                   final byte[] key,\n+                                   final byte[] value,\n+                                   final Headers headers) {\n+        task.addRecords(topicOrPatternPartition, Collections.singleton(new ConsumerRecord<>(\n+            inputTopic,\n+            topicOrPatternPartition.partition(),\n+            offsetsByTopicOrPatternPartition.get(topicOrPatternPartition).incrementAndGet() - 1,\n+            timestamp,\n+            TimestampType.CREATE_TIME,\n+            (long) ConsumerRecord.NULL_CHECKSUM,\n+            key == null ? ConsumerRecord.NULL_SIZE : key.length,\n+            value == null ? ConsumerRecord.NULL_SIZE : value.length,\n+            key,\n+            value,\n+            headers)));\n+    }\n+\n+    private void completeAllProcessableWork() {\n+        // for internally triggered processing (like wall-clock punctuations),\n+        // we might have buffered some records to internal topics that need to\n+        // be piped back in to kick-start the processing loop. This is idempotent\n+        // and therefore harmless in the case where all we've done is enqueued an\n+        // input record from the user.\n+        captureOutputsAndReEnqueueInternalResults();\n+\n+        // If the topology only has global tasks, then `task` would be null.\n+        // For this method, it just means there's nothing to do.\n+        if (task != null) {\n+            while (task.hasRecordsQueued() && task.isProcessable(mockWallClockTime.milliseconds())) {\n+                // Process the record ...\n+                task.process(mockWallClockTime.milliseconds());\n+                task.maybePunctuateStreamTime();\n+                task.commit();\n+                captureOutputsAndReEnqueueInternalResults();\n             }\n-            final long offset = offsetsByTopicPartition.get(globalTopicPartition).incrementAndGet() - 1;\n-            globalStateTask.update(new ConsumerRecord<>(\n-                    globalTopicPartition.topic(),\n-                    globalTopicPartition.partition(),\n-                    offset,\n-                    timestamp,\n-                    TimestampType.CREATE_TIME,\n-                    (long) ConsumerRecord.NULL_CHECKSUM,\n-                    key == null ? ConsumerRecord.NULL_SIZE : key.length,\n-                    value == null ? ConsumerRecord.NULL_SIZE : value.length,\n-                    key,\n-                    value,\n-                    headers));\n-            globalStateTask.flushState();\n         }", "originalCommit": "6e0eff5c2be10be25c5eca053fa40e6eb44c4322", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg3Mjk5Ng==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377872996", "bodyText": "Thanks, @mjsax !\nAdding both log messages as INFO seems appropriate since this is a slight behavior change, which might affect some programs. I'm not sure about WARN for the \"still buffered\" case, since, you'd be guaranteed to get the log message when using task.idle. I.e., we'd be giving an unavoidable warning telling you that nothing at all is wrong. \"Info\" should be high enough visibility.\nThat's a good point about close. How concerning would it be to just have unprocessed records while closing the TTD? That would be the cleanest approach from an implementation perspective, and it seems like the test author couldn't care that much about the results, or they would have actually advanced wall-clock time themselves to get them.", "author": "vvcephei", "createdAt": "2020-02-11T20:14:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzc5NDM0Mg=="}], "type": "inlineReview"}, {"oid": "3cf6d196095e7f37097a1efc22f38d3e4ed51ed0", "url": "https://github.com/apache/kafka/commit/3cf6d196095e7f37097a1efc22f38d3e4ed51ed0", "message": "add logs when there are unprocessable records in TTD", "committedDate": "2020-02-11T21:54:19Z", "type": "commit"}, {"oid": "fc5a693b6ed0ddb4fe76dc78f58a6574b93071f8", "url": "https://github.com/apache/kafka/commit/fc5a693b6ed0ddb4fe76dc78f58a6574b93071f8", "message": "fix npe in close if task is null", "committedDate": "2020-02-11T22:10:23Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkyNzY1OA==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377927658", "bodyText": "nit: add () -> advanceWallClockTime() ?", "author": "mjsax", "createdAt": "2020-02-11T22:05:10Z", "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -287,6 +287,14 @@ private TopologyTestDriver(final InternalTopologyBuilder builder,\n                                final Properties config,\n                                final long initialWallClockTimeMs) {\n         final StreamsConfig streamsConfig = new QuietStreamsConfig(config);\n+        final Long taskIdleTime = streamsConfig.getLong(StreamsConfig.MAX_TASK_IDLE_MS_CONFIG);\n+        if (taskIdleTime > 0) {\n+            log.info(\"Detected {} config in use with TopologyTestDriver (set to {}ms). This means you might need to\" +\n+                         \" use TopologyTestDriver#advanceWallClockTime or enqueue records on all partitions to allow\" +", "originalCommit": "3cf6d196095e7f37097a1efc22f38d3e4ed51ed0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkyNzgzMw==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377927833", "bodyText": "What does Such occurrences means? Might be good to be more elaborative.", "author": "mjsax", "createdAt": "2020-02-11T22:05:32Z", "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -287,6 +287,14 @@ private TopologyTestDriver(final InternalTopologyBuilder builder,\n                                final Properties config,\n                                final long initialWallClockTimeMs) {\n         final StreamsConfig streamsConfig = new QuietStreamsConfig(config);\n+        final Long taskIdleTime = streamsConfig.getLong(StreamsConfig.MAX_TASK_IDLE_MS_CONFIG);\n+        if (taskIdleTime > 0) {\n+            log.info(\"Detected {} config in use with TopologyTestDriver (set to {}ms). This means you might need to\" +\n+                         \" use TopologyTestDriver#advanceWallClockTime or enqueue records on all partitions to allow\" +\n+                         \" Steams to make progress. Such occurrences will be logged.\",", "originalCommit": "3cf6d196095e7f37097a1efc22f38d3e4ed51ed0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkyOTIwMA==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377929200", "bodyText": "empty partitions -> should we say empty topics ?\nJust to clarify that some partitions from different topics are empty?", "author": "mjsax", "createdAt": "2020-02-11T22:08:37Z", "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -516,6 +524,12 @@ private void completeAllProcessableWork() {\n                 task.commit();\n                 captureOutputsAndReEnqueueInternalResults();\n             }\n+            if (task.hasRecordsQueued()) {\n+                log.info(\"Due to the {} configuration, there are currently some records that can't be processed.\" +\n+                             \" Advancing wall-clock time or enqueuing records on the empty partitions will allow\" +", "originalCommit": "3cf6d196095e7f37097a1efc22f38d3e4ed51ed0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkzMDA1Mw==", "url": "https://github.com/apache/kafka/pull/8065#discussion_r377930053", "bodyText": "can't -> could not ?\nI would remove even though TopologyTestDriver is shutting down (read cleaner this way IMHO.)", "author": "mjsax", "createdAt": "2020-02-11T22:10:34Z", "path": "streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java", "diffHunk": "@@ -1058,6 +1072,11 @@ public void close() {\n             }\n         }\n         completeAllProcessableWork();\n+        if (task.hasRecordsQueued()) {\n+            log.warn(\"Due to the {} configuration, there were some records that can't be processed even\" +", "originalCommit": "3cf6d196095e7f37097a1efc22f38d3e4ed51ed0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "f67afd2ff3bf55f17fbe70c45b5d6aa9fe61892d", "url": "https://github.com/apache/kafka/commit/f67afd2ff3bf55f17fbe70c45b5d6aa9fe61892d", "message": "code review comments", "committedDate": "2020-02-11T22:36:32Z", "type": "commit"}, {"oid": "9fc4da0e7f0a29845782c4f3a860289bbdfb356a", "url": "https://github.com/apache/kafka/commit/9fc4da0e7f0a29845782c4f3a860289bbdfb356a", "message": "decrease method length", "committedDate": "2020-02-11T22:50:46Z", "type": "commit"}]}