{"pr_number": 8295, "pr_title": "KAFKA-9627: Replace ListOffset request/response with automated protocol", "pr_createdAt": "2020-03-13T11:56:28Z", "pr_url": "https://github.com/apache/kafka/pull/8295", "timeline": [{"oid": "65ac2f1501fd6ee863a873cd93b64a1534d09461", "url": "https://github.com/apache/kafka/commit/65ac2f1501fd6ee863a873cd93b64a1534d09461", "message": "KAFKA-9627: Replace ListOffset request/response with automated protocol\n\nCo-authored-by: Mickael Maison <mickael.maison@gmail.com>\nCo-authored-by: Edoardo Comar <ecomar@uk.ibm.com>", "committedDate": "2020-07-02T16:07:31Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU3NTU1MA==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r449575550", "bodyText": "I just noticed that we don't ensure that all futures of the current broker are completed. It would be great to ensure it by using completeUnrealizedFutures method if retryTopicPartitionOffsets is empty. We already do this in alterReplicaLogDirs() if you want to see an example.", "author": "dajac", "createdAt": "2020-07-03T13:09:39Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -3883,21 +3886,24 @@ void handleResponse(AbstractResponse abstractResponse) {\n                     ListOffsetResponse response = (ListOffsetResponse) abstractResponse;\n                     Map<TopicPartition, OffsetSpec> retryTopicPartitionOffsets = new HashMap<>();\n \n-                    for (Entry<TopicPartition, PartitionData> result : response.responseData().entrySet()) {\n-                        TopicPartition tp = result.getKey();\n-                        PartitionData partitionData = result.getValue();\n-\n-                        KafkaFutureImpl<ListOffsetsResultInfo> future = futures.get(tp);\n-                        Errors error = partitionData.error;\n-                        OffsetSpec offsetRequestSpec = topicPartitionOffsets.get(tp);\n-                        if (offsetRequestSpec == null) {\n-                            future.completeExceptionally(new KafkaException(\"Unexpected topic partition \" + tp + \" in broker response!\"));\n-                        } else if (MetadataOperationContext.shouldRefreshMetadata(error)) {\n-                            retryTopicPartitionOffsets.put(tp, offsetRequestSpec);\n-                        } else if (error == Errors.NONE) {\n-                            future.complete(new ListOffsetsResultInfo(partitionData.offset, partitionData.timestamp, partitionData.leaderEpoch));\n-                        } else {\n-                            future.completeExceptionally(error.exception());\n+                    for (ListOffsetTopicResponse topic : response.responseData()) {\n+                        for (ListOffsetPartitionResponse partition : topic.partitions()) {\n+                            TopicPartition tp = new TopicPartition(topic.name(), partition.partitionIndex());\n+                            KafkaFutureImpl<ListOffsetsResultInfo> future = futures.get(tp);\n+                            Errors error = Errors.forCode(partition.errorCode());\n+                            OffsetSpec offsetRequestSpec = topicPartitionOffsets.get(tp);\n+                            if (offsetRequestSpec == null) {\n+                                future.completeExceptionally(new KafkaException(\"Unexpected topic partition \" + tp + \" in broker response!\"));\n+                            } else if (MetadataOperationContext.shouldRefreshMetadata(error)) {\n+                                retryTopicPartitionOffsets.put(tp, offsetRequestSpec);\n+                            } else if (error == Errors.NONE) {\n+                                Optional<Integer> leaderEpoch = (partition.leaderEpoch() == ListOffsetResponse.UNKNOWN_EPOCH) \n+                                        ? Optional.empty() \n+                                        : Optional.of(partition.leaderEpoch());\n+                                future.complete(new ListOffsetsResultInfo(partition.offset(), partition.timestamp(), leaderEpoch));\n+                            } else {\n+                                future.completeExceptionally(error.exception());\n+                            }\n                         }\n                     }", "originalCommit": "65ac2f1501fd6ee863a873cd93b64a1534d09461", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU3NzYzNg==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r449577636", "bodyText": "This is not related to your PR at all. It seems that if offsetRequestSpec is null here, future will be null as well cause futures is initialised based on topicPartitionOffsets. If it turns out to be correct, it may be better to just log a warning here like we do in createTopics().", "author": "dajac", "createdAt": "2020-07-03T13:14:27Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -3883,21 +3886,24 @@ void handleResponse(AbstractResponse abstractResponse) {\n                     ListOffsetResponse response = (ListOffsetResponse) abstractResponse;\n                     Map<TopicPartition, OffsetSpec> retryTopicPartitionOffsets = new HashMap<>();\n \n-                    for (Entry<TopicPartition, PartitionData> result : response.responseData().entrySet()) {\n-                        TopicPartition tp = result.getKey();\n-                        PartitionData partitionData = result.getValue();\n-\n-                        KafkaFutureImpl<ListOffsetsResultInfo> future = futures.get(tp);\n-                        Errors error = partitionData.error;\n-                        OffsetSpec offsetRequestSpec = topicPartitionOffsets.get(tp);\n-                        if (offsetRequestSpec == null) {\n-                            future.completeExceptionally(new KafkaException(\"Unexpected topic partition \" + tp + \" in broker response!\"));\n-                        } else if (MetadataOperationContext.shouldRefreshMetadata(error)) {\n-                            retryTopicPartitionOffsets.put(tp, offsetRequestSpec);\n-                        } else if (error == Errors.NONE) {\n-                            future.complete(new ListOffsetsResultInfo(partitionData.offset, partitionData.timestamp, partitionData.leaderEpoch));\n-                        } else {\n-                            future.completeExceptionally(error.exception());\n+                    for (ListOffsetTopicResponse topic : response.responseData()) {\n+                        for (ListOffsetPartitionResponse partition : topic.partitions()) {\n+                            TopicPartition tp = new TopicPartition(topic.name(), partition.partitionIndex());\n+                            KafkaFutureImpl<ListOffsetsResultInfo> future = futures.get(tp);\n+                            Errors error = Errors.forCode(partition.errorCode());\n+                            OffsetSpec offsetRequestSpec = topicPartitionOffsets.get(tp);\n+                            if (offsetRequestSpec == null) {\n+                                future.completeExceptionally(new KafkaException(\"Unexpected topic partition \" + tp + \" in broker response!\"));", "originalCommit": "65ac2f1501fd6ee863a873cd93b64a1534d09461", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU4ODkxMA==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r449588910", "bodyText": "This conversion is a bit unfortunate as we have to traverse all the partitions again to build the List<ListOffsetTopic>. Instead, we could compute it directly within groupListOffsetRequests and could receive Map<Node, List<ListOffsetTopic> directly here. That seems doable but I may have missed something.", "author": "dajac", "createdAt": "2020-07-03T13:39:28Z", "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -965,11 +994,11 @@ public void onFailure(RuntimeException e) {\n      * @return A response which can be polled to obtain the corresponding timestamps and offsets.\n      */\n     private RequestFuture<ListOffsetResult> sendListOffsetRequest(final Node node,\n-                                                                  final Map<TopicPartition, ListOffsetRequest.PartitionData> timestampsToSearch,\n+                                                                  final Map<TopicPartition, ListOffsetPartition> timestampsToSearch,\n                                                                   boolean requireTimestamp) {\n         ListOffsetRequest.Builder builder = ListOffsetRequest.Builder\n                 .forConsumer(requireTimestamp, isolationLevel)\n-                .setTargetTimes(timestampsToSearch);\n+                .setTargetTimes(toListOffsetTopics(timestampsToSearch));", "originalCommit": "65ac2f1501fd6ee863a873cd93b64a1534d09461", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjMyNDA2OQ==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r452324069", "bodyText": "I initially tried to do that but there's a couple of intermediate collections using TopicPartition in these methods and it makes it really hard to update them. For example:\n\nhttps://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L735\nhttps://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L880\nhttps://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L887", "author": "mimaison", "createdAt": "2020-07-09T15:58:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU4ODkxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg0OTM1NQ==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r454849355", "bodyText": "I had a look at this and your are right. It seems that keeping TopicPartition is better and difficult to change. In this case, have you considered pushing the conversion to the Builder by providing an overload of setTargetTimes which accepts a Map<TopicPartition, ListOffsetPartition>? That could make the code in the Fetcher a bit cleaner.", "author": "dajac", "createdAt": "2020-07-15T07:30:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU4ODkxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU5Nzk2Nw==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r449597967", "bodyText": "I wonder if grouping by TopicPartition is really necessary here. We iterate over timestampsToSearch to get the ListOffsetPartitionResponse for the current TopicPartition but we could also iterate over the response set directly and thus avoid grouping. Moreover, we always assume that the result set contains the TopicPartition that we are interested in so it would not change the semantic. Am I missing something? What do you think?", "author": "dajac", "createdAt": "2020-07-03T13:59:48Z", "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -994,30 +1023,29 @@ public void onSuccess(ClientResponse response, RequestFuture<ListOffsetResult> f\n      *               value of each partition may be null only for v0. In v1 and later the ListOffset API would not\n      *               return a null timestamp (-1 is returned instead when necessary).\n      */\n-    private void handleListOffsetResponse(Map<TopicPartition, ListOffsetRequest.PartitionData> timestampsToSearch,\n+    private void handleListOffsetResponse(Map<TopicPartition, ListOffsetPartition> timestampsToSearch,\n                                           ListOffsetResponse listOffsetResponse,\n                                           RequestFuture<ListOffsetResult> future) {\n         Map<TopicPartition, ListOffsetData> fetchedOffsets = new HashMap<>();\n         Set<TopicPartition> partitionsToRetry = new HashSet<>();\n         Set<String> unauthorizedTopics = new HashSet<>();\n \n-        for (Map.Entry<TopicPartition, ListOffsetRequest.PartitionData> entry : timestampsToSearch.entrySet()) {\n+        Map<TopicPartition, ListOffsetPartitionResponse> partitionsData = byTopicPartitions(listOffsetResponse.responseData());", "originalCommit": "65ac2f1501fd6ee863a873cd93b64a1534d09461", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM3NDkyNQ==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r452374925", "bodyText": "We now added logic in the AdminClient to handle partial responses from brokers (based on #8295 (comment)). Shouldn't we do the same here instead of assuming the response is always complete? I'm not even sure if we should retry if a resource is missing from the response but we could at least log it instead of hitting a NPE.", "author": "mimaison", "createdAt": "2020-07-09T17:23:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU5Nzk2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg1MzQxOA==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r454853418", "bodyText": "I agree that we should at minimum avoid hitting a NPE.", "author": "dajac", "createdAt": "2020-07-15T07:38:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU5Nzk2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTY3MTE0MA==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r455671140", "bodyText": "I actually switched logic to loop on the response as you initially suggested", "author": "mimaison", "createdAt": "2020-07-16T10:00:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU5Nzk2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTYwMzY4NA==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r449603684", "bodyText": "It would be great if we could add unit tests for this method and perhaps others as well.", "author": "dajac", "createdAt": "2020-07-03T14:12:19Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/ListOffsetRequest.java", "diffHunk": "@@ -156,145 +66,98 @@ private Builder(short oldestAllowedVersion,\n                         int replicaId,\n                         IsolationLevel isolationLevel) {\n             super(ApiKeys.LIST_OFFSETS, oldestAllowedVersion, latestAllowedVersion);\n-            this.replicaId = replicaId;\n-            this.isolationLevel = isolationLevel;\n+            data = new ListOffsetRequestData()\n+                    .setIsolationLevel(isolationLevel.id())\n+                    .setReplicaId(replicaId);\n         }\n \n-        public Builder setTargetTimes(Map<TopicPartition, PartitionData> partitionTimestamps) {\n-            this.partitionTimestamps = partitionTimestamps;\n+        public Builder setTargetTimes(List<ListOffsetTopic> topics) {\n+            data.setTopics(topics);\n             return this;\n         }\n \n         @Override\n         public ListOffsetRequest build(short version) {\n-            return new ListOffsetRequest(replicaId, partitionTimestamps, isolationLevel, version);\n+            return new ListOffsetRequest(version, data);\n         }\n \n         @Override\n         public String toString() {\n-            StringBuilder bld = new StringBuilder();\n-            bld.append(\"(type=ListOffsetRequest\")\n-               .append(\", replicaId=\").append(replicaId);\n-            if (partitionTimestamps != null) {\n-                bld.append(\", partitionTimestamps=\").append(partitionTimestamps);\n-            }\n-            bld.append(\", isolationLevel=\").append(isolationLevel);\n-            bld.append(\")\");\n-            return bld.toString();\n-        }\n-    }\n-\n-    public static final class PartitionData {\n-        public final long timestamp;\n-        public final int maxNumOffsets; // only supported in v0\n-        public final Optional<Integer> currentLeaderEpoch;\n-\n-        private PartitionData(long timestamp, int maxNumOffsets, Optional<Integer> currentLeaderEpoch) {\n-            this.timestamp = timestamp;\n-            this.maxNumOffsets = maxNumOffsets;\n-            this.currentLeaderEpoch = currentLeaderEpoch;\n-        }\n-\n-        // For V0\n-        public PartitionData(long timestamp, int maxNumOffsets) {\n-            this(timestamp, maxNumOffsets, Optional.empty());\n-        }\n-\n-        public PartitionData(long timestamp, Optional<Integer> currentLeaderEpoch) {\n-            this(timestamp, 1, currentLeaderEpoch);\n-        }\n-\n-        @Override\n-        public boolean equals(Object obj) {\n-            if (!(obj instanceof PartitionData)) return false;\n-            PartitionData other = (PartitionData) obj;\n-            return this.timestamp == other.timestamp &&\n-                this.currentLeaderEpoch.equals(other.currentLeaderEpoch);\n-        }\n-\n-        @Override\n-        public int hashCode() {\n-            return Objects.hash(timestamp, currentLeaderEpoch);\n-        }\n-\n-        @Override\n-        public String toString() {\n-            StringBuilder bld = new StringBuilder();\n-            bld.append(\"{timestamp: \").append(timestamp).\n-                    append(\", maxNumOffsets: \").append(maxNumOffsets).\n-                    append(\", currentLeaderEpoch: \").append(currentLeaderEpoch).\n-                    append(\"}\");\n-            return bld.toString();\n+            return data.toString();\n         }\n     }\n \n     /**\n      * Private constructor with a specified version.\n      */\n-    private ListOffsetRequest(int replicaId,\n-                              Map<TopicPartition, PartitionData> targetTimes,\n-                              IsolationLevel isolationLevel,\n-                              short version) {\n+    private ListOffsetRequest(short version, ListOffsetRequestData data) {\n         super(ApiKeys.LIST_OFFSETS, version);\n-        this.replicaId = replicaId;\n-        this.isolationLevel = isolationLevel;\n-        this.partitionTimestamps = targetTimes;\n+        this.data = data;\n         this.duplicatePartitions = Collections.emptySet();\n     }\n \n     public ListOffsetRequest(Struct struct, short version) {\n         super(ApiKeys.LIST_OFFSETS, version);\n-        Set<TopicPartition> duplicatePartitions = new HashSet<>();\n-        replicaId = struct.get(REPLICA_ID);\n-        isolationLevel = struct.hasField(ISOLATION_LEVEL) ?\n-                IsolationLevel.forId(struct.get(ISOLATION_LEVEL)) :\n-                IsolationLevel.READ_UNCOMMITTED;\n-        partitionTimestamps = new HashMap<>();\n-        for (Object topicResponseObj : struct.get(TOPICS)) {\n-            Struct topicResponse = (Struct) topicResponseObj;\n-            String topic = topicResponse.get(TOPIC_NAME);\n-            for (Object partitionResponseObj : topicResponse.get(PARTITIONS)) {\n-                Struct partitionResponse = (Struct) partitionResponseObj;\n-                int partition = partitionResponse.get(PARTITION_ID);\n-                long timestamp = partitionResponse.get(TIMESTAMP);\n-                TopicPartition tp = new TopicPartition(topic, partition);\n-\n-                int maxNumOffsets = partitionResponse.getOrElse(MAX_NUM_OFFSETS, 1);\n-                Optional<Integer> currentLeaderEpoch = RequestUtils.getLeaderEpoch(partitionResponse, CURRENT_LEADER_EPOCH);\n-                PartitionData partitionData = new PartitionData(timestamp, maxNumOffsets, currentLeaderEpoch);\n-                if (partitionTimestamps.put(tp, partitionData) != null)\n+        data = new ListOffsetRequestData(struct, version);\n+        duplicatePartitions = new HashSet<>();\n+        Set<TopicPartition> partitions = new HashSet<>();\n+        for (ListOffsetTopic topic : data.topics()) {\n+            for (ListOffsetPartition partition : topic.partitions()) {\n+                TopicPartition tp = new TopicPartition(topic.name(), partition.partitionIndex());\n+                if (!partitions.add(tp)) {\n                     duplicatePartitions.add(tp);\n+                }\n             }\n         }\n-        this.duplicatePartitions = duplicatePartitions;\n     }\n \n     @Override\n-    @SuppressWarnings(\"deprecation\")\n     public AbstractResponse getErrorResponse(int throttleTimeMs, Throwable e) {", "originalCommit": "65ac2f1501fd6ee863a873cd93b64a1534d09461", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTYwMzk0NQ==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r449603945", "bodyText": "I would call this one topics() as you did already in the request.", "author": "dajac", "createdAt": "2020-07-03T14:12:57Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/ListOffsetResponse.java", "diffHunk": "@@ -58,239 +47,54 @@\n public class ListOffsetResponse extends AbstractResponse {\n     public static final long UNKNOWN_TIMESTAMP = -1L;\n     public static final long UNKNOWN_OFFSET = -1L;\n+    public static final int UNKNOWN_EPOCH = RecordBatch.NO_PARTITION_LEADER_EPOCH;\n \n-    // top level fields\n-    private static final Field.ComplexArray TOPICS = new Field.ComplexArray(\"responses\",\n-            \"The listed offsets by topic\");\n-\n-    // topic level fields\n-    private static final Field.ComplexArray PARTITIONS = new Field.ComplexArray(\"partition_responses\",\n-            \"The listed offsets by partition\");\n-\n-    // partition level fields\n-    // This key is only used by ListOffsetResponse v0\n-    @Deprecated\n-    private static final Field.Array OFFSETS = new Field.Array(\"offsets\", INT64, \"A list of offsets.\");\n-    private static final Field.Int64 TIMESTAMP = new Field.Int64(\"timestamp\",\n-            \"The timestamp associated with the returned offset\");\n-    private static final Field.Int64 OFFSET = new Field.Int64(\"offset\",\n-            \"The offset found\");\n-\n-    private static final Field PARTITIONS_V0 = PARTITIONS.withFields(\n-            PARTITION_ID,\n-            ERROR_CODE,\n-            OFFSETS);\n-\n-    private static final Field TOPICS_V0 = TOPICS.withFields(\n-            TOPIC_NAME,\n-            PARTITIONS_V0);\n-\n-    private static final Schema LIST_OFFSET_RESPONSE_V0 = new Schema(\n-            TOPICS_V0);\n-\n-    // V1 bumped for the removal of the offsets array\n-    private static final Field PARTITIONS_V1 = PARTITIONS.withFields(\n-            PARTITION_ID,\n-            ERROR_CODE,\n-            TIMESTAMP,\n-            OFFSET);\n-\n-    private static final Field TOPICS_V1 = TOPICS.withFields(\n-            TOPIC_NAME,\n-            PARTITIONS_V1);\n-\n-    private static final Schema LIST_OFFSET_RESPONSE_V1 = new Schema(\n-            TOPICS_V1);\n-\n-    // V2 bumped for the addition of the throttle time\n-    private static final Schema LIST_OFFSET_RESPONSE_V2 = new Schema(\n-            THROTTLE_TIME_MS,\n-            TOPICS_V1);\n-\n-    // V3 bumped to indicate that on quota violation brokers send out responses before throttling.\n-    private static final Schema LIST_OFFSET_RESPONSE_V3 = LIST_OFFSET_RESPONSE_V2;\n-\n-    // V4 bumped for the addition of the current leader epoch in the request schema and the\n-    // leader epoch in the response partition data\n-    private static final Field PARTITIONS_V4 = PARTITIONS.withFields(\n-            PARTITION_ID,\n-            ERROR_CODE,\n-            TIMESTAMP,\n-            OFFSET,\n-            LEADER_EPOCH);\n+    private final ListOffsetResponseData data;\n \n-    private static final Field TOPICS_V4 = TOPICS.withFields(\n-            TOPIC_NAME,\n-            PARTITIONS_V4);\n-\n-    private static final Schema LIST_OFFSET_RESPONSE_V4 = new Schema(\n-            THROTTLE_TIME_MS,\n-            TOPICS_V4);\n-\n-    private static final Schema LIST_OFFSET_RESPONSE_V5 = LIST_OFFSET_RESPONSE_V4;\n-\n-    public static Schema[] schemaVersions() {\n-        return new Schema[] {LIST_OFFSET_RESPONSE_V0, LIST_OFFSET_RESPONSE_V1, LIST_OFFSET_RESPONSE_V2,\n-            LIST_OFFSET_RESPONSE_V3, LIST_OFFSET_RESPONSE_V4, LIST_OFFSET_RESPONSE_V5};\n-    }\n-\n-    public static final class PartitionData {\n-        public final Errors error;\n-        // The offsets list is only used in ListOffsetResponse v0.\n-        public final List<Long> offsets;\n-        public final Long timestamp;\n-        public final Long offset;\n-        public final Optional<Integer> leaderEpoch;\n-\n-        /**\n-         * Constructor for ListOffsetResponse v0\n-         */\n-        public PartitionData(Errors error, List<Long> offsets) {\n-            this.error = error;\n-            this.offsets = offsets;\n-            this.timestamp = null;\n-            this.offset = null;\n-            this.leaderEpoch = Optional.empty();\n-        }\n-\n-        /**\n-         * Constructor for ListOffsetResponse v1\n-         */\n-        public PartitionData(Errors error, long timestamp, long offset, Optional<Integer> leaderEpoch) {\n-            this.error = error;\n-            this.timestamp = timestamp;\n-            this.offset = offset;\n-            this.offsets = null;\n-            this.leaderEpoch = leaderEpoch;\n-        }\n-\n-        @Override\n-        public String toString() {\n-            StringBuilder bld = new StringBuilder();\n-            bld.append(\"PartitionData(\").\n-                    append(\"errorCode: \").append(error.code());\n-\n-            if (offsets == null) {\n-                bld.append(\", timestamp: \").append(timestamp).\n-                        append(\", offset: \").append(offset).\n-                        append(\", leaderEpoch: \").append(leaderEpoch);\n-            } else {\n-                bld.append(\", offsets: \").\n-                        append(\"[\").\n-                        append(Utils.join(this.offsets, \",\")).\n-                        append(\"]\");\n-            }\n-            bld.append(\")\");\n-            return bld.toString();\n-        }\n+    public ListOffsetResponse(ListOffsetResponseData data) {\n+        this.data = data;\n     }\n \n-    private final int throttleTimeMs;\n-    private final Map<TopicPartition, PartitionData> responseData;\n-\n-    /**\n-     * Constructor for all versions without throttle time\n-     */\n-    public ListOffsetResponse(Map<TopicPartition, PartitionData> responseData) {\n-        this(DEFAULT_THROTTLE_TIME, responseData);\n-    }\n-\n-    public ListOffsetResponse(int throttleTimeMs, Map<TopicPartition, PartitionData> responseData) {\n-        this.throttleTimeMs = throttleTimeMs;\n-        this.responseData = responseData;\n-    }\n-\n-    public ListOffsetResponse(Struct struct) {\n-        this.throttleTimeMs = struct.getOrElse(THROTTLE_TIME_MS, DEFAULT_THROTTLE_TIME);\n-        responseData = new HashMap<>();\n-        for (Object topicResponseObj : struct.get(TOPICS)) {\n-            Struct topicResponse = (Struct) topicResponseObj;\n-            String topic = topicResponse.get(TOPIC_NAME);\n-            for (Object partitionResponseObj : topicResponse.get(PARTITIONS)) {\n-                Struct partitionResponse = (Struct) partitionResponseObj;\n-                int partition = partitionResponse.get(PARTITION_ID);\n-                Errors error = Errors.forCode(partitionResponse.get(ERROR_CODE));\n-                PartitionData partitionData;\n-                if (partitionResponse.hasField(OFFSETS)) {\n-                    Object[] offsets = partitionResponse.get(OFFSETS);\n-                    List<Long> offsetsList = new ArrayList<>();\n-                    for (Object offset : offsets)\n-                        offsetsList.add((Long) offset);\n-                    partitionData = new PartitionData(error, offsetsList);\n-                } else {\n-                    long timestamp = partitionResponse.get(TIMESTAMP);\n-                    long offset = partitionResponse.get(OFFSET);\n-                    Optional<Integer> leaderEpoch = RequestUtils.getLeaderEpoch(partitionResponse, LEADER_EPOCH);\n-                    partitionData = new PartitionData(error, timestamp, offset, leaderEpoch);\n-                }\n-                responseData.put(new TopicPartition(topic, partition), partitionData);\n-            }\n-        }\n+    public ListOffsetResponse(Struct struct, short version) {\n+        data = new ListOffsetResponseData(struct, version);\n     }\n \n     @Override\n     public int throttleTimeMs() {\n-        return throttleTimeMs;\n+        return data.throttleTimeMs();\n     }\n \n-    public Map<TopicPartition, PartitionData> responseData() {\n-        return responseData;\n+    public ListOffsetResponseData data() {\n+        return data;\n+    }\n+\n+    public List<ListOffsetTopicResponse> responseData() {", "originalCommit": "65ac2f1501fd6ee863a873cd93b64a1534d09461", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTYwNzA3OA==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r449607078", "bodyText": "nit: What about creating a small helper to create a ListOffsetTopicResponse for a given TopicPartition & co? That would reduce the boilerplate code.", "author": "dajac", "createdAt": "2020-07-03T14:20:14Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3220,12 +3286,30 @@ public void testListOffsetsMetadataRetriableErrors() throws Exception {\n             env.kafkaClient().prepareResponse(prepareMetadataResponse(cluster, Errors.NONE));\n \n             // listoffsets response from broker 0\n-            Map<TopicPartition, PartitionData> responseData = new HashMap<>();\n-            responseData.put(tp0, new PartitionData(Errors.NONE, -1L, 345L, Optional.of(543)));\n+            ListOffsetTopicResponse t0 = new ListOffsetTopicResponse()\n+                    .setName(tp0.topic())\n+                    .setPartitions(Collections.singletonList(new ListOffsetPartitionResponse()\n+                            .setPartitionIndex(tp0.partition())\n+                            .setErrorCode(Errors.NONE.code())\n+                            .setTimestamp(-1L)\n+                            .setOffset(345L)\n+                            .setLeaderEpoch(543)));", "originalCommit": "65ac2f1501fd6ee863a873cd93b64a1534d09461", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTYxMTkzMQ==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r449611931", "bodyText": "Can we remove these two?", "author": "dajac", "createdAt": "2020-07-03T14:31:39Z", "path": "clients/src/test/java/org/apache/kafka/common/security/authenticator/SaslAuthenticatorTest.java", "diffHunk": "@@ -1581,8 +1582,19 @@ SaslClient createSaslClient() {\n \n     @Test\n     public void testConvertListOffsetResponseToSaslHandshakeResponse() {\n-        ListOffsetResponse response = new ListOffsetResponse(0, Collections.singletonMap(new TopicPartition(\"topic\", 0),\n-            new ListOffsetResponse.PartitionData(Errors.NONE, 0, 0, Optional.empty())));\n+        ListOffsetResponseData data = new ListOffsetResponseData()\n+                .setThrottleTimeMs(0)\n+                .setTopics(Collections.singletonList(new ListOffsetTopicResponse()\n+                        .setName(\"topic\")\n+                        .setPartitions(Collections.singletonList(new ListOffsetPartitionResponse()\n+                                .setErrorCode(Errors.NONE.code())\n+                                .setLeaderEpoch(ListOffsetResponse.UNKNOWN_EPOCH)\n+                                .setPartitionIndex(0)\n+                                .setOffset(0)\n+                                .setTimestamp(0)))));\n+        ListOffsetResponse response = new ListOffsetResponse(data);\n+//        ListOffsetResponse response = new ListOffsetResponse(0, Collections.singletonMap(new TopicPartition(\"topic\", 0),\n+//            new ListOffsetResponse.PartitionData(Errors.NONE, 0, 0, Optional.empty())));", "originalCommit": "65ac2f1501fd6ee863a873cd93b64a1534d09461", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "78cb96fc13e5c337372b24b5ea50d7ade30485fc", "url": "https://github.com/apache/kafka/commit/78cb96fc13e5c337372b24b5ea50d7ade30485fc", "message": "KAFKA-9627: Replace ListOffset request/response with automated protocol\n\nCo-authored-by: Mickael Maison <mickael.maison@gmail.com>\nCo-authored-by: Edoardo Comar <ecomar@uk.ibm.com>", "committedDate": "2020-09-24T08:51:31Z", "type": "commit"}, {"oid": "78cb96fc13e5c337372b24b5ea50d7ade30485fc", "url": "https://github.com/apache/kafka/commit/78cb96fc13e5c337372b24b5ea50d7ade30485fc", "message": "KAFKA-9627: Replace ListOffset request/response with automated protocol\n\nCo-authored-by: Mickael Maison <mickael.maison@gmail.com>\nCo-authored-by: Edoardo Comar <ecomar@uk.ibm.com>", "committedDate": "2020-09-24T08:51:31Z", "type": "forcePushed"}]}