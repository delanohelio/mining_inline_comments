{"pr_number": 8031, "pr_title": "KAFKA-9447: Add new customized EOS model example", "pr_createdAt": "2020-02-01T17:17:11Z", "pr_url": "https://github.com/apache/kafka/pull/8031", "timeline": [{"oid": "b4a6a3a72887c18bed1e3cafbef5b68fd8d47ca5", "url": "https://github.com/apache/kafka/commit/b4a6a3a72887c18bed1e3cafbef5b68fd8d47ca5", "message": "add eos example", "committedDate": "2020-01-25T05:47:54Z", "type": "commit"}, {"oid": "0cf5868943fe5f4cde514d14b049d4b7a13a32b5", "url": "https://github.com/apache/kafka/commit/0cf5868943fe5f4cde514d14b049d4b7a13a32b5", "message": "extend producer demo with even number keys", "committedDate": "2020-01-31T21:39:52Z", "type": "commit"}, {"oid": "b038d7fa18760f50c87cd07f87336f9a90084037", "url": "https://github.com/apache/kafka/commit/b038d7fa18760f50c87cd07f87336f9a90084037", "message": "build entire java class with workflow", "committedDate": "2020-02-01T00:24:25Z", "type": "commit"}, {"oid": "bfc1b93c24dc17687456ebfce5adee5282751aeb", "url": "https://github.com/apache/kafka/commit/bfc1b93c24dc17687456ebfce5adee5282751aeb", "message": "create topic", "committedDate": "2020-02-01T03:22:44Z", "type": "commit"}, {"oid": "239e3bb794c978d54ead2578f02c4fc5be7cdd91", "url": "https://github.com/apache/kafka/commit/239e3bb794c978d54ead2578f02c4fc5be7cdd91", "message": "Only topic clean-up remaining", "committedDate": "2020-02-01T05:28:00Z", "type": "commit"}, {"oid": "db292ccd153797c5f9442848eee345a95f1a10e9", "url": "https://github.com/apache/kafka/commit/db292ccd153797c5f9442848eee345a95f1a10e9", "message": "fix topic deletion", "committedDate": "2020-02-01T05:40:45Z", "type": "commit"}, {"oid": "392b7190313411c3949c4a707820088f3754175f", "url": "https://github.com/apache/kafka/commit/392b7190313411c3949c4a707820088f3754175f", "message": "fix group mode", "committedDate": "2020-02-01T17:14:51Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzc5MDg2Mw==", "url": "https://github.com/apache/kafka/pull/8031#discussion_r373790863", "bodyText": "side cleanup", "author": "abbccdda", "createdAt": "2020-02-01T17:17:36Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/DescribeTopicsResult.java", "diffHunk": "@@ -51,21 +51,18 @@\n      */", "originalCommit": "392b7190313411c3949c4a707820088f3754175f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzc5MDg3MA==", "url": "https://github.com/apache/kafka/pull/8031#discussion_r373790870", "bodyText": "side cleanup", "author": "abbccdda", "createdAt": "2020-02-01T17:17:41Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/ListTopicsResult.java", "diffHunk": "@@ -48,23 +48,13 @@\n      * Return a future which yields a collection of TopicListing objects.", "originalCommit": "392b7190313411c3949c4a707820088f3754175f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "a6752749cb3a61b01638c5d2d31e46f2f19cea3c", "url": "https://github.com/apache/kafka/commit/a6752749cb3a61b01638c5d2d31e46f2f19cea3c", "message": "style fix", "committedDate": "2020-02-01T17:21:43Z", "type": "commit"}, {"oid": "217e3db74237e5f10a7c639e52048c00401ed22d", "url": "https://github.com/apache/kafka/commit/217e3db74237e5f10a7c639e52048c00401ed22d", "message": "More comments", "committedDate": "2020-02-01T22:23:55Z", "type": "commit"}, {"oid": "217e3db74237e5f10a7c639e52048c00401ed22d", "url": "https://github.com/apache/kafka/commit/217e3db74237e5f10a7c639e52048c00401ed22d", "message": "More comments", "committedDate": "2020-02-01T22:23:55Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzNzkxNQ==", "url": "https://github.com/apache/kafka/pull/8031#discussion_r374937915", "bodyText": "Just to refresh my memory: are we going to eventually deprecate this API, or are we going to keep both, and let users apply this one with manual assignment (like you did here)? I thought we are going to deprecate, but maybe I remembered it wrong.", "author": "guozhangwang", "createdAt": "2020-02-04T21:39:42Z", "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.consumer.CommitFailedException;\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.FencedInstanceIdException;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * A demo class for how to write a customized EOS app. It takes a consume-process-produce loop.\n+ * Important configurations and APIs are commented.\n+ */\n+public class ExactlyOnceMessageProcessor extends Thread {\n+\n+    private static final boolean READ_COMMITTED = true;\n+\n+    private final String mode;\n+    private final String inputTopic;\n+    private final String outputTopic;\n+    private final String consumerGroupId;\n+    private final int numPartitions;\n+    private final int numInstances;\n+    private final int instanceIdx;\n+    private final String transactionalId;\n+\n+    private final KafkaProducer<Integer, String> producer;\n+    private final KafkaConsumer<Integer, String> consumer;\n+\n+    private final CountDownLatch latch;\n+\n+    public ExactlyOnceMessageProcessor(final String mode,\n+                                       final String inputTopic,\n+                                       final String outputTopic,\n+                                       final int numPartitions,\n+                                       final int numInstances,\n+                                       final int instanceIdx,\n+                                       final CountDownLatch latch) {\n+        this.mode = mode;\n+        this.inputTopic = inputTopic;\n+        this.outputTopic = outputTopic;\n+        this.consumerGroupId = \"Eos-consumer\";\n+        this.numPartitions = numPartitions;\n+        this.numInstances = numInstances;\n+        this.instanceIdx = instanceIdx;\n+        this.transactionalId = \"Processor-\" + instanceIdx;\n+        // A unique transactional.id must be provided in order to properly use EOS.\n+        producer = new Producer(outputTopic, true, transactionalId, true, -1, null).get();\n+        // Consumer must be in read_committed mode, which means it won't be able to read uncommitted data.\n+        consumer = new Consumer(inputTopic, consumerGroupId, READ_COMMITTED, -1, null).get();\n+        this.latch = latch;\n+    }\n+\n+    @Override\n+    public void run() {\n+        // Init transactions call should always happen first in order to clear zombie transactions from previous generation.\n+        producer.initTransactions();\n+\n+        final AtomicLong messageRemaining = new AtomicLong(Long.MAX_VALUE);\n+\n+        // Under group mode, topic based subscription is sufficient as EOS apps are safe to cooperate transactionally after 2.5.\n+        // Under standalone mode, user needs to manually assign the topic partitions and make sure the assignment is unique\n+        // across the consumer group instances.\n+        if (this.mode.equals(\"groupMode\")) {\n+            consumer.subscribe(Collections.singleton(inputTopic), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Revoked partition assignment to kick-off rebalancing: \" + partitions);\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Received partition assignment after rebalancing: \" + partitions);\n+                    messageRemaining.set(messagesRemaining(consumer));\n+                }\n+            });\n+        } else {\n+            // Do a range assignment of topic partitions.\n+            List<TopicPartition> topicPartitions = new ArrayList<>();\n+            int rangeSize = numPartitions / numInstances;\n+            int startPartition = rangeSize * instanceIdx;\n+            int endPartition = Math.min(numPartitions - 1, startPartition + rangeSize - 1);\n+            for (int partition = startPartition; partition <= endPartition; partition++) {\n+                topicPartitions.add(new TopicPartition(inputTopic, partition));\n+            }\n+\n+            consumer.assign(topicPartitions);\n+            printWithTxnId(\"Manually assign partitions: \" + topicPartitions);\n+        }\n+\n+        int messageProcessed = 0;\n+        while (messageRemaining.get() > 0) {\n+            ConsumerRecords<Integer, String> records = consumer.poll(Duration.ofMillis(200));\n+            if (records.count() > 0) {\n+                try {\n+                    // Begin a new transaction session.\n+                    producer.beginTransaction();\n+                    for (ConsumerRecord<Integer, String> record : records) {\n+                        // Process the record and send to downstream.\n+                        ProducerRecord<Integer, String> customizedRecord = transform(record);\n+                        producer.send(customizedRecord);\n+                    }\n+                    Map<TopicPartition, OffsetAndMetadata> positions = new HashMap<>();\n+                    for (TopicPartition topicPartition : consumer.assignment()) {\n+                        positions.put(topicPartition, new OffsetAndMetadata(consumer.position(topicPartition), null));\n+                    }\n+                    // Checkpoint the progress by sending offsets to group coordinator broker.\n+                    // Under group mode, we must apply consumer group metadata for proper fencing.\n+                    if (this.mode.equals(\"groupMode\")) {\n+                        producer.sendOffsetsToTransaction(positions, consumer.groupMetadata());\n+                    } else {\n+                        producer.sendOffsetsToTransaction(positions, consumerGroupId);", "originalCommit": "217e3db74237e5f10a7c639e52048c00401ed22d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk4MjA0Nw==", "url": "https://github.com/apache/kafka/pull/8031#discussion_r374982047", "bodyText": "No, we are intended to keep both", "author": "abbccdda", "createdAt": "2020-02-04T23:27:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzNzkxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzODY2NQ==", "url": "https://github.com/apache/kafka/pull/8031#discussion_r374938665", "bodyText": "Why we need an atomic long here? Seems there's no concurrency.", "author": "guozhangwang", "createdAt": "2020-02-04T21:41:24Z", "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.consumer.CommitFailedException;\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.FencedInstanceIdException;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * A demo class for how to write a customized EOS app. It takes a consume-process-produce loop.\n+ * Important configurations and APIs are commented.\n+ */\n+public class ExactlyOnceMessageProcessor extends Thread {\n+\n+    private static final boolean READ_COMMITTED = true;\n+\n+    private final String mode;\n+    private final String inputTopic;\n+    private final String outputTopic;\n+    private final String consumerGroupId;\n+    private final int numPartitions;\n+    private final int numInstances;\n+    private final int instanceIdx;\n+    private final String transactionalId;\n+\n+    private final KafkaProducer<Integer, String> producer;\n+    private final KafkaConsumer<Integer, String> consumer;\n+\n+    private final CountDownLatch latch;\n+\n+    public ExactlyOnceMessageProcessor(final String mode,\n+                                       final String inputTopic,\n+                                       final String outputTopic,\n+                                       final int numPartitions,\n+                                       final int numInstances,\n+                                       final int instanceIdx,\n+                                       final CountDownLatch latch) {\n+        this.mode = mode;\n+        this.inputTopic = inputTopic;\n+        this.outputTopic = outputTopic;\n+        this.consumerGroupId = \"Eos-consumer\";\n+        this.numPartitions = numPartitions;\n+        this.numInstances = numInstances;\n+        this.instanceIdx = instanceIdx;\n+        this.transactionalId = \"Processor-\" + instanceIdx;\n+        // A unique transactional.id must be provided in order to properly use EOS.\n+        producer = new Producer(outputTopic, true, transactionalId, true, -1, null).get();\n+        // Consumer must be in read_committed mode, which means it won't be able to read uncommitted data.\n+        consumer = new Consumer(inputTopic, consumerGroupId, READ_COMMITTED, -1, null).get();\n+        this.latch = latch;\n+    }\n+\n+    @Override\n+    public void run() {\n+        // Init transactions call should always happen first in order to clear zombie transactions from previous generation.\n+        producer.initTransactions();\n+\n+        final AtomicLong messageRemaining = new AtomicLong(Long.MAX_VALUE);\n+\n+        // Under group mode, topic based subscription is sufficient as EOS apps are safe to cooperate transactionally after 2.5.\n+        // Under standalone mode, user needs to manually assign the topic partitions and make sure the assignment is unique\n+        // across the consumer group instances.\n+        if (this.mode.equals(\"groupMode\")) {\n+            consumer.subscribe(Collections.singleton(inputTopic), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Revoked partition assignment to kick-off rebalancing: \" + partitions);\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Received partition assignment after rebalancing: \" + partitions);\n+                    messageRemaining.set(messagesRemaining(consumer));\n+                }\n+            });\n+        } else {\n+            // Do a range assignment of topic partitions.\n+            List<TopicPartition> topicPartitions = new ArrayList<>();\n+            int rangeSize = numPartitions / numInstances;\n+            int startPartition = rangeSize * instanceIdx;\n+            int endPartition = Math.min(numPartitions - 1, startPartition + rangeSize - 1);\n+            for (int partition = startPartition; partition <= endPartition; partition++) {\n+                topicPartitions.add(new TopicPartition(inputTopic, partition));\n+            }\n+\n+            consumer.assign(topicPartitions);\n+            printWithTxnId(\"Manually assign partitions: \" + topicPartitions);\n+        }\n+\n+        int messageProcessed = 0;\n+        while (messageRemaining.get() > 0) {\n+            ConsumerRecords<Integer, String> records = consumer.poll(Duration.ofMillis(200));\n+            if (records.count() > 0) {\n+                try {\n+                    // Begin a new transaction session.\n+                    producer.beginTransaction();\n+                    for (ConsumerRecord<Integer, String> record : records) {\n+                        // Process the record and send to downstream.\n+                        ProducerRecord<Integer, String> customizedRecord = transform(record);\n+                        producer.send(customizedRecord);\n+                    }\n+                    Map<TopicPartition, OffsetAndMetadata> positions = new HashMap<>();\n+                    for (TopicPartition topicPartition : consumer.assignment()) {\n+                        positions.put(topicPartition, new OffsetAndMetadata(consumer.position(topicPartition), null));\n+                    }\n+                    // Checkpoint the progress by sending offsets to group coordinator broker.\n+                    // Under group mode, we must apply consumer group metadata for proper fencing.\n+                    if (this.mode.equals(\"groupMode\")) {\n+                        producer.sendOffsetsToTransaction(positions, consumer.groupMetadata());\n+                    } else {\n+                        producer.sendOffsetsToTransaction(positions, consumerGroupId);\n+                    }\n+\n+                    // Finish the transaction. All sent records should be visible for consumption now.\n+                    producer.commitTransaction();\n+                    messageProcessed += records.count();\n+                } catch (CommitFailedException e) {\n+                    // In case of a retriable exception, suggest aborting the ongoing transaction first for correctness.\n+                    producer.abortTransaction();\n+                } catch (ProducerFencedException | FencedInstanceIdException e) {\n+                    throw new KafkaException(\"Encountered fatal error during processing: \" + e.getMessage());\n+                }\n+            }\n+            messageRemaining.set(messagesRemaining(consumer));", "originalCommit": "217e3db74237e5f10a7c639e52048c00401ed22d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk4MTM1NA==", "url": "https://github.com/apache/kafka/pull/8031#discussion_r374981354", "bodyText": "The tricky thing here is that if we define a primitive long outside of the rebalance callback, it won't compile.", "author": "abbccdda", "createdAt": "2020-02-04T23:24:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzODY2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzOTE1NA==", "url": "https://github.com/apache/kafka/pull/8031#discussion_r374939154", "bodyText": "Why divide the key by two?", "author": "guozhangwang", "createdAt": "2020-02-04T21:42:21Z", "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.consumer.CommitFailedException;\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.FencedInstanceIdException;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * A demo class for how to write a customized EOS app. It takes a consume-process-produce loop.\n+ * Important configurations and APIs are commented.\n+ */\n+public class ExactlyOnceMessageProcessor extends Thread {\n+\n+    private static final boolean READ_COMMITTED = true;\n+\n+    private final String mode;\n+    private final String inputTopic;\n+    private final String outputTopic;\n+    private final String consumerGroupId;\n+    private final int numPartitions;\n+    private final int numInstances;\n+    private final int instanceIdx;\n+    private final String transactionalId;\n+\n+    private final KafkaProducer<Integer, String> producer;\n+    private final KafkaConsumer<Integer, String> consumer;\n+\n+    private final CountDownLatch latch;\n+\n+    public ExactlyOnceMessageProcessor(final String mode,\n+                                       final String inputTopic,\n+                                       final String outputTopic,\n+                                       final int numPartitions,\n+                                       final int numInstances,\n+                                       final int instanceIdx,\n+                                       final CountDownLatch latch) {\n+        this.mode = mode;\n+        this.inputTopic = inputTopic;\n+        this.outputTopic = outputTopic;\n+        this.consumerGroupId = \"Eos-consumer\";\n+        this.numPartitions = numPartitions;\n+        this.numInstances = numInstances;\n+        this.instanceIdx = instanceIdx;\n+        this.transactionalId = \"Processor-\" + instanceIdx;\n+        // A unique transactional.id must be provided in order to properly use EOS.\n+        producer = new Producer(outputTopic, true, transactionalId, true, -1, null).get();\n+        // Consumer must be in read_committed mode, which means it won't be able to read uncommitted data.\n+        consumer = new Consumer(inputTopic, consumerGroupId, READ_COMMITTED, -1, null).get();\n+        this.latch = latch;\n+    }\n+\n+    @Override\n+    public void run() {\n+        // Init transactions call should always happen first in order to clear zombie transactions from previous generation.\n+        producer.initTransactions();\n+\n+        final AtomicLong messageRemaining = new AtomicLong(Long.MAX_VALUE);\n+\n+        // Under group mode, topic based subscription is sufficient as EOS apps are safe to cooperate transactionally after 2.5.\n+        // Under standalone mode, user needs to manually assign the topic partitions and make sure the assignment is unique\n+        // across the consumer group instances.\n+        if (this.mode.equals(\"groupMode\")) {\n+            consumer.subscribe(Collections.singleton(inputTopic), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Revoked partition assignment to kick-off rebalancing: \" + partitions);\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Received partition assignment after rebalancing: \" + partitions);\n+                    messageRemaining.set(messagesRemaining(consumer));\n+                }\n+            });\n+        } else {\n+            // Do a range assignment of topic partitions.\n+            List<TopicPartition> topicPartitions = new ArrayList<>();\n+            int rangeSize = numPartitions / numInstances;\n+            int startPartition = rangeSize * instanceIdx;\n+            int endPartition = Math.min(numPartitions - 1, startPartition + rangeSize - 1);\n+            for (int partition = startPartition; partition <= endPartition; partition++) {\n+                topicPartitions.add(new TopicPartition(inputTopic, partition));\n+            }\n+\n+            consumer.assign(topicPartitions);\n+            printWithTxnId(\"Manually assign partitions: \" + topicPartitions);\n+        }\n+\n+        int messageProcessed = 0;\n+        while (messageRemaining.get() > 0) {\n+            ConsumerRecords<Integer, String> records = consumer.poll(Duration.ofMillis(200));\n+            if (records.count() > 0) {\n+                try {\n+                    // Begin a new transaction session.\n+                    producer.beginTransaction();\n+                    for (ConsumerRecord<Integer, String> record : records) {\n+                        // Process the record and send to downstream.\n+                        ProducerRecord<Integer, String> customizedRecord = transform(record);\n+                        producer.send(customizedRecord);\n+                    }\n+                    Map<TopicPartition, OffsetAndMetadata> positions = new HashMap<>();\n+                    for (TopicPartition topicPartition : consumer.assignment()) {\n+                        positions.put(topicPartition, new OffsetAndMetadata(consumer.position(topicPartition), null));\n+                    }\n+                    // Checkpoint the progress by sending offsets to group coordinator broker.\n+                    // Under group mode, we must apply consumer group metadata for proper fencing.\n+                    if (this.mode.equals(\"groupMode\")) {\n+                        producer.sendOffsetsToTransaction(positions, consumer.groupMetadata());\n+                    } else {\n+                        producer.sendOffsetsToTransaction(positions, consumerGroupId);\n+                    }\n+\n+                    // Finish the transaction. All sent records should be visible for consumption now.\n+                    producer.commitTransaction();\n+                    messageProcessed += records.count();\n+                } catch (CommitFailedException e) {\n+                    // In case of a retriable exception, suggest aborting the ongoing transaction first for correctness.\n+                    producer.abortTransaction();\n+                } catch (ProducerFencedException | FencedInstanceIdException e) {\n+                    throw new KafkaException(\"Encountered fatal error during processing: \" + e.getMessage());\n+                }\n+            }\n+            messageRemaining.set(messagesRemaining(consumer));\n+            printWithTxnId(\"Message remaining: \" + messageRemaining);\n+        }\n+\n+        printWithTxnId(\"Finished processing \" + messageProcessed + \" records\");\n+        latch.countDown();\n+    }\n+\n+    private void printWithTxnId(final String message) {\n+        System.out.println(transactionalId + \": \" + message);\n+    }\n+\n+    private ProducerRecord<Integer, String> transform(final ConsumerRecord<Integer, String> record) {\n+        printWithTxnId(\"Transformed record (\" + record.key() + \",\" + record.value() + \")\");\n+        return new ProducerRecord<>(outputTopic, record.key() / 2, \"Transformed_\" + record.value());", "originalCommit": "217e3db74237e5f10a7c639e52048c00401ed22d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk4MTU3MA==", "url": "https://github.com/apache/kafka/pull/8031#discussion_r374981570", "bodyText": "It's just a way of showing the key gets processed by message copier, all the produced message key are even keys.", "author": "abbccdda", "createdAt": "2020-02-04T23:25:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzOTE1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzOTQ4OA==", "url": "https://github.com/apache/kafka/pull/8031#discussion_r374939488", "bodyText": "prop: abortTransaction can also throw ProducerFenced.", "author": "guozhangwang", "createdAt": "2020-02-04T21:43:06Z", "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.consumer.CommitFailedException;\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.FencedInstanceIdException;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * A demo class for how to write a customized EOS app. It takes a consume-process-produce loop.\n+ * Important configurations and APIs are commented.\n+ */\n+public class ExactlyOnceMessageProcessor extends Thread {\n+\n+    private static final boolean READ_COMMITTED = true;\n+\n+    private final String mode;\n+    private final String inputTopic;\n+    private final String outputTopic;\n+    private final String consumerGroupId;\n+    private final int numPartitions;\n+    private final int numInstances;\n+    private final int instanceIdx;\n+    private final String transactionalId;\n+\n+    private final KafkaProducer<Integer, String> producer;\n+    private final KafkaConsumer<Integer, String> consumer;\n+\n+    private final CountDownLatch latch;\n+\n+    public ExactlyOnceMessageProcessor(final String mode,\n+                                       final String inputTopic,\n+                                       final String outputTopic,\n+                                       final int numPartitions,\n+                                       final int numInstances,\n+                                       final int instanceIdx,\n+                                       final CountDownLatch latch) {\n+        this.mode = mode;\n+        this.inputTopic = inputTopic;\n+        this.outputTopic = outputTopic;\n+        this.consumerGroupId = \"Eos-consumer\";\n+        this.numPartitions = numPartitions;\n+        this.numInstances = numInstances;\n+        this.instanceIdx = instanceIdx;\n+        this.transactionalId = \"Processor-\" + instanceIdx;\n+        // A unique transactional.id must be provided in order to properly use EOS.\n+        producer = new Producer(outputTopic, true, transactionalId, true, -1, null).get();\n+        // Consumer must be in read_committed mode, which means it won't be able to read uncommitted data.\n+        consumer = new Consumer(inputTopic, consumerGroupId, READ_COMMITTED, -1, null).get();\n+        this.latch = latch;\n+    }\n+\n+    @Override\n+    public void run() {\n+        // Init transactions call should always happen first in order to clear zombie transactions from previous generation.\n+        producer.initTransactions();\n+\n+        final AtomicLong messageRemaining = new AtomicLong(Long.MAX_VALUE);\n+\n+        // Under group mode, topic based subscription is sufficient as EOS apps are safe to cooperate transactionally after 2.5.\n+        // Under standalone mode, user needs to manually assign the topic partitions and make sure the assignment is unique\n+        // across the consumer group instances.\n+        if (this.mode.equals(\"groupMode\")) {\n+            consumer.subscribe(Collections.singleton(inputTopic), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Revoked partition assignment to kick-off rebalancing: \" + partitions);\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Received partition assignment after rebalancing: \" + partitions);\n+                    messageRemaining.set(messagesRemaining(consumer));\n+                }\n+            });\n+        } else {\n+            // Do a range assignment of topic partitions.\n+            List<TopicPartition> topicPartitions = new ArrayList<>();\n+            int rangeSize = numPartitions / numInstances;\n+            int startPartition = rangeSize * instanceIdx;\n+            int endPartition = Math.min(numPartitions - 1, startPartition + rangeSize - 1);\n+            for (int partition = startPartition; partition <= endPartition; partition++) {\n+                topicPartitions.add(new TopicPartition(inputTopic, partition));\n+            }\n+\n+            consumer.assign(topicPartitions);\n+            printWithTxnId(\"Manually assign partitions: \" + topicPartitions);\n+        }\n+\n+        int messageProcessed = 0;\n+        while (messageRemaining.get() > 0) {\n+            ConsumerRecords<Integer, String> records = consumer.poll(Duration.ofMillis(200));\n+            if (records.count() > 0) {\n+                try {\n+                    // Begin a new transaction session.\n+                    producer.beginTransaction();\n+                    for (ConsumerRecord<Integer, String> record : records) {\n+                        // Process the record and send to downstream.\n+                        ProducerRecord<Integer, String> customizedRecord = transform(record);\n+                        producer.send(customizedRecord);\n+                    }\n+                    Map<TopicPartition, OffsetAndMetadata> positions = new HashMap<>();\n+                    for (TopicPartition topicPartition : consumer.assignment()) {\n+                        positions.put(topicPartition, new OffsetAndMetadata(consumer.position(topicPartition), null));\n+                    }\n+                    // Checkpoint the progress by sending offsets to group coordinator broker.\n+                    // Under group mode, we must apply consumer group metadata for proper fencing.\n+                    if (this.mode.equals(\"groupMode\")) {\n+                        producer.sendOffsetsToTransaction(positions, consumer.groupMetadata());\n+                    } else {\n+                        producer.sendOffsetsToTransaction(positions, consumerGroupId);\n+                    }\n+\n+                    // Finish the transaction. All sent records should be visible for consumption now.\n+                    producer.commitTransaction();\n+                    messageProcessed += records.count();\n+                } catch (CommitFailedException e) {\n+                    // In case of a retriable exception, suggest aborting the ongoing transaction first for correctness.\n+                    producer.abortTransaction();", "originalCommit": "217e3db74237e5f10a7c639e52048c00401ed22d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk4NTY0NQ==", "url": "https://github.com/apache/kafka/pull/8031#discussion_r374985645", "bodyText": "Yea, which is ok as we will throw on ProducerFenced anyway", "author": "abbccdda", "createdAt": "2020-02-04T23:38:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzOTQ4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTAyNTYzMw==", "url": "https://github.com/apache/kafka/pull/8031#discussion_r375025633", "bodyText": "Not sure I understand: we try to capture ProducerFenced below and wrap it as a KafkaException, but here if we throw ProducerFenced it would not be captured and wrapped, is that intentional?", "author": "guozhangwang", "createdAt": "2020-02-05T02:07:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzOTQ4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTA1MTk3MA==", "url": "https://github.com/apache/kafka/pull/8031#discussion_r375051970", "bodyText": "Oh I see your pt.", "author": "abbccdda", "createdAt": "2020-02-05T04:17:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzOTQ4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk0MDMxOA==", "url": "https://github.com/apache/kafka/pull/8031#discussion_r374940318", "bodyText": "nit: we can just pass in the boolean values directly.", "author": "guozhangwang", "createdAt": "2020-02-04T21:44:53Z", "path": "examples/src/main/java/kafka/examples/KafkaExactlyOnceDemo.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.errors.TopicExistsException;\n+import org.apache.kafka.common.errors.UnknownTopicOrPartitionException;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeUnit;\n+\n+/**\n+ * This exactly once demo driver takes 4 arguments:\n+ *   - mode: whether to run as standalone app, or a group\n+ *   - partition: number of partitions for input/output topic\n+ *   - instances: number of instances\n+ *   - records: number of records\n+ * An example argument list would be `groupMode 6 3 50000`\n+ *\n+ * The driver could be decomposed as following stages:\n+ *\n+ * 1. Cleanup any topic whose name conflicts with input and output topic, so that we have a clean-start.\n+ *\n+ * 2. Set up a producer in a separate thread to pre-populate a set of records with even number keys into\n+ *    the input topic. The driver will block for the record generation to finish, so the producer\n+ *    must be in synchronous sending mode.\n+ *\n+ * 3. Set up transactional instances in separate threads which does a consume-process-produce loop,\n+ *    tailing data from input topic (See {@link ExactlyOnceMessageProcessor}). Each EOS instance will\n+ *    drain all the records from either given partitions or auto assigned partitions by actively\n+ *    comparing log end offset with committed offset. Each record will be processed exactly once\n+ *    as dividing the key by 2, and extend the value message. The driver will block for all the record\n+ *    processing to finish. The transformed record shall be written to the output topic, with\n+ *    transactional guarantee.\n+ *\n+ * 4. Set up a read committed consumer in a separate thread to verify we have all records within\n+ *    the output topic, while the message ordering on partition level is maintained.\n+ *    The driver will block for the consumption of all committed records.\n+ *\n+ * From this demo, you could see that all the records from pre-population are processed exactly once,\n+ * in either standalone mode or group mode, with strong partition level ordering guarantee.\n+ *\n+ * Note: please start the kafka broker and zookeeper in local first. The broker version must be >= 2.5\n+ * in order to run group mode, otherwise the app could throw\n+ * {@link org.apache.kafka.common.errors.UnsupportedVersionException}.\n+ */\n+public class KafkaExactlyOnceDemo {\n+\n+    private static final String INPUT_TOPIC = \"input-topic\";\n+    private static final String OUTPUT_TOPIC = \"output-topic\";\n+\n+    public static void main(String[] args) throws InterruptedException, ExecutionException {\n+        if (args.length != 4) {\n+            throw new IllegalArgumentException(\"Should accept 4 parameters: [mode], \" +\n+                \"[number of partitions], [number of instances], [number of records]\");\n+        }\n+\n+        String mode = args[0];\n+        int numPartitions = Integer.valueOf(args[1]);\n+        int numInstances = Integer.valueOf(args[2]);\n+        int numRecords = Integer.valueOf(args[3]);\n+\n+        /* Stage 1: topic cleanup and recreation */\n+        recreateTopics(numPartitions);\n+\n+        CountDownLatch prePopulateLatch = new CountDownLatch(1);\n+\n+        /* Stage 2: pre-populate records */\n+        final boolean isAsync = false;\n+        final boolean enableIdempotency = true;", "originalCommit": "217e3db74237e5f10a7c639e52048c00401ed22d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "5827af7522abb6206e61733180e39200811338c3", "url": "https://github.com/apache/kafka/commit/5827af7522abb6206e61733180e39200811338c3", "message": "guozhang's comment", "committedDate": "2020-02-04T23:37:33Z", "type": "commit"}, {"oid": "7a748240c100014e86e831f614bf4219974fbd32", "url": "https://github.com/apache/kafka/commit/7a748240c100014e86e831f614bf4219974fbd32", "message": "move abort txn out and add commit reset", "committedDate": "2020-02-05T20:48:16Z", "type": "commit"}, {"oid": "7a748240c100014e86e831f614bf4219974fbd32", "url": "https://github.com/apache/kafka/commit/7a748240c100014e86e831f614bf4219974fbd32", "message": "move abort txn out and add commit reset", "committedDate": "2020-02-05T20:48:16Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTk5OTIxNQ==", "url": "https://github.com/apache/kafka/pull/8031#discussion_r375999215", "bodyText": "It's a bit unconventional to have abort logic at the start of the loop. I think what users would expect is something like this:\ntry {\n  producer.beginTransaction()\n  producer.send(...)\n  producer.sendOffsetsToTransaction(...)\n  producer.commitTransaction()\n} catch (Exception ) {\n  producer.abortTransaction()\n}", "author": "hachikuji", "createdAt": "2020-02-06T18:13:33Z", "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -0,0 +1,209 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.examples;\n+\n+import org.apache.kafka.clients.consumer.CommitFailedException;\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.FencedInstanceIdException;\n+import org.apache.kafka.common.errors.ProducerFencedException;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * A demo class for how to write a customized EOS app. It takes a consume-process-produce loop.\n+ * Important configurations and APIs are commented.\n+ */\n+public class ExactlyOnceMessageProcessor extends Thread {\n+\n+    private static final boolean READ_COMMITTED = true;\n+\n+    private final String mode;\n+    private final String inputTopic;\n+    private final String outputTopic;\n+    private final String consumerGroupId;\n+    private final int numPartitions;\n+    private final int numInstances;\n+    private final int instanceIdx;\n+    private final String transactionalId;\n+\n+    private final KafkaProducer<Integer, String> producer;\n+    private final KafkaConsumer<Integer, String> consumer;\n+\n+    private final CountDownLatch latch;\n+\n+    public ExactlyOnceMessageProcessor(final String mode,\n+                                       final String inputTopic,\n+                                       final String outputTopic,\n+                                       final int numPartitions,\n+                                       final int numInstances,\n+                                       final int instanceIdx,\n+                                       final CountDownLatch latch) {\n+        this.mode = mode;\n+        this.inputTopic = inputTopic;\n+        this.outputTopic = outputTopic;\n+        this.consumerGroupId = \"Eos-consumer\";\n+        this.numPartitions = numPartitions;\n+        this.numInstances = numInstances;\n+        this.instanceIdx = instanceIdx;\n+        this.transactionalId = \"Processor-\" + instanceIdx;\n+        // A unique transactional.id must be provided in order to properly use EOS.\n+        producer = new Producer(outputTopic, true, transactionalId, true, -1, null).get();\n+        // Consumer must be in read_committed mode, which means it won't be able to read uncommitted data.\n+        consumer = new Consumer(inputTopic, consumerGroupId, READ_COMMITTED, -1, null).get();\n+        this.latch = latch;\n+    }\n+\n+    @Override\n+    public void run() {\n+        // Init transactions call should always happen first in order to clear zombie transactions from previous generation.\n+        producer.initTransactions();\n+\n+        final AtomicLong messageRemaining = new AtomicLong(Long.MAX_VALUE);\n+\n+        // Under group mode, topic based subscription is sufficient as EOS apps are safe to cooperate transactionally after 2.5.\n+        // Under standalone mode, user needs to manually assign the topic partitions and make sure the assignment is unique\n+        // across the consumer group instances.\n+        if (this.mode.equals(\"groupMode\")) {\n+            consumer.subscribe(Collections.singleton(inputTopic), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Revoked partition assignment to kick-off rebalancing: \" + partitions);\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    printWithTxnId(\"Received partition assignment after rebalancing: \" + partitions);\n+                    messageRemaining.set(messagesRemaining(consumer));\n+                }\n+            });\n+        } else {\n+            // Do a range assignment of topic partitions.\n+            List<TopicPartition> topicPartitions = new ArrayList<>();\n+            int rangeSize = numPartitions / numInstances;\n+            int startPartition = rangeSize * instanceIdx;\n+            int endPartition = Math.min(numPartitions - 1, startPartition + rangeSize - 1);\n+            for (int partition = startPartition; partition <= endPartition; partition++) {\n+                topicPartitions.add(new TopicPartition(inputTopic, partition));\n+            }\n+\n+            consumer.assign(topicPartitions);\n+            printWithTxnId(\"Manually assign partitions: \" + topicPartitions);\n+        }\n+\n+        int messageProcessed = 0;\n+        boolean abortPreviousTransaction = false;\n+        while (messageRemaining.get() > 0) {\n+            ConsumerRecords<Integer, String> records = consumer.poll(Duration.ofMillis(200));\n+            if (records.count() > 0) {\n+                try {\n+                    // Abort previous transaction if instructed.\n+                    if (abortPreviousTransaction) {\n+                        producer.abortTransaction();", "originalCommit": "7a748240c100014e86e831f614bf4219974fbd32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}