{"pr_number": 8668, "pr_title": "KAFKA-9987: optimize sticky assignment algorithm for same-subscription case", "pr_createdAt": "2020-05-14T18:46:15Z", "pr_url": "https://github.com/apache/kafka/pull/8668", "timeline": [{"oid": "dbac63dde4ce596ed26e422ec4449ece4113f15e", "url": "https://github.com/apache/kafka/commit/dbac63dde4ce596ed26e422ec4449ece4113f15e", "message": "refactor AbstractStickyAssignor and implement optimized algorithm", "committedDate": "2020-05-29T01:00:48Z", "type": "commit"}, {"oid": "2b2a0113c37e8b4959d2c33af25b67250f0b9938", "url": "https://github.com/apache/kafka/commit/2b2a0113c37e8b4959d2c33af25b67250f0b9938", "message": "rough benchmarks", "committedDate": "2020-05-29T01:00:48Z", "type": "commit"}, {"oid": "a812d348b8b735a941a515ac4d7ea49a2e3e8c63", "url": "https://github.com/apache/kafka/commit/a812d348b8b735a941a515ac4d7ea49a2e3e8c63", "message": "remove areSubscriptionsIdentical when we know they are not", "committedDate": "2020-05-29T01:00:48Z", "type": "commit"}, {"oid": "50486af2af1ae608adb295224f0aff4cb95511f5", "url": "https://github.com/apache/kafka/commit/50486af2af1ae608adb295224f0aff4cb95511f5", "message": "simplify", "committedDate": "2020-05-29T01:00:48Z", "type": "commit"}, {"oid": "afbed930a925cff1b11ec4192b5415334ea5e0b9", "url": "https://github.com/apache/kafka/commit/afbed930a925cff1b11ec4192b5415334ea5e0b9", "message": "renaming", "committedDate": "2020-05-29T01:00:48Z", "type": "commit"}, {"oid": "afbed930a925cff1b11ec4192b5415334ea5e0b9", "url": "https://github.com/apache/kafka/commit/afbed930a925cff1b11ec4192b5415334ea5e0b9", "message": "renaming", "committedDate": "2020-05-29T01:00:48Z", "type": "forcePushed"}, {"oid": "52c328578c71945dd708ceb7974b99c1e6f7255f", "url": "https://github.com/apache/kafka/commit/52c328578c71945dd708ceb7974b99c1e6f7255f", "message": "initialize movements", "committedDate": "2020-05-29T01:07:03Z", "type": "commit"}, {"oid": "f7a6fa4fdd9cfc2e72a0f5ee43dc68544c60a8d7", "url": "https://github.com/apache/kafka/commit/f7a6fa4fdd9cfc2e72a0f5ee43dc68544c60a8d7", "message": "trim by subscription", "committedDate": "2020-05-29T02:21:58Z", "type": "commit"}, {"oid": "f3a074fbd7083af008fcc3115f450e169473eec6", "url": "https://github.com/apache/kafka/commit/f3a074fbd7083af008fcc3115f450e169473eec6", "message": "fixing tests", "committedDate": "2020-05-29T02:58:57Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjIzMDU2MA==", "url": "https://github.com/apache/kafka/pull/8668#discussion_r432230560", "bodyText": "This test was testing an illegal state to begin with: you should never have two consumers in the same generation claim to own the same partition. That fact is the entire reason for the generation field to be added to the StickyAssignor's subscription userdata to begin with.", "author": "ableegoldman", "createdAt": "2020-05-29T03:01:16Z", "path": "clients/src/test/java/org/apache/kafka/clients/consumer/StickyAssignorTest.java", "diffHunk": "@@ -169,10 +169,10 @@ public void testAssignmentWithConflictingPreviousGenerations() {\n         TopicPartition tp5 = new TopicPartition(topic, 5);\n \n         List<TopicPartition> c1partitions0 = partitions(tp0, tp1, tp4);\n-        List<TopicPartition> c2partitions0 = partitions(tp0, tp2, tp3);\n+        List<TopicPartition> c2partitions0 = partitions(tp0, tp1, tp2);", "originalCommit": "f3a074fbd7083af008fcc3115f450e169473eec6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjIzMTEzNw==", "url": "https://github.com/apache/kafka/pull/8668#discussion_r432231137", "bodyText": "See comment above: this test was starting from an illegal state. Also, it doesn't make sense to place this in the AbstractStickyAssignorTest as the cooperative assignor can't have conflicting previous assignments. If a member thinks it still owns a partition that now belongs to another member, it will have to invoke onPartitionsLost before rejoining the group", "author": "ableegoldman", "createdAt": "2020-05-29T03:03:51Z", "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/AbstractStickyAssignorTest.java", "diffHunk": "@@ -582,35 +578,6 @@ public void testNoExceptionThrownWhenOnlySubscribedTopicDeleted() {\n         assertTrue(assignment.get(consumerId).isEmpty());\n     }\n \n-    @Test\n-    public void testConflictingPreviousAssignments() {", "originalCommit": "f3a074fbd7083af008fcc3115f450e169473eec6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "ff28badb115e61e2d08268d89b0db406c78426a8", "url": "https://github.com/apache/kafka/commit/ff28badb115e61e2d08268d89b0db406c78426a8", "message": "remove unused imports", "committedDate": "2020-05-29T03:04:55Z", "type": "commit"}, {"oid": "9a119767f8773b88a20c33d125c1b09845ff906f", "url": "https://github.com/apache/kafka/commit/9a119767f8773b88a20c33d125c1b09845ff906f", "message": "improve data parallelism", "committedDate": "2020-05-29T03:17:56Z", "type": "commit"}, {"oid": "63e797ff3951489c731daa7a846af75458490bc4", "url": "https://github.com/apache/kafka/commit/63e797ff3951489c731daa7a846af75458490bc4", "message": "use Optional.empty", "committedDate": "2020-05-29T03:23:20Z", "type": "commit"}, {"oid": "e3975653d19f97d6eb61cb24e4cb25b7965e16c1", "url": "https://github.com/apache/kafka/commit/e3975653d19f97d6eb61cb24e4cb25b7965e16c1", "message": "apply timeout and increase number of topics", "committedDate": "2020-05-29T04:08:02Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI0NjAyOA==", "url": "https://github.com/apache/kafka/pull/8668#discussion_r432246028", "bodyText": "On trunk, this test fails (hits the 30s timeout) even when you reduce the number of topics to just 1", "author": "ableegoldman", "createdAt": "2020-05-29T04:13:12Z", "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/AbstractStickyAssignorTest.java", "diffHunk": "@@ -425,8 +422,36 @@ public void testSameSubscriptions() {\n         assertTrue(assignor.isSticky());\n     }\n \n+    @Test(timeout = 30 * 1000)\n+    public void testLargeAssignmentAndGroupWithUniformSubscription() {\n+        int topicCount = 200;", "originalCommit": "e3975653d19f97d6eb61cb24e4cb25b7965e16c1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjY4Mzg0MA==", "url": "https://github.com/apache/kafka/pull/8668#discussion_r432683840", "bodyText": "nit: to be consistent, we can just add consumer to membersWithOldGeneration and then let them to be cleared at the end.", "author": "guozhangwang", "createdAt": "2020-05-29T19:12:44Z", "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractStickyAssignor.java", "diffHunk": "@@ -65,9 +69,182 @@ public MemberData(List<TopicPartition> partitions, Optional<Integer> generation)\n     @Override\n     public Map<String, List<TopicPartition>> assign(Map<String, Integer> partitionsPerTopic,\n                                                     Map<String, Subscription> subscriptions) {\n+        Map<String, List<TopicPartition>> consumerToOwnedPartitions = new HashMap<>();\n+        Set<String> subscribedTopics = new HashSet<>();\n+        if (allSubscriptionsEqual(subscriptions, consumerToOwnedPartitions, subscribedTopics)) {\n+            log.debug(\"Detected that all consumers were subscribed to same set of topics, invoking the \"\n+                          + \"optimized assignment algorithm\");\n+            return constrainedAssign(partitionsPerTopic, subscribedTopics, consumerToOwnedPartitions);\n+        } else {\n+            log.debug(\"Detected that all not consumers were subscribed to same set of topics, falling back to the \"\n+                          + \"general case assignment algorithm\");\n+            return generalAssign(partitionsPerTopic, subscriptions);\n+        }\n+    }\n+\n+    private boolean allSubscriptionsEqual(Map<String, Subscription> subscriptions,\n+                                          Map<String, List<TopicPartition>> consumerToOwnedPartitions,\n+                                          Set<String> subscribedTopics) {\n+        Set<String> membersWithOldGeneration = new HashSet<>();\n+        Set<String> membersOfCurrentHighestGeneration = new HashSet<>();\n+        int maxGeneration = -1;\n+\n+        for (Map.Entry<String, Subscription> subscriptionEntry : subscriptions.entrySet()) {\n+            String consumer = subscriptionEntry.getKey();\n+            Subscription subscription = subscriptionEntry.getValue();\n+\n+            // initialize the subscribed topics set if this is the first subscription\n+            if (subscribedTopics.isEmpty()) {\n+                subscribedTopics.addAll(subscription.topics());\n+            } else if (!(subscription.topics().size() == subscribedTopics.size()\n+                && subscribedTopics.containsAll(subscription.topics()))) {\n+                return false;\n+            }\n+\n+            MemberData memberData = memberData(subscription);\n+\n+            // If this member's generation is lower than the current max, or it is not present while\n+            // other generations are, consider it as having lost its owned partition\n+            if (!memberData.generation.isPresent() && maxGeneration > 0\n+                    || memberData.generation.isPresent() && memberData.generation.get() < maxGeneration) {\n+                consumerToOwnedPartitions.put(consumer, new ArrayList<>());", "originalCommit": "e3975653d19f97d6eb61cb24e4cb25b7965e16c1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjczMDE5Mg==", "url": "https://github.com/apache/kafka/pull/8668#discussion_r432730192", "bodyText": "Hm, it seems odd to clear it at the end since it's definitely already empty. Note, we're not overwriting the current partitions with an empty array, we're just initializing the assignment for this consumer. I'll add a comment though", "author": "ableegoldman", "createdAt": "2020-05-29T20:54:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjY4Mzg0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjY5MDIyMA==", "url": "https://github.com/apache/kafka/pull/8668#discussion_r432690220", "bodyText": "Is it possible that this unfilled consumer has N+1 remaining capacity, while there's only N max consumer only?", "author": "guozhangwang", "createdAt": "2020-05-29T19:27:51Z", "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractStickyAssignor.java", "diffHunk": "@@ -65,9 +69,182 @@ public MemberData(List<TopicPartition> partitions, Optional<Integer> generation)\n     @Override\n     public Map<String, List<TopicPartition>> assign(Map<String, Integer> partitionsPerTopic,\n                                                     Map<String, Subscription> subscriptions) {\n+        Map<String, List<TopicPartition>> consumerToOwnedPartitions = new HashMap<>();\n+        Set<String> subscribedTopics = new HashSet<>();\n+        if (allSubscriptionsEqual(subscriptions, consumerToOwnedPartitions, subscribedTopics)) {\n+            log.debug(\"Detected that all consumers were subscribed to same set of topics, invoking the \"\n+                          + \"optimized assignment algorithm\");\n+            return constrainedAssign(partitionsPerTopic, subscribedTopics, consumerToOwnedPartitions);\n+        } else {\n+            log.debug(\"Detected that all not consumers were subscribed to same set of topics, falling back to the \"\n+                          + \"general case assignment algorithm\");\n+            return generalAssign(partitionsPerTopic, subscriptions);\n+        }\n+    }\n+\n+    private boolean allSubscriptionsEqual(Map<String, Subscription> subscriptions,\n+                                          Map<String, List<TopicPartition>> consumerToOwnedPartitions,\n+                                          Set<String> subscribedTopics) {\n+        Set<String> membersWithOldGeneration = new HashSet<>();\n+        Set<String> membersOfCurrentHighestGeneration = new HashSet<>();\n+        int maxGeneration = -1;\n+\n+        for (Map.Entry<String, Subscription> subscriptionEntry : subscriptions.entrySet()) {\n+            String consumer = subscriptionEntry.getKey();\n+            Subscription subscription = subscriptionEntry.getValue();\n+\n+            // initialize the subscribed topics set if this is the first subscription\n+            if (subscribedTopics.isEmpty()) {\n+                subscribedTopics.addAll(subscription.topics());\n+            } else if (!(subscription.topics().size() == subscribedTopics.size()\n+                && subscribedTopics.containsAll(subscription.topics()))) {\n+                return false;\n+            }\n+\n+            MemberData memberData = memberData(subscription);\n+\n+            // If this member's generation is lower than the current max, or it is not present while\n+            // other generations are, consider it as having lost its owned partition\n+            if (!memberData.generation.isPresent() && maxGeneration > 0\n+                    || memberData.generation.isPresent() && memberData.generation.get() < maxGeneration) {\n+                consumerToOwnedPartitions.put(consumer, new ArrayList<>());\n+            } else {\n+                // If the current member's generation is higher, all the previous owned partitions are invalid\n+                if (memberData.generation.isPresent() && memberData.generation.get() > maxGeneration) {\n+                    membersWithOldGeneration.addAll(membersOfCurrentHighestGeneration);\n+                    membersOfCurrentHighestGeneration.clear();\n+                    maxGeneration = memberData.generation.get();\n+                }\n+                membersOfCurrentHighestGeneration.add(consumer);\n+                List<TopicPartition> ownedPartitions = memberData.partitions.stream()\n+                    .filter(tp -> subscribedTopics.contains(tp.topic()))\n+                    .collect(Collectors.toList());\n+                consumerToOwnedPartitions.put(consumer, ownedPartitions);\n+            }\n+        }\n+\n+        for (String consumer : membersWithOldGeneration) {\n+            consumerToOwnedPartitions.get(consumer).clear();\n+        }\n+        return true;\n+    }\n+\n+    private Map<String, List<TopicPartition>> constrainedAssign(Map<String, Integer> partitionsPerTopic,\n+                                                                Set<String> subscribedTopics,\n+                                                                Map<String, List<TopicPartition>> consumerToOwnedPartitions) {\n+        SortedSet<TopicPartition> unassignedPartitions = getTopicPartitions(partitionsPerTopic, subscribedTopics);\n+\n+        // Each consumer should end up in exactly one of the below\n+        List<String> unfilledMembers = new LinkedList<>();\n+        Queue<String> maxCapacityMembers = new LinkedList<>();\n+        Queue<String> minCapacityMembers = new LinkedList<>();\n+\n+        int numberOfConsumers = consumerToOwnedPartitions.size();\n+        int minQuota = (int) Math.floor(((double)unassignedPartitions.size()) / numberOfConsumers);\n+        int maxQuota = (int) Math.ceil(((double)unassignedPartitions.size()) / numberOfConsumers);\n+\n+        // initialize the assignment map with an empty array of size minQuota for all members\n+        Map<String, List<TopicPartition>> assignment = new HashMap<>(\n+            consumerToOwnedPartitions.keySet().stream().collect(Collectors.toMap(c -> c, c -> new ArrayList<>(minQuota))));\n+\n+        for (Map.Entry<String, List<TopicPartition>> consumerEntry : consumerToOwnedPartitions.entrySet()) {\n+            String consumer = consumerEntry.getKey();\n+            List<TopicPartition> ownedPartitions = consumerEntry.getValue();\n+\n+            if (ownedPartitions.size() < minQuota) {\n+                assignment.get(consumer).addAll(ownedPartitions);\n+                unassignedPartitions.removeAll(ownedPartitions);\n+                unfilledMembers.add(consumer);\n+            } else if (ownedPartitions.size() == minQuota) {\n+                assignment.get(consumer).addAll(ownedPartitions);\n+                unassignedPartitions.removeAll(ownedPartitions);\n+                minCapacityMembers.add(consumer);\n+            } else {\n+                List<TopicPartition> assignmentPartitions = assignment.get(consumer);\n+                Iterator<TopicPartition> ownedPartitionsIter = ownedPartitions.iterator();\n+                for (int i = 0; i < maxQuota; ++i) {\n+                    TopicPartition tp = ownedPartitionsIter.next();\n+                    assignmentPartitions.add(tp);\n+                    unassignedPartitions.remove(tp);\n+                }\n+                maxCapacityMembers.add(consumer);\n+            }\n+        }\n+\n+        Collections.sort(unfilledMembers);\n+        Iterator<TopicPartition> unassignedPartitionsIter = unassignedPartitions.iterator();\n+\n+        while (!unfilledMembers.isEmpty() && !unassignedPartitions.isEmpty()) {\n+            Iterator<String> unfilledConsumerIter = unfilledMembers.iterator();\n+\n+            while (unfilledConsumerIter.hasNext()) {\n+                String consumer = unfilledConsumerIter.next();\n+                List<TopicPartition> consumerAssignment = assignment.get(consumer);\n+\n+                if (unassignedPartitionsIter.hasNext()) {\n+                    consumerAssignment.add(unassignedPartitionsIter.next());\n+                    unassignedPartitionsIter.remove();\n+                } else {\n+                    break;\n+                }\n+\n+                if (consumerAssignment.size() == minQuota) {\n+                    minCapacityMembers.add(consumer);\n+                    unfilledConsumerIter.remove();\n+                }\n+            }\n+        }\n+\n+        // If we ran out of unassigned partitions before filling all consumers, we need to start stealing partitions\n+        // from the over-full consumers at max capacity\n+        for (String consumer : unfilledMembers) {\n+            List<TopicPartition> consumerAssignment = assignment.get(consumer);\n+            int remainingCapacity = minQuota - consumerAssignment.size();\n+            while (remainingCapacity > 0) {", "originalCommit": "e3975653d19f97d6eb61cb24e4cb25b7965e16c1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjY5MTY0OQ==", "url": "https://github.com/apache/kafka/pull/8668#discussion_r432691649", "bodyText": "NVM, I realized it should never happen.", "author": "guozhangwang", "createdAt": "2020-05-29T19:31:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjY5MDIyMA=="}], "type": "inlineReview"}, {"oid": "e5eeb7d577df00d89825131c1e097ea5bb5a096e", "url": "https://github.com/apache/kafka/commit/e5eeb7d577df00d89825131c1e097ea5bb5a096e", "message": "add comment and fix checkstyle", "committedDate": "2020-05-29T20:55:48Z", "type": "commit"}, {"oid": "18b13cb1039865660f5bb807e4771694b42894fa", "url": "https://github.com/apache/kafka/commit/18b13cb1039865660f5bb807e4771694b42894fa", "message": "remove more unused code", "committedDate": "2020-05-29T21:04:31Z", "type": "commit"}, {"oid": "07267a24afb09deb5077a5e06ab6fee9d8dc1946", "url": "https://github.com/apache/kafka/commit/07267a24afb09deb5077a5e06ab6fee9d8dc1946", "message": "simplify  a bit", "committedDate": "2020-05-29T21:31:20Z", "type": "commit"}, {"oid": "f6ce366c69d1565cc0be552218e019238a0d4988", "url": "https://github.com/apache/kafka/commit/f6ce366c69d1565cc0be552218e019238a0d4988", "message": "remove unnecessary check and  fix test to verify valid state", "committedDate": "2020-05-29T21:47:10Z", "type": "commit"}, {"oid": "d9c93d0191c554f468845d1a592b27400be40007", "url": "https://github.com/apache/kafka/commit/d9c93d0191c554f468845d1a592b27400be40007", "message": "filter by cluster topics", "committedDate": "2020-05-29T22:00:14Z", "type": "commit"}, {"oid": "e9b243a793c243b5580bf4075c451766ebb49aef", "url": "https://github.com/apache/kafka/commit/e9b243a793c243b5580bf4075c451766ebb49aef", "message": "simplify owned partition aassignemnt", "committedDate": "2020-05-29T22:16:34Z", "type": "commit"}, {"oid": "ad1f63d0ce6c53a1392fd4b1613bb713a5a10791", "url": "https://github.com/apache/kafka/commit/ad1f63d0ce6c53a1392fd4b1613bb713a5a10791", "message": "can reduce further", "committedDate": "2020-05-29T22:20:14Z", "type": "commit"}, {"oid": "28d13ba8d382a41018790ca9ba92083bc418ffe4", "url": "https://github.com/apache/kafka/commit/28d13ba8d382a41018790ca9ba92083bc418ffe4", "message": "condition can be tighter", "committedDate": "2020-05-29T22:21:21Z", "type": "commit"}, {"oid": "7ec6131486215032cc4fec290b9fac80a463316a", "url": "https://github.com/apache/kafka/commit/7ec6131486215032cc4fec290b9fac80a463316a", "message": "optimize by tracking reassigned partitions", "committedDate": "2020-05-29T22:56:08Z", "type": "commit"}, {"oid": "6210a757738f9886fad4676cd22eeee5227bb863", "url": "https://github.com/apache/kafka/commit/6210a757738f9886fad4676cd22eeee5227bb863", "message": "sclae to 1million", "committedDate": "2020-05-30T00:51:33Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgwMjEwNg==", "url": "https://github.com/apache/kafka/pull/8668#discussion_r432802106", "bodyText": "This is just an optimization for the cooperative case: I found that the assignment time for the eager and cooperative assignor began to diverge once you reached partition counts in the millions. At 10 million partitions for example, the eager assignor hovered around 30s but the cooperative assignor was upwards of 5-6 minutes.\nThe discrepancy was entirely due to the adjustAssignment method needing to compute the set of partitions transferring ownership  in the completed assignment. But we can build up this map during assignment much more efficiently, by taking advantage of the additional context we have at various steps in the algorithm. Tracking and exposing this set to the cooperative assignor cut the assignment time for large partition numbers pretty drastically, putting the cooperative assignor  on par with the eager assignor.", "author": "ableegoldman", "createdAt": "2020-05-30T02:29:09Z", "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractStickyAssignor.java", "diffHunk": "@@ -40,7 +43,11 @@\n \n     public static final int DEFAULT_GENERATION = -1;\n \n-    private PartitionMovements partitionMovements;\n+    private PartitionMovements partitionMovements = new PartitionMovements();\n+\n+    // Keep track of the partitions being migrated from one consumer to another during assignment\n+    // so the cooperative assignor can adjust the assignment\n+    protected Map<TopicPartition, String> partitionsTransferringOwnership = new HashMap<>();", "originalCommit": "6210a757738f9886fad4676cd22eeee5227bb863", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgwMjIwNg==", "url": "https://github.com/apache/kafka/pull/8668#discussion_r432802206", "bodyText": "I didn't bother to include this optimization for the general case. We know that the assignment algorithm itself becomes a bottleneck at only 2,000 partitions, so there's no point optimizing something that only becomes a bottleneck in the millions of partitions", "author": "ableegoldman", "createdAt": "2020-05-30T02:30:31Z", "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractStickyAssignor.java", "diffHunk": "@@ -65,9 +72,206 @@ public MemberData(List<TopicPartition> partitions, Optional<Integer> generation)\n     @Override\n     public Map<String, List<TopicPartition>> assign(Map<String, Integer> partitionsPerTopic,\n                                                     Map<String, Subscription> subscriptions) {\n+        Map<String, List<TopicPartition>> consumerToOwnedPartitions = new HashMap<>();\n+        if (allSubscriptionsEqual(partitionsPerTopic.keySet(), subscriptions, consumerToOwnedPartitions)) {\n+            log.debug(\"Detected that all consumers were subscribed to same set of topics, invoking the \"\n+                          + \"optimized assignment algorithm\");\n+            partitionsTransferringOwnership = new HashMap<>();\n+            return constrainedAssign(partitionsPerTopic, consumerToOwnedPartitions);\n+        } else {\n+            log.debug(\"Detected that all not consumers were subscribed to same set of topics, falling back to the \"\n+                          + \"general case assignment algorithm\");\n+            partitionsTransferringOwnership = null;", "originalCommit": "6210a757738f9886fad4676cd22eeee5227bb863", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "e25a323d7e21aa5a554e43d811ac6eb3e3894055", "url": "https://github.com/apache/kafka/commit/e25a323d7e21aa5a554e43d811ac6eb3e3894055", "message": "checkstyle", "committedDate": "2020-05-30T02:34:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgwMjY2OQ==", "url": "https://github.com/apache/kafka/pull/8668#discussion_r432802669", "bodyText": "This test was also starting with an illegal state -- partitionsPerTopic only contains metadata for topics included in the subscription. I noticed that we don't seem to be testing the actual valid case, where some consumers have ownedPartitions which are no longer in the subscription, so I just adapted this test for the related purpose", "author": "ableegoldman", "createdAt": "2020-05-30T02:37:57Z", "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/AbstractStickyAssignorTest.java", "diffHunk": "@@ -105,12 +106,16 @@ public void testOnlyAssignsPartitionsFromSubscribedTopics() {\n         String otherTopic = \"other\";\n \n         Map<String, Integer> partitionsPerTopic = new HashMap<>();\n-        partitionsPerTopic.put(topic, 3);\n-        partitionsPerTopic.put(otherTopic, 3);\n-        subscriptions = Collections.singletonMap(consumerId, new Subscription(topics(topic)));", "originalCommit": "6210a757738f9886fad4676cd22eeee5227bb863", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDAyMTcwNA==", "url": "https://github.com/apache/kafka/pull/8668#discussion_r434021704", "bodyText": "Just FYI, I introduced this bug right before merging. Luckily the tests caught it -- fix is #8777", "author": "ableegoldman", "createdAt": "2020-06-02T16:42:49Z", "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractStickyAssignor.java", "diffHunk": "@@ -65,9 +72,206 @@ public MemberData(List<TopicPartition> partitions, Optional<Integer> generation)\n     @Override\n     public Map<String, List<TopicPartition>> assign(Map<String, Integer> partitionsPerTopic,\n                                                     Map<String, Subscription> subscriptions) {\n+        Map<String, List<TopicPartition>> consumerToOwnedPartitions = new HashMap<>();\n+        if (allSubscriptionsEqual(partitionsPerTopic.keySet(), subscriptions, consumerToOwnedPartitions)) {\n+            log.debug(\"Detected that all consumers were subscribed to same set of topics, invoking the \"\n+                          + \"optimized assignment algorithm\");\n+            partitionsTransferringOwnership = new HashMap<>();\n+            return constrainedAssign(partitionsPerTopic, consumerToOwnedPartitions);\n+        } else {\n+            log.debug(\"Detected that all not consumers were subscribed to same set of topics, falling back to the \"\n+                          + \"general case assignment algorithm\");\n+            partitionsTransferringOwnership = null;\n+            return generalAssign(partitionsPerTopic, subscriptions);\n+        }\n+    }\n+\n+    /**\n+     * Returns true iff all consumers have an identical subscription. Also fills out the passed in\n+     * {@code consumerToOwnedPartitions} with each consumer's previously owned and still-subscribed partitions\n+     */\n+    private boolean allSubscriptionsEqual(Set<String> allTopics,\n+                                          Map<String, Subscription> subscriptions,\n+                                          Map<String, List<TopicPartition>> consumerToOwnedPartitions) {\n+        Set<String> membersWithOldGeneration = new HashSet<>();\n+        Set<String> membersOfCurrentHighestGeneration = new HashSet<>();\n+        int maxGeneration = DEFAULT_GENERATION;\n+\n+        Set<String> subscribedTopics = new HashSet<>();\n+\n+        for (Map.Entry<String, Subscription> subscriptionEntry : subscriptions.entrySet()) {\n+            String consumer = subscriptionEntry.getKey();\n+            Subscription subscription = subscriptionEntry.getValue();\n+\n+            // initialize the subscribed topics set if this is the first subscription\n+            if (subscribedTopics.isEmpty()) {\n+                subscribedTopics.addAll(subscription.topics());\n+            } else if (!(subscription.topics().size() == subscribedTopics.size()\n+                && subscribedTopics.containsAll(subscription.topics()))) {\n+                return false;\n+            }\n+\n+            MemberData memberData = memberData(subscription);\n+\n+            List<TopicPartition> ownedPartitions = new ArrayList<>();\n+            consumerToOwnedPartitions.put(consumer, ownedPartitions);\n+\n+            // Only consider this consumer's owned partitions as valid if it is a member of the current highest\n+            // generation, or it's generation is not present but we have not seen any known generation so far\n+            if (memberData.generation.isPresent() && memberData.generation.get() >= maxGeneration\n+                || !memberData.generation.isPresent() && maxGeneration == DEFAULT_GENERATION) {\n+\n+                membersOfCurrentHighestGeneration.add(consumer);\n+                for (final TopicPartition tp : memberData.partitions) {\n+                    // filter out any topics that no longer exist or aren't part of the current subscription\n+                    if (allTopics.contains(tp.topic())) {\n+                        ownedPartitions.add(tp);\n+                    }\n+                }\n+\n+                // If the current member's generation is higher, all the previous owned partitions are invalid\n+                if (memberData.generation.isPresent() && memberData.generation.get() > maxGeneration) {\n+                    membersWithOldGeneration.addAll(membersOfCurrentHighestGeneration);\n+                    membersOfCurrentHighestGeneration.clear();", "originalCommit": "e25a323d7e21aa5a554e43d811ac6eb3e3894055", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}