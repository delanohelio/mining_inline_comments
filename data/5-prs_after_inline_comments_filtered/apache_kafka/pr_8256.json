{"pr_number": 8256, "pr_title": "KAFKA-9675: Fix bug that prevents RocksDB metrics to be updated", "pr_createdAt": "2020-03-09T13:18:42Z", "pr_url": "https://github.com/apache/kafka/pull/8256", "timeline": [{"oid": "a652b378d51c3a83b71758d191d439457123b6ba", "url": "https://github.com/apache/kafka/commit/a652b378d51c3a83b71758d191d439457123b6ba", "message": "KAFKA-9675: Fix bug that prevents RocksDB metrics to be updated", "committedDate": "2020-03-09T12:22:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY4NDM4MQ==", "url": "https://github.com/apache/kafka/pull/8256#discussion_r389684381", "bodyText": "This is the fix. For an explanation read the commit message.", "author": "cadonna", "createdAt": "2020-03-09T13:46:50Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -188,11 +188,12 @@ void openDB(final ProcessorContext context) {\n             throw new ProcessorStateException(fatal);\n         }\n \n+        // Setup metrics before the database is opened, otherwise the metrics are not updated\n+        // with the measurements from Rocks DB\n+        maybeSetUpMetricsRecorder(context, configs);\n+\n         openRocksDB(dbOptions, columnFamilyOptions);\n         open = true;\n-\n-        // Do this last because the prior operations could throw exceptions.\n-        maybeSetUpMetricsRecorder(context, configs);", "originalCommit": "a652b378d51c3a83b71758d191d439457123b6ba", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY4OTkyNw==", "url": "https://github.com/apache/kafka/pull/8256#discussion_r389689927", "bodyText": "This verifies that the metrics are updated when the measurements in the statistics object change. Unfortunately, the tests the use this verification run more than a minute, because the thread that triggers the recordings of the RocksDB metrics takes some time to record its first value.", "author": "cadonna", "createdAt": "2020-03-09T13:51:37Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/RocksDBMetricsIntegrationTest.java", "diffHunk": "@@ -283,13 +333,38 @@ private void verifyRocksDBMetrics(final KafkaStreams kafkaStreams, final String\n         checkMetricByName(listMetricStore, NUMBER_OF_FILE_ERRORS, 1);\n     }\n \n-    private void checkMetricByName(final List<Metric> listMetric, final String metricName, final int numMetric) {\n+    private void checkMetricByName(final List<Metric> listMetric,\n+                                   final String metricName,\n+                                   final int numMetric) {\n         final List<Metric> metrics = listMetric.stream()\n             .filter(m -> m.metricName().name().equals(metricName))\n             .collect(Collectors.toList());\n-        Assert.assertEquals(\"Size of metrics of type:'\" + metricName + \"' must be equal to \" + numMetric + \" but it's equal to \" + metrics.size(), numMetric, metrics.size());\n-        for (final Metric m : metrics) {\n-            Assert.assertNotNull(\"Metric:'\" + m.metricName() + \"' must be not null\", m.metricValue());\n+        assertThat(\n+            \"Size of metrics of type:'\" + metricName + \"' must be equal to \" + numMetric + \" but it's equal to \" + metrics.size(),\n+            metrics.size(),\n+            is(numMetric)\n+        );\n+        for (final Metric metric : metrics) {\n+            assertThat(\"Metric:'\" + metric.metricName() + \"' must be not null\", metric.metricValue(), is(notNullValue()));\n         }\n     }\n-}\n+\n+    private void verifyThatBytesWrittenTotalIncreases(final KafkaStreams kafkaStreams,\n+                                                      final String metricsScope) throws InterruptedException {\n+        final List<Metric> metric = getRocksDBMetrics(kafkaStreams, metricsScope).stream()\n+            .filter(m -> BYTES_WRITTEN_TOTAL.equals(m.metricName().name()))\n+            .collect(Collectors.toList());\n+        TestUtils.waitForCondition(\n+            () -> (double) metric.get(0).metricValue() > 0,\n+            TIMEOUT,\n+            () -> \"RocksDB metric bytes.written.total did not increase in \" + TIMEOUT + \" ms\"\n+        );\n+    }", "originalCommit": "a652b378d51c3a83b71758d191d439457123b6ba", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY5MDUyMQ==", "url": "https://github.com/apache/kafka/pull/8256#discussion_r389690521", "bodyText": "This is one of the tests that verifies the fix. See my comment in verifyThatBytesWrittenTotalIncreases().", "author": "cadonna", "createdAt": "2020-03-09T13:52:12Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/RocksDBMetricsIntegrationTest.java", "diffHunk": "@@ -142,21 +155,58 @@ public void shouldExposeRocksDBMetricsForSegmentedStateStoreBeforeAndAfterFailur\n         final Properties streamsConfiguration = streamsConfig();\n         IntegrationTestUtils.purgeLocalStreamsState(streamsConfiguration);\n         final StreamsBuilder builder = builderForSegmentedStateStore();\n+        final String metricsScope = \"rocksdb-window-state-id\";\n+\n+        cleanUpStateRunAndVerify(\n+            builder,\n+            streamsConfiguration,\n+            LongDeserializer.class,\n+            LongDeserializer.class,\n+            this::verifyThatRocksDBMetricsAreExposed,\n+            metricsScope\n+        );\n \n         cleanUpStateRunAndVerify(\n             builder,\n             streamsConfiguration,\n             LongDeserializer.class,\n             LongDeserializer.class,\n-            \"rocksdb-window-state-id\"\n+            this::verifyThatRocksDBMetricsAreExposed,\n+            metricsScope\n+        );\n+    }\n+\n+    @Test\n+    public void shouldVerifyThatMetricsGetMeasurementsFromRocksDBForNonSegmentedStateStore() throws Exception {\n+        final Properties streamsConfiguration = streamsConfig();\n+        IntegrationTestUtils.purgeLocalStreamsState(streamsConfiguration);\n+        final StreamsBuilder builder = builderForNonSegmentedStateStore();\n+        final String metricsScope = \"rocksdb-state-id\";\n+\n+        cleanUpStateRunAndVerify(\n+            builder,\n+            streamsConfiguration,\n+            IntegerDeserializer.class,\n+            StringDeserializer.class,\n+            this::verifyThatBytesWrittenTotalIncreases,\n+            metricsScope\n         );\n+    }", "originalCommit": "a652b378d51c3a83b71758d191d439457123b6ba", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY0NTk0MQ==", "url": "https://github.com/apache/kafka/pull/8256#discussion_r390645941", "bodyText": "Do I read this right, that we're doing the exact same thing twice?", "author": "vvcephei", "createdAt": "2020-03-10T22:22:32Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/RocksDBMetricsIntegrationTest.java", "diffHunk": "@@ -142,21 +155,58 @@ public void shouldExposeRocksDBMetricsForSegmentedStateStoreBeforeAndAfterFailur\n         final Properties streamsConfiguration = streamsConfig();\n         IntegrationTestUtils.purgeLocalStreamsState(streamsConfiguration);\n         final StreamsBuilder builder = builderForSegmentedStateStore();\n+        final String metricsScope = \"rocksdb-window-state-id\";\n+\n+        cleanUpStateRunAndVerify(\n+            builder,\n+            streamsConfiguration,\n+            LongDeserializer.class,\n+            LongDeserializer.class,\n+            this::verifyThatRocksDBMetricsAreExposed,\n+            metricsScope\n+        );\n \n         cleanUpStateRunAndVerify(\n             builder,\n             streamsConfiguration,\n             LongDeserializer.class,\n             LongDeserializer.class,\n-            \"rocksdb-window-state-id\"\n+            this::verifyThatRocksDBMetricsAreExposed,\n+            metricsScope\n+        );", "originalCommit": "a652b378d51c3a83b71758d191d439457123b6ba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDgxOTAyOA==", "url": "https://github.com/apache/kafka/pull/8256#discussion_r390819028", "bodyText": "As the test name says shouldExposeRocksDBMetricsForSegmentedStateStoreBeforeAndAfterFailureWithEmptyStateDir() this test verifies that the metrics are still there after a failure and an empty state dir (see https://issues.apache.org/jira/browse/KAFKA-9355).\nI basically simulate a failure by closing the app, wiping out the state, re-starting the app, verifying the exposure of the metrics, and closing the app. That results in doing twice the same thing.", "author": "cadonna", "createdAt": "2020-03-11T08:48:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY0NTk0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE2NjE4OQ==", "url": "https://github.com/apache/kafka/pull/8256#discussion_r391166189", "bodyText": "Oh, I see. Definitely wasn't obvious by reading the test.", "author": "vvcephei", "createdAt": "2020-03-11T18:07:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY0NTk0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTQ5OTkxOQ==", "url": "https://github.com/apache/kafka/pull/8256#discussion_r391499919", "bodyText": "I tried to make the tests clearer.", "author": "cadonna", "createdAt": "2020-03-12T09:39:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY0NTk0MQ=="}], "type": "inlineReview"}, {"oid": "e145830422a24c6802bb1d7cef9e80008025c182", "url": "https://github.com/apache/kafka/commit/e145830422a24c6802bb1d7cef9e80008025c182", "message": "Clarify some tests", "committedDate": "2020-03-12T09:38:38Z", "type": "commit"}]}