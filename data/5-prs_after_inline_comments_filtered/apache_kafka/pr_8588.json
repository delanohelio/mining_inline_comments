{"pr_number": 8588, "pr_title": "KAFKA-6145: KIP-441: Improve assignment balance", "pr_createdAt": "2020-04-30T03:07:33Z", "pr_url": "https://github.com/apache/kafka/pull/8588", "timeline": [{"oid": "084dbfa84f86811f44cc9632e01cd5ca3d787b55", "url": "https://github.com/apache/kafka/commit/084dbfa84f86811f44cc9632e01cd5ca3d787b55", "message": "KAFKA-6145: KIP-441: Balanced active, stateful, and task assignment\n\n* Add verification to ensure assignments are balanced in each way\n* Tweak the assignment algorithm to produced assignments that are\n  balanced in each way", "committedDate": "2020-05-08T03:35:52Z", "type": "commit"}, {"oid": "7773df0f529c89164c43c67ec12a3ce4a44109a7", "url": "https://github.com/apache/kafka/commit/7773df0f529c89164c43c67ec12a3ce4a44109a7", "message": "checkstyle", "committedDate": "2020-05-08T04:53:12Z", "type": "commit"}, {"oid": "7bf19a2eaeb804b91d9ca201340a961f87937606", "url": "https://github.com/apache/kafka/commit/7bf19a2eaeb804b91d9ca201340a961f87937606", "message": "final tweaks", "committedDate": "2020-05-08T17:56:32Z", "type": "commit"}, {"oid": "5201493c4cb1ada01fe75f59b3db65539144794e", "url": "https://github.com/apache/kafka/commit/5201493c4cb1ada01fe75f59b3db65539144794e", "message": "remove rebase error from trunk", "committedDate": "2020-05-08T20:36:55Z", "type": "commit"}, {"oid": "01b2831ef17091761da2ced2c8bacb015df555ac", "url": "https://github.com/apache/kafka/commit/01b2831ef17091761da2ced2c8bacb015df555ac", "message": "tweak balance algorithm", "committedDate": "2020-05-08T21:14:31Z", "type": "commit"}, {"oid": "d70cca8c190b552e7fe7b81afe16731a17ebd034", "url": "https://github.com/apache/kafka/commit/d70cca8c190b552e7fe7b81afe16731a17ebd034", "message": "fix exception wording", "committedDate": "2020-05-08T21:15:32Z", "type": "commit"}, {"oid": "a29a8a078cd03f746b5aa4285d6d874c306dbdca", "url": "https://github.com/apache/kafka/commit/a29a8a078cd03f746b5aa4285d6d874c306dbdca", "message": "better variable name", "committedDate": "2020-05-08T21:23:37Z", "type": "commit"}, {"oid": "cbd08807fb74217207502606cfb86325515077f5", "url": "https://github.com/apache/kafka/commit/cbd08807fb74217207502606cfb86325515077f5", "message": "log message tweak", "committedDate": "2020-05-08T21:47:55Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2MjYzMw==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422362633", "bodyText": "I wound up wanting the other two as well, so I went ahead and added them. Added tests as well.", "author": "vvcephei", "createdAt": "2020-05-08T20:40:05Z", "path": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java", "diffHunk": "@@ -1155,4 +1155,21 @@ private static byte checkRange(final byte i) {\n         }\n         return result;\n     }\n+\n+    @SafeVarargs\n+    public static <E> Set<E> intersection(final Supplier<Set<E>> constructor, final Set<E> first, final Set<E>... set) {", "originalCommit": "5201493c4cb1ada01fe75f59b3db65539144794e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2Mjg4Mg==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422362882", "bodyText": "It's just nice to be able to print them in tests and debugging.", "author": "vvcephei", "createdAt": "2020-05-08T20:40:36Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java", "diffHunk": "@@ -364,5 +364,15 @@ private AssignmentConfigs(final StreamsConfig configs) {\n             this.numStandbyReplicas = numStandbyReplicas;\n             this.probingRebalanceIntervalMs = probingRebalanceIntervalMs;\n         }\n+\n+        @Override\n+        public String toString() {", "originalCommit": "5201493c4cb1ada01fe75f59b3db65539144794e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2MzQwMA==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422363400", "bodyText": "We added this before because we were thinking this algorithm would be pluggable, but now that we made TaskAssignor the pluggable component, we might as well get rid of this interface.", "author": "vvcephei", "createdAt": "2020-05-08T20:41:40Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/BalancedAssignor.java", "diffHunk": "@@ -1,30 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements. See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License. You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.kafka.streams.processor.internals.assignment;\n-\n-import java.util.UUID;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.SortedSet;\n-import org.apache.kafka.streams.processor.TaskId;\n-\n-public interface BalancedAssignor {", "originalCommit": "5201493c4cb1ada01fe75f59b3db65539144794e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2NDc4MQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422364781", "bodyText": "I dropped these collections, but maintained the getter method, which now just computes the union of the active and standby tasks. The reason was some inconsistencies that crept in when I was refactoring the balancing algorithm and messed up the code to maintain both collections consistently. I fixed that bug, but it doesn't seem worth the risk to have to maintain this collection correctly going forward.", "author": "vvcephei", "createdAt": "2020-05-08T20:44:48Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -23,27 +23,30 @@\n import org.slf4j.LoggerFactory;\n \n import java.util.Collection;\n-import java.util.HashMap;\n+import java.util.Comparator;\n import java.util.HashSet;\n import java.util.Map;\n import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import java.util.TreeSet;\n import java.util.UUID;\n \n import static java.util.Collections.emptyMap;\n import static java.util.Collections.unmodifiableMap;\n import static java.util.Collections.unmodifiableSet;\n+import static java.util.Comparator.comparing;\n import static org.apache.kafka.common.utils.Utils.union;\n import static org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.UNKNOWN_OFFSET_SUM;\n \n public class ClientState {\n     private static final Logger LOG = LoggerFactory.getLogger(ClientState.class);\n+    public static final Comparator<TopicPartition> TOPIC_PARTITION_COMPARATOR = comparing(TopicPartition::topic).thenComparing(TopicPartition::partition);\n \n     private final Set<TaskId> activeTasks;\n     private final Set<TaskId> standbyTasks;\n-    private final Set<TaskId> assignedTasks;", "originalCommit": "5201493c4cb1ada01fe75f59b3db65539144794e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2NTczOQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422365739", "bodyText": "I made all these collections sorted just to help readability while debugging. What do you think about keeping them sorted going forward (for the same reason)?", "author": "vvcephei", "createdAt": "2020-05-08T20:46:50Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -56,34 +59,28 @@ public ClientState() {\n     }\n \n     ClientState(final int capacity) {\n-        this(new HashSet<>(),\n-             new HashSet<>(),\n-             new HashSet<>(),\n-             new HashSet<>(),\n-             new HashSet<>(),\n-             new HashSet<>(),\n-             new HashMap<>(),\n-             new HashMap<>(),\n-             new HashMap<>(),\n-             capacity);\n+        activeTasks = new TreeSet<>();", "originalCommit": "5201493c4cb1ada01fe75f59b3db65539144794e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU0ODA5NA==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423548094", "bodyText": "Fine with me.", "author": "cadonna", "createdAt": "2020-05-12T08:17:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2NTczOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2NjM4Mg==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422366382", "bodyText": "Unfortunately, TreeMap doesn't have a constructor that takes both a collection to copy and a comparator.", "author": "vvcephei", "createdAt": "2020-05-08T20:48:10Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -94,75 +91,80 @@ public ClientState(final Set<TaskId> previousActiveTasks,\n                        final Set<TaskId> previousStandbyTasks,\n                        final Map<TaskId, Long> taskLagTotals,\n                        final int capacity) {\n-        activeTasks = new HashSet<>();\n-        standbyTasks = new HashSet<>();\n-        assignedTasks = new HashSet<>();\n-        prevActiveTasks = unmodifiableSet(new HashSet<>(previousActiveTasks));\n-        prevStandbyTasks = unmodifiableSet(new HashSet<>(previousStandbyTasks));\n-        prevAssignedTasks = unmodifiableSet(union(HashSet::new, previousActiveTasks, previousStandbyTasks));\n-        ownedPartitions = emptyMap();\n+        activeTasks = new TreeSet<>();\n+        standbyTasks = new TreeSet<>();\n+        prevActiveTasks = unmodifiableSet(new TreeSet<>(previousActiveTasks));\n+        prevStandbyTasks = unmodifiableSet(new TreeSet<>(previousStandbyTasks));\n+        ownedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n         taskOffsetSums = emptyMap();\n         this.taskLagTotals = unmodifiableMap(taskLagTotals);\n         this.capacity = capacity;\n     }\n \n     public ClientState copy() {\n+        final TreeMap<TopicPartition, String> newOwnedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n+        newOwnedPartitions.putAll(ownedPartitions);", "originalCommit": "5201493c4cb1ada01fe75f59b3db65539144794e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2Njg1Nw==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422366857", "bodyText": "In addition to writing tests to verify that assignments are valid at the cluster level, it seemed wise to at least make sure that we can't create an invalid assignment for a single node.", "author": "vvcephei", "createdAt": "2020-05-08T20:49:16Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -94,75 +91,80 @@ public ClientState(final Set<TaskId> previousActiveTasks,\n                        final Set<TaskId> previousStandbyTasks,\n                        final Map<TaskId, Long> taskLagTotals,\n                        final int capacity) {\n-        activeTasks = new HashSet<>();\n-        standbyTasks = new HashSet<>();\n-        assignedTasks = new HashSet<>();\n-        prevActiveTasks = unmodifiableSet(new HashSet<>(previousActiveTasks));\n-        prevStandbyTasks = unmodifiableSet(new HashSet<>(previousStandbyTasks));\n-        prevAssignedTasks = unmodifiableSet(union(HashSet::new, previousActiveTasks, previousStandbyTasks));\n-        ownedPartitions = emptyMap();\n+        activeTasks = new TreeSet<>();\n+        standbyTasks = new TreeSet<>();\n+        prevActiveTasks = unmodifiableSet(new TreeSet<>(previousActiveTasks));\n+        prevStandbyTasks = unmodifiableSet(new TreeSet<>(previousStandbyTasks));\n+        ownedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n         taskOffsetSums = emptyMap();\n         this.taskLagTotals = unmodifiableMap(taskLagTotals);\n         this.capacity = capacity;\n     }\n \n     public ClientState copy() {\n+        final TreeMap<TopicPartition, String> newOwnedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n+        newOwnedPartitions.putAll(ownedPartitions);\n         return new ClientState(\n-            new HashSet<>(activeTasks),\n-            new HashSet<>(standbyTasks),\n-            new HashSet<>(assignedTasks),\n-            new HashSet<>(prevActiveTasks),\n-            new HashSet<>(prevStandbyTasks),\n-            new HashSet<>(prevAssignedTasks),\n-            new HashMap<>(ownedPartitions),\n-            new HashMap<>(taskOffsetSums),\n-            new HashMap<>(taskLagTotals),\n+            new TreeSet<>(activeTasks),\n+            new TreeSet<>(standbyTasks),\n+            new TreeSet<>(prevActiveTasks),\n+            new TreeSet<>(prevStandbyTasks),\n+            newOwnedPartitions,\n+            new TreeMap<>(taskOffsetSums),\n+            new TreeMap<>(taskLagTotals),\n             capacity);\n     }\n \n     void assignActive(final TaskId task) {\n+        assertNotAssigned(task);", "originalCommit": "5201493c4cb1ada01fe75f59b3db65539144794e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU3NTA3NQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423575075", "bodyText": "req: Please add unit test to verify IllegalStateException.", "author": "cadonna", "createdAt": "2020-05-12T08:58:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2Njg1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2Nzc2NQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422367765", "bodyText": "We previously did our \"task shuffling\" on secondary collections, like a map of client id to list of tasks. But after unifying the balancing logic for both active and standby tasks, it became more sensible just to shuffle tasks directly in the assignment, which necessitates these \"unassign\" methods.", "author": "vvcephei", "createdAt": "2020-05-08T20:51:14Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -94,75 +91,80 @@ public ClientState(final Set<TaskId> previousActiveTasks,\n                        final Set<TaskId> previousStandbyTasks,\n                        final Map<TaskId, Long> taskLagTotals,\n                        final int capacity) {\n-        activeTasks = new HashSet<>();\n-        standbyTasks = new HashSet<>();\n-        assignedTasks = new HashSet<>();\n-        prevActiveTasks = unmodifiableSet(new HashSet<>(previousActiveTasks));\n-        prevStandbyTasks = unmodifiableSet(new HashSet<>(previousStandbyTasks));\n-        prevAssignedTasks = unmodifiableSet(union(HashSet::new, previousActiveTasks, previousStandbyTasks));\n-        ownedPartitions = emptyMap();\n+        activeTasks = new TreeSet<>();\n+        standbyTasks = new TreeSet<>();\n+        prevActiveTasks = unmodifiableSet(new TreeSet<>(previousActiveTasks));\n+        prevStandbyTasks = unmodifiableSet(new TreeSet<>(previousStandbyTasks));\n+        ownedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n         taskOffsetSums = emptyMap();\n         this.taskLagTotals = unmodifiableMap(taskLagTotals);\n         this.capacity = capacity;\n     }\n \n     public ClientState copy() {\n+        final TreeMap<TopicPartition, String> newOwnedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n+        newOwnedPartitions.putAll(ownedPartitions);\n         return new ClientState(\n-            new HashSet<>(activeTasks),\n-            new HashSet<>(standbyTasks),\n-            new HashSet<>(assignedTasks),\n-            new HashSet<>(prevActiveTasks),\n-            new HashSet<>(prevStandbyTasks),\n-            new HashSet<>(prevAssignedTasks),\n-            new HashMap<>(ownedPartitions),\n-            new HashMap<>(taskOffsetSums),\n-            new HashMap<>(taskLagTotals),\n+            new TreeSet<>(activeTasks),\n+            new TreeSet<>(standbyTasks),\n+            new TreeSet<>(prevActiveTasks),\n+            new TreeSet<>(prevStandbyTasks),\n+            newOwnedPartitions,\n+            new TreeMap<>(taskOffsetSums),\n+            new TreeMap<>(taskLagTotals),\n             capacity);\n     }\n \n     void assignActive(final TaskId task) {\n+        assertNotAssigned(task);\n         activeTasks.add(task);\n-        assignedTasks.add(task);\n+    }\n+\n+    void unAssignActive(final TaskId task) {", "originalCommit": "5201493c4cb1ada01fe75f59b3db65539144794e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU3MDM0Ng==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423570346", "bodyText": "super-nit: I would just call it unassignActive.", "author": "cadonna", "createdAt": "2020-05-12T08:51:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2Nzc2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU3NTE1Mg==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423575152", "bodyText": "req: Please add unit test to verify at least IllegalStateException.", "author": "cadonna", "createdAt": "2020-05-12T08:58:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2Nzc2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2ODQxMA==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422368410", "bodyText": "I was briefly mutating these returned collections to implement \"task shuffling\" before I realized what a terrible decision that is. To prevent similar poor choices (mostly by me) in the future, I've made all these return unmodifiable views.", "author": "vvcephei", "createdAt": "2020-05-08T20:52:40Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -94,75 +91,80 @@ public ClientState(final Set<TaskId> previousActiveTasks,\n                        final Set<TaskId> previousStandbyTasks,\n                        final Map<TaskId, Long> taskLagTotals,\n                        final int capacity) {\n-        activeTasks = new HashSet<>();\n-        standbyTasks = new HashSet<>();\n-        assignedTasks = new HashSet<>();\n-        prevActiveTasks = unmodifiableSet(new HashSet<>(previousActiveTasks));\n-        prevStandbyTasks = unmodifiableSet(new HashSet<>(previousStandbyTasks));\n-        prevAssignedTasks = unmodifiableSet(union(HashSet::new, previousActiveTasks, previousStandbyTasks));\n-        ownedPartitions = emptyMap();\n+        activeTasks = new TreeSet<>();\n+        standbyTasks = new TreeSet<>();\n+        prevActiveTasks = unmodifiableSet(new TreeSet<>(previousActiveTasks));\n+        prevStandbyTasks = unmodifiableSet(new TreeSet<>(previousStandbyTasks));\n+        ownedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n         taskOffsetSums = emptyMap();\n         this.taskLagTotals = unmodifiableMap(taskLagTotals);\n         this.capacity = capacity;\n     }\n \n     public ClientState copy() {\n+        final TreeMap<TopicPartition, String> newOwnedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n+        newOwnedPartitions.putAll(ownedPartitions);\n         return new ClientState(\n-            new HashSet<>(activeTasks),\n-            new HashSet<>(standbyTasks),\n-            new HashSet<>(assignedTasks),\n-            new HashSet<>(prevActiveTasks),\n-            new HashSet<>(prevStandbyTasks),\n-            new HashSet<>(prevAssignedTasks),\n-            new HashMap<>(ownedPartitions),\n-            new HashMap<>(taskOffsetSums),\n-            new HashMap<>(taskLagTotals),\n+            new TreeSet<>(activeTasks),\n+            new TreeSet<>(standbyTasks),\n+            new TreeSet<>(prevActiveTasks),\n+            new TreeSet<>(prevStandbyTasks),\n+            newOwnedPartitions,\n+            new TreeMap<>(taskOffsetSums),\n+            new TreeMap<>(taskLagTotals),\n             capacity);\n     }\n \n     void assignActive(final TaskId task) {\n+        assertNotAssigned(task);\n         activeTasks.add(task);\n-        assignedTasks.add(task);\n+    }\n+\n+    void unAssignActive(final TaskId task) {\n+        if (!activeTasks.contains(task)) {\n+            throw new IllegalArgumentException(\"Tried to unassign active task \" + task + \", but it is not currently assigned: \" + this);\n+        }\n+        activeTasks.remove(task);\n     }\n \n     void assignStandby(final TaskId task) {\n+        assertNotAssigned(task);\n         standbyTasks.add(task);\n-        assignedTasks.add(task);\n     }\n \n-    public void assignActiveTasks(final Collection<TaskId> tasks) {\n-        activeTasks.addAll(tasks);\n-        assignedTasks.addAll(tasks);\n+    void unAssignStandby(final TaskId task) {\n+        if (!standbyTasks.contains(task)) {\n+            throw new IllegalArgumentException(\"Tried to unassign standby task \" + task + \", but it is not currently assigned: \" + this);\n+        }\n+        standbyTasks.remove(task);\n     }\n \n-    void assignStandbyTasks(final Collection<TaskId> tasks) {\n-        standbyTasks.addAll(tasks);\n-        assignedTasks.addAll(tasks);\n+    public void assignActiveTasks(final Collection<TaskId> tasks) {\n+        activeTasks.addAll(tasks);\n     }\n \n     public Set<TaskId> activeTasks() {\n-        return activeTasks;\n+        return unmodifiableSet(activeTasks);", "originalCommit": "5201493c4cb1ada01fe75f59b3db65539144794e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM3ODkyOQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422378929", "bodyText": "This was handy for eyeballing balance issues.", "author": "vvcephei", "createdAt": "2020-05-08T21:15:36Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -338,35 +337,37 @@ private void initializeRemainingPrevTasksFromTaskOffsetSums() {\n \n     private void addPreviousActiveTask(final TaskId task) {\n         prevActiveTasks.add(task);\n-        prevAssignedTasks.add(task);\n     }\n \n     private void addPreviousStandbyTask(final TaskId task) {\n         prevStandbyTasks.add(task);\n-        prevAssignedTasks.add(task);\n+    }\n+\n+    private void assertNotAssigned(final TaskId task) {\n+        if (standbyTasks.contains(task) || activeTasks.contains(task)) {\n+            throw new IllegalArgumentException(\"Tried to assign task \" + task + \", but it is already assigned: \" + this);\n+        }\n     }\n \n     @Override\n     public String toString() {\n         return \"[activeTasks: (\" + activeTasks +\n             \") standbyTasks: (\" + standbyTasks +\n-            \") assignedTasks: (\" + assignedTasks +\n             \") prevActiveTasks: (\" + prevActiveTasks +\n             \") prevStandbyTasks: (\" + prevStandbyTasks +\n-            \") prevAssignedTasks: (\" + prevAssignedTasks +\n             \") prevOwnedPartitionsByConsumerId: (\" + ownedPartitions.keySet() +\n             \") changelogOffsetTotalsByTask: (\" + taskOffsetSums.entrySet() +\n             \") capacity: \" + capacity +\n+            \" assigned: \" + (activeTasks.size() + standbyTasks.size()) +", "originalCommit": "d70cca8c190b552e7fe7b81afe16731a17ebd034", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM3OTE4Mg==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422379182", "bodyText": "These didn't really provide much value on top of the active/standby tasks to be worth the noise.", "author": "vvcephei", "createdAt": "2020-05-08T21:16:10Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -338,35 +337,37 @@ private void initializeRemainingPrevTasksFromTaskOffsetSums() {\n \n     private void addPreviousActiveTask(final TaskId task) {\n         prevActiveTasks.add(task);\n-        prevAssignedTasks.add(task);\n     }\n \n     private void addPreviousStandbyTask(final TaskId task) {\n         prevStandbyTasks.add(task);\n-        prevAssignedTasks.add(task);\n+    }\n+\n+    private void assertNotAssigned(final TaskId task) {\n+        if (standbyTasks.contains(task) || activeTasks.contains(task)) {\n+            throw new IllegalArgumentException(\"Tried to assign task \" + task + \", but it is already assigned: \" + this);\n+        }\n     }\n \n     @Override\n     public String toString() {\n         return \"[activeTasks: (\" + activeTasks +\n             \") standbyTasks: (\" + standbyTasks +\n-            \") assignedTasks: (\" + assignedTasks +", "originalCommit": "d70cca8c190b552e7fe7b81afe16731a17ebd034", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM3OTgxNg==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422379816", "bodyText": "I generalized this data structure a little, and the name didn't really make sense anymore, so I went for a more \"abstract\" name that hopefully wraps up what the thing is like.", "author": "vvcephei", "createdAt": "2020-05-08T21:17:43Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ConstrainedPrioritySet.java", "diffHunk": "@@ -16,77 +16,58 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n+import org.apache.kafka.streams.processor.TaskId;\n+\n import java.util.Collection;\n+import java.util.Comparator;\n import java.util.HashSet;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n import java.util.PriorityQueue;\n import java.util.Set;\n import java.util.UUID;\n import java.util.function.BiFunction;\n-import org.apache.kafka.streams.processor.TaskId;\n+import java.util.function.Function;\n \n /**\n  * Wraps a priority queue of clients and returns the next valid candidate(s) based on the current task assignment\n  */\n-class ValidClientsByTaskLoadQueue {\n+class ConstrainedPrioritySet {", "originalCommit": "d70cca8c190b552e7fe7b81afe16731a17ebd034", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4MTQ4Mg==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422381482", "bodyText": "I moved the \"weight\" computation to the caller (was the taskLoad stuff below), so we can use this data structure for more stuff. As a consequence, we don't need the clientStates map in here anymore.", "author": "vvcephei", "createdAt": "2020-05-08T21:21:28Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ConstrainedPrioritySet.java", "diffHunk": "@@ -16,77 +16,58 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n+import org.apache.kafka.streams.processor.TaskId;\n+\n import java.util.Collection;\n+import java.util.Comparator;\n import java.util.HashSet;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n import java.util.PriorityQueue;\n import java.util.Set;\n import java.util.UUID;\n import java.util.function.BiFunction;\n-import org.apache.kafka.streams.processor.TaskId;\n+import java.util.function.Function;\n \n /**\n  * Wraps a priority queue of clients and returns the next valid candidate(s) based on the current task assignment\n  */\n-class ValidClientsByTaskLoadQueue {\n+class ConstrainedPrioritySet {\n \n     private final PriorityQueue<UUID> clientsByTaskLoad;\n-    private final BiFunction<UUID, TaskId, Boolean> validClientCriteria;\n+    private final BiFunction<UUID, TaskId, Boolean> constraint;\n     private final Set<UUID> uniqueClients = new HashSet<>();\n \n-    ValidClientsByTaskLoadQueue(final Map<UUID, ClientState> clientStates,\n-                                final BiFunction<UUID, TaskId, Boolean> validClientCriteria) {\n-        this.validClientCriteria = validClientCriteria;\n-\n-        clientsByTaskLoad = new PriorityQueue<>(\n-            (client, other) -> {\n-                final double clientTaskLoad = clientStates.get(client).taskLoad();\n-                final double otherTaskLoad = clientStates.get(other).taskLoad();\n-                if (clientTaskLoad < otherTaskLoad) {\n-                    return -1;\n-                } else if (clientTaskLoad > otherTaskLoad) {\n-                    return 1;\n-                } else {\n-                    return client.compareTo(other);\n-                }\n-            });\n+    ConstrainedPrioritySet(final BiFunction<UUID, TaskId, Boolean> constraint,\n+                           final Function<UUID, Double> weight) {", "originalCommit": "d70cca8c190b552e7fe7b81afe16731a17ebd034", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4MjMxNw==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422382317", "bodyText": "I discovered by the way that you can compose Comparators!", "author": "vvcephei", "createdAt": "2020-05-08T21:23:32Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ConstrainedPrioritySet.java", "diffHunk": "@@ -16,77 +16,58 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n+import org.apache.kafka.streams.processor.TaskId;\n+\n import java.util.Collection;\n+import java.util.Comparator;\n import java.util.HashSet;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n import java.util.PriorityQueue;\n import java.util.Set;\n import java.util.UUID;\n import java.util.function.BiFunction;\n-import org.apache.kafka.streams.processor.TaskId;\n+import java.util.function.Function;\n \n /**\n  * Wraps a priority queue of clients and returns the next valid candidate(s) based on the current task assignment\n  */\n-class ValidClientsByTaskLoadQueue {\n+class ConstrainedPrioritySet {\n \n     private final PriorityQueue<UUID> clientsByTaskLoad;\n-    private final BiFunction<UUID, TaskId, Boolean> validClientCriteria;\n+    private final BiFunction<UUID, TaskId, Boolean> constraint;\n     private final Set<UUID> uniqueClients = new HashSet<>();\n \n-    ValidClientsByTaskLoadQueue(final Map<UUID, ClientState> clientStates,\n-                                final BiFunction<UUID, TaskId, Boolean> validClientCriteria) {\n-        this.validClientCriteria = validClientCriteria;\n-\n-        clientsByTaskLoad = new PriorityQueue<>(\n-            (client, other) -> {\n-                final double clientTaskLoad = clientStates.get(client).taskLoad();\n-                final double otherTaskLoad = clientStates.get(other).taskLoad();\n-                if (clientTaskLoad < otherTaskLoad) {\n-                    return -1;\n-                } else if (clientTaskLoad > otherTaskLoad) {\n-                    return 1;\n-                } else {\n-                    return client.compareTo(other);\n-                }\n-            });\n+    ConstrainedPrioritySet(final BiFunction<UUID, TaskId, Boolean> constraint,\n+                           final Function<UUID, Double> weight) {\n+        this.constraint = constraint;\n+        clientsByTaskLoad = new PriorityQueue<>(Comparator.comparing(weight).thenComparing(clientId -> clientId));", "originalCommit": "a29a8a078cd03f746b5aa4285d6d874c306dbdca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4NDA3Ng==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422384076", "bodyText": "I added a \"last mile\" constraint so that we can use the same priority queue when computing task movements, but first try to find a caught-up standby to move to and then fall back to any caught-up client. I thought about capturing this with the weight function itself, but it depends on which task you're interested in, so this worked better.\nI also dropped the \"poll N clients\" method. Now, if we need 4 clients, we'll poll 4 times. This actually results in better balancing characteristics when assigning standbys because the relative weights of the clients changes while you're assigning standbys to them, so it's better to poll/assign/offer one at a time.", "author": "vvcephei", "createdAt": "2020-05-08T21:28:06Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ConstrainedPrioritySet.java", "diffHunk": "@@ -16,77 +16,58 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n+import org.apache.kafka.streams.processor.TaskId;\n+\n import java.util.Collection;\n+import java.util.Comparator;\n import java.util.HashSet;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n import java.util.PriorityQueue;\n import java.util.Set;\n import java.util.UUID;\n import java.util.function.BiFunction;\n-import org.apache.kafka.streams.processor.TaskId;\n+import java.util.function.Function;\n \n /**\n  * Wraps a priority queue of clients and returns the next valid candidate(s) based on the current task assignment\n  */\n-class ValidClientsByTaskLoadQueue {\n+class ConstrainedPrioritySet {\n \n     private final PriorityQueue<UUID> clientsByTaskLoad;\n-    private final BiFunction<UUID, TaskId, Boolean> validClientCriteria;\n+    private final BiFunction<UUID, TaskId, Boolean> constraint;\n     private final Set<UUID> uniqueClients = new HashSet<>();\n \n-    ValidClientsByTaskLoadQueue(final Map<UUID, ClientState> clientStates,\n-                                final BiFunction<UUID, TaskId, Boolean> validClientCriteria) {\n-        this.validClientCriteria = validClientCriteria;\n-\n-        clientsByTaskLoad = new PriorityQueue<>(\n-            (client, other) -> {\n-                final double clientTaskLoad = clientStates.get(client).taskLoad();\n-                final double otherTaskLoad = clientStates.get(other).taskLoad();\n-                if (clientTaskLoad < otherTaskLoad) {\n-                    return -1;\n-                } else if (clientTaskLoad > otherTaskLoad) {\n-                    return 1;\n-                } else {\n-                    return client.compareTo(other);\n-                }\n-            });\n+    ConstrainedPrioritySet(final BiFunction<UUID, TaskId, Boolean> constraint,\n+                           final Function<UUID, Double> weight) {\n+        this.constraint = constraint;\n+        clientsByTaskLoad = new PriorityQueue<>(Comparator.comparing(weight).thenComparing(clientId -> clientId));\n     }\n \n     /**\n      * @return the next least loaded client that satisfies the given criteria, or null if none do\n      */\n-    UUID poll(final TaskId task) {\n-        final List<UUID> validClient = poll(task, 1);\n-        return validClient.isEmpty() ? null : validClient.get(0);\n-    }\n-\n-    /**\n-     * @return the next N <= {@code numClientsPerTask} clients in the underlying priority queue that are valid candidates for the given task\n-     */\n-    List<UUID> poll(final TaskId task, final int numClients) {\n-        final List<UUID> nextLeastLoadedValidClients = new LinkedList<>();\n+    UUID poll(final TaskId task, final Function<UUID, Boolean> extraConstraint) {", "originalCommit": "a29a8a078cd03f746b5aa4285d6d874c306dbdca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDcyOTgwNg==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r424729806", "bodyText": "I'm not sure I see how the returned clients could ever be different using \"poll N clients\" vs \"poll N times\". Only the clients which are getting a new task assigned will have their weight changed while in the middle of an N poll, and once we assign this task to that client it no longer meets the criteria so we don't care about it anyway right?\nThe reason for the \"poll N clients\" method was to save on some of the poll-and-reoffer of clients that don't meet the criteria, but I don't think that's really worth worrying over. I'm fine with whatever code is easiest to read -- just want to understand why this affects the balance?", "author": "ableegoldman", "createdAt": "2020-05-13T21:03:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4NDA3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDgyNTY0MQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r424825641", "bodyText": "I was operating more on intuition here. To be honest, I had a suspicion you would call this out, so I probably should have just saved time and taken the time to prove it.\nForgetting about the constraint for a minute, I think that what I had in mind for balance is something like, suppose you have two clients \"C1\" and \"C2\"... C1 has one task and C2 has two. You poll and get C1 and add a task. Now, they both have two.\nIf you add it back and poll again, you might prefer to get C1 back again. Maybe because the \"weight\" function takes into account more than just the task load, or maybe just because of the total order we impose based on clientId, in which C1 < C2. But if you just poll two clients to begin with, then C1 doesn't get a chance to be included for the second poll, you just automatically get C1 and C2.\nIn retrospect, this might be moot in practice, because the only time we actually polled for multiple clients was when assigning standbys, and specifically when we were assigning multiple replicas of the same task, in which case, we know that we cannot consider C1 again for the second poll.\nFrom a computer-sciencey perspective, it doesn't seem like the data structure should be able to make this assumption, though, since it can't know that polling a client also invalidates it for a subsequent poll with the same last-mile predicate.\nSo, even in retrospect, I'm tempted to leave it this way (big surprise there), although I'd acknowledge that the outcome is actually not different in the way that we would use the method.", "author": "vvcephei", "createdAt": "2020-05-14T01:48:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4NDA3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDgzMDc2Mg==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r424830762", "bodyText": "we know that we cannot consider C1 again for the second poll\n\nYep, that's what I was getting at above. I'm totally on board with reducing the number of assumptions, especially as this class becomes more generally used. I was just intrigued by what you said initially and thought \"This actually results in better balancing characteristics when assigning standbys\" meant that you had actually seen a difference in the tests.\nThanks for continuing to improve this class!", "author": "ableegoldman", "createdAt": "2020-05-14T02:08:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4NDA3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4NDQ5Nw==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422384497", "bodyText": "Just a little mild refactoring here.", "author": "vvcephei", "createdAt": "2020-05-08T21:29:07Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ConstrainedPrioritySet.java", "diffHunk": "@@ -16,77 +16,58 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n+import org.apache.kafka.streams.processor.TaskId;\n+\n import java.util.Collection;\n+import java.util.Comparator;\n import java.util.HashSet;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n import java.util.PriorityQueue;\n import java.util.Set;\n import java.util.UUID;\n import java.util.function.BiFunction;\n-import org.apache.kafka.streams.processor.TaskId;\n+import java.util.function.Function;\n \n /**\n  * Wraps a priority queue of clients and returns the next valid candidate(s) based on the current task assignment\n  */\n-class ValidClientsByTaskLoadQueue {\n+class ConstrainedPrioritySet {\n \n     private final PriorityQueue<UUID> clientsByTaskLoad;\n-    private final BiFunction<UUID, TaskId, Boolean> validClientCriteria;\n+    private final BiFunction<UUID, TaskId, Boolean> constraint;\n     private final Set<UUID> uniqueClients = new HashSet<>();\n \n-    ValidClientsByTaskLoadQueue(final Map<UUID, ClientState> clientStates,\n-                                final BiFunction<UUID, TaskId, Boolean> validClientCriteria) {\n-        this.validClientCriteria = validClientCriteria;\n-\n-        clientsByTaskLoad = new PriorityQueue<>(\n-            (client, other) -> {\n-                final double clientTaskLoad = clientStates.get(client).taskLoad();\n-                final double otherTaskLoad = clientStates.get(other).taskLoad();\n-                if (clientTaskLoad < otherTaskLoad) {\n-                    return -1;\n-                } else if (clientTaskLoad > otherTaskLoad) {\n-                    return 1;\n-                } else {\n-                    return client.compareTo(other);\n-                }\n-            });\n+    ConstrainedPrioritySet(final BiFunction<UUID, TaskId, Boolean> constraint,\n+                           final Function<UUID, Double> weight) {\n+        this.constraint = constraint;\n+        clientsByTaskLoad = new PriorityQueue<>(Comparator.comparing(weight).thenComparing(clientId -> clientId));\n     }\n \n     /**\n      * @return the next least loaded client that satisfies the given criteria, or null if none do\n      */\n-    UUID poll(final TaskId task) {\n-        final List<UUID> validClient = poll(task, 1);\n-        return validClient.isEmpty() ? null : validClient.get(0);\n-    }\n-\n-    /**\n-     * @return the next N <= {@code numClientsPerTask} clients in the underlying priority queue that are valid candidates for the given task\n-     */\n-    List<UUID> poll(final TaskId task, final int numClients) {\n-        final List<UUID> nextLeastLoadedValidClients = new LinkedList<>();\n+    UUID poll(final TaskId task, final Function<UUID, Boolean> extraConstraint) {\n         final Set<UUID> invalidPolledClients = new HashSet<>();\n-        while (nextLeastLoadedValidClients.size() < numClients) {\n-            UUID candidateClient;\n-            while (true) {\n-                candidateClient = pollNextClient();\n-                if (candidateClient == null) {\n-                    offerAll(invalidPolledClients);\n-                    return nextLeastLoadedValidClients;\n-                }\n-\n-                if (validClientCriteria.apply(candidateClient, task)) {\n-                    nextLeastLoadedValidClients.add(candidateClient);\n-                    break;\n-                } else {\n-                    invalidPolledClients.add(candidateClient);\n-                }\n+        while (!clientsByTaskLoad.isEmpty()) {\n+            final UUID candidateClient = pollNextClient();\n+            if (constraint.apply(candidateClient, task) && extraConstraint.apply(candidateClient)) {\n+                // then we found the lightest, valid client\n+                offerAll(invalidPolledClients);\n+                return candidateClient;\n+            } else {\n+                // remember this client and try again later\n+                invalidPolledClients.add(candidateClient);\n             }", "originalCommit": "a29a8a078cd03f746b5aa4285d6d874c306dbdca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4NDg4Ng==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422384886", "bodyText": "Kept this as a convenience method when you don't care to specify a last-mile constraint.", "author": "vvcephei", "createdAt": "2020-05-08T21:30:04Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ConstrainedPrioritySet.java", "diffHunk": "@@ -16,77 +16,58 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n+import org.apache.kafka.streams.processor.TaskId;\n+\n import java.util.Collection;\n+import java.util.Comparator;\n import java.util.HashSet;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n import java.util.PriorityQueue;\n import java.util.Set;\n import java.util.UUID;\n import java.util.function.BiFunction;\n-import org.apache.kafka.streams.processor.TaskId;\n+import java.util.function.Function;\n \n /**\n  * Wraps a priority queue of clients and returns the next valid candidate(s) based on the current task assignment\n  */\n-class ValidClientsByTaskLoadQueue {\n+class ConstrainedPrioritySet {\n \n     private final PriorityQueue<UUID> clientsByTaskLoad;\n-    private final BiFunction<UUID, TaskId, Boolean> validClientCriteria;\n+    private final BiFunction<UUID, TaskId, Boolean> constraint;\n     private final Set<UUID> uniqueClients = new HashSet<>();\n \n-    ValidClientsByTaskLoadQueue(final Map<UUID, ClientState> clientStates,\n-                                final BiFunction<UUID, TaskId, Boolean> validClientCriteria) {\n-        this.validClientCriteria = validClientCriteria;\n-\n-        clientsByTaskLoad = new PriorityQueue<>(\n-            (client, other) -> {\n-                final double clientTaskLoad = clientStates.get(client).taskLoad();\n-                final double otherTaskLoad = clientStates.get(other).taskLoad();\n-                if (clientTaskLoad < otherTaskLoad) {\n-                    return -1;\n-                } else if (clientTaskLoad > otherTaskLoad) {\n-                    return 1;\n-                } else {\n-                    return client.compareTo(other);\n-                }\n-            });\n+    ConstrainedPrioritySet(final BiFunction<UUID, TaskId, Boolean> constraint,\n+                           final Function<UUID, Double> weight) {\n+        this.constraint = constraint;\n+        clientsByTaskLoad = new PriorityQueue<>(Comparator.comparing(weight).thenComparing(clientId -> clientId));\n     }\n \n     /**\n      * @return the next least loaded client that satisfies the given criteria, or null if none do\n      */\n-    UUID poll(final TaskId task) {\n-        final List<UUID> validClient = poll(task, 1);\n-        return validClient.isEmpty() ? null : validClient.get(0);\n-    }\n-\n-    /**\n-     * @return the next N <= {@code numClientsPerTask} clients in the underlying priority queue that are valid candidates for the given task\n-     */\n-    List<UUID> poll(final TaskId task, final int numClients) {\n-        final List<UUID> nextLeastLoadedValidClients = new LinkedList<>();\n+    UUID poll(final TaskId task, final Function<UUID, Boolean> extraConstraint) {\n         final Set<UUID> invalidPolledClients = new HashSet<>();\n-        while (nextLeastLoadedValidClients.size() < numClients) {\n-            UUID candidateClient;\n-            while (true) {\n-                candidateClient = pollNextClient();\n-                if (candidateClient == null) {\n-                    offerAll(invalidPolledClients);\n-                    return nextLeastLoadedValidClients;\n-                }\n-\n-                if (validClientCriteria.apply(candidateClient, task)) {\n-                    nextLeastLoadedValidClients.add(candidateClient);\n-                    break;\n-                } else {\n-                    invalidPolledClients.add(candidateClient);\n-                }\n+        while (!clientsByTaskLoad.isEmpty()) {\n+            final UUID candidateClient = pollNextClient();\n+            if (constraint.apply(candidateClient, task) && extraConstraint.apply(candidateClient)) {\n+                // then we found the lightest, valid client\n+                offerAll(invalidPolledClients);\n+                return candidateClient;\n+            } else {\n+                // remember this client and try again later\n+                invalidPolledClients.add(candidateClient);\n             }\n         }\n+        // we tried all the clients, and none met the constraint (or there are no clients)\n         offerAll(invalidPolledClients);\n-        return nextLeastLoadedValidClients;\n+        return null;\n+    }\n+\n+    /**\n+     * @return the next least loaded client that satisfies the given criteria, or null if none do\n+     */\n+    UUID poll(final TaskId task) {\n+        return poll(task, client -> true);", "originalCommit": "a29a8a078cd03f746b5aa4285d6d874c306dbdca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4NTg2Mg==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422385862", "bodyText": "This was a lurking bug. We should never call this method if there's nothing in the queue, but we actually were in some cases before. This should have been an exception, since you're not supposed to pass null to Set#remove, but it turns out that we were using a HashSet, which just happens to actually accept a null to remove (and just ignore it).\nAnyway, Queue#remove should be used if you expect the queue to contain an element (it guarantees no null return but throws a NoSuchElementException if it's empty).", "author": "vvcephei", "createdAt": "2020-05-08T21:32:40Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ConstrainedPrioritySet.java", "diffHunk": "@@ -105,7 +86,7 @@ void offer(final UUID client) {\n     }\n \n     private UUID pollNextClient() {\n-        final UUID client = clientsByTaskLoad.poll();\n+        final UUID client = clientsByTaskLoad.remove();\n         uniqueClients.remove(client);\n         return client;", "originalCommit": "a29a8a078cd03f746b5aa4285d6d874c306dbdca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4NjQ2NQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422386465", "bodyText": "I just inlined this into the HighAvailabilityTaskAssignor. After refactorings, we'd wound up in a weird place where standby and stateless assignment logic was in HATA, but active assignment logic was here. Plus, I needed the balance method for standby assignment as well, so putting them all in one place allows for reuse.\nYou'll note that I moved all the tests for this class into the HATATest. Conveniently, if you request no standbys and no warmups, and also only provide stateful tasks, you effectively only exercise the active stateful task assignment logic, so we can still make the same assertions.", "author": "vvcephei", "createdAt": "2020-05-08T21:34:22Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/DefaultBalancedAssignor.java", "diffHunk": "@@ -1,86 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements. See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License. You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.kafka.streams.processor.internals.assignment;\n-\n-import java.util.UUID;\n-import java.util.ArrayList;\n-import java.util.HashMap;\n-import java.util.Iterator;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.SortedSet;\n-import org.apache.kafka.streams.processor.TaskId;\n-\n-public class DefaultBalancedAssignor implements BalancedAssignor {", "originalCommit": "a29a8a078cd03f746b5aa4285d6d874c306dbdca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4NzEyNA==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422387124", "bodyText": "It was subtly misnamed.", "author": "vvcephei", "createdAt": "2020-05-08T21:36:15Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/FallbackPriorTaskAssignor.java", "diffHunk": "@@ -41,9 +41,9 @@ public FallbackPriorTaskAssignor() {\n     @Override\n     public boolean assign(final Map<UUID, ClientState> clients,\n                           final Set<TaskId> allTaskIds,\n-                          final Set<TaskId> standbyTaskIds,\n+                          final Set<TaskId> statefulTaskIds,", "originalCommit": "a29a8a078cd03f746b5aa4285d6d874c306dbdca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4ODA1MQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422388051", "bodyText": "These were all fields before because we used to partially set things up in the constructor. Now that we have to use the 0-arg constructor, and we pass everything in the method call, it's better to just use local fields. The actual reason I changed it was that it became confusing to figure out what references what, which is now clear because everything is passed explicitly.", "author": "vvcephei", "createdAt": "2020-05-08T21:38:35Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;", "originalCommit": "a29a8a078cd03f746b5aa4285d6d874c306dbdca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4ODQ1Mw==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422388453", "bodyText": "This used to do the active assignment AND the movements, but now it only does the active assignment (and balance it).", "author": "vvcephei", "createdAt": "2020-05-08T21:39:43Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);", "originalCommit": "a29a8a078cd03f746b5aa4285d6d874c306dbdca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM4ODYzNQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422388635", "bodyText": "Then, we do the standby assignment (and balance it).\nThat last bit was the thing that set off most of these other refactors. We had problems with the ultimate assignment balance because we can't guarantee to provide a balanced total assignment (including active and standby) analytically, due to unfortunate numerical relationships between the number of instances, number of standbys, and number of partitions of each task.\nSo, it's better to just apply the same optimization algorithm to shuffle tasks around until they're balanced.", "author": "vvcephei", "createdAt": "2020-05-08T21:40:12Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );", "originalCommit": "a29a8a078cd03f746b5aa4285d6d874c306dbdca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM5MDk1NQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422390955", "bodyText": "Finally, we compute movements. Since we're now starting from a nicely balanced active and standby assignment, the warmup assignment is pretty straightforward, and limited in the amount of skew it can introduce (by the max warmups config).\nIt would have been especially tricky to balance the standbys after the warmup assignments, since we couldn't know which ones needed to stay put and which could move.", "author": "vvcephei", "createdAt": "2020-05-08T21:46:21Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );", "originalCommit": "a29a8a078cd03f746b5aa4285d6d874c306dbdca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM5MTgzMA==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422391830", "bodyText": "The algorithm is the same as before, although I refactored the initial loop a little to avoid the break.", "author": "vvcephei", "createdAt": "2020-05-08T21:48:48Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );\n \n-        assignStatelessActiveTasks();\n+        assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n                      \" with \" +\n                      (probingRebalanceNeeded ? \"\" : \"no\") +\n                      \" followup probing rebalance.\");\n+\n         return probingRebalanceNeeded;\n     }\n \n-    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment = new DefaultBalancedAssignor().assign(\n-            sortedClients,\n-            statefulTasks,\n-            clientsToNumberOfThreads\n-        );\n+    private static void assignActiveStatefulTasks(final SortedMap<UUID, ClientState> clientStates,", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM5MjIzNA==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422392234", "bodyText": "This is where we repeatedly poll one, rather than polling a bunch at once.", "author": "vvcephei", "createdAt": "2020-05-08T21:49:55Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );\n \n-        assignStatelessActiveTasks();\n+        assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n                      \" with \" +\n                      (probingRebalanceNeeded ? \"\" : \"no\") +\n                      \" followup probing rebalance.\");\n+\n         return probingRebalanceNeeded;\n     }\n \n-    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment = new DefaultBalancedAssignor().assign(\n-            sortedClients,\n-            statefulTasks,\n-            clientsToNumberOfThreads\n-        );\n+    private static void assignActiveStatefulTasks(final SortedMap<UUID, ClientState> clientStates,\n+                                                  final SortedSet<TaskId> statefulTasks) {\n+        Iterator<ClientState> clientStateIterator = null;\n+        for (final TaskId task : statefulTasks) {\n+            if (clientStateIterator == null || !clientStateIterator.hasNext()) {\n+                clientStateIterator = clientStates.values().iterator();\n+            }\n+            clientStateIterator.next().assignActive(task);\n+        }\n \n-        return assignTaskMovements(\n-            statefulActiveTaskAssignment,\n-            tasksToCaughtUpClients,\n+        balanceTasksOverThreads(\n             clientStates,\n-            tasksToRemainingStandbys,\n-            configs.maxWarmupReplicas\n+            ClientState::activeTasks,\n+            ClientState::unAssignActive,\n+            ClientState::assignActive\n         );\n     }\n \n-    private void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n+    private static void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                                  final TreeMap<UUID, ClientState> clientStates,\n+                                                  final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates,\n+                                                  final int numStandbyReplicas) {\n+        final ConstrainedPrioritySet standbyTaskClientsByTaskLoad = new ConstrainedPrioritySet(\n+            (client, task) -> !clientStates.get(client).assignedTasks().contains(task),\n+            client -> clientStates.get(client).taskLoad()\n         );\n         standbyTaskClientsByTaskLoad.offerAll(clientStates.keySet());\n \n         for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n-            final int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n-            final List<UUID> clients = standbyTaskClientsByTaskLoad.poll(task, numRemainingStandbys);\n-            for (final UUID client : clients) {\n+            int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n+            while (numRemainingStandbys > 0) {\n+                final UUID client = standbyTaskClientsByTaskLoad.poll(task);", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM5MjYwNQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422392605", "bodyText": "This is the key fix for the balance problem. We actually balance the standby tasks after we assign them.", "author": "vvcephei", "createdAt": "2020-05-08T21:51:02Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );\n \n-        assignStatelessActiveTasks();\n+        assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n                      \" with \" +\n                      (probingRebalanceNeeded ? \"\" : \"no\") +\n                      \" followup probing rebalance.\");\n+\n         return probingRebalanceNeeded;\n     }\n \n-    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment = new DefaultBalancedAssignor().assign(\n-            sortedClients,\n-            statefulTasks,\n-            clientsToNumberOfThreads\n-        );\n+    private static void assignActiveStatefulTasks(final SortedMap<UUID, ClientState> clientStates,\n+                                                  final SortedSet<TaskId> statefulTasks) {\n+        Iterator<ClientState> clientStateIterator = null;\n+        for (final TaskId task : statefulTasks) {\n+            if (clientStateIterator == null || !clientStateIterator.hasNext()) {\n+                clientStateIterator = clientStates.values().iterator();\n+            }\n+            clientStateIterator.next().assignActive(task);\n+        }\n \n-        return assignTaskMovements(\n-            statefulActiveTaskAssignment,\n-            tasksToCaughtUpClients,\n+        balanceTasksOverThreads(\n             clientStates,\n-            tasksToRemainingStandbys,\n-            configs.maxWarmupReplicas\n+            ClientState::activeTasks,\n+            ClientState::unAssignActive,\n+            ClientState::assignActive\n         );\n     }\n \n-    private void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n+    private static void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                                  final TreeMap<UUID, ClientState> clientStates,\n+                                                  final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates,\n+                                                  final int numStandbyReplicas) {\n+        final ConstrainedPrioritySet standbyTaskClientsByTaskLoad = new ConstrainedPrioritySet(\n+            (client, task) -> !clientStates.get(client).assignedTasks().contains(task),\n+            client -> clientStates.get(client).taskLoad()\n         );\n         standbyTaskClientsByTaskLoad.offerAll(clientStates.keySet());\n \n         for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n-            final int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n-            final List<UUID> clients = standbyTaskClientsByTaskLoad.poll(task, numRemainingStandbys);\n-            for (final UUID client : clients) {\n+            int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n+            while (numRemainingStandbys > 0) {\n+                final UUID client = standbyTaskClientsByTaskLoad.poll(task);\n+                if (client == null) {\n+                    break;\n+                }\n                 clientStates.get(client).assignStandby(task);\n+                numRemainingStandbys--;\n+                standbyTaskClientsByTaskLoad.offer(client);\n             }\n-            standbyTaskClientsByTaskLoad.offerAll(clients);\n \n-            final int numStandbysAssigned = clients.size();\n-            if (numStandbysAssigned < numRemainingStandbys) {\n+            if (numRemainingStandbys > 0) {\n                 log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n                              \"There is not enough available capacity. You should \" +\n                              \"increase the number of threads and/or application instances \" +\n                              \"to maintain the requested number of standby replicas.\",\n-                         numRemainingStandbys - numStandbysAssigned, configs.numStandbyReplicas, task);\n+                         numRemainingStandbys, numStandbyReplicas, task);\n             }\n         }\n-    }\n \n-    private void assignStatelessActiveTasks() {\n-        final ValidClientsByTaskLoadQueue statelessActiveTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n+        balanceTasksOverThreads(\n             clientStates,\n-            (client, task) -> true\n+            ClientState::standbyTasks,\n+            ClientState::unAssignStandby,\n+            ClientState::assignStandby\n+        );", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM5MzEzMg==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422393132", "bodyText": "I had to make a few additions to the balance algorithm to adapt it to standbys. With actives, we know the tasks are unique, so we can just move them anywhere, but with standbys, there are more constraints, so now we loop over the tasks and only make legal movements. It nets out the same for active tasks.", "author": "vvcephei", "createdAt": "2020-05-08T21:52:38Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );\n \n-        assignStatelessActiveTasks();\n+        assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n                      \" with \" +\n                      (probingRebalanceNeeded ? \"\" : \"no\") +\n                      \" followup probing rebalance.\");\n+\n         return probingRebalanceNeeded;\n     }\n \n-    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment = new DefaultBalancedAssignor().assign(\n-            sortedClients,\n-            statefulTasks,\n-            clientsToNumberOfThreads\n-        );\n+    private static void assignActiveStatefulTasks(final SortedMap<UUID, ClientState> clientStates,\n+                                                  final SortedSet<TaskId> statefulTasks) {\n+        Iterator<ClientState> clientStateIterator = null;\n+        for (final TaskId task : statefulTasks) {\n+            if (clientStateIterator == null || !clientStateIterator.hasNext()) {\n+                clientStateIterator = clientStates.values().iterator();\n+            }\n+            clientStateIterator.next().assignActive(task);\n+        }\n \n-        return assignTaskMovements(\n-            statefulActiveTaskAssignment,\n-            tasksToCaughtUpClients,\n+        balanceTasksOverThreads(\n             clientStates,\n-            tasksToRemainingStandbys,\n-            configs.maxWarmupReplicas\n+            ClientState::activeTasks,\n+            ClientState::unAssignActive,\n+            ClientState::assignActive\n         );\n     }\n \n-    private void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n+    private static void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                                  final TreeMap<UUID, ClientState> clientStates,\n+                                                  final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates,\n+                                                  final int numStandbyReplicas) {\n+        final ConstrainedPrioritySet standbyTaskClientsByTaskLoad = new ConstrainedPrioritySet(\n+            (client, task) -> !clientStates.get(client).assignedTasks().contains(task),\n+            client -> clientStates.get(client).taskLoad()\n         );\n         standbyTaskClientsByTaskLoad.offerAll(clientStates.keySet());\n \n         for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n-            final int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n-            final List<UUID> clients = standbyTaskClientsByTaskLoad.poll(task, numRemainingStandbys);\n-            for (final UUID client : clients) {\n+            int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n+            while (numRemainingStandbys > 0) {\n+                final UUID client = standbyTaskClientsByTaskLoad.poll(task);\n+                if (client == null) {\n+                    break;\n+                }\n                 clientStates.get(client).assignStandby(task);\n+                numRemainingStandbys--;\n+                standbyTaskClientsByTaskLoad.offer(client);\n             }\n-            standbyTaskClientsByTaskLoad.offerAll(clients);\n \n-            final int numStandbysAssigned = clients.size();\n-            if (numStandbysAssigned < numRemainingStandbys) {\n+            if (numRemainingStandbys > 0) {\n                 log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n                              \"There is not enough available capacity. You should \" +\n                              \"increase the number of threads and/or application instances \" +\n                              \"to maintain the requested number of standby replicas.\",\n-                         numRemainingStandbys - numStandbysAssigned, configs.numStandbyReplicas, task);\n+                         numRemainingStandbys, numStandbyReplicas, task);\n             }\n         }\n-    }\n \n-    private void assignStatelessActiveTasks() {\n-        final ValidClientsByTaskLoadQueue statelessActiveTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n+        balanceTasksOverThreads(\n             clientStates,\n-            (client, task) -> true\n+            ClientState::standbyTasks,\n+            ClientState::unAssignStandby,\n+            ClientState::assignStandby\n+        );\n+    }\n+\n+    private static void balanceTasksOverThreads(final SortedMap<UUID, ClientState> clientStates,\n+                                                final Function<ClientState, Set<TaskId>> currentAssignmentAccessor,\n+                                                final BiConsumer<ClientState, TaskId> taskUnassignor,\n+                                                final BiConsumer<ClientState, TaskId> taskAssignor) {\n+        boolean keepBalancing = true;\n+        while (keepBalancing) {\n+            keepBalancing = false;\n+            for (final Map.Entry<UUID, ClientState> sourceEntry : clientStates.entrySet()) {\n+                final UUID sourceClient = sourceEntry.getKey();\n+                final ClientState sourceClientState = sourceEntry.getValue();\n+\n+                for (final Map.Entry<UUID, ClientState> destinationEntry : clientStates.entrySet()) {\n+                    final UUID destinationClient = destinationEntry.getKey();\n+                    final ClientState destinationClientState = destinationEntry.getValue();\n+                    if (sourceClient.equals(destinationClient)) {\n+                        continue;\n+                    }\n+\n+                    final Set<TaskId> sourceTasks = new TreeSet<>(currentAssignmentAccessor.apply(sourceClientState));\n+                    final Iterator<TaskId> sourceIterator = sourceTasks.iterator();\n+                    while (shouldMoveATask(sourceClientState, destinationClientState) && sourceIterator.hasNext()) {\n+                        final TaskId taskToMove = sourceIterator.next();\n+                        final boolean canMove = !destinationClientState.assignedTasks().contains(taskToMove);", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM5NDUwMg==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422394502", "bodyText": "This is a secondary fix. Once I started measuring the balance we're producing, a few other issues surfaced. so now, rather than just considering whether the source has more load than the destination, we also check to make sure that doing a movement would actually improve the skew, which it doesn't always.\nOne example that popped up was with two nodes both with two threads and five tasks overall. If the first had three, it would give one to the second, which would then give it right back, etc., for an infinite loop. Now, it terminates because we detect that the overall balance is the same whether the first node gives a task to the second or keeps it.", "author": "vvcephei", "createdAt": "2020-05-08T21:56:46Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );\n \n-        assignStatelessActiveTasks();\n+        assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n                      \" with \" +\n                      (probingRebalanceNeeded ? \"\" : \"no\") +\n                      \" followup probing rebalance.\");\n+\n         return probingRebalanceNeeded;\n     }\n \n-    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment = new DefaultBalancedAssignor().assign(\n-            sortedClients,\n-            statefulTasks,\n-            clientsToNumberOfThreads\n-        );\n+    private static void assignActiveStatefulTasks(final SortedMap<UUID, ClientState> clientStates,\n+                                                  final SortedSet<TaskId> statefulTasks) {\n+        Iterator<ClientState> clientStateIterator = null;\n+        for (final TaskId task : statefulTasks) {\n+            if (clientStateIterator == null || !clientStateIterator.hasNext()) {\n+                clientStateIterator = clientStates.values().iterator();\n+            }\n+            clientStateIterator.next().assignActive(task);\n+        }\n \n-        return assignTaskMovements(\n-            statefulActiveTaskAssignment,\n-            tasksToCaughtUpClients,\n+        balanceTasksOverThreads(\n             clientStates,\n-            tasksToRemainingStandbys,\n-            configs.maxWarmupReplicas\n+            ClientState::activeTasks,\n+            ClientState::unAssignActive,\n+            ClientState::assignActive\n         );\n     }\n \n-    private void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n+    private static void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                                  final TreeMap<UUID, ClientState> clientStates,\n+                                                  final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates,\n+                                                  final int numStandbyReplicas) {\n+        final ConstrainedPrioritySet standbyTaskClientsByTaskLoad = new ConstrainedPrioritySet(\n+            (client, task) -> !clientStates.get(client).assignedTasks().contains(task),\n+            client -> clientStates.get(client).taskLoad()\n         );\n         standbyTaskClientsByTaskLoad.offerAll(clientStates.keySet());\n \n         for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n-            final int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n-            final List<UUID> clients = standbyTaskClientsByTaskLoad.poll(task, numRemainingStandbys);\n-            for (final UUID client : clients) {\n+            int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n+            while (numRemainingStandbys > 0) {\n+                final UUID client = standbyTaskClientsByTaskLoad.poll(task);\n+                if (client == null) {\n+                    break;\n+                }\n                 clientStates.get(client).assignStandby(task);\n+                numRemainingStandbys--;\n+                standbyTaskClientsByTaskLoad.offer(client);\n             }\n-            standbyTaskClientsByTaskLoad.offerAll(clients);\n \n-            final int numStandbysAssigned = clients.size();\n-            if (numStandbysAssigned < numRemainingStandbys) {\n+            if (numRemainingStandbys > 0) {\n                 log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n                              \"There is not enough available capacity. You should \" +\n                              \"increase the number of threads and/or application instances \" +\n                              \"to maintain the requested number of standby replicas.\",\n-                         numRemainingStandbys - numStandbysAssigned, configs.numStandbyReplicas, task);\n+                         numRemainingStandbys, numStandbyReplicas, task);\n             }\n         }\n-    }\n \n-    private void assignStatelessActiveTasks() {\n-        final ValidClientsByTaskLoadQueue statelessActiveTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n+        balanceTasksOverThreads(\n             clientStates,\n-            (client, task) -> true\n+            ClientState::standbyTasks,\n+            ClientState::unAssignStandby,\n+            ClientState::assignStandby\n+        );\n+    }\n+\n+    private static void balanceTasksOverThreads(final SortedMap<UUID, ClientState> clientStates,\n+                                                final Function<ClientState, Set<TaskId>> currentAssignmentAccessor,\n+                                                final BiConsumer<ClientState, TaskId> taskUnassignor,\n+                                                final BiConsumer<ClientState, TaskId> taskAssignor) {\n+        boolean keepBalancing = true;\n+        while (keepBalancing) {\n+            keepBalancing = false;\n+            for (final Map.Entry<UUID, ClientState> sourceEntry : clientStates.entrySet()) {\n+                final UUID sourceClient = sourceEntry.getKey();\n+                final ClientState sourceClientState = sourceEntry.getValue();\n+\n+                for (final Map.Entry<UUID, ClientState> destinationEntry : clientStates.entrySet()) {\n+                    final UUID destinationClient = destinationEntry.getKey();\n+                    final ClientState destinationClientState = destinationEntry.getValue();\n+                    if (sourceClient.equals(destinationClient)) {\n+                        continue;\n+                    }\n+\n+                    final Set<TaskId> sourceTasks = new TreeSet<>(currentAssignmentAccessor.apply(sourceClientState));\n+                    final Iterator<TaskId> sourceIterator = sourceTasks.iterator();\n+                    while (shouldMoveATask(sourceClientState, destinationClientState) && sourceIterator.hasNext()) {\n+                        final TaskId taskToMove = sourceIterator.next();\n+                        final boolean canMove = !destinationClientState.assignedTasks().contains(taskToMove);\n+                        if (canMove) {\n+                            taskUnassignor.accept(sourceClientState, taskToMove);\n+                            taskAssignor.accept(destinationClientState, taskToMove);\n+                            keepBalancing = true;\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    private static boolean shouldMoveATask(final ClientState sourceClientState,\n+                                           final ClientState destinationClientState) {\n+        final double assignedTasksPerStreamThreadAtDestination =\n+            1.0 * destinationClientState.assignedTasks().size() / destinationClientState.capacity();\n+        final double assignedTasksPerStreamThreadAtSource =\n+            1.0 * sourceClientState.assignedTasks().size() / sourceClientState.capacity();\n+        final double skew = assignedTasksPerStreamThreadAtSource - assignedTasksPerStreamThreadAtDestination;\n+\n+        if (skew <= 0) {\n+            return false;\n+        }\n+\n+        final double proposedAssignedTasksPerStreamThreadAtDestination =\n+            (destinationClientState.assignedTasks().size() + 1.0) / destinationClientState.capacity();\n+        final double proposedAssignedTasksPerStreamThreadAtSource =\n+            (sourceClientState.assignedTasks().size() - 1.0) / sourceClientState.capacity();\n+        final double proposedSkew = proposedAssignedTasksPerStreamThreadAtSource - proposedAssignedTasksPerStreamThreadAtDestination;\n+\n+        if (proposedSkew < 0) {\n+            // then the move would only create an imbalance in the other direction.\n+            return false;\n+        }\n+        // we should only move a task if doing so would actually improve the skew.\n+        return proposedSkew < skew;\n+    }", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM5NDk4Nw==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422394987", "bodyText": "This was yet another minor balance problem that came up. Stateless tasks should only consider the active task load when computing their balance.", "author": "vvcephei", "createdAt": "2020-05-08T21:58:06Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );\n \n-        assignStatelessActiveTasks();\n+        assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n                      \" with \" +\n                      (probingRebalanceNeeded ? \"\" : \"no\") +\n                      \" followup probing rebalance.\");\n+\n         return probingRebalanceNeeded;\n     }\n \n-    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment = new DefaultBalancedAssignor().assign(\n-            sortedClients,\n-            statefulTasks,\n-            clientsToNumberOfThreads\n-        );\n+    private static void assignActiveStatefulTasks(final SortedMap<UUID, ClientState> clientStates,\n+                                                  final SortedSet<TaskId> statefulTasks) {\n+        Iterator<ClientState> clientStateIterator = null;\n+        for (final TaskId task : statefulTasks) {\n+            if (clientStateIterator == null || !clientStateIterator.hasNext()) {\n+                clientStateIterator = clientStates.values().iterator();\n+            }\n+            clientStateIterator.next().assignActive(task);\n+        }\n \n-        return assignTaskMovements(\n-            statefulActiveTaskAssignment,\n-            tasksToCaughtUpClients,\n+        balanceTasksOverThreads(\n             clientStates,\n-            tasksToRemainingStandbys,\n-            configs.maxWarmupReplicas\n+            ClientState::activeTasks,\n+            ClientState::unAssignActive,\n+            ClientState::assignActive\n         );\n     }\n \n-    private void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n+    private static void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                                  final TreeMap<UUID, ClientState> clientStates,\n+                                                  final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates,\n+                                                  final int numStandbyReplicas) {\n+        final ConstrainedPrioritySet standbyTaskClientsByTaskLoad = new ConstrainedPrioritySet(\n+            (client, task) -> !clientStates.get(client).assignedTasks().contains(task),\n+            client -> clientStates.get(client).taskLoad()\n         );\n         standbyTaskClientsByTaskLoad.offerAll(clientStates.keySet());\n \n         for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n-            final int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n-            final List<UUID> clients = standbyTaskClientsByTaskLoad.poll(task, numRemainingStandbys);\n-            for (final UUID client : clients) {\n+            int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n+            while (numRemainingStandbys > 0) {\n+                final UUID client = standbyTaskClientsByTaskLoad.poll(task);\n+                if (client == null) {\n+                    break;\n+                }\n                 clientStates.get(client).assignStandby(task);\n+                numRemainingStandbys--;\n+                standbyTaskClientsByTaskLoad.offer(client);\n             }\n-            standbyTaskClientsByTaskLoad.offerAll(clients);\n \n-            final int numStandbysAssigned = clients.size();\n-            if (numStandbysAssigned < numRemainingStandbys) {\n+            if (numRemainingStandbys > 0) {\n                 log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n                              \"There is not enough available capacity. You should \" +\n                              \"increase the number of threads and/or application instances \" +\n                              \"to maintain the requested number of standby replicas.\",\n-                         numRemainingStandbys - numStandbysAssigned, configs.numStandbyReplicas, task);\n+                         numRemainingStandbys, numStandbyReplicas, task);\n             }\n         }\n-    }\n \n-    private void assignStatelessActiveTasks() {\n-        final ValidClientsByTaskLoadQueue statelessActiveTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n+        balanceTasksOverThreads(\n             clientStates,\n-            (client, task) -> true\n+            ClientState::standbyTasks,\n+            ClientState::unAssignStandby,\n+            ClientState::assignStandby\n+        );\n+    }\n+\n+    private static void balanceTasksOverThreads(final SortedMap<UUID, ClientState> clientStates,\n+                                                final Function<ClientState, Set<TaskId>> currentAssignmentAccessor,\n+                                                final BiConsumer<ClientState, TaskId> taskUnassignor,\n+                                                final BiConsumer<ClientState, TaskId> taskAssignor) {\n+        boolean keepBalancing = true;\n+        while (keepBalancing) {\n+            keepBalancing = false;\n+            for (final Map.Entry<UUID, ClientState> sourceEntry : clientStates.entrySet()) {\n+                final UUID sourceClient = sourceEntry.getKey();\n+                final ClientState sourceClientState = sourceEntry.getValue();\n+\n+                for (final Map.Entry<UUID, ClientState> destinationEntry : clientStates.entrySet()) {\n+                    final UUID destinationClient = destinationEntry.getKey();\n+                    final ClientState destinationClientState = destinationEntry.getValue();\n+                    if (sourceClient.equals(destinationClient)) {\n+                        continue;\n+                    }\n+\n+                    final Set<TaskId> sourceTasks = new TreeSet<>(currentAssignmentAccessor.apply(sourceClientState));\n+                    final Iterator<TaskId> sourceIterator = sourceTasks.iterator();\n+                    while (shouldMoveATask(sourceClientState, destinationClientState) && sourceIterator.hasNext()) {\n+                        final TaskId taskToMove = sourceIterator.next();\n+                        final boolean canMove = !destinationClientState.assignedTasks().contains(taskToMove);\n+                        if (canMove) {\n+                            taskUnassignor.accept(sourceClientState, taskToMove);\n+                            taskAssignor.accept(destinationClientState, taskToMove);\n+                            keepBalancing = true;\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    private static boolean shouldMoveATask(final ClientState sourceClientState,\n+                                           final ClientState destinationClientState) {\n+        final double assignedTasksPerStreamThreadAtDestination =\n+            1.0 * destinationClientState.assignedTasks().size() / destinationClientState.capacity();\n+        final double assignedTasksPerStreamThreadAtSource =\n+            1.0 * sourceClientState.assignedTasks().size() / sourceClientState.capacity();\n+        final double skew = assignedTasksPerStreamThreadAtSource - assignedTasksPerStreamThreadAtDestination;\n+\n+        if (skew <= 0) {\n+            return false;\n+        }\n+\n+        final double proposedAssignedTasksPerStreamThreadAtDestination =\n+            (destinationClientState.assignedTasks().size() + 1.0) / destinationClientState.capacity();\n+        final double proposedAssignedTasksPerStreamThreadAtSource =\n+            (sourceClientState.assignedTasks().size() - 1.0) / sourceClientState.capacity();\n+        final double proposedSkew = proposedAssignedTasksPerStreamThreadAtSource - proposedAssignedTasksPerStreamThreadAtDestination;\n+\n+        if (proposedSkew < 0) {\n+            // then the move would only create an imbalance in the other direction.\n+            return false;\n+        }\n+        // we should only move a task if doing so would actually improve the skew.\n+        return proposedSkew < skew;\n+    }\n+\n+    private static void assignStatelessActiveTasks(final TreeMap<UUID, ClientState> clientStates,\n+                                                   final Iterable<TaskId> statelessTasks) {\n+        final ConstrainedPrioritySet statelessActiveTaskClientsByTaskLoad = new ConstrainedPrioritySet(\n+            (client, task) -> true,\n+            client -> {\n+                final ClientState clientState = clientStates.get(client);\n+                final double activeTaskLoad = 1.0 * clientState.activeTasks().size() / clientState.capacity();", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM5NTM3MA==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422395370", "bodyText": "Needed these for method references below.", "author": "vvcephei", "createdAt": "2020-05-08T21:59:21Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -40,6 +46,14 @@ private TaskMovement(final TaskId task, final UUID destination, final SortedSet<\n         }\n     }\n \n+    private TaskId task() {\n+        return task;\n+    }\n+\n+    private int numCaughtUpClients() {\n+        return caughtUpClients.size();\n+    }\n+", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM5NTU5MQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422395591", "bodyText": "Here are the method references, and the nifty comparator composition thingy.", "author": "vvcephei", "createdAt": "2020-05-08T22:00:05Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -53,75 +67,94 @@ private static boolean taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(final Task\n     /**\n      * @return whether any warmup replicas were assigned\n      */\n-    static boolean assignTaskMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n-                                       final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n+    static boolean assignTaskMovements(final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n                                        final Map<UUID, ClientState> clientStates,\n-                                       final Map<TaskId, Integer> tasksToRemainingStandbys,\n                                        final int maxWarmupReplicas) {\n-        boolean warmupReplicasAssigned = false;\n+        final BiFunction<UUID, TaskId, Boolean> caughtUpPredicate =\n+            (client, task) -> taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients);\n \n-        final ValidClientsByTaskLoadQueue clientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients)\n+        final ConstrainedPrioritySet clientsByTaskLoad = new ConstrainedPrioritySet(\n+            caughtUpPredicate,\n+            client -> clientStates.get(client).taskLoad()\n         );\n \n-        final SortedSet<TaskMovement> taskMovements = new TreeSet<>(\n-            (movement, other) -> {\n-                final int numCaughtUpClients = movement.caughtUpClients.size();\n-                final int otherNumCaughtUpClients = other.caughtUpClients.size();\n-                if (numCaughtUpClients != otherNumCaughtUpClients) {\n-                    return Integer.compare(numCaughtUpClients, otherNumCaughtUpClients);\n-                } else {\n-                    return movement.task.compareTo(other.task);\n-                }\n-            }\n+        final Queue<TaskMovement> taskMovements = new PriorityQueue<>(\n+            Comparator.comparing(TaskMovement::numCaughtUpClients).thenComparing(TaskMovement::task)", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM5NjA0Nw==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422396047", "bodyText": "This logic has changed a little because now we're dealing with an assignment that has active and stateful tasks in it. But the basic algorithm is the same. Hopefully, the code comments clarify everything.", "author": "vvcephei", "createdAt": "2020-05-08T22:01:23Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -53,75 +67,94 @@ private static boolean taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(final Task\n     /**\n      * @return whether any warmup replicas were assigned\n      */\n-    static boolean assignTaskMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n-                                       final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n+    static boolean assignTaskMovements(final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQwNTQyNg==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422405426", "bodyText": "Here's where we try to re-use existing standbys first. If there's not a good candidate, we just settle for any client below.", "author": "vvcephei", "createdAt": "2020-05-08T22:32:59Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -53,75 +67,94 @@ private static boolean taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(final Task\n     /**\n      * @return whether any warmup replicas were assigned\n      */\n-    static boolean assignTaskMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n-                                       final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n+    static boolean assignTaskMovements(final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n                                        final Map<UUID, ClientState> clientStates,\n-                                       final Map<TaskId, Integer> tasksToRemainingStandbys,\n                                        final int maxWarmupReplicas) {\n-        boolean warmupReplicasAssigned = false;\n+        final BiFunction<UUID, TaskId, Boolean> caughtUpPredicate =\n+            (client, task) -> taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients);\n \n-        final ValidClientsByTaskLoadQueue clientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients)\n+        final ConstrainedPrioritySet clientsByTaskLoad = new ConstrainedPrioritySet(\n+            caughtUpPredicate,\n+            client -> clientStates.get(client).taskLoad()\n         );\n \n-        final SortedSet<TaskMovement> taskMovements = new TreeSet<>(\n-            (movement, other) -> {\n-                final int numCaughtUpClients = movement.caughtUpClients.size();\n-                final int otherNumCaughtUpClients = other.caughtUpClients.size();\n-                if (numCaughtUpClients != otherNumCaughtUpClients) {\n-                    return Integer.compare(numCaughtUpClients, otherNumCaughtUpClients);\n-                } else {\n-                    return movement.task.compareTo(other.task);\n-                }\n-            }\n+        final Queue<TaskMovement> taskMovements = new PriorityQueue<>(\n+            Comparator.comparing(TaskMovement::numCaughtUpClients).thenComparing(TaskMovement::task)\n         );\n \n-        for (final Map.Entry<UUID, List<TaskId>> assignmentEntry : statefulActiveTaskAssignment.entrySet()) {\n-            final UUID client = assignmentEntry.getKey();\n-            final ClientState state = clientStates.get(client);\n-            for (final TaskId task : assignmentEntry.getValue()) {\n-                if (taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients)) {\n-                    state.assignActive(task);\n-                } else {\n-                    final TaskMovement taskMovement = new TaskMovement(task, client, tasksToCaughtUpClients.get(task));\n-                    taskMovements.add(taskMovement);\n+        for (final Map.Entry<UUID, ClientState> clientStateEntry : clientStates.entrySet()) {\n+            final UUID client = clientStateEntry.getKey();\n+            final ClientState state = clientStateEntry.getValue();\n+            for (final TaskId task : state.activeTasks()) {\n+                // if the desired client is not caught up, and there is another client that _is_ caught up, then\n+                // we schedule a movement, so we can move the active task to the caught-up client. We'll try to\n+                // assign a warm-up to the desired client so that we can move it later on.\n+                if (!taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients)) {\n+                    taskMovements.add(new TaskMovement(task, client, tasksToCaughtUpClients.get(task)));\n                 }\n             }\n             clientsByTaskLoad.offer(client);\n         }\n \n+        final boolean movementsNeeded = !taskMovements.isEmpty();\n+\n         final AtomicInteger remainingWarmupReplicas = new AtomicInteger(maxWarmupReplicas);\n         for (final TaskMovement movement : taskMovements) {\n-            final UUID sourceClient = clientsByTaskLoad.poll(movement.task);\n-            if (sourceClient == null) {\n-                throw new IllegalStateException(\"Tried to move task to caught-up client but none exist\");\n-            }\n-\n-            final ClientState sourceClientState = clientStates.get(sourceClient);\n-            sourceClientState.assignActive(movement.task);\n-            clientsByTaskLoad.offer(sourceClient);\n+            final UUID standbySourceClient = clientsByTaskLoad.poll(", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc0OTMyMQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r424749321", "bodyText": "This is a really nice touch \ud83d\udc4d Although without attempting some degree of stickiness in the standby task assignment it seems unlikely to actually find a standby on a caught-up client..", "author": "ableegoldman", "createdAt": "2020-05-13T21:45:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQwNTQyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDgyMDgzNg==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r424820836", "bodyText": "Agreed, it would just be good luck right now, but I figured we might as well capitalize on the luck. I'm planning to follow up pretty soon with the standby stickiness.", "author": "vvcephei", "createdAt": "2020-05-14T01:29:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQwNTQyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQwNjE3NQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422406175", "bodyText": "I'm not sure where this came from. I'll fix it.", "author": "vvcephei", "createdAt": "2020-05-08T22:35:44Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java", "diffHunk": "@@ -632,7 +632,7 @@ public void shouldAllowConcurrentAccesses() throws Exception {\n         ProducerRunnable producerRunnable = new ProducerRunnable(streamThree, inputValues, 1);\n         producerRunnable.run();\n \n-        producerRunnable = new ProducerRunnable(streamConcurrent, inputValues, numIterations - 1);\n+        producerRunnable = new ProducerRunnable(streamConcurrent, inputValues, numIterations);", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQwNjUxNg==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422406516", "bodyText": "I moved these validations from the TaskAssignorConvergence test so that we can also use them to make assertions about the correctness of our assignor implementations without having to expect specific assignments everywhere.", "author": "vvcephei", "createdAt": "2020-05-08T22:37:03Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentTestUtils.java", "diffHunk": "@@ -75,4 +94,303 @@\n     static UUID uuidForInt(final int n) {\n         return new UUID(0, n);\n     }\n+\n+    static void assertValidAssignment(final int numStandbyReplicas,", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxMDMzNw==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422410337", "bodyText": "After wrestling a bit with the balance definitions I proposed earlier, I think it makes sense to define three kinds of balance:\n\noverall stateful tasks per thread (including both active and standby)\noverall active task per thread (including both stateful active and stateless)\ntask-parallelism (so that the number of partitions of the same task is as distributed as possible over the instances)\n\nWe can generally require both stateful and active task balance, but task parallelism may get sacrificed in favor of the other two at times. It's beyond the scope of this assignor to try and optimize these tradeoffs, so we just don't worry too much about task-parallel balance when it's overconstrained.", "author": "vvcephei", "createdAt": "2020-05-08T22:52:23Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentTestUtils.java", "diffHunk": "@@ -75,4 +94,303 @@\n     static UUID uuidForInt(final int n) {\n         return new UUID(0, n);\n     }\n+\n+    static void assertValidAssignment(final int numStandbyReplicas,\n+                                      final Set<TaskId> statefulTasks,\n+                                      final Set<TaskId> statelessTasks,\n+                                      final Map<UUID, ClientState> assignedStates,\n+                                      final StringBuilder failureContext) {\n+        assertValidAssignment(\n+            numStandbyReplicas,\n+            0,\n+            statefulTasks,\n+            statelessTasks,\n+            assignedStates,\n+            failureContext\n+        );\n+    }\n+\n+    static void assertValidAssignment(final int numStandbyReplicas,\n+                                      final int maxWarmupReplicas,\n+                                      final Set<TaskId> statefulTasks,\n+                                      final Set<TaskId> statelessTasks,\n+                                      final Map<UUID, ClientState> assignedStates,\n+                                      final StringBuilder failureContext) {\n+        final Map<TaskId, Set<UUID>> assignments = new TreeMap<>();\n+        for (final TaskId taskId : statefulTasks) {\n+            assignments.put(taskId, new TreeSet<>());\n+        }\n+        for (final TaskId taskId : statelessTasks) {\n+            assignments.put(taskId, new TreeSet<>());\n+        }\n+        for (final Map.Entry<UUID, ClientState> entry : assignedStates.entrySet()) {\n+            validateAndAddActiveAssignments(statefulTasks, statelessTasks, failureContext, assignments, entry);\n+            validateAndAddStandbyAssignments(statefulTasks, statelessTasks, failureContext, assignments, entry);\n+        }\n+\n+        final AtomicInteger remainingWarmups = new AtomicInteger(maxWarmupReplicas);\n+\n+        final TreeMap<TaskId, Set<UUID>> misassigned =\n+            assignments\n+                .entrySet()\n+                .stream()\n+                .filter(entry -> {\n+                    final int expectedActives = 1;\n+                    final boolean isStateless = statelessTasks.contains(entry.getKey());\n+                    final int expectedStandbys = isStateless ? 0 : numStandbyReplicas;\n+                    // We'll never assign even the expected number of standbys if they don't actually fit in the cluster\n+                    final int expectedAssignments = Math.min(\n+                        assignedStates.size(),\n+                        expectedActives + expectedStandbys\n+                    );\n+                    final int actualAssignments = entry.getValue().size();\n+                    if (actualAssignments == expectedAssignments) {\n+                        return false; // not misassigned\n+                    } else {\n+                        if (actualAssignments == expectedAssignments + 1 && remainingWarmups.get() > 0) {\n+                            remainingWarmups.getAndDecrement();\n+                            return false; // it's a warmup, so it's fine\n+                        } else {\n+                            return true; // misassigned\n+                        }\n+                    }\n+                })\n+                .collect(entriesToMap(TreeMap::new));\n+\n+        if (!misassigned.isEmpty()) {\n+            assertThat(\n+                new StringBuilder().append(\"Found some over- or under-assigned tasks in the final assignment with \")\n+                                   .append(numStandbyReplicas)\n+                                   .append(\" and max warmups \")\n+                                   .append(maxWarmupReplicas)\n+                                   .append(\" standby replicas, stateful tasks:\")\n+                                   .append(statefulTasks)\n+                                   .append(\", and stateless tasks:\")\n+                                   .append(statelessTasks)\n+                                   .append(failureContext)\n+                                   .toString(),\n+                misassigned,\n+                is(emptyMap()));\n+        }\n+    }\n+\n+    private static void validateAndAddStandbyAssignments(final Set<TaskId> statefulTasks,\n+                                                         final Set<TaskId> statelessTasks,\n+                                                         final StringBuilder failureContext,\n+                                                         final Map<TaskId, Set<UUID>> assignments,\n+                                                         final Map.Entry<UUID, ClientState> entry) {\n+        for (final TaskId standbyTask : entry.getValue().standbyTasks()) {\n+            if (statelessTasks.contains(standbyTask)) {\n+                throw new AssertionError(\n+                    new StringBuilder().append(\"Found a standby task for stateless task \")\n+                                       .append(standbyTask)\n+                                       .append(\" on client \")\n+                                       .append(entry)\n+                                       .append(\" stateless tasks:\")\n+                                       .append(statelessTasks)\n+                                       .append(failureContext)\n+                                       .toString()\n+                );\n+            } else if (assignments.containsKey(standbyTask)) {\n+                assignments.get(standbyTask).add(entry.getKey());\n+            } else {\n+                throw new AssertionError(\n+                    new StringBuilder().append(\"Found an extra standby task \")\n+                                       .append(standbyTask)\n+                                       .append(\" on client \")\n+                                       .append(entry)\n+                                       .append(\" but expected stateful tasks:\")\n+                                       .append(statefulTasks)\n+                                       .append(failureContext)\n+                                       .toString()\n+                );\n+            }\n+        }\n+    }\n+\n+    private static void validateAndAddActiveAssignments(final Set<TaskId> statefulTasks,\n+                                                        final Set<TaskId> statelessTasks,\n+                                                        final StringBuilder failureContext,\n+                                                        final Map<TaskId, Set<UUID>> assignments,\n+                                                        final Map.Entry<UUID, ClientState> entry) {\n+        for (final TaskId activeTask : entry.getValue().activeTasks()) {\n+            if (assignments.containsKey(activeTask)) {\n+                assignments.get(activeTask).add(entry.getKey());\n+            } else {\n+                throw new AssertionError(\n+                    new StringBuilder().append(\"Found an extra active task \")\n+                                       .append(activeTask)\n+                                       .append(\" on client \")\n+                                       .append(entry)\n+                                       .append(\" but expected stateful tasks:\")\n+                                       .append(statefulTasks)\n+                                       .append(\" and stateless tasks:\")\n+                                       .append(statelessTasks)\n+                                       .append(failureContext)\n+                                       .toString()\n+                );\n+            }\n+        }\n+    }\n+\n+    static void assertBalancedStatefulAssignment(final Set<TaskId> allStatefulTasks,", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxMDc5NQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422410795", "bodyText": "I added a few matchers so that we can get more informative errors when our assertions fail.", "author": "vvcephei", "createdAt": "2020-05-08T22:54:16Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentTestUtils.java", "diffHunk": "@@ -75,4 +94,303 @@\n     static UUID uuidForInt(final int n) {\n         return new UUID(0, n);\n     }\n+\n+    static void assertValidAssignment(final int numStandbyReplicas,\n+                                      final Set<TaskId> statefulTasks,\n+                                      final Set<TaskId> statelessTasks,\n+                                      final Map<UUID, ClientState> assignedStates,\n+                                      final StringBuilder failureContext) {\n+        assertValidAssignment(\n+            numStandbyReplicas,\n+            0,\n+            statefulTasks,\n+            statelessTasks,\n+            assignedStates,\n+            failureContext\n+        );\n+    }\n+\n+    static void assertValidAssignment(final int numStandbyReplicas,\n+                                      final int maxWarmupReplicas,\n+                                      final Set<TaskId> statefulTasks,\n+                                      final Set<TaskId> statelessTasks,\n+                                      final Map<UUID, ClientState> assignedStates,\n+                                      final StringBuilder failureContext) {\n+        final Map<TaskId, Set<UUID>> assignments = new TreeMap<>();\n+        for (final TaskId taskId : statefulTasks) {\n+            assignments.put(taskId, new TreeSet<>());\n+        }\n+        for (final TaskId taskId : statelessTasks) {\n+            assignments.put(taskId, new TreeSet<>());\n+        }\n+        for (final Map.Entry<UUID, ClientState> entry : assignedStates.entrySet()) {\n+            validateAndAddActiveAssignments(statefulTasks, statelessTasks, failureContext, assignments, entry);\n+            validateAndAddStandbyAssignments(statefulTasks, statelessTasks, failureContext, assignments, entry);\n+        }\n+\n+        final AtomicInteger remainingWarmups = new AtomicInteger(maxWarmupReplicas);\n+\n+        final TreeMap<TaskId, Set<UUID>> misassigned =\n+            assignments\n+                .entrySet()\n+                .stream()\n+                .filter(entry -> {\n+                    final int expectedActives = 1;\n+                    final boolean isStateless = statelessTasks.contains(entry.getKey());\n+                    final int expectedStandbys = isStateless ? 0 : numStandbyReplicas;\n+                    // We'll never assign even the expected number of standbys if they don't actually fit in the cluster\n+                    final int expectedAssignments = Math.min(\n+                        assignedStates.size(),\n+                        expectedActives + expectedStandbys\n+                    );\n+                    final int actualAssignments = entry.getValue().size();\n+                    if (actualAssignments == expectedAssignments) {\n+                        return false; // not misassigned\n+                    } else {\n+                        if (actualAssignments == expectedAssignments + 1 && remainingWarmups.get() > 0) {\n+                            remainingWarmups.getAndDecrement();\n+                            return false; // it's a warmup, so it's fine\n+                        } else {\n+                            return true; // misassigned\n+                        }\n+                    }\n+                })\n+                .collect(entriesToMap(TreeMap::new));\n+\n+        if (!misassigned.isEmpty()) {\n+            assertThat(\n+                new StringBuilder().append(\"Found some over- or under-assigned tasks in the final assignment with \")\n+                                   .append(numStandbyReplicas)\n+                                   .append(\" and max warmups \")\n+                                   .append(maxWarmupReplicas)\n+                                   .append(\" standby replicas, stateful tasks:\")\n+                                   .append(statefulTasks)\n+                                   .append(\", and stateless tasks:\")\n+                                   .append(statelessTasks)\n+                                   .append(failureContext)\n+                                   .toString(),\n+                misassigned,\n+                is(emptyMap()));\n+        }\n+    }\n+\n+    private static void validateAndAddStandbyAssignments(final Set<TaskId> statefulTasks,\n+                                                         final Set<TaskId> statelessTasks,\n+                                                         final StringBuilder failureContext,\n+                                                         final Map<TaskId, Set<UUID>> assignments,\n+                                                         final Map.Entry<UUID, ClientState> entry) {\n+        for (final TaskId standbyTask : entry.getValue().standbyTasks()) {\n+            if (statelessTasks.contains(standbyTask)) {\n+                throw new AssertionError(\n+                    new StringBuilder().append(\"Found a standby task for stateless task \")\n+                                       .append(standbyTask)\n+                                       .append(\" on client \")\n+                                       .append(entry)\n+                                       .append(\" stateless tasks:\")\n+                                       .append(statelessTasks)\n+                                       .append(failureContext)\n+                                       .toString()\n+                );\n+            } else if (assignments.containsKey(standbyTask)) {\n+                assignments.get(standbyTask).add(entry.getKey());\n+            } else {\n+                throw new AssertionError(\n+                    new StringBuilder().append(\"Found an extra standby task \")\n+                                       .append(standbyTask)\n+                                       .append(\" on client \")\n+                                       .append(entry)\n+                                       .append(\" but expected stateful tasks:\")\n+                                       .append(statefulTasks)\n+                                       .append(failureContext)\n+                                       .toString()\n+                );\n+            }\n+        }\n+    }\n+\n+    private static void validateAndAddActiveAssignments(final Set<TaskId> statefulTasks,\n+                                                        final Set<TaskId> statelessTasks,\n+                                                        final StringBuilder failureContext,\n+                                                        final Map<TaskId, Set<UUID>> assignments,\n+                                                        final Map.Entry<UUID, ClientState> entry) {\n+        for (final TaskId activeTask : entry.getValue().activeTasks()) {\n+            if (assignments.containsKey(activeTask)) {\n+                assignments.get(activeTask).add(entry.getKey());\n+            } else {\n+                throw new AssertionError(\n+                    new StringBuilder().append(\"Found an extra active task \")\n+                                       .append(activeTask)\n+                                       .append(\" on client \")\n+                                       .append(entry)\n+                                       .append(\" but expected stateful tasks:\")\n+                                       .append(statefulTasks)\n+                                       .append(\" and stateless tasks:\")\n+                                       .append(statelessTasks)\n+                                       .append(failureContext)\n+                                       .toString()\n+                );\n+            }\n+        }\n+    }\n+\n+    static void assertBalancedStatefulAssignment(final Set<TaskId> allStatefulTasks,\n+                                                 final Map<UUID, ClientState> clientStates,\n+                                                 final StringBuilder failureContext) {\n+        double maxStateful = Double.MIN_VALUE;\n+        double minStateful = Double.MAX_VALUE;\n+        for (final ClientState clientState : clientStates.values()) {\n+            final Set<TaskId> statefulTasks =\n+                intersection(HashSet::new, clientState.assignedTasks(), allStatefulTasks);\n+            final double statefulTaskLoad = 1.0 * statefulTasks.size() / clientState.capacity();\n+            maxStateful = Math.max(maxStateful, statefulTaskLoad);\n+            minStateful = Math.min(minStateful, statefulTaskLoad);\n+        }\n+        final double statefulDiff = maxStateful - minStateful;\n+\n+        if (statefulDiff > 1.0) {\n+            final StringBuilder builder = new StringBuilder()\n+                .append(\"detected a stateful assignment balance factor violation: \")\n+                .append(statefulDiff)\n+                .append(\">\")\n+                .append(1.0)\n+                .append(\" in: \");\n+            appendClientStates(builder, clientStates);\n+            fail(builder.append(failureContext).toString());\n+        }\n+    }\n+\n+    static void assertBalancedActiveAssignment(final Map<UUID, ClientState> clientStates,\n+                                               final StringBuilder failureContext) {\n+        double maxActive = Double.MIN_VALUE;\n+        double minActive = Double.MAX_VALUE;\n+        for (final ClientState clientState : clientStates.values()) {\n+            final double activeTaskLoad = 1.0 * clientState.activeTaskCount() / clientState.capacity();\n+            maxActive = Math.max(maxActive, activeTaskLoad);\n+            minActive = Math.min(minActive, activeTaskLoad);\n+        }\n+        final double activeDiff = maxActive - minActive;\n+        if (activeDiff > 1.0) {\n+            final StringBuilder builder = new StringBuilder()\n+                .append(\"detected an active assignment balance factor violation: \")\n+                .append(activeDiff)\n+                .append(\">\")\n+                .append(1.0)\n+                .append(\" in: \");\n+            appendClientStates(builder, clientStates);\n+            fail(builder.append(failureContext).toString());\n+        }\n+    }\n+\n+    static void assertBalancedTasks(final Map<UUID, ClientState> clientStates) {\n+        final TaskSkewReport taskSkewReport = analyzeTaskAssignmentBalance(clientStates);\n+        if (taskSkewReport.totalSkewedTasks() > 0) {\n+            fail(\"Expected a balanced task assignment, but was: \" + taskSkewReport);\n+        }\n+    }\n+\n+    static TaskSkewReport analyzeTaskAssignmentBalance(final Map<UUID, ClientState> clientStates) {\n+        final Function<Integer, Map<UUID, AtomicInteger>> initialClientCounts =\n+            i -> clientStates.keySet().stream().collect(Collectors.toMap(c -> c, c -> new AtomicInteger(0)));\n+\n+        final Map<Integer, Map<UUID, AtomicInteger>> subtopologyToClientsWithPartition = new TreeMap<>();\n+        for (final Map.Entry<UUID, ClientState> entry : clientStates.entrySet()) {\n+            final UUID client = entry.getKey();\n+            final ClientState clientState = entry.getValue();\n+            for (final TaskId task : clientState.activeTasks()) {\n+                final int subtopology = task.topicGroupId;\n+                subtopologyToClientsWithPartition\n+                    .computeIfAbsent(subtopology, initialClientCounts)\n+                    .get(client)\n+                    .incrementAndGet();\n+            }\n+        }\n+\n+        int maxTaskSkew = 0;\n+        final Set<Integer> skewedSubtopologies = new TreeSet<>();\n+\n+        for (final Map.Entry<Integer, Map<UUID, AtomicInteger>> entry : subtopologyToClientsWithPartition.entrySet()) {\n+            final Map<UUID, AtomicInteger> clientsWithPartition = entry.getValue();\n+            int max = Integer.MIN_VALUE;\n+            int min = Integer.MAX_VALUE;\n+            for (final AtomicInteger count : clientsWithPartition.values()) {\n+                max = Math.max(max, count.get());\n+                min = Math.min(min, count.get());\n+            }\n+            final int taskSkew = max - min;\n+            maxTaskSkew = Math.max(maxTaskSkew, taskSkew);\n+            if (taskSkew > 1) {\n+                skewedSubtopologies.add(entry.getKey());\n+            }\n+        }\n+\n+        return new TaskSkewReport(maxTaskSkew, skewedSubtopologies, subtopologyToClientsWithPartition);\n+    }\n+\n+    static Matcher<ClientState> hasActiveTasks(final int taskCount) {", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxMTM0NA==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422411344", "bodyText": "This is the renamed test from the ValidClientPriorityQueue. The assertions are basically the same, but now we don't need to construct a full ClientState map, we can just craft a \"weight function\" for each assertion.", "author": "vvcephei", "createdAt": "2020-05-08T22:56:35Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/ConstrainedPrioritySetTest.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.junit.Test;\n+\n+import java.util.UUID;\n+import java.util.function.BiFunction;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.singleton;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_1;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_2;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_3;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.nullValue;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+public class ConstrainedPrioritySetTest {", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxMTUwMQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422411501", "bodyText": "These tests all migrated into the HATATest.", "author": "vvcephei", "createdAt": "2020-05-08T22:57:22Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/DefaultBalancedAssignorTest.java", "diffHunk": "@@ -1,295 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements. See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License. You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.kafka.streams.processor.internals.assignment;\n-\n-import java.util.UUID;\n-import org.apache.kafka.streams.processor.TaskId;\n-import org.junit.Test;\n-\n-import java.util.Arrays;\n-import java.util.Collections;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.SortedSet;\n-import java.util.TreeSet;\n-\n-import static org.apache.kafka.common.utils.Utils.mkEntry;\n-import static org.apache.kafka.common.utils.Utils.mkMap;\n-import static org.apache.kafka.common.utils.Utils.mkSortedSet;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_0;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_1;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_2;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_1_0;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_1_1;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_1_2;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_2_0;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_2_1;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_2_2;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_1;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_2;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_3;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.Matchers.is;\n-\n-public class DefaultBalancedAssignorTest {\n-    private static final SortedSet<UUID> TWO_CLIENTS = new TreeSet<>(Arrays.asList(UUID_1, UUID_2));\n-    private static final SortedSet<UUID> THREE_CLIENTS = new TreeSet<>(Arrays.asList(UUID_1, UUID_2, UUID_3));\n-\n-    @Test\n-    public void shouldAssignTasksEvenlyOverClientsWhereNumberOfClientsIntegralDivisorOfNumberOfTasks() {", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxMTk3Mg==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422411972", "bodyText": "And here they are! I adjusted the assertions to generally be less sensitive to the specific assignment, since my changes to the balance function had some trivial effects on the assignment, IIRC.", "author": "vvcephei", "createdAt": "2020-05-08T22:59:18Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -73,6 +77,164 @@\n         /*probingRebalanceIntervalMs*/ 60 * 1000L\n     );\n \n+    @Test\n+    public void shouldAssignActiveStatefulTasksEvenlyOverClientsWhereNumberOfClientsIntegralDivisorOfNumberOfTasks() {", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxMjIxNw==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422412217", "bodyText": "Since the computeBalanceFactor method was only used by these tests, I removed both it and the tests.", "author": "vvcephei", "createdAt": "2020-05-08T23:00:22Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -125,89 +296,10 @@ public void shouldComputeNewAssignmentIfActiveTasksWasNotOnCaughtUpClient() {\n         // we'll warm up task 0_0 on client1 because it's first in sorted order,\n         // although this isn't an optimal convergence\n         assertThat(probingRebalanceNeeded, is(true));\n-    }\n-\n-    @Test\n-    public void shouldComputeBalanceFactorAsDifferenceBetweenMostAndLeastLoadedClients() {", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxMzA0OA==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422413048", "bodyText": "I was able to reproduce some more edge cases by not creating exactly 4 partitions per subtopology. Now, we randomly generate the number of partitions for each subtopology. There's a supplier instead of a PRNG because for the specific scenarios, we still want a fixed partition scheme.", "author": "vvcephei", "createdAt": "2020-05-08T23:04:08Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/TaskAssignorConvergenceTest.java", "diffHunk": "@@ -45,36 +46,29 @@\n \n         private static Harness initializeCluster(final int numStatelessTasks,\n                                                  final int numStatefulTasks,\n-                                                 final int numNodes) {\n+                                                 final int numNodes,\n+                                                 final Supplier<Integer> partitionCountSupplier) {\n             int subtopology = 0;\n             final Set<TaskId> statelessTasks = new TreeSet<>();\n-            {\n-                int partition = 0;\n-                for (int i = 0; i < numStatelessTasks; i++) {\n-                    statelessTasks.add(new TaskId(subtopology, partition));\n-                    if (partition == 4) {\n-                        subtopology++;\n-                        partition = 0;\n-                    } else {\n-                        partition++;\n-                    }\n+            int remainingStatelessTasks = numStatelessTasks;\n+            while (remainingStatelessTasks > 0) {\n+                final int partitions = Math.min(remainingStatelessTasks, partitionCountSupplier.get());", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxMzg3Ng==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422413876", "bodyText": "Not sure what I was thinking before. What I wanted was for all the key numbers to be coprime. Of course, the easiest way to do this is to only use prime numbers, but you might notice that 15 is not prime ;) Now it's fixed.", "author": "vvcephei", "createdAt": "2020-05-08T23:07:46Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/TaskAssignorConvergenceTest.java", "diffHunk": "@@ -236,16 +233,17 @@ public void staticAssignmentShouldConvergeWithTheFirstAssignment() {\n                                                                 0,\n                                                                 1000L);\n \n-        final Harness harness = Harness.initializeCluster(1, 1, 1);\n+        final Harness harness = Harness.initializeCluster(1, 1, 1, () -> 1);\n \n         testForConvergence(harness, configs, 1);\n         verifyValidAssignment(0, harness);\n+        verifyBalancedAssignment(harness);\n     }\n \n     @Test\n     public void assignmentShouldConvergeAfterAddingNode() {\n-        final int numStatelessTasks = 15;\n-        final int numStatefulTasks = 13;\n+        final int numStatelessTasks = 7;", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc1ODEyNw==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r424758127", "bodyText": "Well, if you have a set of N prime numbers and one number which isn't, aren't they all still coprime? :P", "author": "ableegoldman", "createdAt": "2020-05-13T22:05:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxMzg3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDgyMjE2Ng==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r424822166", "bodyText": "Right, but I think there's a \"3\" or a \"5\" in there somewhere, maybe in the other test. Anyway, my intent was to make them all prime so I wouldn't have to think to hard about whether they were all coprime. But, in reality, I managed to screw up both.", "author": "vvcephei", "createdAt": "2020-05-14T01:34:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxMzg3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxNTkyOA==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422415928", "bodyText": "The interface of assignTaskMovements has changed a little, since it no longer gets the assignments in a map and then applies them to the clientStates. Now, it just gets pre-assigned clientStates and mutates it. I think I've mostly faithfully reproduced the tests, but there were a few that I couldn't quite follow what they were previously doing.", "author": "vvcephei", "createdAt": "2020-05-08T23:17:10Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovementTest.java", "diffHunk": "@@ -35,262 +44,161 @@\n import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_2;\n import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_3;\n import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.getClientStatesMap;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.hasProperty;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.Matchers.equalTo;\n-import static org.junit.Assert.assertTrue;\n-import static org.junit.Assert.assertFalse;\n-\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.SortedSet;\n-import java.util.UUID;\n-import java.util.stream.Collectors;\n-import org.apache.kafka.streams.processor.TaskId;\n-import org.junit.Test;\n+import static org.hamcrest.Matchers.is;\n \n public class TaskMovementTest {\n-    private final ClientState client1 = new ClientState(1);\n-    private final ClientState client2 = new ClientState(1);\n-    private final ClientState client3 = new ClientState(1);\n-\n-    private final Map<UUID, ClientState> clientStates = getClientStatesMap(client1, client2, client3);\n-\n-    private final Map<UUID, List<TaskId>> emptyWarmupAssignment = mkMap(\n-        mkEntry(UUID_1, EMPTY_TASK_LIST),\n-        mkEntry(UUID_2, EMPTY_TASK_LIST),\n-        mkEntry(UUID_3, EMPTY_TASK_LIST)\n-    );\n-\n     @Test\n     public void shouldAssignTasksToClientsAndReturnFalseWhenAllClientsCaughtUp() {\n         final int maxWarmupReplicas = Integer.MAX_VALUE;\n         final Set<TaskId> allTasks = mkSet(TASK_0_0, TASK_0_1, TASK_0_2, TASK_1_0, TASK_1_1, TASK_1_2);\n \n-        final Map<UUID, List<TaskId>> balancedAssignment = mkMap(\n-            mkEntry(UUID_1, asList(TASK_0_0, TASK_1_0)),\n-            mkEntry(UUID_2, asList(TASK_0_1, TASK_1_1)),\n-            mkEntry(UUID_3, asList(TASK_0_2, TASK_1_2))\n-        );\n-\n         final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients = new HashMap<>();\n         for (final TaskId task : allTasks) {\n             tasksToCaughtUpClients.put(task, mkSortedSet(UUID_1, UUID_2, UUID_3));\n         }\n-        \n-        assertFalse(\n+\n+        final ClientState client1 = getClientStateWithActiveAssignment(asList(TASK_0_0, TASK_1_0));\n+        final ClientState client2 = getClientStateWithActiveAssignment(asList(TASK_0_1, TASK_1_1));\n+        final ClientState client3 = getClientStateWithActiveAssignment(asList(TASK_0_2, TASK_1_2));\n+\n+        assertThat(\n             assignTaskMovements(\n-                balancedAssignment,\n                 tasksToCaughtUpClients,\n-                clientStates,\n-                getMapWithNumStandbys(allTasks, 1),\n-                maxWarmupReplicas)\n+                getClientStatesMap(client1, client2, client3),\n+                maxWarmupReplicas),\n+            is(false)", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxNjI2Mg==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422416262", "bodyText": "I couldn't follow exactly what this test was doing, but it seemed like other tests already verify both balance and constrainedness, so I deleted this one. Not sure if that's the right call.", "author": "vvcephei", "createdAt": "2020-05-08T23:18:43Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovementTest.java", "diffHunk": "@@ -35,262 +44,161 @@\n import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_2;\n import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_3;\n import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.getClientStatesMap;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.hasProperty;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.Matchers.equalTo;\n-import static org.junit.Assert.assertTrue;\n-import static org.junit.Assert.assertFalse;\n-\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.SortedSet;\n-import java.util.UUID;\n-import java.util.stream.Collectors;\n-import org.apache.kafka.streams.processor.TaskId;\n-import org.junit.Test;\n+import static org.hamcrest.Matchers.is;\n \n public class TaskMovementTest {\n-    private final ClientState client1 = new ClientState(1);\n-    private final ClientState client2 = new ClientState(1);\n-    private final ClientState client3 = new ClientState(1);\n-\n-    private final Map<UUID, ClientState> clientStates = getClientStatesMap(client1, client2, client3);\n-\n-    private final Map<UUID, List<TaskId>> emptyWarmupAssignment = mkMap(\n-        mkEntry(UUID_1, EMPTY_TASK_LIST),\n-        mkEntry(UUID_2, EMPTY_TASK_LIST),\n-        mkEntry(UUID_3, EMPTY_TASK_LIST)\n-    );\n-\n     @Test\n     public void shouldAssignTasksToClientsAndReturnFalseWhenAllClientsCaughtUp() {\n         final int maxWarmupReplicas = Integer.MAX_VALUE;\n         final Set<TaskId> allTasks = mkSet(TASK_0_0, TASK_0_1, TASK_0_2, TASK_1_0, TASK_1_1, TASK_1_2);\n \n-        final Map<UUID, List<TaskId>> balancedAssignment = mkMap(\n-            mkEntry(UUID_1, asList(TASK_0_0, TASK_1_0)),\n-            mkEntry(UUID_2, asList(TASK_0_1, TASK_1_1)),\n-            mkEntry(UUID_3, asList(TASK_0_2, TASK_1_2))\n-        );\n-\n         final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients = new HashMap<>();\n         for (final TaskId task : allTasks) {\n             tasksToCaughtUpClients.put(task, mkSortedSet(UUID_1, UUID_2, UUID_3));\n         }\n-        \n-        assertFalse(\n+\n+        final ClientState client1 = getClientStateWithActiveAssignment(asList(TASK_0_0, TASK_1_0));\n+        final ClientState client2 = getClientStateWithActiveAssignment(asList(TASK_0_1, TASK_1_1));\n+        final ClientState client3 = getClientStateWithActiveAssignment(asList(TASK_0_2, TASK_1_2));\n+\n+        assertThat(\n             assignTaskMovements(\n-                balancedAssignment,\n                 tasksToCaughtUpClients,\n-                clientStates,\n-                getMapWithNumStandbys(allTasks, 1),\n-                maxWarmupReplicas)\n+                getClientStatesMap(client1, client2, client3),\n+                maxWarmupReplicas),\n+            is(false)\n         );\n-\n-        verifyClientStateAssignments(balancedAssignment, emptyWarmupAssignment);\n     }\n \n     @Test\n     public void shouldAssignAllTasksToClientsAndReturnFalseIfNoClientsAreCaughtUp() {\n-        final int maxWarmupReplicas = 2;\n-        final Set<TaskId> allTasks = mkSet(TASK_0_0, TASK_0_1, TASK_0_2, TASK_1_0, TASK_1_1, TASK_1_2);\n+        final int maxWarmupReplicas = Integer.MAX_VALUE;\n \n-        final Map<UUID, List<TaskId>> balancedAssignment = mkMap(\n-            mkEntry(UUID_1, asList(TASK_0_0, TASK_1_0)),\n-            mkEntry(UUID_2, asList(TASK_0_1, TASK_1_1)),\n-            mkEntry(UUID_3, asList(TASK_0_2, TASK_1_2))\n-        );\n+        final ClientState client1 = getClientStateWithActiveAssignment(asList(TASK_0_0, TASK_1_0));\n+        final ClientState client2 = getClientStateWithActiveAssignment(asList(TASK_0_1, TASK_1_1));\n+        final ClientState client3 = getClientStateWithActiveAssignment(asList(TASK_0_2, TASK_1_2));\n \n-        assertFalse(\n+        assertThat(\n             assignTaskMovements(\n-                balancedAssignment,\n                 emptyMap(),\n-                clientStates,\n-                getMapWithNumStandbys(allTasks, 1),\n-                maxWarmupReplicas)\n+                getClientStatesMap(client1, client2, client3),\n+                maxWarmupReplicas),\n+            is(false)\n         );\n-        verifyClientStateAssignments(balancedAssignment, emptyWarmupAssignment);\n     }\n \n     @Test\n     public void shouldMoveTasksToCaughtUpClientsAndAssignWarmupReplicasInTheirPlace() {\n         final int maxWarmupReplicas = Integer.MAX_VALUE;\n-        final Set<TaskId> allTasks = mkSet(TASK_0_0, TASK_0_1, TASK_0_2);\n+        final ClientState client1 = getClientStateWithActiveAssignment(singletonList(TASK_0_0));\n+        final ClientState client2 = getClientStateWithActiveAssignment(singletonList(TASK_0_1));\n+        final ClientState client3 = getClientStateWithActiveAssignment(singletonList(TASK_0_2));\n+        final Map<UUID, ClientState> clientStates = getClientStatesMap(client1, client2, client3);\n \n-        final Map<UUID, List<TaskId>> balancedAssignment = mkMap(\n-            mkEntry(UUID_1, singletonList(TASK_0_0)),\n-            mkEntry(UUID_2, singletonList(TASK_0_1)),\n-            mkEntry(UUID_3, singletonList(TASK_0_2))\n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients = mkMap(\n+            mkEntry(TASK_0_0, mkSortedSet(UUID_1)),\n+            mkEntry(TASK_0_1, mkSortedSet(UUID_3)),\n+            mkEntry(TASK_0_2, mkSortedSet(UUID_2))\n         );\n \n-        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients = new HashMap<>();\n-        tasksToCaughtUpClients.put(TASK_0_0, mkSortedSet(UUID_1));\n-        tasksToCaughtUpClients.put(TASK_0_1, mkSortedSet(UUID_3));\n-        tasksToCaughtUpClients.put(TASK_0_2, mkSortedSet(UUID_2));\n-\n-        final Map<UUID, List<TaskId>> expectedActiveTaskAssignment = mkMap(\n-            mkEntry(UUID_1, singletonList(TASK_0_0)),\n-            mkEntry(UUID_2, singletonList(TASK_0_2)),\n-            mkEntry(UUID_3, singletonList(TASK_0_1))\n-        );\n-\n-        final Map<UUID, List<TaskId>> expectedWarmupTaskAssignment = mkMap(\n-            mkEntry(UUID_1, EMPTY_TASK_LIST),\n-            mkEntry(UUID_2, singletonList(TASK_0_1)),\n-            mkEntry(UUID_3, singletonList(TASK_0_2))\n-        );\n-\n-        assertTrue(\n+        assertThat(\n+            \"should have assigned movements\",\n             assignTaskMovements(\n-                balancedAssignment,\n                 tasksToCaughtUpClients,\n                 clientStates,\n-                getMapWithNumStandbys(allTasks, 1),\n-                maxWarmupReplicas)\n-        );\n-        verifyClientStateAssignments(expectedActiveTaskAssignment, expectedWarmupTaskAssignment);\n-    }\n-\n-    @Test\n-    public void shouldProduceBalancedAndStateConstrainedAssignment() {", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDc0NTc5MA==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r424745790", "bodyText": "IIRC this was covering an edge case where it might produce an unbalanced assignment. But it may be moot at this point (and besides, we don't necessarily need to produce a perfectly balanced assignment here)", "author": "ableegoldman", "createdAt": "2020-05-13T21:37:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxNjI2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDgyMTY0MQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r424821641", "bodyText": "Ah, thanks. I guess I'll just leave it out, then.", "author": "vvcephei", "createdAt": "2020-05-14T01:32:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxNjI2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQxNjQ3Nw==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r422416477", "bodyText": "This test is now the ConstrainedPrioritySetTest.", "author": "vvcephei", "createdAt": "2020-05-08T23:19:31Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/ValidClientsByTaskLoadQueueTest.java", "diffHunk": "@@ -1,126 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements. See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License. You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.kafka.streams.processor.internals.assignment;\n-\n-\n-import static java.util.Arrays.asList;\n-import static java.util.Collections.singletonList;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_0;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_1;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_2;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_1_1;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_1_2;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_2_2;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_1;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_2;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_3;\n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.getClientStatesMap;\n-import static org.hamcrest.CoreMatchers.equalTo;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.junit.Assert.assertNull;\n-\n-import java.util.Map;\n-import java.util.UUID;\n-import java.util.function.BiFunction;\n-import org.apache.kafka.streams.processor.TaskId;\n-import org.junit.Test;\n-\n-public class ValidClientsByTaskLoadQueueTest {", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzUzOTk2OA==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423539968", "bodyText": "req: Please add a test for the case where no other set is passed in, since it is not clear from the signature how the method should behave when only one set is passed in.", "author": "cadonna", "createdAt": "2020-05-12T08:04:33Z", "path": "clients/src/test/java/org/apache/kafka/common/utils/UtilsTest.java", "diffHunk": "@@ -597,4 +599,24 @@ public void testUnion() {\n         assertThat(union, is(mkSet(\"a\", \"b\", \"c\", \"d\", \"e\")));\n         assertThat(union.getClass(), equalTo(TreeSet.class));\n     }\n+\n+    @Test\n+    public void testIntersection() {\n+        final Set<String> oneSet = mkSet(\"a\", \"b\", \"c\");\n+        final Set<String> anotherSet = mkSet(\"c\", \"d\", \"e\");\n+        final Set<String> intersection = intersection(TreeSet::new, oneSet, anotherSet);\n+\n+        assertThat(intersection, is(mkSet(\"c\")));\n+        assertThat(intersection.getClass(), equalTo(TreeSet.class));\n+    }\n+", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDAxMjIzMA==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r424012230", "bodyText": "Added a bunch more tests in this spirit. They all passed (whew!). It's a good idea; thanks.", "author": "vvcephei", "createdAt": "2020-05-12T20:26:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzUzOTk2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU0NDc5NQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423544795", "bodyText": "prop: Wouldn't it be better to use the config names as labels instead of the names of the member fields?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        return \"AssignmentConfigs{\" +\n          \n          \n            \n                            \"\\n  acceptableRecoveryLag=\" + acceptableRecoveryLag +\n          \n          \n            \n                            \"\\n  maxWarmupReplicas=\" + maxWarmupReplicas +\n          \n          \n            \n                            \"\\n  numStandbyReplicas=\" + numStandbyReplicas +\n          \n          \n            \n                            \"\\n  probingRebalanceIntervalMs=\" + probingRebalanceIntervalMs +\n          \n          \n            \n                            \"\\n}\";\n          \n          \n            \n                        return \"AssignmentConfigs{\" +\n          \n          \n            \n                            \"\\n \" + StreamsConfig.ACCEPTABLE_RECOVERY_LAG_CONFIG + \" = \" + acceptableRecoveryLag +\n          \n          \n            \n                            \"\\n \" + StreamsConfig.MAX_WARMUP_REPLICAS_CONFIG + \" = \" + maxWarmupReplicas +\n          \n          \n            \n                            \"\\n \" + StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG + \" = \" + numStandbyReplicas +\n          \n          \n            \n                            \"\\n \" + StreamsConfig.PROBING_REBALANCE_INTERVAL_MS_CONFIG + \" = \" + probingRebalanceIntervalMs +\n          \n          \n            \n                            \"\\n}\";", "author": "cadonna", "createdAt": "2020-05-12T08:12:32Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java", "diffHunk": "@@ -364,5 +364,15 @@ private AssignmentConfigs(final StreamsConfig configs) {\n             this.numStandbyReplicas = numStandbyReplicas;\n             this.probingRebalanceIntervalMs = probingRebalanceIntervalMs;\n         }\n+\n+        @Override\n+        public String toString() {\n+            return \"AssignmentConfigs{\" +\n+                \"\\n  acceptableRecoveryLag=\" + acceptableRecoveryLag +\n+                \"\\n  maxWarmupReplicas=\" + maxWarmupReplicas +\n+                \"\\n  numStandbyReplicas=\" + numStandbyReplicas +\n+                \"\\n  probingRebalanceIntervalMs=\" + probingRebalanceIntervalMs +\n+                \"\\n}\";", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzk3NTY0MQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423975641", "bodyText": "Huh, thanks for the suggestion!\nI guess I'm weakly opposed. It seems strange for the toString representation of an object not to represent the field of that object. Not for any good reason, except that that's what I typically see. It's not for \"public\" consumption in any way; it just gets printed in our own unit tests if they fail. In that case, it actually seems more roundabout because we don't use the config property names in those unit tests, we directly instantiate the AssignmentConfig object, which uses the same field names as constructor arguments.\nIt's an interesting idea, but if it's ok with you, I'll just leave it as \"vanilla\" (autogenerated) style for now.", "author": "vvcephei", "createdAt": "2020-05-12T19:20:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU0NDc5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDQ2ODA2MQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r424468061", "bodyText": "Fair enough", "author": "cadonna", "createdAt": "2020-05-13T14:09:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU0NDc5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU3MTAyMg==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423571022", "bodyText": "see above about naming and testing", "author": "cadonna", "createdAt": "2020-05-12T08:52:27Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -94,75 +91,80 @@ public ClientState(final Set<TaskId> previousActiveTasks,\n                        final Set<TaskId> previousStandbyTasks,\n                        final Map<TaskId, Long> taskLagTotals,\n                        final int capacity) {\n-        activeTasks = new HashSet<>();\n-        standbyTasks = new HashSet<>();\n-        assignedTasks = new HashSet<>();\n-        prevActiveTasks = unmodifiableSet(new HashSet<>(previousActiveTasks));\n-        prevStandbyTasks = unmodifiableSet(new HashSet<>(previousStandbyTasks));\n-        prevAssignedTasks = unmodifiableSet(union(HashSet::new, previousActiveTasks, previousStandbyTasks));\n-        ownedPartitions = emptyMap();\n+        activeTasks = new TreeSet<>();\n+        standbyTasks = new TreeSet<>();\n+        prevActiveTasks = unmodifiableSet(new TreeSet<>(previousActiveTasks));\n+        prevStandbyTasks = unmodifiableSet(new TreeSet<>(previousStandbyTasks));\n+        ownedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n         taskOffsetSums = emptyMap();\n         this.taskLagTotals = unmodifiableMap(taskLagTotals);\n         this.capacity = capacity;\n     }\n \n     public ClientState copy() {\n+        final TreeMap<TopicPartition, String> newOwnedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n+        newOwnedPartitions.putAll(ownedPartitions);\n         return new ClientState(\n-            new HashSet<>(activeTasks),\n-            new HashSet<>(standbyTasks),\n-            new HashSet<>(assignedTasks),\n-            new HashSet<>(prevActiveTasks),\n-            new HashSet<>(prevStandbyTasks),\n-            new HashSet<>(prevAssignedTasks),\n-            new HashMap<>(ownedPartitions),\n-            new HashMap<>(taskOffsetSums),\n-            new HashMap<>(taskLagTotals),\n+            new TreeSet<>(activeTasks),\n+            new TreeSet<>(standbyTasks),\n+            new TreeSet<>(prevActiveTasks),\n+            new TreeSet<>(prevStandbyTasks),\n+            newOwnedPartitions,\n+            new TreeMap<>(taskOffsetSums),\n+            new TreeMap<>(taskLagTotals),\n             capacity);\n     }\n \n     void assignActive(final TaskId task) {\n+        assertNotAssigned(task);\n         activeTasks.add(task);\n-        assignedTasks.add(task);\n+    }\n+\n+    void unAssignActive(final TaskId task) {\n+        if (!activeTasks.contains(task)) {\n+            throw new IllegalArgumentException(\"Tried to unassign active task \" + task + \", but it is not currently assigned: \" + this);\n+        }\n+        activeTasks.remove(task);\n     }\n \n     void assignStandby(final TaskId task) {\n+        assertNotAssigned(task);\n         standbyTasks.add(task);\n-        assignedTasks.add(task);\n     }\n \n-    public void assignActiveTasks(final Collection<TaskId> tasks) {\n-        activeTasks.addAll(tasks);\n-        assignedTasks.addAll(tasks);\n+    void unAssignStandby(final TaskId task) {", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU3NTI1MQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423575251", "bodyText": "req: Please add unit test to verify at least IllegalStateException.", "author": "cadonna", "createdAt": "2020-05-12T08:58:51Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -94,75 +91,80 @@ public ClientState(final Set<TaskId> previousActiveTasks,\n                        final Set<TaskId> previousStandbyTasks,\n                        final Map<TaskId, Long> taskLagTotals,\n                        final int capacity) {\n-        activeTasks = new HashSet<>();\n-        standbyTasks = new HashSet<>();\n-        assignedTasks = new HashSet<>();\n-        prevActiveTasks = unmodifiableSet(new HashSet<>(previousActiveTasks));\n-        prevStandbyTasks = unmodifiableSet(new HashSet<>(previousStandbyTasks));\n-        prevAssignedTasks = unmodifiableSet(union(HashSet::new, previousActiveTasks, previousStandbyTasks));\n-        ownedPartitions = emptyMap();\n+        activeTasks = new TreeSet<>();\n+        standbyTasks = new TreeSet<>();\n+        prevActiveTasks = unmodifiableSet(new TreeSet<>(previousActiveTasks));\n+        prevStandbyTasks = unmodifiableSet(new TreeSet<>(previousStandbyTasks));\n+        ownedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n         taskOffsetSums = emptyMap();\n         this.taskLagTotals = unmodifiableMap(taskLagTotals);\n         this.capacity = capacity;\n     }\n \n     public ClientState copy() {\n+        final TreeMap<TopicPartition, String> newOwnedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n+        newOwnedPartitions.putAll(ownedPartitions);\n         return new ClientState(\n-            new HashSet<>(activeTasks),\n-            new HashSet<>(standbyTasks),\n-            new HashSet<>(assignedTasks),\n-            new HashSet<>(prevActiveTasks),\n-            new HashSet<>(prevStandbyTasks),\n-            new HashSet<>(prevAssignedTasks),\n-            new HashMap<>(ownedPartitions),\n-            new HashMap<>(taskOffsetSums),\n-            new HashMap<>(taskLagTotals),\n+            new TreeSet<>(activeTasks),\n+            new TreeSet<>(standbyTasks),\n+            new TreeSet<>(prevActiveTasks),\n+            new TreeSet<>(prevStandbyTasks),\n+            newOwnedPartitions,\n+            new TreeMap<>(taskOffsetSums),\n+            new TreeMap<>(taskLagTotals),\n             capacity);\n     }\n \n     void assignActive(final TaskId task) {\n+        assertNotAssigned(task);\n         activeTasks.add(task);\n-        assignedTasks.add(task);\n+    }\n+\n+    void unAssignActive(final TaskId task) {\n+        if (!activeTasks.contains(task)) {\n+            throw new IllegalArgumentException(\"Tried to unassign active task \" + task + \", but it is not currently assigned: \" + this);\n+        }\n+        activeTasks.remove(task);\n     }\n \n     void assignStandby(final TaskId task) {", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU4NDk3MA==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423584970", "bodyText": "prop: Since we assert that active and stand-by tasks are disjunct, we could avoid to compute the union here and just sum up the sizes of activeTasks and standbyTasks.", "author": "cadonna", "createdAt": "2020-05-12T09:14:03Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -94,75 +91,80 @@ public ClientState(final Set<TaskId> previousActiveTasks,\n                        final Set<TaskId> previousStandbyTasks,\n                        final Map<TaskId, Long> taskLagTotals,\n                        final int capacity) {\n-        activeTasks = new HashSet<>();\n-        standbyTasks = new HashSet<>();\n-        assignedTasks = new HashSet<>();\n-        prevActiveTasks = unmodifiableSet(new HashSet<>(previousActiveTasks));\n-        prevStandbyTasks = unmodifiableSet(new HashSet<>(previousStandbyTasks));\n-        prevAssignedTasks = unmodifiableSet(union(HashSet::new, previousActiveTasks, previousStandbyTasks));\n-        ownedPartitions = emptyMap();\n+        activeTasks = new TreeSet<>();\n+        standbyTasks = new TreeSet<>();\n+        prevActiveTasks = unmodifiableSet(new TreeSet<>(previousActiveTasks));\n+        prevStandbyTasks = unmodifiableSet(new TreeSet<>(previousStandbyTasks));\n+        ownedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n         taskOffsetSums = emptyMap();\n         this.taskLagTotals = unmodifiableMap(taskLagTotals);\n         this.capacity = capacity;\n     }\n \n     public ClientState copy() {\n+        final TreeMap<TopicPartition, String> newOwnedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n+        newOwnedPartitions.putAll(ownedPartitions);\n         return new ClientState(\n-            new HashSet<>(activeTasks),\n-            new HashSet<>(standbyTasks),\n-            new HashSet<>(assignedTasks),\n-            new HashSet<>(prevActiveTasks),\n-            new HashSet<>(prevStandbyTasks),\n-            new HashSet<>(prevAssignedTasks),\n-            new HashMap<>(ownedPartitions),\n-            new HashMap<>(taskOffsetSums),\n-            new HashMap<>(taskLagTotals),\n+            new TreeSet<>(activeTasks),\n+            new TreeSet<>(standbyTasks),\n+            new TreeSet<>(prevActiveTasks),\n+            new TreeSet<>(prevStandbyTasks),\n+            newOwnedPartitions,\n+            new TreeMap<>(taskOffsetSums),\n+            new TreeMap<>(taskLagTotals),\n             capacity);\n     }\n \n     void assignActive(final TaskId task) {\n+        assertNotAssigned(task);\n         activeTasks.add(task);\n-        assignedTasks.add(task);\n+    }\n+\n+    void unAssignActive(final TaskId task) {\n+        if (!activeTasks.contains(task)) {\n+            throw new IllegalArgumentException(\"Tried to unassign active task \" + task + \", but it is not currently assigned: \" + this);\n+        }\n+        activeTasks.remove(task);\n     }\n \n     void assignStandby(final TaskId task) {\n+        assertNotAssigned(task);\n         standbyTasks.add(task);\n-        assignedTasks.add(task);\n     }\n \n-    public void assignActiveTasks(final Collection<TaskId> tasks) {\n-        activeTasks.addAll(tasks);\n-        assignedTasks.addAll(tasks);\n+    void unAssignStandby(final TaskId task) {\n+        if (!standbyTasks.contains(task)) {\n+            throw new IllegalArgumentException(\"Tried to unassign standby task \" + task + \", but it is not currently assigned: \" + this);\n+        }\n+        standbyTasks.remove(task);\n     }\n \n-    void assignStandbyTasks(final Collection<TaskId> tasks) {\n-        standbyTasks.addAll(tasks);\n-        assignedTasks.addAll(tasks);\n+    public void assignActiveTasks(final Collection<TaskId> tasks) {\n+        activeTasks.addAll(tasks);\n     }\n \n     public Set<TaskId> activeTasks() {\n-        return activeTasks;\n+        return unmodifiableSet(activeTasks);\n     }\n \n     public Set<TaskId> standbyTasks() {\n-        return standbyTasks;\n+        return unmodifiableSet(standbyTasks);\n     }\n \n     Set<TaskId> prevActiveTasks() {\n-        return prevActiveTasks;\n+        return unmodifiableSet(prevActiveTasks);\n     }\n \n     Set<TaskId> prevStandbyTasks() {\n-        return prevStandbyTasks;\n+        return unmodifiableSet(prevStandbyTasks);\n     }\n \n     public Map<TopicPartition, String> ownedPartitions() {\n-        return ownedPartitions;\n+        return unmodifiableMap(ownedPartitions);\n     }\n \n-    @SuppressWarnings(\"WeakerAccess\")\n     public int assignedTaskCount() {\n-        return assignedTasks.size();\n+        return assignedTasks().size();", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU4NTkwMw==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423585903", "bodyText": "req: I think here it would be better to call assignedTaskCount() instead of assignedTasks().size().", "author": "cadonna", "createdAt": "2020-05-12T09:15:29Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -254,11 +254,10 @@ long lagFor(final TaskId task) {\n \n     public void removeFromAssignment(final TaskId task) {\n         activeTasks.remove(task);\n-        assignedTasks.remove(task);\n     }\n \n     boolean reachedCapacity() {\n-        return assignedTasks.size() >= capacity;\n+        return assignedTasks().size() >= capacity;", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU5MDY1OQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423590659", "bodyText": "Is this still true? I see assignedTasks() used in HighAvailabilityTaskAssignor.", "author": "cadonna", "createdAt": "2020-05-12T09:22:56Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -338,35 +337,37 @@ private void initializeRemainingPrevTasksFromTaskOffsetSums() {\n \n     private void addPreviousActiveTask(final TaskId task) {\n         prevActiveTasks.add(task);\n-        prevAssignedTasks.add(task);\n     }\n \n     private void addPreviousStandbyTask(final TaskId task) {\n         prevStandbyTasks.add(task);\n-        prevAssignedTasks.add(task);\n+    }\n+\n+    private void assertNotAssigned(final TaskId task) {\n+        if (standbyTasks.contains(task) || activeTasks.contains(task)) {\n+            throw new IllegalArgumentException(\"Tried to assign task \" + task + \", but it is already assigned: \" + this);\n+        }\n     }\n \n     @Override\n     public String toString() {\n         return \"[activeTasks: (\" + activeTasks +\n             \") standbyTasks: (\" + standbyTasks +\n-            \") assignedTasks: (\" + assignedTasks +\n             \") prevActiveTasks: (\" + prevActiveTasks +\n             \") prevStandbyTasks: (\" + prevStandbyTasks +\n-            \") prevAssignedTasks: (\" + prevAssignedTasks +\n             \") prevOwnedPartitionsByConsumerId: (\" + ownedPartitions.keySet() +\n             \") changelogOffsetTotalsByTask: (\" + taskOffsetSums.entrySet() +\n             \") capacity: \" + capacity +\n+            \" assigned: \" + (activeTasks.size() + standbyTasks.size()) +\n             \"]\";\n     }\n \n     // Visible for testing", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkxMzg3OA==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423913878", "bodyText": "Oh, gosh. I'm so close to grepping for this comment and deleting it everywhere.", "author": "vvcephei", "createdAt": "2020-05-12T17:36:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU5MDY1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDQ3NDI0OA==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r424474248", "bodyText": "Do it!!! I've got your back!", "author": "cadonna", "createdAt": "2020-05-13T14:17:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU5MDY1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU5MjMwNg==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423592306", "bodyText": "Just an idea: Is it worth to unit test them as an attempt to ensure that returned sets stay unmodifiable after possible refactorings?", "author": "cadonna", "createdAt": "2020-05-12T09:25:39Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -94,75 +91,80 @@ public ClientState(final Set<TaskId> previousActiveTasks,\n                        final Set<TaskId> previousStandbyTasks,\n                        final Map<TaskId, Long> taskLagTotals,\n                        final int capacity) {\n-        activeTasks = new HashSet<>();\n-        standbyTasks = new HashSet<>();\n-        assignedTasks = new HashSet<>();\n-        prevActiveTasks = unmodifiableSet(new HashSet<>(previousActiveTasks));\n-        prevStandbyTasks = unmodifiableSet(new HashSet<>(previousStandbyTasks));\n-        prevAssignedTasks = unmodifiableSet(union(HashSet::new, previousActiveTasks, previousStandbyTasks));\n-        ownedPartitions = emptyMap();\n+        activeTasks = new TreeSet<>();\n+        standbyTasks = new TreeSet<>();\n+        prevActiveTasks = unmodifiableSet(new TreeSet<>(previousActiveTasks));\n+        prevStandbyTasks = unmodifiableSet(new TreeSet<>(previousStandbyTasks));\n+        ownedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n         taskOffsetSums = emptyMap();\n         this.taskLagTotals = unmodifiableMap(taskLagTotals);\n         this.capacity = capacity;\n     }\n \n     public ClientState copy() {\n+        final TreeMap<TopicPartition, String> newOwnedPartitions = new TreeMap<>(TOPIC_PARTITION_COMPARATOR);\n+        newOwnedPartitions.putAll(ownedPartitions);\n         return new ClientState(\n-            new HashSet<>(activeTasks),\n-            new HashSet<>(standbyTasks),\n-            new HashSet<>(assignedTasks),\n-            new HashSet<>(prevActiveTasks),\n-            new HashSet<>(prevStandbyTasks),\n-            new HashSet<>(prevAssignedTasks),\n-            new HashMap<>(ownedPartitions),\n-            new HashMap<>(taskOffsetSums),\n-            new HashMap<>(taskLagTotals),\n+            new TreeSet<>(activeTasks),\n+            new TreeSet<>(standbyTasks),\n+            new TreeSet<>(prevActiveTasks),\n+            new TreeSet<>(prevStandbyTasks),\n+            newOwnedPartitions,\n+            new TreeMap<>(taskOffsetSums),\n+            new TreeMap<>(taskLagTotals),\n             capacity);\n     }\n \n     void assignActive(final TaskId task) {\n+        assertNotAssigned(task);\n         activeTasks.add(task);\n-        assignedTasks.add(task);\n+    }\n+\n+    void unAssignActive(final TaskId task) {\n+        if (!activeTasks.contains(task)) {\n+            throw new IllegalArgumentException(\"Tried to unassign active task \" + task + \", but it is not currently assigned: \" + this);\n+        }\n+        activeTasks.remove(task);\n     }\n \n     void assignStandby(final TaskId task) {\n+        assertNotAssigned(task);\n         standbyTasks.add(task);\n-        assignedTasks.add(task);\n     }\n \n-    public void assignActiveTasks(final Collection<TaskId> tasks) {\n-        activeTasks.addAll(tasks);\n-        assignedTasks.addAll(tasks);\n+    void unAssignStandby(final TaskId task) {\n+        if (!standbyTasks.contains(task)) {\n+            throw new IllegalArgumentException(\"Tried to unassign standby task \" + task + \", but it is not currently assigned: \" + this);\n+        }\n+        standbyTasks.remove(task);\n     }\n \n-    void assignStandbyTasks(final Collection<TaskId> tasks) {\n-        standbyTasks.addAll(tasks);\n-        assignedTasks.addAll(tasks);\n+    public void assignActiveTasks(final Collection<TaskId> tasks) {\n+        activeTasks.addAll(tasks);\n     }\n \n     public Set<TaskId> activeTasks() {\n-        return activeTasks;\n+        return unmodifiableSet(activeTasks);\n     }\n \n     public Set<TaskId> standbyTasks() {\n-        return standbyTasks;\n+        return unmodifiableSet(standbyTasks);", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzYxNjIyNw==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423616227", "bodyText": "req: Please use assignedTaskCount() instead of assignedTasks().size().", "author": "cadonna", "createdAt": "2020-05-12T10:05:01Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );\n \n-        assignStatelessActiveTasks();\n+        assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n                      \" with \" +\n                      (probingRebalanceNeeded ? \"\" : \"no\") +\n                      \" followup probing rebalance.\");\n+\n         return probingRebalanceNeeded;\n     }\n \n-    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment = new DefaultBalancedAssignor().assign(\n-            sortedClients,\n-            statefulTasks,\n-            clientsToNumberOfThreads\n-        );\n+    private static void assignActiveStatefulTasks(final SortedMap<UUID, ClientState> clientStates,\n+                                                  final SortedSet<TaskId> statefulTasks) {\n+        Iterator<ClientState> clientStateIterator = null;\n+        for (final TaskId task : statefulTasks) {\n+            if (clientStateIterator == null || !clientStateIterator.hasNext()) {\n+                clientStateIterator = clientStates.values().iterator();\n+            }\n+            clientStateIterator.next().assignActive(task);\n+        }\n \n-        return assignTaskMovements(\n-            statefulActiveTaskAssignment,\n-            tasksToCaughtUpClients,\n+        balanceTasksOverThreads(\n             clientStates,\n-            tasksToRemainingStandbys,\n-            configs.maxWarmupReplicas\n+            ClientState::activeTasks,\n+            ClientState::unAssignActive,\n+            ClientState::assignActive\n         );\n     }\n \n-    private void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n+    private static void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                                  final TreeMap<UUID, ClientState> clientStates,\n+                                                  final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates,\n+                                                  final int numStandbyReplicas) {\n+        final ConstrainedPrioritySet standbyTaskClientsByTaskLoad = new ConstrainedPrioritySet(\n+            (client, task) -> !clientStates.get(client).assignedTasks().contains(task),\n+            client -> clientStates.get(client).taskLoad()\n         );\n         standbyTaskClientsByTaskLoad.offerAll(clientStates.keySet());\n \n         for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n-            final int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n-            final List<UUID> clients = standbyTaskClientsByTaskLoad.poll(task, numRemainingStandbys);\n-            for (final UUID client : clients) {\n+            int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n+            while (numRemainingStandbys > 0) {\n+                final UUID client = standbyTaskClientsByTaskLoad.poll(task);\n+                if (client == null) {\n+                    break;\n+                }\n                 clientStates.get(client).assignStandby(task);\n+                numRemainingStandbys--;\n+                standbyTaskClientsByTaskLoad.offer(client);\n             }\n-            standbyTaskClientsByTaskLoad.offerAll(clients);\n \n-            final int numStandbysAssigned = clients.size();\n-            if (numStandbysAssigned < numRemainingStandbys) {\n+            if (numRemainingStandbys > 0) {\n                 log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n                              \"There is not enough available capacity. You should \" +\n                              \"increase the number of threads and/or application instances \" +\n                              \"to maintain the requested number of standby replicas.\",\n-                         numRemainingStandbys - numStandbysAssigned, configs.numStandbyReplicas, task);\n+                         numRemainingStandbys, numStandbyReplicas, task);\n             }\n         }\n-    }\n \n-    private void assignStatelessActiveTasks() {\n-        final ValidClientsByTaskLoadQueue statelessActiveTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n+        balanceTasksOverThreads(\n             clientStates,\n-            (client, task) -> true\n+            ClientState::standbyTasks,\n+            ClientState::unAssignStandby,\n+            ClientState::assignStandby\n+        );\n+    }\n+\n+    private static void balanceTasksOverThreads(final SortedMap<UUID, ClientState> clientStates,\n+                                                final Function<ClientState, Set<TaskId>> currentAssignmentAccessor,\n+                                                final BiConsumer<ClientState, TaskId> taskUnassignor,\n+                                                final BiConsumer<ClientState, TaskId> taskAssignor) {\n+        boolean keepBalancing = true;\n+        while (keepBalancing) {\n+            keepBalancing = false;\n+            for (final Map.Entry<UUID, ClientState> sourceEntry : clientStates.entrySet()) {\n+                final UUID sourceClient = sourceEntry.getKey();\n+                final ClientState sourceClientState = sourceEntry.getValue();\n+\n+                for (final Map.Entry<UUID, ClientState> destinationEntry : clientStates.entrySet()) {\n+                    final UUID destinationClient = destinationEntry.getKey();\n+                    final ClientState destinationClientState = destinationEntry.getValue();\n+                    if (sourceClient.equals(destinationClient)) {\n+                        continue;\n+                    }\n+\n+                    final Set<TaskId> sourceTasks = new TreeSet<>(currentAssignmentAccessor.apply(sourceClientState));\n+                    final Iterator<TaskId> sourceIterator = sourceTasks.iterator();\n+                    while (shouldMoveATask(sourceClientState, destinationClientState) && sourceIterator.hasNext()) {\n+                        final TaskId taskToMove = sourceIterator.next();\n+                        final boolean canMove = !destinationClientState.assignedTasks().contains(taskToMove);\n+                        if (canMove) {\n+                            taskUnassignor.accept(sourceClientState, taskToMove);\n+                            taskAssignor.accept(destinationClientState, taskToMove);\n+                            keepBalancing = true;\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    private static boolean shouldMoveATask(final ClientState sourceClientState,\n+                                           final ClientState destinationClientState) {\n+        final double assignedTasksPerStreamThreadAtDestination =\n+            1.0 * destinationClientState.assignedTasks().size() / destinationClientState.capacity();\n+        final double assignedTasksPerStreamThreadAtSource =\n+            1.0 * sourceClientState.assignedTasks().size() / sourceClientState.capacity();\n+        final double skew = assignedTasksPerStreamThreadAtSource - assignedTasksPerStreamThreadAtDestination;\n+\n+        if (skew <= 0) {\n+            return false;\n+        }\n+\n+        final double proposedAssignedTasksPerStreamThreadAtDestination =\n+            (destinationClientState.assignedTasks().size() + 1.0) / destinationClientState.capacity();", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzYxNjUyMA==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423616520", "bodyText": "req: Please use assignedTaskCount() instead of assignedTasks().size().", "author": "cadonna", "createdAt": "2020-05-12T10:05:32Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );\n \n-        assignStatelessActiveTasks();\n+        assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n                      \" with \" +\n                      (probingRebalanceNeeded ? \"\" : \"no\") +\n                      \" followup probing rebalance.\");\n+\n         return probingRebalanceNeeded;\n     }\n \n-    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment = new DefaultBalancedAssignor().assign(\n-            sortedClients,\n-            statefulTasks,\n-            clientsToNumberOfThreads\n-        );\n+    private static void assignActiveStatefulTasks(final SortedMap<UUID, ClientState> clientStates,\n+                                                  final SortedSet<TaskId> statefulTasks) {\n+        Iterator<ClientState> clientStateIterator = null;\n+        for (final TaskId task : statefulTasks) {\n+            if (clientStateIterator == null || !clientStateIterator.hasNext()) {\n+                clientStateIterator = clientStates.values().iterator();\n+            }\n+            clientStateIterator.next().assignActive(task);\n+        }\n \n-        return assignTaskMovements(\n-            statefulActiveTaskAssignment,\n-            tasksToCaughtUpClients,\n+        balanceTasksOverThreads(\n             clientStates,\n-            tasksToRemainingStandbys,\n-            configs.maxWarmupReplicas\n+            ClientState::activeTasks,\n+            ClientState::unAssignActive,\n+            ClientState::assignActive\n         );\n     }\n \n-    private void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n+    private static void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                                  final TreeMap<UUID, ClientState> clientStates,\n+                                                  final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates,\n+                                                  final int numStandbyReplicas) {\n+        final ConstrainedPrioritySet standbyTaskClientsByTaskLoad = new ConstrainedPrioritySet(\n+            (client, task) -> !clientStates.get(client).assignedTasks().contains(task),\n+            client -> clientStates.get(client).taskLoad()\n         );\n         standbyTaskClientsByTaskLoad.offerAll(clientStates.keySet());\n \n         for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n-            final int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n-            final List<UUID> clients = standbyTaskClientsByTaskLoad.poll(task, numRemainingStandbys);\n-            for (final UUID client : clients) {\n+            int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n+            while (numRemainingStandbys > 0) {\n+                final UUID client = standbyTaskClientsByTaskLoad.poll(task);\n+                if (client == null) {\n+                    break;\n+                }\n                 clientStates.get(client).assignStandby(task);\n+                numRemainingStandbys--;\n+                standbyTaskClientsByTaskLoad.offer(client);\n             }\n-            standbyTaskClientsByTaskLoad.offerAll(clients);\n \n-            final int numStandbysAssigned = clients.size();\n-            if (numStandbysAssigned < numRemainingStandbys) {\n+            if (numRemainingStandbys > 0) {\n                 log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n                              \"There is not enough available capacity. You should \" +\n                              \"increase the number of threads and/or application instances \" +\n                              \"to maintain the requested number of standby replicas.\",\n-                         numRemainingStandbys - numStandbysAssigned, configs.numStandbyReplicas, task);\n+                         numRemainingStandbys, numStandbyReplicas, task);\n             }\n         }\n-    }\n \n-    private void assignStatelessActiveTasks() {\n-        final ValidClientsByTaskLoadQueue statelessActiveTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n+        balanceTasksOverThreads(\n             clientStates,\n-            (client, task) -> true\n+            ClientState::standbyTasks,\n+            ClientState::unAssignStandby,\n+            ClientState::assignStandby\n+        );\n+    }\n+\n+    private static void balanceTasksOverThreads(final SortedMap<UUID, ClientState> clientStates,\n+                                                final Function<ClientState, Set<TaskId>> currentAssignmentAccessor,\n+                                                final BiConsumer<ClientState, TaskId> taskUnassignor,\n+                                                final BiConsumer<ClientState, TaskId> taskAssignor) {\n+        boolean keepBalancing = true;\n+        while (keepBalancing) {\n+            keepBalancing = false;\n+            for (final Map.Entry<UUID, ClientState> sourceEntry : clientStates.entrySet()) {\n+                final UUID sourceClient = sourceEntry.getKey();\n+                final ClientState sourceClientState = sourceEntry.getValue();\n+\n+                for (final Map.Entry<UUID, ClientState> destinationEntry : clientStates.entrySet()) {\n+                    final UUID destinationClient = destinationEntry.getKey();\n+                    final ClientState destinationClientState = destinationEntry.getValue();\n+                    if (sourceClient.equals(destinationClient)) {\n+                        continue;\n+                    }\n+\n+                    final Set<TaskId> sourceTasks = new TreeSet<>(currentAssignmentAccessor.apply(sourceClientState));\n+                    final Iterator<TaskId> sourceIterator = sourceTasks.iterator();\n+                    while (shouldMoveATask(sourceClientState, destinationClientState) && sourceIterator.hasNext()) {\n+                        final TaskId taskToMove = sourceIterator.next();\n+                        final boolean canMove = !destinationClientState.assignedTasks().contains(taskToMove);\n+                        if (canMove) {\n+                            taskUnassignor.accept(sourceClientState, taskToMove);\n+                            taskAssignor.accept(destinationClientState, taskToMove);\n+                            keepBalancing = true;\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    private static boolean shouldMoveATask(final ClientState sourceClientState,\n+                                           final ClientState destinationClientState) {\n+        final double assignedTasksPerStreamThreadAtDestination =\n+            1.0 * destinationClientState.assignedTasks().size() / destinationClientState.capacity();\n+        final double assignedTasksPerStreamThreadAtSource =\n+            1.0 * sourceClientState.assignedTasks().size() / sourceClientState.capacity();\n+        final double skew = assignedTasksPerStreamThreadAtSource - assignedTasksPerStreamThreadAtDestination;\n+\n+        if (skew <= 0) {\n+            return false;\n+        }\n+\n+        final double proposedAssignedTasksPerStreamThreadAtDestination =\n+            (destinationClientState.assignedTasks().size() + 1.0) / destinationClientState.capacity();\n+        final double proposedAssignedTasksPerStreamThreadAtSource =\n+            (sourceClientState.assignedTasks().size() - 1.0) / sourceClientState.capacity();", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzYxNzI5Ng==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423617296", "bodyText": "req: Please use assignedTaskCount() instead of assignedTasks().size().", "author": "cadonna", "createdAt": "2020-05-12T10:06:50Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );\n \n-        assignStatelessActiveTasks();\n+        assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n                      \" with \" +\n                      (probingRebalanceNeeded ? \"\" : \"no\") +\n                      \" followup probing rebalance.\");\n+\n         return probingRebalanceNeeded;\n     }\n \n-    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment = new DefaultBalancedAssignor().assign(\n-            sortedClients,\n-            statefulTasks,\n-            clientsToNumberOfThreads\n-        );\n+    private static void assignActiveStatefulTasks(final SortedMap<UUID, ClientState> clientStates,\n+                                                  final SortedSet<TaskId> statefulTasks) {\n+        Iterator<ClientState> clientStateIterator = null;\n+        for (final TaskId task : statefulTasks) {\n+            if (clientStateIterator == null || !clientStateIterator.hasNext()) {\n+                clientStateIterator = clientStates.values().iterator();\n+            }\n+            clientStateIterator.next().assignActive(task);\n+        }\n \n-        return assignTaskMovements(\n-            statefulActiveTaskAssignment,\n-            tasksToCaughtUpClients,\n+        balanceTasksOverThreads(\n             clientStates,\n-            tasksToRemainingStandbys,\n-            configs.maxWarmupReplicas\n+            ClientState::activeTasks,\n+            ClientState::unAssignActive,\n+            ClientState::assignActive\n         );\n     }\n \n-    private void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n+    private static void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                                  final TreeMap<UUID, ClientState> clientStates,\n+                                                  final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates,\n+                                                  final int numStandbyReplicas) {\n+        final ConstrainedPrioritySet standbyTaskClientsByTaskLoad = new ConstrainedPrioritySet(\n+            (client, task) -> !clientStates.get(client).assignedTasks().contains(task),\n+            client -> clientStates.get(client).taskLoad()\n         );\n         standbyTaskClientsByTaskLoad.offerAll(clientStates.keySet());\n \n         for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n-            final int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n-            final List<UUID> clients = standbyTaskClientsByTaskLoad.poll(task, numRemainingStandbys);\n-            for (final UUID client : clients) {\n+            int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n+            while (numRemainingStandbys > 0) {\n+                final UUID client = standbyTaskClientsByTaskLoad.poll(task);\n+                if (client == null) {\n+                    break;\n+                }\n                 clientStates.get(client).assignStandby(task);\n+                numRemainingStandbys--;\n+                standbyTaskClientsByTaskLoad.offer(client);\n             }\n-            standbyTaskClientsByTaskLoad.offerAll(clients);\n \n-            final int numStandbysAssigned = clients.size();\n-            if (numStandbysAssigned < numRemainingStandbys) {\n+            if (numRemainingStandbys > 0) {\n                 log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n                              \"There is not enough available capacity. You should \" +\n                              \"increase the number of threads and/or application instances \" +\n                              \"to maintain the requested number of standby replicas.\",\n-                         numRemainingStandbys - numStandbysAssigned, configs.numStandbyReplicas, task);\n+                         numRemainingStandbys, numStandbyReplicas, task);\n             }\n         }\n-    }\n \n-    private void assignStatelessActiveTasks() {\n-        final ValidClientsByTaskLoadQueue statelessActiveTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n+        balanceTasksOverThreads(\n             clientStates,\n-            (client, task) -> true\n+            ClientState::standbyTasks,\n+            ClientState::unAssignStandby,\n+            ClientState::assignStandby\n+        );\n+    }\n+\n+    private static void balanceTasksOverThreads(final SortedMap<UUID, ClientState> clientStates,\n+                                                final Function<ClientState, Set<TaskId>> currentAssignmentAccessor,\n+                                                final BiConsumer<ClientState, TaskId> taskUnassignor,\n+                                                final BiConsumer<ClientState, TaskId> taskAssignor) {\n+        boolean keepBalancing = true;\n+        while (keepBalancing) {\n+            keepBalancing = false;\n+            for (final Map.Entry<UUID, ClientState> sourceEntry : clientStates.entrySet()) {\n+                final UUID sourceClient = sourceEntry.getKey();\n+                final ClientState sourceClientState = sourceEntry.getValue();\n+\n+                for (final Map.Entry<UUID, ClientState> destinationEntry : clientStates.entrySet()) {\n+                    final UUID destinationClient = destinationEntry.getKey();\n+                    final ClientState destinationClientState = destinationEntry.getValue();\n+                    if (sourceClient.equals(destinationClient)) {\n+                        continue;\n+                    }\n+\n+                    final Set<TaskId> sourceTasks = new TreeSet<>(currentAssignmentAccessor.apply(sourceClientState));\n+                    final Iterator<TaskId> sourceIterator = sourceTasks.iterator();\n+                    while (shouldMoveATask(sourceClientState, destinationClientState) && sourceIterator.hasNext()) {\n+                        final TaskId taskToMove = sourceIterator.next();\n+                        final boolean canMove = !destinationClientState.assignedTasks().contains(taskToMove);\n+                        if (canMove) {\n+                            taskUnassignor.accept(sourceClientState, taskToMove);\n+                            taskAssignor.accept(destinationClientState, taskToMove);\n+                            keepBalancing = true;\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    private static boolean shouldMoveATask(final ClientState sourceClientState,\n+                                           final ClientState destinationClientState) {\n+        final double assignedTasksPerStreamThreadAtDestination =\n+            1.0 * destinationClientState.assignedTasks().size() / destinationClientState.capacity();", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzYxNzM4NQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423617385", "bodyText": "req: Please use assignedTaskCount() instead of assignedTasks().size().", "author": "cadonna", "createdAt": "2020-05-12T10:07:00Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );\n \n-        assignStatelessActiveTasks();\n+        assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n                      \" with \" +\n                      (probingRebalanceNeeded ? \"\" : \"no\") +\n                      \" followup probing rebalance.\");\n+\n         return probingRebalanceNeeded;\n     }\n \n-    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment = new DefaultBalancedAssignor().assign(\n-            sortedClients,\n-            statefulTasks,\n-            clientsToNumberOfThreads\n-        );\n+    private static void assignActiveStatefulTasks(final SortedMap<UUID, ClientState> clientStates,\n+                                                  final SortedSet<TaskId> statefulTasks) {\n+        Iterator<ClientState> clientStateIterator = null;\n+        for (final TaskId task : statefulTasks) {\n+            if (clientStateIterator == null || !clientStateIterator.hasNext()) {\n+                clientStateIterator = clientStates.values().iterator();\n+            }\n+            clientStateIterator.next().assignActive(task);\n+        }\n \n-        return assignTaskMovements(\n-            statefulActiveTaskAssignment,\n-            tasksToCaughtUpClients,\n+        balanceTasksOverThreads(\n             clientStates,\n-            tasksToRemainingStandbys,\n-            configs.maxWarmupReplicas\n+            ClientState::activeTasks,\n+            ClientState::unAssignActive,\n+            ClientState::assignActive\n         );\n     }\n \n-    private void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n+    private static void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                                  final TreeMap<UUID, ClientState> clientStates,\n+                                                  final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates,\n+                                                  final int numStandbyReplicas) {\n+        final ConstrainedPrioritySet standbyTaskClientsByTaskLoad = new ConstrainedPrioritySet(\n+            (client, task) -> !clientStates.get(client).assignedTasks().contains(task),\n+            client -> clientStates.get(client).taskLoad()\n         );\n         standbyTaskClientsByTaskLoad.offerAll(clientStates.keySet());\n \n         for (final TaskId task : statefulTasksToRankedCandidates.keySet()) {\n-            final int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n-            final List<UUID> clients = standbyTaskClientsByTaskLoad.poll(task, numRemainingStandbys);\n-            for (final UUID client : clients) {\n+            int numRemainingStandbys = tasksToRemainingStandbys.get(task);\n+            while (numRemainingStandbys > 0) {\n+                final UUID client = standbyTaskClientsByTaskLoad.poll(task);\n+                if (client == null) {\n+                    break;\n+                }\n                 clientStates.get(client).assignStandby(task);\n+                numRemainingStandbys--;\n+                standbyTaskClientsByTaskLoad.offer(client);\n             }\n-            standbyTaskClientsByTaskLoad.offerAll(clients);\n \n-            final int numStandbysAssigned = clients.size();\n-            if (numStandbysAssigned < numRemainingStandbys) {\n+            if (numRemainingStandbys > 0) {\n                 log.warn(\"Unable to assign {} of {} standby tasks for task [{}]. \" +\n                              \"There is not enough available capacity. You should \" +\n                              \"increase the number of threads and/or application instances \" +\n                              \"to maintain the requested number of standby replicas.\",\n-                         numRemainingStandbys - numStandbysAssigned, configs.numStandbyReplicas, task);\n+                         numRemainingStandbys, numStandbyReplicas, task);\n             }\n         }\n-    }\n \n-    private void assignStatelessActiveTasks() {\n-        final ValidClientsByTaskLoadQueue statelessActiveTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n+        balanceTasksOverThreads(\n             clientStates,\n-            (client, task) -> true\n+            ClientState::standbyTasks,\n+            ClientState::unAssignStandby,\n+            ClientState::assignStandby\n+        );\n+    }\n+\n+    private static void balanceTasksOverThreads(final SortedMap<UUID, ClientState> clientStates,\n+                                                final Function<ClientState, Set<TaskId>> currentAssignmentAccessor,\n+                                                final BiConsumer<ClientState, TaskId> taskUnassignor,\n+                                                final BiConsumer<ClientState, TaskId> taskAssignor) {\n+        boolean keepBalancing = true;\n+        while (keepBalancing) {\n+            keepBalancing = false;\n+            for (final Map.Entry<UUID, ClientState> sourceEntry : clientStates.entrySet()) {\n+                final UUID sourceClient = sourceEntry.getKey();\n+                final ClientState sourceClientState = sourceEntry.getValue();\n+\n+                for (final Map.Entry<UUID, ClientState> destinationEntry : clientStates.entrySet()) {\n+                    final UUID destinationClient = destinationEntry.getKey();\n+                    final ClientState destinationClientState = destinationEntry.getValue();\n+                    if (sourceClient.equals(destinationClient)) {\n+                        continue;\n+                    }\n+\n+                    final Set<TaskId> sourceTasks = new TreeSet<>(currentAssignmentAccessor.apply(sourceClientState));\n+                    final Iterator<TaskId> sourceIterator = sourceTasks.iterator();\n+                    while (shouldMoveATask(sourceClientState, destinationClientState) && sourceIterator.hasNext()) {\n+                        final TaskId taskToMove = sourceIterator.next();\n+                        final boolean canMove = !destinationClientState.assignedTasks().contains(taskToMove);\n+                        if (canMove) {\n+                            taskUnassignor.accept(sourceClientState, taskToMove);\n+                            taskAssignor.accept(destinationClientState, taskToMove);\n+                            keepBalancing = true;\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    private static boolean shouldMoveATask(final ClientState sourceClientState,\n+                                           final ClientState destinationClientState) {\n+        final double assignedTasksPerStreamThreadAtDestination =\n+            1.0 * destinationClientState.assignedTasks().size() / destinationClientState.capacity();\n+        final double assignedTasksPerStreamThreadAtSource =\n+            1.0 * sourceClientState.assignedTasks().size() / sourceClientState.capacity();", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzYyMjM0OA==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423622348", "bodyText": "prop: I see assignedTasks().contains(task) a couple of times in the assignment algorithm. Should we provide a ClientState#containsAssignedTask() to avoid computing the union each time it is called?", "author": "cadonna", "createdAt": "2020-05-12T10:15:52Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =\n+            buildClientRankingsByTask(statefulTasks, clients, configs.acceptableRecoveryLag);\n \n+        final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients =\n+            tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n \n         final Map<TaskId, Integer> tasksToRemainingStandbys =\n             statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> configs.numStandbyReplicas));\n \n-        final boolean probingRebalanceNeeded = assignStatefulActiveTasks(tasksToRemainingStandbys);\n+        assignActiveStatefulTasks(clientStates, statefulTasks);\n \n-        assignStandbyReplicaTasks(tasksToRemainingStandbys);\n+        assignStandbyReplicaTasks(\n+            tasksToRemainingStandbys,\n+            clientStates,\n+            statefulTasksToRankedCandidates,\n+            configs.numStandbyReplicas\n+        );\n+\n+        final boolean probingRebalanceNeeded = assignTaskMovements(\n+            tasksToCaughtUpClients,\n+            clientStates,\n+            configs.maxWarmupReplicas\n+        );\n \n-        assignStatelessActiveTasks();\n+        assignStatelessActiveTasks(clientStates, diff(TreeSet::new, allTaskIds, statefulTasks));\n \n         log.info(\"Decided on assignment: \" +\n                      clientStates +\n                      \" with \" +\n                      (probingRebalanceNeeded ? \"\" : \"no\") +\n                      \" followup probing rebalance.\");\n+\n         return probingRebalanceNeeded;\n     }\n \n-    private boolean assignStatefulActiveTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final Map<UUID, List<TaskId>> statefulActiveTaskAssignment = new DefaultBalancedAssignor().assign(\n-            sortedClients,\n-            statefulTasks,\n-            clientsToNumberOfThreads\n-        );\n+    private static void assignActiveStatefulTasks(final SortedMap<UUID, ClientState> clientStates,\n+                                                  final SortedSet<TaskId> statefulTasks) {\n+        Iterator<ClientState> clientStateIterator = null;\n+        for (final TaskId task : statefulTasks) {\n+            if (clientStateIterator == null || !clientStateIterator.hasNext()) {\n+                clientStateIterator = clientStates.values().iterator();\n+            }\n+            clientStateIterator.next().assignActive(task);\n+        }\n \n-        return assignTaskMovements(\n-            statefulActiveTaskAssignment,\n-            tasksToCaughtUpClients,\n+        balanceTasksOverThreads(\n             clientStates,\n-            tasksToRemainingStandbys,\n-            configs.maxWarmupReplicas\n+            ClientState::activeTasks,\n+            ClientState::unAssignActive,\n+            ClientState::assignActive\n         );\n     }\n \n-    private void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys) {\n-        final ValidClientsByTaskLoadQueue standbyTaskClientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> !clientStates.get(client).assignedTasks().contains(task)\n+    private static void assignStandbyReplicaTasks(final Map<TaskId, Integer> tasksToRemainingStandbys,\n+                                                  final TreeMap<UUID, ClientState> clientStates,\n+                                                  final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates,\n+                                                  final int numStandbyReplicas) {\n+        final ConstrainedPrioritySet standbyTaskClientsByTaskLoad = new ConstrainedPrioritySet(\n+            (client, task) -> !clientStates.get(client).assignedTasks().contains(task),", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzk5ODk2OA==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423998968", "bodyText": "Haha, I just found that we already had it: hasAssignedTask", "author": "vvcephei", "createdAt": "2020-05-12T20:01:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzYyMjM0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzY2MTA1MA==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423661050", "bodyText": "prop: It seems like we do not need statefulTasksToRankedCandidates anymore. We could directly build tasksToCaughtUpClients from statefulTasks, clients, and configs.acceptableRecoveryLag. Also in assignStandbyReplicaTasks(), we only use the keySet() of statefulTasksToRankedCandidates. So, we can remove statefulTasksToRankedCandidates.", "author": "cadonna", "createdAt": "2020-05-12T11:29:36Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -21,126 +21,195 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n+import java.util.TreeMap;\n import java.util.TreeSet;\n import java.util.UUID;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.common.utils.Utils.diff;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private Map<UUID, ClientState> clientStates;\n-    private Map<UUID, Integer> clientsToNumberOfThreads;\n-    private SortedSet<UUID> sortedClients;\n-\n-    private Set<TaskId> allTasks;\n-    private SortedSet<TaskId> statefulTasks;\n-    private SortedSet<TaskId> statelessTasks;\n-\n-    private AssignmentConfigs configs;\n-\n-    private SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates;\n-    private Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients;\n-\n     @Override\n-    public boolean assign(final Map<UUID, ClientState> clientStates,\n-                          final Set<TaskId> allTasks,\n-                          final Set<TaskId> statefulTasks,\n+    public boolean assign(final Map<UUID, ClientState> clients,\n+                          final Set<TaskId> allTaskIds,\n+                          final Set<TaskId> statefulTaskIds,\n                           final AssignmentConfigs configs) {\n-        this.configs = configs;\n-        this.clientStates = clientStates;\n-        this.allTasks = allTasks;\n-        this.statefulTasks = new TreeSet<>(statefulTasks);\n-\n-        statelessTasks = new TreeSet<>(allTasks);\n-        statelessTasks.removeAll(statefulTasks);\n-\n-        sortedClients = new TreeSet<>();\n-        clientsToNumberOfThreads = new HashMap<>();\n-        clientStates.forEach((client, state) -> {\n-            sortedClients.add(client);\n-            clientsToNumberOfThreads.put(client, state.capacity());\n-        });\n+        final SortedSet<TaskId> statefulTasks = new TreeSet<>(statefulTaskIds);\n+        final TreeMap<UUID, ClientState> clientStates = new TreeMap<>(clients);\n \n-        statefulTasksToRankedCandidates =\n-            buildClientRankingsByTask(statefulTasks, clientStates, configs.acceptableRecoveryLag);\n-        tasksToCaughtUpClients = tasksToCaughtUpClients(statefulTasksToRankedCandidates);\n+        final SortedMap<TaskId, SortedSet<RankedClient>> statefulTasksToRankedCandidates =", "originalCommit": "cbd08807fb74217207502606cfb86325515077f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkxNTM2Nw==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r423915367", "bodyText": "Sure thing. I'll give it a shot.", "author": "vvcephei", "createdAt": "2020-05-12T17:39:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzY2MTA1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDAxMDE4OA==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r424010188", "bodyText": "Woah, this actually means we don't wind up needing RankedClient at all (after having spent so much time talking about how to define the rankings... sheesh). Nice catch!", "author": "vvcephei", "createdAt": "2020-05-12T20:22:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzY2MTA1MA=="}], "type": "inlineReview"}, {"oid": "c036caa6c06cb32e8811c918b0abb09767f658fe", "url": "https://github.com/apache/kafka/commit/c036caa6c06cb32e8811c918b0abb09767f658fe", "message": "Merge branch 'trunk' into kafka-6145-validate-balanced-assignment", "committedDate": "2020-05-12T18:15:35Z", "type": "commit"}, {"oid": "6b0655e27b98733d69d4767b37dd0f570419c1a6", "url": "https://github.com/apache/kafka/commit/6b0655e27b98733d69d4767b37dd0f570419c1a6", "message": "more util tests", "committedDate": "2020-05-12T19:16:07Z", "type": "commit"}, {"oid": "7026acf24da182171ebf4bdb62aa08cc40cc75da", "url": "https://github.com/apache/kafka/commit/7026acf24da182171ebf4bdb62aa08cc40cc75da", "message": "unassign", "committedDate": "2020-05-12T19:22:10Z", "type": "commit"}, {"oid": "0d443ca845e85631faed2644754b73b5a74f0ff2", "url": "https://github.com/apache/kafka/commit/0d443ca845e85631faed2644754b73b5a74f0ff2", "message": "CR regarding ClientState", "committedDate": "2020-05-12T20:04:23Z", "type": "commit"}, {"oid": "4ddcbd7b56075581de8c22968de3c90c6efc299c", "url": "https://github.com/apache/kafka/commit/4ddcbd7b56075581de8c22968de3c90c6efc299c", "message": "remove RankedClient", "committedDate": "2020-05-12T20:17:09Z", "type": "commit"}, {"oid": "8bf161b94198a536d812ad553f0f92d75d0f154c", "url": "https://github.com/apache/kafka/commit/8bf161b94198a536d812ad553f0f92d75d0f154c", "message": "CR cleanup", "committedDate": "2020-05-12T20:23:19Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUxMTEwNQ==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r424511105", "bodyText": "Q: Why do we need this additional check?", "author": "cadonna", "createdAt": "2020-05-13T15:04:51Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentTestUtils.java", "diffHunk": "@@ -75,4 +94,303 @@\n     static UUID uuidForInt(final int n) {\n         return new UUID(0, n);\n     }\n+\n+    static void assertValidAssignment(final int numStandbyReplicas,\n+                                      final Set<TaskId> statefulTasks,\n+                                      final Set<TaskId> statelessTasks,\n+                                      final Map<UUID, ClientState> assignedStates,\n+                                      final StringBuilder failureContext) {\n+        assertValidAssignment(\n+            numStandbyReplicas,\n+            0,\n+            statefulTasks,\n+            statelessTasks,\n+            assignedStates,\n+            failureContext\n+        );\n+    }\n+\n+    static void assertValidAssignment(final int numStandbyReplicas,\n+                                      final int maxWarmupReplicas,\n+                                      final Set<TaskId> statefulTasks,\n+                                      final Set<TaskId> statelessTasks,\n+                                      final Map<UUID, ClientState> assignedStates,\n+                                      final StringBuilder failureContext) {\n+        final Map<TaskId, Set<UUID>> assignments = new TreeMap<>();\n+        for (final TaskId taskId : statefulTasks) {\n+            assignments.put(taskId, new TreeSet<>());\n+        }\n+        for (final TaskId taskId : statelessTasks) {\n+            assignments.put(taskId, new TreeSet<>());\n+        }\n+        for (final Map.Entry<UUID, ClientState> entry : assignedStates.entrySet()) {\n+            validateAndAddActiveAssignments(statefulTasks, statelessTasks, failureContext, assignments, entry);\n+            validateAndAddStandbyAssignments(statefulTasks, statelessTasks, failureContext, assignments, entry);\n+        }\n+\n+        final AtomicInteger remainingWarmups = new AtomicInteger(maxWarmupReplicas);\n+\n+        final TreeMap<TaskId, Set<UUID>> misassigned =\n+            assignments\n+                .entrySet()\n+                .stream()\n+                .filter(entry -> {\n+                    final int expectedActives = 1;\n+                    final boolean isStateless = statelessTasks.contains(entry.getKey());\n+                    final int expectedStandbys = isStateless ? 0 : numStandbyReplicas;\n+                    // We'll never assign even the expected number of standbys if they don't actually fit in the cluster\n+                    final int expectedAssignments = Math.min(\n+                        assignedStates.size(),\n+                        expectedActives + expectedStandbys\n+                    );\n+                    final int actualAssignments = entry.getValue().size();\n+                    if (actualAssignments == expectedAssignments) {\n+                        return false; // not misassigned\n+                    } else {\n+                        if (actualAssignments == expectedAssignments + 1 && remainingWarmups.get() > 0) {\n+                            remainingWarmups.getAndDecrement();\n+                            return false; // it's a warmup, so it's fine\n+                        } else {\n+                            return true; // misassigned\n+                        }\n+                    }\n+                })\n+                .collect(entriesToMap(TreeMap::new));\n+\n+        if (!misassigned.isEmpty()) {", "originalCommit": "8bf161b94198a536d812ad553f0f92d75d0f154c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDcxODI5MA==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r424718290", "bodyText": "L131-158 is just gathering the information about whether each task is correctly assigned or not, based on its type and the standby configs (and maybe the warmup config). It doesn't make any assertions. So this check is actually the assertion, that no tasks are incorrectly assigned.\nDoing it this way is nicer, since when it fails, it tells you all the incorrectly assigned tasks, not just the first one.", "author": "vvcephei", "createdAt": "2020-05-13T20:41:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDUxMTEwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDczNjY0Ng==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r424736646", "bodyText": "nit: I know I named this in the first place but can we change it to caughtUpClientsByTaskLoad or something?", "author": "ableegoldman", "createdAt": "2020-05-13T21:17:44Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java", "diffHunk": "@@ -53,75 +67,94 @@ private static boolean taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(final Task\n     /**\n      * @return whether any warmup replicas were assigned\n      */\n-    static boolean assignTaskMovements(final Map<UUID, List<TaskId>> statefulActiveTaskAssignment,\n-                                       final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n+    static boolean assignTaskMovements(final Map<TaskId, SortedSet<UUID>> tasksToCaughtUpClients,\n                                        final Map<UUID, ClientState> clientStates,\n-                                       final Map<TaskId, Integer> tasksToRemainingStandbys,\n                                        final int maxWarmupReplicas) {\n-        boolean warmupReplicasAssigned = false;\n+        final BiFunction<UUID, TaskId, Boolean> caughtUpPredicate =\n+            (client, task) -> taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients);\n \n-        final ValidClientsByTaskLoadQueue clientsByTaskLoad = new ValidClientsByTaskLoadQueue(\n-            clientStates,\n-            (client, task) -> taskIsCaughtUpOnClientOrNoCaughtUpClientsExist(task, client, tasksToCaughtUpClients)\n+        final ConstrainedPrioritySet clientsByTaskLoad = new ConstrainedPrioritySet(", "originalCommit": "8bf161b94198a536d812ad553f0f92d75d0f154c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDgxOTgzOA==", "url": "https://github.com/apache/kafka/pull/8588#discussion_r424819838", "bodyText": "sure, that's a good idea.", "author": "vvcephei", "createdAt": "2020-05-14T01:25:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDczNjY0Ng=="}], "type": "inlineReview"}, {"oid": "9cf8316444c205ea04f6fa0874619451f5b94d92", "url": "https://github.com/apache/kafka/commit/9cf8316444c205ea04f6fa0874619451f5b94d92", "message": "cr feedback", "committedDate": "2020-05-14T01:52:04Z", "type": "commit"}]}