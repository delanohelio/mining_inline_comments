{"pr_number": 8213, "pr_title": "KAFKA-9615: Clean up task/producer create and close", "pr_createdAt": "2020-03-03T22:16:50Z", "pr_url": "https://github.com/apache/kafka/pull/8213", "timeline": [{"oid": "578c68ff311c955c1658eee8410c5288b4213288", "url": "https://github.com/apache/kafka/commit/578c68ff311c955c1658eee8410c5288b4213288", "message": "wip", "committedDate": "2020-03-02T22:19:45Z", "type": "commit"}, {"oid": "5a101364a5c95a9b50e2de79bd7c68160ecc0e38", "url": "https://github.com/apache/kafka/commit/5a101364a5c95a9b50e2de79bd7c68160ecc0e38", "message": "step 1: pull out task creators", "committedDate": "2020-03-03T18:37:54Z", "type": "commit"}, {"oid": "5aaf9dae22b464b839a8cf23e75beddcb44e00ab", "url": "https://github.com/apache/kafka/commit/5aaf9dae22b464b839a8cf23e75beddcb44e00ab", "message": "step 2: encapsulate producers", "committedDate": "2020-03-03T19:29:46Z", "type": "commit"}, {"oid": "0a8c8fe6f84968abff59fe40571b677402f6e7b5", "url": "https://github.com/apache/kafka/commit/0a8c8fe6f84968abff59fe40571b677402f6e7b5", "message": "step 3: clean up StreamsProducer", "committedDate": "2020-03-03T21:23:57Z", "type": "commit"}, {"oid": "b63153a3b84f49ba36aff8db9115841979145390", "url": "https://github.com/apache/kafka/commit/b63153a3b84f49ba36aff8db9115841979145390", "message": "step 4: clean up abstract task creator", "committedDate": "2020-03-03T22:07:50Z", "type": "commit"}, {"oid": "f53ef6da113430fbff915c590985658f2d6166e9", "url": "https://github.com/apache/kafka/commit/f53ef6da113430fbff915c590985658f2d6166e9", "message": "final cleanup", "committedDate": "2020-03-03T22:35:08Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM1MjQ4MQ==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387352481", "bodyText": "nit: These two functions are not for testing only.", "author": "guozhangwang", "createdAt": "2020-03-03T23:16:05Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -624,9 +624,11 @@ StandbyTask standbyTask(final TopicPartition partition) {\n         return null;\n     }\n \n-    // TODO K9113: this is used from StreamThread only for a hack to collect metrics from the record collectors inside of StreamTasks\n-    // Instead, we should register and record the metrics properly inside of the record collector.\n-    Map<TaskId, StreamTask> fixmeStreamTasks() {\n-        return tasks.values().stream().filter(t -> t instanceof StreamTask).map(t -> (StreamTask) t).collect(Collectors.toMap(Task::id, t -> t));\n+    Map<MetricName, Metric> producerMetrics() {", "originalCommit": "f53ef6da113430fbff915c590985658f2d6166e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzgyNTY2NQ==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387825665", "bodyText": "Thanks for the feedback; I didn't understand this particular comment, though.", "author": "vvcephei", "createdAt": "2020-03-04T17:38:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM1MjQ4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzg0OTU4NQ==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387849585", "bodyText": "The comment line above these two method declaration says the following functions are for test only, but these two functions are not.", "author": "guozhangwang", "createdAt": "2020-03-04T18:23:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM1MjQ4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzkzNjg3MA==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387936870", "bodyText": "Ah, I just found what you were talking about:\n    // below are for testing only\n\nI didn't notice that up there.", "author": "vvcephei", "createdAt": "2020-03-04T21:11:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM1MjQ4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM1NDMxOQ==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387354319", "bodyText": "In shutdown(final boolean clean) we should also release task producers as well right?", "author": "guozhangwang", "createdAt": "2020-03-03T23:21:56Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -81,9 +81,8 @@\n                 final UUID processId,\n                 final String logPrefix,\n                 final StreamsMetricsImpl streamsMetrics,\n-                final StreamThread.AbstractTaskCreator<? extends Task> activeTaskCreator,\n-                final StreamThread.AbstractTaskCreator<? extends Task> standbyTaskCreator,\n-                final Map<TaskId, Producer<byte[], byte[]>> taskProducers,\n+                final ActiveTaskCreator activeTaskCreator,", "originalCommit": "f53ef6da113430fbff915c590985658f2d6166e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzg0OTM1NQ==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387849355", "bodyText": "Yep. Good catch.", "author": "vvcephei", "createdAt": "2020-03-04T18:22:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM1NDMxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2MTIxMA==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387361210", "bodyText": "I'm wondering if we are introducing a latency regression without EOS here: in the old code when closing without EOS we actually do nothing, and now we would block on flushing.\nOn the other hand, flushing maybe needed when we close a task to make sure all the tasks' records are acked already.\nIf the task is in RUNNING before shutting down, we would always commit before closing, so flush is already called; if the task is in RESTORING / SUSPENDED there's nothing written from this task, so a flush is not needed. So I think it is safe to not call flush after all.", "author": "guozhangwang", "createdAt": "2020-03-03T23:42:26Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -262,7 +262,7 @@ public void close() {\n         if (eosEnabled) {\n             streamsProducer.abortTransaction();\n         }\n-        streamsProducer.close();\n+        streamsProducer.flush();", "originalCommit": "f53ef6da113430fbff915c590985658f2d6166e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2OTc4NA==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387369784", "bodyText": "Sounds reasonable to me", "author": "mjsax", "createdAt": "2020-03-04T00:09:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2MTIxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzc1MTE0NA==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387751144", "bodyText": "Ok, I'll swap this out.", "author": "vvcephei", "createdAt": "2020-03-04T15:42:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2MTIxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2NDQ3MQ==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387364471", "bodyText": "We do not created new test classes of ActiveTaskCreator / StandbyTaskCreator, but we should still have those coverage to make sure the exception thrown from producers are wrapped correctly.\nAlso it seems in the new code we no long rethrow -- is that intentional. I left a comment above.", "author": "guozhangwang", "createdAt": "2020-03-03T23:52:19Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsProducerTest.java", "diffHunk": "@@ -612,25 +582,4 @@ public void shouldFailOnEosAbortTxFatal() {\n \n         assertThat(thrown.getMessage(), equalTo(\"KABOOM!\"));\n     }\n-\n-    @Test\n-    public void shouldFailOnCloseFatal() {", "originalCommit": "f53ef6da113430fbff915c590985658f2d6166e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzg0OTc3Mw==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387849773", "bodyText": "Good catch. Actually, there's a bunch of coverage missing from TaskManager. I'll add several more tests.", "author": "vvcephei", "createdAt": "2020-03-04T18:23:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2NDQ3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2NDg5OQ==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387364899", "bodyText": "The old code we re-throw exceptions whereas here we just swallow the error. Is that intentional?", "author": "guozhangwang", "createdAt": "2020-03-03T23:53:38Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Metric;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.processor.internals.metrics.ThreadMetrics;\n+import org.apache.kafka.streams.state.internals.ThreadCache;\n+import org.slf4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+\n+class ActiveTaskCreator {\n+    private final String applicationId;\n+    private final InternalTopologyBuilder builder;\n+    private final StreamsConfig config;\n+    private final StreamsMetricsImpl streamsMetrics;\n+    private final StateDirectory stateDirectory;\n+    private final ChangelogReader storeChangelogReader;\n+    private final Time time;\n+    private final Logger log;\n+    private final String threadId;\n+    private final ThreadCache cache;\n+    private final Producer<byte[], byte[]> threadProducer;\n+    private final KafkaClientSupplier clientSupplier;\n+    private final Map<TaskId, Producer<byte[], byte[]>> taskProducers;\n+    private final Sensor createTaskSensor;\n+\n+    private static String getThreadProducerClientId(final String threadClientId) {\n+        return threadClientId + \"-producer\";\n+    }\n+\n+    private static String getTaskProducerClientId(final String threadClientId, final TaskId taskId) {\n+        return threadClientId + \"-\" + taskId + \"-producer\";\n+    }\n+\n+    ActiveTaskCreator(final InternalTopologyBuilder builder,\n+                      final StreamsConfig config,\n+                      final StreamsMetricsImpl streamsMetrics,\n+                      final StateDirectory stateDirectory,\n+                      final ChangelogReader storeChangelogReader,\n+                      final ThreadCache cache,\n+                      final Time time,\n+                      final KafkaClientSupplier clientSupplier,\n+                      final String threadId,\n+                      final Logger log) {\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+        this.builder = builder;\n+        this.config = config;\n+        this.streamsMetrics = streamsMetrics;\n+        this.stateDirectory = stateDirectory;\n+        this.storeChangelogReader = storeChangelogReader;\n+        this.time = time;\n+        this.log = log;\n+\n+        if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n+            threadProducer = null;\n+            taskProducers = new HashMap<>();\n+        } else {\n+            final String threadProducerClientId = getThreadProducerClientId(threadId);\n+            final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n+            log.info(\"Creating thread producer client\");\n+            threadProducer = clientSupplier.getProducer(producerConfigs);\n+            taskProducers = Collections.emptyMap();\n+        }\n+\n+\n+        this.cache = cache;\n+        this.threadId = threadId;\n+        this.clientSupplier = clientSupplier;\n+\n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+    }\n+\n+    Collection<Task> createTasks(final Consumer<byte[], byte[]> consumer,\n+                                 final Map<TaskId, Set<TopicPartition>> tasksToBeCreated) {\n+        final List<Task> createdTasks = new ArrayList<>();\n+        for (final Map.Entry<TaskId, Set<TopicPartition>> newTaskAndPartitions : tasksToBeCreated.entrySet()) {\n+            final TaskId taskId = newTaskAndPartitions.getKey();\n+            final Set<TopicPartition> partitions = newTaskAndPartitions.getValue();\n+\n+            final String threadIdPrefix = String.format(\"stream-thread [%s] \", Thread.currentThread().getName());\n+            final String logPrefix = threadIdPrefix + String.format(\"%s [%s] \", \"task\", taskId);\n+            final LogContext logContext = new LogContext(logPrefix);\n+\n+            final ProcessorTopology topology = builder.buildSubtopology(taskId.topicGroupId);\n+\n+            final ProcessorStateManager stateManager = new ProcessorStateManager(\n+                taskId,\n+                partitions,\n+                Task.TaskType.ACTIVE,\n+                stateDirectory,\n+                topology.storeToChangelogTopic(),\n+                storeChangelogReader,\n+                logContext\n+            );\n+\n+            if (threadProducer == null) {\n+                // create one producer per task for EOS\n+                // TODO: after KIP-447 this would be removed\n+                final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n+                final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n+                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n+                log.info(\"Creating producer client for task {}\", taskId);\n+                taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n+            }\n+\n+            final RecordCollector recordCollector = new RecordCollectorImpl(\n+                logContext,\n+                taskId,\n+                consumer,\n+                threadProducer != null ?\n+                    new StreamsProducer(threadProducer, false, logContext, applicationId) :\n+                    new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n+                config.defaultProductionExceptionHandler(),\n+                EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n+                streamsMetrics\n+            );\n+\n+            final Task task = new StreamTask(\n+                taskId,\n+                partitions,\n+                topology,\n+                consumer,\n+                config,\n+                streamsMetrics,\n+                stateDirectory,\n+                cache,\n+                time,\n+                stateManager,\n+                recordCollector\n+            );\n+\n+            log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n+            createdTasks.add(task);\n+            createTaskSensor.record();\n+        }\n+        return createdTasks;\n+    }\n+\n+    public void releaseProducer() {\n+        if (threadProducer != null) {\n+            try {\n+                threadProducer.close();\n+            } catch (final RuntimeException e) {\n+                log.error(\"Failed to close producer due to the following error:\", e);", "originalCommit": "f53ef6da113430fbff915c590985658f2d6166e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzgzNTM3NA==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387835374", "bodyText": "Ah, thanks for noticing this. I meant to ask about it in here.\nI thought it was strange that we would only re-throw when closing the task producers, not the thread producer. It seems like we should do the same thing in both cases, but which thing should we do?\nI went with an error log in both cases, but it sounds like you wanted to throw the exception instead. Should we also rethrow for the thread producer?", "author": "vvcephei", "createdAt": "2020-03-04T17:56:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2NDg5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzg1MDcxOQ==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387850719", "bodyText": "Yeah I think we should make them consistent: previously the closing producer is within task#close and if it is called via closeDirty we should make sure it never throws. Now since it is extracted out of the close call we should just rethrow for both cases.", "author": "guozhangwang", "createdAt": "2020-03-04T18:25:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2NDg5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2NTY0NQ==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387365645", "bodyText": "null != null?", "author": "guozhangwang", "createdAt": "2020-03-03T23:56:01Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/KeyValueStoreTestDriver.java", "diffHunk": "@@ -201,7 +201,7 @@ private KeyValueStoreTestDriver(final StateSerdes<K, V> serdes) {\n             logContext,\n             new TaskId(0, 0),\n             consumer,\n-            new StreamsProducer(logContext, producer),\n+            new StreamsProducer(producer, null != null, logContext, null),", "originalCommit": "f53ef6da113430fbff915c590985658f2d6166e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM5Njc5NQ==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387396795", "bodyText": "lol", "author": "abbccdda", "createdAt": "2020-03-04T01:18:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2NTY0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzgzNTY5Nw==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387835697", "bodyText": "Oops! Auto-refactoring. I already made two passes to clean these up, looks like I missed one.", "author": "vvcephei", "createdAt": "2020-03-04T17:57:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2NTY0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2ODM4MA==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387368380", "bodyText": "closeThreadProducer()\nThe method below is not public -- does this one need to be public?", "author": "mjsax", "createdAt": "2020-03-04T00:05:01Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Metric;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.processor.internals.metrics.ThreadMetrics;\n+import org.apache.kafka.streams.state.internals.ThreadCache;\n+import org.slf4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+\n+class ActiveTaskCreator {\n+    private final String applicationId;\n+    private final InternalTopologyBuilder builder;\n+    private final StreamsConfig config;\n+    private final StreamsMetricsImpl streamsMetrics;\n+    private final StateDirectory stateDirectory;\n+    private final ChangelogReader storeChangelogReader;\n+    private final Time time;\n+    private final Logger log;\n+    private final String threadId;\n+    private final ThreadCache cache;\n+    private final Producer<byte[], byte[]> threadProducer;\n+    private final KafkaClientSupplier clientSupplier;\n+    private final Map<TaskId, Producer<byte[], byte[]>> taskProducers;\n+    private final Sensor createTaskSensor;\n+\n+    private static String getThreadProducerClientId(final String threadClientId) {\n+        return threadClientId + \"-producer\";\n+    }\n+\n+    private static String getTaskProducerClientId(final String threadClientId, final TaskId taskId) {\n+        return threadClientId + \"-\" + taskId + \"-producer\";\n+    }\n+\n+    ActiveTaskCreator(final InternalTopologyBuilder builder,\n+                      final StreamsConfig config,\n+                      final StreamsMetricsImpl streamsMetrics,\n+                      final StateDirectory stateDirectory,\n+                      final ChangelogReader storeChangelogReader,\n+                      final ThreadCache cache,\n+                      final Time time,\n+                      final KafkaClientSupplier clientSupplier,\n+                      final String threadId,\n+                      final Logger log) {\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+        this.builder = builder;\n+        this.config = config;\n+        this.streamsMetrics = streamsMetrics;\n+        this.stateDirectory = stateDirectory;\n+        this.storeChangelogReader = storeChangelogReader;\n+        this.time = time;\n+        this.log = log;\n+\n+        if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n+            threadProducer = null;\n+            taskProducers = new HashMap<>();\n+        } else {\n+            final String threadProducerClientId = getThreadProducerClientId(threadId);\n+            final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n+            log.info(\"Creating thread producer client\");\n+            threadProducer = clientSupplier.getProducer(producerConfigs);\n+            taskProducers = Collections.emptyMap();\n+        }\n+\n+\n+        this.cache = cache;\n+        this.threadId = threadId;\n+        this.clientSupplier = clientSupplier;\n+\n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+    }\n+\n+    Collection<Task> createTasks(final Consumer<byte[], byte[]> consumer,\n+                                 final Map<TaskId, Set<TopicPartition>> tasksToBeCreated) {\n+        final List<Task> createdTasks = new ArrayList<>();\n+        for (final Map.Entry<TaskId, Set<TopicPartition>> newTaskAndPartitions : tasksToBeCreated.entrySet()) {\n+            final TaskId taskId = newTaskAndPartitions.getKey();\n+            final Set<TopicPartition> partitions = newTaskAndPartitions.getValue();\n+\n+            final String threadIdPrefix = String.format(\"stream-thread [%s] \", Thread.currentThread().getName());\n+            final String logPrefix = threadIdPrefix + String.format(\"%s [%s] \", \"task\", taskId);\n+            final LogContext logContext = new LogContext(logPrefix);\n+\n+            final ProcessorTopology topology = builder.buildSubtopology(taskId.topicGroupId);\n+\n+            final ProcessorStateManager stateManager = new ProcessorStateManager(\n+                taskId,\n+                partitions,\n+                Task.TaskType.ACTIVE,\n+                stateDirectory,\n+                topology.storeToChangelogTopic(),\n+                storeChangelogReader,\n+                logContext\n+            );\n+\n+            if (threadProducer == null) {\n+                // create one producer per task for EOS\n+                // TODO: after KIP-447 this would be removed\n+                final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n+                final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n+                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n+                log.info(\"Creating producer client for task {}\", taskId);\n+                taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n+            }\n+\n+            final RecordCollector recordCollector = new RecordCollectorImpl(\n+                logContext,\n+                taskId,\n+                consumer,\n+                threadProducer != null ?\n+                    new StreamsProducer(threadProducer, false, logContext, applicationId) :\n+                    new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n+                config.defaultProductionExceptionHandler(),\n+                EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n+                streamsMetrics\n+            );\n+\n+            final Task task = new StreamTask(\n+                taskId,\n+                partitions,\n+                topology,\n+                consumer,\n+                config,\n+                streamsMetrics,\n+                stateDirectory,\n+                cache,\n+                time,\n+                stateManager,\n+                recordCollector\n+            );\n+\n+            log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n+            createdTasks.add(task);\n+            createTaskSensor.record();\n+        }\n+        return createdTasks;\n+    }\n+\n+    public void releaseProducer() {", "originalCommit": "f53ef6da113430fbff915c590985658f2d6166e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM5OTc1Mg==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387399752", "bodyText": "even better maybeCloseThreadProducer", "author": "abbccdda", "createdAt": "2020-03-04T01:26:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2ODM4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2ODUxNQ==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387368515", "bodyText": "closeProducerForTask(TaskId) ?", "author": "mjsax", "createdAt": "2020-03-04T00:05:35Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Metric;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.processor.internals.metrics.ThreadMetrics;\n+import org.apache.kafka.streams.state.internals.ThreadCache;\n+import org.slf4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+\n+class ActiveTaskCreator {\n+    private final String applicationId;\n+    private final InternalTopologyBuilder builder;\n+    private final StreamsConfig config;\n+    private final StreamsMetricsImpl streamsMetrics;\n+    private final StateDirectory stateDirectory;\n+    private final ChangelogReader storeChangelogReader;\n+    private final Time time;\n+    private final Logger log;\n+    private final String threadId;\n+    private final ThreadCache cache;\n+    private final Producer<byte[], byte[]> threadProducer;\n+    private final KafkaClientSupplier clientSupplier;\n+    private final Map<TaskId, Producer<byte[], byte[]>> taskProducers;\n+    private final Sensor createTaskSensor;\n+\n+    private static String getThreadProducerClientId(final String threadClientId) {\n+        return threadClientId + \"-producer\";\n+    }\n+\n+    private static String getTaskProducerClientId(final String threadClientId, final TaskId taskId) {\n+        return threadClientId + \"-\" + taskId + \"-producer\";\n+    }\n+\n+    ActiveTaskCreator(final InternalTopologyBuilder builder,\n+                      final StreamsConfig config,\n+                      final StreamsMetricsImpl streamsMetrics,\n+                      final StateDirectory stateDirectory,\n+                      final ChangelogReader storeChangelogReader,\n+                      final ThreadCache cache,\n+                      final Time time,\n+                      final KafkaClientSupplier clientSupplier,\n+                      final String threadId,\n+                      final Logger log) {\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+        this.builder = builder;\n+        this.config = config;\n+        this.streamsMetrics = streamsMetrics;\n+        this.stateDirectory = stateDirectory;\n+        this.storeChangelogReader = storeChangelogReader;\n+        this.time = time;\n+        this.log = log;\n+\n+        if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n+            threadProducer = null;\n+            taskProducers = new HashMap<>();\n+        } else {\n+            final String threadProducerClientId = getThreadProducerClientId(threadId);\n+            final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n+            log.info(\"Creating thread producer client\");\n+            threadProducer = clientSupplier.getProducer(producerConfigs);\n+            taskProducers = Collections.emptyMap();\n+        }\n+\n+\n+        this.cache = cache;\n+        this.threadId = threadId;\n+        this.clientSupplier = clientSupplier;\n+\n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+    }\n+\n+    Collection<Task> createTasks(final Consumer<byte[], byte[]> consumer,\n+                                 final Map<TaskId, Set<TopicPartition>> tasksToBeCreated) {\n+        final List<Task> createdTasks = new ArrayList<>();\n+        for (final Map.Entry<TaskId, Set<TopicPartition>> newTaskAndPartitions : tasksToBeCreated.entrySet()) {\n+            final TaskId taskId = newTaskAndPartitions.getKey();\n+            final Set<TopicPartition> partitions = newTaskAndPartitions.getValue();\n+\n+            final String threadIdPrefix = String.format(\"stream-thread [%s] \", Thread.currentThread().getName());\n+            final String logPrefix = threadIdPrefix + String.format(\"%s [%s] \", \"task\", taskId);\n+            final LogContext logContext = new LogContext(logPrefix);\n+\n+            final ProcessorTopology topology = builder.buildSubtopology(taskId.topicGroupId);\n+\n+            final ProcessorStateManager stateManager = new ProcessorStateManager(\n+                taskId,\n+                partitions,\n+                Task.TaskType.ACTIVE,\n+                stateDirectory,\n+                topology.storeToChangelogTopic(),\n+                storeChangelogReader,\n+                logContext\n+            );\n+\n+            if (threadProducer == null) {\n+                // create one producer per task for EOS\n+                // TODO: after KIP-447 this would be removed\n+                final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n+                final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n+                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n+                log.info(\"Creating producer client for task {}\", taskId);\n+                taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n+            }\n+\n+            final RecordCollector recordCollector = new RecordCollectorImpl(\n+                logContext,\n+                taskId,\n+                consumer,\n+                threadProducer != null ?\n+                    new StreamsProducer(threadProducer, false, logContext, applicationId) :\n+                    new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n+                config.defaultProductionExceptionHandler(),\n+                EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n+                streamsMetrics\n+            );\n+\n+            final Task task = new StreamTask(\n+                taskId,\n+                partitions,\n+                topology,\n+                consumer,\n+                config,\n+                streamsMetrics,\n+                stateDirectory,\n+                cache,\n+                time,\n+                stateManager,\n+                recordCollector\n+            );\n+\n+            log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n+            createdTasks.add(task);\n+            createTaskSensor.record();\n+        }\n+        return createdTasks;\n+    }\n+\n+    public void releaseProducer() {\n+        if (threadProducer != null) {\n+            try {\n+                threadProducer.close();\n+            } catch (final RuntimeException e) {\n+                log.error(\"Failed to close producer due to the following error:\", e);\n+            }\n+        }\n+        if (!taskProducers.isEmpty()) {\n+            throw new IllegalStateException(\"Expected task producers to have been cleared before closing\");\n+        }\n+    }\n+\n+    void releaseProducer(final TaskId id) {", "originalCommit": "f53ef6da113430fbff915c590985658f2d6166e9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2ODgxMA==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387368810", "bodyText": "Does this need to be public?", "author": "mjsax", "createdAt": "2020-03-04T00:06:26Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Metric;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.processor.internals.metrics.ThreadMetrics;\n+import org.apache.kafka.streams.state.internals.ThreadCache;\n+import org.slf4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+\n+class ActiveTaskCreator {\n+    private final String applicationId;\n+    private final InternalTopologyBuilder builder;\n+    private final StreamsConfig config;\n+    private final StreamsMetricsImpl streamsMetrics;\n+    private final StateDirectory stateDirectory;\n+    private final ChangelogReader storeChangelogReader;\n+    private final Time time;\n+    private final Logger log;\n+    private final String threadId;\n+    private final ThreadCache cache;\n+    private final Producer<byte[], byte[]> threadProducer;\n+    private final KafkaClientSupplier clientSupplier;\n+    private final Map<TaskId, Producer<byte[], byte[]>> taskProducers;\n+    private final Sensor createTaskSensor;\n+\n+    private static String getThreadProducerClientId(final String threadClientId) {\n+        return threadClientId + \"-producer\";\n+    }\n+\n+    private static String getTaskProducerClientId(final String threadClientId, final TaskId taskId) {\n+        return threadClientId + \"-\" + taskId + \"-producer\";\n+    }\n+\n+    ActiveTaskCreator(final InternalTopologyBuilder builder,\n+                      final StreamsConfig config,\n+                      final StreamsMetricsImpl streamsMetrics,\n+                      final StateDirectory stateDirectory,\n+                      final ChangelogReader storeChangelogReader,\n+                      final ThreadCache cache,\n+                      final Time time,\n+                      final KafkaClientSupplier clientSupplier,\n+                      final String threadId,\n+                      final Logger log) {\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+        this.builder = builder;\n+        this.config = config;\n+        this.streamsMetrics = streamsMetrics;\n+        this.stateDirectory = stateDirectory;\n+        this.storeChangelogReader = storeChangelogReader;\n+        this.time = time;\n+        this.log = log;\n+\n+        if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n+            threadProducer = null;\n+            taskProducers = new HashMap<>();\n+        } else {\n+            final String threadProducerClientId = getThreadProducerClientId(threadId);\n+            final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n+            log.info(\"Creating thread producer client\");\n+            threadProducer = clientSupplier.getProducer(producerConfigs);\n+            taskProducers = Collections.emptyMap();\n+        }\n+\n+\n+        this.cache = cache;\n+        this.threadId = threadId;\n+        this.clientSupplier = clientSupplier;\n+\n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+    }\n+\n+    Collection<Task> createTasks(final Consumer<byte[], byte[]> consumer,\n+                                 final Map<TaskId, Set<TopicPartition>> tasksToBeCreated) {\n+        final List<Task> createdTasks = new ArrayList<>();\n+        for (final Map.Entry<TaskId, Set<TopicPartition>> newTaskAndPartitions : tasksToBeCreated.entrySet()) {\n+            final TaskId taskId = newTaskAndPartitions.getKey();\n+            final Set<TopicPartition> partitions = newTaskAndPartitions.getValue();\n+\n+            final String threadIdPrefix = String.format(\"stream-thread [%s] \", Thread.currentThread().getName());\n+            final String logPrefix = threadIdPrefix + String.format(\"%s [%s] \", \"task\", taskId);\n+            final LogContext logContext = new LogContext(logPrefix);\n+\n+            final ProcessorTopology topology = builder.buildSubtopology(taskId.topicGroupId);\n+\n+            final ProcessorStateManager stateManager = new ProcessorStateManager(\n+                taskId,\n+                partitions,\n+                Task.TaskType.ACTIVE,\n+                stateDirectory,\n+                topology.storeToChangelogTopic(),\n+                storeChangelogReader,\n+                logContext\n+            );\n+\n+            if (threadProducer == null) {\n+                // create one producer per task for EOS\n+                // TODO: after KIP-447 this would be removed\n+                final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n+                final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n+                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n+                log.info(\"Creating producer client for task {}\", taskId);\n+                taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n+            }\n+\n+            final RecordCollector recordCollector = new RecordCollectorImpl(\n+                logContext,\n+                taskId,\n+                consumer,\n+                threadProducer != null ?\n+                    new StreamsProducer(threadProducer, false, logContext, applicationId) :\n+                    new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n+                config.defaultProductionExceptionHandler(),\n+                EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n+                streamsMetrics\n+            );\n+\n+            final Task task = new StreamTask(\n+                taskId,\n+                partitions,\n+                topology,\n+                consumer,\n+                config,\n+                streamsMetrics,\n+                stateDirectory,\n+                cache,\n+                time,\n+                stateManager,\n+                recordCollector\n+            );\n+\n+            log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n+            createdTasks.add(task);\n+            createTaskSensor.record();\n+        }\n+        return createdTasks;\n+    }\n+\n+    public void releaseProducer() {\n+        if (threadProducer != null) {\n+            try {\n+                threadProducer.close();\n+            } catch (final RuntimeException e) {\n+                log.error(\"Failed to close producer due to the following error:\", e);\n+            }\n+        }\n+        if (!taskProducers.isEmpty()) {\n+            throw new IllegalStateException(\"Expected task producers to have been cleared before closing\");\n+        }\n+    }\n+\n+    void releaseProducer(final TaskId id) {\n+        final Producer<byte[], byte[]> producer = taskProducers.remove(id);\n+        if (producer != null) {\n+            try {\n+                producer.close();\n+            } catch (final RuntimeException e) {\n+                log.error(\"Failed to close producer due to the following error:\", e);\n+            }\n+        }\n+    }\n+\n+    Map<MetricName, Metric> producerMetrics() {\n+        final Map<MetricName, Metric> result = new LinkedHashMap<>();\n+        if (threadProducer != null) {\n+            final Map<MetricName, ? extends Metric> producerMetrics = threadProducer.metrics();\n+            if (producerMetrics != null) {\n+                result.putAll(producerMetrics);\n+            }\n+        } else {\n+            // When EOS is turned on, each task will have its own producer client\n+            // and the producer object passed in here will be null. We would then iterate through\n+            // all the active tasks and add their metrics to the output metrics map.\n+            for (final Map.Entry<TaskId, Producer<byte[], byte[]>> entry : taskProducers.entrySet()) {\n+                final Map<MetricName, ? extends Metric> taskProducerMetrics = entry.getValue().metrics();\n+                result.putAll(taskProducerMetrics);\n+            }\n+        }\n+        return result;\n+    }\n+\n+    Set<String> producerClientIds() {\n+        if (threadProducer != null) {\n+            return Collections.singleton(getThreadProducerClientId(threadId));\n+        } else {\n+            return taskProducers.keySet()\n+                                .stream()\n+                                .map(taskId -> getTaskProducerClientId(threadId, taskId))\n+                                .collect(Collectors.toSet());\n+        }\n+    }\n+\n+    public InternalTopologyBuilder builder() {", "originalCommit": "f53ef6da113430fbff915c590985658f2d6166e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzgzODg2Nw==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387838867", "bodyText": "Hah, doesn't need to be there at all, actually.", "author": "vvcephei", "createdAt": "2020-03-04T18:03:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2ODgxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2ODgyOQ==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387368829", "bodyText": "Does this need to be public?", "author": "mjsax", "createdAt": "2020-03-04T00:06:30Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Metric;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.processor.internals.metrics.ThreadMetrics;\n+import org.apache.kafka.streams.state.internals.ThreadCache;\n+import org.slf4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+\n+class ActiveTaskCreator {\n+    private final String applicationId;\n+    private final InternalTopologyBuilder builder;\n+    private final StreamsConfig config;\n+    private final StreamsMetricsImpl streamsMetrics;\n+    private final StateDirectory stateDirectory;\n+    private final ChangelogReader storeChangelogReader;\n+    private final Time time;\n+    private final Logger log;\n+    private final String threadId;\n+    private final ThreadCache cache;\n+    private final Producer<byte[], byte[]> threadProducer;\n+    private final KafkaClientSupplier clientSupplier;\n+    private final Map<TaskId, Producer<byte[], byte[]>> taskProducers;\n+    private final Sensor createTaskSensor;\n+\n+    private static String getThreadProducerClientId(final String threadClientId) {\n+        return threadClientId + \"-producer\";\n+    }\n+\n+    private static String getTaskProducerClientId(final String threadClientId, final TaskId taskId) {\n+        return threadClientId + \"-\" + taskId + \"-producer\";\n+    }\n+\n+    ActiveTaskCreator(final InternalTopologyBuilder builder,\n+                      final StreamsConfig config,\n+                      final StreamsMetricsImpl streamsMetrics,\n+                      final StateDirectory stateDirectory,\n+                      final ChangelogReader storeChangelogReader,\n+                      final ThreadCache cache,\n+                      final Time time,\n+                      final KafkaClientSupplier clientSupplier,\n+                      final String threadId,\n+                      final Logger log) {\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+        this.builder = builder;\n+        this.config = config;\n+        this.streamsMetrics = streamsMetrics;\n+        this.stateDirectory = stateDirectory;\n+        this.storeChangelogReader = storeChangelogReader;\n+        this.time = time;\n+        this.log = log;\n+\n+        if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n+            threadProducer = null;\n+            taskProducers = new HashMap<>();\n+        } else {\n+            final String threadProducerClientId = getThreadProducerClientId(threadId);\n+            final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n+            log.info(\"Creating thread producer client\");\n+            threadProducer = clientSupplier.getProducer(producerConfigs);\n+            taskProducers = Collections.emptyMap();\n+        }\n+\n+\n+        this.cache = cache;\n+        this.threadId = threadId;\n+        this.clientSupplier = clientSupplier;\n+\n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+    }\n+\n+    Collection<Task> createTasks(final Consumer<byte[], byte[]> consumer,\n+                                 final Map<TaskId, Set<TopicPartition>> tasksToBeCreated) {\n+        final List<Task> createdTasks = new ArrayList<>();\n+        for (final Map.Entry<TaskId, Set<TopicPartition>> newTaskAndPartitions : tasksToBeCreated.entrySet()) {\n+            final TaskId taskId = newTaskAndPartitions.getKey();\n+            final Set<TopicPartition> partitions = newTaskAndPartitions.getValue();\n+\n+            final String threadIdPrefix = String.format(\"stream-thread [%s] \", Thread.currentThread().getName());\n+            final String logPrefix = threadIdPrefix + String.format(\"%s [%s] \", \"task\", taskId);\n+            final LogContext logContext = new LogContext(logPrefix);\n+\n+            final ProcessorTopology topology = builder.buildSubtopology(taskId.topicGroupId);\n+\n+            final ProcessorStateManager stateManager = new ProcessorStateManager(\n+                taskId,\n+                partitions,\n+                Task.TaskType.ACTIVE,\n+                stateDirectory,\n+                topology.storeToChangelogTopic(),\n+                storeChangelogReader,\n+                logContext\n+            );\n+\n+            if (threadProducer == null) {\n+                // create one producer per task for EOS\n+                // TODO: after KIP-447 this would be removed\n+                final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n+                final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n+                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n+                log.info(\"Creating producer client for task {}\", taskId);\n+                taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n+            }\n+\n+            final RecordCollector recordCollector = new RecordCollectorImpl(\n+                logContext,\n+                taskId,\n+                consumer,\n+                threadProducer != null ?\n+                    new StreamsProducer(threadProducer, false, logContext, applicationId) :\n+                    new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n+                config.defaultProductionExceptionHandler(),\n+                EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n+                streamsMetrics\n+            );\n+\n+            final Task task = new StreamTask(\n+                taskId,\n+                partitions,\n+                topology,\n+                consumer,\n+                config,\n+                streamsMetrics,\n+                stateDirectory,\n+                cache,\n+                time,\n+                stateManager,\n+                recordCollector\n+            );\n+\n+            log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n+            createdTasks.add(task);\n+            createTaskSensor.record();\n+        }\n+        return createdTasks;\n+    }\n+\n+    public void releaseProducer() {\n+        if (threadProducer != null) {\n+            try {\n+                threadProducer.close();\n+            } catch (final RuntimeException e) {\n+                log.error(\"Failed to close producer due to the following error:\", e);\n+            }\n+        }\n+        if (!taskProducers.isEmpty()) {\n+            throw new IllegalStateException(\"Expected task producers to have been cleared before closing\");\n+        }\n+    }\n+\n+    void releaseProducer(final TaskId id) {\n+        final Producer<byte[], byte[]> producer = taskProducers.remove(id);\n+        if (producer != null) {\n+            try {\n+                producer.close();\n+            } catch (final RuntimeException e) {\n+                log.error(\"Failed to close producer due to the following error:\", e);\n+            }\n+        }\n+    }\n+\n+    Map<MetricName, Metric> producerMetrics() {\n+        final Map<MetricName, Metric> result = new LinkedHashMap<>();\n+        if (threadProducer != null) {\n+            final Map<MetricName, ? extends Metric> producerMetrics = threadProducer.metrics();\n+            if (producerMetrics != null) {\n+                result.putAll(producerMetrics);\n+            }\n+        } else {\n+            // When EOS is turned on, each task will have its own producer client\n+            // and the producer object passed in here will be null. We would then iterate through\n+            // all the active tasks and add their metrics to the output metrics map.\n+            for (final Map.Entry<TaskId, Producer<byte[], byte[]>> entry : taskProducers.entrySet()) {\n+                final Map<MetricName, ? extends Metric> taskProducerMetrics = entry.getValue().metrics();\n+                result.putAll(taskProducerMetrics);\n+            }\n+        }\n+        return result;\n+    }\n+\n+    Set<String> producerClientIds() {\n+        if (threadProducer != null) {\n+            return Collections.singleton(getThreadProducerClientId(threadId));\n+        } else {\n+            return taskProducers.keySet()\n+                                .stream()\n+                                .map(taskId -> getTaskProducerClientId(threadId, taskId))\n+                                .collect(Collectors.toSet());\n+        }\n+    }\n+\n+    public InternalTopologyBuilder builder() {\n+        return builder;\n+    }\n+\n+    public StateDirectory stateDirectory() {", "originalCommit": "f53ef6da113430fbff915c590985658f2d6166e9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM5ODkxNA==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387398914", "bodyText": "I don't think we need to keep this TODO, as only after a stream 3.0 is there, we shall remove the support for task producer.", "author": "abbccdda", "createdAt": "2020-03-04T01:23:17Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Metric;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.processor.internals.metrics.ThreadMetrics;\n+import org.apache.kafka.streams.state.internals.ThreadCache;\n+import org.slf4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+\n+class ActiveTaskCreator {\n+    private final String applicationId;\n+    private final InternalTopologyBuilder builder;\n+    private final StreamsConfig config;\n+    private final StreamsMetricsImpl streamsMetrics;\n+    private final StateDirectory stateDirectory;\n+    private final ChangelogReader storeChangelogReader;\n+    private final Time time;\n+    private final Logger log;\n+    private final String threadId;\n+    private final ThreadCache cache;\n+    private final Producer<byte[], byte[]> threadProducer;\n+    private final KafkaClientSupplier clientSupplier;\n+    private final Map<TaskId, Producer<byte[], byte[]>> taskProducers;\n+    private final Sensor createTaskSensor;\n+\n+    private static String getThreadProducerClientId(final String threadClientId) {\n+        return threadClientId + \"-producer\";\n+    }\n+\n+    private static String getTaskProducerClientId(final String threadClientId, final TaskId taskId) {\n+        return threadClientId + \"-\" + taskId + \"-producer\";\n+    }\n+\n+    ActiveTaskCreator(final InternalTopologyBuilder builder,\n+                      final StreamsConfig config,\n+                      final StreamsMetricsImpl streamsMetrics,\n+                      final StateDirectory stateDirectory,\n+                      final ChangelogReader storeChangelogReader,\n+                      final ThreadCache cache,\n+                      final Time time,\n+                      final KafkaClientSupplier clientSupplier,\n+                      final String threadId,\n+                      final Logger log) {\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+        this.builder = builder;\n+        this.config = config;\n+        this.streamsMetrics = streamsMetrics;\n+        this.stateDirectory = stateDirectory;\n+        this.storeChangelogReader = storeChangelogReader;\n+        this.time = time;\n+        this.log = log;\n+\n+        if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n+            threadProducer = null;\n+            taskProducers = new HashMap<>();\n+        } else {\n+            final String threadProducerClientId = getThreadProducerClientId(threadId);\n+            final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n+            log.info(\"Creating thread producer client\");\n+            threadProducer = clientSupplier.getProducer(producerConfigs);\n+            taskProducers = Collections.emptyMap();\n+        }\n+\n+\n+        this.cache = cache;\n+        this.threadId = threadId;\n+        this.clientSupplier = clientSupplier;\n+\n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+    }\n+\n+    Collection<Task> createTasks(final Consumer<byte[], byte[]> consumer,\n+                                 final Map<TaskId, Set<TopicPartition>> tasksToBeCreated) {\n+        final List<Task> createdTasks = new ArrayList<>();\n+        for (final Map.Entry<TaskId, Set<TopicPartition>> newTaskAndPartitions : tasksToBeCreated.entrySet()) {\n+            final TaskId taskId = newTaskAndPartitions.getKey();\n+            final Set<TopicPartition> partitions = newTaskAndPartitions.getValue();\n+\n+            final String threadIdPrefix = String.format(\"stream-thread [%s] \", Thread.currentThread().getName());\n+            final String logPrefix = threadIdPrefix + String.format(\"%s [%s] \", \"task\", taskId);\n+            final LogContext logContext = new LogContext(logPrefix);\n+\n+            final ProcessorTopology topology = builder.buildSubtopology(taskId.topicGroupId);\n+\n+            final ProcessorStateManager stateManager = new ProcessorStateManager(\n+                taskId,\n+                partitions,\n+                Task.TaskType.ACTIVE,\n+                stateDirectory,\n+                topology.storeToChangelogTopic(),\n+                storeChangelogReader,\n+                logContext\n+            );\n+\n+            if (threadProducer == null) {\n+                // create one producer per task for EOS\n+                // TODO: after KIP-447 this would be removed", "originalCommit": "f53ef6da113430fbff915c590985658f2d6166e9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM5OTI0NA==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387399244", "bodyText": "Should we throw illegal state first, since we are already in an error state?", "author": "abbccdda", "createdAt": "2020-03-04T01:24:30Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Metric;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.processor.internals.metrics.ThreadMetrics;\n+import org.apache.kafka.streams.state.internals.ThreadCache;\n+import org.slf4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+\n+class ActiveTaskCreator {\n+    private final String applicationId;\n+    private final InternalTopologyBuilder builder;\n+    private final StreamsConfig config;\n+    private final StreamsMetricsImpl streamsMetrics;\n+    private final StateDirectory stateDirectory;\n+    private final ChangelogReader storeChangelogReader;\n+    private final Time time;\n+    private final Logger log;\n+    private final String threadId;\n+    private final ThreadCache cache;\n+    private final Producer<byte[], byte[]> threadProducer;\n+    private final KafkaClientSupplier clientSupplier;\n+    private final Map<TaskId, Producer<byte[], byte[]>> taskProducers;\n+    private final Sensor createTaskSensor;\n+\n+    private static String getThreadProducerClientId(final String threadClientId) {\n+        return threadClientId + \"-producer\";\n+    }\n+\n+    private static String getTaskProducerClientId(final String threadClientId, final TaskId taskId) {\n+        return threadClientId + \"-\" + taskId + \"-producer\";\n+    }\n+\n+    ActiveTaskCreator(final InternalTopologyBuilder builder,\n+                      final StreamsConfig config,\n+                      final StreamsMetricsImpl streamsMetrics,\n+                      final StateDirectory stateDirectory,\n+                      final ChangelogReader storeChangelogReader,\n+                      final ThreadCache cache,\n+                      final Time time,\n+                      final KafkaClientSupplier clientSupplier,\n+                      final String threadId,\n+                      final Logger log) {\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+        this.builder = builder;\n+        this.config = config;\n+        this.streamsMetrics = streamsMetrics;\n+        this.stateDirectory = stateDirectory;\n+        this.storeChangelogReader = storeChangelogReader;\n+        this.time = time;\n+        this.log = log;\n+\n+        if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n+            threadProducer = null;\n+            taskProducers = new HashMap<>();\n+        } else {\n+            final String threadProducerClientId = getThreadProducerClientId(threadId);\n+            final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n+            log.info(\"Creating thread producer client\");\n+            threadProducer = clientSupplier.getProducer(producerConfigs);\n+            taskProducers = Collections.emptyMap();\n+        }\n+\n+\n+        this.cache = cache;\n+        this.threadId = threadId;\n+        this.clientSupplier = clientSupplier;\n+\n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+    }\n+\n+    Collection<Task> createTasks(final Consumer<byte[], byte[]> consumer,\n+                                 final Map<TaskId, Set<TopicPartition>> tasksToBeCreated) {\n+        final List<Task> createdTasks = new ArrayList<>();\n+        for (final Map.Entry<TaskId, Set<TopicPartition>> newTaskAndPartitions : tasksToBeCreated.entrySet()) {\n+            final TaskId taskId = newTaskAndPartitions.getKey();\n+            final Set<TopicPartition> partitions = newTaskAndPartitions.getValue();\n+\n+            final String threadIdPrefix = String.format(\"stream-thread [%s] \", Thread.currentThread().getName());\n+            final String logPrefix = threadIdPrefix + String.format(\"%s [%s] \", \"task\", taskId);\n+            final LogContext logContext = new LogContext(logPrefix);\n+\n+            final ProcessorTopology topology = builder.buildSubtopology(taskId.topicGroupId);\n+\n+            final ProcessorStateManager stateManager = new ProcessorStateManager(\n+                taskId,\n+                partitions,\n+                Task.TaskType.ACTIVE,\n+                stateDirectory,\n+                topology.storeToChangelogTopic(),\n+                storeChangelogReader,\n+                logContext\n+            );\n+\n+            if (threadProducer == null) {\n+                // create one producer per task for EOS\n+                // TODO: after KIP-447 this would be removed\n+                final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n+                final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n+                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n+                log.info(\"Creating producer client for task {}\", taskId);\n+                taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n+            }\n+\n+            final RecordCollector recordCollector = new RecordCollectorImpl(\n+                logContext,\n+                taskId,\n+                consumer,\n+                threadProducer != null ?\n+                    new StreamsProducer(threadProducer, false, logContext, applicationId) :\n+                    new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n+                config.defaultProductionExceptionHandler(),\n+                EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n+                streamsMetrics\n+            );\n+\n+            final Task task = new StreamTask(\n+                taskId,\n+                partitions,\n+                topology,\n+                consumer,\n+                config,\n+                streamsMetrics,\n+                stateDirectory,\n+                cache,\n+                time,\n+                stateManager,\n+                recordCollector\n+            );\n+\n+            log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n+            createdTasks.add(task);\n+            createTaskSensor.record();\n+        }\n+        return createdTasks;\n+    }\n+\n+    public void releaseProducer() {\n+        if (threadProducer != null) {\n+            try {\n+                threadProducer.close();\n+            } catch (final RuntimeException e) {\n+                log.error(\"Failed to close producer due to the following error:\", e);\n+            }\n+        }\n+        if (!taskProducers.isEmpty()) {", "originalCommit": "f53ef6da113430fbff915c590985658f2d6166e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzg0MDc2Mg==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387840762", "bodyText": "If we're going to rename the method to specify that it should do exactly \"close thread producer\", then this check is no longer appropriate.", "author": "vvcephei", "createdAt": "2020-03-04T18:06:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM5OTI0NA=="}], "type": "inlineReview"}, {"oid": "dd7dce052166ffa72632068a68cc829221d576de", "url": "https://github.com/apache/kafka/commit/dd7dce052166ffa72632068a68cc829221d576de", "message": "format exception messages in StreamsProducer", "committedDate": "2020-03-04T17:37:37Z", "type": "commit"}, {"oid": "977893a3c56138c419b27a3c3292e8e0c3ccc83c", "url": "https://github.com/apache/kafka/commit/977893a3c56138c419b27a3c3292e8e0c3ccc83c", "message": "misc cr", "committedDate": "2020-03-04T18:09:02Z", "type": "commit"}, {"oid": "46a0baa535b91f2e3c66f0e2d678731532db3bb4", "url": "https://github.com/apache/kafka/commit/46a0baa535b91f2e3c66f0e2d678731532db3bb4", "message": "fixed missing coverage in TaskManagerTest", "committedDate": "2020-03-04T23:08:43Z", "type": "commit"}, {"oid": "3d51081fc6fd2d63bfe25fa7dd1fdf329f4f39ca", "url": "https://github.com/apache/kafka/commit/3d51081fc6fd2d63bfe25fa7dd1fdf329f4f39ca", "message": "fix style", "committedDate": "2020-03-04T23:17:26Z", "type": "commit"}, {"oid": "7405aaf4cf851518e165425cb9139b43e8786756", "url": "https://github.com/apache/kafka/commit/7405aaf4cf851518e165425cb9139b43e8786756", "message": "repair tests", "committedDate": "2020-03-04T23:22:59Z", "type": "commit"}, {"oid": "26205cbda2dcb67f0c2d1acbc6124da5b26ee96f", "url": "https://github.com/apache/kafka/commit/26205cbda2dcb67f0c2d1acbc6124da5b26ee96f", "message": "catch producer close exceptions during unclean shutdown", "committedDate": "2020-03-04T23:44:14Z", "type": "commit"}, {"oid": "2c4a2bcb2a111c9d3b866daca3481f970bababef", "url": "https://github.com/apache/kafka/commit/2c4a2bcb2a111c9d3b866daca3481f970bababef", "message": "suppress complexity", "committedDate": "2020-03-04T23:46:52Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAwMzk2OA==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r388003968", "bodyText": "Let's make it a warn instead of a debug.\nThe error message can be more specific here: Error closing task producer for task {} while handling lostAll.", "author": "guozhangwang", "createdAt": "2020-03-04T23:53:03Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -345,7 +359,11 @@ void handleLostAll() {\n                 cleanupTask(task);\n                 task.closeDirty();\n                 iterator.remove();\n-                taskProducers.remove(task.id());\n+                try {\n+                    activeTaskCreator.closeAndRemoveTaskProducerIfNeeded(task.id());\n+                } catch (final RuntimeException e) {\n+                    log.debug(\"Error handling lostAll\", e);", "originalCommit": "2c4a2bcb2a111c9d3b866daca3481f970bababef", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ1MTQ2MA==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r388451460", "bodyText": "+1", "author": "abbccdda", "createdAt": "2020-03-05T17:35:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAwMzk2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAwNDEyOQ==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r388004129", "bodyText": "Ditto here about error message", "author": "guozhangwang", "createdAt": "2020-03-04T23:53:32Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -419,14 +439,33 @@ void shutdown(final boolean clean) {\n             } else {\n                 task.closeDirty();\n             }\n+            if (task.isActive()) {\n+                try {\n+                    activeTaskCreator.closeAndRemoveTaskProducerIfNeeded(task.id());\n+                } catch (final RuntimeException e) {\n+                    if (clean) {\n+                        firstException.compareAndSet(null, e);\n+                    } else {\n+                        log.warn(\"Ignoring an exception while closing task producer.\", e);", "originalCommit": "2c4a2bcb2a111c9d3b866daca3481f970bababef", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAwNDgyNQ==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r388004825", "bodyText": "nit: ...WhileRebalanceInProgress", "author": "guozhangwang", "createdAt": "2020-03-04T23:55:43Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -440,6 +872,41 @@ public void shouldCommitActiveAndStandbyTasks() {\n         assertThat(taskManager.commitAll(), equalTo(2));\n     }\n \n+    @Test\n+    public void shouldNotCommitActiveAndStandbyTasks() {", "originalCommit": "2c4a2bcb2a111c9d3b866daca3481f970bababef", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "2eb596b0721bcf1908d19f811d96e5121ca4e41d", "url": "https://github.com/apache/kafka/commit/2eb596b0721bcf1908d19f811d96e5121ca4e41d", "message": "minor final CR feedback", "committedDate": "2020-03-05T17:10:35Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ0NDQ2OA==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r388444468", "bodyText": "I was wondering what's the standard for using assertThat vs assertTrue? Do we have a convention to follow?", "author": "abbccdda", "createdAt": "2020-03-05T17:23:05Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsProducerTest.java", "diffHunk": "@@ -254,30 +224,30 @@ public void shouldNotCloseProducerIfEosDisabled() {\n \n     @Test\n     public void shouldInitTxOnEos() {\n-        assertTrue(eosMockProducer.transactionInitialized());\n+        assertThat(eosMockProducer.transactionInitialized(), is(true));\n     }\n \n     @Test\n     public void shouldBeginTxOnEosSend() {\n         eosStreamsProducer.send(record, null);\n-        assertTrue(eosMockProducer.transactionInFlight());\n+        assertThat(eosMockProducer.transactionInFlight(), is(true));\n     }\n \n     @Test\n     public void shouldContinueTxnSecondEosSend() {\n         eosStreamsProducer.send(record, null);\n         eosStreamsProducer.send(record, null);\n-        assertTrue(eosMockProducer.transactionInFlight());\n-        assertThat(eosMockProducer.uncommittedRecords().size(), equalTo(2));\n+        assertThat(eosMockProducer.transactionInFlight(), is(true));\n+        assertThat(eosMockProducer.uncommittedRecords().size(), is(2));\n     }\n \n     @Test\n     public void shouldForwardRecordButNotCommitOnEosSend() {\n         eosStreamsProducer.send(record, null);\n-        assertTrue(eosMockProducer.transactionInFlight());\n-        assertTrue(eosMockProducer.history().isEmpty());\n-        assertThat(eosMockProducer.uncommittedRecords().size(), equalTo(1));\n-        assertThat(eosMockProducer.uncommittedRecords().get(0), equalTo(record));\n+        assertThat(eosMockProducer.transactionInFlight(), is(true));", "originalCommit": "2c4a2bcb2a111c9d3b866daca3481f970bababef", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODUzOTYwMg==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r388539602", "bodyText": "assertThat is nicer in general, but it doesn't really matter. In this case, IDEA offered to translate, and I was already changing a lot of assertions, so I just accepted the translation.", "author": "vvcephei", "createdAt": "2020-03-05T20:17:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ0NDQ2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ0NzgwMw==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r388447803", "bodyText": "Are the tests added in TaskManager only trying for more coverage? @vvcephei", "author": "abbccdda", "createdAt": "2020-03-05T17:29:08Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -24,14 +24,20 @@\n import org.apache.kafka.clients.consumer.Consumer;\n import org.apache.kafka.clients.consumer.ConsumerRecord;\n import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.Metric;", "originalCommit": "2c4a2bcb2a111c9d3b866daca3481f970bababef", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODUzOTc4MA==", "url": "https://github.com/apache/kafka/pull/8213#discussion_r388539780", "bodyText": "yep, that's right.", "author": "vvcephei", "createdAt": "2020-03-05T20:17:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ0NzgwMw=="}], "type": "inlineReview"}]}