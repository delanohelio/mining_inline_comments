{"pr_number": 8058, "pr_title": "KAFKA-9481: Graceful handling TaskMigrated and TaskCorrupted", "pr_createdAt": "2020-02-07T00:44:06Z", "pr_url": "https://github.com/apache/kafka/pull/8058", "timeline": [{"oid": "3bfd87c37032b1a6bdd343e2ee135d0b3bf33b6f", "url": "https://github.com/apache/kafka/commit/3bfd87c37032b1a6bdd343e2ee135d0b3bf33b6f", "message": "add timer for update limit offsets", "committedDate": "2020-02-05T20:44:05Z", "type": "commit"}, {"oid": "7493782dc28e02a132823b236ea275b066f058ac", "url": "https://github.com/apache/kafka/commit/7493782dc28e02a132823b236ea275b066f058ac", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into KMinor-changelog-reader-limit-offset-update", "committedDate": "2020-02-06T19:54:05Z", "type": "commit"}, {"oid": "33af581d8cb942c5123e4dfc019514aa99129ba8", "url": "https://github.com/apache/kafka/commit/33af581d8cb942c5123e4dfc019514aa99129ba8", "message": "consumer.position invalid offset", "committedDate": "2020-02-06T21:09:55Z", "type": "commit"}, {"oid": "4419900a7f26906a60c0658f4290cd7a2e2d07c6", "url": "https://github.com/apache/kafka/commit/4419900a7f26906a60c0658f4290cd7a2e2d07c6", "message": "add task corrupted logic", "committedDate": "2020-02-06T23:14:09Z", "type": "commit"}, {"oid": "23cd292d4497e5c9d64c6c2efea69e570b72791a", "url": "https://github.com/apache/kafka/commit/23cd292d4497e5c9d64c6c2efea69e570b72791a", "message": "re-enable the unit test", "committedDate": "2020-02-07T00:31:57Z", "type": "commit"}, {"oid": "112d9c6c4cc2113333561d2c4581131491fe217a", "url": "https://github.com/apache/kafka/commit/112d9c6c4cc2113333561d2c4581131491fe217a", "message": "rebased", "committedDate": "2020-02-07T00:45:48Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE2Mjc0MQ==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376162741", "bodyText": "We need to clear the stores map now since we may re-initialize the state stores upon reviving a task.", "author": "guozhangwang", "createdAt": "2020-02-07T00:49:09Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -415,6 +415,8 @@ public void close() throws ProcessorStateException {\n                     log.error(\"Failed to close state store {}: \", store.name(), exception);\n                 }\n             }\n+\n+            stores.clear();", "originalCommit": "112d9c6c4cc2113333561d2c4581131491fe217a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE3MjYzNg==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376172636", "bodyText": "Do we also need to clear storeToChangelogTopic, etc?", "author": "ableegoldman", "createdAt": "2020-02-07T01:27:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE2Mjc0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUwMDEwOA==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376500108", "bodyText": "storeToChangelogTopic and sourcePartitions are passed in at construction time and final, so we cannot clear them (since they would only be initialized once).", "author": "guozhangwang", "createdAt": "2020-02-07T16:58:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE2Mjc0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE2MjkxNw==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376162917", "bodyText": "This case 1) would be done in another PR, I just added the java-doc here to complete the scope.", "author": "guozhangwang", "createdAt": "2020-02-07T00:49:48Z", "path": "streams/src/main/java/org/apache/kafka/streams/errors/TaskCorruptedException.java", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.errors;\n+\n+import org.apache.kafka.streams.processor.TaskId;\n+\n+import java.util.Set;\n+\n+/**\n+ * Indicates a specific task is corrupted and need to be re-initialized. It can be thrown when\n+ *\n+ * 1) Under EOS, if the checkpoint file does not contain offsets for corresponding store's changelogs, meaning", "originalCommit": "112d9c6c4cc2113333561d2c4581131491fe217a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE2MzA4MQ==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376163081", "bodyText": "Here I made the remove call idempotent.", "author": "guozhangwang", "createdAt": "2020-02-07T00:50:24Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -690,18 +704,18 @@ private void pauseChangelogsFromRestoreConsumer(final Collection<TopicPartition>\n                 \"does not contain some of the partitions \" + partitions + \" for pausing.\");\n         }\n         restoreConsumer.pause(partitions);\n+\n+        log.debug(\"Paused partitions {} from the restore consumer\", partitions);\n     }\n \n     private void removeChangelogsFromRestoreConsumer(final Collection<TopicPartition> partitions) {\n         final Set<TopicPartition> assignment = new HashSet<>(restoreConsumer.assignment());\n \n-        // the current assignment should contain the all partitions to remove\n-        if (!assignment.containsAll(partitions)) {\n-            throw new IllegalStateException(\"The current assignment \" + assignment + \" \" +\n-                \"does not contain some of the partitions \" + partitions + \" for removing.\");\n+        if (assignment.removeAll(partitions)) {", "originalCommit": "112d9c6c4cc2113333561d2c4581131491fe217a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE2NDEyOQ==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376164129", "bodyText": "Okay this might be a bit controversial: I tried to make the re-initialization logic as cheap as possible since otherwise we may be kicked out of the group for not calling consumer.poll in time. After looking at the source code I found the only part is the closure of the task-level sensors which I moved out of the task.close functions. So from CLOSED -> CREATED there's nothing necessary.", "author": "guozhangwang", "createdAt": "2020-02-07T00:54:07Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "diffHunk": "@@ -154,6 +154,11 @@ public boolean isValidTransition(final State newState) {\n      */\n     void closeDirty();\n \n+    /**\n+     * Revive a closed task to a created one; should never throw an exception\n+     */\n+    void revive();", "originalCommit": "112d9c6c4cc2113333561d2c4581131491fe217a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE2NDYwMA==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376164600", "bodyText": "We have to call changelog.remove() before task.closeDirty since now we clear the stores map in closeDirty, and after that task.changelogPartitions() would return nothing.", "author": "guozhangwang", "createdAt": "2020-02-07T00:55:47Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -117,6 +120,19 @@ void handleRebalanceComplete() {\n         rebalanceInProgress = false;\n     }\n \n+    void handleCorruption(final Set<TaskId> taskIds) {\n+        for (final TaskId taskId : taskIds) {\n+            final Task task = tasks.get(taskId);\n+\n+            // this call is idempotent so even if the task is only CREATED we can still call it\n+            changelogReader.remove(task.changelogPartitions());", "originalCommit": "112d9c6c4cc2113333561d2c4581131491fe217a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE2NDg0Mw==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376164843", "bodyText": "Consolidated a couple of the same pattern into this private function.", "author": "guozhangwang", "createdAt": "2020-02-07T00:56:39Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -147,20 +163,18 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 task.resume();\n                 standbyTasksToCreate.remove(task.id());\n             } else /* we previously owned this task, and we don't have it anymore, or it has changed active/standby state */ {\n-                final Set<TopicPartition> inputPartitions = task.inputPartitions();\n+                cleanupTask(task);", "originalCommit": "112d9c6c4cc2113333561d2c4581131491fe217a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE2NTE4MA==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376165180", "bodyText": "Not sure why we do not remove the input partitions previously.. is that intentional @vvcephei ? If not I'd move the removal into the block (I already did just to clarify :P)", "author": "guozhangwang", "createdAt": "2020-02-07T00:58:00Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -285,18 +299,13 @@ void handleLostAll() {\n         final Iterator<Task> iterator = tasks.values().iterator();\n         while (iterator.hasNext()) {\n             final Task task = iterator.next();\n-            final Set<TopicPartition> inputPartitions = task.inputPartitions();\n             // Even though we've apparently dropped out of the group, we can continue safely to maintain our\n             // standby tasks while we rejoin.\n             if (task.isActive()) {\n+                cleanupTask(task);\n                 task.closeDirty();\n-                changelogReader.remove(task.changelogPartitions());\n-            }\n-\n-            for (final TopicPartition inputPartition : inputPartitions) {", "originalCommit": "112d9c6c4cc2113333561d2c4581131491fe217a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4NjY5NA==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376686694", "bodyText": "The intent was to remove the input partitions from the map any time we remove a task from tasks. It looks like your code maintains this (in a clearer and cleaner way).", "author": "vvcephei", "createdAt": "2020-02-08T04:35:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE2NTE4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4OTk5Mg==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376689992", "bodyText": "Got it. The previous code is that we would partitionToTask.remove(inputPartition); and remove the task no matter if the task is closed or not, which is a bit weird --- for standby tasks, we do not close them, but we still remove from the iterator and remove from the materialized partitionToTask. My modification is to ONLY do this logic if we are closing the task.\nAs long as you agree this is correct I'm relieved.", "author": "guozhangwang", "createdAt": "2020-02-08T05:48:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE2NTE4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzEyMzQ5MA==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r377123490", "bodyText": "Ah, then it was my mistake before! Good catch.", "author": "vvcephei", "createdAt": "2020-02-10T15:14:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE2NTE4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE2NTMwMw==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376165303", "bodyText": "The order here cannot be changed so I left this comment.", "author": "guozhangwang", "createdAt": "2020-02-07T00:58:18Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -331,12 +340,27 @@ void handleLostAll() {\n         return locallyStoredTasks;\n     }\n \n+    private void cleanupTask(final Task task) {\n+        // 1. remove the changelog partitions from changelog reader;", "originalCommit": "112d9c6c4cc2113333561d2c4581131491fe217a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE3MDA2MA==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376170060", "bodyText": "Remove \"standby\" from error message (unless this only applies to standbys?)", "author": "ableegoldman", "createdAt": "2020-02-07T01:17:04Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java", "diffHunk": "@@ -70,6 +71,15 @@ public boolean isClosed() {\n         return state;\n     }\n \n+    @Override\n+    public void revive() {\n+        if (state == CLOSED) {\n+            transitionTo(CREATED);\n+        } else {\n+            throw new IllegalStateException(\"Illegal state \" + state() + \" while committing standby task \" + id);", "originalCommit": "112d9c6c4cc2113333561d2c4581131491fe217a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQ5OTMzNA==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376499334", "bodyText": "Ack.", "author": "guozhangwang", "createdAt": "2020-02-07T16:57:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE3MDA2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE3MzM3NQ==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376173375", "bodyText": "Just wondering, why is it ok to get InvalidOffsetException during restore/poll but not here? When might this get thrown from #position? ditto for in prepareChangelogs down below", "author": "ableegoldman", "createdAt": "2020-02-07T01:30:57Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -258,6 +260,8 @@ private boolean hasRestoredToEnd(final ChangelogMetadata metadata) {\n                 // if we cannot get the position of the consumer within timeout, just return false\n                 return false;\n             } catch (final KafkaException e) {\n+                // this also includes InvalidOffsetException, which should not happen under normal", "originalCommit": "112d9c6c4cc2113333561d2c4581131491fe217a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUwMDY4Mw==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376500683", "bodyText": "consumer.poll throwing InvalidOffsetException should be handled as TaskCorrupted; consumer.position throwing InvalidOffsetException should not happen under normal scenarios, when it happens it indicates a bug and hence we do not need to special handle it.", "author": "guozhangwang", "createdAt": "2020-02-07T16:59:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE3MzM3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE3NjQ5OQ==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376176499", "bodyText": "Hm. I think we should actually be concerned if we ever get to here -- I'm not sure the TaskMigratedException made sense either. Trying to add records to a closed task would imply that we closed the task due to shutting down or because we no longer own it, both cases which should also involves trimming its topic(s) from the consumer's assignment, but were still returned records for said topic(s).\nUnless, the consumer may still return already-fetched records from partitions no longer in its assignment during poll? I thought we would trim those records out and only return from the actual assignment", "author": "ableegoldman", "createdAt": "2020-02-07T01:43:58Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -692,9 +688,10 @@ private void closeRecordCollector(final boolean clean) {\n     @Override\n     public void addRecords(final TopicPartition partition, final Iterable<ConsumerRecord<byte[], byte[]>> records) {\n         if (state() == State.CLOSED || state() == State.CLOSING) {\n-            log.info(\"Stream task {} is already closed, probably because it got unexpectedly migrated to another thread already. \" +\n-                         \"Notifying the thread to trigger a new rebalance immediately.\", id());\n-            throw new TaskMigratedException(id());\n+            // a task is only closing / closed when 1) task manager is closing, 2) a rebalance is undergoing;\n+            // in either case we can just log it and move on without notifying the thread since the consumer\n+            // would soon be updated to not return any records for this task anymore.\n+            log.info(\"Stream task {} is already in {} state, skip adding records to it.\", id(), state());", "originalCommit": "112d9c6c4cc2113333561d2c4581131491fe217a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUwMTk2OQ==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376501969", "bodyText": "Here's my rationale:\nIf the task is closed due to rebalance (i.e. we #handleAssignment or #handleLostAll), there might still be some buffered records from the consumer that are returning (since we update the subscription of consumer after), in this case since the subscription would be updated in the next iteration and no records would be returned, it is okay to just skip this once.\nIf the task is closed due to closing the thread, then there's no need to throw an exception either.", "author": "guozhangwang", "createdAt": "2020-02-07T17:02:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE3NjQ5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE3NjgyNg==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376176826", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                \"Will migrate out all assigned tasks and rejoin the consumer group.\");\n          \n          \n            \n                                \"Will close out all assigned tasks and rejoin the consumer group.\");", "author": "ableegoldman", "createdAt": "2020-02-07T01:45:30Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -745,14 +752,20 @@ private void runLoop() {\n             try {\n                 runOnce();\n                 if (assignmentErrorCode.get() == AssignorError.VERSION_PROBING.code()) {\n-                    log.info(\"Version probing detected. Triggering new rebalance.\");\n+                    log.info(\"Version probing detected. Rejoin the consumer group to trigger a new rebalance.\");\n+\n                     assignmentErrorCode.set(AssignorError.NONE.code());\n                     enforceRebalance();\n                 }\n-            } catch (final TaskMigratedException ignoreAndRejoinGroup) {\n-                log.warn(\"Detected task {} that got migrated to another thread. \" +\n-                             \"This implies that this thread missed a rebalance and dropped out of the consumer group. \" +\n-                             \"Will try to rejoin the consumer group.\", ignoreAndRejoinGroup.migratedTaskId());\n+            } catch (final TaskCorruptedException e) {\n+                log.warn(\"Detected the states of tasks {} are corrupted. \" +\n+                    \"Will close the task as dirty and re-create and bootstrap from scratch.\", e.corruptedTaskIds());\n+\n+                taskManager.handleCorruption(e.corruptedTaskIds());\n+            } catch (final TaskMigratedException e) {\n+                log.warn(\"Detected that the thread is being fenced. \" +\n+                    \"This implies that this thread missed a rebalance and dropped out of the consumer group. \" +\n+                    \"Will migrate out all assigned tasks and rejoin the consumer group.\");", "originalCommit": "112d9c6c4cc2113333561d2c4581131491fe217a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUwMjEyMA==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376502120", "bodyText": "Ack.", "author": "guozhangwang", "createdAt": "2020-02-07T17:02:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjE3NjgyNg=="}], "type": "inlineReview"}, {"oid": "a4af5a53069376147dcc2326cd2b6465dc1f0adb", "url": "https://github.com/apache/kafka/commit/a4af5a53069376147dcc2326cd2b6465dc1f0adb", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into KMinor-invalid-offset-changelog-reader", "committedDate": "2020-02-07T16:56:15Z", "type": "commit"}, {"oid": "64dad101c5c2d8282210c0e7b431fe1f4d5f036b", "url": "https://github.com/apache/kafka/commit/64dad101c5c2d8282210c0e7b431fe1f4d5f036b", "message": "PR comments", "committedDate": "2020-02-07T17:15:54Z", "type": "commit"}, {"oid": "47f3e0929ce262cbd2bfec8b122255ab6677907f", "url": "https://github.com/apache/kafka/commit/47f3e0929ce262cbd2bfec8b122255ab6677907f", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into KMinor-invalid-offset-changelog-reader", "committedDate": "2020-02-07T21:29:12Z", "type": "commit"}, {"oid": "134dda3187ff41addddcb7c022ff34cbea0154b3", "url": "https://github.com/apache/kafka/commit/134dda3187ff41addddcb7c022ff34cbea0154b3", "message": "remove FixedOrderMapTest", "committedDate": "2020-02-07T21:30:16Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4NjAyMw==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376686023", "bodyText": "Just now having this thought... Supposing this happens, is it guaranteed to apply to all the stores in the task? I.e., do we really need to re-bootstrap all the stores, or just the one(s) for which our offset is out of range?", "author": "vvcephei", "createdAt": "2020-02-08T04:20:08Z", "path": "streams/src/main/java/org/apache/kafka/streams/errors/TaskCorruptedException.java", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.errors;\n+\n+import org.apache.kafka.streams.processor.TaskId;\n+\n+import java.util.Set;\n+\n+/**\n+ * Indicates a specific task is corrupted and need to be re-initialized. It can be thrown when\n+ *\n+ * 1) Under EOS, if the checkpoint file does not contain offsets for corresponding store's changelogs, meaning\n+ *    previously it was not close cleanly;\n+ * 2) Out-of-range exception thrown during restoration, meaning that the changelog has been modified and we re-bootstrap", "originalCommit": "134dda3187ff41addddcb7c022ff34cbea0154b3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY5MzQzMg==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376693432", "bodyText": "Yes, we can re-bootstrap the state stores only -- this is what we did in the past but that was a lot messier (remember we have to use the optional in fixed-order map? :P). My thoughts are that for non-EOS, the checkpoint file would likely exist so even re-bootstrap the whole task would be okay, for EOS, it is safer to re-bootstrap the whole task.", "author": "guozhangwang", "createdAt": "2020-02-08T07:12:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4NjAyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzEyMjkxMw==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r377122913", "bodyText": "It's certainly safer, but the performance hit seems concerning... restoration i/o is already one of the things people complain about most, and this choice could amplify it multiple times over.\nMaybe we can handle it more cleanly by closing all the stores nicely, writing a checkpoint file with the out-of-range stores' checkpoints at 0, and then re-bootstrapping the task, so it only has to restore the broken stores?", "author": "vvcephei", "createdAt": "2020-02-10T15:13:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4NjAyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzMxOTExMA==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r377319110", "bodyText": "That sounds a good idea. Let me try it out.", "author": "guozhangwang", "createdAt": "2020-02-10T21:12:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4NjAyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4NjMzMA==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376686330", "bodyText": "I guess it wouldn't hurt to explain what the exception means (our position is too old and has been deleted or compacted by the broker) and what we hope to accomplish by marking the task as corrupted (to re-bootstrap the stores from the changelog and return to normal processing).", "author": "vvcephei", "createdAt": "2020-02-08T04:26:41Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -409,6 +413,15 @@ public void restore() {\n             } catch (final FencedInstanceIdException e) {\n                 // when the consumer gets fenced, all its tasks should be migrated\n                 throw new TaskMigratedException(\"Restore consumer get fenced by instance-id polling records.\", e);\n+            } catch (final InvalidOffsetException e) {\n+                log.warn(\"Encountered {} fetching records from restore consumer for partitions {}, \" +\n+                    \"marking the corresponding tasks as corrupted.\", e.getClass().getName(), e.partitions());", "originalCommit": "134dda3187ff41addddcb7c022ff34cbea0154b3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY5MzQzNQ==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r376693435", "bodyText": "Ack.", "author": "guozhangwang", "createdAt": "2020-02-08T07:12:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4NjMzMA=="}], "type": "inlineReview"}, {"oid": "29545718e2aaa6fb7fbc367a18846abaf95fc01f", "url": "https://github.com/apache/kafka/commit/29545718e2aaa6fb7fbc367a18846abaf95fc01f", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into KMinor-invalid-offset-changelog-reader", "committedDate": "2020-02-08T07:02:12Z", "type": "commit"}, {"oid": "e037be08e111b3d5cda3e7180d58d797dbd34dfd", "url": "https://github.com/apache/kafka/commit/e037be08e111b3d5cda3e7180d58d797dbd34dfd", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into KMinor-invalid-offset-changelog-reader", "committedDate": "2020-02-08T18:23:19Z", "type": "commit"}, {"oid": "a8d81c0dbb67c1da50e23bec0f49421c6f491a49", "url": "https://github.com/apache/kafka/commit/a8d81c0dbb67c1da50e23bec0f49421c6f491a49", "message": "address comments", "committedDate": "2020-02-08T18:28:39Z", "type": "commit"}, {"oid": "bbd19bc104d8ad962edfde4b72eb1c1f211f2f05", "url": "https://github.com/apache/kafka/commit/bbd19bc104d8ad962edfde4b72eb1c1f211f2f05", "message": "minor fix", "committedDate": "2020-02-08T18:29:17Z", "type": "commit"}, {"oid": "ae9457b5e59164a08085184e4d0c50910483b2a6", "url": "https://github.com/apache/kafka/commit/ae9457b5e59164a08085184e4d0c50910483b2a6", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into KMinor-invalid-offset-changelog-reader", "committedDate": "2020-02-10T21:12:44Z", "type": "commit"}, {"oid": "6b6c391334df1d1461f0587ac269d605ccb2c908", "url": "https://github.com/apache/kafka/commit/6b6c391334df1d1461f0587ac269d605ccb2c908", "message": "github comments", "committedDate": "2020-02-10T22:29:11Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM1Nzg5Ng==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r377357896", "bodyText": "We can checkpoint (excluding the corrupted partitions) for 1) standby tasks and 2) non-eos active tasks, for eos active tasks we should not write the checkpoint since for eos we HAVE TO reboot every store from scratch to maintain consistency.", "author": "guozhangwang", "createdAt": "2020-02-10T22:34:33Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -436,6 +436,15 @@ private void close(final boolean clean) {\n         transitionTo(State.CLOSED);\n     }\n \n+    @Override\n+    public void markChangelogAsCorrupted(final Set<TopicPartition> partitions) {\n+        stateMgr.markChangelogAsCorrupted(partitions);\n+\n+        // only write a new checkpoint (excluding the corrupted partitions) if eos is disabled", "originalCommit": "6b6c391334df1d1461f0587ac269d605ccb2c908", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "2fd2eae63c36c86798e5d6e331869602edb63aac", "url": "https://github.com/apache/kafka/commit/2fd2eae63c36c86798e5d6e331869602edb63aac", "message": "fix checkstyle", "committedDate": "2020-02-10T23:38:35Z", "type": "commit"}, {"oid": "f5ce3dbdf6b734aa8b8c0bb2d368adaae809ee68", "url": "https://github.com/apache/kafka/commit/f5ce3dbdf6b734aa8b8c0bb2d368adaae809ee68", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into KMinor-invalid-offset-changelog-reader", "committedDate": "2020-02-12T01:41:10Z", "type": "commit"}, {"oid": "d444d84a5f3bbcc9930c65db152bb5aeb73d3686", "url": "https://github.com/apache/kafka/commit/d444d84a5f3bbcc9930c65db152bb5aeb73d3686", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into KMinor-invalid-offset-changelog-reader", "committedDate": "2020-02-12T22:02:45Z", "type": "commit"}, {"oid": "40731e90c7123dc0ff88b2438ebd9c8d2d66ae5b", "url": "https://github.com/apache/kafka/commit/40731e90c7123dc0ff88b2438ebd9c8d2d66ae5b", "message": "comments", "committedDate": "2020-02-12T23:56:51Z", "type": "commit"}, {"oid": "8738cf58085da5d0a6c41439117de4a05d5b14b2", "url": "https://github.com/apache/kafka/commit/8738cf58085da5d0a6c41439117de4a05d5b14b2", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into KMinor-invalid-offset-changelog-reader", "committedDate": "2020-02-14T19:37:43Z", "type": "commit"}, {"oid": "9831638e2af3d59c85253bd809d3a6114c4ec98d", "url": "https://github.com/apache/kafka/commit/9831638e2af3d59c85253bd809d3a6114c4ec98d", "message": "move javadoc", "committedDate": "2020-02-14T20:57:17Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk4Nzc4MA==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r380987780", "bodyText": "Is this what you meant?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        log.info(\"Stream task {} is already in {} state, skip adding records to it.\", id(), state());\n          \n          \n            \n                        log.info(\"Stream task {} is already in {} state, skip processing it.\", id(), state());", "author": "vvcephei", "createdAt": "2020-02-18T23:02:11Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -460,6 +460,15 @@ private void close(final boolean clean) {\n      * source topic partitions, or if it is enforced to be processable\n      */\n     public boolean isProcessable(final long wallClockTime) {\n+        if (state() == State.CLOSED || state() == State.CLOSING) {\n+            // a task is only closing / closed when 1) task manager is closing, 2) a rebalance is undergoing;\n+            // in either case we can just log it and move on without notifying the thread since the consumer\n+            // would soon be updated to not return any records for this task anymore.\n+            log.info(\"Stream task {} is already in {} state, skip adding records to it.\", id(), state());", "originalCommit": "9831638e2af3d59c85253bd809d3a6114c4ec98d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk4ODg2MA==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r380988860", "bodyText": "Should we still skip adding records? It looks like that was the intent, but I think what actually would happen is that we'd still add the records, but skip processing them.", "author": "vvcephei", "createdAt": "2020-02-18T23:05:10Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -706,12 +715,6 @@ private void closeRecordCollector(final boolean clean) {\n      */\n     @Override\n     public void addRecords(final TopicPartition partition, final Iterable<ConsumerRecord<byte[], byte[]>> records) {\n-        if (state() == State.CLOSED || state() == State.CLOSING) {\n-            log.info(\"Stream task {} is already closed, probably because it got unexpectedly migrated to another thread already. \" +\n-                         \"Notifying the thread to trigger a new rebalance immediately.\", id());\n-            throw new TaskMigratedException(id());\n-        }\n-", "originalCommit": "9831638e2af3d59c85253bd809d3a6114c4ec98d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjMwMTU3MA==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r382301570", "bodyText": "This is intentional: as in this PR #8091 where we fixed the committing offset we would look into the buffered record so that we can get the correct \"next\" offset to commit, if we skip adding records here when the task is closing we would return incorrect results potentially.", "author": "guozhangwang", "createdAt": "2020-02-20T22:46:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk4ODg2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk4OTIxNg==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r380989216", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            \"and the thread is going to shutdown: \", e);\n          \n          \n            \n                            \"and the thread is going to shut down: \", e);\n          \n      \n    \n    \n  \n\nI'm a nerd, I know... \"shutdown\" is a noun, \"shut down\" is a verb phrase (\"down\" modifies the verb \"shut\").", "author": "vvcephei", "createdAt": "2020-02-18T23:06:06Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -718,14 +719,11 @@ public void run() {\n         try {\n             runLoop();\n             cleanRun = true;\n-        } catch (final KafkaException e) {\n-            log.error(\"Encountered the following unexpected Kafka exception during processing, \" +\n-                          \"this usually indicate Streams internal errors:\", e);\n-            throw e;\n         } catch (final Exception e) {\n             // we have caught all Kafka related exceptions, and other runtime exceptions\n             // should be due to user application errors\n-            log.error(\"Encountered the following error during processing:\", e);\n+            log.error(\"Encountered the following exception during processing \" +\n+                \"and the thread is going to shutdown: \", e);", "originalCommit": "9831638e2af3d59c85253bd809d3a6114c4ec98d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk5MDUwNg==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r380990506", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                log.info(\"Version probing detected. Rejoin the consumer group to trigger a new rebalance.\");\n          \n          \n            \n                                log.info(\"Version probing detected. Rejoining the consumer group to trigger a new rebalance.\");\n          \n      \n    \n    \n  \n\nIt sounded like you were telling the operator to do something, but you're just telling them that something is going to happen.", "author": "vvcephei", "createdAt": "2020-02-18T23:09:42Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -745,15 +743,22 @@ private void runLoop() {\n             try {\n                 runOnce();\n                 if (assignmentErrorCode.get() == AssignorError.VERSION_PROBING.code()) {\n-                    log.info(\"Version probing detected. Triggering new rebalance.\");\n+                    log.info(\"Version probing detected. Rejoin the consumer group to trigger a new rebalance.\");", "originalCommit": "9831638e2af3d59c85253bd809d3a6114c4ec98d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "946a944c52361e7998bc38f6664c6ad59dbb5874", "url": "https://github.com/apache/kafka/commit/946a944c52361e7998bc38f6664c6ad59dbb5874", "message": "further fix", "committedDate": "2020-02-19T00:00:42Z", "type": "commit"}, {"oid": "c7719c215917908b91d287e371c8ab64b4e93282", "url": "https://github.com/apache/kafka/commit/c7719c215917908b91d287e371c8ab64b4e93282", "message": "Update streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java\n\nCo-Authored-By: John Roesler <vvcephei@users.noreply.github.com>", "committedDate": "2020-02-20T22:36:39Z", "type": "commit"}, {"oid": "9210ceb07077ca44b1cdac3588a79c0e5867e851", "url": "https://github.com/apache/kafka/commit/9210ceb07077ca44b1cdac3588a79c0e5867e851", "message": "Update streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java\n\nCo-Authored-By: John Roesler <vvcephei@users.noreply.github.com>", "committedDate": "2020-02-20T22:36:52Z", "type": "commit"}, {"oid": "5f3628829892c4a96a51cfa95aba026e808a6510", "url": "https://github.com/apache/kafka/commit/5f3628829892c4a96a51cfa95aba026e808a6510", "message": "Update streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java\n\nCo-Authored-By: John Roesler <vvcephei@users.noreply.github.com>", "committedDate": "2020-02-20T22:37:05Z", "type": "commit"}, {"oid": "6bbc2f61e46516f5a8332373530bea18aca4e628", "url": "https://github.com/apache/kafka/commit/6bbc2f61e46516f5a8332373530bea18aca4e628", "message": "rebase from trunk", "committedDate": "2020-02-20T22:38:13Z", "type": "commit"}, {"oid": "f4049d00c74220400cc34103fdf9effcbaa43bf8", "url": "https://github.com/apache/kafka/commit/f4049d00c74220400cc34103fdf9effcbaa43bf8", "message": "Merge branch 'KMinor-invalid-offset-changelog-reader' of https://github.com/guozhangwang/kafka into KMinor-invalid-offset-changelog-reader", "committedDate": "2020-02-20T22:38:16Z", "type": "commit"}, {"oid": "03f4778cba8697bad6e5c5d7ce40df6e59214c02", "url": "https://github.com/apache/kafka/commit/03f4778cba8697bad6e5c5d7ce40df6e59214c02", "message": "add unit tests", "committedDate": "2020-02-20T23:03:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjMwMzEyOQ==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r382303129", "bodyText": "Here is the attempted fix of https://issues.apache.org/jira/browse/KAFKA-9572: if we are closing / suspending a restoring task, we should only update the checkpoint file but should NOT commit offsets, since the committed offsets indicate the  \"restore end\" and should not be updated, cc @cadonna who filed the JIRA.", "author": "guozhangwang", "createdAt": "2020-02-20T22:50:45Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -239,25 +236,32 @@ public void suspend() {\n         if (state() == State.CREATED || state() == State.CLOSING || state() == State.SUSPENDED) {\n             // do nothing\n             log.trace(\"Skip suspending since state is {}\", state());\n+        } else if (state() == State.RUNNING) {", "originalCommit": "f4049d00c74220400cc34103fdf9effcbaa43bf8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjMwMzQwMw==", "url": "https://github.com/apache/kafka/pull/8058#discussion_r382303403", "bodyText": "This is part of the fix as well: only flushing / checkpointing, but not committing.", "author": "guozhangwang", "createdAt": "2020-02-20T22:51:30Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -413,26 +417,23 @@ private void close(final boolean clean) {\n         } else {\n             if (state() == State.RUNNING) {\n                 closeTopology(clean);\n-            }\n \n-            if (state() == State.RUNNING || state() == State.RESTORING) {\n                 if (clean) {\n                     commitState();\n                     // whenever we have successfully committed state, it is safe to checkpoint\n                     // the state as well no matter if EOS is enabled or not\n                     stateMgr.checkpoint(checkpointableOffsets());\n-                } else if (eosDisabled) {\n-                    // if from unclean close, then only need to flush state to make sure that when later\n-                    // closing the states, there's no records triggering any processing anymore; also swallow all caught exceptions\n-                    // However, for a _clean_ shutdown, we try to commit and checkpoint. If there are any exceptions, they become\n-                    // fatal for the \"closeClean()\" call, and the caller can try again with closeDirty() to complete the shutdown.\n-                    try {\n-                        stateMgr.flush();\n-                    } catch (final RuntimeException error) {\n-                        log.debug(\"Ignoring flush error in unclean close.\", error);\n-                    }\n+                } else {\n+                    executeAndMaybeSwallow(false, stateMgr::flush, \"state manager flush\");\n                 }\n \n+                transitionTo(State.CLOSING);\n+            } else if (state() == State.RESTORING) {", "originalCommit": "f4049d00c74220400cc34103fdf9effcbaa43bf8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}