{"pr_number": 14495, "pr_title": "Initial implementation of Broadcast join for Presto on Spark", "pr_createdAt": "2020-05-07T13:54:21Z", "pr_url": "https://github.com/prestodb/presto/pull/14495", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgxNjA5Nw==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r421816097", "bodyText": "what about naming them as shuffleInputs and broadcastInputs or some thing similar? :)", "author": "wenleix", "createdAt": "2020-05-07T21:51:33Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRemoteSourceFactory.java", "diffHunk": "@@ -29,24 +32,46 @@\n import java.util.Map;\n \n import static com.facebook.presto.spark.util.PrestoSparkUtils.transformRowsToPages;\n-import static java.lang.String.format;\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.Iterators.concat;\n+import static com.google.common.collect.Iterators.transform;\n import static java.util.Objects.requireNonNull;\n \n public class PrestoSparkRemoteSourceFactory\n         implements RemoteSourceFactory\n {\n-    private final Map<PlanNodeId, Iterator<PrestoSparkRow>> inputs;\n+    private final PagesSerde pagesSerde;\n+    private final Map<PlanNodeId, Iterator<PrestoSparkRow>> rowInputs;", "originalCommit": "bb5b91389336b0fefa3f4c47d052b3cef1e333cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgxODUzNQ==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r421818535", "bodyText": "nit: One way to avoid nested transform is to inline the operation with lambda:\n            Iterator<Page> iterator = transform(\n                    pageInputs.get(planNodeId),\n                    prestoSparkSerializedPage -> pagesSerde.deserialize(toSerializedPage(prestoSparkSerializedPage))\n            );\nBut I can see why you want to use nested transform, and I have no strong opinion here :)", "author": "wenleix", "createdAt": "2020-05-07T21:57:11Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRemoteSourceFactory.java", "diffHunk": "@@ -29,24 +32,46 @@\n import java.util.Map;\n \n import static com.facebook.presto.spark.util.PrestoSparkUtils.transformRowsToPages;\n-import static java.lang.String.format;\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.Iterators.concat;\n+import static com.google.common.collect.Iterators.transform;\n import static java.util.Objects.requireNonNull;\n \n public class PrestoSparkRemoteSourceFactory\n         implements RemoteSourceFactory\n {\n-    private final Map<PlanNodeId, Iterator<PrestoSparkRow>> inputs;\n+    private final PagesSerde pagesSerde;\n+    private final Map<PlanNodeId, Iterator<PrestoSparkRow>> rowInputs;\n+    private final Map<PlanNodeId, Iterator<PrestoSparkSerializedPage>> pageInputs;\n \n-    public PrestoSparkRemoteSourceFactory(Map<PlanNodeId, Iterator<PrestoSparkRow>> inputs)\n+    public PrestoSparkRemoteSourceFactory(\n+            PagesSerde pagesSerde,\n+            Map<PlanNodeId, Iterator<PrestoSparkRow>> rowInputs,\n+            Map<PlanNodeId, Iterator<PrestoSparkSerializedPage>> pageInputs)\n     {\n-        this.inputs = ImmutableMap.copyOf(requireNonNull(inputs, \"inputs is null\"));\n+        this.pagesSerde = requireNonNull(pagesSerde, \"pagesSerde is null\");\n+        this.rowInputs = ImmutableMap.copyOf(requireNonNull(rowInputs, \"rowInputs is null\"));\n+        this.pageInputs = ImmutableMap.copyOf(requireNonNull(pageInputs, \"pageInputs is null\"));\n     }\n \n     @Override\n     public OperatorFactory createRemoteSource(Session session, int operatorId, PlanNodeId planNodeId, List<Type> types)\n     {\n-        Iterator<PrestoSparkRow> rowsIterator = requireNonNull(inputs.get(planNodeId), format(\"input is missing for plan node: %s\", planNodeId));\n-        Iterator<Page> pagesIterator = transformRowsToPages(rowsIterator, types);\n+        Iterator<Page> pagesIterator = null;\n+        if (rowInputs.containsKey(planNodeId)) {\n+            Iterator<PrestoSparkRow> rowsIterator = rowInputs.get(planNodeId);\n+            pagesIterator = transformRowsToPages(rowsIterator, types);\n+        }\n+        if (pageInputs.containsKey(planNodeId)) {\n+            Iterator<Page> iterator = transform(transform(pageInputs.get(planNodeId), PrestoSparkUtils::toSerializedPage), pagesSerde::deserialize);", "originalCommit": "bb5b91389336b0fefa3f4c47d052b3cef1e333cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgxOTc5MQ==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r421819791", "bodyText": "curious: in which situation the planNodeId is only contained in broadcastInputs but not shuffleInputs?", "author": "wenleix", "createdAt": "2020-05-07T22:00:05Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRemoteSourceFactory.java", "diffHunk": "@@ -29,24 +32,46 @@\n import java.util.Map;\n \n import static com.facebook.presto.spark.util.PrestoSparkUtils.transformRowsToPages;\n-import static java.lang.String.format;\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.Iterators.concat;\n+import static com.google.common.collect.Iterators.transform;\n import static java.util.Objects.requireNonNull;\n \n public class PrestoSparkRemoteSourceFactory\n         implements RemoteSourceFactory\n {\n-    private final Map<PlanNodeId, Iterator<PrestoSparkRow>> inputs;\n+    private final PagesSerde pagesSerde;\n+    private final Map<PlanNodeId, Iterator<PrestoSparkRow>> rowInputs;\n+    private final Map<PlanNodeId, Iterator<PrestoSparkSerializedPage>> pageInputs;\n \n-    public PrestoSparkRemoteSourceFactory(Map<PlanNodeId, Iterator<PrestoSparkRow>> inputs)\n+    public PrestoSparkRemoteSourceFactory(\n+            PagesSerde pagesSerde,\n+            Map<PlanNodeId, Iterator<PrestoSparkRow>> rowInputs,\n+            Map<PlanNodeId, Iterator<PrestoSparkSerializedPage>> pageInputs)\n     {\n-        this.inputs = ImmutableMap.copyOf(requireNonNull(inputs, \"inputs is null\"));\n+        this.pagesSerde = requireNonNull(pagesSerde, \"pagesSerde is null\");\n+        this.rowInputs = ImmutableMap.copyOf(requireNonNull(rowInputs, \"rowInputs is null\"));\n+        this.pageInputs = ImmutableMap.copyOf(requireNonNull(pageInputs, \"pageInputs is null\"));\n     }\n \n     @Override\n     public OperatorFactory createRemoteSource(Session session, int operatorId, PlanNodeId planNodeId, List<Type> types)\n     {\n-        Iterator<PrestoSparkRow> rowsIterator = requireNonNull(inputs.get(planNodeId), format(\"input is missing for plan node: %s\", planNodeId));\n-        Iterator<Page> pagesIterator = transformRowsToPages(rowsIterator, types);\n+        Iterator<Page> pagesIterator = null;\n+        if (rowInputs.containsKey(planNodeId)) {\n+            Iterator<PrestoSparkRow> rowsIterator = rowInputs.get(planNodeId);\n+            pagesIterator = transformRowsToPages(rowsIterator, types);\n+        }\n+        if (pageInputs.containsKey(planNodeId)) {\n+            Iterator<Page> iterator = transform(transform(pageInputs.get(planNodeId), PrestoSparkUtils::toSerializedPage), pagesSerde::deserialize);\n+            if (pagesIterator == null) {", "originalCommit": "bb5b91389336b0fefa3f4c47d052b3cef1e333cc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM5OTY3NA==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r422399674", "bodyText": "Update: As per discussed offline, a simple A JOIN B with B get broadcasted will cause this. This is because table A is directly read rather than from a shuffle input.", "author": "wenleix", "createdAt": "2020-05-08T22:13:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgxOTc5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgyMTYxMg==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r421821612", "bodyText": "So, it looks like shuffle input and broadcast input will be present EXACTLY once. Two questions:\n\nCan we guard this condition? (e.g. One and only input will be present -- either broadcast or shuffle)\nIn PrestoSparkRemoteSourceFactory , from the code it looks like both shuffle input and broadcast input can present?", "author": "wenleix", "createdAt": "2020-05-07T22:04:33Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java", "diffHunk": "@@ -228,14 +237,33 @@ public IPrestoSparkTaskExecutor create(\n         PrestoSparkRowBuffer rowBuffer = new PrestoSparkRowBuffer(memoryManager);\n \n         ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkRow>> shuffleInputs = ImmutableMap.builder();\n+        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkSerializedPage>> broadcastInputs = ImmutableMap.builder();\n         for (RemoteSourceNode remoteSource : fragment.getRemoteSourceNodes()) {\n-            ImmutableList.Builder<Iterator<PrestoSparkRow>> remoteSourceInputs = ImmutableList.builder();\n+            List<Iterator<PrestoSparkRow>> shuffleRemoteSourceInputs = new ArrayList<>();\n+            List<Iterator<PrestoSparkSerializedPage>> broadcastRemoteSourceInputs = new ArrayList<>();\n             for (PlanFragmentId sourceFragmentId : remoteSource.getSourceFragmentIds()) {\n-                Iterator<Tuple2<Integer, PrestoSparkRow>> input = inputs.getShuffleInputs().get(sourceFragmentId.toString());\n-                checkArgument(input != null, \"input is missing for fragmentId: %s\", sourceFragmentId);\n-                remoteSourceInputs.add(Iterators.transform(input, tuple -> tuple._2));\n+                Iterator<Tuple2<Integer, PrestoSparkRow>> shuffleInput = inputs.getShuffleInputs().get(sourceFragmentId.toString());\n+                if (shuffleInput != null) {\n+                    shuffleRemoteSourceInputs.add(Iterators.transform(shuffleInput, tuple -> tuple._2));\n+                    continue;", "originalCommit": "bb5b91389336b0fefa3f4c47d052b3cef1e333cc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjIxMDEzMA==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r422210130", "bodyText": "Currently I'm pretty sure Presto doesn't create such plans. But those are theoretically possible. For example when planning union of a table and values.\nCurrently the query like\nSELECT *\nFROM table_a\nJOIN (\nSELECT * FROM table b\nUNION ALL\nSELECT * FROM VALUES (...)\n)\n\nresults in an extra stage that gathers SELECT * FROM table b and SELECT * FROM VALUES (...) into a single node. However in theory this extra exchange is not needed. The broadcast RemoteSource can pull the data directly from 2 stages that are set to have Broadcast output buffer.\nI'm considering to improving planning. Ideally on the broadcast join we shouldn't have\n... RemoteExchange[SINGLE] -> RemoteExchange[BROADCAST] ..., but instead only a single RemoteExchange[BROADCAST] that takes two inputs.", "author": "arhimondr", "createdAt": "2020-05-08T15:30:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgyMTYxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjcxMzA3OA==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r422713078", "bodyText": "@arhimondr\n\nCurrently I'm pretty sure Presto doesn't create such plans.\n\nBut looks like for a remote source, there can be both shuffle and broadcast input: bb5b913#r421819791. Just want to double check this is expected?", "author": "wenleix", "createdAt": "2020-05-10T23:01:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgyMTYxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzExMjg1NA==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423112854", "bodyText": "Actually yeah. I don't think it is possible for a single RemoteSource to accept both, broadcast and shuffle inputs.", "author": "arhimondr", "createdAt": "2020-05-11T15:12:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgyMTYxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzExOTExNw==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423119117", "bodyText": "Refactored", "author": "arhimondr", "createdAt": "2020-05-11T15:21:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgyMTYxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM4MjY4Mg==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423382682", "bodyText": "@arhimondr : Great. It's  about keep the logic consistent in these two places .", "author": "wenleix", "createdAt": "2020-05-11T23:47:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgyMTYxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTk0MTU3Mg==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r421941572", "bodyText": "It loos like this Iterators.concat is essentially doing a \"flatmap\" operation over all the remote source inputs.\nI understand for shuffle remote source, there can be one RemoteSourceNode corresponding to multiple inputs (e.g. UNION). But will this also be the case for broadcast remote source ?", "author": "wenleix", "createdAt": "2020-05-08T05:11:25Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java", "diffHunk": "@@ -228,14 +237,33 @@ public IPrestoSparkTaskExecutor create(\n         PrestoSparkRowBuffer rowBuffer = new PrestoSparkRowBuffer(memoryManager);\n \n         ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkRow>> shuffleInputs = ImmutableMap.builder();\n+        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkSerializedPage>> broadcastInputs = ImmutableMap.builder();\n         for (RemoteSourceNode remoteSource : fragment.getRemoteSourceNodes()) {\n-            ImmutableList.Builder<Iterator<PrestoSparkRow>> remoteSourceInputs = ImmutableList.builder();\n+            List<Iterator<PrestoSparkRow>> shuffleRemoteSourceInputs = new ArrayList<>();\n+            List<Iterator<PrestoSparkSerializedPage>> broadcastRemoteSourceInputs = new ArrayList<>();\n             for (PlanFragmentId sourceFragmentId : remoteSource.getSourceFragmentIds()) {\n-                Iterator<Tuple2<Integer, PrestoSparkRow>> input = inputs.getShuffleInputs().get(sourceFragmentId.toString());\n-                checkArgument(input != null, \"input is missing for fragmentId: %s\", sourceFragmentId);\n-                remoteSourceInputs.add(Iterators.transform(input, tuple -> tuple._2));\n+                Iterator<Tuple2<Integer, PrestoSparkRow>> shuffleInput = inputs.getShuffleInputs().get(sourceFragmentId.toString());\n+                if (shuffleInput != null) {\n+                    shuffleRemoteSourceInputs.add(Iterators.transform(shuffleInput, tuple -> tuple._2));\n+                    continue;\n+                }\n+                Broadcast<List<PrestoSparkSerializedPage>> broadcastInput = inputs.getBroadcastInputs().get(sourceFragmentId.toString());\n+                if (broadcastInput != null) {\n+                    // TODO: Enable NullifyingIterator once migrated to one task per JVM model\n+                    // NullifyingIterator removes element from the list upon return\n+                    // This allows GC to gradually reclaim memory\n+                    // broadcastRemoteSourceInputs.add(getNullifyingIterator(broadcastInput.value()));\n+                    broadcastRemoteSourceInputs.add(broadcastInput.value().iterator());\n+                    continue;\n+                }\n+                throw new IllegalArgumentException(\"Input not found for sourceFragmentId: \" + sourceFragmentId);\n+            }\n+            if (!shuffleRemoteSourceInputs.isEmpty()) {\n+                shuffleInputs.put(remoteSource.getId(), Iterators.concat(shuffleRemoteSourceInputs.iterator()));\n+            }\n+            if (!broadcastRemoteSourceInputs.isEmpty()) {\n+                broadcastInputs.put(remoteSource.getId(), Iterators.concat(broadcastRemoteSourceInputs.iterator()));", "originalCommit": "bb5b91389336b0fefa3f4c47d052b3cef1e333cc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjIxMDU3Mg==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r422210572", "bodyText": "The answer is same as for the previous comment. But instead of VALUES think of an one more table.", "author": "arhimondr", "createdAt": "2020-05-08T15:31:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTk0MTU3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTk0MzM0OA==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r421943348", "bodyText": "Curious: can we just do actualInputs.equals(expectedInputs)?", "author": "wenleix", "createdAt": "2020-05-08T05:18:18Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -220,13 +227,16 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n                 .flatMap(List::stream)\n                 .collect(toImmutableSet());\n \n-        Set<PlanFragmentId> missingInputs = difference(expectedInputs, rddInputs.keySet());\n-        Set<PlanFragmentId> extraInputs = difference(rddInputs.keySet(), expectedInputs);\n+        Set<PlanFragmentId> actualInputs = union(rddInputs.keySet(), broadcastInputs.keySet());\n+\n+        Set<PlanFragmentId> missingInputs = difference(expectedInputs, actualInputs);\n+        Set<PlanFragmentId> extraInputs = difference(actualInputs, expectedInputs);\n         checkArgument(\n                 missingInputs.isEmpty() && extraInputs.isEmpty(),", "originalCommit": "bb5b91389336b0fefa3f4c47d052b3cef1e333cc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjIxMTEyMw==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r422211123", "bodyText": "I was going to add missingInputs  and extraInputs  to the error message, but then forgot. Let me do this. It might be convenient when debugging.", "author": "arhimondr", "createdAt": "2020-05-08T15:32:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTk0MzM0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4MzU2Nw==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r422783567", "bodyText": "@arhimondr : In that case maybe consider refactor the check logic into a separate method (e.g. checkRddInputs to keep the core logic straightforward? :)", "author": "wenleix", "createdAt": "2020-05-11T05:09:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTk0MzM0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTk0MzgxMQ==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r421943811", "bodyText": "nit: is it possible to check all remote sources node is reading from broadcast source? If the check is not easy then no worry.", "author": "wenleix", "createdAt": "2020-05-08T05:19:48Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -269,11 +281,9 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n             PlanFragment fragment,\n             PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n             CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-            TableWriteInfo tableWriteInfo)\n+            TableWriteInfo tableWriteInfo,\n+            Map<PlanFragmentId, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n     {\n-        // TODO: Possible in case of a broadcast join\n-        checkArgument(fragment.getRemoteSourceNodes().isEmpty(), \"source task with remote sources is not supported\");", "originalCommit": "bb5b91389336b0fefa3f4c47d052b3cef1e333cc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzEyMjc5MQ==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423122791", "bodyText": "Refactored inputs check in a separate method. Calling it here as well.", "author": "arhimondr", "createdAt": "2020-05-11T15:26:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTk0MzgxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTk0NDQxOA==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r421944418", "bodyText": "nit: what about add a new line before .collect?", "author": "wenleix", "createdAt": "2020-05-08T05:22:09Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -377,4 +390,9 @@ private PrestoSparkTaskDescriptor createSourceTaskDescriptor(\n                 .where(TableScanNode.class::isInstance)\n                 .findAll();\n     }\n+\n+    private static Map<String, Broadcast<List<PrestoSparkSerializedPage>>> toTaskProcessorBroadcastInputs(Map<PlanFragmentId, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+    {\n+        return broadcastInputs.entrySet().stream().collect(toImmutableMap(entry -> entry.getKey().toString(), Map.Entry::getValue));", "originalCommit": "bb5b91389336b0fefa3f4c47d052b3cef1e333cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTk0NTQxMA==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r421945410", "bodyText": "nit: consider use BlockUtil#compactArray", "author": "wenleix", "createdAt": "2020-05-08T05:25:46Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/util/PrestoSparkUtils.java", "diffHunk": "@@ -58,4 +64,48 @@ protected Page computeNext()\n             }\n         };\n     }\n+\n+    public static PrestoSparkSerializedPage toPrestoSparkSerializedPage(SerializedPage serializedPage)\n+    {\n+        Slice slice = serializedPage.getSlice();\n+        checkArgument(slice.hasByteArray(), \"slice is expected to be based on a byte array\");\n+        byte[] array = slice.byteArray();\n+        if (slice.byteArrayOffset() != 0 || slice.length() != array.length) {\n+            array = copyOfRange(array, slice.byteArrayOffset(), slice.byteArrayOffset() + slice.length());", "originalCommit": "bb5b91389336b0fefa3f4c47d052b3cef1e333cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTk1Njc4Mg==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r421956782", "bodyText": "It looks to me this method actually returns two values:\n\nAn RDD JavaPairRDD<Integer, PrestoSparkRow> that transform Presto SubPlan to Spark RDD.\nA List of Broadcast, indicates \"we have done these Broadcast in preparing for executing this RDD\". As a result,  once the returned RDD finish execution, the broadcasts should be destroyed.\n\nHowever, this seems a bit difficult to understand -- as the broadcasts list is stateful and get appended in the recursive call. I am wondering if it is possible to have a RddAndBroadcast class to return both the RDD and the broadcasts? (Or even RddAndMore, see our TableAndMore, PartitionAndMore classes in SemiTransactionMetastore :D )", "author": "wenleix", "createdAt": "2020-05-08T06:05:06Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java", "diffHunk": "@@ -385,29 +403,54 @@ private PrestoSparkQueryExecution(\n                 SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(sparkTaskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n                 SubPlan child = getOnlyElement(root.getChildren());\n-                JavaPairRDD<Integer, PrestoSparkRow> rdd = createRdd(child);\n+                JavaPairRDD<Integer, PrestoSparkRow> rdd = createRdd(child, broadcastsToDestroy);\n                 List<Tuple2<Integer, PrestoSparkRow>> sparkDriverInput = rdd.collect();\n+                broadcastsToDestroy.forEach(Broadcast::destroy);\n                 return ImmutableList.copyOf(executorFactoryProvider.get().create(\n                         0,\n                         0,\n                         serializedTaskDescriptor,\n-                        new PrestoSparkTaskInputs(ImmutableMap.of(child.getFragment().getId().toString(), sparkDriverInput.iterator())),\n+                        new PrestoSparkTaskInputs(ImmutableMap.of(child.getFragment().getId().toString(), sparkDriverInput.iterator()), ImmutableMap.of()),\n                         taskStatsCollector));\n             }\n \n-            return createRdd(root).collect();\n+            List<Tuple2<Integer, PrestoSparkRow>> result = createRdd(root, broadcastsToDestroy).collect();\n+            broadcastsToDestroy.forEach(Broadcast::destroy);\n+            return result;\n         }\n \n-        private JavaPairRDD<Integer, PrestoSparkRow> createRdd(SubPlan subPlan)\n+        private JavaPairRDD<Integer, PrestoSparkRow> createRdd(SubPlan subPlan, List<Broadcast<?>> broadcasts)", "originalCommit": "bb5b91389336b0fefa3f4c47d052b3cef1e333cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjcxMzY3OQ==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r422713679", "bodyText": "nit: what about fragementIdToBroadcastInputs? Same for the other variables in this class.", "author": "wenleix", "createdAt": "2020-05-10T23:07:02Z", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/TaskProcessors.java", "diffHunk": "@@ -34,7 +37,8 @@ private TaskProcessors() {}\n \n     public static PairFlatMapFunction<Iterator<SerializedPrestoSparkTaskDescriptor>, Integer, PrestoSparkRow> createTaskProcessor(\n             PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector)\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)", "originalCommit": "bb5b91389336b0fefa3f4c47d052b3cef1e333cc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzEzOTI3OQ==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423139279", "bodyText": "Added a comment instead", "author": "arhimondr", "createdAt": "2020-05-11T15:50:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjcxMzY3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4MjEzNw==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r422782137", "bodyText": "Similar to https://github.com/apache/spark/blob/5d5866be12259c40972f7404f64d830cab87401f/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala#L109-L112 , it might help to have some sanity check on data being broadcasted to prevent Driver from OOM. This can be left as a future work.", "author": "wenleix", "createdAt": "2020-05-11T05:03:47Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java", "diffHunk": "@@ -385,29 +403,54 @@ private PrestoSparkQueryExecution(\n                 SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(sparkTaskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n                 SubPlan child = getOnlyElement(root.getChildren());\n-                JavaPairRDD<Integer, PrestoSparkRow> rdd = createRdd(child);\n+                JavaPairRDD<Integer, PrestoSparkRow> rdd = createRdd(child, broadcastsToDestroy);\n                 List<Tuple2<Integer, PrestoSparkRow>> sparkDriverInput = rdd.collect();\n+                broadcastsToDestroy.forEach(Broadcast::destroy);\n                 return ImmutableList.copyOf(executorFactoryProvider.get().create(\n                         0,\n                         0,\n                         serializedTaskDescriptor,\n-                        new PrestoSparkTaskInputs(ImmutableMap.of(child.getFragment().getId().toString(), sparkDriverInput.iterator())),\n+                        new PrestoSparkTaskInputs(ImmutableMap.of(child.getFragment().getId().toString(), sparkDriverInput.iterator()), ImmutableMap.of()),\n                         taskStatsCollector));\n             }\n \n-            return createRdd(root).collect();\n+            List<Tuple2<Integer, PrestoSparkRow>> result = createRdd(root, broadcastsToDestroy).collect();\n+            broadcastsToDestroy.forEach(Broadcast::destroy);\n+            return result;\n         }\n \n-        private JavaPairRDD<Integer, PrestoSparkRow> createRdd(SubPlan subPlan)\n+        private JavaPairRDD<Integer, PrestoSparkRow> createRdd(SubPlan subPlan, List<Broadcast<?>> broadcasts)\n         {\n-            PlanFragment fragment = subPlan.getFragment();\n-            Map<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> rddInputs = subPlan.getChildren().stream()\n-                    .collect(toImmutableMap(children -> children.getFragment().getId(), this::createRdd));\n+            ImmutableMap.Builder<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> rddInputs = ImmutableMap.builder();\n+            ImmutableMap.Builder<PlanFragmentId, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs = ImmutableMap.builder();\n+            for (SubPlan child : subPlan.getChildren()) {\n+                PlanFragment childFragment = child.getFragment();\n+                if (childFragment.getPartitioningScheme().getPartitioning().getHandle().equals(FIXED_BROADCAST_DISTRIBUTION)) {\n+                    List<Broadcast<?>> broadcastsToDestroy = new ArrayList<>();\n+                    JavaPairRDD<Integer, PrestoSparkRow> childRdd = createRdd(child, broadcastsToDestroy);\n+                    List<Tuple2<Integer, PrestoSparkRow>> broadcastRows = childRdd.collect();\n+                    // TODO: Transform rows to pages on executors (using `RDD#map` function)\n+                    // TODO: Transforming it on coordinator results in 2x memory utilization as both,\n+                    // TODO: rows and pages have to be kept in memory all at the same time\n+                    Iterator<Page> pagesIterator = transformRowsToPages(transform(broadcastRows.iterator(), Tuple2::_2), childFragment.getTypes());\n+                    Iterator<PrestoSparkSerializedPage> serializedPagesIterator = transform(transform(pagesIterator, pagesSerde::serialize), PrestoSparkUtils::toPrestoSparkSerializedPage);\n+                    List<PrestoSparkSerializedPage> serializedPages = new ArrayList<>();\n+                    serializedPagesIterator.forEachRemaining(serializedPages::add);\n+                    Broadcast<List<PrestoSparkSerializedPage>> broadcast = sparkContext.broadcast(serializedPages);", "originalCommit": "bb5b91389336b0fefa3f4c47d052b3cef1e333cc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzE0MDYwMw==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423140603", "bodyText": "I don't like checking the row size, and I don't think there's a way to figure out the actual size in bytes in advance (@sameeragarwal ?). I don't like limiting the number of rows, as the row size could be anywhere from several bytes to hundred of kilobytes. Thus it doesn't give us much.", "author": "arhimondr", "createdAt": "2020-05-11T15:52:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4MjEzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM4MDgwMg==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423380802", "bodyText": "@arhimondr\n\nI don't like checking the row size\n\nDo you mean the size of byte array after serializing page?  Doesn't the array length of PrestoSparkSerializedPage#bytes work?", "author": "wenleix", "createdAt": "2020-05-11T23:42:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4MjEzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM4MTU1NQ==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423381555", "bodyText": "It is too late at that step. The data is already on the driver. If we expect executors to have less memory available than a driver we can add a check. But that might be cluster specific.", "author": "arhimondr", "createdAt": "2020-05-11T23:44:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4MjEzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM4Njk2Nw==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423386967", "bodyText": "@arhimondr : I see. So it won't protect us from OOM. It may still help with organic growth of the broadcast table (e.g. the broadcast table gradually grows from 10G to 20G...). So it may still somewhat help a little but the value is not that large.", "author": "wenleix", "createdAt": "2020-05-12T00:01:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4MjEzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4NDI0MA==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r422784240", "bodyText": "When supporting bucket table join unbucketed table, does the join stage consider as SourceTask? -- it has to read from table, but also it's sort of a \"intermediate task\" since it has shuffle inputs.", "author": "wenleix", "createdAt": "2020-05-11T05:12:09Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -15,283 +15,367 @@\n \n import com.facebook.airlift.json.JsonCodec;\n import com.facebook.presto.Session;\n+import com.facebook.presto.execution.Lifespan;\n import com.facebook.presto.execution.ScheduledSplit;\n import com.facebook.presto.execution.TaskSource;\n import com.facebook.presto.execution.scheduler.TableWriteInfo;\n+import com.facebook.presto.metadata.Metadata;\n+import com.facebook.presto.metadata.Split;\n import com.facebook.presto.spark.PrestoSparkTaskDescriptor;\n import com.facebook.presto.spark.classloader_interface.IntegerIdentityPartitioner;\n import com.facebook.presto.spark.classloader_interface.PrestoSparkRow;\n import com.facebook.presto.spark.classloader_interface.PrestoSparkTaskExecutorFactoryProvider;\n import com.facebook.presto.spark.classloader_interface.SerializedPrestoSparkTaskDescriptor;\n import com.facebook.presto.spark.classloader_interface.SerializedTaskStats;\n+import com.facebook.presto.spi.PrestoException;\n+import com.facebook.presto.spi.plan.PlanNode;\n import com.facebook.presto.spi.plan.PlanNodeId;\n+import com.facebook.presto.spi.plan.TableScanNode;\n+import com.facebook.presto.split.SplitManager;\n+import com.facebook.presto.split.SplitSource;\n import com.facebook.presto.sql.planner.PartitioningHandle;\n import com.facebook.presto.sql.planner.PlanFragment;\n+import com.facebook.presto.sql.planner.SystemPartitioningHandle;\n import com.facebook.presto.sql.planner.plan.PlanFragmentId;\n import com.facebook.presto.sql.planner.plan.RemoteSourceNode;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableSet;\n+import org.apache.spark.Partitioner;\n import org.apache.spark.api.java.JavaPairRDD;\n import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction2;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n \n import javax.inject.Inject;\n \n import java.util.ArrayList;\n+import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n-import java.util.Objects;\n import java.util.Optional;\n import java.util.Set;\n import java.util.stream.Collectors;\n import java.util.stream.IntStream;\n \n+import static com.facebook.airlift.concurrent.MoreFutures.getFutureValue;\n import static com.facebook.presto.SystemSessionProperties.getHashPartitionCount;\n import static com.facebook.presto.spark.PrestoSparkSessionProperties.getSparkInitialPartitionCount;\n import static com.facebook.presto.spark.classloader_interface.TaskProcessors.createTaskProcessor;\n+import static com.facebook.presto.spi.StandardErrorCode.NOT_SUPPORTED;\n+import static com.facebook.presto.spi.connector.ConnectorSplitManager.SplitSchedulingStrategy.UNGROUPED_SCHEDULING;\n+import static com.facebook.presto.spi.connector.NotPartitionedPartitionHandle.NOT_PARTITIONED;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.ARBITRARY_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.COORDINATOR_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_ARBITRARY_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_BROADCAST_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_HASH_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_PASSTHROUGH_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.SCALED_WRITER_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.SINGLE_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.SOURCE_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.optimizations.PlanNodeSearcher.searchFrom;\n import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Verify.verify;\n import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableMap.toImmutableMap;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n import static com.google.common.collect.Iterables.getOnlyElement;\n+import static com.google.common.collect.Sets.difference;\n+import static java.lang.String.format;\n+import static java.util.Collections.shuffle;\n import static java.util.Objects.requireNonNull;\n import static java.util.function.Function.identity;\n import static java.util.stream.Collectors.mapping;\n import static java.util.stream.Collectors.toSet;\n \n public class PrestoSparkRddFactory\n {\n-    private final JsonCodec<PrestoSparkTaskDescriptor> sparkTaskRequestJsonCodec;\n+    private final SplitManager splitManager;\n+    private final Metadata metadata;\n+    private final JsonCodec<PrestoSparkTaskDescriptor> taskDescriptorJsonCodec;\n \n     @Inject\n-    public PrestoSparkRddFactory(JsonCodec<PrestoSparkTaskDescriptor> sparkTaskRequestJsonCodec)\n+    public PrestoSparkRddFactory(SplitManager splitManager, Metadata metadata, JsonCodec<PrestoSparkTaskDescriptor> taskDescriptorJsonCodec)\n     {\n-        this.sparkTaskRequestJsonCodec = requireNonNull(sparkTaskRequestJsonCodec, \"sparkTaskRequestJsonCodec is null\");\n+        this.splitManager = requireNonNull(splitManager, \"splitManager is null\");\n+        this.metadata = requireNonNull(metadata, \"metadata is null\");\n+        this.taskDescriptorJsonCodec = requireNonNull(taskDescriptorJsonCodec, \"taskDescriptorJsonCodec is null\");\n     }\n \n     public JavaPairRDD<Integer, PrestoSparkRow> createSparkRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n-            PrestoSparkPlan prestoSparkPlan,\n-            PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector)\n+            PlanFragment fragment,\n+            Map<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> rddInputs,\n+            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            TableWriteInfo tableWriteInfo)\n     {\n-        RddFactory rddFactory = new RddFactory(\n-                session,\n-                sparkTaskRequestJsonCodec,\n-                sparkContext,\n-                taskExecutorFactoryProvider,\n-                getSparkInitialPartitionCount(session),\n-                getHashPartitionCount(session),\n-                taskStatsCollector,\n-                prestoSparkPlan.getTableWriteInfo());\n-        return rddFactory.createRdd(prestoSparkPlan.getPlan());\n-    }\n+        checkArgument(!fragment.getStageExecutionDescriptor().isStageGroupedExecution(), \"unexpected grouped execution fragment: %s\", fragment.getId());\n \n-    private static class RddFactory\n-    {\n-        private final Session session;\n-        private final JsonCodec<PrestoSparkTaskDescriptor> sparkTaskDescriptorJsonCodec;\n-        private final JavaSparkContext sparkContext;\n-        private final PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider;\n-        private final int initialSparkPartitionCount;\n-        private final int hashPartitionCount;\n-        private final CollectionAccumulator<SerializedTaskStats> taskStatsCollector;\n-        private final TableWriteInfo tableWriteInfo;\n-\n-        private RddFactory(\n-                Session session,\n-                JsonCodec<PrestoSparkTaskDescriptor> sparkTaskDescriptorJsonCodec,\n-                JavaSparkContext sparkContext,\n-                PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n-                int initialSparkPartitionCount,\n-                int hashPartitionCount,\n-                CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-                TableWriteInfo tableWriteInfo)\n-        {\n-            this.session = requireNonNull(session, \"session is null\");\n-            this.sparkTaskDescriptorJsonCodec = requireNonNull(sparkTaskDescriptorJsonCodec, \"sparkTaskDescriptorJsonCodec is null\");\n-            this.sparkContext = requireNonNull(sparkContext, \"sparkContext is null\");\n-            this.executorFactoryProvider = requireNonNull(executorFactoryProvider, \"executorFactoryProvider is null\");\n-            this.initialSparkPartitionCount = initialSparkPartitionCount;\n-            this.hashPartitionCount = hashPartitionCount;\n-            this.taskStatsCollector = requireNonNull(taskStatsCollector, \"taskStatsCollector is null\");\n-            this.tableWriteInfo = requireNonNull(tableWriteInfo, \"tableWriteInfo is null\");\n+        PartitioningHandle partitioning = fragment.getPartitioning();\n+\n+        if (!(partitioning.getConnectorHandle() instanceof SystemPartitioningHandle)) {\n+            // TODO: add support for bucketed table\n+            throw new PrestoException(NOT_SUPPORTED, \"Partitioned (bucketed) tables are not yet supported by Presto on Spark\");\n         }\n \n-        public JavaPairRDD<Integer, PrestoSparkRow> createRdd(PrestoSparkSubPlan subPlan)\n-        {\n-            PlanFragment fragment;\n-            // TODO: fragment adaption should be done prior to RDD creation\n-            if (subPlan.getFragment().getPartitioningScheme().getPartitioning().getHandle().equals(FIXED_HASH_DISTRIBUTION)) {\n-                fragment = subPlan.getFragment().withBucketToPartition(Optional.of(IntStream.range(0, hashPartitionCount).toArray()));\n-            }\n-            else {\n-                fragment = subPlan.getFragment();\n-            }\n+        if (partitioning.equals(SCALED_WRITER_DISTRIBUTION)) {\n+            throw new PrestoException(NOT_SUPPORTED, \"Automatic writers scaling is not supported by Presto on Spark\");\n+        }\n \n-            checkArgument(!fragment.getStageExecutionDescriptor().isStageGroupedExecution(), \"unexpected grouped execution fragment: %s\", fragment.getId());\n+        checkArgument(!partitioning.equals(COORDINATOR_DISTRIBUTION), \"COORDINATOR_DISTRIBUTION fragment must be run on the driver\");\n+        checkArgument(!partitioning.equals(FIXED_BROADCAST_DISTRIBUTION), \"FIXED_BROADCAST_DISTRIBUTION can only be set as an output partitioning scheme, and not as a fragment distribution\");\n+        checkArgument(!partitioning.equals(FIXED_PASSTHROUGH_DISTRIBUTION), \"FIXED_PASSTHROUGH_DISTRIBUTION can only be set as local exchange partitioning\");\n \n-            // scans\n-            List<PlanNodeId> tableScans = fragment.getTableScanSchedulingOrder();\n+        // TODO: ARBITRARY_DISTRIBUTION is something very weird.\n+        // TODO: It doesn't have partitioning function, and it is never set as a fragment partitioning.\n+        // TODO: We should consider removing ARBITRARY_DISTRIBUTION.\n+        checkArgument(!partitioning.equals(ARBITRARY_DISTRIBUTION), \"ARBITRARY_DISTRIBUTION is not expected to be set as a fragment distribution\");\n \n-            // source stages\n-            List<RemoteSourceNode> remoteSources = fragment.getRemoteSourceNodes();\n-            checkArgument(tableScans.isEmpty() || remoteSources.isEmpty(), \"stages that have both, remote sources and table scans, are not supported\");\n+        int hashPartitionCount = getHashPartitionCount(session);\n \n-            if (!tableScans.isEmpty()) {\n-                checkArgument(fragment.getPartitioning().equals(SOURCE_DISTRIBUTION), \"unexpected table scan partitioning: %s\", fragment.getPartitioning());\n+        // configure number of output partitions\n+        if (fragment.getPartitioningScheme().getPartitioning().getHandle().equals(FIXED_HASH_DISTRIBUTION)) {\n+            fragment = fragment.withBucketToPartition(Optional.of(IntStream.range(0, hashPartitionCount).toArray()));\n+        }\n \n-                // get all scheduled splits\n-                List<ScheduledSplit> scheduledSplits = subPlan.getTaskSources().stream()\n-                        .flatMap(taskSource -> taskSource.getSplits().stream())\n-                        .collect(toImmutableList());\n+        if (partitioning.equals(SINGLE_DISTRIBUTION) || partitioning.equals(FIXED_HASH_DISTRIBUTION) || partitioning.equals(FIXED_ARBITRARY_DISTRIBUTION)) {\n+            checkArgument(\n+                    fragment.getTableScanSchedulingOrder().isEmpty(),\n+                    \"Fragment with is not expected to have table scans. fragmentId: %s, fragment partitioning %s\",\n+                    fragment.getId(),\n+                    fragment.getPartitioning());\n+\n+            for (RemoteSourceNode remoteSource : fragment.getRemoteSourceNodes()) {\n+                if (remoteSource.isEnsureSourceOrdering() || remoteSource.getOrderingScheme().isPresent()) {\n+                    throw new PrestoException(NOT_SUPPORTED, format(\n+                            \"Order sensitive exchange is not supported by Presto on Spark. fragmentId: %s, sourceFragmentIds: %s\",\n+                            fragment.getId(),\n+                            remoteSource.getSourceFragmentIds()));\n+                }\n+            }\n \n-                // get scheduled splits by task\n-                List<List<ScheduledSplit>> assignedSplits = assignSplitsToTasks(scheduledSplits, initialSparkPartitionCount);\n+            Partitioner inputPartitioner = createPartitioner(\n+                    partitioning,\n+                    // TODO: consider using getMaxTasksPerStage\n+                    hashPartitionCount);\n \n-                List<SerializedPrestoSparkTaskDescriptor> serializedRequests = assignedSplits.stream()\n-                        .map(splits -> createTaskDescriptor(fragment, splits))\n-                        .map(sparkTaskDescriptorJsonCodec::toJsonBytes)\n-                        .map(SerializedPrestoSparkTaskDescriptor::new)\n-                        .collect(toImmutableList());\n+            Map<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> partitionedInputs = rddInputs.entrySet().stream()\n+                    .collect(toImmutableMap(Map.Entry::getKey, entry -> entry.getValue().partitionBy(inputPartitioner)));\n \n-                return sparkContext.parallelize(serializedRequests, initialSparkPartitionCount)\n-                        .mapPartitionsToPair(createTaskProcessor(executorFactoryProvider, taskStatsCollector));\n-            }\n+            return createIntermediateRdd(\n+                    session,\n+                    fragment,\n+                    executorFactoryProvider,\n+                    taskStatsCollector,\n+                    tableWriteInfo,\n+                    partitionedInputs);\n+        }\n+        else if (partitioning.equals(SOURCE_DISTRIBUTION)) {\n+            checkArgument(rddInputs.isEmpty(), \"rddInputs is expected to be empty for SOURCE_DISTRIBUTION fragment: %s\", fragment.getId());\n+            return createSourceRdd(\n+                    sparkContext,\n+                    session,\n+                    fragment,\n+                    executorFactoryProvider,\n+                    taskStatsCollector,\n+                    tableWriteInfo);\n+        }\n+        else {\n+            throw new IllegalArgumentException(format(\"Unexpected fragment partitioning %s, fragmentId: %s\", partitioning, fragment.getId()));\n+        }\n+    }\n \n-            List<PrestoSparkSubPlan> children = subPlan.getChildren();\n-            checkArgument(\n-                    remoteSources.size() == children.size(),\n-                    \"number of remote sources doesn't match the number of child stages: %s != %s\",\n-                    remoteSources.size(),\n-                    children.size());\n-\n-            if (children.size() == 1) {\n-                // Single remote source\n-                PrestoSparkSubPlan childSubPlan = getOnlyElement(children);\n-                JavaPairRDD<Integer, PrestoSparkRow> childRdd = createRdd(childSubPlan);\n-                PartitioningHandle partitioning = fragment.getPartitioning();\n-\n-                if (partitioning.equals(COORDINATOR_DISTRIBUTION)) {\n-                    // coordinator side work will be handled after JavaPairRDD#collect() call in PrestoSparkExecution\n-                    return childRdd;\n-                }\n+    private static Partitioner createPartitioner(PartitioningHandle partitioning, int partitionCount)\n+    {\n+        if (partitioning.equals(SINGLE_DISTRIBUTION)) {\n+            return new IntegerIdentityPartitioner(1);\n+        }\n+        if (partitioning.equals(FIXED_HASH_DISTRIBUTION)) {\n+            return new IntegerIdentityPartitioner(partitionCount);\n+        }\n+        if (partitioning.equals(FIXED_ARBITRARY_DISTRIBUTION)) {\n+            throw new PrestoException(NOT_SUPPORTED, \"FIXED_ARBITRARY_DISTRIBUTION partitioning is not yet supported\");\n+        }\n+        throw new IllegalArgumentException(format(\"Unexpected fragment partitioning %s\", partitioning));\n+    }\n \n-                PlanFragment childFragment = childSubPlan.getFragment();\n-                RemoteSourceNode remoteSource = getOnlyElement(remoteSources);\n-                List<PlanFragmentId> sourceFragmentIds = remoteSource.getSourceFragmentIds();\n-                checkArgument(sourceFragmentIds.size() == 1, \"expected to have exactly only a single source fragment\");\n-                checkArgument(childFragment.getId().equals(getOnlyElement(sourceFragmentIds)));\n-\n-                PrestoSparkTaskDescriptor taskDescriptor = createTaskDescriptor(fragment, ImmutableList.of());\n-                SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(sparkTaskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-                if (partitioning.equals(FIXED_HASH_DISTRIBUTION) ||\n-                        // when single distribution - there will be a single partition 0\n-                        partitioning.equals(SINGLE_DISTRIBUTION)) {\n-                    String planNodeId = remoteSource.getId().toString();\n-                    return childRdd\n-                            .partitionBy(partitioning.equals(FIXED_HASH_DISTRIBUTION) ? new IntegerIdentityPartitioner(hashPartitionCount) : new IntegerIdentityPartitioner(1))\n-                            .mapPartitionsToPair(createTaskProcessor(executorFactoryProvider, serializedTaskDescriptor, planNodeId, taskStatsCollector));\n-                }\n-                else {\n-                    // TODO: support (or do check state over) the following fragment partitioning:\n-                    //  - SOURCE_DISTRIBUTION\n-                    //  - FIXED_PASSTHROUGH_DISTRIBUTION\n-                    //  - ARBITRARY_DISTRIBUTION\n-                    //  - SCALED_WRITER_DISTRIBUTION\n-                    //  - FIXED_BROADCAST_DISTRIBUTION\n-                    //  - FIXED_ARBITRARY_DISTRIBUTION\n-                    throw new IllegalArgumentException(\"Unsupported fragment partitioning: \" + partitioning);\n-                }\n-            }\n-            else if (children.size() == 2) {\n-                // TODO: support N way join\n-                PrestoSparkSubPlan leftSubPlan = children.get(0);\n-                PrestoSparkSubPlan rightSubPlan = children.get(1);\n-\n-                RemoteSourceNode leftRemoteSource = remoteSources.get(0);\n-                RemoteSourceNode rightRemoteSource = remoteSources.get(1);\n-\n-                // We need String representation since PlanNodeId is not serializable...\n-                String leftRemoteSourcePlanId = leftRemoteSource.getId().toString();\n-                String rightRemoteSourcePlanId = rightRemoteSource.getId().toString();\n-\n-                JavaPairRDD<Integer, PrestoSparkRow> leftChildRdd = createRdd(leftSubPlan);\n-                JavaPairRDD<Integer, PrestoSparkRow> rightChildRdd = createRdd(rightSubPlan);\n-\n-                PlanFragment leftFragment = leftSubPlan.getFragment();\n-                PlanFragment rightFragment = rightSubPlan.getFragment();\n-\n-                List<PlanFragmentId> leftFragmentIds = leftRemoteSource.getSourceFragmentIds();\n-                checkArgument(leftFragmentIds.size() == 1, \"expected to have exactly only a single source fragment\");\n-                checkArgument(leftFragment.getId().equals(getOnlyElement(leftFragmentIds)));\n-                List<PlanFragmentId> rightFragmentIds = rightRemoteSource.getSourceFragmentIds();\n-                checkArgument(rightFragmentIds.size() == 1, \"expected to have exactly only a single source fragment\");\n-                checkArgument(rightFragment.getId().equals(getOnlyElement(rightFragmentIds)));\n-\n-                // This fragment only contains remote source, thus there is no splits\n-                PrestoSparkTaskDescriptor taskDescriptor = createTaskDescriptor(fragment, ImmutableList.of());\n-                SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(sparkTaskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-                PartitioningHandle partitioning = fragment.getPartitioning();\n-                checkArgument(partitioning.equals(FIXED_HASH_DISTRIBUTION));\n-\n-                JavaPairRDD<Integer, PrestoSparkRow> shuffledLeftChildRdd = leftChildRdd.partitionBy(new IntegerIdentityPartitioner(hashPartitionCount));\n-                JavaPairRDD<Integer, PrestoSparkRow> shuffledRightChildRdd = rightChildRdd.partitionBy(new IntegerIdentityPartitioner(hashPartitionCount));\n-                return JavaPairRDD.fromJavaRDD(\n-                        shuffledLeftChildRdd.zipPartitions(\n-                                shuffledRightChildRdd,\n-                                createTaskProcessor(executorFactoryProvider, serializedTaskDescriptor, leftRemoteSourcePlanId, rightRemoteSourcePlanId, taskStatsCollector)));\n-            }\n-            else {\n-                throw new UnsupportedOperationException();\n-            }\n+    private JavaPairRDD<Integer, PrestoSparkRow> createIntermediateRdd(\n+            Session session,\n+            PlanFragment fragment,\n+            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            TableWriteInfo tableWriteInfo,\n+            Map<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> rddInputs)\n+    {\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        verify(tableScans.isEmpty(), \"no table scans is expected\");\n+\n+        Set<PlanFragmentId> expectedInputs = fragment.getRemoteSourceNodes().stream()\n+                .map(RemoteSourceNode::getSourceFragmentIds)\n+                .flatMap(List::stream)\n+                .collect(toImmutableSet());\n+\n+        Set<PlanFragmentId> missingInputs = difference(expectedInputs, rddInputs.keySet());\n+        Set<PlanFragmentId> extraInputs = difference(rddInputs.keySet(), expectedInputs);\n+        checkArgument(\n+                missingInputs.isEmpty() && extraInputs.isEmpty(),\n+                \"rddInputs mismatch discovered. expected: %s, actual: %s\",\n+                expectedInputs,\n+                rddInputs.keySet());\n+\n+        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n+\n+        if (rddInputs.size() == 1) {\n+            RemoteSourceNode remoteSourceNode = getOnlyElement(fragment.getRemoteSourceNodes());\n+            PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> taskProcessor =\n+                    createTaskProcessor(\n+                            executorFactoryProvider,\n+                            serializedTaskDescriptor,\n+                            remoteSourceNode.getId().toString(),\n+                            taskStatsCollector);\n+            return getOnlyElement(rddInputs.values())\n+                    .mapPartitionsToPair(taskProcessor);\n+        }\n+        else if (rddInputs.size() == 2) {\n+            List<RemoteSourceNode> remoteSources = fragment.getRemoteSourceNodes();\n+            checkArgument(remoteSources.size() == 2, \"two remote sources are expected, got: %s\", remoteSources.size());\n+            RemoteSourceNode firstRemoteSource = remoteSources.get(0);\n+            RemoteSourceNode secondRemoteSource = remoteSources.get(1);\n+            JavaPairRDD<Integer, PrestoSparkRow> firstRdd = rddInputs.get(firstRemoteSource.getSourceFragmentIds().get(0));\n+            JavaPairRDD<Integer, PrestoSparkRow> secondRdd = rddInputs.get(secondRemoteSource.getSourceFragmentIds().get(0));\n+            FlatMapFunction2<Iterator<Tuple2<Integer, PrestoSparkRow>>, Iterator<Tuple2<Integer, PrestoSparkRow>>, Tuple2<Integer, PrestoSparkRow>> taskProcessor =\n+                    createTaskProcessor(\n+                            executorFactoryProvider,\n+                            serializedTaskDescriptor,\n+                            firstRemoteSource.getId().toString(),\n+                            secondRemoteSource.getId().toString(),\n+                            taskStatsCollector);\n+            return JavaPairRDD.fromJavaRDD(\n+                    firstRdd.zipPartitions(\n+                            secondRdd,\n+                            taskProcessor));\n         }\n \n-        private static List<List<ScheduledSplit>> assignSplitsToTasks(List<ScheduledSplit> scheduledSplits, int numTasks)\n-        {\n-            List<List<ScheduledSplit>> assignedSplits = new ArrayList<>();\n-            for (int i = 0; i < numTasks; i++) {\n-                assignedSplits.add(new ArrayList<>());\n-            }\n+        throw new IllegalArgumentException(format(\"unsupported number of inputs: %s\", rddInputs.size()));\n+    }\n \n-            for (ScheduledSplit split : scheduledSplits) {\n-                int taskId = Objects.hash(split.getPlanNodeId(), split.getSequenceId()) % numTasks;\n-                if (taskId < 0) {\n-                    taskId += numTasks;\n-                }\n+    private JavaPairRDD<Integer, PrestoSparkRow> createSourceRdd(\n+            JavaSparkContext sparkContext,\n+            Session session,\n+            PlanFragment fragment,\n+            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            TableWriteInfo tableWriteInfo)\n+    {\n+        // TODO: Possible in case of a broadcast join\n+        checkArgument(fragment.getRemoteSourceNodes().isEmpty(), \"source task with remote sources is not supported\");\n+\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        checkArgument(\n+                tableScans.size() == 1,\n+                \"exactly one table scan is expected in SOURCE_DISTRIBUTION fragment. fragmentId: %s, actual number of table scans: %s\",\n+                fragment.getId(),\n+                tableScans.size());\n+        verify(tableScans.size() == fragment.getTableScanSchedulingOrder().size());\n+\n+        TableScanNode tableScan = tableScans.get(0);\n+\n+        List<ScheduledSplit> splits = getSplits(session, tableScan);\n+        shuffle(splits);\n+        int initialPartitionCount = getSparkInitialPartitionCount(session);\n+        int numTasks = Math.min(splits.size(), initialPartitionCount);\n+        if (numTasks == 0) {\n+            return JavaPairRDD.fromJavaRDD(sparkContext.emptyRDD());\n+        }\n \n-                assignedSplits.get(taskId).add(split);\n-            }\n+        List<List<ScheduledSplit>> assignedSplits = assignSplitsToTasks(splits, numTasks);\n \n-            return assignedSplits;\n+        // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n+        splits = null;\n+\n+        ImmutableList.Builder<SerializedPrestoSparkTaskDescriptor> serializedTaskDescriptors = ImmutableList.builder();\n+        for (int i = 0; i < assignedSplits.size(); i++) {\n+            List<ScheduledSplit> splitBatch = assignedSplits.get(i);\n+            PrestoSparkTaskDescriptor taskDescriptor = createSourceTaskDescriptor(session, tableWriteInfo, fragment, splitBatch);\n+            // TODO: consider more efficient serialization or apply compression to save precious memory on the Driver\n+            byte[] jsonSerializedTaskDescriptor = taskDescriptorJsonCodec.toJsonBytes(taskDescriptor);\n+            serializedTaskDescriptors.add(new SerializedPrestoSparkTaskDescriptor(jsonSerializedTaskDescriptor));\n+            // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n+            assignedSplits.set(i, null);\n         }\n \n-        private PrestoSparkTaskDescriptor createTaskDescriptor(PlanFragment fragment, List<ScheduledSplit> splits)\n-        {\n-            Map<PlanNodeId, Set<ScheduledSplit>> splitsByPlanNode = splits.stream()\n-                    .collect(Collectors.groupingBy(\n-                            ScheduledSplit::getPlanNodeId,\n-                            mapping(identity(), toSet())));\n-\n-            List<TaskSource> taskSourceByPlanNode = splitsByPlanNode.entrySet().stream()\n-                    .map(entry -> new TaskSource(\n-                            entry.getKey(),\n-                            entry.getValue(),\n-                            ImmutableSet.of(),\n-                            true))\n-                    .collect(toImmutableList());\n-\n-            return new PrestoSparkTaskDescriptor(\n-                    session.toSessionRepresentation(),\n-                    session.getIdentity().getExtraCredentials(),\n-                    fragment,\n-                    taskSourceByPlanNode,\n-                    tableWriteInfo);\n+        return sparkContext.parallelize(serializedTaskDescriptors.build(), numTasks)\n+                .mapPartitionsToPair(createTaskProcessor(executorFactoryProvider, taskStatsCollector));\n+    }\n+\n+    private List<ScheduledSplit> getSplits(Session session, TableScanNode tableScan)\n+    {\n+        List<ScheduledSplit> splits = new ArrayList<>();\n+        SplitSource splitSource = splitManager.getSplits(session, tableScan.getTable(), UNGROUPED_SCHEDULING);\n+        long sequenceId = 0;\n+        while (!splitSource.isFinished()) {\n+            List<Split> splitBatch = getFutureValue(splitSource.getNextBatch(NOT_PARTITIONED, Lifespan.taskWide(), 1000)).getSplits();\n+            for (Split split : splitBatch) {\n+                splits.add(new ScheduledSplit(sequenceId++, tableScan.getId(), split));\n+            }\n+        }\n+        return splits;\n+    }\n+\n+    private static List<List<ScheduledSplit>> assignSplitsToTasks(List<ScheduledSplit> splits, int numTasks)\n+    {\n+        checkArgument(numTasks > 0, \"numTasks must be greater then zero\");\n+        List<List<ScheduledSplit>> assignedSplits = new ArrayList<>();\n+        for (int i = 0; i < numTasks; i++) {\n+            assignedSplits.add(new ArrayList<>());\n+        }\n+        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+            assignedSplits.get(splitIndex % numTasks).add(splits.get(splitIndex));\n         }\n+        return assignedSplits;\n+    }\n+\n+    private PrestoSparkTaskDescriptor createIntermediateTaskDescriptor(Session session, TableWriteInfo tableWriteInfo, PlanFragment fragment)\n+    {\n+        return createSourceTaskDescriptor(session, tableWriteInfo, fragment, ImmutableList.of());\n+    }\n+\n+    private PrestoSparkTaskDescriptor createSourceTaskDescriptor(", "originalCommit": "3d4bd6d4836e14974b9247082f649bc5bad82938", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzE0MTI3Nw==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423141277", "bodyText": "I haven't started prototyping bucketed table support. I might need to change the names / refactor it a little bit at some point. Let's not try to overthink it for now.", "author": "arhimondr", "createdAt": "2020-05-11T15:53:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4NDI0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM4MDAxMQ==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423380011", "bodyText": "@arhimondr : I am raising questions rather than proposing changes here. I agree we don't want to address them for now in the code.  But I usually find a bit thinking about the future direction useful.", "author": "wenleix", "createdAt": "2020-05-11T23:39:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4NDI0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM4MjM4OA==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423382388", "bodyText": "It makes sense. I aggree that source and intermediate don't work very well in case of a tasks that has both, inputs and table scans. Eventually these two methods might even be merged into a single one. Unfortunately It is very hard to say what is the right way before we try to prototype it =\\", "author": "arhimondr", "createdAt": "2020-05-11T23:47:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4NDI0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM4NTA5OQ==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423385099", "bodyText": "@arhimondr : FWIW the code structure of SectionExecutionFactory#createStageScheduler might give some light on it. I start to smell convergence between RDD creation and stage scheduler creation... \ud83d\ude03", "author": "wenleix", "createdAt": "2020-05-11T23:55:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4NDI0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4NDcyOQ==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r422784729", "bodyText": "nit: getOnlyElement", "author": "wenleix", "createdAt": "2020-05-11T05:13:54Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -15,283 +15,367 @@\n \n import com.facebook.airlift.json.JsonCodec;\n import com.facebook.presto.Session;\n+import com.facebook.presto.execution.Lifespan;\n import com.facebook.presto.execution.ScheduledSplit;\n import com.facebook.presto.execution.TaskSource;\n import com.facebook.presto.execution.scheduler.TableWriteInfo;\n+import com.facebook.presto.metadata.Metadata;\n+import com.facebook.presto.metadata.Split;\n import com.facebook.presto.spark.PrestoSparkTaskDescriptor;\n import com.facebook.presto.spark.classloader_interface.IntegerIdentityPartitioner;\n import com.facebook.presto.spark.classloader_interface.PrestoSparkRow;\n import com.facebook.presto.spark.classloader_interface.PrestoSparkTaskExecutorFactoryProvider;\n import com.facebook.presto.spark.classloader_interface.SerializedPrestoSparkTaskDescriptor;\n import com.facebook.presto.spark.classloader_interface.SerializedTaskStats;\n+import com.facebook.presto.spi.PrestoException;\n+import com.facebook.presto.spi.plan.PlanNode;\n import com.facebook.presto.spi.plan.PlanNodeId;\n+import com.facebook.presto.spi.plan.TableScanNode;\n+import com.facebook.presto.split.SplitManager;\n+import com.facebook.presto.split.SplitSource;\n import com.facebook.presto.sql.planner.PartitioningHandle;\n import com.facebook.presto.sql.planner.PlanFragment;\n+import com.facebook.presto.sql.planner.SystemPartitioningHandle;\n import com.facebook.presto.sql.planner.plan.PlanFragmentId;\n import com.facebook.presto.sql.planner.plan.RemoteSourceNode;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableSet;\n+import org.apache.spark.Partitioner;\n import org.apache.spark.api.java.JavaPairRDD;\n import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction2;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n \n import javax.inject.Inject;\n \n import java.util.ArrayList;\n+import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n-import java.util.Objects;\n import java.util.Optional;\n import java.util.Set;\n import java.util.stream.Collectors;\n import java.util.stream.IntStream;\n \n+import static com.facebook.airlift.concurrent.MoreFutures.getFutureValue;\n import static com.facebook.presto.SystemSessionProperties.getHashPartitionCount;\n import static com.facebook.presto.spark.PrestoSparkSessionProperties.getSparkInitialPartitionCount;\n import static com.facebook.presto.spark.classloader_interface.TaskProcessors.createTaskProcessor;\n+import static com.facebook.presto.spi.StandardErrorCode.NOT_SUPPORTED;\n+import static com.facebook.presto.spi.connector.ConnectorSplitManager.SplitSchedulingStrategy.UNGROUPED_SCHEDULING;\n+import static com.facebook.presto.spi.connector.NotPartitionedPartitionHandle.NOT_PARTITIONED;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.ARBITRARY_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.COORDINATOR_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_ARBITRARY_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_BROADCAST_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_HASH_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_PASSTHROUGH_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.SCALED_WRITER_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.SINGLE_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.SOURCE_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.optimizations.PlanNodeSearcher.searchFrom;\n import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Verify.verify;\n import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableMap.toImmutableMap;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n import static com.google.common.collect.Iterables.getOnlyElement;\n+import static com.google.common.collect.Sets.difference;\n+import static java.lang.String.format;\n+import static java.util.Collections.shuffle;\n import static java.util.Objects.requireNonNull;\n import static java.util.function.Function.identity;\n import static java.util.stream.Collectors.mapping;\n import static java.util.stream.Collectors.toSet;\n \n public class PrestoSparkRddFactory\n {\n-    private final JsonCodec<PrestoSparkTaskDescriptor> sparkTaskRequestJsonCodec;\n+    private final SplitManager splitManager;\n+    private final Metadata metadata;\n+    private final JsonCodec<PrestoSparkTaskDescriptor> taskDescriptorJsonCodec;\n \n     @Inject\n-    public PrestoSparkRddFactory(JsonCodec<PrestoSparkTaskDescriptor> sparkTaskRequestJsonCodec)\n+    public PrestoSparkRddFactory(SplitManager splitManager, Metadata metadata, JsonCodec<PrestoSparkTaskDescriptor> taskDescriptorJsonCodec)\n     {\n-        this.sparkTaskRequestJsonCodec = requireNonNull(sparkTaskRequestJsonCodec, \"sparkTaskRequestJsonCodec is null\");\n+        this.splitManager = requireNonNull(splitManager, \"splitManager is null\");\n+        this.metadata = requireNonNull(metadata, \"metadata is null\");\n+        this.taskDescriptorJsonCodec = requireNonNull(taskDescriptorJsonCodec, \"taskDescriptorJsonCodec is null\");\n     }\n \n     public JavaPairRDD<Integer, PrestoSparkRow> createSparkRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n-            PrestoSparkPlan prestoSparkPlan,\n-            PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector)\n+            PlanFragment fragment,\n+            Map<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> rddInputs,\n+            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            TableWriteInfo tableWriteInfo)\n     {\n-        RddFactory rddFactory = new RddFactory(\n-                session,\n-                sparkTaskRequestJsonCodec,\n-                sparkContext,\n-                taskExecutorFactoryProvider,\n-                getSparkInitialPartitionCount(session),\n-                getHashPartitionCount(session),\n-                taskStatsCollector,\n-                prestoSparkPlan.getTableWriteInfo());\n-        return rddFactory.createRdd(prestoSparkPlan.getPlan());\n-    }\n+        checkArgument(!fragment.getStageExecutionDescriptor().isStageGroupedExecution(), \"unexpected grouped execution fragment: %s\", fragment.getId());\n \n-    private static class RddFactory\n-    {\n-        private final Session session;\n-        private final JsonCodec<PrestoSparkTaskDescriptor> sparkTaskDescriptorJsonCodec;\n-        private final JavaSparkContext sparkContext;\n-        private final PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider;\n-        private final int initialSparkPartitionCount;\n-        private final int hashPartitionCount;\n-        private final CollectionAccumulator<SerializedTaskStats> taskStatsCollector;\n-        private final TableWriteInfo tableWriteInfo;\n-\n-        private RddFactory(\n-                Session session,\n-                JsonCodec<PrestoSparkTaskDescriptor> sparkTaskDescriptorJsonCodec,\n-                JavaSparkContext sparkContext,\n-                PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n-                int initialSparkPartitionCount,\n-                int hashPartitionCount,\n-                CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-                TableWriteInfo tableWriteInfo)\n-        {\n-            this.session = requireNonNull(session, \"session is null\");\n-            this.sparkTaskDescriptorJsonCodec = requireNonNull(sparkTaskDescriptorJsonCodec, \"sparkTaskDescriptorJsonCodec is null\");\n-            this.sparkContext = requireNonNull(sparkContext, \"sparkContext is null\");\n-            this.executorFactoryProvider = requireNonNull(executorFactoryProvider, \"executorFactoryProvider is null\");\n-            this.initialSparkPartitionCount = initialSparkPartitionCount;\n-            this.hashPartitionCount = hashPartitionCount;\n-            this.taskStatsCollector = requireNonNull(taskStatsCollector, \"taskStatsCollector is null\");\n-            this.tableWriteInfo = requireNonNull(tableWriteInfo, \"tableWriteInfo is null\");\n+        PartitioningHandle partitioning = fragment.getPartitioning();\n+\n+        if (!(partitioning.getConnectorHandle() instanceof SystemPartitioningHandle)) {\n+            // TODO: add support for bucketed table\n+            throw new PrestoException(NOT_SUPPORTED, \"Partitioned (bucketed) tables are not yet supported by Presto on Spark\");\n         }\n \n-        public JavaPairRDD<Integer, PrestoSparkRow> createRdd(PrestoSparkSubPlan subPlan)\n-        {\n-            PlanFragment fragment;\n-            // TODO: fragment adaption should be done prior to RDD creation\n-            if (subPlan.getFragment().getPartitioningScheme().getPartitioning().getHandle().equals(FIXED_HASH_DISTRIBUTION)) {\n-                fragment = subPlan.getFragment().withBucketToPartition(Optional.of(IntStream.range(0, hashPartitionCount).toArray()));\n-            }\n-            else {\n-                fragment = subPlan.getFragment();\n-            }\n+        if (partitioning.equals(SCALED_WRITER_DISTRIBUTION)) {\n+            throw new PrestoException(NOT_SUPPORTED, \"Automatic writers scaling is not supported by Presto on Spark\");\n+        }\n \n-            checkArgument(!fragment.getStageExecutionDescriptor().isStageGroupedExecution(), \"unexpected grouped execution fragment: %s\", fragment.getId());\n+        checkArgument(!partitioning.equals(COORDINATOR_DISTRIBUTION), \"COORDINATOR_DISTRIBUTION fragment must be run on the driver\");\n+        checkArgument(!partitioning.equals(FIXED_BROADCAST_DISTRIBUTION), \"FIXED_BROADCAST_DISTRIBUTION can only be set as an output partitioning scheme, and not as a fragment distribution\");\n+        checkArgument(!partitioning.equals(FIXED_PASSTHROUGH_DISTRIBUTION), \"FIXED_PASSTHROUGH_DISTRIBUTION can only be set as local exchange partitioning\");\n \n-            // scans\n-            List<PlanNodeId> tableScans = fragment.getTableScanSchedulingOrder();\n+        // TODO: ARBITRARY_DISTRIBUTION is something very weird.\n+        // TODO: It doesn't have partitioning function, and it is never set as a fragment partitioning.\n+        // TODO: We should consider removing ARBITRARY_DISTRIBUTION.\n+        checkArgument(!partitioning.equals(ARBITRARY_DISTRIBUTION), \"ARBITRARY_DISTRIBUTION is not expected to be set as a fragment distribution\");\n \n-            // source stages\n-            List<RemoteSourceNode> remoteSources = fragment.getRemoteSourceNodes();\n-            checkArgument(tableScans.isEmpty() || remoteSources.isEmpty(), \"stages that have both, remote sources and table scans, are not supported\");\n+        int hashPartitionCount = getHashPartitionCount(session);\n \n-            if (!tableScans.isEmpty()) {\n-                checkArgument(fragment.getPartitioning().equals(SOURCE_DISTRIBUTION), \"unexpected table scan partitioning: %s\", fragment.getPartitioning());\n+        // configure number of output partitions\n+        if (fragment.getPartitioningScheme().getPartitioning().getHandle().equals(FIXED_HASH_DISTRIBUTION)) {\n+            fragment = fragment.withBucketToPartition(Optional.of(IntStream.range(0, hashPartitionCount).toArray()));\n+        }\n \n-                // get all scheduled splits\n-                List<ScheduledSplit> scheduledSplits = subPlan.getTaskSources().stream()\n-                        .flatMap(taskSource -> taskSource.getSplits().stream())\n-                        .collect(toImmutableList());\n+        if (partitioning.equals(SINGLE_DISTRIBUTION) || partitioning.equals(FIXED_HASH_DISTRIBUTION) || partitioning.equals(FIXED_ARBITRARY_DISTRIBUTION)) {\n+            checkArgument(\n+                    fragment.getTableScanSchedulingOrder().isEmpty(),\n+                    \"Fragment with is not expected to have table scans. fragmentId: %s, fragment partitioning %s\",\n+                    fragment.getId(),\n+                    fragment.getPartitioning());\n+\n+            for (RemoteSourceNode remoteSource : fragment.getRemoteSourceNodes()) {\n+                if (remoteSource.isEnsureSourceOrdering() || remoteSource.getOrderingScheme().isPresent()) {\n+                    throw new PrestoException(NOT_SUPPORTED, format(\n+                            \"Order sensitive exchange is not supported by Presto on Spark. fragmentId: %s, sourceFragmentIds: %s\",\n+                            fragment.getId(),\n+                            remoteSource.getSourceFragmentIds()));\n+                }\n+            }\n \n-                // get scheduled splits by task\n-                List<List<ScheduledSplit>> assignedSplits = assignSplitsToTasks(scheduledSplits, initialSparkPartitionCount);\n+            Partitioner inputPartitioner = createPartitioner(\n+                    partitioning,\n+                    // TODO: consider using getMaxTasksPerStage\n+                    hashPartitionCount);\n \n-                List<SerializedPrestoSparkTaskDescriptor> serializedRequests = assignedSplits.stream()\n-                        .map(splits -> createTaskDescriptor(fragment, splits))\n-                        .map(sparkTaskDescriptorJsonCodec::toJsonBytes)\n-                        .map(SerializedPrestoSparkTaskDescriptor::new)\n-                        .collect(toImmutableList());\n+            Map<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> partitionedInputs = rddInputs.entrySet().stream()\n+                    .collect(toImmutableMap(Map.Entry::getKey, entry -> entry.getValue().partitionBy(inputPartitioner)));\n \n-                return sparkContext.parallelize(serializedRequests, initialSparkPartitionCount)\n-                        .mapPartitionsToPair(createTaskProcessor(executorFactoryProvider, taskStatsCollector));\n-            }\n+            return createIntermediateRdd(\n+                    session,\n+                    fragment,\n+                    executorFactoryProvider,\n+                    taskStatsCollector,\n+                    tableWriteInfo,\n+                    partitionedInputs);\n+        }\n+        else if (partitioning.equals(SOURCE_DISTRIBUTION)) {\n+            checkArgument(rddInputs.isEmpty(), \"rddInputs is expected to be empty for SOURCE_DISTRIBUTION fragment: %s\", fragment.getId());\n+            return createSourceRdd(\n+                    sparkContext,\n+                    session,\n+                    fragment,\n+                    executorFactoryProvider,\n+                    taskStatsCollector,\n+                    tableWriteInfo);\n+        }\n+        else {\n+            throw new IllegalArgumentException(format(\"Unexpected fragment partitioning %s, fragmentId: %s\", partitioning, fragment.getId()));\n+        }\n+    }\n \n-            List<PrestoSparkSubPlan> children = subPlan.getChildren();\n-            checkArgument(\n-                    remoteSources.size() == children.size(),\n-                    \"number of remote sources doesn't match the number of child stages: %s != %s\",\n-                    remoteSources.size(),\n-                    children.size());\n-\n-            if (children.size() == 1) {\n-                // Single remote source\n-                PrestoSparkSubPlan childSubPlan = getOnlyElement(children);\n-                JavaPairRDD<Integer, PrestoSparkRow> childRdd = createRdd(childSubPlan);\n-                PartitioningHandle partitioning = fragment.getPartitioning();\n-\n-                if (partitioning.equals(COORDINATOR_DISTRIBUTION)) {\n-                    // coordinator side work will be handled after JavaPairRDD#collect() call in PrestoSparkExecution\n-                    return childRdd;\n-                }\n+    private static Partitioner createPartitioner(PartitioningHandle partitioning, int partitionCount)\n+    {\n+        if (partitioning.equals(SINGLE_DISTRIBUTION)) {\n+            return new IntegerIdentityPartitioner(1);\n+        }\n+        if (partitioning.equals(FIXED_HASH_DISTRIBUTION)) {\n+            return new IntegerIdentityPartitioner(partitionCount);\n+        }\n+        if (partitioning.equals(FIXED_ARBITRARY_DISTRIBUTION)) {\n+            throw new PrestoException(NOT_SUPPORTED, \"FIXED_ARBITRARY_DISTRIBUTION partitioning is not yet supported\");\n+        }\n+        throw new IllegalArgumentException(format(\"Unexpected fragment partitioning %s\", partitioning));\n+    }\n \n-                PlanFragment childFragment = childSubPlan.getFragment();\n-                RemoteSourceNode remoteSource = getOnlyElement(remoteSources);\n-                List<PlanFragmentId> sourceFragmentIds = remoteSource.getSourceFragmentIds();\n-                checkArgument(sourceFragmentIds.size() == 1, \"expected to have exactly only a single source fragment\");\n-                checkArgument(childFragment.getId().equals(getOnlyElement(sourceFragmentIds)));\n-\n-                PrestoSparkTaskDescriptor taskDescriptor = createTaskDescriptor(fragment, ImmutableList.of());\n-                SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(sparkTaskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-                if (partitioning.equals(FIXED_HASH_DISTRIBUTION) ||\n-                        // when single distribution - there will be a single partition 0\n-                        partitioning.equals(SINGLE_DISTRIBUTION)) {\n-                    String planNodeId = remoteSource.getId().toString();\n-                    return childRdd\n-                            .partitionBy(partitioning.equals(FIXED_HASH_DISTRIBUTION) ? new IntegerIdentityPartitioner(hashPartitionCount) : new IntegerIdentityPartitioner(1))\n-                            .mapPartitionsToPair(createTaskProcessor(executorFactoryProvider, serializedTaskDescriptor, planNodeId, taskStatsCollector));\n-                }\n-                else {\n-                    // TODO: support (or do check state over) the following fragment partitioning:\n-                    //  - SOURCE_DISTRIBUTION\n-                    //  - FIXED_PASSTHROUGH_DISTRIBUTION\n-                    //  - ARBITRARY_DISTRIBUTION\n-                    //  - SCALED_WRITER_DISTRIBUTION\n-                    //  - FIXED_BROADCAST_DISTRIBUTION\n-                    //  - FIXED_ARBITRARY_DISTRIBUTION\n-                    throw new IllegalArgumentException(\"Unsupported fragment partitioning: \" + partitioning);\n-                }\n-            }\n-            else if (children.size() == 2) {\n-                // TODO: support N way join\n-                PrestoSparkSubPlan leftSubPlan = children.get(0);\n-                PrestoSparkSubPlan rightSubPlan = children.get(1);\n-\n-                RemoteSourceNode leftRemoteSource = remoteSources.get(0);\n-                RemoteSourceNode rightRemoteSource = remoteSources.get(1);\n-\n-                // We need String representation since PlanNodeId is not serializable...\n-                String leftRemoteSourcePlanId = leftRemoteSource.getId().toString();\n-                String rightRemoteSourcePlanId = rightRemoteSource.getId().toString();\n-\n-                JavaPairRDD<Integer, PrestoSparkRow> leftChildRdd = createRdd(leftSubPlan);\n-                JavaPairRDD<Integer, PrestoSparkRow> rightChildRdd = createRdd(rightSubPlan);\n-\n-                PlanFragment leftFragment = leftSubPlan.getFragment();\n-                PlanFragment rightFragment = rightSubPlan.getFragment();\n-\n-                List<PlanFragmentId> leftFragmentIds = leftRemoteSource.getSourceFragmentIds();\n-                checkArgument(leftFragmentIds.size() == 1, \"expected to have exactly only a single source fragment\");\n-                checkArgument(leftFragment.getId().equals(getOnlyElement(leftFragmentIds)));\n-                List<PlanFragmentId> rightFragmentIds = rightRemoteSource.getSourceFragmentIds();\n-                checkArgument(rightFragmentIds.size() == 1, \"expected to have exactly only a single source fragment\");\n-                checkArgument(rightFragment.getId().equals(getOnlyElement(rightFragmentIds)));\n-\n-                // This fragment only contains remote source, thus there is no splits\n-                PrestoSparkTaskDescriptor taskDescriptor = createTaskDescriptor(fragment, ImmutableList.of());\n-                SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(sparkTaskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-                PartitioningHandle partitioning = fragment.getPartitioning();\n-                checkArgument(partitioning.equals(FIXED_HASH_DISTRIBUTION));\n-\n-                JavaPairRDD<Integer, PrestoSparkRow> shuffledLeftChildRdd = leftChildRdd.partitionBy(new IntegerIdentityPartitioner(hashPartitionCount));\n-                JavaPairRDD<Integer, PrestoSparkRow> shuffledRightChildRdd = rightChildRdd.partitionBy(new IntegerIdentityPartitioner(hashPartitionCount));\n-                return JavaPairRDD.fromJavaRDD(\n-                        shuffledLeftChildRdd.zipPartitions(\n-                                shuffledRightChildRdd,\n-                                createTaskProcessor(executorFactoryProvider, serializedTaskDescriptor, leftRemoteSourcePlanId, rightRemoteSourcePlanId, taskStatsCollector)));\n-            }\n-            else {\n-                throw new UnsupportedOperationException();\n-            }\n+    private JavaPairRDD<Integer, PrestoSparkRow> createIntermediateRdd(\n+            Session session,\n+            PlanFragment fragment,\n+            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            TableWriteInfo tableWriteInfo,\n+            Map<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> rddInputs)\n+    {\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        verify(tableScans.isEmpty(), \"no table scans is expected\");\n+\n+        Set<PlanFragmentId> expectedInputs = fragment.getRemoteSourceNodes().stream()\n+                .map(RemoteSourceNode::getSourceFragmentIds)\n+                .flatMap(List::stream)\n+                .collect(toImmutableSet());\n+\n+        Set<PlanFragmentId> missingInputs = difference(expectedInputs, rddInputs.keySet());\n+        Set<PlanFragmentId> extraInputs = difference(rddInputs.keySet(), expectedInputs);\n+        checkArgument(\n+                missingInputs.isEmpty() && extraInputs.isEmpty(),\n+                \"rddInputs mismatch discovered. expected: %s, actual: %s\",\n+                expectedInputs,\n+                rddInputs.keySet());\n+\n+        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n+\n+        if (rddInputs.size() == 1) {\n+            RemoteSourceNode remoteSourceNode = getOnlyElement(fragment.getRemoteSourceNodes());\n+            PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> taskProcessor =\n+                    createTaskProcessor(\n+                            executorFactoryProvider,\n+                            serializedTaskDescriptor,\n+                            remoteSourceNode.getId().toString(),\n+                            taskStatsCollector);\n+            return getOnlyElement(rddInputs.values())\n+                    .mapPartitionsToPair(taskProcessor);\n+        }\n+        else if (rddInputs.size() == 2) {\n+            List<RemoteSourceNode> remoteSources = fragment.getRemoteSourceNodes();\n+            checkArgument(remoteSources.size() == 2, \"two remote sources are expected, got: %s\", remoteSources.size());\n+            RemoteSourceNode firstRemoteSource = remoteSources.get(0);\n+            RemoteSourceNode secondRemoteSource = remoteSources.get(1);\n+            JavaPairRDD<Integer, PrestoSparkRow> firstRdd = rddInputs.get(firstRemoteSource.getSourceFragmentIds().get(0));\n+            JavaPairRDD<Integer, PrestoSparkRow> secondRdd = rddInputs.get(secondRemoteSource.getSourceFragmentIds().get(0));\n+            FlatMapFunction2<Iterator<Tuple2<Integer, PrestoSparkRow>>, Iterator<Tuple2<Integer, PrestoSparkRow>>, Tuple2<Integer, PrestoSparkRow>> taskProcessor =\n+                    createTaskProcessor(\n+                            executorFactoryProvider,\n+                            serializedTaskDescriptor,\n+                            firstRemoteSource.getId().toString(),\n+                            secondRemoteSource.getId().toString(),\n+                            taskStatsCollector);\n+            return JavaPairRDD.fromJavaRDD(\n+                    firstRdd.zipPartitions(\n+                            secondRdd,\n+                            taskProcessor));\n         }\n \n-        private static List<List<ScheduledSplit>> assignSplitsToTasks(List<ScheduledSplit> scheduledSplits, int numTasks)\n-        {\n-            List<List<ScheduledSplit>> assignedSplits = new ArrayList<>();\n-            for (int i = 0; i < numTasks; i++) {\n-                assignedSplits.add(new ArrayList<>());\n-            }\n+        throw new IllegalArgumentException(format(\"unsupported number of inputs: %s\", rddInputs.size()));\n+    }\n \n-            for (ScheduledSplit split : scheduledSplits) {\n-                int taskId = Objects.hash(split.getPlanNodeId(), split.getSequenceId()) % numTasks;\n-                if (taskId < 0) {\n-                    taskId += numTasks;\n-                }\n+    private JavaPairRDD<Integer, PrestoSparkRow> createSourceRdd(\n+            JavaSparkContext sparkContext,\n+            Session session,\n+            PlanFragment fragment,\n+            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            TableWriteInfo tableWriteInfo)\n+    {\n+        // TODO: Possible in case of a broadcast join\n+        checkArgument(fragment.getRemoteSourceNodes().isEmpty(), \"source task with remote sources is not supported\");\n+\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        checkArgument(\n+                tableScans.size() == 1,\n+                \"exactly one table scan is expected in SOURCE_DISTRIBUTION fragment. fragmentId: %s, actual number of table scans: %s\",\n+                fragment.getId(),\n+                tableScans.size());\n+        verify(tableScans.size() == fragment.getTableScanSchedulingOrder().size());\n+\n+        TableScanNode tableScan = tableScans.get(0);", "originalCommit": "3d4bd6d4836e14974b9247082f649bc5bad82938", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4NDk0OA==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r422784948", "bodyText": "hmm. maybe we can make the previous checkArugment to assert both tableScan.size() == 1 && fragment.getTableScanSchedulingOrder().size() == 1?", "author": "wenleix", "createdAt": "2020-05-11T05:14:48Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -15,283 +15,367 @@\n \n import com.facebook.airlift.json.JsonCodec;\n import com.facebook.presto.Session;\n+import com.facebook.presto.execution.Lifespan;\n import com.facebook.presto.execution.ScheduledSplit;\n import com.facebook.presto.execution.TaskSource;\n import com.facebook.presto.execution.scheduler.TableWriteInfo;\n+import com.facebook.presto.metadata.Metadata;\n+import com.facebook.presto.metadata.Split;\n import com.facebook.presto.spark.PrestoSparkTaskDescriptor;\n import com.facebook.presto.spark.classloader_interface.IntegerIdentityPartitioner;\n import com.facebook.presto.spark.classloader_interface.PrestoSparkRow;\n import com.facebook.presto.spark.classloader_interface.PrestoSparkTaskExecutorFactoryProvider;\n import com.facebook.presto.spark.classloader_interface.SerializedPrestoSparkTaskDescriptor;\n import com.facebook.presto.spark.classloader_interface.SerializedTaskStats;\n+import com.facebook.presto.spi.PrestoException;\n+import com.facebook.presto.spi.plan.PlanNode;\n import com.facebook.presto.spi.plan.PlanNodeId;\n+import com.facebook.presto.spi.plan.TableScanNode;\n+import com.facebook.presto.split.SplitManager;\n+import com.facebook.presto.split.SplitSource;\n import com.facebook.presto.sql.planner.PartitioningHandle;\n import com.facebook.presto.sql.planner.PlanFragment;\n+import com.facebook.presto.sql.planner.SystemPartitioningHandle;\n import com.facebook.presto.sql.planner.plan.PlanFragmentId;\n import com.facebook.presto.sql.planner.plan.RemoteSourceNode;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableSet;\n+import org.apache.spark.Partitioner;\n import org.apache.spark.api.java.JavaPairRDD;\n import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction2;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n \n import javax.inject.Inject;\n \n import java.util.ArrayList;\n+import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n-import java.util.Objects;\n import java.util.Optional;\n import java.util.Set;\n import java.util.stream.Collectors;\n import java.util.stream.IntStream;\n \n+import static com.facebook.airlift.concurrent.MoreFutures.getFutureValue;\n import static com.facebook.presto.SystemSessionProperties.getHashPartitionCount;\n import static com.facebook.presto.spark.PrestoSparkSessionProperties.getSparkInitialPartitionCount;\n import static com.facebook.presto.spark.classloader_interface.TaskProcessors.createTaskProcessor;\n+import static com.facebook.presto.spi.StandardErrorCode.NOT_SUPPORTED;\n+import static com.facebook.presto.spi.connector.ConnectorSplitManager.SplitSchedulingStrategy.UNGROUPED_SCHEDULING;\n+import static com.facebook.presto.spi.connector.NotPartitionedPartitionHandle.NOT_PARTITIONED;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.ARBITRARY_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.COORDINATOR_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_ARBITRARY_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_BROADCAST_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_HASH_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_PASSTHROUGH_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.SCALED_WRITER_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.SINGLE_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.SOURCE_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.optimizations.PlanNodeSearcher.searchFrom;\n import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Verify.verify;\n import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableMap.toImmutableMap;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n import static com.google.common.collect.Iterables.getOnlyElement;\n+import static com.google.common.collect.Sets.difference;\n+import static java.lang.String.format;\n+import static java.util.Collections.shuffle;\n import static java.util.Objects.requireNonNull;\n import static java.util.function.Function.identity;\n import static java.util.stream.Collectors.mapping;\n import static java.util.stream.Collectors.toSet;\n \n public class PrestoSparkRddFactory\n {\n-    private final JsonCodec<PrestoSparkTaskDescriptor> sparkTaskRequestJsonCodec;\n+    private final SplitManager splitManager;\n+    private final Metadata metadata;\n+    private final JsonCodec<PrestoSparkTaskDescriptor> taskDescriptorJsonCodec;\n \n     @Inject\n-    public PrestoSparkRddFactory(JsonCodec<PrestoSparkTaskDescriptor> sparkTaskRequestJsonCodec)\n+    public PrestoSparkRddFactory(SplitManager splitManager, Metadata metadata, JsonCodec<PrestoSparkTaskDescriptor> taskDescriptorJsonCodec)\n     {\n-        this.sparkTaskRequestJsonCodec = requireNonNull(sparkTaskRequestJsonCodec, \"sparkTaskRequestJsonCodec is null\");\n+        this.splitManager = requireNonNull(splitManager, \"splitManager is null\");\n+        this.metadata = requireNonNull(metadata, \"metadata is null\");\n+        this.taskDescriptorJsonCodec = requireNonNull(taskDescriptorJsonCodec, \"taskDescriptorJsonCodec is null\");\n     }\n \n     public JavaPairRDD<Integer, PrestoSparkRow> createSparkRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n-            PrestoSparkPlan prestoSparkPlan,\n-            PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector)\n+            PlanFragment fragment,\n+            Map<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> rddInputs,\n+            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            TableWriteInfo tableWriteInfo)\n     {\n-        RddFactory rddFactory = new RddFactory(\n-                session,\n-                sparkTaskRequestJsonCodec,\n-                sparkContext,\n-                taskExecutorFactoryProvider,\n-                getSparkInitialPartitionCount(session),\n-                getHashPartitionCount(session),\n-                taskStatsCollector,\n-                prestoSparkPlan.getTableWriteInfo());\n-        return rddFactory.createRdd(prestoSparkPlan.getPlan());\n-    }\n+        checkArgument(!fragment.getStageExecutionDescriptor().isStageGroupedExecution(), \"unexpected grouped execution fragment: %s\", fragment.getId());\n \n-    private static class RddFactory\n-    {\n-        private final Session session;\n-        private final JsonCodec<PrestoSparkTaskDescriptor> sparkTaskDescriptorJsonCodec;\n-        private final JavaSparkContext sparkContext;\n-        private final PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider;\n-        private final int initialSparkPartitionCount;\n-        private final int hashPartitionCount;\n-        private final CollectionAccumulator<SerializedTaskStats> taskStatsCollector;\n-        private final TableWriteInfo tableWriteInfo;\n-\n-        private RddFactory(\n-                Session session,\n-                JsonCodec<PrestoSparkTaskDescriptor> sparkTaskDescriptorJsonCodec,\n-                JavaSparkContext sparkContext,\n-                PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n-                int initialSparkPartitionCount,\n-                int hashPartitionCount,\n-                CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-                TableWriteInfo tableWriteInfo)\n-        {\n-            this.session = requireNonNull(session, \"session is null\");\n-            this.sparkTaskDescriptorJsonCodec = requireNonNull(sparkTaskDescriptorJsonCodec, \"sparkTaskDescriptorJsonCodec is null\");\n-            this.sparkContext = requireNonNull(sparkContext, \"sparkContext is null\");\n-            this.executorFactoryProvider = requireNonNull(executorFactoryProvider, \"executorFactoryProvider is null\");\n-            this.initialSparkPartitionCount = initialSparkPartitionCount;\n-            this.hashPartitionCount = hashPartitionCount;\n-            this.taskStatsCollector = requireNonNull(taskStatsCollector, \"taskStatsCollector is null\");\n-            this.tableWriteInfo = requireNonNull(tableWriteInfo, \"tableWriteInfo is null\");\n+        PartitioningHandle partitioning = fragment.getPartitioning();\n+\n+        if (!(partitioning.getConnectorHandle() instanceof SystemPartitioningHandle)) {\n+            // TODO: add support for bucketed table\n+            throw new PrestoException(NOT_SUPPORTED, \"Partitioned (bucketed) tables are not yet supported by Presto on Spark\");\n         }\n \n-        public JavaPairRDD<Integer, PrestoSparkRow> createRdd(PrestoSparkSubPlan subPlan)\n-        {\n-            PlanFragment fragment;\n-            // TODO: fragment adaption should be done prior to RDD creation\n-            if (subPlan.getFragment().getPartitioningScheme().getPartitioning().getHandle().equals(FIXED_HASH_DISTRIBUTION)) {\n-                fragment = subPlan.getFragment().withBucketToPartition(Optional.of(IntStream.range(0, hashPartitionCount).toArray()));\n-            }\n-            else {\n-                fragment = subPlan.getFragment();\n-            }\n+        if (partitioning.equals(SCALED_WRITER_DISTRIBUTION)) {\n+            throw new PrestoException(NOT_SUPPORTED, \"Automatic writers scaling is not supported by Presto on Spark\");\n+        }\n \n-            checkArgument(!fragment.getStageExecutionDescriptor().isStageGroupedExecution(), \"unexpected grouped execution fragment: %s\", fragment.getId());\n+        checkArgument(!partitioning.equals(COORDINATOR_DISTRIBUTION), \"COORDINATOR_DISTRIBUTION fragment must be run on the driver\");\n+        checkArgument(!partitioning.equals(FIXED_BROADCAST_DISTRIBUTION), \"FIXED_BROADCAST_DISTRIBUTION can only be set as an output partitioning scheme, and not as a fragment distribution\");\n+        checkArgument(!partitioning.equals(FIXED_PASSTHROUGH_DISTRIBUTION), \"FIXED_PASSTHROUGH_DISTRIBUTION can only be set as local exchange partitioning\");\n \n-            // scans\n-            List<PlanNodeId> tableScans = fragment.getTableScanSchedulingOrder();\n+        // TODO: ARBITRARY_DISTRIBUTION is something very weird.\n+        // TODO: It doesn't have partitioning function, and it is never set as a fragment partitioning.\n+        // TODO: We should consider removing ARBITRARY_DISTRIBUTION.\n+        checkArgument(!partitioning.equals(ARBITRARY_DISTRIBUTION), \"ARBITRARY_DISTRIBUTION is not expected to be set as a fragment distribution\");\n \n-            // source stages\n-            List<RemoteSourceNode> remoteSources = fragment.getRemoteSourceNodes();\n-            checkArgument(tableScans.isEmpty() || remoteSources.isEmpty(), \"stages that have both, remote sources and table scans, are not supported\");\n+        int hashPartitionCount = getHashPartitionCount(session);\n \n-            if (!tableScans.isEmpty()) {\n-                checkArgument(fragment.getPartitioning().equals(SOURCE_DISTRIBUTION), \"unexpected table scan partitioning: %s\", fragment.getPartitioning());\n+        // configure number of output partitions\n+        if (fragment.getPartitioningScheme().getPartitioning().getHandle().equals(FIXED_HASH_DISTRIBUTION)) {\n+            fragment = fragment.withBucketToPartition(Optional.of(IntStream.range(0, hashPartitionCount).toArray()));\n+        }\n \n-                // get all scheduled splits\n-                List<ScheduledSplit> scheduledSplits = subPlan.getTaskSources().stream()\n-                        .flatMap(taskSource -> taskSource.getSplits().stream())\n-                        .collect(toImmutableList());\n+        if (partitioning.equals(SINGLE_DISTRIBUTION) || partitioning.equals(FIXED_HASH_DISTRIBUTION) || partitioning.equals(FIXED_ARBITRARY_DISTRIBUTION)) {\n+            checkArgument(\n+                    fragment.getTableScanSchedulingOrder().isEmpty(),\n+                    \"Fragment with is not expected to have table scans. fragmentId: %s, fragment partitioning %s\",\n+                    fragment.getId(),\n+                    fragment.getPartitioning());\n+\n+            for (RemoteSourceNode remoteSource : fragment.getRemoteSourceNodes()) {\n+                if (remoteSource.isEnsureSourceOrdering() || remoteSource.getOrderingScheme().isPresent()) {\n+                    throw new PrestoException(NOT_SUPPORTED, format(\n+                            \"Order sensitive exchange is not supported by Presto on Spark. fragmentId: %s, sourceFragmentIds: %s\",\n+                            fragment.getId(),\n+                            remoteSource.getSourceFragmentIds()));\n+                }\n+            }\n \n-                // get scheduled splits by task\n-                List<List<ScheduledSplit>> assignedSplits = assignSplitsToTasks(scheduledSplits, initialSparkPartitionCount);\n+            Partitioner inputPartitioner = createPartitioner(\n+                    partitioning,\n+                    // TODO: consider using getMaxTasksPerStage\n+                    hashPartitionCount);\n \n-                List<SerializedPrestoSparkTaskDescriptor> serializedRequests = assignedSplits.stream()\n-                        .map(splits -> createTaskDescriptor(fragment, splits))\n-                        .map(sparkTaskDescriptorJsonCodec::toJsonBytes)\n-                        .map(SerializedPrestoSparkTaskDescriptor::new)\n-                        .collect(toImmutableList());\n+            Map<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> partitionedInputs = rddInputs.entrySet().stream()\n+                    .collect(toImmutableMap(Map.Entry::getKey, entry -> entry.getValue().partitionBy(inputPartitioner)));\n \n-                return sparkContext.parallelize(serializedRequests, initialSparkPartitionCount)\n-                        .mapPartitionsToPair(createTaskProcessor(executorFactoryProvider, taskStatsCollector));\n-            }\n+            return createIntermediateRdd(\n+                    session,\n+                    fragment,\n+                    executorFactoryProvider,\n+                    taskStatsCollector,\n+                    tableWriteInfo,\n+                    partitionedInputs);\n+        }\n+        else if (partitioning.equals(SOURCE_DISTRIBUTION)) {\n+            checkArgument(rddInputs.isEmpty(), \"rddInputs is expected to be empty for SOURCE_DISTRIBUTION fragment: %s\", fragment.getId());\n+            return createSourceRdd(\n+                    sparkContext,\n+                    session,\n+                    fragment,\n+                    executorFactoryProvider,\n+                    taskStatsCollector,\n+                    tableWriteInfo);\n+        }\n+        else {\n+            throw new IllegalArgumentException(format(\"Unexpected fragment partitioning %s, fragmentId: %s\", partitioning, fragment.getId()));\n+        }\n+    }\n \n-            List<PrestoSparkSubPlan> children = subPlan.getChildren();\n-            checkArgument(\n-                    remoteSources.size() == children.size(),\n-                    \"number of remote sources doesn't match the number of child stages: %s != %s\",\n-                    remoteSources.size(),\n-                    children.size());\n-\n-            if (children.size() == 1) {\n-                // Single remote source\n-                PrestoSparkSubPlan childSubPlan = getOnlyElement(children);\n-                JavaPairRDD<Integer, PrestoSparkRow> childRdd = createRdd(childSubPlan);\n-                PartitioningHandle partitioning = fragment.getPartitioning();\n-\n-                if (partitioning.equals(COORDINATOR_DISTRIBUTION)) {\n-                    // coordinator side work will be handled after JavaPairRDD#collect() call in PrestoSparkExecution\n-                    return childRdd;\n-                }\n+    private static Partitioner createPartitioner(PartitioningHandle partitioning, int partitionCount)\n+    {\n+        if (partitioning.equals(SINGLE_DISTRIBUTION)) {\n+            return new IntegerIdentityPartitioner(1);\n+        }\n+        if (partitioning.equals(FIXED_HASH_DISTRIBUTION)) {\n+            return new IntegerIdentityPartitioner(partitionCount);\n+        }\n+        if (partitioning.equals(FIXED_ARBITRARY_DISTRIBUTION)) {\n+            throw new PrestoException(NOT_SUPPORTED, \"FIXED_ARBITRARY_DISTRIBUTION partitioning is not yet supported\");\n+        }\n+        throw new IllegalArgumentException(format(\"Unexpected fragment partitioning %s\", partitioning));\n+    }\n \n-                PlanFragment childFragment = childSubPlan.getFragment();\n-                RemoteSourceNode remoteSource = getOnlyElement(remoteSources);\n-                List<PlanFragmentId> sourceFragmentIds = remoteSource.getSourceFragmentIds();\n-                checkArgument(sourceFragmentIds.size() == 1, \"expected to have exactly only a single source fragment\");\n-                checkArgument(childFragment.getId().equals(getOnlyElement(sourceFragmentIds)));\n-\n-                PrestoSparkTaskDescriptor taskDescriptor = createTaskDescriptor(fragment, ImmutableList.of());\n-                SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(sparkTaskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-                if (partitioning.equals(FIXED_HASH_DISTRIBUTION) ||\n-                        // when single distribution - there will be a single partition 0\n-                        partitioning.equals(SINGLE_DISTRIBUTION)) {\n-                    String planNodeId = remoteSource.getId().toString();\n-                    return childRdd\n-                            .partitionBy(partitioning.equals(FIXED_HASH_DISTRIBUTION) ? new IntegerIdentityPartitioner(hashPartitionCount) : new IntegerIdentityPartitioner(1))\n-                            .mapPartitionsToPair(createTaskProcessor(executorFactoryProvider, serializedTaskDescriptor, planNodeId, taskStatsCollector));\n-                }\n-                else {\n-                    // TODO: support (or do check state over) the following fragment partitioning:\n-                    //  - SOURCE_DISTRIBUTION\n-                    //  - FIXED_PASSTHROUGH_DISTRIBUTION\n-                    //  - ARBITRARY_DISTRIBUTION\n-                    //  - SCALED_WRITER_DISTRIBUTION\n-                    //  - FIXED_BROADCAST_DISTRIBUTION\n-                    //  - FIXED_ARBITRARY_DISTRIBUTION\n-                    throw new IllegalArgumentException(\"Unsupported fragment partitioning: \" + partitioning);\n-                }\n-            }\n-            else if (children.size() == 2) {\n-                // TODO: support N way join\n-                PrestoSparkSubPlan leftSubPlan = children.get(0);\n-                PrestoSparkSubPlan rightSubPlan = children.get(1);\n-\n-                RemoteSourceNode leftRemoteSource = remoteSources.get(0);\n-                RemoteSourceNode rightRemoteSource = remoteSources.get(1);\n-\n-                // We need String representation since PlanNodeId is not serializable...\n-                String leftRemoteSourcePlanId = leftRemoteSource.getId().toString();\n-                String rightRemoteSourcePlanId = rightRemoteSource.getId().toString();\n-\n-                JavaPairRDD<Integer, PrestoSparkRow> leftChildRdd = createRdd(leftSubPlan);\n-                JavaPairRDD<Integer, PrestoSparkRow> rightChildRdd = createRdd(rightSubPlan);\n-\n-                PlanFragment leftFragment = leftSubPlan.getFragment();\n-                PlanFragment rightFragment = rightSubPlan.getFragment();\n-\n-                List<PlanFragmentId> leftFragmentIds = leftRemoteSource.getSourceFragmentIds();\n-                checkArgument(leftFragmentIds.size() == 1, \"expected to have exactly only a single source fragment\");\n-                checkArgument(leftFragment.getId().equals(getOnlyElement(leftFragmentIds)));\n-                List<PlanFragmentId> rightFragmentIds = rightRemoteSource.getSourceFragmentIds();\n-                checkArgument(rightFragmentIds.size() == 1, \"expected to have exactly only a single source fragment\");\n-                checkArgument(rightFragment.getId().equals(getOnlyElement(rightFragmentIds)));\n-\n-                // This fragment only contains remote source, thus there is no splits\n-                PrestoSparkTaskDescriptor taskDescriptor = createTaskDescriptor(fragment, ImmutableList.of());\n-                SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(sparkTaskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-                PartitioningHandle partitioning = fragment.getPartitioning();\n-                checkArgument(partitioning.equals(FIXED_HASH_DISTRIBUTION));\n-\n-                JavaPairRDD<Integer, PrestoSparkRow> shuffledLeftChildRdd = leftChildRdd.partitionBy(new IntegerIdentityPartitioner(hashPartitionCount));\n-                JavaPairRDD<Integer, PrestoSparkRow> shuffledRightChildRdd = rightChildRdd.partitionBy(new IntegerIdentityPartitioner(hashPartitionCount));\n-                return JavaPairRDD.fromJavaRDD(\n-                        shuffledLeftChildRdd.zipPartitions(\n-                                shuffledRightChildRdd,\n-                                createTaskProcessor(executorFactoryProvider, serializedTaskDescriptor, leftRemoteSourcePlanId, rightRemoteSourcePlanId, taskStatsCollector)));\n-            }\n-            else {\n-                throw new UnsupportedOperationException();\n-            }\n+    private JavaPairRDD<Integer, PrestoSparkRow> createIntermediateRdd(\n+            Session session,\n+            PlanFragment fragment,\n+            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            TableWriteInfo tableWriteInfo,\n+            Map<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> rddInputs)\n+    {\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        verify(tableScans.isEmpty(), \"no table scans is expected\");\n+\n+        Set<PlanFragmentId> expectedInputs = fragment.getRemoteSourceNodes().stream()\n+                .map(RemoteSourceNode::getSourceFragmentIds)\n+                .flatMap(List::stream)\n+                .collect(toImmutableSet());\n+\n+        Set<PlanFragmentId> missingInputs = difference(expectedInputs, rddInputs.keySet());\n+        Set<PlanFragmentId> extraInputs = difference(rddInputs.keySet(), expectedInputs);\n+        checkArgument(\n+                missingInputs.isEmpty() && extraInputs.isEmpty(),\n+                \"rddInputs mismatch discovered. expected: %s, actual: %s\",\n+                expectedInputs,\n+                rddInputs.keySet());\n+\n+        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n+\n+        if (rddInputs.size() == 1) {\n+            RemoteSourceNode remoteSourceNode = getOnlyElement(fragment.getRemoteSourceNodes());\n+            PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> taskProcessor =\n+                    createTaskProcessor(\n+                            executorFactoryProvider,\n+                            serializedTaskDescriptor,\n+                            remoteSourceNode.getId().toString(),\n+                            taskStatsCollector);\n+            return getOnlyElement(rddInputs.values())\n+                    .mapPartitionsToPair(taskProcessor);\n+        }\n+        else if (rddInputs.size() == 2) {\n+            List<RemoteSourceNode> remoteSources = fragment.getRemoteSourceNodes();\n+            checkArgument(remoteSources.size() == 2, \"two remote sources are expected, got: %s\", remoteSources.size());\n+            RemoteSourceNode firstRemoteSource = remoteSources.get(0);\n+            RemoteSourceNode secondRemoteSource = remoteSources.get(1);\n+            JavaPairRDD<Integer, PrestoSparkRow> firstRdd = rddInputs.get(firstRemoteSource.getSourceFragmentIds().get(0));\n+            JavaPairRDD<Integer, PrestoSparkRow> secondRdd = rddInputs.get(secondRemoteSource.getSourceFragmentIds().get(0));\n+            FlatMapFunction2<Iterator<Tuple2<Integer, PrestoSparkRow>>, Iterator<Tuple2<Integer, PrestoSparkRow>>, Tuple2<Integer, PrestoSparkRow>> taskProcessor =\n+                    createTaskProcessor(\n+                            executorFactoryProvider,\n+                            serializedTaskDescriptor,\n+                            firstRemoteSource.getId().toString(),\n+                            secondRemoteSource.getId().toString(),\n+                            taskStatsCollector);\n+            return JavaPairRDD.fromJavaRDD(\n+                    firstRdd.zipPartitions(\n+                            secondRdd,\n+                            taskProcessor));\n         }\n \n-        private static List<List<ScheduledSplit>> assignSplitsToTasks(List<ScheduledSplit> scheduledSplits, int numTasks)\n-        {\n-            List<List<ScheduledSplit>> assignedSplits = new ArrayList<>();\n-            for (int i = 0; i < numTasks; i++) {\n-                assignedSplits.add(new ArrayList<>());\n-            }\n+        throw new IllegalArgumentException(format(\"unsupported number of inputs: %s\", rddInputs.size()));\n+    }\n \n-            for (ScheduledSplit split : scheduledSplits) {\n-                int taskId = Objects.hash(split.getPlanNodeId(), split.getSequenceId()) % numTasks;\n-                if (taskId < 0) {\n-                    taskId += numTasks;\n-                }\n+    private JavaPairRDD<Integer, PrestoSparkRow> createSourceRdd(\n+            JavaSparkContext sparkContext,\n+            Session session,\n+            PlanFragment fragment,\n+            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            TableWriteInfo tableWriteInfo)\n+    {\n+        // TODO: Possible in case of a broadcast join\n+        checkArgument(fragment.getRemoteSourceNodes().isEmpty(), \"source task with remote sources is not supported\");\n+\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        checkArgument(\n+                tableScans.size() == 1,\n+                \"exactly one table scan is expected in SOURCE_DISTRIBUTION fragment. fragmentId: %s, actual number of table scans: %s\",\n+                fragment.getId(),\n+                tableScans.size());\n+        verify(tableScans.size() == fragment.getTableScanSchedulingOrder().size());", "originalCommit": "3d4bd6d4836e14974b9247082f649bc5bad82938", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzE0NDMxOQ==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423144319", "bodyText": "If the number of tableScanSchedulingOrder is not equal to the number of table scans that means that the plan generation is somehow broken, thus it feels like an assertion", "author": "arhimondr", "createdAt": "2020-05-11T15:57:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4NDk0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM3NTEyNQ==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423375125", "bodyText": "@arhimondr : But the previous assertion already asserts tableScans.size() is always 1...", "author": "wenleix", "createdAt": "2020-05-11T23:24:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4NDk0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM3ODIzMw==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423378233", "bodyText": "Actually yeah, let me simply remove this assertion", "author": "arhimondr", "createdAt": "2020-05-11T23:33:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4NDk0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4NTQ4Ng==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r422785486", "bodyText": "I am not sure. If so then we should also use getMaxTasksPerStage for the initial partition count : \n  \n    \n      presto/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n    \n    \n        Lines 161 to 162\n      in\n      472538a\n    \n    \n    \n    \n\n        \n          \n           return sparkContext.parallelize(serializedRequests, initialSparkPartitionCount) \n        \n\n        \n          \n                   .mapPartitionsToPair(createTaskProcessor(executorFactoryProvider, taskStatsCollector));", "author": "wenleix", "createdAt": "2020-05-11T05:16:49Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -15,283 +15,367 @@\n \n import com.facebook.airlift.json.JsonCodec;\n import com.facebook.presto.Session;\n+import com.facebook.presto.execution.Lifespan;\n import com.facebook.presto.execution.ScheduledSplit;\n import com.facebook.presto.execution.TaskSource;\n import com.facebook.presto.execution.scheduler.TableWriteInfo;\n+import com.facebook.presto.metadata.Metadata;\n+import com.facebook.presto.metadata.Split;\n import com.facebook.presto.spark.PrestoSparkTaskDescriptor;\n import com.facebook.presto.spark.classloader_interface.IntegerIdentityPartitioner;\n import com.facebook.presto.spark.classloader_interface.PrestoSparkRow;\n import com.facebook.presto.spark.classloader_interface.PrestoSparkTaskExecutorFactoryProvider;\n import com.facebook.presto.spark.classloader_interface.SerializedPrestoSparkTaskDescriptor;\n import com.facebook.presto.spark.classloader_interface.SerializedTaskStats;\n+import com.facebook.presto.spi.PrestoException;\n+import com.facebook.presto.spi.plan.PlanNode;\n import com.facebook.presto.spi.plan.PlanNodeId;\n+import com.facebook.presto.spi.plan.TableScanNode;\n+import com.facebook.presto.split.SplitManager;\n+import com.facebook.presto.split.SplitSource;\n import com.facebook.presto.sql.planner.PartitioningHandle;\n import com.facebook.presto.sql.planner.PlanFragment;\n+import com.facebook.presto.sql.planner.SystemPartitioningHandle;\n import com.facebook.presto.sql.planner.plan.PlanFragmentId;\n import com.facebook.presto.sql.planner.plan.RemoteSourceNode;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableSet;\n+import org.apache.spark.Partitioner;\n import org.apache.spark.api.java.JavaPairRDD;\n import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction2;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n \n import javax.inject.Inject;\n \n import java.util.ArrayList;\n+import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n-import java.util.Objects;\n import java.util.Optional;\n import java.util.Set;\n import java.util.stream.Collectors;\n import java.util.stream.IntStream;\n \n+import static com.facebook.airlift.concurrent.MoreFutures.getFutureValue;\n import static com.facebook.presto.SystemSessionProperties.getHashPartitionCount;\n import static com.facebook.presto.spark.PrestoSparkSessionProperties.getSparkInitialPartitionCount;\n import static com.facebook.presto.spark.classloader_interface.TaskProcessors.createTaskProcessor;\n+import static com.facebook.presto.spi.StandardErrorCode.NOT_SUPPORTED;\n+import static com.facebook.presto.spi.connector.ConnectorSplitManager.SplitSchedulingStrategy.UNGROUPED_SCHEDULING;\n+import static com.facebook.presto.spi.connector.NotPartitionedPartitionHandle.NOT_PARTITIONED;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.ARBITRARY_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.COORDINATOR_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_ARBITRARY_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_BROADCAST_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_HASH_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_PASSTHROUGH_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.SCALED_WRITER_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.SINGLE_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.SOURCE_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.optimizations.PlanNodeSearcher.searchFrom;\n import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Verify.verify;\n import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableMap.toImmutableMap;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n import static com.google.common.collect.Iterables.getOnlyElement;\n+import static com.google.common.collect.Sets.difference;\n+import static java.lang.String.format;\n+import static java.util.Collections.shuffle;\n import static java.util.Objects.requireNonNull;\n import static java.util.function.Function.identity;\n import static java.util.stream.Collectors.mapping;\n import static java.util.stream.Collectors.toSet;\n \n public class PrestoSparkRddFactory\n {\n-    private final JsonCodec<PrestoSparkTaskDescriptor> sparkTaskRequestJsonCodec;\n+    private final SplitManager splitManager;\n+    private final Metadata metadata;\n+    private final JsonCodec<PrestoSparkTaskDescriptor> taskDescriptorJsonCodec;\n \n     @Inject\n-    public PrestoSparkRddFactory(JsonCodec<PrestoSparkTaskDescriptor> sparkTaskRequestJsonCodec)\n+    public PrestoSparkRddFactory(SplitManager splitManager, Metadata metadata, JsonCodec<PrestoSparkTaskDescriptor> taskDescriptorJsonCodec)\n     {\n-        this.sparkTaskRequestJsonCodec = requireNonNull(sparkTaskRequestJsonCodec, \"sparkTaskRequestJsonCodec is null\");\n+        this.splitManager = requireNonNull(splitManager, \"splitManager is null\");\n+        this.metadata = requireNonNull(metadata, \"metadata is null\");\n+        this.taskDescriptorJsonCodec = requireNonNull(taskDescriptorJsonCodec, \"taskDescriptorJsonCodec is null\");\n     }\n \n     public JavaPairRDD<Integer, PrestoSparkRow> createSparkRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n-            PrestoSparkPlan prestoSparkPlan,\n-            PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector)\n+            PlanFragment fragment,\n+            Map<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> rddInputs,\n+            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            TableWriteInfo tableWriteInfo)\n     {\n-        RddFactory rddFactory = new RddFactory(\n-                session,\n-                sparkTaskRequestJsonCodec,\n-                sparkContext,\n-                taskExecutorFactoryProvider,\n-                getSparkInitialPartitionCount(session),\n-                getHashPartitionCount(session),\n-                taskStatsCollector,\n-                prestoSparkPlan.getTableWriteInfo());\n-        return rddFactory.createRdd(prestoSparkPlan.getPlan());\n-    }\n+        checkArgument(!fragment.getStageExecutionDescriptor().isStageGroupedExecution(), \"unexpected grouped execution fragment: %s\", fragment.getId());\n \n-    private static class RddFactory\n-    {\n-        private final Session session;\n-        private final JsonCodec<PrestoSparkTaskDescriptor> sparkTaskDescriptorJsonCodec;\n-        private final JavaSparkContext sparkContext;\n-        private final PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider;\n-        private final int initialSparkPartitionCount;\n-        private final int hashPartitionCount;\n-        private final CollectionAccumulator<SerializedTaskStats> taskStatsCollector;\n-        private final TableWriteInfo tableWriteInfo;\n-\n-        private RddFactory(\n-                Session session,\n-                JsonCodec<PrestoSparkTaskDescriptor> sparkTaskDescriptorJsonCodec,\n-                JavaSparkContext sparkContext,\n-                PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n-                int initialSparkPartitionCount,\n-                int hashPartitionCount,\n-                CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-                TableWriteInfo tableWriteInfo)\n-        {\n-            this.session = requireNonNull(session, \"session is null\");\n-            this.sparkTaskDescriptorJsonCodec = requireNonNull(sparkTaskDescriptorJsonCodec, \"sparkTaskDescriptorJsonCodec is null\");\n-            this.sparkContext = requireNonNull(sparkContext, \"sparkContext is null\");\n-            this.executorFactoryProvider = requireNonNull(executorFactoryProvider, \"executorFactoryProvider is null\");\n-            this.initialSparkPartitionCount = initialSparkPartitionCount;\n-            this.hashPartitionCount = hashPartitionCount;\n-            this.taskStatsCollector = requireNonNull(taskStatsCollector, \"taskStatsCollector is null\");\n-            this.tableWriteInfo = requireNonNull(tableWriteInfo, \"tableWriteInfo is null\");\n+        PartitioningHandle partitioning = fragment.getPartitioning();\n+\n+        if (!(partitioning.getConnectorHandle() instanceof SystemPartitioningHandle)) {\n+            // TODO: add support for bucketed table\n+            throw new PrestoException(NOT_SUPPORTED, \"Partitioned (bucketed) tables are not yet supported by Presto on Spark\");\n         }\n \n-        public JavaPairRDD<Integer, PrestoSparkRow> createRdd(PrestoSparkSubPlan subPlan)\n-        {\n-            PlanFragment fragment;\n-            // TODO: fragment adaption should be done prior to RDD creation\n-            if (subPlan.getFragment().getPartitioningScheme().getPartitioning().getHandle().equals(FIXED_HASH_DISTRIBUTION)) {\n-                fragment = subPlan.getFragment().withBucketToPartition(Optional.of(IntStream.range(0, hashPartitionCount).toArray()));\n-            }\n-            else {\n-                fragment = subPlan.getFragment();\n-            }\n+        if (partitioning.equals(SCALED_WRITER_DISTRIBUTION)) {\n+            throw new PrestoException(NOT_SUPPORTED, \"Automatic writers scaling is not supported by Presto on Spark\");\n+        }\n \n-            checkArgument(!fragment.getStageExecutionDescriptor().isStageGroupedExecution(), \"unexpected grouped execution fragment: %s\", fragment.getId());\n+        checkArgument(!partitioning.equals(COORDINATOR_DISTRIBUTION), \"COORDINATOR_DISTRIBUTION fragment must be run on the driver\");\n+        checkArgument(!partitioning.equals(FIXED_BROADCAST_DISTRIBUTION), \"FIXED_BROADCAST_DISTRIBUTION can only be set as an output partitioning scheme, and not as a fragment distribution\");\n+        checkArgument(!partitioning.equals(FIXED_PASSTHROUGH_DISTRIBUTION), \"FIXED_PASSTHROUGH_DISTRIBUTION can only be set as local exchange partitioning\");\n \n-            // scans\n-            List<PlanNodeId> tableScans = fragment.getTableScanSchedulingOrder();\n+        // TODO: ARBITRARY_DISTRIBUTION is something very weird.\n+        // TODO: It doesn't have partitioning function, and it is never set as a fragment partitioning.\n+        // TODO: We should consider removing ARBITRARY_DISTRIBUTION.\n+        checkArgument(!partitioning.equals(ARBITRARY_DISTRIBUTION), \"ARBITRARY_DISTRIBUTION is not expected to be set as a fragment distribution\");\n \n-            // source stages\n-            List<RemoteSourceNode> remoteSources = fragment.getRemoteSourceNodes();\n-            checkArgument(tableScans.isEmpty() || remoteSources.isEmpty(), \"stages that have both, remote sources and table scans, are not supported\");\n+        int hashPartitionCount = getHashPartitionCount(session);\n \n-            if (!tableScans.isEmpty()) {\n-                checkArgument(fragment.getPartitioning().equals(SOURCE_DISTRIBUTION), \"unexpected table scan partitioning: %s\", fragment.getPartitioning());\n+        // configure number of output partitions\n+        if (fragment.getPartitioningScheme().getPartitioning().getHandle().equals(FIXED_HASH_DISTRIBUTION)) {\n+            fragment = fragment.withBucketToPartition(Optional.of(IntStream.range(0, hashPartitionCount).toArray()));\n+        }\n \n-                // get all scheduled splits\n-                List<ScheduledSplit> scheduledSplits = subPlan.getTaskSources().stream()\n-                        .flatMap(taskSource -> taskSource.getSplits().stream())\n-                        .collect(toImmutableList());\n+        if (partitioning.equals(SINGLE_DISTRIBUTION) || partitioning.equals(FIXED_HASH_DISTRIBUTION) || partitioning.equals(FIXED_ARBITRARY_DISTRIBUTION)) {\n+            checkArgument(\n+                    fragment.getTableScanSchedulingOrder().isEmpty(),\n+                    \"Fragment with is not expected to have table scans. fragmentId: %s, fragment partitioning %s\",\n+                    fragment.getId(),\n+                    fragment.getPartitioning());\n+\n+            for (RemoteSourceNode remoteSource : fragment.getRemoteSourceNodes()) {\n+                if (remoteSource.isEnsureSourceOrdering() || remoteSource.getOrderingScheme().isPresent()) {\n+                    throw new PrestoException(NOT_SUPPORTED, format(\n+                            \"Order sensitive exchange is not supported by Presto on Spark. fragmentId: %s, sourceFragmentIds: %s\",\n+                            fragment.getId(),\n+                            remoteSource.getSourceFragmentIds()));\n+                }\n+            }\n \n-                // get scheduled splits by task\n-                List<List<ScheduledSplit>> assignedSplits = assignSplitsToTasks(scheduledSplits, initialSparkPartitionCount);\n+            Partitioner inputPartitioner = createPartitioner(\n+                    partitioning,\n+                    // TODO: consider using getMaxTasksPerStage", "originalCommit": "3d4bd6d4836e14974b9247082f649bc5bad82938", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzE0NDgzMg==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423144832", "bodyText": "I agree this comment is somehow confusing. Let me remove it for now.", "author": "arhimondr", "createdAt": "2020-05-11T15:58:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4NTQ4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4NjMzNA==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r422786334", "bodyText": "nit: Add a TODO state we want to refactor List<List<ScheduledSplit>> to be a dedicated class such as SplitAssignment.\nThe motivation is while I suggest this part of code, when I reread my own code my first question is : hmm... what's this List of List? ...", "author": "wenleix", "createdAt": "2020-05-11T05:19:45Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -15,283 +15,367 @@\n \n import com.facebook.airlift.json.JsonCodec;\n import com.facebook.presto.Session;\n+import com.facebook.presto.execution.Lifespan;\n import com.facebook.presto.execution.ScheduledSplit;\n import com.facebook.presto.execution.TaskSource;\n import com.facebook.presto.execution.scheduler.TableWriteInfo;\n+import com.facebook.presto.metadata.Metadata;\n+import com.facebook.presto.metadata.Split;\n import com.facebook.presto.spark.PrestoSparkTaskDescriptor;\n import com.facebook.presto.spark.classloader_interface.IntegerIdentityPartitioner;\n import com.facebook.presto.spark.classloader_interface.PrestoSparkRow;\n import com.facebook.presto.spark.classloader_interface.PrestoSparkTaskExecutorFactoryProvider;\n import com.facebook.presto.spark.classloader_interface.SerializedPrestoSparkTaskDescriptor;\n import com.facebook.presto.spark.classloader_interface.SerializedTaskStats;\n+import com.facebook.presto.spi.PrestoException;\n+import com.facebook.presto.spi.plan.PlanNode;\n import com.facebook.presto.spi.plan.PlanNodeId;\n+import com.facebook.presto.spi.plan.TableScanNode;\n+import com.facebook.presto.split.SplitManager;\n+import com.facebook.presto.split.SplitSource;\n import com.facebook.presto.sql.planner.PartitioningHandle;\n import com.facebook.presto.sql.planner.PlanFragment;\n+import com.facebook.presto.sql.planner.SystemPartitioningHandle;\n import com.facebook.presto.sql.planner.plan.PlanFragmentId;\n import com.facebook.presto.sql.planner.plan.RemoteSourceNode;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableSet;\n+import org.apache.spark.Partitioner;\n import org.apache.spark.api.java.JavaPairRDD;\n import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction2;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n \n import javax.inject.Inject;\n \n import java.util.ArrayList;\n+import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n-import java.util.Objects;\n import java.util.Optional;\n import java.util.Set;\n import java.util.stream.Collectors;\n import java.util.stream.IntStream;\n \n+import static com.facebook.airlift.concurrent.MoreFutures.getFutureValue;\n import static com.facebook.presto.SystemSessionProperties.getHashPartitionCount;\n import static com.facebook.presto.spark.PrestoSparkSessionProperties.getSparkInitialPartitionCount;\n import static com.facebook.presto.spark.classloader_interface.TaskProcessors.createTaskProcessor;\n+import static com.facebook.presto.spi.StandardErrorCode.NOT_SUPPORTED;\n+import static com.facebook.presto.spi.connector.ConnectorSplitManager.SplitSchedulingStrategy.UNGROUPED_SCHEDULING;\n+import static com.facebook.presto.spi.connector.NotPartitionedPartitionHandle.NOT_PARTITIONED;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.ARBITRARY_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.COORDINATOR_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_ARBITRARY_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_BROADCAST_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_HASH_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_PASSTHROUGH_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.SCALED_WRITER_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.SINGLE_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.SOURCE_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.optimizations.PlanNodeSearcher.searchFrom;\n import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Verify.verify;\n import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableMap.toImmutableMap;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n import static com.google.common.collect.Iterables.getOnlyElement;\n+import static com.google.common.collect.Sets.difference;\n+import static java.lang.String.format;\n+import static java.util.Collections.shuffle;\n import static java.util.Objects.requireNonNull;\n import static java.util.function.Function.identity;\n import static java.util.stream.Collectors.mapping;\n import static java.util.stream.Collectors.toSet;\n \n public class PrestoSparkRddFactory\n {\n-    private final JsonCodec<PrestoSparkTaskDescriptor> sparkTaskRequestJsonCodec;\n+    private final SplitManager splitManager;\n+    private final Metadata metadata;\n+    private final JsonCodec<PrestoSparkTaskDescriptor> taskDescriptorJsonCodec;\n \n     @Inject\n-    public PrestoSparkRddFactory(JsonCodec<PrestoSparkTaskDescriptor> sparkTaskRequestJsonCodec)\n+    public PrestoSparkRddFactory(SplitManager splitManager, Metadata metadata, JsonCodec<PrestoSparkTaskDescriptor> taskDescriptorJsonCodec)\n     {\n-        this.sparkTaskRequestJsonCodec = requireNonNull(sparkTaskRequestJsonCodec, \"sparkTaskRequestJsonCodec is null\");\n+        this.splitManager = requireNonNull(splitManager, \"splitManager is null\");\n+        this.metadata = requireNonNull(metadata, \"metadata is null\");\n+        this.taskDescriptorJsonCodec = requireNonNull(taskDescriptorJsonCodec, \"taskDescriptorJsonCodec is null\");\n     }\n \n     public JavaPairRDD<Integer, PrestoSparkRow> createSparkRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n-            PrestoSparkPlan prestoSparkPlan,\n-            PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector)\n+            PlanFragment fragment,\n+            Map<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> rddInputs,\n+            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            TableWriteInfo tableWriteInfo)\n     {\n-        RddFactory rddFactory = new RddFactory(\n-                session,\n-                sparkTaskRequestJsonCodec,\n-                sparkContext,\n-                taskExecutorFactoryProvider,\n-                getSparkInitialPartitionCount(session),\n-                getHashPartitionCount(session),\n-                taskStatsCollector,\n-                prestoSparkPlan.getTableWriteInfo());\n-        return rddFactory.createRdd(prestoSparkPlan.getPlan());\n-    }\n+        checkArgument(!fragment.getStageExecutionDescriptor().isStageGroupedExecution(), \"unexpected grouped execution fragment: %s\", fragment.getId());\n \n-    private static class RddFactory\n-    {\n-        private final Session session;\n-        private final JsonCodec<PrestoSparkTaskDescriptor> sparkTaskDescriptorJsonCodec;\n-        private final JavaSparkContext sparkContext;\n-        private final PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider;\n-        private final int initialSparkPartitionCount;\n-        private final int hashPartitionCount;\n-        private final CollectionAccumulator<SerializedTaskStats> taskStatsCollector;\n-        private final TableWriteInfo tableWriteInfo;\n-\n-        private RddFactory(\n-                Session session,\n-                JsonCodec<PrestoSparkTaskDescriptor> sparkTaskDescriptorJsonCodec,\n-                JavaSparkContext sparkContext,\n-                PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n-                int initialSparkPartitionCount,\n-                int hashPartitionCount,\n-                CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-                TableWriteInfo tableWriteInfo)\n-        {\n-            this.session = requireNonNull(session, \"session is null\");\n-            this.sparkTaskDescriptorJsonCodec = requireNonNull(sparkTaskDescriptorJsonCodec, \"sparkTaskDescriptorJsonCodec is null\");\n-            this.sparkContext = requireNonNull(sparkContext, \"sparkContext is null\");\n-            this.executorFactoryProvider = requireNonNull(executorFactoryProvider, \"executorFactoryProvider is null\");\n-            this.initialSparkPartitionCount = initialSparkPartitionCount;\n-            this.hashPartitionCount = hashPartitionCount;\n-            this.taskStatsCollector = requireNonNull(taskStatsCollector, \"taskStatsCollector is null\");\n-            this.tableWriteInfo = requireNonNull(tableWriteInfo, \"tableWriteInfo is null\");\n+        PartitioningHandle partitioning = fragment.getPartitioning();\n+\n+        if (!(partitioning.getConnectorHandle() instanceof SystemPartitioningHandle)) {\n+            // TODO: add support for bucketed table\n+            throw new PrestoException(NOT_SUPPORTED, \"Partitioned (bucketed) tables are not yet supported by Presto on Spark\");\n         }\n \n-        public JavaPairRDD<Integer, PrestoSparkRow> createRdd(PrestoSparkSubPlan subPlan)\n-        {\n-            PlanFragment fragment;\n-            // TODO: fragment adaption should be done prior to RDD creation\n-            if (subPlan.getFragment().getPartitioningScheme().getPartitioning().getHandle().equals(FIXED_HASH_DISTRIBUTION)) {\n-                fragment = subPlan.getFragment().withBucketToPartition(Optional.of(IntStream.range(0, hashPartitionCount).toArray()));\n-            }\n-            else {\n-                fragment = subPlan.getFragment();\n-            }\n+        if (partitioning.equals(SCALED_WRITER_DISTRIBUTION)) {\n+            throw new PrestoException(NOT_SUPPORTED, \"Automatic writers scaling is not supported by Presto on Spark\");\n+        }\n \n-            checkArgument(!fragment.getStageExecutionDescriptor().isStageGroupedExecution(), \"unexpected grouped execution fragment: %s\", fragment.getId());\n+        checkArgument(!partitioning.equals(COORDINATOR_DISTRIBUTION), \"COORDINATOR_DISTRIBUTION fragment must be run on the driver\");\n+        checkArgument(!partitioning.equals(FIXED_BROADCAST_DISTRIBUTION), \"FIXED_BROADCAST_DISTRIBUTION can only be set as an output partitioning scheme, and not as a fragment distribution\");\n+        checkArgument(!partitioning.equals(FIXED_PASSTHROUGH_DISTRIBUTION), \"FIXED_PASSTHROUGH_DISTRIBUTION can only be set as local exchange partitioning\");\n \n-            // scans\n-            List<PlanNodeId> tableScans = fragment.getTableScanSchedulingOrder();\n+        // TODO: ARBITRARY_DISTRIBUTION is something very weird.\n+        // TODO: It doesn't have partitioning function, and it is never set as a fragment partitioning.\n+        // TODO: We should consider removing ARBITRARY_DISTRIBUTION.\n+        checkArgument(!partitioning.equals(ARBITRARY_DISTRIBUTION), \"ARBITRARY_DISTRIBUTION is not expected to be set as a fragment distribution\");\n \n-            // source stages\n-            List<RemoteSourceNode> remoteSources = fragment.getRemoteSourceNodes();\n-            checkArgument(tableScans.isEmpty() || remoteSources.isEmpty(), \"stages that have both, remote sources and table scans, are not supported\");\n+        int hashPartitionCount = getHashPartitionCount(session);\n \n-            if (!tableScans.isEmpty()) {\n-                checkArgument(fragment.getPartitioning().equals(SOURCE_DISTRIBUTION), \"unexpected table scan partitioning: %s\", fragment.getPartitioning());\n+        // configure number of output partitions\n+        if (fragment.getPartitioningScheme().getPartitioning().getHandle().equals(FIXED_HASH_DISTRIBUTION)) {\n+            fragment = fragment.withBucketToPartition(Optional.of(IntStream.range(0, hashPartitionCount).toArray()));\n+        }\n \n-                // get all scheduled splits\n-                List<ScheduledSplit> scheduledSplits = subPlan.getTaskSources().stream()\n-                        .flatMap(taskSource -> taskSource.getSplits().stream())\n-                        .collect(toImmutableList());\n+        if (partitioning.equals(SINGLE_DISTRIBUTION) || partitioning.equals(FIXED_HASH_DISTRIBUTION) || partitioning.equals(FIXED_ARBITRARY_DISTRIBUTION)) {\n+            checkArgument(\n+                    fragment.getTableScanSchedulingOrder().isEmpty(),\n+                    \"Fragment with is not expected to have table scans. fragmentId: %s, fragment partitioning %s\",\n+                    fragment.getId(),\n+                    fragment.getPartitioning());\n+\n+            for (RemoteSourceNode remoteSource : fragment.getRemoteSourceNodes()) {\n+                if (remoteSource.isEnsureSourceOrdering() || remoteSource.getOrderingScheme().isPresent()) {\n+                    throw new PrestoException(NOT_SUPPORTED, format(\n+                            \"Order sensitive exchange is not supported by Presto on Spark. fragmentId: %s, sourceFragmentIds: %s\",\n+                            fragment.getId(),\n+                            remoteSource.getSourceFragmentIds()));\n+                }\n+            }\n \n-                // get scheduled splits by task\n-                List<List<ScheduledSplit>> assignedSplits = assignSplitsToTasks(scheduledSplits, initialSparkPartitionCount);\n+            Partitioner inputPartitioner = createPartitioner(\n+                    partitioning,\n+                    // TODO: consider using getMaxTasksPerStage\n+                    hashPartitionCount);\n \n-                List<SerializedPrestoSparkTaskDescriptor> serializedRequests = assignedSplits.stream()\n-                        .map(splits -> createTaskDescriptor(fragment, splits))\n-                        .map(sparkTaskDescriptorJsonCodec::toJsonBytes)\n-                        .map(SerializedPrestoSparkTaskDescriptor::new)\n-                        .collect(toImmutableList());\n+            Map<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> partitionedInputs = rddInputs.entrySet().stream()\n+                    .collect(toImmutableMap(Map.Entry::getKey, entry -> entry.getValue().partitionBy(inputPartitioner)));\n \n-                return sparkContext.parallelize(serializedRequests, initialSparkPartitionCount)\n-                        .mapPartitionsToPair(createTaskProcessor(executorFactoryProvider, taskStatsCollector));\n-            }\n+            return createIntermediateRdd(\n+                    session,\n+                    fragment,\n+                    executorFactoryProvider,\n+                    taskStatsCollector,\n+                    tableWriteInfo,\n+                    partitionedInputs);\n+        }\n+        else if (partitioning.equals(SOURCE_DISTRIBUTION)) {\n+            checkArgument(rddInputs.isEmpty(), \"rddInputs is expected to be empty for SOURCE_DISTRIBUTION fragment: %s\", fragment.getId());\n+            return createSourceRdd(\n+                    sparkContext,\n+                    session,\n+                    fragment,\n+                    executorFactoryProvider,\n+                    taskStatsCollector,\n+                    tableWriteInfo);\n+        }\n+        else {\n+            throw new IllegalArgumentException(format(\"Unexpected fragment partitioning %s, fragmentId: %s\", partitioning, fragment.getId()));\n+        }\n+    }\n \n-            List<PrestoSparkSubPlan> children = subPlan.getChildren();\n-            checkArgument(\n-                    remoteSources.size() == children.size(),\n-                    \"number of remote sources doesn't match the number of child stages: %s != %s\",\n-                    remoteSources.size(),\n-                    children.size());\n-\n-            if (children.size() == 1) {\n-                // Single remote source\n-                PrestoSparkSubPlan childSubPlan = getOnlyElement(children);\n-                JavaPairRDD<Integer, PrestoSparkRow> childRdd = createRdd(childSubPlan);\n-                PartitioningHandle partitioning = fragment.getPartitioning();\n-\n-                if (partitioning.equals(COORDINATOR_DISTRIBUTION)) {\n-                    // coordinator side work will be handled after JavaPairRDD#collect() call in PrestoSparkExecution\n-                    return childRdd;\n-                }\n+    private static Partitioner createPartitioner(PartitioningHandle partitioning, int partitionCount)\n+    {\n+        if (partitioning.equals(SINGLE_DISTRIBUTION)) {\n+            return new IntegerIdentityPartitioner(1);\n+        }\n+        if (partitioning.equals(FIXED_HASH_DISTRIBUTION)) {\n+            return new IntegerIdentityPartitioner(partitionCount);\n+        }\n+        if (partitioning.equals(FIXED_ARBITRARY_DISTRIBUTION)) {\n+            throw new PrestoException(NOT_SUPPORTED, \"FIXED_ARBITRARY_DISTRIBUTION partitioning is not yet supported\");\n+        }\n+        throw new IllegalArgumentException(format(\"Unexpected fragment partitioning %s\", partitioning));\n+    }\n \n-                PlanFragment childFragment = childSubPlan.getFragment();\n-                RemoteSourceNode remoteSource = getOnlyElement(remoteSources);\n-                List<PlanFragmentId> sourceFragmentIds = remoteSource.getSourceFragmentIds();\n-                checkArgument(sourceFragmentIds.size() == 1, \"expected to have exactly only a single source fragment\");\n-                checkArgument(childFragment.getId().equals(getOnlyElement(sourceFragmentIds)));\n-\n-                PrestoSparkTaskDescriptor taskDescriptor = createTaskDescriptor(fragment, ImmutableList.of());\n-                SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(sparkTaskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-                if (partitioning.equals(FIXED_HASH_DISTRIBUTION) ||\n-                        // when single distribution - there will be a single partition 0\n-                        partitioning.equals(SINGLE_DISTRIBUTION)) {\n-                    String planNodeId = remoteSource.getId().toString();\n-                    return childRdd\n-                            .partitionBy(partitioning.equals(FIXED_HASH_DISTRIBUTION) ? new IntegerIdentityPartitioner(hashPartitionCount) : new IntegerIdentityPartitioner(1))\n-                            .mapPartitionsToPair(createTaskProcessor(executorFactoryProvider, serializedTaskDescriptor, planNodeId, taskStatsCollector));\n-                }\n-                else {\n-                    // TODO: support (or do check state over) the following fragment partitioning:\n-                    //  - SOURCE_DISTRIBUTION\n-                    //  - FIXED_PASSTHROUGH_DISTRIBUTION\n-                    //  - ARBITRARY_DISTRIBUTION\n-                    //  - SCALED_WRITER_DISTRIBUTION\n-                    //  - FIXED_BROADCAST_DISTRIBUTION\n-                    //  - FIXED_ARBITRARY_DISTRIBUTION\n-                    throw new IllegalArgumentException(\"Unsupported fragment partitioning: \" + partitioning);\n-                }\n-            }\n-            else if (children.size() == 2) {\n-                // TODO: support N way join\n-                PrestoSparkSubPlan leftSubPlan = children.get(0);\n-                PrestoSparkSubPlan rightSubPlan = children.get(1);\n-\n-                RemoteSourceNode leftRemoteSource = remoteSources.get(0);\n-                RemoteSourceNode rightRemoteSource = remoteSources.get(1);\n-\n-                // We need String representation since PlanNodeId is not serializable...\n-                String leftRemoteSourcePlanId = leftRemoteSource.getId().toString();\n-                String rightRemoteSourcePlanId = rightRemoteSource.getId().toString();\n-\n-                JavaPairRDD<Integer, PrestoSparkRow> leftChildRdd = createRdd(leftSubPlan);\n-                JavaPairRDD<Integer, PrestoSparkRow> rightChildRdd = createRdd(rightSubPlan);\n-\n-                PlanFragment leftFragment = leftSubPlan.getFragment();\n-                PlanFragment rightFragment = rightSubPlan.getFragment();\n-\n-                List<PlanFragmentId> leftFragmentIds = leftRemoteSource.getSourceFragmentIds();\n-                checkArgument(leftFragmentIds.size() == 1, \"expected to have exactly only a single source fragment\");\n-                checkArgument(leftFragment.getId().equals(getOnlyElement(leftFragmentIds)));\n-                List<PlanFragmentId> rightFragmentIds = rightRemoteSource.getSourceFragmentIds();\n-                checkArgument(rightFragmentIds.size() == 1, \"expected to have exactly only a single source fragment\");\n-                checkArgument(rightFragment.getId().equals(getOnlyElement(rightFragmentIds)));\n-\n-                // This fragment only contains remote source, thus there is no splits\n-                PrestoSparkTaskDescriptor taskDescriptor = createTaskDescriptor(fragment, ImmutableList.of());\n-                SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(sparkTaskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-                PartitioningHandle partitioning = fragment.getPartitioning();\n-                checkArgument(partitioning.equals(FIXED_HASH_DISTRIBUTION));\n-\n-                JavaPairRDD<Integer, PrestoSparkRow> shuffledLeftChildRdd = leftChildRdd.partitionBy(new IntegerIdentityPartitioner(hashPartitionCount));\n-                JavaPairRDD<Integer, PrestoSparkRow> shuffledRightChildRdd = rightChildRdd.partitionBy(new IntegerIdentityPartitioner(hashPartitionCount));\n-                return JavaPairRDD.fromJavaRDD(\n-                        shuffledLeftChildRdd.zipPartitions(\n-                                shuffledRightChildRdd,\n-                                createTaskProcessor(executorFactoryProvider, serializedTaskDescriptor, leftRemoteSourcePlanId, rightRemoteSourcePlanId, taskStatsCollector)));\n-            }\n-            else {\n-                throw new UnsupportedOperationException();\n-            }\n+    private JavaPairRDD<Integer, PrestoSparkRow> createIntermediateRdd(\n+            Session session,\n+            PlanFragment fragment,\n+            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            TableWriteInfo tableWriteInfo,\n+            Map<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> rddInputs)\n+    {\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        verify(tableScans.isEmpty(), \"no table scans is expected\");\n+\n+        Set<PlanFragmentId> expectedInputs = fragment.getRemoteSourceNodes().stream()\n+                .map(RemoteSourceNode::getSourceFragmentIds)\n+                .flatMap(List::stream)\n+                .collect(toImmutableSet());\n+\n+        Set<PlanFragmentId> missingInputs = difference(expectedInputs, rddInputs.keySet());\n+        Set<PlanFragmentId> extraInputs = difference(rddInputs.keySet(), expectedInputs);\n+        checkArgument(\n+                missingInputs.isEmpty() && extraInputs.isEmpty(),\n+                \"rddInputs mismatch discovered. expected: %s, actual: %s\",\n+                expectedInputs,\n+                rddInputs.keySet());\n+\n+        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n+\n+        if (rddInputs.size() == 1) {\n+            RemoteSourceNode remoteSourceNode = getOnlyElement(fragment.getRemoteSourceNodes());\n+            PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> taskProcessor =\n+                    createTaskProcessor(\n+                            executorFactoryProvider,\n+                            serializedTaskDescriptor,\n+                            remoteSourceNode.getId().toString(),\n+                            taskStatsCollector);\n+            return getOnlyElement(rddInputs.values())\n+                    .mapPartitionsToPair(taskProcessor);\n+        }\n+        else if (rddInputs.size() == 2) {\n+            List<RemoteSourceNode> remoteSources = fragment.getRemoteSourceNodes();\n+            checkArgument(remoteSources.size() == 2, \"two remote sources are expected, got: %s\", remoteSources.size());\n+            RemoteSourceNode firstRemoteSource = remoteSources.get(0);\n+            RemoteSourceNode secondRemoteSource = remoteSources.get(1);\n+            JavaPairRDD<Integer, PrestoSparkRow> firstRdd = rddInputs.get(firstRemoteSource.getSourceFragmentIds().get(0));\n+            JavaPairRDD<Integer, PrestoSparkRow> secondRdd = rddInputs.get(secondRemoteSource.getSourceFragmentIds().get(0));\n+            FlatMapFunction2<Iterator<Tuple2<Integer, PrestoSparkRow>>, Iterator<Tuple2<Integer, PrestoSparkRow>>, Tuple2<Integer, PrestoSparkRow>> taskProcessor =\n+                    createTaskProcessor(\n+                            executorFactoryProvider,\n+                            serializedTaskDescriptor,\n+                            firstRemoteSource.getId().toString(),\n+                            secondRemoteSource.getId().toString(),\n+                            taskStatsCollector);\n+            return JavaPairRDD.fromJavaRDD(\n+                    firstRdd.zipPartitions(\n+                            secondRdd,\n+                            taskProcessor));\n         }\n \n-        private static List<List<ScheduledSplit>> assignSplitsToTasks(List<ScheduledSplit> scheduledSplits, int numTasks)\n-        {\n-            List<List<ScheduledSplit>> assignedSplits = new ArrayList<>();\n-            for (int i = 0; i < numTasks; i++) {\n-                assignedSplits.add(new ArrayList<>());\n-            }\n+        throw new IllegalArgumentException(format(\"unsupported number of inputs: %s\", rddInputs.size()));\n+    }\n \n-            for (ScheduledSplit split : scheduledSplits) {\n-                int taskId = Objects.hash(split.getPlanNodeId(), split.getSequenceId()) % numTasks;\n-                if (taskId < 0) {\n-                    taskId += numTasks;\n-                }\n+    private JavaPairRDD<Integer, PrestoSparkRow> createSourceRdd(\n+            JavaSparkContext sparkContext,\n+            Session session,\n+            PlanFragment fragment,\n+            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            TableWriteInfo tableWriteInfo)\n+    {\n+        // TODO: Possible in case of a broadcast join\n+        checkArgument(fragment.getRemoteSourceNodes().isEmpty(), \"source task with remote sources is not supported\");\n+\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        checkArgument(\n+                tableScans.size() == 1,\n+                \"exactly one table scan is expected in SOURCE_DISTRIBUTION fragment. fragmentId: %s, actual number of table scans: %s\",\n+                fragment.getId(),\n+                tableScans.size());\n+        verify(tableScans.size() == fragment.getTableScanSchedulingOrder().size());\n+\n+        TableScanNode tableScan = tableScans.get(0);\n+\n+        List<ScheduledSplit> splits = getSplits(session, tableScan);\n+        shuffle(splits);\n+        int initialPartitionCount = getSparkInitialPartitionCount(session);\n+        int numTasks = Math.min(splits.size(), initialPartitionCount);\n+        if (numTasks == 0) {\n+            return JavaPairRDD.fromJavaRDD(sparkContext.emptyRDD());\n+        }\n \n-                assignedSplits.get(taskId).add(split);\n-            }\n+        List<List<ScheduledSplit>> assignedSplits = assignSplitsToTasks(splits, numTasks);", "originalCommit": "3d4bd6d4836e14974b9247082f649bc5bad82938", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzE0NjA4Nw==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423146087", "bodyText": "Frankly It feels like this code is pretty local. I'm not sure it really deserves a separate class. Let's keep it as is for now.", "author": "arhimondr", "createdAt": "2020-05-11T15:59:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4NjMzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4NzM4Nw==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r422787387", "bodyText": "As a TODO, I think someday we want to have dedicated class for assignedSplits; just use a Map<Integer, List<Split>> and remove the map entry would also be better than set some element in list as null XDDD.", "author": "wenleix", "createdAt": "2020-05-11T05:23:20Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -15,283 +15,367 @@\n \n import com.facebook.airlift.json.JsonCodec;\n import com.facebook.presto.Session;\n+import com.facebook.presto.execution.Lifespan;\n import com.facebook.presto.execution.ScheduledSplit;\n import com.facebook.presto.execution.TaskSource;\n import com.facebook.presto.execution.scheduler.TableWriteInfo;\n+import com.facebook.presto.metadata.Metadata;\n+import com.facebook.presto.metadata.Split;\n import com.facebook.presto.spark.PrestoSparkTaskDescriptor;\n import com.facebook.presto.spark.classloader_interface.IntegerIdentityPartitioner;\n import com.facebook.presto.spark.classloader_interface.PrestoSparkRow;\n import com.facebook.presto.spark.classloader_interface.PrestoSparkTaskExecutorFactoryProvider;\n import com.facebook.presto.spark.classloader_interface.SerializedPrestoSparkTaskDescriptor;\n import com.facebook.presto.spark.classloader_interface.SerializedTaskStats;\n+import com.facebook.presto.spi.PrestoException;\n+import com.facebook.presto.spi.plan.PlanNode;\n import com.facebook.presto.spi.plan.PlanNodeId;\n+import com.facebook.presto.spi.plan.TableScanNode;\n+import com.facebook.presto.split.SplitManager;\n+import com.facebook.presto.split.SplitSource;\n import com.facebook.presto.sql.planner.PartitioningHandle;\n import com.facebook.presto.sql.planner.PlanFragment;\n+import com.facebook.presto.sql.planner.SystemPartitioningHandle;\n import com.facebook.presto.sql.planner.plan.PlanFragmentId;\n import com.facebook.presto.sql.planner.plan.RemoteSourceNode;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableSet;\n+import org.apache.spark.Partitioner;\n import org.apache.spark.api.java.JavaPairRDD;\n import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction2;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n import org.apache.spark.util.CollectionAccumulator;\n+import scala.Tuple2;\n \n import javax.inject.Inject;\n \n import java.util.ArrayList;\n+import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n-import java.util.Objects;\n import java.util.Optional;\n import java.util.Set;\n import java.util.stream.Collectors;\n import java.util.stream.IntStream;\n \n+import static com.facebook.airlift.concurrent.MoreFutures.getFutureValue;\n import static com.facebook.presto.SystemSessionProperties.getHashPartitionCount;\n import static com.facebook.presto.spark.PrestoSparkSessionProperties.getSparkInitialPartitionCount;\n import static com.facebook.presto.spark.classloader_interface.TaskProcessors.createTaskProcessor;\n+import static com.facebook.presto.spi.StandardErrorCode.NOT_SUPPORTED;\n+import static com.facebook.presto.spi.connector.ConnectorSplitManager.SplitSchedulingStrategy.UNGROUPED_SCHEDULING;\n+import static com.facebook.presto.spi.connector.NotPartitionedPartitionHandle.NOT_PARTITIONED;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.ARBITRARY_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.COORDINATOR_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_ARBITRARY_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_BROADCAST_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_HASH_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.FIXED_PASSTHROUGH_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.SystemPartitioningHandle.SCALED_WRITER_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.SINGLE_DISTRIBUTION;\n import static com.facebook.presto.sql.planner.SystemPartitioningHandle.SOURCE_DISTRIBUTION;\n+import static com.facebook.presto.sql.planner.optimizations.PlanNodeSearcher.searchFrom;\n import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Verify.verify;\n import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableMap.toImmutableMap;\n+import static com.google.common.collect.ImmutableSet.toImmutableSet;\n import static com.google.common.collect.Iterables.getOnlyElement;\n+import static com.google.common.collect.Sets.difference;\n+import static java.lang.String.format;\n+import static java.util.Collections.shuffle;\n import static java.util.Objects.requireNonNull;\n import static java.util.function.Function.identity;\n import static java.util.stream.Collectors.mapping;\n import static java.util.stream.Collectors.toSet;\n \n public class PrestoSparkRddFactory\n {\n-    private final JsonCodec<PrestoSparkTaskDescriptor> sparkTaskRequestJsonCodec;\n+    private final SplitManager splitManager;\n+    private final Metadata metadata;\n+    private final JsonCodec<PrestoSparkTaskDescriptor> taskDescriptorJsonCodec;\n \n     @Inject\n-    public PrestoSparkRddFactory(JsonCodec<PrestoSparkTaskDescriptor> sparkTaskRequestJsonCodec)\n+    public PrestoSparkRddFactory(SplitManager splitManager, Metadata metadata, JsonCodec<PrestoSparkTaskDescriptor> taskDescriptorJsonCodec)\n     {\n-        this.sparkTaskRequestJsonCodec = requireNonNull(sparkTaskRequestJsonCodec, \"sparkTaskRequestJsonCodec is null\");\n+        this.splitManager = requireNonNull(splitManager, \"splitManager is null\");\n+        this.metadata = requireNonNull(metadata, \"metadata is null\");\n+        this.taskDescriptorJsonCodec = requireNonNull(taskDescriptorJsonCodec, \"taskDescriptorJsonCodec is null\");\n     }\n \n     public JavaPairRDD<Integer, PrestoSparkRow> createSparkRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n-            PrestoSparkPlan prestoSparkPlan,\n-            PrestoSparkTaskExecutorFactoryProvider taskExecutorFactoryProvider,\n-            CollectionAccumulator<SerializedTaskStats> taskStatsCollector)\n+            PlanFragment fragment,\n+            Map<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> rddInputs,\n+            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            TableWriteInfo tableWriteInfo)\n     {\n-        RddFactory rddFactory = new RddFactory(\n-                session,\n-                sparkTaskRequestJsonCodec,\n-                sparkContext,\n-                taskExecutorFactoryProvider,\n-                getSparkInitialPartitionCount(session),\n-                getHashPartitionCount(session),\n-                taskStatsCollector,\n-                prestoSparkPlan.getTableWriteInfo());\n-        return rddFactory.createRdd(prestoSparkPlan.getPlan());\n-    }\n+        checkArgument(!fragment.getStageExecutionDescriptor().isStageGroupedExecution(), \"unexpected grouped execution fragment: %s\", fragment.getId());\n \n-    private static class RddFactory\n-    {\n-        private final Session session;\n-        private final JsonCodec<PrestoSparkTaskDescriptor> sparkTaskDescriptorJsonCodec;\n-        private final JavaSparkContext sparkContext;\n-        private final PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider;\n-        private final int initialSparkPartitionCount;\n-        private final int hashPartitionCount;\n-        private final CollectionAccumulator<SerializedTaskStats> taskStatsCollector;\n-        private final TableWriteInfo tableWriteInfo;\n-\n-        private RddFactory(\n-                Session session,\n-                JsonCodec<PrestoSparkTaskDescriptor> sparkTaskDescriptorJsonCodec,\n-                JavaSparkContext sparkContext,\n-                PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n-                int initialSparkPartitionCount,\n-                int hashPartitionCount,\n-                CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n-                TableWriteInfo tableWriteInfo)\n-        {\n-            this.session = requireNonNull(session, \"session is null\");\n-            this.sparkTaskDescriptorJsonCodec = requireNonNull(sparkTaskDescriptorJsonCodec, \"sparkTaskDescriptorJsonCodec is null\");\n-            this.sparkContext = requireNonNull(sparkContext, \"sparkContext is null\");\n-            this.executorFactoryProvider = requireNonNull(executorFactoryProvider, \"executorFactoryProvider is null\");\n-            this.initialSparkPartitionCount = initialSparkPartitionCount;\n-            this.hashPartitionCount = hashPartitionCount;\n-            this.taskStatsCollector = requireNonNull(taskStatsCollector, \"taskStatsCollector is null\");\n-            this.tableWriteInfo = requireNonNull(tableWriteInfo, \"tableWriteInfo is null\");\n+        PartitioningHandle partitioning = fragment.getPartitioning();\n+\n+        if (!(partitioning.getConnectorHandle() instanceof SystemPartitioningHandle)) {\n+            // TODO: add support for bucketed table\n+            throw new PrestoException(NOT_SUPPORTED, \"Partitioned (bucketed) tables are not yet supported by Presto on Spark\");\n         }\n \n-        public JavaPairRDD<Integer, PrestoSparkRow> createRdd(PrestoSparkSubPlan subPlan)\n-        {\n-            PlanFragment fragment;\n-            // TODO: fragment adaption should be done prior to RDD creation\n-            if (subPlan.getFragment().getPartitioningScheme().getPartitioning().getHandle().equals(FIXED_HASH_DISTRIBUTION)) {\n-                fragment = subPlan.getFragment().withBucketToPartition(Optional.of(IntStream.range(0, hashPartitionCount).toArray()));\n-            }\n-            else {\n-                fragment = subPlan.getFragment();\n-            }\n+        if (partitioning.equals(SCALED_WRITER_DISTRIBUTION)) {\n+            throw new PrestoException(NOT_SUPPORTED, \"Automatic writers scaling is not supported by Presto on Spark\");\n+        }\n \n-            checkArgument(!fragment.getStageExecutionDescriptor().isStageGroupedExecution(), \"unexpected grouped execution fragment: %s\", fragment.getId());\n+        checkArgument(!partitioning.equals(COORDINATOR_DISTRIBUTION), \"COORDINATOR_DISTRIBUTION fragment must be run on the driver\");\n+        checkArgument(!partitioning.equals(FIXED_BROADCAST_DISTRIBUTION), \"FIXED_BROADCAST_DISTRIBUTION can only be set as an output partitioning scheme, and not as a fragment distribution\");\n+        checkArgument(!partitioning.equals(FIXED_PASSTHROUGH_DISTRIBUTION), \"FIXED_PASSTHROUGH_DISTRIBUTION can only be set as local exchange partitioning\");\n \n-            // scans\n-            List<PlanNodeId> tableScans = fragment.getTableScanSchedulingOrder();\n+        // TODO: ARBITRARY_DISTRIBUTION is something very weird.\n+        // TODO: It doesn't have partitioning function, and it is never set as a fragment partitioning.\n+        // TODO: We should consider removing ARBITRARY_DISTRIBUTION.\n+        checkArgument(!partitioning.equals(ARBITRARY_DISTRIBUTION), \"ARBITRARY_DISTRIBUTION is not expected to be set as a fragment distribution\");\n \n-            // source stages\n-            List<RemoteSourceNode> remoteSources = fragment.getRemoteSourceNodes();\n-            checkArgument(tableScans.isEmpty() || remoteSources.isEmpty(), \"stages that have both, remote sources and table scans, are not supported\");\n+        int hashPartitionCount = getHashPartitionCount(session);\n \n-            if (!tableScans.isEmpty()) {\n-                checkArgument(fragment.getPartitioning().equals(SOURCE_DISTRIBUTION), \"unexpected table scan partitioning: %s\", fragment.getPartitioning());\n+        // configure number of output partitions\n+        if (fragment.getPartitioningScheme().getPartitioning().getHandle().equals(FIXED_HASH_DISTRIBUTION)) {\n+            fragment = fragment.withBucketToPartition(Optional.of(IntStream.range(0, hashPartitionCount).toArray()));\n+        }\n \n-                // get all scheduled splits\n-                List<ScheduledSplit> scheduledSplits = subPlan.getTaskSources().stream()\n-                        .flatMap(taskSource -> taskSource.getSplits().stream())\n-                        .collect(toImmutableList());\n+        if (partitioning.equals(SINGLE_DISTRIBUTION) || partitioning.equals(FIXED_HASH_DISTRIBUTION) || partitioning.equals(FIXED_ARBITRARY_DISTRIBUTION)) {\n+            checkArgument(\n+                    fragment.getTableScanSchedulingOrder().isEmpty(),\n+                    \"Fragment with is not expected to have table scans. fragmentId: %s, fragment partitioning %s\",\n+                    fragment.getId(),\n+                    fragment.getPartitioning());\n+\n+            for (RemoteSourceNode remoteSource : fragment.getRemoteSourceNodes()) {\n+                if (remoteSource.isEnsureSourceOrdering() || remoteSource.getOrderingScheme().isPresent()) {\n+                    throw new PrestoException(NOT_SUPPORTED, format(\n+                            \"Order sensitive exchange is not supported by Presto on Spark. fragmentId: %s, sourceFragmentIds: %s\",\n+                            fragment.getId(),\n+                            remoteSource.getSourceFragmentIds()));\n+                }\n+            }\n \n-                // get scheduled splits by task\n-                List<List<ScheduledSplit>> assignedSplits = assignSplitsToTasks(scheduledSplits, initialSparkPartitionCount);\n+            Partitioner inputPartitioner = createPartitioner(\n+                    partitioning,\n+                    // TODO: consider using getMaxTasksPerStage\n+                    hashPartitionCount);\n \n-                List<SerializedPrestoSparkTaskDescriptor> serializedRequests = assignedSplits.stream()\n-                        .map(splits -> createTaskDescriptor(fragment, splits))\n-                        .map(sparkTaskDescriptorJsonCodec::toJsonBytes)\n-                        .map(SerializedPrestoSparkTaskDescriptor::new)\n-                        .collect(toImmutableList());\n+            Map<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> partitionedInputs = rddInputs.entrySet().stream()\n+                    .collect(toImmutableMap(Map.Entry::getKey, entry -> entry.getValue().partitionBy(inputPartitioner)));\n \n-                return sparkContext.parallelize(serializedRequests, initialSparkPartitionCount)\n-                        .mapPartitionsToPair(createTaskProcessor(executorFactoryProvider, taskStatsCollector));\n-            }\n+            return createIntermediateRdd(\n+                    session,\n+                    fragment,\n+                    executorFactoryProvider,\n+                    taskStatsCollector,\n+                    tableWriteInfo,\n+                    partitionedInputs);\n+        }\n+        else if (partitioning.equals(SOURCE_DISTRIBUTION)) {\n+            checkArgument(rddInputs.isEmpty(), \"rddInputs is expected to be empty for SOURCE_DISTRIBUTION fragment: %s\", fragment.getId());\n+            return createSourceRdd(\n+                    sparkContext,\n+                    session,\n+                    fragment,\n+                    executorFactoryProvider,\n+                    taskStatsCollector,\n+                    tableWriteInfo);\n+        }\n+        else {\n+            throw new IllegalArgumentException(format(\"Unexpected fragment partitioning %s, fragmentId: %s\", partitioning, fragment.getId()));\n+        }\n+    }\n \n-            List<PrestoSparkSubPlan> children = subPlan.getChildren();\n-            checkArgument(\n-                    remoteSources.size() == children.size(),\n-                    \"number of remote sources doesn't match the number of child stages: %s != %s\",\n-                    remoteSources.size(),\n-                    children.size());\n-\n-            if (children.size() == 1) {\n-                // Single remote source\n-                PrestoSparkSubPlan childSubPlan = getOnlyElement(children);\n-                JavaPairRDD<Integer, PrestoSparkRow> childRdd = createRdd(childSubPlan);\n-                PartitioningHandle partitioning = fragment.getPartitioning();\n-\n-                if (partitioning.equals(COORDINATOR_DISTRIBUTION)) {\n-                    // coordinator side work will be handled after JavaPairRDD#collect() call in PrestoSparkExecution\n-                    return childRdd;\n-                }\n+    private static Partitioner createPartitioner(PartitioningHandle partitioning, int partitionCount)\n+    {\n+        if (partitioning.equals(SINGLE_DISTRIBUTION)) {\n+            return new IntegerIdentityPartitioner(1);\n+        }\n+        if (partitioning.equals(FIXED_HASH_DISTRIBUTION)) {\n+            return new IntegerIdentityPartitioner(partitionCount);\n+        }\n+        if (partitioning.equals(FIXED_ARBITRARY_DISTRIBUTION)) {\n+            throw new PrestoException(NOT_SUPPORTED, \"FIXED_ARBITRARY_DISTRIBUTION partitioning is not yet supported\");\n+        }\n+        throw new IllegalArgumentException(format(\"Unexpected fragment partitioning %s\", partitioning));\n+    }\n \n-                PlanFragment childFragment = childSubPlan.getFragment();\n-                RemoteSourceNode remoteSource = getOnlyElement(remoteSources);\n-                List<PlanFragmentId> sourceFragmentIds = remoteSource.getSourceFragmentIds();\n-                checkArgument(sourceFragmentIds.size() == 1, \"expected to have exactly only a single source fragment\");\n-                checkArgument(childFragment.getId().equals(getOnlyElement(sourceFragmentIds)));\n-\n-                PrestoSparkTaskDescriptor taskDescriptor = createTaskDescriptor(fragment, ImmutableList.of());\n-                SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(sparkTaskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-                if (partitioning.equals(FIXED_HASH_DISTRIBUTION) ||\n-                        // when single distribution - there will be a single partition 0\n-                        partitioning.equals(SINGLE_DISTRIBUTION)) {\n-                    String planNodeId = remoteSource.getId().toString();\n-                    return childRdd\n-                            .partitionBy(partitioning.equals(FIXED_HASH_DISTRIBUTION) ? new IntegerIdentityPartitioner(hashPartitionCount) : new IntegerIdentityPartitioner(1))\n-                            .mapPartitionsToPair(createTaskProcessor(executorFactoryProvider, serializedTaskDescriptor, planNodeId, taskStatsCollector));\n-                }\n-                else {\n-                    // TODO: support (or do check state over) the following fragment partitioning:\n-                    //  - SOURCE_DISTRIBUTION\n-                    //  - FIXED_PASSTHROUGH_DISTRIBUTION\n-                    //  - ARBITRARY_DISTRIBUTION\n-                    //  - SCALED_WRITER_DISTRIBUTION\n-                    //  - FIXED_BROADCAST_DISTRIBUTION\n-                    //  - FIXED_ARBITRARY_DISTRIBUTION\n-                    throw new IllegalArgumentException(\"Unsupported fragment partitioning: \" + partitioning);\n-                }\n-            }\n-            else if (children.size() == 2) {\n-                // TODO: support N way join\n-                PrestoSparkSubPlan leftSubPlan = children.get(0);\n-                PrestoSparkSubPlan rightSubPlan = children.get(1);\n-\n-                RemoteSourceNode leftRemoteSource = remoteSources.get(0);\n-                RemoteSourceNode rightRemoteSource = remoteSources.get(1);\n-\n-                // We need String representation since PlanNodeId is not serializable...\n-                String leftRemoteSourcePlanId = leftRemoteSource.getId().toString();\n-                String rightRemoteSourcePlanId = rightRemoteSource.getId().toString();\n-\n-                JavaPairRDD<Integer, PrestoSparkRow> leftChildRdd = createRdd(leftSubPlan);\n-                JavaPairRDD<Integer, PrestoSparkRow> rightChildRdd = createRdd(rightSubPlan);\n-\n-                PlanFragment leftFragment = leftSubPlan.getFragment();\n-                PlanFragment rightFragment = rightSubPlan.getFragment();\n-\n-                List<PlanFragmentId> leftFragmentIds = leftRemoteSource.getSourceFragmentIds();\n-                checkArgument(leftFragmentIds.size() == 1, \"expected to have exactly only a single source fragment\");\n-                checkArgument(leftFragment.getId().equals(getOnlyElement(leftFragmentIds)));\n-                List<PlanFragmentId> rightFragmentIds = rightRemoteSource.getSourceFragmentIds();\n-                checkArgument(rightFragmentIds.size() == 1, \"expected to have exactly only a single source fragment\");\n-                checkArgument(rightFragment.getId().equals(getOnlyElement(rightFragmentIds)));\n-\n-                // This fragment only contains remote source, thus there is no splits\n-                PrestoSparkTaskDescriptor taskDescriptor = createTaskDescriptor(fragment, ImmutableList.of());\n-                SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(sparkTaskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n-\n-                PartitioningHandle partitioning = fragment.getPartitioning();\n-                checkArgument(partitioning.equals(FIXED_HASH_DISTRIBUTION));\n-\n-                JavaPairRDD<Integer, PrestoSparkRow> shuffledLeftChildRdd = leftChildRdd.partitionBy(new IntegerIdentityPartitioner(hashPartitionCount));\n-                JavaPairRDD<Integer, PrestoSparkRow> shuffledRightChildRdd = rightChildRdd.partitionBy(new IntegerIdentityPartitioner(hashPartitionCount));\n-                return JavaPairRDD.fromJavaRDD(\n-                        shuffledLeftChildRdd.zipPartitions(\n-                                shuffledRightChildRdd,\n-                                createTaskProcessor(executorFactoryProvider, serializedTaskDescriptor, leftRemoteSourcePlanId, rightRemoteSourcePlanId, taskStatsCollector)));\n-            }\n-            else {\n-                throw new UnsupportedOperationException();\n-            }\n+    private JavaPairRDD<Integer, PrestoSparkRow> createIntermediateRdd(\n+            Session session,\n+            PlanFragment fragment,\n+            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            TableWriteInfo tableWriteInfo,\n+            Map<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> rddInputs)\n+    {\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        verify(tableScans.isEmpty(), \"no table scans is expected\");\n+\n+        Set<PlanFragmentId> expectedInputs = fragment.getRemoteSourceNodes().stream()\n+                .map(RemoteSourceNode::getSourceFragmentIds)\n+                .flatMap(List::stream)\n+                .collect(toImmutableSet());\n+\n+        Set<PlanFragmentId> missingInputs = difference(expectedInputs, rddInputs.keySet());\n+        Set<PlanFragmentId> extraInputs = difference(rddInputs.keySet(), expectedInputs);\n+        checkArgument(\n+                missingInputs.isEmpty() && extraInputs.isEmpty(),\n+                \"rddInputs mismatch discovered. expected: %s, actual: %s\",\n+                expectedInputs,\n+                rddInputs.keySet());\n+\n+        PrestoSparkTaskDescriptor taskDescriptor = createIntermediateTaskDescriptor(session, tableWriteInfo, fragment);\n+        SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n+\n+        if (rddInputs.size() == 1) {\n+            RemoteSourceNode remoteSourceNode = getOnlyElement(fragment.getRemoteSourceNodes());\n+            PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> taskProcessor =\n+                    createTaskProcessor(\n+                            executorFactoryProvider,\n+                            serializedTaskDescriptor,\n+                            remoteSourceNode.getId().toString(),\n+                            taskStatsCollector);\n+            return getOnlyElement(rddInputs.values())\n+                    .mapPartitionsToPair(taskProcessor);\n+        }\n+        else if (rddInputs.size() == 2) {\n+            List<RemoteSourceNode> remoteSources = fragment.getRemoteSourceNodes();\n+            checkArgument(remoteSources.size() == 2, \"two remote sources are expected, got: %s\", remoteSources.size());\n+            RemoteSourceNode firstRemoteSource = remoteSources.get(0);\n+            RemoteSourceNode secondRemoteSource = remoteSources.get(1);\n+            JavaPairRDD<Integer, PrestoSparkRow> firstRdd = rddInputs.get(firstRemoteSource.getSourceFragmentIds().get(0));\n+            JavaPairRDD<Integer, PrestoSparkRow> secondRdd = rddInputs.get(secondRemoteSource.getSourceFragmentIds().get(0));\n+            FlatMapFunction2<Iterator<Tuple2<Integer, PrestoSparkRow>>, Iterator<Tuple2<Integer, PrestoSparkRow>>, Tuple2<Integer, PrestoSparkRow>> taskProcessor =\n+                    createTaskProcessor(\n+                            executorFactoryProvider,\n+                            serializedTaskDescriptor,\n+                            firstRemoteSource.getId().toString(),\n+                            secondRemoteSource.getId().toString(),\n+                            taskStatsCollector);\n+            return JavaPairRDD.fromJavaRDD(\n+                    firstRdd.zipPartitions(\n+                            secondRdd,\n+                            taskProcessor));\n         }\n \n-        private static List<List<ScheduledSplit>> assignSplitsToTasks(List<ScheduledSplit> scheduledSplits, int numTasks)\n-        {\n-            List<List<ScheduledSplit>> assignedSplits = new ArrayList<>();\n-            for (int i = 0; i < numTasks; i++) {\n-                assignedSplits.add(new ArrayList<>());\n-            }\n+        throw new IllegalArgumentException(format(\"unsupported number of inputs: %s\", rddInputs.size()));\n+    }\n \n-            for (ScheduledSplit split : scheduledSplits) {\n-                int taskId = Objects.hash(split.getPlanNodeId(), split.getSequenceId()) % numTasks;\n-                if (taskId < 0) {\n-                    taskId += numTasks;\n-                }\n+    private JavaPairRDD<Integer, PrestoSparkRow> createSourceRdd(\n+            JavaSparkContext sparkContext,\n+            Session session,\n+            PlanFragment fragment,\n+            PrestoSparkTaskExecutorFactoryProvider executorFactoryProvider,\n+            CollectionAccumulator<SerializedTaskStats> taskStatsCollector,\n+            TableWriteInfo tableWriteInfo)\n+    {\n+        // TODO: Possible in case of a broadcast join\n+        checkArgument(fragment.getRemoteSourceNodes().isEmpty(), \"source task with remote sources is not supported\");\n+\n+        List<TableScanNode> tableScans = findTableScanNodes(fragment.getRoot());\n+        checkArgument(\n+                tableScans.size() == 1,\n+                \"exactly one table scan is expected in SOURCE_DISTRIBUTION fragment. fragmentId: %s, actual number of table scans: %s\",\n+                fragment.getId(),\n+                tableScans.size());\n+        verify(tableScans.size() == fragment.getTableScanSchedulingOrder().size());\n+\n+        TableScanNode tableScan = tableScans.get(0);\n+\n+        List<ScheduledSplit> splits = getSplits(session, tableScan);\n+        shuffle(splits);\n+        int initialPartitionCount = getSparkInitialPartitionCount(session);\n+        int numTasks = Math.min(splits.size(), initialPartitionCount);\n+        if (numTasks == 0) {\n+            return JavaPairRDD.fromJavaRDD(sparkContext.emptyRDD());\n+        }\n \n-                assignedSplits.get(taskId).add(split);\n-            }\n+        List<List<ScheduledSplit>> assignedSplits = assignSplitsToTasks(splits, numTasks);\n \n-            return assignedSplits;\n+        // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n+        splits = null;\n+\n+        ImmutableList.Builder<SerializedPrestoSparkTaskDescriptor> serializedTaskDescriptors = ImmutableList.builder();\n+        for (int i = 0; i < assignedSplits.size(); i++) {\n+            List<ScheduledSplit> splitBatch = assignedSplits.get(i);\n+            PrestoSparkTaskDescriptor taskDescriptor = createSourceTaskDescriptor(session, tableWriteInfo, fragment, splitBatch);\n+            // TODO: consider more efficient serialization or apply compression to save precious memory on the Driver\n+            byte[] jsonSerializedTaskDescriptor = taskDescriptorJsonCodec.toJsonBytes(taskDescriptor);\n+            serializedTaskDescriptors.add(new SerializedPrestoSparkTaskDescriptor(jsonSerializedTaskDescriptor));\n+            // let the garbage collector reclaim the memory used by the decoded splits as soon as the task descriptor is encoded\n+            assignedSplits.set(i, null);", "originalCommit": "3d4bd6d4836e14974b9247082f649bc5bad82938", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzE0NjY0NA==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423146644", "bodyText": "I wouldn't try to overthink it. Although I agree that a List of Lists is not very readable, the scope of this variable is very low.", "author": "arhimondr", "createdAt": "2020-05-11T16:00:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4NzM4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM3NjQ1OQ==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423376459", "bodyText": "@arhimondr : Setting some element to be null in a List to make garbage collector re-claim definitely works, but I don't really think it's a common way... so I would argue it's not a \"overthink\" :). The operation really need here is remove.\nHowever, I understand we want to use null setting to keep the commit straightforward  for now. And it's too early to sweat on how to organize this part of the code for now. So I suggest to leave a comment.", "author": "wenleix", "createdAt": "2020-05-11T23:28:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4NzM4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM3ODgwOA==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423378808", "bodyText": "Setting some element to be null in a List to make garbage collector re-claim definitely works, but I don't really think it's a common way... so I would argue it's not a \"overthink\" :).\n\nOh, i see what you mean.\n\nSo I suggest to leave a comment.\n\nI have a comment there:\n\n// let the garbage collector reclaim the memory us\n\nDo you think it requires further clarification?", "author": "arhimondr", "createdAt": "2020-05-11T23:35:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4NzM4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM4MTgzNg==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423381836", "bodyText": "@arhimondr : I am just thinking we probably want to refactor the code in the future. Even a Map<Integer, List<Split>> seems to be better than List<List<Split>> since now you can do Map.remove(i) instead of List#set(i, null) :)\nI mean, to put an comment like the following:\n// let the garbage collector reclaim the memory us\n// TODO: Investigate if we can avoid this by not using List", "author": "wenleix", "createdAt": "2020-05-11T23:45:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4NzM4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM4NDY3NA==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423384674", "bodyText": "List also has a remove(index) method: https://docs.oracle.com/javase/8/docs/api/java/util/List.html#remove-int-\nI just thought that an explicit nullification can be more readable, as I had an impression that's usually the way these tricks are implemented. But I don't have any strong opinion here.\nI also don't think using List<List> is very exotic. It is what the Lists<List> is returned by Lists.partition from Guava. And we use it in Presto in several places to split a larger list into smaller partitions (batches).\nWrapping it into a class can make it a little bit more readable, but given the very small scope of this variable It feels like it is more of a matter of taste.", "author": "arhimondr", "createdAt": "2020-05-11T23:54:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4NzM4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ2OTE2MA==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423469160", "bodyText": "I just thought that an explicit nullification can be more readable, as I had an impression that's usually the way these tricks are implemented. But I don't have any strong opinion here.\n\nRight, but removing front element in List is O(n). I thought that's why you nullify it instead of remove it. We can also use ArrayDeque, but anyway, I agree it's just a matter of taste. Let's just keep it as is.", "author": "wenleix", "createdAt": "2020-05-12T05:24:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4NzM4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU5ODMzMQ==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423598331", "bodyText": "Oh, right. It has to rewrite the array to preserve list semantics. In theory it is possible to iterate backwards though =)", "author": "arhimondr", "createdAt": "2020-05-12T09:35:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4NzM4Nw=="}], "type": "inlineReview"}, {"oid": "f55ec4371182ef29762745dd9b24c92c8f74847f", "url": "https://github.com/prestodb/presto/commit/f55ec4371182ef29762745dd9b24c92c8f74847f", "message": "Implement broadcast join", "committedDate": "2020-05-11T16:08:06Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ2MzE3OQ==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423463179", "bodyText": "Looks like you are already preparing for multi-way join  \ud83d\ude03", "author": "wenleix", "createdAt": "2020-05-12T05:01:57Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -228,33 +229,31 @@ private static Partitioner createPartitioner(PartitioningHandle partitioning, in\n         SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(taskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n         if (rddInputs.size() == 1) {\n-            RemoteSourceNode remoteSourceNode = getOnlyElement(fragment.getRemoteSourceNodes());\n+            Entry<PlanFragmentId, JavaPairRDD<Integer, PrestoSparkRow>> input = getOnlyElement(rddInputs.entrySet());\n             PairFlatMapFunction<Iterator<Tuple2<Integer, PrestoSparkRow>>, Integer, PrestoSparkRow> taskProcessor =\n                     createTaskProcessor(\n                             executorFactoryProvider,\n                             serializedTaskDescriptor,\n-                            remoteSourceNode.getId().toString(),\n+                            input.getKey().toString(),\n                             taskStatsCollector);\n-            return getOnlyElement(rddInputs.values())\n+            return input.getValue()\n                     .mapPartitionsToPair(taskProcessor);\n         }\n-        else if (rddInputs.size() == 2) {\n-            List<RemoteSourceNode> remoteSources = fragment.getRemoteSourceNodes();\n-            checkArgument(remoteSources.size() == 2, \"two remote sources are expected, got: %s\", remoteSources.size());\n-            RemoteSourceNode firstRemoteSource = remoteSources.get(0);\n-            RemoteSourceNode secondRemoteSource = remoteSources.get(1);\n-            JavaPairRDD<Integer, PrestoSparkRow> firstRdd = rddInputs.get(firstRemoteSource.getSourceFragmentIds().get(0));\n-            JavaPairRDD<Integer, PrestoSparkRow> secondRdd = rddInputs.get(secondRemoteSource.getSourceFragmentIds().get(0));\n+        if (rddInputs.size() == 2) {\n+            List<PlanFragmentId> fragmentIds = ImmutableList.copyOf(rddInputs.keySet());\n+            List<JavaPairRDD<Integer, PrestoSparkRow>> rdds = fragmentIds.stream()", "originalCommit": "0cbff4c1ff0d65074cfc811b07111c3c496cb016", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ2NDI1Mw==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423464253", "bodyText": "Because of the continue in line 248, shuffleRemoteSourceInputs and broadcastRemoteSourceInputs cannot both be non-empty right? -- Maybe check shuffleInputs and broadcastInput cannot both present earlier?", "author": "wenleix", "createdAt": "2020-05-12T05:06:07Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java", "diffHunk": "@@ -228,14 +237,34 @@ public IPrestoSparkTaskExecutor create(\n         PrestoSparkRowBuffer rowBuffer = new PrestoSparkRowBuffer(memoryManager);\n \n         ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkRow>> shuffleInputs = ImmutableMap.builder();\n+        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkSerializedPage>> broadcastInputs = ImmutableMap.builder();\n         for (RemoteSourceNode remoteSource : fragment.getRemoteSourceNodes()) {\n-            ImmutableList.Builder<Iterator<PrestoSparkRow>> remoteSourceInputs = ImmutableList.builder();\n+            List<Iterator<PrestoSparkRow>> shuffleRemoteSourceInputs = new ArrayList<>();\n+            List<Iterator<PrestoSparkSerializedPage>> broadcastRemoteSourceInputs = new ArrayList<>();\n             for (PlanFragmentId sourceFragmentId : remoteSource.getSourceFragmentIds()) {\n-                Iterator<Tuple2<Integer, PrestoSparkRow>> input = inputs.getShuffleInputs().get(sourceFragmentId.toString());\n-                checkArgument(input != null, \"input is missing for fragmentId: %s\", sourceFragmentId);\n-                remoteSourceInputs.add(Iterators.transform(input, tuple -> tuple._2));\n+                Iterator<Tuple2<Integer, PrestoSparkRow>> shuffleInput = inputs.getShuffleInputs().get(sourceFragmentId.toString());\n+                if (shuffleInput != null) {\n+                    shuffleRemoteSourceInputs.add(Iterators.transform(shuffleInput, tuple -> tuple._2));\n+                    continue;\n+                }\n+                Broadcast<List<PrestoSparkSerializedPage>> broadcastInput = inputs.getBroadcastInputs().get(sourceFragmentId.toString());\n+                if (broadcastInput != null) {\n+                    // TODO: Enable NullifyingIterator once migrated to one task per JVM model\n+                    // NullifyingIterator removes element from the list upon return\n+                    // This allows GC to gradually reclaim memory\n+                    // broadcastRemoteSourceInputs.add(getNullifyingIterator(broadcastInput.value()));\n+                    broadcastRemoteSourceInputs.add(broadcastInput.value().iterator());\n+                    continue;\n+                }\n+                throw new IllegalArgumentException(\"Input not found for sourceFragmentId: \" + sourceFragmentId);\n+            }\n+            verify(shuffleRemoteSourceInputs.isEmpty() || broadcastRemoteSourceInputs.isEmpty(), \"single remote source cannot accept both, broadcast and shuffle inputs\");", "originalCommit": "f55ec4371182ef29762745dd9b24c92c8f74847f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU5ODg4Mg==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423598882", "bodyText": "Makes sense. Let me do that.", "author": "arhimondr", "createdAt": "2020-05-12T09:35:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ2NDI1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ2NDgzOA==", "url": "https://github.com/prestodb/presto/pull/14495#discussion_r423464838", "bodyText": "nice encapsulation!", "author": "wenleix", "createdAt": "2020-05-12T05:08:16Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java", "diffHunk": "@@ -431,4 +472,37 @@ private QueryInfo createQueryInfo(Optional<Throwable> failure)\n             return null;\n         }\n     }\n+\n+    private static class RddAndMore\n+    {\n+        private final JavaPairRDD<Integer, PrestoSparkRow> rdd;\n+        private final List<Broadcast<?>> broadcastDependencies;\n+\n+        private boolean collected;\n+\n+        private RddAndMore(JavaPairRDD<Integer, PrestoSparkRow> rdd, List<Broadcast<?>> broadcastDependencies)\n+        {\n+            this.rdd = requireNonNull(rdd, \"rdd is null\");\n+            this.broadcastDependencies = ImmutableList.copyOf(requireNonNull(broadcastDependencies, \"broadcastDependencies is null\"));\n+        }\n+\n+        public List<Tuple2<Integer, PrestoSparkRow>> collectAndDestroyDependencies()", "originalCommit": "f55ec4371182ef29762745dd9b24c92c8f74847f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "19b88805b09642277ee7f059b6009d6402f28dda", "url": "https://github.com/prestodb/presto/commit/19b88805b09642277ee7f059b6009d6402f28dda", "message": "Refactor PrestoSparkRddFactory\n\nThis is preparation for Broadcast join implementation\n\nBroadcast join is special, as the broadcasted table must be first collected on the driver,\nso it can be broadcasted with a Broadcast variable. This results in more than a single\njob per query. This commit prepares the codebase to support more than a single job\nper query.\n\n- PrestoSparkRddFactory now creates RDD only for a single fragment, and not for a\n  fragment tree. The dependent RDD have to be provided as a parameter. This is needed\n  to allow Broadcast sources to be provided.\n- SubPlan traversal is now being done in PrestoSparkQueryExecutionFactory. This allows\n  us to execute dependency broadcast jobs in the `execute()` method.\n- PrestoSparkSplitEnumerator is removed and PrestoSparkPlan is inlined. Splits are lazily\n  enumerated in the PrestoSparkRddFactory to avoid enumerating splits that are only necessary\n  for the current job being executed. That allows us to save precious memory for the broadcast\n  table.", "committedDate": "2020-05-12T09:39:02Z", "type": "commit"}, {"oid": "86c5eaf32c152c1331a12685b729960fc4d220c3", "url": "https://github.com/prestodb/presto/commit/86c5eaf32c152c1331a12685b729960fc4d220c3", "message": "Improve rollback handling in PrestoSparkQueryExecutionFactory", "committedDate": "2020-05-12T09:39:02Z", "type": "commit"}, {"oid": "b0e0857158ab230d6e6968b911a832d22ba0c226", "url": "https://github.com/prestodb/presto/commit/b0e0857158ab230d6e6968b911a832d22ba0c226", "message": "Refactor PrestoSparkRemoteSourceOperator\n\nExtract transformation code into an iterator", "committedDate": "2020-05-12T09:39:02Z", "type": "commit"}, {"oid": "acf49bde88e5e862dfdf85d78365f76d3c930c42", "url": "https://github.com/prestodb/presto/commit/acf49bde88e5e862dfdf85d78365f76d3c930c42", "message": "Refactor PrestoSparkTaskInputs\n\nUse fragmentId as a key as a single remote source may have multiple\nfragments as its input", "committedDate": "2020-05-12T09:39:02Z", "type": "commit"}, {"oid": "b1c048653ab294165e043d506b78dd31b603a1d3", "url": "https://github.com/prestodb/presto/commit/b1c048653ab294165e043d506b78dd31b603a1d3", "message": "Increase visibility of BlockUtil", "committedDate": "2020-05-12T09:39:02Z", "type": "commit"}, {"oid": "7f30d610b4fade23ba0e94c67189c9e8aaa8f4ba", "url": "https://github.com/prestodb/presto/commit/7f30d610b4fade23ba0e94c67189c9e8aaa8f4ba", "message": "Implement broadcast join", "committedDate": "2020-05-12T09:39:02Z", "type": "commit"}, {"oid": "7f30d610b4fade23ba0e94c67189c9e8aaa8f4ba", "url": "https://github.com/prestodb/presto/commit/7f30d610b4fade23ba0e94c67189c9e8aaa8f4ba", "message": "Implement broadcast join", "committedDate": "2020-05-12T09:39:02Z", "type": "forcePushed"}]}