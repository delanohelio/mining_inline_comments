{"pr_number": 14989, "pr_title": "Batch presto spark rows", "pr_createdAt": "2020-08-07T16:54:18Z", "pr_url": "https://github.com/prestodb/presto/pull/14989", "timeline": [{"oid": "99a3724921617370bdc90147bf7429ac7520f16a", "url": "https://github.com/prestodb/presto/commit/99a3724921617370bdc90147bf7429ac7520f16a", "message": "Implement batching of tiny rows for Presto on Spark", "committedDate": "2020-08-11T03:54:13Z", "type": "forcePushed"}, {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5", "url": "https://github.com/prestodb/presto/commit/5efc3982ea46e269f0d24f480691ce3d754935a5", "message": "Implement batching of tiny rows for Presto on Spark", "committedDate": "2020-08-11T20:44:26Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc2ODY2MQ==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468768661", "bodyText": "nit. totalSizeInBytes", "author": "viczhang861", "createdAt": "2020-08-11T18:07:37Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -43,25 +43,27 @@\n     private final int rowCount;\n     private final byte[] rowData;\n     private final int[] rowPartitions;\n-    private final int[] rowSizes;\n+    private final int[] rowOffsets;\n+    private final int totalSize;", "originalCommit": "fd40afa0853b9c593e63e54b4cccf5cb3644e450", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg2ODk2MA==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468868960", "bodyText": "Put comment separately in the end or use a new line?", "author": "viczhang861", "createdAt": "2020-08-11T21:15:02Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -242,4 +384,62 @@ private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[]\n             return tuple;\n         }\n     }\n+\n+    public static class RowIndex\n+    {\n+        private static final int NIL = -1;\n+\n+        private final int[] nextRow;\n+        private final int[] rowIndex;\n+\n+        public static RowIndex create(int rowCount, int partitionCount, int[] partitions)\n+        {\n+            int[] nextRow = new int[partitionCount + 1 /*one more slot for replicated partition*/];", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwMTI3Mg==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468901272", "bodyText": "peekRow(partition) != NIL", "author": "viczhang861", "createdAt": "2020-08-11T22:31:58Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -242,4 +384,62 @@ private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[]\n             return tuple;\n         }\n     }\n+\n+    public static class RowIndex\n+    {\n+        private static final int NIL = -1;\n+\n+        private final int[] nextRow;\n+        private final int[] rowIndex;\n+\n+        public static RowIndex create(int rowCount, int partitionCount, int[] partitions)\n+        {\n+            int[] nextRow = new int[partitionCount + 1 /*one more slot for replicated partition*/];\n+            fill(nextRow, NIL);\n+            int[] rowIndex = new int[rowCount];\n+            fill(rowIndex, NIL);\n+\n+            for (int row = rowCount - 1; row >= 0; row--) {\n+                int partition = partitions[row];\n+                int partitionIndex = getPartitionIndex(partition, nextRow);\n+                int currentPointer = nextRow[partitionIndex];\n+                nextRow[partitionIndex] = row;\n+                rowIndex[row] = currentPointer;\n+            }\n+\n+            return new RowIndex(nextRow, rowIndex);\n+        }\n+\n+        private RowIndex(int[] nextRow, int[] rowIndex)\n+        {\n+            this.nextRow = requireNonNull(nextRow, \"nextRow is null\");\n+            this.rowIndex = requireNonNull(rowIndex, \"rowIndex is null\");\n+        }\n+\n+        public boolean hasNextRow(int partition)\n+        {\n+            return nextRow[getPartitionIndex(partition, nextRow)] != NIL;", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc5NjQ4Mg==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468796482", "bodyText": "when would this happen? essentially it's an empty row batch?", "author": "wenleix", "createdAt": "2020-08-11T18:53:54Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -171,6 +220,21 @@ private void closeEntry(int partitionId)\n         public PrestoSparkRowBatch build()\n         {\n             checkState(!openEntry, \"entry must be closed before creating a row batch\");\n+\n+            if (rowCount == 0) {", "originalCommit": "99a3724921617370bdc90147bf7429ac7520f16a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM4NzI1Nw==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469387257", "bodyText": "In theory it should never happen. We create the builder lazily. It should never be created if theres no rows to be appended. However I added it here for completeness, as the interface in theory allows you to create a zero rows batch.", "author": "arhimondr", "createdAt": "2020-08-12T16:27:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc5NjQ4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwMzY0Mg==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468903642", "bodyText": "Add some comment like \"Array-simulated linked-list to find the row offset for each partition\" assume i read the intention of this class right :)", "author": "wenleix", "createdAt": "2020-08-11T22:38:15Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -242,4 +384,62 @@ private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[]\n             return tuple;\n         }\n     }\n+\n+    public static class RowIndex", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM5NTk2Mg==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469395962", "bodyText": "Added more descriptive comment", "author": "arhimondr", "createdAt": "2020-08-12T16:42:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwMzY0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMTA2OA==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468911068", "bodyText": "hmm? what's this for? :)", "author": "wenleix", "createdAt": "2020-08-11T23:00:02Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -204,8 +333,9 @@ private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[]\n             this.totalSize = totalSize;\n \n             this.rowData = ByteBuffer.wrap(requireNonNull(rowData, \"rowData is null\"));\n+            this.rowData.order(LITTLE_ENDIAN);", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM0OTU4Mw==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469349583", "bodyText": "To make sure we are writing short in LITTLE_ENDIAN byte order, as the default byte order is BIG_ENDIAN that is different that the machine byte order.", "author": "arhimondr", "createdAt": "2020-08-12T15:34:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMTA2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMTQwMQ==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468911401", "bodyText": "is this while try to skip empty partition? -- in that case can it be an if statement instead ? :)", "author": "wenleix", "createdAt": "2020-08-11T23:01:02Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -179,6 +243,70 @@ public PrestoSparkRowBatch build()\n                     rowOffsets,\n                     totalSize);\n         }\n+\n+        private PrestoSparkRowBatch createGroupedRowBatch()\n+        {\n+            RowIndex rowIndex = RowIndex.create(rowCount, partitionCount, rowPartitions);\n+            byte[] data = sliceOutput.getUnderlyingSlice().byteArray();\n+\n+            DynamicSliceOutput output = new DynamicSliceOutput((int) (totalSize * 1.2f));\n+            int expectedEntriesCount = (int) ((totalSize / targetAverageRowSizeInBytes) * 1.2f);\n+            int[] entryOffsets = new int[expectedEntriesCount];\n+            int[] entryPartitions = new int[expectedEntriesCount];\n+            int entriesCount = 0;\n+            for (int partition = REPLICATED_ROW_PARTITION_ID; partition < partitionCount; partition++) {\n+                while (rowIndex.hasNextRow(partition)) {", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3MzM5NA==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468973394", "bodyText": "I think one partition may receive multiple row batches (entries), thus requires a while loop.", "author": "viczhang861", "createdAt": "2020-08-12T02:46:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMTQwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxNDQ1NQ==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468914455", "bodyText": "nit: new line.", "author": "wenleix", "createdAt": "2020-08-11T23:10:23Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowOutputOperator.java", "diffHunk": "@@ -170,7 +179,7 @@ public OperatorFactory duplicate()\n                     partitionChannels,\n                     partitionConstants,\n                     replicateNullsAndAny,\n-                    nullChannel);\n+                    nullChannel, targetAverageRowSizeInBytes);", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxNTczNg==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468915736", "bodyText": "lol", "author": "wenleix", "createdAt": "2020-08-11T23:14:42Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkShufflePageInput.java", "diffHunk": "@@ -76,32 +78,76 @@ public Page getNextPage()\n             while (currentIteratorIndex < shuffleInputs.size()) {\n                 PrestoSparkShuffleInput input = shuffleInputs.get(currentIteratorIndex);\n                 Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>> iterator = input.getIterator();\n-                long processedBytes = 0;\n+                long currentIteratorProcessedBytes = 0;\n+                long currentIteratorProcessedRows = 0;\n+                long currentIteratorProcessedEntries = 0;\n                 long start = System.currentTimeMillis();\n                 while (iterator.hasNext() && output.size() <= TARGET_SIZE && rowCount <= MAX_ROWS_PER_PAGE) {\n+                    currentIteratorProcessedEntries++;\n                     PrestoSparkMutableRow row = iterator.next()._2;\n                     if (row.getBuffer() != null) {\n                         ByteBuffer buffer = row.getBuffer();\n-                        output.writeBytes(buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n-                        processedBytes += buffer.remaining();\n+                        verify(buffer.remaining() >= 1, \"row must contain at least a single byte\");", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM1MDA5MQ==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469350091", "bodyText": "Now even a zero columns row will contain at least a single byte, that is a marker.", "author": "arhimondr", "createdAt": "2020-08-12T15:35:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxNTczNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk1OTI3OA==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468959278", "bodyText": "This is a nice implementation but a little difficult to understand, rowIndex[] is next next row I would prefer to rename nextRow to currentRow and rowIndex to nextRow. If you convert this implementation to an array of stacks, it could be simplified a lot. Are you worried about creating a lot objects when partitionCount is more than a few thousands.", "author": "viczhang861", "createdAt": "2020-08-12T01:50:54Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -242,4 +384,62 @@ private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[]\n             return tuple;\n         }\n     }\n+\n+    public static class RowIndex", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM5MDk4Mw==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469390983", "bodyText": "The main reason for implementing RowIndex was to avoid allocating extra objects along the way.\nLet me add a comment explaining how it works at the top of the class.", "author": "arhimondr", "createdAt": "2020-08-12T16:33:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk1OTI3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk2NDUzOA==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468964538", "bodyText": "Entries is a little difficult to guess what it means without reading the code. Have you consider to use totalProcessedRowBatches, at least for logging.", "author": "viczhang861", "createdAt": "2020-08-12T02:11:20Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java", "diffHunk": "@@ -819,33 +819,44 @@ private void processShuffleStats()\n         private void logShuffleStatsSummary(ShuffleStatsKey key, List<PrestoSparkShuffleStats> statsList)\n         {\n             long totalProcessedRows = 0;\n+            long totalProcessedEntries = 0;\n             long totalProcessedBytes = 0;\n             long totalElapsedWallTimeMills = 0;\n             for (PrestoSparkShuffleStats stats : statsList) {\n                 totalProcessedRows += stats.getProcessedRows();\n+                totalProcessedEntries += stats.getProcessedEntries();\n                 totalProcessedBytes += stats.getProcessedBytes();\n                 totalElapsedWallTimeMills += stats.getElapsedWallTimeMills();\n             }\n             long totalElapsedWallTimeSeconds = totalElapsedWallTimeMills / 1000;\n             long rowsPerSecond = totalProcessedRows;\n+            long entriesPerSecond = totalProcessedEntries;\n             long bytesPerSecond = totalProcessedBytes;\n             if (totalElapsedWallTimeSeconds > 0) {\n                 rowsPerSecond = totalProcessedRows / totalElapsedWallTimeSeconds;\n+                entriesPerSecond = totalProcessedEntries / totalElapsedWallTimeSeconds;\n                 bytesPerSecond = totalProcessedBytes / totalElapsedWallTimeSeconds;\n             }\n             long averageRowSize = 0;\n             if (totalProcessedRows > 0) {\n                 averageRowSize = totalProcessedBytes / totalProcessedRows;\n             }\n+            long averageEntrySize = 0;\n+            if (totalProcessedEntries > 0) {\n+                averageEntrySize = totalProcessedBytes / totalProcessedEntries;\n+            }\n             log.info(\n-                    \"Fragment: %s, Operation: %s, Rows: %s, Size: %s, Avg Row Size: %s, Time: %s, %srows/s, %s/s\",\n+                    \"Fragment: %s, Operation: %s, Rows: %s, Entries: %s, Size: %s, Avg Row Size: %s, Avg Entry Size: %s, Time: %s, %s rows/s, %s entries/s, %s/s\",", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk2ODIyMA==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468968220", "bodyText": "Ideally each partition is expected to receive at least one row before doing a shuffle to make a shuffle meaningful ?", "author": "viczhang861", "createdAt": "2020-08-12T02:25:59Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -77,14 +89,38 @@ public int getPositionCount()\n         return rowCount;\n     }\n \n-    public static PrestoSparkRowBatchBuilder builder(int partitionCount)\n+    public static PrestoSparkRowBatchBuilder builder(int partitionCount, int targetAverageRowSizeInBytes)\n     {\n-        return new PrestoSparkRowBatchBuilder(partitionCount, DEFAULT_TARGET_SIZE, DEFAULT_EXPECTED_ROWS_COUNT);\n+        checkArgument(partitionCount > 0, \"partitionCount must be greater then zero: %s\", partitionCount);\n+        int targetSizeInBytes = partitionCount * targetAverageRowSizeInBytes;", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM1MTcwMQ==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469351701", "bodyText": "I'm trying to figure out an average here. It is fine when one partitions receives more than the other.", "author": "arhimondr", "createdAt": "2020-08-12T15:37:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk2ODIyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NTYzNA==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468975634", "bodyText": "Constructor parameter names for PrestoSparkRowBatch is not updated.", "author": "viczhang861", "createdAt": "2020-08-12T02:55:17Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -179,6 +243,70 @@ public PrestoSparkRowBatch build()\n                     rowOffsets,\n                     totalSize);\n         }\n+\n+        private PrestoSparkRowBatch createGroupedRowBatch()\n+        {\n+            RowIndex rowIndex = RowIndex.create(rowCount, partitionCount, rowPartitions);\n+            byte[] data = sliceOutput.getUnderlyingSlice().byteArray();\n+\n+            DynamicSliceOutput output = new DynamicSliceOutput((int) (totalSize * 1.2f));\n+            int expectedEntriesCount = (int) ((totalSize / targetAverageRowSizeInBytes) * 1.2f);\n+            int[] entryOffsets = new int[expectedEntriesCount];\n+            int[] entryPartitions = new int[expectedEntriesCount];\n+            int entriesCount = 0;\n+            for (int partition = REPLICATED_ROW_PARTITION_ID; partition < partitionCount; partition++) {\n+                while (rowIndex.hasNextRow(partition)) {\n+                    // start entry\n+                    short currentEntrySize = 0;\n+                    short currentEntryRowCount = 0;\n+                    int currentEntryOffset = output.size();\n+                    output.writeByte(MULTI_ROW_ENTRY_MARKER);\n+                    // Reserve space for the row count, the actual row count will be set later\n+                    output.writeShort(0);\n+\n+                    entryOffsets = ensureCapacity(entryOffsets, entriesCount + 1);\n+                    entryOffsets[entriesCount] = currentEntryOffset;\n+                    entryPartitions = ensureCapacity(entryPartitions, entriesCount + 1);\n+                    entryPartitions[entriesCount] = partition;\n+\n+                    while (rowIndex.hasNextRow(partition)) {\n+                        int row = rowIndex.peekRow(partition);\n+                        int followingRow = row + 1;\n+                        int rowOffset = rowOffsets[row];\n+                        int followingRowOffset = followingRow < rowCount ? rowOffsets[followingRow] : totalSize;\n+                        int rowSize = followingRowOffset - rowOffset;\n+                        // each row entry should contain at least a marker;\n+                        verify(rowSize >= 1, \"rowSize is expected to be greater than or equal to zero: %s\", rowSize);\n+\n+                        // skip the marker\n+                        rowOffset++;\n+                        rowSize--;\n+\n+                        if (currentEntryRowCount > 0 && (currentEntrySize + rowSize > maxEntrySizeInBytes || currentEntryRowCount + 1 > maxRowsPerEntry)) {\n+                            break;\n+                        }\n+\n+                        output.writeBytes(data, rowOffset, rowSize);\n+                        currentEntrySize += rowSize;\n+                        currentEntryRowCount++;\n+\n+                        rowIndex.nextRow(partition);\n+                    }\n+\n+                    // entry is done\n+                    output.getUnderlyingSlice().setShort(currentEntryOffset + 1, currentEntryRowCount);\n+                    entriesCount++;\n+                }\n+            }\n+\n+            return new PrestoSparkRowBatch(", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM5ODcyMQ==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469398721", "bodyText": "Yeah, I deliberately decided not to do a more extensive rename, as this code should be pretty transitional", "author": "arhimondr", "createdAt": "2020-08-12T16:46:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NTYzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjE5OQ==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468976199", "bodyText": "Have you considered to not distinguish SINGLE and MULTIPLE.  For SINGLE_ROW, one byte (marker) is changed to short (rowCount),  for MULTIPLE row, three bytes (marker and rowCount) becomes two bytes (rowCount).", "author": "viczhang861", "createdAt": "2020-08-12T02:57:34Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -27,17 +28,28 @@\n \n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Verify.verify;\n import static io.airlift.slice.SizeOf.sizeOf;\n+import static java.lang.Math.max;\n+import static java.lang.Math.min;\n+import static java.nio.ByteOrder.LITTLE_ENDIAN;\n+import static java.util.Arrays.fill;\n import static java.util.Objects.requireNonNull;\n \n public class PrestoSparkRowBatch\n         implements PrestoSparkBufferedResult\n {\n     private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n \n-    private static final int DEFAULT_TARGET_SIZE = 1024 * 1024;\n+    public static final byte SINGLE_ROW_ENTRY_MARKER = 1;", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM1MzI0Ng==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469353246", "bodyText": "That is a very good point. Let me refactor it.", "author": "arhimondr", "createdAt": "2020-08-12T15:39:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjE5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM2MTM5Ng==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469361396", "bodyText": "Actually now I realize that we might not even need short to store number of rows.\nWe can use unsigned byte instead that gives us a good 0 - 255 range and allows us to store 255 rows per batch. The theoretical minimum row size is 2 bytes (1 byte for null marker and 1 byte for data). With 255 rows per batch we can get to moderately decent 510 bytes. It is still less that expected 1kb, but with a single short column row we are already getting very close to it (3 * 255 = 765 bytes). And with a single integer column rows we are even above the threshold (5 * 255 = 1275)\nOn the other end one byte of overhead per row (to store short) is also nothing terrible. We can do that as well.\nWhat do you think?", "author": "arhimondr", "createdAt": "2020-08-12T15:47:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjE5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM3NjE4Ng==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469376186", "bodyText": "Actually let me proceed with short, as 1 vs 2 bytes won't make a big difference with the target entry size ~1kb", "author": "arhimondr", "createdAt": "2020-08-12T16:09:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjE5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTcyNDUxNg==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469724516", "bodyText": "as 1 vs 2 bytes won't make a big difference with the target entry size ~1kb\n\nYeah, it's also valuable to make encoding simple :).  Just make sure I understand correctly -- for now a single row is considered as a \"row batch\" with rowCount = 1?", "author": "wenleix", "createdAt": "2020-08-13T06:28:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjE5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDAwNjU3NQ==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r470006575", "bodyText": "Yeah", "author": "arhimondr", "createdAt": "2020-08-13T14:45:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjE5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NzUzOQ==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468977539", "bodyText": "entryData, entryOffsets, entryPartitions", "author": "viczhang861", "createdAt": "2020-08-12T03:02:41Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -43,25 +43,27 @@\n     private final int rowCount;\n     private final byte[] rowData;\n     private final int[] rowPartitions;\n-    private final int[] rowSizes;\n+    private final int[] rowOffsets;\n+    private final int totalSize;\n     private final long retainedSizeInBytes;\n \n-    private PrestoSparkRowBatch(int partitionCount, int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n+    private PrestoSparkRowBatch(int partitionCount, int rowCount, byte[] rowData, int[] rowPartitions, int[] rowOffsets, int totalSize)", "originalCommit": "707081b6641904d0c71fcf0ff65d90c3ecff6f59", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM2MzE2MQ==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469363161", "bodyText": "I deliberately didn't want to do the extensive renames, as this might potentially go to a very large scope. Since this code is transitional I would like to keep it the way it is, as then when we revert this patch we would have to rename everything back.", "author": "arhimondr", "createdAt": "2020-08-12T15:50:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NzUzOQ=="}], "type": "inlineReview"}, {"oid": "e77a7081b87d3e8bd67a469b317a2a18db676a81", "url": "https://github.com/prestodb/presto/commit/e77a7081b87d3e8bd67a469b317a2a18db676a81", "message": "Implement batching of tiny rows for Presto on Spark", "committedDate": "2020-08-12T16:50:27Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2OTkxMw==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469569913", "bodyText": "Out of dated comment", "author": "viczhang861", "createdAt": "2020-08-12T21:57:41Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -171,13 +217,91 @@ private void closeEntry(int partitionId)\n         public PrestoSparkRowBatch build()\n         {\n             checkState(!openEntry, \"entry must be closed before creating a row batch\");\n+\n+            if (rowCount == 0) {\n+                return createDirectRowBatch();\n+            }\n+\n+            int averageRowSize = totalSizeInBytes / rowCount;\n+            if (averageRowSize < targetAverageRowSizeInBytes) {\n+                return createGroupedRowBatch();\n+            }\n+\n+            return createDirectRowBatch();\n+        }\n+\n+        private PrestoSparkRowBatch createDirectRowBatch()\n+        {\n             return new PrestoSparkRowBatch(\n                     partitionCount,\n                     rowCount,\n                     sliceOutput.getUnderlyingSlice().byteArray(),\n                     rowPartitions,\n                     rowOffsets,\n-                    totalSize);\n+                    totalSizeInBytes);\n+        }\n+\n+        private PrestoSparkRowBatch createGroupedRowBatch()\n+        {\n+            RowIndex rowIndex = RowIndex.create(rowCount, partitionCount, rowPartitions);\n+            byte[] data = sliceOutput.getUnderlyingSlice().byteArray();\n+\n+            DynamicSliceOutput output = new DynamicSliceOutput((int) (totalSizeInBytes * 1.2f));\n+            int expectedEntriesCount = (int) ((totalSizeInBytes / targetAverageRowSizeInBytes) * 1.2f);\n+            int[] entryOffsets = new int[expectedEntriesCount];\n+            int[] entryPartitions = new int[expectedEntriesCount];\n+            int entriesCount = 0;\n+            for (int partition = REPLICATED_ROW_PARTITION_ID; partition < partitionCount; partition++) {\n+                while (rowIndex.hasNextRow(partition)) {\n+                    // start entry\n+                    short currentEntrySize = 0;\n+                    short currentEntryRowCount = 0;\n+                    int currentEntryOffset = output.size();\n+                    // Reserve space for the row count, the actual row count will be set later\n+                    output.writeShort(0);\n+\n+                    entryOffsets = ensureCapacity(entryOffsets, entriesCount + 1);\n+                    entryOffsets[entriesCount] = currentEntryOffset;\n+                    entryPartitions = ensureCapacity(entryPartitions, entriesCount + 1);\n+                    entryPartitions[entriesCount] = partition;\n+\n+                    while (rowIndex.hasNextRow(partition)) {\n+                        int row = rowIndex.peekRow(partition);\n+                        int followingRow = row + 1;\n+                        int rowOffset = rowOffsets[row];\n+                        int followingRowOffset = followingRow < rowCount ? rowOffsets[followingRow] : totalSizeInBytes;\n+                        int rowSize = followingRowOffset - rowOffset;\n+                        // each row entry should contain at least a marker;", "originalCommit": "e77a7081b87d3e8bd67a469b317a2a18db676a81", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "6c35eeee2c5a000c7f0f6770aca24812b5c3239a", "url": "https://github.com/prestodb/presto/commit/6c35eeee2c5a000c7f0f6770aca24812b5c3239a", "message": "Use offset instead of size in PrestoSparkRowBatch", "committedDate": "2020-08-13T02:17:11Z", "type": "commit"}, {"oid": "0d8ba5cc581f395de9ab2301e627ede08c239ca1", "url": "https://github.com/prestodb/presto/commit/0d8ba5cc581f395de9ab2301e627ede08c239ca1", "message": "Implement batching of tiny rows for Presto on Spark", "committedDate": "2020-08-13T02:18:51Z", "type": "commit"}, {"oid": "0d8ba5cc581f395de9ab2301e627ede08c239ca1", "url": "https://github.com/prestodb/presto/commit/0d8ba5cc581f395de9ab2301e627ede08c239ca1", "message": "Implement batching of tiny rows for Presto on Spark", "committedDate": "2020-08-13T02:18:51Z", "type": "forcePushed"}]}