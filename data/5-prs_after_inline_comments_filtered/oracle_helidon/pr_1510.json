{"pr_number": 1510, "pr_title": "Kafka support", "pr_createdAt": "2020-03-13T12:09:39Z", "pr_url": "https://github.com/oracle/helidon/pull/1510", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjg5ODMyNA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392898324", "bodyText": "Helidon has a flat package structure - each module can only use one package (and spi if needed).", "author": "tomas-langer", "createdAt": "2020-03-16T09:56:40Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ1NzE5Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394457197", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-18T15:57:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjg5ODMyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkxNjgwOA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392916808", "bodyText": "This method should be private (or package local)", "author": "tomas-langer", "createdAt": "2020-03-16T10:30:22Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.Collection;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+\n+import io.helidon.common.configurable.ThreadPoolSupplier;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n+import io.helidon.messaging.kafka.SimpleKafkaProducer;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n+\n+    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MDU3Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397780576", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T11:22:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkxNjgwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkxODA3Mg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392918072", "bodyText": "public method with no javadoc.\nAlso Helidon uses getters without get verb, so the method should be called consumers.\nI am not sure it should be public at all. Please minimize number of public methods that are not implementing interface methods.", "author": "tomas-langer", "createdAt": "2020-03-16T10:31:56Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.Collection;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+\n+import io.helidon.common.configurable.ThreadPoolSupplier;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n+import io.helidon.messaging.kafka.SimpleKafkaProducer;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n+\n+    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        SimpleKafkaConsumer<Object, Object> consumer;\n+        while ((consumer = consumers.poll()) != null) {\n+            consumer.close();\n+        }\n+    }\n+\n+    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MTAxMQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397781011", "bodyText": "It is for testing purposes, to check that all resources are closed. Not it has package visibility and other name without get.", "author": "jbescos", "createdAt": "2020-03-25T11:23:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkxODA3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkxOTc3Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392919773", "bodyText": "You are creating a new thread pool for each publisher. That is probably not intended.", "author": "tomas-langer", "createdAt": "2020-03-16T10:33:57Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.Collection;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+\n+import io.helidon.common.configurable.ThreadPoolSupplier;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n+import io.helidon.messaging.kafka.SimpleKafkaProducer;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n+\n+    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        SimpleKafkaConsumer<Object, Object> consumer;\n+        while ((consumer = consumers.poll()) != null) {\n+            consumer.close();\n+        }\n+    }\n+\n+    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {\n+        return consumers;\n+    }\n+\n+    @Override\n+    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n+        Config helidonConfig = (Config) config;\n+        SimpleKafkaConsumer<Object, Object> simpleKafkaConsumer = new SimpleKafkaConsumer<>(helidonConfig);\n+        consumers.add(simpleKafkaConsumer);\n+        return simpleKafkaConsumer.createPushPublisherBuilder(ThreadPoolSupplier.create(helidonConfig.get(\"executor-service\")).get());", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MTM3Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397781377", "bodyText": "Right, now it reuses a scheduler thread pool", "author": "jbescos", "createdAt": "2020-03-25T11:23:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkxOTc3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkyMDU1Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392920553", "bodyText": "In this case you do not close the producer.", "author": "tomas-langer", "createdAt": "2020-03-16T10:34:57Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.Collection;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+\n+import io.helidon.common.configurable.ThreadPoolSupplier;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n+import io.helidon.messaging.kafka.SimpleKafkaProducer;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n+\n+    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        SimpleKafkaConsumer<Object, Object> consumer;\n+        while ((consumer = consumers.poll()) != null) {\n+            consumer.close();\n+        }\n+    }\n+\n+    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {\n+        return consumers;\n+    }\n+\n+    @Override\n+    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n+        Config helidonConfig = (Config) config;\n+        SimpleKafkaConsumer<Object, Object> simpleKafkaConsumer = new SimpleKafkaConsumer<>(helidonConfig);\n+        consumers.add(simpleKafkaConsumer);\n+        return simpleKafkaConsumer.createPushPublisherBuilder(ThreadPoolSupplier.create(helidonConfig.get(\"executor-service\")).get());\n+    }\n+\n+    @Override\n+    public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n+        Config helidonConfig = (Config) config;\n+        SimpleKafkaProducer<Object, Object> simpleKafkaProducer = new SimpleKafkaProducer<>(helidonConfig);\n+        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n+\n+            @Override\n+            public void onSubscribe(Subscription s) {\n+                s.request(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public void onNext(Message<?> message) {\n+                simpleKafkaProducer.produce(message.getPayload());\n+                message.ack();\n+            }\n+\n+            @Override\n+            public void onError(Throwable t) {\n+                LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MTU0OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397781549", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T11:24:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkyMDU1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkyMTAwMw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392921003", "bodyText": "No backpressure support may cause issues in reactive environment.", "author": "tomas-langer", "createdAt": "2020-03-16T10:35:25Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.Collection;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+\n+import io.helidon.common.configurable.ThreadPoolSupplier;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.SimpleKafkaConsumer;\n+import io.helidon.messaging.kafka.SimpleKafkaProducer;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Partial implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+public class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    public static final String CONNECTOR_NAME = \"helidon-kafka\";\n+\n+    private Queue<SimpleKafkaConsumer<Object, Object>> consumers = new ConcurrentLinkedQueue<>();\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        SimpleKafkaConsumer<Object, Object> consumer;\n+        while ((consumer = consumers.poll()) != null) {\n+            consumer.close();\n+        }\n+    }\n+\n+    public Collection<SimpleKafkaConsumer<Object, Object>> getConsumers() {\n+        return consumers;\n+    }\n+\n+    @Override\n+    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n+        Config helidonConfig = (Config) config;\n+        SimpleKafkaConsumer<Object, Object> simpleKafkaConsumer = new SimpleKafkaConsumer<>(helidonConfig);\n+        consumers.add(simpleKafkaConsumer);\n+        return simpleKafkaConsumer.createPushPublisherBuilder(ThreadPoolSupplier.create(helidonConfig.get(\"executor-service\")).get());\n+    }\n+\n+    @Override\n+    public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n+        Config helidonConfig = (Config) config;\n+        SimpleKafkaProducer<Object, Object> simpleKafkaProducer = new SimpleKafkaProducer<>(helidonConfig);\n+        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n+\n+            @Override\n+            public void onSubscribe(Subscription s) {\n+                s.request(Long.MAX_VALUE);", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MTc4NA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397781784", "bodyText": "It is now configured a configured parameter.", "author": "jbescos", "createdAt": "2020-03-25T11:24:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkyMTAwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkyMTU5Mg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392921592", "bodyText": "This class should not be public maybe?", "author": "tomas-langer", "createdAt": "2020-03-16T10:36:01Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+\n+/**\n+ * Kafka specific MP messaging message.\n+ *\n+ * @param <K> kafka record key type\n+ * @param <V> kafka record value type\n+ */\n+public class KafkaMessage<K, V> implements Message<ConsumerRecord<K, V>> {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MjEyMA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397782120", "bodyText": "It has packaged visibility now", "author": "jbescos", "createdAt": "2020-03-25T11:25:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkyMTU5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMDc4NA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392930784", "bodyText": "Getter should not use get, also I guess this should not be public.", "author": "tomas-langer", "createdAt": "2020-03-16T10:47:09Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+\n+/**\n+ * Kafka specific MP messaging message.\n+ *\n+ * @param <K> kafka record key type\n+ * @param <V> kafka record value type\n+ */\n+public class KafkaMessage<K, V> implements Message<ConsumerRecord<K, V>> {\n+\n+    private ConsumerRecord<K, V> consumerRecord;\n+    private CompletableFuture<Void> ackFuture = new CompletableFuture<>();\n+\n+    /**\n+     * Kafka specific MP messaging message.\n+     *\n+     * @param consumerRecord {@link org.apache.kafka.clients.consumer.ConsumerRecord}\n+     */\n+    public KafkaMessage(ConsumerRecord<K, V> consumerRecord) {\n+        this.consumerRecord = consumerRecord;\n+    }\n+\n+    @Override\n+    public ConsumerRecord<K, V> getPayload() {\n+        return consumerRecord;\n+    }\n+\n+    public CompletableFuture<Void> getAckFuture() {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MjI3MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397782270", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T11:25:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMDc4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMTI2OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392931269", "bodyText": "Constructors may never be public in Helidon, unless required by CDI or JAX-RS.\nWe use factory methods (if need to be public).", "author": "tomas-langer", "createdAt": "2020-03-16T10:48:03Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+\n+/**\n+ * Kafka specific MP messaging message.\n+ *\n+ * @param <K> kafka record key type\n+ * @param <V> kafka record value type\n+ */\n+public class KafkaMessage<K, V> implements Message<ConsumerRecord<K, V>> {\n+\n+    private ConsumerRecord<K, V> consumerRecord;\n+    private CompletableFuture<Void> ackFuture = new CompletableFuture<>();\n+\n+    /**\n+     * Kafka specific MP messaging message.\n+     *\n+     * @param consumerRecord {@link org.apache.kafka.clients.consumer.ConsumerRecord}\n+     */\n+    public KafkaMessage(ConsumerRecord<K, V> consumerRecord) {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc4MzAyMQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397783021", "bodyText": "It has package visibility now", "author": "jbescos", "createdAt": "2020-03-25T11:27:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMTI2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMTU4NQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392931585", "bodyText": "Is there no support for ack in Kafka itself? This basically makes the ack method a no-op.", "author": "tomas-langer", "createdAt": "2020-03-16T10:48:36Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/KafkaMessage.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+\n+/**\n+ * Kafka specific MP messaging message.\n+ *\n+ * @param <K> kafka record key type\n+ * @param <V> kafka record value type\n+ */\n+public class KafkaMessage<K, V> implements Message<ConsumerRecord<K, V>> {\n+\n+    private ConsumerRecord<K, V> consumerRecord;\n+    private CompletableFuture<Void> ackFuture = new CompletableFuture<>();\n+\n+    /**\n+     * Kafka specific MP messaging message.\n+     *\n+     * @param consumerRecord {@link org.apache.kafka.clients.consumer.ConsumerRecord}\n+     */\n+    public KafkaMessage(ConsumerRecord<K, V> consumerRecord) {\n+        this.consumerRecord = consumerRecord;\n+    }\n+\n+    @Override\n+    public ConsumerRecord<K, V> getPayload() {\n+        return consumerRecord;\n+    }\n+\n+    public CompletableFuture<Void> getAckFuture() {\n+        return ackFuture;\n+    }\n+\n+    @Override\n+    public CompletionStage<Void> ack() {\n+        ackFuture.complete(null);", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc5MTU4OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397791588", "bodyText": "I spoke with @danielkec about this. The thing is that polling from Kafka is a blocking operation, and in reactive streams we cannot have threads blocked. So we need some way to make it in non-blocking way.\nThe workaround to deal with this is the BackPressureLayer. There we are buffering events coming from polling. That KafkaMessage.ack() is communication with BackPressureLayer, instead of doing it with Kafka.", "author": "jbescos", "createdAt": "2020-03-25T11:43:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMTU4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMjA0Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392932046", "bodyText": "The name is not good.", "author": "tomas-langer", "createdAt": "2020-03-16T10:49:25Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/SimplePublisher.java", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.function.Consumer;\n+\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+/**\n+ * Reactive streams publisher using {@link java.util.function.Consumer} instead of reactive streams.\n+ *\n+ * @param <K> kafka record key type\n+ * @param <V> kafka record value type\n+ */\n+public class SimplePublisher<K, V> implements Publisher<KafkaMessage<K, V>> {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNDU2Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392934566", "bodyText": "Also do you expect users to use this class? If not, it must not be public.", "author": "tomas-langer", "createdAt": "2020-03-16T10:54:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMjA0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc5MTg3MQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397791871", "bodyText": "It is not public and it is renamed.", "author": "jbescos", "createdAt": "2020-03-25T11:43:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzMjA0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNDgyOA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392934828", "bodyText": "Public constructor cannot be used. Also if class is not to be public, this method will not be public.", "author": "tomas-langer", "createdAt": "2020-03-16T10:54:33Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/connector/SimplePublisher.java", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka.connector;\n+\n+import java.util.function.Consumer;\n+\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+/**\n+ * Reactive streams publisher using {@link java.util.function.Consumer} instead of reactive streams.\n+ *\n+ * @param <K> kafka record key type\n+ * @param <V> kafka record value type\n+ */\n+public class SimplePublisher<K, V> implements Publisher<KafkaMessage<K, V>> {\n+\n+    private Consumer<Subscriber<? super KafkaMessage<K, V>>> publisher;\n+\n+    /**\n+     * Create new Reactive Streams publisher using {@link java.util.function.Consumer} instead of reactive streams.\n+     *\n+     * @param publisher {@link java.util.function.Consumer}\n+     */\n+    public SimplePublisher(Consumer<Subscriber<? super KafkaMessage<K, V>>> publisher) {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzc5MjEzOQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397792139", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T11:44:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNDgyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjA1OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392936058", "bodyText": "I am not sure why this class implements Properties - is this used to send to Kafka itself?\nIf not, this class should not implement Properties.\nI am not sure if the properties are \"free\" - if so, use an internal Map<String, String> to store them.\nIf not free, use explicit fields to store such configuration.", "author": "tomas-langer", "createdAt": "2020-03-16T10:56:57Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import io.helidon.config.Config;\n+\n+/**\n+ * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config}.\n+ * Configuration format as specified in the MicroProfile Reactive Messaging\n+ * Specification https://github.com/eclipse/microprofile-reactive-messaging\n+ *\n+ * <p>\n+ * See example with YAML configuration:\n+ * <pre>{@code\n+ * mp.messaging:\n+ *   incoming:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.deserializer: org.apache.kafka.common.serialization.LongDeserializer\n+ *       value.deserializer: org.apache.kafka.common.serialization.StringDeserializer\n+ *\n+ *   outgoing:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.serializer: org.apache.kafka.common.serialization.LongSerializer\n+ *       value.serializer: org.apache.kafka.common.serialization.StringSerializer\n+ *\n+ * }</pre>\n+ * <p>\n+ *\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaConfigProperties extends Properties {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjQyMg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392936422", "bodyText": "Do not use get verb in getters.", "author": "tomas-langer", "createdAt": "2020-03-16T10:57:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjA1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNzM1MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392937350", "bodyText": "Helidon classes (except for very specific cases) must be immutable. Properties is not immutable.", "author": "tomas-langer", "createdAt": "2020-03-16T10:59:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjA1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzgwOTMwMQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397809301", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T12:16:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjA1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjMyOA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392936328", "bodyText": "This should happen at the time this instance is created and stored in a field.", "author": "tomas-langer", "createdAt": "2020-03-16T10:57:30Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import io.helidon.config.Config;\n+\n+/**\n+ * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config}.\n+ * Configuration format as specified in the MicroProfile Reactive Messaging\n+ * Specification https://github.com/eclipse/microprofile-reactive-messaging\n+ *\n+ * <p>\n+ * See example with YAML configuration:\n+ * <pre>{@code\n+ * mp.messaging:\n+ *   incoming:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.deserializer: org.apache.kafka.common.serialization.LongDeserializer\n+ *       value.deserializer: org.apache.kafka.common.serialization.StringDeserializer\n+ *\n+ *   outgoing:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.serializer: org.apache.kafka.common.serialization.LongSerializer\n+ *       value.serializer: org.apache.kafka.common.serialization.StringSerializer\n+ *\n+ * }</pre>\n+ * <p>\n+ *\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaConfigProperties extends Properties {\n+\n+    /**\n+     * Topic or topics delimited by commas.\n+     */\n+    static final String TOPIC_NAME = \"topic\";\n+\n+    /**\n+     * Consumer group id.\n+     */\n+    static final String GROUP_ID = \"group.id\";\n+\n+    /**\n+     * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config},\n+     * underscores in keys are translated to dots.\n+     *\n+     * @param config parent config of kafka key\n+     */\n+    KafkaConfigProperties(Config config) {\n+        config.asNodeList().get().forEach(this::addProperty);\n+    }\n+\n+    /**\n+     * Split comma separated topic names.\n+     *\n+     * @return list of topic names\n+     */\n+    public List<String> getTopicNameList() {\n+        return Arrays.stream(getProperty(TOPIC_NAME)", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzgwOTc1NA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397809754", "bodyText": "This class is not instanced anymore. It contains some utility static methods.", "author": "jbescos", "createdAt": "2020-03-25T12:17:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNjMyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNzAxMg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392937012", "bodyText": "If you want to get everything into a map, just use config.detach().asMap().ifPresent(map -> ...)", "author": "tomas-langer", "createdAt": "2020-03-16T10:58:46Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/KafkaConfigProperties.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import io.helidon.config.Config;\n+\n+/**\n+ * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config}.\n+ * Configuration format as specified in the MicroProfile Reactive Messaging\n+ * Specification https://github.com/eclipse/microprofile-reactive-messaging\n+ *\n+ * <p>\n+ * See example with YAML configuration:\n+ * <pre>{@code\n+ * mp.messaging:\n+ *   incoming:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.deserializer: org.apache.kafka.common.serialization.LongDeserializer\n+ *       value.deserializer: org.apache.kafka.common.serialization.StringDeserializer\n+ *\n+ *   outgoing:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.serializer: org.apache.kafka.common.serialization.LongSerializer\n+ *       value.serializer: org.apache.kafka.common.serialization.StringSerializer\n+ *\n+ * }</pre>\n+ * <p>\n+ *\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaConfigProperties extends Properties {\n+\n+    /**\n+     * Topic or topics delimited by commas.\n+     */\n+    static final String TOPIC_NAME = \"topic\";\n+\n+    /**\n+     * Consumer group id.\n+     */\n+    static final String GROUP_ID = \"group.id\";\n+\n+    /**\n+     * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config},\n+     * underscores in keys are translated to dots.\n+     *\n+     * @param config parent config of kafka key\n+     */\n+    KafkaConfigProperties(Config config) {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3NTczOQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397875739", "bodyText": "Great, thanks", "author": "jbescos", "createdAt": "2020-03-25T14:00:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNzAxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNzQ4Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392937487", "bodyText": "Why is this class public?", "author": "tomas-langer", "createdAt": "2020-03-16T10:59:35Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/PartitionsAssignedLatch.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.util.Collection;\n+import java.util.concurrent.CountDownLatch;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.common.TopicPartition;\n+\n+/**\n+ * Waiting latch for partition assigment, after that is consumer ready to receive.\n+ */\n+public class PartitionsAssignedLatch extends CountDownLatch implements ConsumerRebalanceListener {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3NTg0Mg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397875842", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:00:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNzQ4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzODY3MQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392938671", "bodyText": "If I understand correctly, we want an implementation for Microprofile reactive messaging. In such a case, this class should not be public.\nAlso the name is not very good.", "author": "tomas-langer", "createdAt": "2020-03-16T11:01:42Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.Consumer;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.connector.KafkaMessage;\n+import io.helidon.messaging.kafka.connector.SimplePublisher;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Simple Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ * <p>\n+ * Usage:\n+ * <pre>{@code\n+ *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n+ *         c.consumeAsync(r -> System.out.println(r.value()));\n+ *   }\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+public class SimpleKafkaConsumer<K, V> implements Closeable {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ1NzU1MQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394457551", "bodyText": "I have changed the name and visibility", "author": "jbescos", "createdAt": "2020-03-18T15:57:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzODY3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzOTMzNA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392939334", "bodyText": "Never use public constructors in Helidon.\nIf this class should be public (in which case the name should change), use a Builder pattern as in other Helidon classes.\nYou may have a static factory method, such as create(String, Config), but should be limited to one such method.", "author": "tomas-langer", "createdAt": "2020-03-16T11:02:53Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.Consumer;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.connector.KafkaMessage;\n+import io.helidon.messaging.kafka.connector.SimplePublisher;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Simple Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ * <p>\n+ * Usage:\n+ * <pre>{@code\n+ *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n+ *         c.consumeAsync(r -> System.out.println(r.value()));\n+ *   }\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+public class SimpleKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(SimpleKafkaConsumer.class.getName());\n+    private final KafkaConfigProperties properties;\n+\n+    private final AtomicBoolean closed = new AtomicBoolean(false);\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final String consumerId;\n+    private ExecutorService executorService;\n+    private ExecutorService externalExecutorService;\n+    private final List<String> topicNameList;\n+    private final KafkaConsumer<K, V> consumer;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param channelName key in configuration\n+     * @param config      Helidon {@link io.helidon.config.Config config}\n+     * @see KafkaConfigProperties\n+     * @see io.helidon.config.Config\n+     */\n+    public SimpleKafkaConsumer(String channelName, Config config) {", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ2MTE2OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394461168", "bodyText": "I think that class was designed with 2 purposes:\n\nTo be integrated with cdi\nTo be used from any other code to consume from Kafka.\n\nI have simplified this class for the first point. There is only one constructor.", "author": "jbescos", "createdAt": "2020-03-18T16:02:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzOTMzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzOTYwMw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392939603", "bodyText": "Creating a new executor service for each request is wrong use of executors.", "author": "tomas-langer", "createdAt": "2020-03-16T11:03:22Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.Consumer;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.connector.KafkaMessage;\n+import io.helidon.messaging.kafka.connector.SimplePublisher;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Simple Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ * <p>\n+ * Usage:\n+ * <pre>{@code\n+ *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n+ *         c.consumeAsync(r -> System.out.println(r.value()));\n+ *   }\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+public class SimpleKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(SimpleKafkaConsumer.class.getName());\n+    private final KafkaConfigProperties properties;\n+\n+    private final AtomicBoolean closed = new AtomicBoolean(false);\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final String consumerId;\n+    private ExecutorService executorService;\n+    private ExecutorService externalExecutorService;\n+    private final List<String> topicNameList;\n+    private final KafkaConsumer<K, V> consumer;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param channelName key in configuration\n+     * @param config      Helidon {@link io.helidon.config.Config config}\n+     * @see KafkaConfigProperties\n+     * @see io.helidon.config.Config\n+     */\n+    public SimpleKafkaConsumer(String channelName, Config config) {\n+        this(channelName, config, null);\n+    }\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param channelName     key in configuration\n+     * @param config          Helidon {@link io.helidon.config.Config config}\n+     * @param consumerGroupId Custom group.id, can be null, overrides group.id from configuration\n+     * @see KafkaConfigProperties\n+     * @see io.helidon.config.Config\n+     */\n+    public SimpleKafkaConsumer(String channelName, Config config, String consumerGroupId) {\n+        this.properties = new KafkaConfigProperties(config.get(\"mp.messaging.incoming\").get(channelName));\n+        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(consumerGroupId));\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumerId = channelName;\n+        this.consumer = new KafkaConsumer<>(properties);\n+    }\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    public SimpleKafkaConsumer(Config config) {\n+        this.properties = new KafkaConfigProperties(config);\n+        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(null));\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumerId = null;\n+        this.consumer = new KafkaConsumer<>(properties);\n+    }\n+\n+    /**\n+     * Execute supplied consumer for each received record.\n+     *\n+     * @param function to be executed for each received record\n+     * @return {@link java.util.concurrent.Future}\n+     */\n+    public Future<?> consumeAsync(Consumer<ConsumerRecord<K, V>> function) {\n+        return this.consumeAsync(Executors.newWorkStealingPool(), null, function);", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ2MTg1NA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394461854", "bodyText": "This part is out.", "author": "jbescos", "createdAt": "2020-03-18T16:03:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzOTYwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk0MDM1Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392940357", "bodyText": "Info messages should be only printed if we know it will happen only once per runtime of Helidon. I am not sure this is the case.", "author": "tomas-langer", "createdAt": "2020-03-16T11:04:49Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.Consumer;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.connector.KafkaMessage;\n+import io.helidon.messaging.kafka.connector.SimplePublisher;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Simple Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ * <p>\n+ * Usage:\n+ * <pre>{@code\n+ *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n+ *         c.consumeAsync(r -> System.out.println(r.value()));\n+ *   }\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+public class SimpleKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(SimpleKafkaConsumer.class.getName());\n+    private final KafkaConfigProperties properties;\n+\n+    private final AtomicBoolean closed = new AtomicBoolean(false);\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final String consumerId;\n+    private ExecutorService executorService;\n+    private ExecutorService externalExecutorService;\n+    private final List<String> topicNameList;\n+    private final KafkaConsumer<K, V> consumer;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param channelName key in configuration\n+     * @param config      Helidon {@link io.helidon.config.Config config}\n+     * @see KafkaConfigProperties\n+     * @see io.helidon.config.Config\n+     */\n+    public SimpleKafkaConsumer(String channelName, Config config) {\n+        this(channelName, config, null);\n+    }\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param channelName     key in configuration\n+     * @param config          Helidon {@link io.helidon.config.Config config}\n+     * @param consumerGroupId Custom group.id, can be null, overrides group.id from configuration\n+     * @see KafkaConfigProperties\n+     * @see io.helidon.config.Config\n+     */\n+    public SimpleKafkaConsumer(String channelName, Config config, String consumerGroupId) {\n+        this.properties = new KafkaConfigProperties(config.get(\"mp.messaging.incoming\").get(channelName));\n+        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(consumerGroupId));\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumerId = channelName;\n+        this.consumer = new KafkaConsumer<>(properties);\n+    }\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    public SimpleKafkaConsumer(Config config) {\n+        this.properties = new KafkaConfigProperties(config);\n+        this.properties.setProperty(KafkaConfigProperties.GROUP_ID, getOrGenerateGroupId(null));\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumerId = null;\n+        this.consumer = new KafkaConsumer<>(properties);\n+    }\n+\n+    /**\n+     * Execute supplied consumer for each received record.\n+     *\n+     * @param function to be executed for each received record\n+     * @return {@link java.util.concurrent.Future}\n+     */\n+    public Future<?> consumeAsync(Consumer<ConsumerRecord<K, V>> function) {\n+        return this.consumeAsync(Executors.newWorkStealingPool(), null, function);\n+    }\n+\n+    /**\n+     * Execute supplied consumer by provided executor service for each received record.\n+     *\n+     * @param executorService Custom executor service used for spinning up polling thread and record consuming threads\n+     * @param customTopics    Can be null, list of topics appended to the list from configuration\n+     * @param function        Consumer method executed in new thread for each received record\n+     * @return The Future's get method will return null when consumer is closed\n+     */\n+    public Future<?> consumeAsync(ExecutorService executorService, List<String> customTopics,\n+                                  Consumer<ConsumerRecord<K, V>> function) {\n+        LOGGER.info(String.format(\"Initiating kafka consumer %s listening to topics: %s with groupId: %s\",", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ2MjEwMw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394462103", "bodyText": "This is removed too.", "author": "jbescos", "createdAt": "2020-03-18T16:03:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk0MDM1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk0MjMzNQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r392942335", "bodyText": "This seems to be mutable (and wrongly).\nIf the consumer is only for a single use, make sure you construct the instance with all the configuration, then have a method to start listening. This will simplify a lot of checks.\nAlso not sure why there is executorService and externalExecutorService", "author": "tomas-langer", "createdAt": "2020-03-16T11:08:32Z", "path": "messaging/kafka/src/main/java/io/helidon/messaging/kafka/SimpleKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.Consumer;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+import io.helidon.messaging.kafka.connector.KafkaMessage;\n+import io.helidon.messaging.kafka.connector.SimplePublisher;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Simple Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ * <p>\n+ * Usage:\n+ * <pre>{@code\n+ *   try (SimpleKafkaConsumer<Long, String> c = new SimpleKafkaConsumer<>(\"test-channel\", Config.create())) {\n+ *         c.consumeAsync(r -> System.out.println(r.value()));\n+ *   }\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+public class SimpleKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(SimpleKafkaConsumer.class.getName());\n+    private final KafkaConfigProperties properties;\n+\n+    private final AtomicBoolean closed = new AtomicBoolean(false);\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final String consumerId;\n+    private ExecutorService executorService;", "originalCommit": "ffd1d154247f0dd9a9252b0ea12aa2e948531bee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ2ODY4Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394468687", "bodyText": "Now it is simpler.\nRegarding the executors, I followed your suggestion about usage of one unique ScheduledExecutorService that is shared between all the kafka consumers.", "author": "jbescos", "createdAt": "2020-03-18T16:12:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjk0MjMzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ1NTA0Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394455046", "bodyText": "I am not sure if this while is acceptable because of CPU usage. However I put it there because the time frame must be very small (few milliseconds) or zero.\nLet me know if you prefer other way, with count down latch for example.", "author": "jbescos", "createdAt": "2020-03-18T15:54:45Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    // We need this flag to avoid this task is executed more than one time at the same time by ScheduledExecutorService\n+    private final AtomicBoolean running = new AtomicBoolean(false);\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }\n+\n+                @Override\n+                public void cancel() {\n+                    BasicKafkaConsumer.this.close();\n+                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n+                }\n+            });\n+            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber, \n+                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0, \n+                    config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElseGet(() -> 100L), TimeUnit.MILLISECONDS);\n+        }));\n+    }\n+\n+    /**\n+     * Blocks current thread until partitions are assigned,\n+     * since when is consumer effectively ready to receive.\n+     *\n+     * @param timeout the maximum time to wait\n+     * @param unit    the time unit of the timeout argument\n+     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n+     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n+     */\n+    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n+        if (!partitionsAssignedLatch.await(timeout, unit)) {\n+            throw new TimeoutException(\"Timeout for subscription reached\");\n+        }\n+    }\n+\n+    /**\n+     * Close gracefully. Stops wakes possible blocked poll and close consumer.\n+     */\n+    @Override\n+    public void close() {\n+        // Stops pooling\n+        consumer.wakeup();\n+        while (running.get()) {", "originalCommit": "2afe1de6f909848b6b58d0a9079522ee9ef5858f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDg0NTQ2MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394845460", "bodyText": "I will do this with a synchronize.", "author": "jbescos", "createdAt": "2020-03-19T07:59:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ1NTA0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDg0Njc1Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394846756", "bodyText": "In this case I think it is much better to use a lock.", "author": "jbescos", "createdAt": "2020-03-19T08:02:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ1NTA0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3OTMyNg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394479326", "bodyText": "Uf this was my super bad idea, we have to do something about this, it basically ignores backpressure. Something like this would be much better:\nhttps://github.com/oracle/helidon/blob/1e5ae594bc356ecd1283e487a7e7f85e26355ee9/microprofile/reactive-streams/src/main/java/io/helidon/microprofile/reactive/EmittingPublisher.java\nBut that depends on protected RS with SequentialSubscriber,\nI expect David to remove SequentialSubscriber from RS implemetation in #1511 so it gets little more complicated then.", "author": "danielkec", "createdAt": "2020-03-18T16:27:43Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    // We need this flag to avoid this task is executed more than one time at the same time by ScheduledExecutorService\n+    private final AtomicBoolean running = new AtomicBoolean(false);\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }", "originalCommit": "2afe1de6f909848b6b58d0a9079522ee9ef5858f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3NjQwNQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397876405", "bodyText": "I integrated the EmittingSubscriber", "author": "jbescos", "createdAt": "2020-03-25T14:00:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3OTMyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk4OTM4MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394989380", "bodyText": "This field is never used", "author": "tomas-langer", "createdAt": "2020-03-19T12:29:14Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3NjUwNA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397876504", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:01:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk4OTM4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk4OTgwNQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394989805", "bodyText": "Please do not extends Properties in KafkaConfigProperties.\nAdd a method toProperties to that class that would return the properties required by KafkaConsumer", "author": "tomas-langer", "createdAt": "2020-03-19T12:30:02Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3NzY5MQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397877691", "bodyText": "I implemented it differently", "author": "jbescos", "createdAt": "2020-03-25T14:02:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk4OTgwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MDUyNw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394990527", "bodyText": "config is an immutable snapshot - read configuration options when creating this instance.", "author": "tomas-langer", "createdAt": "2020-03-19T12:31:20Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }\n+\n+                @Override\n+                public void cancel() {\n+                    BasicKafkaConsumer.this.close();\n+                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n+                }\n+            });\n+            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber,\n+                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0,", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MDk2OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394990969", "bodyText": "Not sure - is this about \"polling\" or \"pooling\"?\nIf this is how often we poll Kafka for changes, then the correct key should be poll-timeout and constant POLL_TIMEOUT.", "author": "tomas-langer", "createdAt": "2020-03-19T12:32:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MDUyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3ODgzMw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397878833", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:04:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MDUyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MTM1MQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394991351", "bodyText": "If you return a constant from Optional.orElseGet, then use Optional.orElse", "author": "tomas-langer", "createdAt": "2020-03-19T12:32:53Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }\n+\n+                @Override\n+                public void cancel() {\n+                    BasicKafkaConsumer.this.close();\n+                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n+                }\n+            });\n+            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber,\n+                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0,", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3ODA4Mg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397878082", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:03:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MTM1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MTY3Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394991673", "bodyText": "Method should be package private", "author": "tomas-langer", "createdAt": "2020-03-19T12:33:27Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3ODkyMg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397878922", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:04:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MTY3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MTc4MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394991780", "bodyText": "Method should be package private", "author": "tomas-langer", "createdAt": "2020-03-19T12:33:39Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }\n+\n+                @Override\n+                public void cancel() {\n+                    BasicKafkaConsumer.this.close();\n+                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n+                }\n+            });\n+            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber,\n+                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0,\n+                    config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElseGet(() -> 100L), TimeUnit.MILLISECONDS);\n+        }));\n+    }\n+\n+    /**\n+     * Blocks current thread until partitions are assigned,\n+     * since when is consumer effectively ready to receive.\n+     *\n+     * @param timeout the maximum time to wait\n+     * @param unit    the time unit of the timeout argument\n+     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n+     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n+     */\n+    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3OTE0MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397879140", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:04:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MTc4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MjQwNA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394992404", "bodyText": "If you unlock a lock, another thread may lock it before you close the consumer.\nThe taskLock should be unlocked in a finally block after consumer.close()", "author": "tomas-langer", "createdAt": "2020-03-19T12:34:51Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }\n+\n+                @Override\n+                public void cancel() {\n+                    BasicKafkaConsumer.this.close();\n+                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n+                }\n+            });\n+            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber,\n+                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0,\n+                    config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElseGet(() -> 100L), TimeUnit.MILLISECONDS);\n+        }));\n+    }\n+\n+    /**\n+     * Blocks current thread until partitions are assigned,\n+     * since when is consumer effectively ready to receive.\n+     *\n+     * @param timeout the maximum time to wait\n+     * @param unit    the time unit of the timeout argument\n+     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n+     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n+     */\n+    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n+        if (!partitionsAssignedLatch.await(timeout, unit)) {\n+            throw new TimeoutException(\"Timeout for subscription reached\");\n+        }\n+    }\n+\n+    /**\n+     * Close gracefully. Stops wakes possible blocked poll and close consumer.\n+     */\n+    @Override\n+    public void close() {\n+        // Stops pooling\n+        consumer.wakeup();\n+        // Wait that current task finishes in case it is still running\n+        taskLock.lock();\n+        taskLock.unlock();", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAyNDgzMQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r395024831", "bodyText": "In this case there is no other thread that could run later because the scheduler was stopped before forever. Still I'm thinking there is a very rare scenario that:\n\nScheduler started a task, and it doesn't reach the lock.\nShutdown is executed.\nclose() is executed and blocks. So task of point 1 is waiting.\nTask run and fails with unexpected error because the kafka connection is closed.\n\nSo a part of doing what you said, I will modify BackPressureLayer to check !scheduler.isShutdown()", "author": "jbescos", "createdAt": "2020-03-19T13:30:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MjQwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NDczNg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394994736", "bodyText": "Please replace with:\nContext.Builder contextBuilder = Context.builder()\n                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n\nContexts.context().ifPresent(contextBuilder::parent);\n        \nContexts.runInContext(contextBuilder.build(), runnable);", "author": "tomas-langer", "createdAt": "2020-03-19T12:39:03Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }\n+\n+                @Override\n+                public void cancel() {\n+                    BasicKafkaConsumer.this.close();\n+                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n+                }\n+            });\n+            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber,\n+                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0,\n+                    config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElseGet(() -> 100L), TimeUnit.MILLISECONDS);\n+        }));\n+    }\n+\n+    /**\n+     * Blocks current thread until partitions are assigned,\n+     * since when is consumer effectively ready to receive.\n+     *\n+     * @param timeout the maximum time to wait\n+     * @param unit    the time unit of the timeout argument\n+     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n+     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n+     */\n+    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n+        if (!partitionsAssignedLatch.await(timeout, unit)) {\n+            throw new TimeoutException(\"Timeout for subscription reached\");\n+        }\n+    }\n+\n+    /**\n+     * Close gracefully. Stops wakes possible blocked poll and close consumer.\n+     */\n+    @Override\n+    public void close() {\n+        // Stops pooling\n+        consumer.wakeup();\n+        // Wait that current task finishes in case it is still running\n+        taskLock.lock();\n+        taskLock.unlock();\n+        LOGGER.fine(\"Closing kafka consumer\");\n+        consumer.close();\n+    }\n+\n+    //Move to messaging incoming connector\n+    private void runInNewContext(Runnable runnable) {\n+        Context parentContext = Context.create();\n+        Context context = Context\n+                .builder()\n+                .parent(parentContext)\n+                .id(String.format(\"%s:message-%s\", parentContext.id(), UUID.randomUUID().toString()))\n+                .build();\n+        Contexts.runInContext(context, runnable);", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3OTI2MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397879260", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:04:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NDczNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NTIzNA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394995234", "bodyText": "Definitely pollTimeout not poolTimeout", "author": "tomas-langer", "createdAt": "2020-03-19T12:39:51Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaConsumer.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Basic Kafka consumer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaConsumer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaConsumer.class.getName());\n+    private static final String POOL_TIMEOUT = \"pool.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private final KafkaConfigProperties properties;\n+    private final Config config;\n+    private final Lock taskLock = new ReentrantLock();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final List<String> topicNameList;\n+    // It is not thread safe. It needs to be closed in the same thread it reads events.\n+    // We need to keep the reference here to be able to wake up from pooling when shuting down\n+    private final KafkaConsumer<K, V> consumer;\n+    private final ScheduledExecutorService scheduler;\n+\n+    /**\n+     * Kafka consumer created from {@link io.helidon.config.Config config}\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     * @param scheduler Helidon {@link java.util.concurrent.ScheduledExecutorService scheduler}\n+     */\n+    BasicKafkaConsumer(Config config, ScheduledExecutorService scheduler) {\n+        this.config = config;\n+        this.properties = new KafkaConfigProperties(config);\n+        this.topicNameList = properties.getTopicNameList();\n+        this.consumer = new KafkaConsumer<>(properties);\n+        this.scheduler = scheduler;\n+    }\n+\n+    /**\n+     * Create publisher builder.\n+     *\n+     * @return {@link org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder}\n+     */\n+    public PublisherBuilder<? extends Message<?>> createPushPublisherBuilder() {\n+        return ReactiveStreams.fromPublisher(new BasicPublisher<K, V>(subscriber -> {\n+            subscriber.onSubscribe(new Subscription() {\n+                @Override\n+                public void request(long n) {\n+                    // Pushing Kafka consumer doesn't support requests.\n+                }\n+\n+                @Override\n+                public void cancel() {\n+                    BasicKafkaConsumer.this.close();\n+                    LOGGER.log(Level.FINE, \"Subscription cancelled.\");\n+                }\n+            });\n+            consumer.subscribe(topicNameList, partitionsAssignedLatch);\n+            scheduler.scheduleAtFixedRate(new BackPressureLayer(subscriber,\n+                    config.get(POOL_TIMEOUT).asLong().asOptional().orElseGet(() -> 50L)), 0,\n+                    config.get(PERIOD_EXECUTIONS).asLong().asOptional().orElseGet(() -> 100L), TimeUnit.MILLISECONDS);\n+        }));\n+    }\n+\n+    /**\n+     * Blocks current thread until partitions are assigned,\n+     * since when is consumer effectively ready to receive.\n+     *\n+     * @param timeout the maximum time to wait\n+     * @param unit    the time unit of the timeout argument\n+     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n+     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n+     */\n+    public void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n+        if (!partitionsAssignedLatch.await(timeout, unit)) {\n+            throw new TimeoutException(\"Timeout for subscription reached\");\n+        }\n+    }\n+\n+    /**\n+     * Close gracefully. Stops wakes possible blocked poll and close consumer.\n+     */\n+    @Override\n+    public void close() {\n+        // Stops pooling\n+        consumer.wakeup();\n+        // Wait that current task finishes in case it is still running\n+        taskLock.lock();\n+        taskLock.unlock();\n+        LOGGER.fine(\"Closing kafka consumer\");\n+        consumer.close();\n+    }\n+\n+    //Move to messaging incoming connector\n+    private void runInNewContext(Runnable runnable) {\n+        Context parentContext = Context.create();\n+        Context context = Context\n+                .builder()\n+                .parent(parentContext)\n+                .id(String.format(\"%s:message-%s\", parentContext.id(), UUID.randomUUID().toString()))\n+                .build();\n+        Contexts.runInContext(context, runnable);\n+    }\n+\n+    private final class BackPressureLayer implements Runnable {\n+\n+        private final LinkedList<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n+        private final LinkedList<CompletableFuture<Void>> ackFutures = new LinkedList<>();\n+        private final Subscriber<? super KafkaMessage<K, V>> subscriber;\n+        private final long poolTimeout;\n+\n+        private BackPressureLayer(Subscriber<? super KafkaMessage<K, V>> subscriber, long poolTimeout) {\n+            this.subscriber = subscriber;\n+            this.poolTimeout = poolTimeout;\n+        }\n+\n+        @Override\n+        public void run() {\n+            try {\n+                taskLock.lock();\n+                waitForAcksAndPoll();\n+                ConsumerRecord<K, V> cr;\n+                while ((cr = backPressureBuffer.poll()) != null) {\n+                    KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr);\n+                    ackFutures.add(kafkaMessage.getAckFuture());\n+                    runInNewContext(() -> subscriber.onNext(kafkaMessage));\n+                }\n+            } finally {\n+                taskLock.unlock();\n+            }\n+        }\n+\n+        /**\n+         * Naive impl of back pressure wise lazy poll.\n+         * Wait for the last batch of records to be acknowledged before commit and another poll.\n+         */\n+        private void waitForAcksAndPoll() {\n+            if (backPressureBuffer.isEmpty()) {\n+                try {\n+                    if (!ackFutures.isEmpty()) {\n+                        LOGGER.fine(String.format(\"Wait for %s ACKs\", ackFutures.size()));\n+                        CompletableFuture.allOf(ackFutures.toArray(new CompletableFuture[0])).get();\n+                        ackFutures.clear();\n+                        consumer.commitSync();\n+                    }\n+                    consumer.poll(Duration.ofMillis(poolTimeout)).forEach(backPressureBuffer::add);", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3OTM4Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397879386", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:04:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NTIzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NTQ5NA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394995494", "bodyText": "method should be package local", "author": "tomas-langer", "createdAt": "2020-03-19T12:40:22Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.Header;\n+\n+/**\n+ * Basic Kafka producer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}.\n+ * <p>\n+ * Usage:\n+ * <pre>{@code new SimpleKafkaProducer<Long, String>(\"job-done-producer\", Config.create())\n+ *             .produce(\"Hello world!\");\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaProducer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaProducer.class.getName());\n+    private final KafkaConfigProperties properties;\n+    private final KafkaProducer<K, V> producer;\n+\n+    /**\n+     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    BasicKafkaProducer(Config config) {\n+        properties = new KafkaConfigProperties(config);\n+        producer = new KafkaProducer<>(properties);\n+    }\n+\n+    /**\n+     * Send record to all provided topics,\n+     * blocking until all records are acknowledged by broker.\n+     *\n+     * @param value Will be serialized by <b>value.serializer</b> class\n+     *              defined in {@link KafkaConfigProperties configuration}\n+     * @return Server acknowledged metadata about sent topics\n+     */\n+    public List<RecordMetadata> produce(V value) {", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NTc5OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394995798", "bodyText": "(and all other public methods in this class)", "author": "tomas-langer", "createdAt": "2020-03-19T12:40:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NTQ5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3OTUzNg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397879536", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:05:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NTQ5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NjM2Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394996367", "bodyText": "Using null is not encouraged in Helidon. Would be better to send correct defaults rather than nulls - this is very error prone", "author": "tomas-langer", "createdAt": "2020-03-19T12:41:54Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.Header;\n+\n+/**\n+ * Basic Kafka producer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}.\n+ * <p>\n+ * Usage:\n+ * <pre>{@code new SimpleKafkaProducer<Long, String>(\"job-done-producer\", Config.create())\n+ *             .produce(\"Hello world!\");\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaProducer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaProducer.class.getName());\n+    private final KafkaConfigProperties properties;\n+    private final KafkaProducer<K, V> producer;\n+\n+    /**\n+     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    BasicKafkaProducer(Config config) {\n+        properties = new KafkaConfigProperties(config);\n+        producer = new KafkaProducer<>(properties);\n+    }\n+\n+    /**\n+     * Send record to all provided topics,\n+     * blocking until all records are acknowledged by broker.\n+     *\n+     * @param value Will be serialized by <b>value.serializer</b> class\n+     *              defined in {@link KafkaConfigProperties configuration}\n+     * @return Server acknowledged metadata about sent topics\n+     */\n+    public List<RecordMetadata> produce(V value) {\n+        List<Future<RecordMetadata>> futureRecords =\n+                produceAsync(null, null, null, null, value, null);", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg3OTY4OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397879689", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:05:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NjM2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5Njk0OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394996949", "bodyText": "This is an error - throw an exception", "author": "tomas-langer", "createdAt": "2020-03-19T12:42:55Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/BasicKafkaProducer.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.Header;\n+\n+/**\n+ * Basic Kafka producer covering basic use-cases.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ * For more info about configuration see {@link KafkaConfigProperties}.\n+ * <p>\n+ * Usage:\n+ * <pre>{@code new SimpleKafkaProducer<Long, String>(\"job-done-producer\", Config.create())\n+ *             .produce(\"Hello world!\");\n+ * }</pre>\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see KafkaConfigProperties\n+ * @see io.helidon.config.Config\n+ */\n+class BasicKafkaProducer<K, V> implements Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(BasicKafkaProducer.class.getName());\n+    private final KafkaConfigProperties properties;\n+    private final KafkaProducer<K, V> producer;\n+\n+    /**\n+     * Kafka producer created from {@link io.helidon.config.Config config} under kafka-producerId,\n+     * see configuration {@link KafkaConfigProperties example}.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    BasicKafkaProducer(Config config) {\n+        properties = new KafkaConfigProperties(config);\n+        producer = new KafkaProducer<>(properties);\n+    }\n+\n+    /**\n+     * Send record to all provided topics,\n+     * blocking until all records are acknowledged by broker.\n+     *\n+     * @param value Will be serialized by <b>value.serializer</b> class\n+     *              defined in {@link KafkaConfigProperties configuration}\n+     * @return Server acknowledged metadata about sent topics\n+     */\n+    public List<RecordMetadata> produce(V value) {\n+        List<Future<RecordMetadata>> futureRecords =\n+                produceAsync(null, null, null, null, value, null);\n+        List<RecordMetadata> metadataList = new ArrayList<>(futureRecords.size());\n+\n+        for (Future<RecordMetadata> future : futureRecords) {\n+            try {\n+                metadataList.add(future.get());\n+            } catch (InterruptedException | ExecutionException e) {\n+                throw new RuntimeException(\"Failed to send topic\", e);\n+            }\n+        }\n+        return metadataList;\n+    }\n+\n+    /**\n+     * Produce asynchronously.\n+     *\n+     * @param value value to be produced\n+     * @return list of futures\n+     */\n+    public List<Future<RecordMetadata>> produceAsync(V value) {\n+        return produceAsync(null, null, null, null, value, null);\n+    }\n+\n+    /**\n+     * Send record to all provided topics, don't wait for server acknowledgement.\n+     *\n+     * @param customTopics Can be null, list of topics appended to the list from configuration,\n+     *                     record will be sent to all topics iteratively\n+     * @param partition    Can be null, if key is also null topic is sent to random partition\n+     * @param timestamp    Can be null System.currentTimeMillis() is used then\n+     * @param key          Can be null, if not, topics are grouped to partitions by key\n+     * @param value        Will be serialized by value.serializer class defined in configuration\n+     * @param headers      Can be null, custom headers for additional meta information if needed\n+     * @return Futures of server acknowledged metadata about sent topics\n+     */\n+    public List<Future<RecordMetadata>> produceAsync(List<String> customTopics,\n+                                                     Integer partition,\n+                                                     Long timestamp,\n+                                                     K key,\n+                                                     V value,\n+                                                     Iterable<Header> headers) {\n+\n+        List<String> mergedTopics = new ArrayList<>();\n+        mergedTopics.addAll(properties.getTopicNameList());\n+        mergedTopics.addAll(Optional.ofNullable(customTopics).orElse(Collections.emptyList()));\n+\n+        if (mergedTopics.isEmpty()) {\n+            LOGGER.warning(\"No topic names provided in configuration or by parameter. Nothing sent.\");", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg4MDg4Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397880887", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:06:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5Njk0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5Nzk0OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394997949", "bodyText": "This is never used.", "author": "tomas-langer", "createdAt": "2020-03-19T12:44:43Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConfigProperties.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import io.helidon.config.Config;\n+\n+/**\n+ * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config}.\n+ * Configuration format as specified in the MicroProfile Reactive Messaging\n+ * Specification https://github.com/eclipse/microprofile-reactive-messaging\n+ *\n+ * <p>\n+ * See example with YAML configuration:\n+ * <pre>{@code\n+ * mp.messaging:\n+ *   incoming:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.deserializer: org.apache.kafka.common.serialization.LongDeserializer\n+ *       value.deserializer: org.apache.kafka.common.serialization.StringDeserializer\n+ *\n+ *   outgoing:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.serializer: org.apache.kafka.common.serialization.LongSerializer\n+ *       value.serializer: org.apache.kafka.common.serialization.StringSerializer\n+ *\n+ * }</pre>\n+ * <p>\n+ *\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaConfigProperties extends Properties {\n+\n+    /**\n+     * Topic or topics delimited by commas.\n+     */\n+    static final String TOPIC_NAME = \"topic\";\n+\n+    /**\n+     * Consumer group id.\n+     */\n+    static final String GROUP_ID = \"group.id\";", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg4MDk4OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397880988", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:07:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5Nzk0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5ODk3Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394998973", "bodyText": "This is not the correct way to do this (I mentioned this in my previous review.", "author": "tomas-langer", "createdAt": "2020-03-19T12:46:33Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConfigProperties.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import io.helidon.config.Config;\n+\n+/**\n+ * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config}.\n+ * Configuration format as specified in the MicroProfile Reactive Messaging\n+ * Specification https://github.com/eclipse/microprofile-reactive-messaging\n+ *\n+ * <p>\n+ * See example with YAML configuration:\n+ * <pre>{@code\n+ * mp.messaging:\n+ *   incoming:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.deserializer: org.apache.kafka.common.serialization.LongDeserializer\n+ *       value.deserializer: org.apache.kafka.common.serialization.StringDeserializer\n+ *\n+ *   outgoing:\n+ *     test-channel:\n+ *       bootstrap.servers: localhost:9092\n+ *       topic: graph-done\n+ *       key.serializer: org.apache.kafka.common.serialization.LongSerializer\n+ *       value.serializer: org.apache.kafka.common.serialization.StringSerializer\n+ *\n+ * }</pre>\n+ * <p>\n+ *\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaConfigProperties extends Properties {\n+\n+    /**\n+     * Topic or topics delimited by commas.\n+     */\n+    static final String TOPIC_NAME = \"topic\";\n+\n+    /**\n+     * Consumer group id.\n+     */\n+    static final String GROUP_ID = \"group.id\";\n+\n+    /**\n+     * Prepare Kafka properties from Helidon {@link io.helidon.config.Config Config},\n+     * underscores in keys are translated to dots.\n+     *\n+     * @param config parent config of kafka key\n+     */\n+    KafkaConfigProperties(Config config) {\n+        config.asNodeList().get().forEach(this::addProperty);\n+    }\n+\n+    /**\n+     * Split comma separated topic names.\n+     *\n+     * @return list of topic names\n+     */\n+    public List<String> getTopicNameList() {\n+        return Arrays.stream(getProperty(TOPIC_NAME)\n+                .split(\",\"))\n+                .map(String::trim)\n+                .collect(Collectors.toList());\n+    }\n+\n+    private void addProperty(Config c) {", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg4MjgwMA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397882800", "bodyText": "Sorry I didn't notice github was hiding some comments.", "author": "jbescos", "createdAt": "2020-03-25T14:09:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5ODk3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5OTYwOQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394999609", "bodyText": "Please use Helidon ScheduledThreadPoolSupplier, that is already fully configurable using Helidon Config", "author": "tomas-langer", "createdAt": "2020-03-19T12:47:45Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Collection;\n+import java.util.LinkedList;\n+import java.util.Queue;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final String POOL_SIZE = \"kafka.connector.pool.size\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<BasicKafkaConsumer<Object, Object>> consumers = new LinkedList<>();\n+    private final Queue<BasicKafkaProducer<Object, Object>> producers = new LinkedList<>();\n+\n+    @Inject\n+    KafkaConnectorFactory(Config config) {\n+        scheduler = Executors.newScheduledThreadPool(config.get(POOL_SIZE)", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5OTY4Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r394999686", "bodyText": "Also supports context propagation.", "author": "tomas-langer", "createdAt": "2020-03-19T12:47:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5OTYwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMDM4Nw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r395000387", "bodyText": "scheduler = ScheduledThreadPoolSupplier.builder()\n                .threadNamePrefix(\"kafka-\")\n                .config(config)\n                .build()\n                .get();", "author": "tomas-langer", "createdAt": "2020-03-19T12:49:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5OTYwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg4MTQ3MQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397881471", "bodyText": "Thanks for the information, I have modified it to that", "author": "jbescos", "createdAt": "2020-03-25T14:07:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5OTYwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMDY0Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r395000643", "bodyText": "This method should be private or package local.", "author": "tomas-langer", "createdAt": "2020-03-19T12:49:43Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Collection;\n+import java.util.LinkedList;\n+import java.util.Queue;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final String POOL_SIZE = \"kafka.connector.pool.size\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<BasicKafkaConsumer<Object, Object>> consumers = new LinkedList<>();\n+    private final Queue<BasicKafkaProducer<Object, Object>> producers = new LinkedList<>();\n+\n+    @Inject\n+    KafkaConnectorFactory(Config config) {\n+        scheduler = Executors.newScheduledThreadPool(config.get(POOL_SIZE)\n+                .asInt().asOptional().orElseGet(() -> 10));\n+    }\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg4Mjk0OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397882948", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:09:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMDY0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMTEwMg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r395001102", "bodyText": "Do not use info log level so much. If you want to log an info message, just log a single one in this method.", "author": "tomas-langer", "createdAt": "2020-03-19T12:50:35Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Collection;\n+import java.util.LinkedList;\n+import java.util.Queue;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final String POOL_SIZE = \"kafka.connector.pool.size\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<BasicKafkaConsumer<Object, Object>> consumers = new LinkedList<>();\n+    private final Queue<BasicKafkaProducer<Object, Object>> producers = new LinkedList<>();\n+\n+    @Inject\n+    KafkaConnectorFactory(Config config) {\n+        scheduler = Executors.newScheduledThreadPool(config.get(POOL_SIZE)\n+                .asInt().asOptional().orElseGet(() -> 10));\n+    }\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        LOGGER.info(\"Terminating KafkaConnectorFactory...\");\n+        // Stops the scheduler first to make sure no new task will be triggered meanwhile consumers are closing\n+        scheduler.shutdown();\n+        BasicKafkaConsumer<Object, Object> consumer;\n+        while ((consumer = consumers.poll()) != null) {\n+            consumer.close();\n+        }\n+        BasicKafkaProducer<Object, Object> producer;\n+        while ((producer = producers.poll()) != null) {\n+            producer.close();\n+        }\n+        LOGGER.info(\"KafkaConnectorFactory terminated successfuly\");", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMTI1Mg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r395001252", "bodyText": "Method should be package local.", "author": "tomas-langer", "createdAt": "2020-03-19T12:50:51Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Collection;\n+import java.util.LinkedList;\n+import java.util.Queue;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final String POOL_SIZE = \"kafka.connector.pool.size\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<BasicKafkaConsumer<Object, Object>> consumers = new LinkedList<>();\n+    private final Queue<BasicKafkaProducer<Object, Object>> producers = new LinkedList<>();\n+\n+    @Inject\n+    KafkaConnectorFactory(Config config) {\n+        scheduler = Executors.newScheduledThreadPool(config.get(POOL_SIZE)\n+                .asInt().asOptional().orElseGet(() -> 10));\n+    }\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        LOGGER.info(\"Terminating KafkaConnectorFactory...\");\n+        // Stops the scheduler first to make sure no new task will be triggered meanwhile consumers are closing\n+        scheduler.shutdown();\n+        BasicKafkaConsumer<Object, Object> consumer;\n+        while ((consumer = consumers.poll()) != null) {\n+            consumer.close();\n+        }\n+        BasicKafkaProducer<Object, Object> producer;\n+        while ((producer = producers.poll()) != null) {\n+            producer.close();\n+        }\n+        LOGGER.info(\"KafkaConnectorFactory terminated successfuly\");\n+    }\n+\n+    public Collection<BasicKafkaConsumer<Object, Object>> getConsumers() {", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg4MzM1Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397883353", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:10:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMTI1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMTcwOA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r395001708", "bodyText": "You should close the producer in onError as well, as onComplete may never be called.", "author": "tomas-langer", "createdAt": "2020-03-19T12:51:42Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnectorFactory.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Collection;\n+import java.util.LinkedList;\n+import java.util.Queue;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Implementation of Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnectorFactory.CONNECTOR_NAME)\n+class KafkaConnectorFactory implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final String POOL_SIZE = \"kafka.connector.pool.size\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnectorFactory.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<BasicKafkaConsumer<Object, Object>> consumers = new LinkedList<>();\n+    private final Queue<BasicKafkaProducer<Object, Object>> producers = new LinkedList<>();\n+\n+    @Inject\n+    KafkaConnectorFactory(Config config) {\n+        scheduler = Executors.newScheduledThreadPool(config.get(POOL_SIZE)\n+                .asInt().asOptional().orElseGet(() -> 10));\n+    }\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {\n+        LOGGER.info(\"Terminating KafkaConnectorFactory...\");\n+        // Stops the scheduler first to make sure no new task will be triggered meanwhile consumers are closing\n+        scheduler.shutdown();\n+        BasicKafkaConsumer<Object, Object> consumer;\n+        while ((consumer = consumers.poll()) != null) {\n+            consumer.close();\n+        }\n+        BasicKafkaProducer<Object, Object> producer;\n+        while ((producer = producers.poll()) != null) {\n+            producer.close();\n+        }\n+        LOGGER.info(\"KafkaConnectorFactory terminated successfuly\");\n+    }\n+\n+    public Collection<BasicKafkaConsumer<Object, Object>> getConsumers() {\n+        return consumers;\n+    }\n+\n+    @Override\n+    public PublisherBuilder<? extends Message<?>> getPublisherBuilder(org.eclipse.microprofile.config.Config config) {\n+        Config helidonConfig = (Config) config;\n+        BasicKafkaConsumer<Object, Object> basicKafkaConsumer = new BasicKafkaConsumer<>(helidonConfig, scheduler);\n+        consumers.add(basicKafkaConsumer);\n+        return basicKafkaConsumer.createPushPublisherBuilder();\n+    }\n+\n+    @Override\n+    public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(org.eclipse.microprofile.config.Config config) {\n+        Config helidonConfig = (Config) config;\n+        BasicKafkaProducer<Object, Object> basicKafkaProducer = new BasicKafkaProducer<>(helidonConfig);\n+        producers.add(basicKafkaProducer);\n+        return ReactiveStreams.fromSubscriber(new Subscriber<Message<?>>() {\n+\n+            @Override\n+            public void onSubscribe(Subscription s) {\n+                s.request(Long.MAX_VALUE);\n+            }\n+\n+            @Override\n+            public void onNext(Message<?> message) {\n+                LOGGER.fine(\"On next received \" + message.getPayload());\n+                basicKafkaProducer.produce(message.getPayload());\n+                message.ack();\n+            }\n+\n+            @Override\n+            public void onError(Throwable t) {", "originalCommit": "14b719af384ec92656a7de6608546824a533d797", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg4MzU2Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r397883566", "bodyText": "Done", "author": "jbescos", "createdAt": "2020-03-25T14:10:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMTcwOA=="}], "type": "inlineReview"}, {"oid": "e260c32d99f3f0078ee691ebf7063b02e7aba188", "url": "https://github.com/oracle/helidon/commit/e260c32d99f3f0078ee691ebf7063b02e7aba188", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-25T11:18:43Z", "type": "forcePushed"}, {"oid": "c8a21d8159f5e1153fe8dbff0db36ac3b665a7e0", "url": "https://github.com/oracle/helidon/commit/c8a21d8159f5e1153fe8dbff0db36ac3b665a7e0", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-25T12:07:54Z", "type": "forcePushed"}, {"oid": "07a11a58331466f830337cf71cd033aec1022418", "url": "https://github.com/oracle/helidon/commit/07a11a58331466f830337cf71cd033aec1022418", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-25T13:59:28Z", "type": "forcePushed"}, {"oid": "c63e74cbc47b8f480e92e3d0c804576d1b144061", "url": "https://github.com/oracle/helidon/commit/c63e74cbc47b8f480e92e3d0c804576d1b144061", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-26T11:51:26Z", "type": "forcePushed"}, {"oid": "10612d66d9b6094133053f2f2778682db3616ef3", "url": "https://github.com/oracle/helidon/commit/10612d66d9b6094133053f2f2778682db3616ef3", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-27T23:33:51Z", "type": "forcePushed"}, {"oid": "aad54c4aefca8c3c7235adc20db2515630a88681", "url": "https://github.com/oracle/helidon/commit/aad54c4aefca8c3c7235adc20db2515630a88681", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-30T06:42:11Z", "type": "forcePushed"}, {"oid": "ede74fc9dedaffdf9b79684633b7b33e341ff5db", "url": "https://github.com/oracle/helidon/commit/ede74fc9dedaffdf9b79684633b7b33e341ff5db", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-30T07:05:03Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDExMjY2Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r400112666", "bodyText": "NPE rethrow was intentional, as its reserved as a signal for upstream (\u00a72.13) and this is not the place we are able to solve kafka client errors. On the other hand I am more and more convinced we should remove abstraction layer between BasicKafkaConsumer, BasicKafkaPublisher,  EmittingPublisher and create one specialized KafkaConsumingPublisher, which can be used in both Helidon MP and SE", "author": "danielkec", "createdAt": "2020-03-30T11:17:04Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.logging.Logger;\n+\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Emitting reactive streams publisher to be used by {@code ReactiveStreams.fromPublisher},\n+ * should be deprecated in favor of {@code org.eclipse.microprofile.reactive.messaging.Emitter}\n+ * in the future version of messaging.\n+ *\n+ * @param <T> type of emitted item\n+ */\n+class EmittingPublisher<T> implements Publisher<T> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(EmittingPublisher.class.getName());\n+    private Subscriber<? super T> subscriber;\n+    private final AtomicReference<State> state = new AtomicReference<>(State.NOT_REQUESTED_YET);\n+    private final AtomicLong requested = new AtomicLong();\n+    private final AtomicBoolean terminated = new AtomicBoolean();\n+    private final Optional<Callback<Long>> requestsCallback;\n+\n+    protected EmittingPublisher(Optional<Callback<Long>> requestsCallback) {\n+        this.requestsCallback = requestsCallback;\n+    }\n+\n+    @Override\n+    public void subscribe(Subscriber<? super T> subscriber) {\n+        Objects.requireNonNull(subscriber, \"subscriber is null\");\n+        this.subscriber = subscriber;\n+        subscriber.onSubscribe(new Subscription() {\n+            @Override\n+            public void request(final long n) {\n+                if (n < 1) {\n+                    fail(new IllegalArgumentException(\"Rule \u00a73.9 violated: non-positive request amount is forbidden\"));\n+                }\n+                LOGGER.fine(String.format(\"Request %s events\", n));\n+                requested.updateAndGet(r -> Long.MAX_VALUE - r > n ? n + r : Long.MAX_VALUE);\n+                state.compareAndSet(State.NOT_REQUESTED_YET, State.READY_TO_EMIT);\n+                requestsCallback.ifPresent(callback -> callback.nofity(n));\n+            }\n+\n+            @Override\n+            public void cancel() {\n+                LOGGER.fine(\"Subscription cancelled\");\n+                state.compareAndSet(State.NOT_REQUESTED_YET, State.CANCELLED);\n+                state.compareAndSet(State.READY_TO_EMIT, State.CANCELLED);\n+            }\n+\n+        });\n+    }\n+\n+    /**\n+     * Properly fail the stream, set publisher to cancelled state and send {@code onError} signal downstream.\n+     * Signal {@code onError} is sent only once, any other call to this method is no-op.\n+     *\n+     * @param throwable Sent as {@code onError} signal\n+     */\n+    void fail(Throwable throwable) {\n+        if (!terminated.getAndSet(true) && subscriber != null) {\n+            state.compareAndSet(State.NOT_REQUESTED_YET, State.CANCELLED);\n+            state.compareAndSet(State.READY_TO_EMIT, State.CANCELLED);\n+            this.subscriber.onError(throwable);\n+        }\n+    }\n+\n+    /**\n+     * Properly complete the stream, set publisher to completed state and send {@code onComplete} signal downstream.\n+     * Signal {@code onComplete} is sent only once, any other call to this method is no-op.\n+     */\n+    void complete() {\n+        if (!terminated.getAndSet(true) && subscriber != null) {\n+            state.compareAndSet(State.NOT_REQUESTED_YET, State.COMPLETED);\n+            state.compareAndSet(State.READY_TO_EMIT, State.COMPLETED);\n+            this.subscriber.onComplete();\n+        }\n+    }\n+\n+    /**\n+     * Emit one item to the stream, if there is enough requested, item is signaled to downstream as {@code onNext}\n+     * and method returns true. If there is requested less than 1, nothing is sent and method returns false.\n+     *\n+     * @param item to be sent downstream\n+     * @return true if item successfully sent\n+     * @throws java.lang.IllegalStateException if publisher is cancelled\n+     */\n+    boolean emit(T item) {\n+        return this.state.get().emit(this, item);\n+    }\n+\n+    /**\n+     * Check if publisher is in terminal state CANCELLED.\n+     *\n+     * @return true if so\n+     */\n+    boolean isCancelled() {\n+        return this.state.get() == State.CANCELLED;\n+    }\n+\n+    /**\n+     * Check if publisher is in terminal state COMPLETED.\n+     *\n+     * @return true if so\n+     */\n+    boolean isCompleted() {\n+        return this.state.get() == State.COMPLETED;\n+    }\n+\n+    private enum State {\n+        NOT_REQUESTED_YET {\n+            @Override\n+            <T> boolean emit(EmittingPublisher<T> publisher, T item) {\n+                return false;\n+            }\n+        },\n+        READY_TO_EMIT {\n+            @Override\n+            <T> boolean emit(EmittingPublisher<T> publisher, T item) {\n+                if (publisher.requested.getAndDecrement() < 1) {\n+                    return false;\n+                }\n+                try {\n+                    publisher.subscriber.onNext(item);\n+                    return true;\n+                } catch (Throwable t) {\n+                    publisher.fail(t);\n+                    return false;\n+                }", "originalCommit": "dcf791bbc62232b546d3e492a4e9bfb127a7a7a3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "4f234782d06edb4522b570f6f3c42e6d90e41866", "url": "https://github.com/oracle/helidon/commit/4f234782d06edb4522b570f6f3c42e6d90e41866", "message": "Refactoring and making public the KafkaSubscriber and KafkaPublisher\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-30T17:43:40Z", "type": "forcePushed"}, {"oid": "1a55afff628b5ccfb12aac8637b96b8550f77569", "url": "https://github.com/oracle/helidon/commit/1a55afff628b5ccfb12aac8637b96b8550f77569", "message": "Refactoring and making public the KafkaSubscriber and KafkaPublisher\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-30T17:53:47Z", "type": "forcePushed"}, {"oid": "47cc1f1145e35d142b11fc8e6ea9d46ccebd3bd5", "url": "https://github.com/oracle/helidon/commit/47cc1f1145e35d142b11fc8e6ea9d46ccebd3bd5", "message": "Refactoring and making public the KafkaSubscriber and KafkaPublisher\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-30T18:45:28Z", "type": "forcePushed"}, {"oid": "434d47cdac68eedd818d8e4e74d60b553343d6f2", "url": "https://github.com/oracle/helidon/commit/434d47cdac68eedd818d8e4e74d60b553343d6f2", "message": "Refactoring and making public the KafkaSubscriber and KafkaPublisher\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-31T06:24:43Z", "type": "forcePushed"}, {"oid": "7fc5933aa86e4078b94479a1f6138996f560a714", "url": "https://github.com/oracle/helidon/commit/7fc5933aa86e4078b94479a1f6138996f560a714", "message": "Refactoring and making public the KafkaSubscriber and KafkaPublisher\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-31T07:16:57Z", "type": "forcePushed"}, {"oid": "e594ef5c5ada615d6aa957e7109f10fdab5caf09", "url": "https://github.com/oracle/helidon/commit/e594ef5c5ada615d6aa957e7109f10fdab5caf09", "message": "Refactoring and making public the KafkaSubscriber and KafkaPublisher\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-03-31T10:39:01Z", "type": "forcePushed"}, {"oid": "f2e4290553f1092f9033a01b928e13843519e282", "url": "https://github.com/oracle/helidon/commit/f2e4290553f1092f9033a01b928e13843519e282", "message": "Refactoring and making public the KafkaSubscriber and KafkaPublisher\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-02T06:23:31Z", "type": "forcePushed"}, {"oid": "82af0ca033b1cd7c19efff465bcf7c7f8908b5f9", "url": "https://github.com/oracle/helidon/commit/82af0ca033b1cd7c19efff465bcf7c7f8908b5f9", "message": "Refactoring\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-02T07:28:03Z", "type": "forcePushed"}, {"oid": "716fd4729baca6548038a27ada9f6dbe6bd9acac", "url": "https://github.com/oracle/helidon/commit/716fd4729baca6548038a27ada9f6dbe6bd9acac", "message": "To trigger the build\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-02T08:14:58Z", "type": "forcePushed"}, {"oid": "89106e8b66fc27f890e8515fc484f72082c1a23b", "url": "https://github.com/oracle/helidon/commit/89106e8b66fc27f890e8515fc484f72082c1a23b", "message": "TCK in KafkaConnector\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-03T06:44:17Z", "type": "forcePushed"}, {"oid": "3aac54995e0518abe8d0da8c50bc26fb69ebc591", "url": "https://github.com/oracle/helidon/commit/3aac54995e0518abe8d0da8c50bc26fb69ebc591", "message": "TCK in KafkaConnector\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-03T06:45:30Z", "type": "forcePushed"}, {"oid": "cd91f520f05970477ea967c68ef7119789ccf874", "url": "https://github.com/oracle/helidon/commit/cd91f520f05970477ea967c68ef7119789ccf874", "message": "TCK in KafkaConnector\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-03T09:46:39Z", "type": "forcePushed"}, {"oid": "fd9112ad67622206ce02eaeec03751c1199671e9", "url": "https://github.com/oracle/helidon/commit/fd9112ad67622206ce02eaeec03751c1199671e9", "message": "TCK in KafkaConnector\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-03T11:04:04Z", "type": "forcePushed"}, {"oid": "22dc4e2f129a748097971fa3e1cf8999ba083ce4", "url": "https://github.com/oracle/helidon/commit/22dc4e2f129a748097971fa3e1cf8999ba083ce4", "message": "TCK in KafkaConnector\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-03T12:33:05Z", "type": "forcePushed"}, {"oid": "e47932434f60abdfc1a9bdc6ee2a786bb27b742f", "url": "https://github.com/oracle/helidon/commit/e47932434f60abdfc1a9bdc6ee2a786bb27b742f", "message": "Refactorings and ACK fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-09T16:38:27Z", "type": "forcePushed"}, {"oid": "f634ae36ef5b9bd9df04b1349f7ef424f8949c65", "url": "https://github.com/oracle/helidon/commit/f634ae36ef5b9bd9df04b1349f7ef424f8949c65", "message": "Refactorings and ACK fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-09T16:40:56Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ3MjYyMA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r406472620", "bodyText": "Spotbugs is \"picky\", maxEvents doesn't have to be volatile at all. Kafka publisher should be unbounded in runtime so maxEvents shoudn't be needed. It can be tested like this:\npublic Publisher<KafkaMessage<String, Long>> createPublisher(long elements) {\n...\n        return ReactiveStreams.fromPublisher(\n                KafkaPublisher.build(Executors.newScheduledThreadPool(2), \n                        kafkaConsumer, \n                        Arrays.asList(TEST_TOPIC_1), \n                        1L, \n                        POLL_TIMEOUT, \n                        true))\n                .limit(elements);", "author": "danielkec", "createdAt": "2020-04-09T20:56:46Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java", "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Optional;\n+import java.util.Queue;\n+import java.util.UUID;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+/**\n+ * This is an implementation of {@link org.reactivestreams.Publisher} that read events from\n+ * Kafka and push them downstream to one subscriber.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaPublisher.class.getName());\n+    private static final String POLL_TIMEOUT = \"poll.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private static final String MAX_EVENTS = \"max.events\";\n+    private static final String ENABLE_AUTOCOMMIT = \"enable.auto.commit\";\n+    private static final String ACK_TIMEOUT = \"ack.timeout.millis\";\n+    private static final String LIMIT_NO_ACK = \"limit.no.ack\";\n+    private final Lock taskLock = new ReentrantLock();\n+    private final Queue<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n+    private final Map<TopicPartition, List<KafkaMessage<K, V>>> pendingCommits = new HashMap<>();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final ScheduledExecutorService scheduler;\n+    private final Consumer<K, V> kafkaConsumer;\n+    private final AtomicLong requests = new AtomicLong();\n+    private final EmittingPublisher<KafkaMessage<K, V>> emiter =\n+            new EmittingPublisher<>(requested -> requests.addAndGet(requested));\n+    private final List<String> topics;\n+    private final long periodExecutions;\n+    private final long pollTimeout;\n+    private final boolean autoCommit;\n+    private final long ackTimeout;\n+    private final int limitNoAck;\n+    private volatile long maxEvents;\n+\n+    private KafkaPublisher(ScheduledExecutorService scheduler, Consumer<K, V> kafkaConsumer,\n+            List<String> topics, long pollTimeout, long periodExecutions, long maxEvents,\n+            boolean autoCommit, long ackTimeout, int limitNoAck) {\n+        this.scheduler = scheduler;\n+        this.kafkaConsumer = kafkaConsumer;\n+        this.topics = topics;\n+        this.periodExecutions = periodExecutions;\n+        this.pollTimeout = pollTimeout;\n+        this.maxEvents = maxEvents;\n+        this.autoCommit = autoCommit;\n+        this.ackTimeout = ackTimeout;\n+        this.limitNoAck = limitNoAck;\n+    }\n+\n+    /**\n+     * Starts to consume events from Kafka to send them downstream till\n+     * {@link io.helidon.microprofile.connectors.kafka.KafkaPublisher#close()} is invoked.\n+     * This execution runs in one thread that is triggered by the scheduler.\n+     */\n+    private void execute() {\n+        kafkaConsumer.subscribe(topics, partitionsAssignedLatch);\n+        // This thread reads from Kafka topics and push in kafkaBufferedEvents\n+        scheduler.scheduleAtFixedRate(() -> {\n+            try {\n+                // Need to lock to avoid onClose() is executed meanwhile task is running\n+                taskLock.lock();\n+                if (!scheduler.isShutdown() && !emiter.isTerminated()) {\n+                    int currentNoAck = currentNoAck();\n+                    if (currentNoAck < limitNoAck) {\n+                        if (backPressureBuffer.isEmpty()) {\n+                            try {\n+                                kafkaConsumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n+                            } catch (WakeupException e) {\n+                                LOGGER.fine(() -> \"It was requested to stop polling from channel\");\n+                            }\n+                        } else {\n+                            long totalToEmit = requests.get();\n+                            // Avoid index out bound exceptions\n+                            long eventsToEmit = Math.min(totalToEmit, backPressureBuffer.size());\n+                            for (long i = 0; i < eventsToEmit; i++) {\n+                                if (maxEvents == 0) {\n+                                    emiter.complete();\n+                                    break;\n+                                }\n+                                ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n+                                KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr, autoCommit, ackTimeout);\n+                                if (!autoCommit) {\n+                                    TopicPartition key = new TopicPartition(kafkaMessage.getPayload().topic(),\n+                                            kafkaMessage.getPayload().partition());\n+                                    pendingCommits.computeIfAbsent(key, k -> new LinkedList<>()).add(kafkaMessage);\n+                                }\n+                                // Note that next execution will reach the user code inside @Incoming method.\n+                                // By spec, onNext MUST NOT block the Publisher, otherwise it will make problems.\n+                                runInNewContext(() ->  emiter.emit(kafkaMessage));\n+                                requests.decrementAndGet();\n+                                maxEvents--;", "originalCommit": "f634ae36ef5b9bd9df04b1349f7ef424f8949c65", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ4MzA3OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r406483079", "bodyText": "Wakeup should normally get you from polling loop, but in our case its inside the scheduled runnable, It can be better to stop scheduler before wakeup and in the catch block jump out:\n} catch (WakeupException e) {\n  LOGGER.fine(() -> \"It was requested to stop polling from channel\");\n  return;\n}", "author": "danielkec", "createdAt": "2020-04-09T21:18:14Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java", "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Optional;\n+import java.util.Queue;\n+import java.util.UUID;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+/**\n+ * This is an implementation of {@link org.reactivestreams.Publisher} that read events from\n+ * Kafka and push them downstream to one subscriber.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaPublisher.class.getName());\n+    private static final String POLL_TIMEOUT = \"poll.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private static final String MAX_EVENTS = \"max.events\";\n+    private static final String ENABLE_AUTOCOMMIT = \"enable.auto.commit\";\n+    private static final String ACK_TIMEOUT = \"ack.timeout.millis\";\n+    private static final String LIMIT_NO_ACK = \"limit.no.ack\";\n+    private final Lock taskLock = new ReentrantLock();\n+    private final Queue<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n+    private final Map<TopicPartition, List<KafkaMessage<K, V>>> pendingCommits = new HashMap<>();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final ScheduledExecutorService scheduler;\n+    private final Consumer<K, V> kafkaConsumer;\n+    private final AtomicLong requests = new AtomicLong();\n+    private final EmittingPublisher<KafkaMessage<K, V>> emiter =\n+            new EmittingPublisher<>(requested -> requests.addAndGet(requested));\n+    private final List<String> topics;\n+    private final long periodExecutions;\n+    private final long pollTimeout;\n+    private final boolean autoCommit;\n+    private final long ackTimeout;\n+    private final int limitNoAck;\n+    private volatile long maxEvents;\n+\n+    private KafkaPublisher(ScheduledExecutorService scheduler, Consumer<K, V> kafkaConsumer,\n+            List<String> topics, long pollTimeout, long periodExecutions, long maxEvents,\n+            boolean autoCommit, long ackTimeout, int limitNoAck) {\n+        this.scheduler = scheduler;\n+        this.kafkaConsumer = kafkaConsumer;\n+        this.topics = topics;\n+        this.periodExecutions = periodExecutions;\n+        this.pollTimeout = pollTimeout;\n+        this.maxEvents = maxEvents;\n+        this.autoCommit = autoCommit;\n+        this.ackTimeout = ackTimeout;\n+        this.limitNoAck = limitNoAck;\n+    }\n+\n+    /**\n+     * Starts to consume events from Kafka to send them downstream till\n+     * {@link io.helidon.microprofile.connectors.kafka.KafkaPublisher#close()} is invoked.\n+     * This execution runs in one thread that is triggered by the scheduler.\n+     */\n+    private void execute() {\n+        kafkaConsumer.subscribe(topics, partitionsAssignedLatch);\n+        // This thread reads from Kafka topics and push in kafkaBufferedEvents\n+        scheduler.scheduleAtFixedRate(() -> {\n+            try {\n+                // Need to lock to avoid onClose() is executed meanwhile task is running\n+                taskLock.lock();\n+                if (!scheduler.isShutdown() && !emiter.isTerminated()) {\n+                    int currentNoAck = currentNoAck();\n+                    if (currentNoAck < limitNoAck) {\n+                        if (backPressureBuffer.isEmpty()) {\n+                            try {\n+                                kafkaConsumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n+                            } catch (WakeupException e) {\n+                                LOGGER.fine(() -> \"It was requested to stop polling from channel\");\n+                            }\n+                        } else {\n+                            long totalToEmit = requests.get();\n+                            // Avoid index out bound exceptions\n+                            long eventsToEmit = Math.min(totalToEmit, backPressureBuffer.size());\n+                            for (long i = 0; i < eventsToEmit; i++) {\n+                                if (maxEvents == 0) {\n+                                    emiter.complete();\n+                                    break;\n+                                }\n+                                ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n+                                KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr, autoCommit, ackTimeout);\n+                                if (!autoCommit) {\n+                                    TopicPartition key = new TopicPartition(kafkaMessage.getPayload().topic(),\n+                                            kafkaMessage.getPayload().partition());\n+                                    pendingCommits.computeIfAbsent(key, k -> new LinkedList<>()).add(kafkaMessage);\n+                                }\n+                                // Note that next execution will reach the user code inside @Incoming method.\n+                                // By spec, onNext MUST NOT block the Publisher, otherwise it will make problems.\n+                                runInNewContext(() ->  emiter.emit(kafkaMessage));\n+                                requests.decrementAndGet();\n+                                maxEvents--;\n+                            }\n+                        }\n+                    } else {\n+                        throw new IllegalStateException(\n+                                String.format(\"Current pending %s acks has overflown the limit of %s \",\n+                                        currentNoAck, limitNoAck));\n+                    }\n+                }\n+                // Commit ACKs\n+                processACK();\n+            } catch (Exception e) {\n+                LOGGER.log(Level.SEVERE, \"KafkaPublisher failed\", e);\n+                emiter.fail(e);\n+            } finally {\n+                taskLock.unlock();\n+            }\n+        }, 0, periodExecutions, TimeUnit.MILLISECONDS);\n+    }\n+\n+    private int currentNoAck() {\n+        return pendingCommits.values().stream().map(list -> list.size()).reduce((a, b) -> a + b).orElse(0);\n+    }\n+\n+    /**\n+     * Process the ACKs only if enable.auto.commit is false.\n+     * This will search events that are ACK and it will commit them to Kafka.\n+     * What ever the commit was success of not, it will be notified to the message.\n+     */\n+    private void processACK() {\n+        Map<TopicPartition, OffsetAndMetadata> offsets = new LinkedHashMap<>();\n+        List<KafkaMessage<K, V>> notifications = new LinkedList<>();\n+        // Commit highest offset + 1 of each partition that was ACK, and remove from pending\n+        for (Entry<TopicPartition, List<KafkaMessage<K, V>>> entry : pendingCommits.entrySet()) {\n+            // No need to sort it, offsets are consumed in order\n+            List<KafkaMessage<K, V>> byPartition = entry.getValue();\n+            Iterator<KafkaMessage<K, V>> iterator = byPartition.iterator();\n+            KafkaMessage<K, V> highest = null;\n+            while (iterator.hasNext()) {\n+                KafkaMessage<K, V> element = iterator.next();\n+                if (element.isAck()) {\n+                    notifications.add(element);\n+                    highest = element;\n+                    iterator.remove();\n+                } else {\n+                    break;\n+                }\n+            }\n+            if (highest != null) {\n+                OffsetAndMetadata offset = new OffsetAndMetadata(highest.getPayload().offset() + 1);\n+                LOGGER.fine(() -> String.format(\"Will commit %s %s\", entry.getKey(), offset));\n+                offsets.put(entry.getKey(), offset);\n+            }\n+        }\n+        if (!notifications.isEmpty()) {\n+            Optional<RuntimeException> exception = commitInKafka(offsets);\n+            notifications.stream().forEach(message -> {\n+                exception.ifPresent(ex -> message.exception(ex));\n+                message.wakeUp();\n+            });\n+        }\n+    }\n+\n+    private Optional<RuntimeException> commitInKafka(Map<TopicPartition, OffsetAndMetadata> offsets) {\n+        LOGGER.fine(() -> String.format(\"%s events to commit: \", offsets.size()));\n+        LOGGER.fine(() -> String.format(\"%s\", offsets));\n+        try {\n+            kafkaConsumer.commitSync(offsets);\n+            LOGGER.fine(() -> \"The commit was successful\");\n+            return Optional.empty();\n+        } catch (RuntimeException e) {\n+            LOGGER.log(Level.SEVERE, \"Unable to commit in Kafka \" + offsets, e);\n+            return Optional.of(e);\n+        }\n+    }\n+\n+    /**\n+     * Closes the connections to Kafka and stops to process new events.\n+     */\n+    @Override\n+    public void close() {\n+        // Stops pooling\n+        kafkaConsumer.wakeup();", "originalCommit": "f634ae36ef5b9bd9df04b1349f7ef424f8949c65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzk5MTQ3Mg==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r407991472", "bodyText": "You already stopped it in connector, sorry", "author": "danielkec", "createdAt": "2020-04-14T09:22:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ4MzA3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ4NDE1MQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r406484151", "bodyText": "Can be swallowed by used code, would be great to log it", "author": "danielkec", "createdAt": "2020-04-09T21:20:31Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaPublisher.java", "diffHunk": "@@ -0,0 +1,311 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Optional;\n+import java.util.Queue;\n+import java.util.UUID;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+/**\n+ * This is an implementation of {@link org.reactivestreams.Publisher} that read events from\n+ * Kafka and push them downstream to one subscriber.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaPublisher.class.getName());\n+    private static final String POLL_TIMEOUT = \"poll.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private static final String MAX_EVENTS = \"max.events\";\n+    private static final String ENABLE_AUTOCOMMIT = \"enable.auto.commit\";\n+    private static final String ACK_TIMEOUT = \"ack.timeout.millis\";\n+    private static final String LIMIT_NO_ACK = \"limit.no.ack\";\n+    private final Lock taskLock = new ReentrantLock();\n+    private final Queue<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n+    private final Map<TopicPartition, List<KafkaMessage<K, V>>> pendingCommits = new HashMap<>();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final ScheduledExecutorService scheduler;\n+    private final Consumer<K, V> kafkaConsumer;\n+    private final AtomicLong requests = new AtomicLong();\n+    private final EmittingPublisher<KafkaMessage<K, V>> emiter =\n+            new EmittingPublisher<>(requested -> requests.addAndGet(requested));\n+    private final List<String> topics;\n+    private final long periodExecutions;\n+    private final long pollTimeout;\n+    private final boolean autoCommit;\n+    private final long ackTimeout;\n+    private final int limitNoAck;\n+    private volatile long maxEvents;\n+\n+    private KafkaPublisher(ScheduledExecutorService scheduler, Consumer<K, V> kafkaConsumer,\n+            List<String> topics, long pollTimeout, long periodExecutions, long maxEvents,\n+            boolean autoCommit, long ackTimeout, int limitNoAck) {\n+        this.scheduler = scheduler;\n+        this.kafkaConsumer = kafkaConsumer;\n+        this.topics = topics;\n+        this.periodExecutions = periodExecutions;\n+        this.pollTimeout = pollTimeout;\n+        this.maxEvents = maxEvents;\n+        this.autoCommit = autoCommit;\n+        this.ackTimeout = ackTimeout;\n+        this.limitNoAck = limitNoAck;\n+    }\n+\n+    /**\n+     * Starts to consume events from Kafka to send them downstream till\n+     * {@link io.helidon.microprofile.connectors.kafka.KafkaPublisher#close()} is invoked.\n+     * This execution runs in one thread that is triggered by the scheduler.\n+     */\n+    private void execute() {\n+        kafkaConsumer.subscribe(topics, partitionsAssignedLatch);\n+        // This thread reads from Kafka topics and push in kafkaBufferedEvents\n+        scheduler.scheduleAtFixedRate(() -> {\n+            try {\n+                // Need to lock to avoid onClose() is executed meanwhile task is running\n+                taskLock.lock();\n+                if (!scheduler.isShutdown() && !emiter.isTerminated()) {\n+                    int currentNoAck = currentNoAck();\n+                    if (currentNoAck < limitNoAck) {\n+                        if (backPressureBuffer.isEmpty()) {\n+                            try {\n+                                kafkaConsumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n+                            } catch (WakeupException e) {\n+                                LOGGER.fine(() -> \"It was requested to stop polling from channel\");\n+                            }\n+                        } else {\n+                            long totalToEmit = requests.get();\n+                            // Avoid index out bound exceptions\n+                            long eventsToEmit = Math.min(totalToEmit, backPressureBuffer.size());\n+                            for (long i = 0; i < eventsToEmit; i++) {\n+                                if (maxEvents == 0) {\n+                                    emiter.complete();\n+                                    break;\n+                                }\n+                                ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n+                                KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr, autoCommit, ackTimeout);\n+                                if (!autoCommit) {\n+                                    TopicPartition key = new TopicPartition(kafkaMessage.getPayload().topic(),\n+                                            kafkaMessage.getPayload().partition());\n+                                    pendingCommits.computeIfAbsent(key, k -> new LinkedList<>()).add(kafkaMessage);\n+                                }\n+                                // Note that next execution will reach the user code inside @Incoming method.\n+                                // By spec, onNext MUST NOT block the Publisher, otherwise it will make problems.\n+                                runInNewContext(() ->  emiter.emit(kafkaMessage));\n+                                requests.decrementAndGet();\n+                                maxEvents--;\n+                            }\n+                        }\n+                    } else {\n+                        throw new IllegalStateException(\n+                                String.format(\"Current pending %s acks has overflown the limit of %s \",\n+                                        currentNoAck, limitNoAck));\n+                    }\n+                }\n+                // Commit ACKs\n+                processACK();\n+            } catch (Exception e) {\n+                LOGGER.log(Level.SEVERE, \"KafkaPublisher failed\", e);\n+                emiter.fail(e);\n+            } finally {\n+                taskLock.unlock();\n+            }\n+        }, 0, periodExecutions, TimeUnit.MILLISECONDS);\n+    }\n+\n+    private int currentNoAck() {\n+        return pendingCommits.values().stream().map(list -> list.size()).reduce((a, b) -> a + b).orElse(0);\n+    }\n+\n+    /**\n+     * Process the ACKs only if enable.auto.commit is false.\n+     * This will search events that are ACK and it will commit them to Kafka.\n+     * What ever the commit was success of not, it will be notified to the message.\n+     */\n+    private void processACK() {\n+        Map<TopicPartition, OffsetAndMetadata> offsets = new LinkedHashMap<>();\n+        List<KafkaMessage<K, V>> notifications = new LinkedList<>();\n+        // Commit highest offset + 1 of each partition that was ACK, and remove from pending\n+        for (Entry<TopicPartition, List<KafkaMessage<K, V>>> entry : pendingCommits.entrySet()) {\n+            // No need to sort it, offsets are consumed in order\n+            List<KafkaMessage<K, V>> byPartition = entry.getValue();\n+            Iterator<KafkaMessage<K, V>> iterator = byPartition.iterator();\n+            KafkaMessage<K, V> highest = null;\n+            while (iterator.hasNext()) {\n+                KafkaMessage<K, V> element = iterator.next();\n+                if (element.isAck()) {\n+                    notifications.add(element);\n+                    highest = element;\n+                    iterator.remove();\n+                } else {\n+                    break;\n+                }\n+            }\n+            if (highest != null) {\n+                OffsetAndMetadata offset = new OffsetAndMetadata(highest.getPayload().offset() + 1);\n+                LOGGER.fine(() -> String.format(\"Will commit %s %s\", entry.getKey(), offset));\n+                offsets.put(entry.getKey(), offset);\n+            }\n+        }\n+        if (!notifications.isEmpty()) {\n+            Optional<RuntimeException> exception = commitInKafka(offsets);\n+            notifications.stream().forEach(message -> {\n+                exception.ifPresent(ex -> message.exception(ex));\n+                message.wakeUp();\n+            });\n+        }\n+    }\n+\n+    private Optional<RuntimeException> commitInKafka(Map<TopicPartition, OffsetAndMetadata> offsets) {\n+        LOGGER.fine(() -> String.format(\"%s events to commit: \", offsets.size()));\n+        LOGGER.fine(() -> String.format(\"%s\", offsets));\n+        try {\n+            kafkaConsumer.commitSync(offsets);\n+            LOGGER.fine(() -> \"The commit was successful\");\n+            return Optional.empty();\n+        } catch (RuntimeException e) {\n+            LOGGER.log(Level.SEVERE, \"Unable to commit in Kafka \" + offsets, e);\n+            return Optional.of(e);\n+        }\n+    }\n+\n+    /**\n+     * Closes the connections to Kafka and stops to process new events.\n+     */\n+    @Override\n+    public void close() {\n+        // Stops pooling\n+        kafkaConsumer.wakeup();\n+        // Wait that current task finishes in case it is still running\n+        try {\n+            taskLock.lock();\n+            processACK();\n+            LOGGER.fine(() -> \"Pending ACKs: \" + pendingCommits.size());\n+            // Terminate waiting ACKs\n+            pendingCommits.values().stream().flatMap(List::stream).forEach(message -> message.wakeUp());\n+            kafkaConsumer.close();\n+            emiter.complete();\n+        } catch (RuntimeException e) {\n+            emiter.fail(e);", "originalCommit": "f634ae36ef5b9bd9df04b1349f7ef424f8949c65", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjc4MjE0Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r406782143", "bodyText": "ack is going to be called frequently, can we avoid spinning up ForkJoinPool and reuse emit loop we already have? I know it introduces another queue", "author": "danielkec", "createdAt": "2020-04-10T14:27:45Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaMessage.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+\n+/**\n+ * Kafka specific MP messaging message.\n+ *\n+ * @param <K> kafka record key type\n+ * @param <V> kafka record value type\n+ */\n+class KafkaMessage<K, V> implements Message<ConsumerRecord<K, V>> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaMessage.class.getName());\n+    private final ConsumerRecord<K, V> consumerRecord;\n+    private final AtomicBoolean ack = new AtomicBoolean(false);\n+    private final CountDownLatch waitForCommit;\n+    private final long millisWaitingTimeout;\n+    private final AtomicReference<Exception> ackException = new AtomicReference<>();\n+\n+    /**\n+     * Kafka specific MP messaging message.\n+     *\n+     * @param consumerRecord {@link org.apache.kafka.clients.consumer.ConsumerRecord}\n+     * @param autoCommit when false it will ack will wait till it is really commited in Kafka,\n+     *        otherwise there is no waiting time because it was committed already.\n+     * @param millisWaitingTimeout this is the time in milliseconds that the ack will be waiting\n+     *        the commit in Kafka. Applies only if autoCommit is false.\n+     */\n+    KafkaMessage(ConsumerRecord<K, V> consumerRecord, boolean autoCommit, long millisWaitingTimeout) {\n+        this.consumerRecord = consumerRecord;\n+        this.waitForCommit = new CountDownLatch(autoCommit ? 0 : 1);\n+        this.millisWaitingTimeout = millisWaitingTimeout;\n+    }\n+\n+    @Override\n+    public ConsumerRecord<K, V> getPayload() {\n+        return consumerRecord;\n+    }\n+\n+    @Override\n+    public CompletionStage<Void> ack() {\n+        ack.set(true);\n+        return CompletableFuture.runAsync(() -> {", "originalCommit": "f634ae36ef5b9bd9df04b1349f7ef424f8949c65", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "800055dd4a6dc3ded303ab27234f22af832ded5b", "url": "https://github.com/oracle/helidon/commit/800055dd4a6dc3ded303ab27234f22af832ded5b", "message": "Refactorings and ACK fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-14T05:19:31Z", "type": "forcePushed"}, {"oid": "9b49fcca8972f8c4fac0ccfbf292545d351b57a7", "url": "https://github.com/oracle/helidon/commit/9b49fcca8972f8c4fac0ccfbf292545d351b57a7", "message": "Refactorings and ACK fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-14T11:36:01Z", "type": "forcePushed"}, {"oid": "bb915d2406926c4a51ac9237bf797680511beb7a", "url": "https://github.com/oracle/helidon/commit/bb915d2406926c4a51ac9237bf797680511beb7a", "message": "Refactorings and ACK fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-14T11:39:08Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODExODAzOQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408118039", "bodyText": "This is my bug, it should have had a bottom bound\nif (publisher.requested.getAndUpdate(r -> r > 0 ? r - 1 : 0) < 1) {", "author": "danielkec", "createdAt": "2020-04-14T13:02:50Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/EmittingPublisher.java", "diffHunk": "@@ -0,0 +1,177 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.util.Objects;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.logging.Logger;\n+\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Emitting reactive streams publisher to be used by {@code ReactiveStreams.fromPublisher},\n+ * should be deprecated in favor of {@code org.eclipse.microprofile.reactive.messaging.Emitter}\n+ * in the future version of messaging.\n+ *\n+ * @param <T> type of emitted item\n+ */\n+class EmittingPublisher<T> implements Publisher<T> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(EmittingPublisher.class.getName());\n+    private Subscriber<? super T> subscriber;\n+    private final AtomicReference<State> state = new AtomicReference<>(State.NOT_REQUESTED_YET);\n+    private final AtomicLong requested = new AtomicLong();\n+    private final AtomicBoolean terminated = new AtomicBoolean();\n+    private final Callback<Long> requestsCallback;\n+\n+    protected EmittingPublisher(Callback<Long> requestsCallback) {\n+        this.requestsCallback = requestsCallback;\n+    }\n+\n+    @Override\n+    public void subscribe(Subscriber<? super T> subscriber) {\n+        Objects.requireNonNull(subscriber, \"subscriber is null\");\n+        this.subscriber = subscriber;\n+        subscriber.onSubscribe(new Subscription() {\n+            @Override\n+            public void request(final long n) {\n+                if (n < 1) {\n+                    fail(new IllegalArgumentException(\"Rule \u00a73.9 violated: non-positive request amount is forbidden\"));\n+                }\n+                LOGGER.fine(String.format(\"Request %s events\", n));\n+                requested.updateAndGet(r -> Long.MAX_VALUE - r > n ? n + r : Long.MAX_VALUE);\n+                state.compareAndSet(State.NOT_REQUESTED_YET, State.READY_TO_EMIT);\n+                requestsCallback.nofity(n);\n+            }\n+\n+            @Override\n+            public void cancel() {\n+                LOGGER.fine(\"Subscription cancelled\");\n+                state.compareAndSet(State.NOT_REQUESTED_YET, State.CANCELLED);\n+                state.compareAndSet(State.READY_TO_EMIT, State.CANCELLED);\n+                EmittingPublisher.this.subscriber = null;\n+            }\n+\n+        });\n+    }\n+\n+    /**\n+     * Properly fail the stream, set publisher to cancelled state and send {@code onError} signal downstream.\n+     * Signal {@code onError} is sent only once, any other call to this method is no-op.\n+     *\n+     * @param throwable Sent as {@code onError} signal\n+     */\n+    void fail(Throwable throwable) {\n+        if (!terminated.getAndSet(true) && subscriber != null) {\n+            state.compareAndSet(State.NOT_REQUESTED_YET, State.CANCELLED);\n+            state.compareAndSet(State.READY_TO_EMIT, State.CANCELLED);\n+            this.subscriber.onError(throwable);\n+        }\n+    }\n+\n+    /**\n+     * Properly complete the stream, set publisher to completed state and send {@code onComplete} signal downstream.\n+     * Signal {@code onComplete} is sent only once, any other call to this method is no-op.\n+     */\n+    void complete() {\n+        if (!terminated.getAndSet(true) && subscriber != null) {\n+            state.compareAndSet(State.NOT_REQUESTED_YET, State.COMPLETED);\n+            state.compareAndSet(State.READY_TO_EMIT, State.COMPLETED);\n+            this.subscriber.onComplete();\n+        }\n+    }\n+\n+    /**\n+     * Emit one item to the stream, if there is enough requested, item is signaled to downstream as {@code onNext}\n+     * and method returns true. If there is requested less than 1, nothing is sent and method returns false.\n+     *\n+     * @param item to be sent downstream\n+     * @return true if item successfully sent\n+     * @throws java.lang.IllegalStateException if publisher is cancelled\n+     */\n+    boolean emit(T item) {\n+        return this.state.get().emit(this, item);\n+    }\n+\n+    boolean isTerminated() {\n+        return terminated.get();\n+    }\n+\n+    /**\n+     * Check if publisher is in terminal state CANCELLED.\n+     *\n+     * @return true if so\n+     */\n+    boolean isCancelled() {\n+        return this.state.get() == State.CANCELLED;\n+    }\n+\n+    /**\n+     * Check if publisher is in terminal state COMPLETED.\n+     *\n+     * @return true if so\n+     */\n+    boolean isCompleted() {\n+        return this.state.get() == State.COMPLETED;\n+    }\n+\n+    private enum State {\n+        NOT_REQUESTED_YET {\n+            @Override\n+            <T> boolean emit(EmittingPublisher<T> publisher, T item) {\n+                return false;\n+            }\n+        },\n+        READY_TO_EMIT {\n+            @Override\n+            <T> boolean emit(EmittingPublisher<T> publisher, T item) {\n+                if (publisher.requested.getAndDecrement() < 1) {", "originalCommit": "bb915d2406926c4a51ac9237bf797680511beb7a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODEzMjc4OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408132788", "bodyText": "Needs to be public to be usable in Helidon SE", "author": "danielkec", "createdAt": "2020-04-14T13:24:24Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.common.configurable.ScheduledThreadPoolSupplier;\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+\n+/**\n+ * Implementation of Kafka Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnector.CONNECTOR_NAME)\n+public class KafkaConnector implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnector.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<Closeable> resourcesToClose = new LinkedList<>();\n+\n+    /**\n+     * Constructor to instance KafkaConnectorFactory.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    @Inject\n+    KafkaConnector(Config config) {", "originalCommit": "bb915d2406926c4a51ac9237bf797680511beb7a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODEzMzE4MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408133180", "bodyText": "Needs to be public to be usable in Helidon SE", "author": "danielkec", "createdAt": "2020-04-14T13:24:55Z", "path": "microprofile/connectors/kafka/src/main/java/io/helidon/microprofile/connectors/kafka/KafkaConnector.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.microprofile.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.common.configurable.ScheduledThreadPoolSupplier;\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+\n+/**\n+ * Implementation of Kafka Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnector.CONNECTOR_NAME)\n+public class KafkaConnector implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnector.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<Closeable> resourcesToClose = new LinkedList<>();\n+\n+    /**\n+     * Constructor to instance KafkaConnectorFactory.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    @Inject\n+    KafkaConnector(Config config) {\n+        scheduler = ScheduledThreadPoolSupplier.builder()\n+                .threadNamePrefix(\"kafka-\")\n+                .config(config)\n+                .build()\n+                .get();\n+    }\n+\n+    /**\n+     * Called when container is terminated.\n+     *\n+     * @param event termination event\n+     */\n+    void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {", "originalCommit": "bb915d2406926c4a51ac9237bf797680511beb7a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b0c66b2fff95eb07754c409c655227e897c886c5", "url": "https://github.com/oracle/helidon/commit/b0c66b2fff95eb07754c409c655227e897c886c5", "message": "Refactorings and ACK fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-14T13:36:36Z", "type": "forcePushed"}, {"oid": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "url": "https://github.com/oracle/helidon/commit/29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-14T14:27:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE5NDQxOA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408194418", "bodyText": "Backpressure is not driven by anything here, use .request(Long.MAX) instead pls. no need for request counting", "author": "danielkec", "createdAt": "2020-04-14T14:45:38Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+/**\n+ * Reactive streams subscriber implementation.\n+ *\n+ * @param <T> kafka record value type\n+ */\n+class KafkaSubscriber<T> implements Subscriber<Message<T>> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaSubscriber.class.getName());\n+    private static final String BACKPRESSURE_SIZE_KEY = \"backpressure.size\";\n+    private static final long BACKPRESSURE_SIZE_DEFAULT = 5;\n+    private final long backpressure;\n+    private final AtomicLong backpressureCounter = new AtomicLong();\n+    private final BasicKafkaProducer<?, T> producer;\n+    private Subscription subscription;\n+\n+    private KafkaSubscriber(BasicKafkaProducer<?, T> producer, long backpressure){\n+        this.backpressure = backpressure;\n+        this.producer = producer;\n+    }\n+\n+    @Override\n+    public void onSubscribe(Subscription subscription) {\n+        if (this.subscription == null) {\n+            this.subscription = subscription;\n+            this.subscription.request(backpressure);", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxNTM0OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408315349", "bodyText": "Backpressure has its place\n\nwhen reading messages from Kafka and delivering them to consumers, we should not deliver more than requested\nwhen writing message to Kafka and reading them from producers, we should not request more than we can send at that time\n\nIn both cases, requesting Long.MAX may result in memory issues, as the messages need to be buffered, or in thread issues, as you would block threads.", "author": "tomas-langer", "createdAt": "2020-04-14T17:35:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE5NDQxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMzOTMzNA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408339334", "bodyText": "No doubt about that, but without taking result of producer.produceAsync(message.getPayload()); in to the account its just unbounded stream with more bureaucracy", "author": "danielkec", "createdAt": "2020-04-14T18:14:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE5NDQxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODM0MzYwMQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408343601", "bodyText": "Jorge already came with nice idea of combining callbacks", "author": "danielkec", "createdAt": "2020-04-14T18:22:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODE5NDQxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODI5MDY3MA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408290670", "bodyText": "When you do less than warning levels of logging, please use the lambda approach:\nLogger.fine(() -> String.format(\"...\", n)) - otherwise the string formatting is evaluated for every single request.", "author": "tomas-langer", "createdAt": "2020-04-14T16:56:18Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/EmittingPublisher.java", "diffHunk": "@@ -0,0 +1,177 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.util.Objects;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.logging.Logger;\n+\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * Emitting reactive streams publisher to be used by {@code ReactiveStreams.fromPublisher},\n+ * should be deprecated in favor of {@code org.eclipse.microprofile.reactive.messaging.Emitter}\n+ * in the future version of messaging.\n+ *\n+ * @param <T> type of emitted item\n+ */\n+class EmittingPublisher<T> implements Publisher<T> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(EmittingPublisher.class.getName());\n+    private Subscriber<? super T> subscriber;\n+    private final AtomicReference<State> state = new AtomicReference<>(State.NOT_REQUESTED_YET);\n+    private final AtomicLong requested = new AtomicLong();\n+    private final AtomicBoolean terminated = new AtomicBoolean();\n+    private final Callback<Long> requestsCallback;\n+\n+    EmittingPublisher(Callback<Long> requestsCallback) {\n+        this.requestsCallback = requestsCallback;\n+    }\n+\n+    @Override\n+    public void subscribe(Subscriber<? super T> subscriber) {\n+        Objects.requireNonNull(subscriber, \"subscriber is null\");\n+        this.subscriber = subscriber;\n+        subscriber.onSubscribe(new Subscription() {\n+            @Override\n+            public void request(final long n) {\n+                if (n < 1) {\n+                    fail(new IllegalArgumentException(\"Rule \u00a73.9 violated: non-positive request amount is forbidden\"));\n+                }\n+                LOGGER.fine(String.format(\"Request %s events\", n));", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxMTkwNQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408311905", "bodyText": "If this class should be part of Helidon SE, then the public constructor is an issue.\nAlso you need to add a Builder for any configurable options - in Helidon SE, most of things that can be done using Config should be doable using Builder and vice-versa", "author": "tomas-langer", "createdAt": "2020-04-14T17:30:06Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.common.configurable.ScheduledThreadPoolSupplier;\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+\n+/**\n+ * Implementation of Kafka Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnector.CONNECTOR_NAME)\n+public class KafkaConnector implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnector.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<Closeable> resourcesToClose = new LinkedList<>();\n+\n+    /**\n+     * Constructor to instance KafkaConnectorFactory.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    @Inject\n+    public KafkaConnector(Config config) {", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxMjk2Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408312966", "bodyText": "For correct SE/MP split, I think you should:\n\nRemove public constructor from this class\nCreate a builder that also supports config through config(Config) method as other builders in Helidon\nAdd a CDI extension for Kafka connector to another module, that would create the connector instance for MP messaging correctly injecting values using CDI", "author": "tomas-langer", "createdAt": "2020-04-14T17:31:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxMTkwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxMzIwMQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408313201", "bodyText": "@Observes methods should not be public", "author": "tomas-langer", "createdAt": "2020-04-14T17:32:18Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaConnector.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.context.BeforeDestroyed;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+\n+import io.helidon.common.configurable.ScheduledThreadPoolSupplier;\n+import io.helidon.config.Config;\n+\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.spi.Connector;\n+import org.eclipse.microprofile.reactive.messaging.spi.IncomingConnectorFactory;\n+import org.eclipse.microprofile.reactive.messaging.spi.OutgoingConnectorFactory;\n+import org.eclipse.microprofile.reactive.streams.operators.PublisherBuilder;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+\n+/**\n+ * Implementation of Kafka Connector as described in the MicroProfile Reactive Messaging Specification.\n+ */\n+@ApplicationScoped\n+@Connector(KafkaConnector.CONNECTOR_NAME)\n+public class KafkaConnector implements IncomingConnectorFactory, OutgoingConnectorFactory {\n+\n+    /**\n+     * Microprofile messaging Kafka connector name.\n+     */\n+    static final String CONNECTOR_NAME = \"helidon-kafka\";\n+    private static final Logger LOGGER = Logger.getLogger(KafkaConnector.class.getName());\n+    private final ScheduledExecutorService scheduler;\n+    private final Queue<Closeable> resourcesToClose = new LinkedList<>();\n+\n+    /**\n+     * Constructor to instance KafkaConnectorFactory.\n+     *\n+     * @param config Helidon {@link io.helidon.config.Config config}\n+     */\n+    @Inject\n+    public KafkaConnector(Config config) {\n+        scheduler = ScheduledThreadPoolSupplier.builder()\n+                .threadNamePrefix(\"kafka-\")\n+                .config(config)\n+                .build()\n+                .get();\n+    }\n+\n+    /**\n+     * Called when container is terminated. If it is not running in a container it must be explicitly invoked\n+     * to terminate the messaging and release Kafka connections.\n+     *\n+     * @param event termination event\n+     */\n+    public void terminate(@Observes @BeforeDestroyed(ApplicationScoped.class) Object event) {", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxMzk2OA==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408313968", "bodyText": "This is a use case for a builder.", "author": "tomas-langer", "createdAt": "2020-04-14T17:33:30Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaPublisher.java", "diffHunk": "@@ -0,0 +1,312 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.io.Closeable;\n+import java.time.Duration;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Optional;\n+import java.util.Queue;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.common.context.Context;\n+import io.helidon.common.context.Contexts;\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.WakeupException;\n+import org.reactivestreams.Publisher;\n+import org.reactivestreams.Subscriber;\n+\n+/**\n+ * This is an implementation of {@link org.reactivestreams.Publisher} that read events from\n+ * Kafka and push them downstream to one subscriber.\n+ * Configurable by Helidon {@link io.helidon.config.Config Config},\n+ *\n+ * @param <K> Key type\n+ * @param <V> Value type\n+ * @see io.helidon.config.Config\n+ */\n+class KafkaPublisher<K, V> implements Publisher<KafkaMessage<K, V>>, Closeable {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaPublisher.class.getName());\n+    private static final String POLL_TIMEOUT = \"poll.timeout\";\n+    private static final String PERIOD_EXECUTIONS = \"period.executions\";\n+    private static final String ENABLE_AUTOCOMMIT = \"enable.auto.commit\";\n+    private static final String ACK_TIMEOUT = \"ack.timeout.millis\";\n+    private static final String LIMIT_NO_ACK = \"limit.no.ack\";\n+    private final Lock taskLock = new ReentrantLock();\n+    private final Queue<ConsumerRecord<K, V>> backPressureBuffer = new LinkedList<>();\n+    private final Map<TopicPartition, List<KafkaMessage<K, V>>> pendingCommits = new HashMap<>();\n+    private final PartitionsAssignedLatch partitionsAssignedLatch = new PartitionsAssignedLatch();\n+    private final ScheduledExecutorService scheduler;\n+    private final Consumer<K, V> kafkaConsumer;\n+    private final AtomicLong requests = new AtomicLong();\n+    private final EmittingPublisher<KafkaMessage<K, V>> emiter =\n+            new EmittingPublisher<>(requested -> requests.addAndGet(requested));\n+    private final List<String> topics;\n+    private final long periodExecutions;\n+    private final long pollTimeout;\n+    private final boolean autoCommit;\n+    private final long ackTimeout;\n+    private final int limitNoAck;\n+\n+    private KafkaPublisher(ScheduledExecutorService scheduler, Consumer<K, V> kafkaConsumer,\n+            List<String> topics, long pollTimeout, long periodExecutions,\n+            boolean autoCommit, long ackTimeout, int limitNoAck) {\n+        this.scheduler = scheduler;\n+        this.kafkaConsumer = kafkaConsumer;\n+        this.topics = topics;\n+        this.periodExecutions = periodExecutions;\n+        this.pollTimeout = pollTimeout;\n+        this.autoCommit = autoCommit;\n+        this.ackTimeout = ackTimeout;\n+        this.limitNoAck = limitNoAck;\n+    }\n+\n+    /**\n+     * Starts to consume events from Kafka to send them downstream till\n+     * {@link io.helidon.messaging.connectors.kafka.KafkaPublisher#close()} is invoked.\n+     * This execution runs in one thread that is triggered by the scheduler.\n+     */\n+    private void execute() {\n+        kafkaConsumer.subscribe(topics, partitionsAssignedLatch);\n+        // This thread reads from Kafka topics and push in kafkaBufferedEvents\n+        scheduler.scheduleAtFixedRate(() -> {\n+            try {\n+                // Need to lock to avoid onClose() is executed meanwhile task is running\n+                taskLock.lock();\n+                if (!scheduler.isShutdown() && !emiter.isTerminated()) {\n+                    int currentNoAck = currentNoAck();\n+                    if (currentNoAck < limitNoAck) {\n+                        if (backPressureBuffer.isEmpty()) {\n+                            try {\n+                                kafkaConsumer.poll(Duration.ofMillis(pollTimeout)).forEach(backPressureBuffer::add);\n+                            } catch (WakeupException e) {\n+                                LOGGER.fine(() -> \"It was requested to stop polling from channel\");\n+                            }\n+                        } else {\n+                            long totalToEmit = requests.get();\n+                            // Avoid index out bound exceptions\n+                            long eventsToEmit = Math.min(totalToEmit, backPressureBuffer.size());\n+                            for (long i = 0; i < eventsToEmit; i++) {\n+                                ConsumerRecord<K, V> cr = backPressureBuffer.poll();\n+                                CompletableFuture<Void> kafkaCommit = new CompletableFuture<>();\n+                                KafkaMessage<K, V> kafkaMessage = new KafkaMessage<>(cr, kafkaCommit, ackTimeout);\n+                                if (!autoCommit) {\n+                                    TopicPartition key = new TopicPartition(kafkaMessage.getPayload().topic(),\n+                                            kafkaMessage.getPayload().partition());\n+                                    pendingCommits.computeIfAbsent(key, k -> new LinkedList<>()).add(kafkaMessage);\n+                                } else {\n+                                    kafkaCommit.complete(null);\n+                                }\n+                                // Note that next execution will reach the user code inside @Incoming method.\n+                                // By spec, onNext MUST NOT block the Publisher, otherwise it will make problems.\n+                                runInNewContext(() ->  emiter.emit(kafkaMessage));\n+                                requests.decrementAndGet();\n+                            }\n+                        }\n+                    } else {\n+                        throw new IllegalStateException(\n+                                String.format(\"Current pending %s acks has overflown the limit of %s \",\n+                                        currentNoAck, limitNoAck));\n+                    }\n+                }\n+                // Commit ACKs\n+                processACK();\n+            } catch (Exception e) {\n+                LOGGER.log(Level.SEVERE, \"KafkaPublisher failed\", e);\n+                emiter.fail(e);\n+            } finally {\n+                taskLock.unlock();\n+            }\n+        }, 0, periodExecutions, TimeUnit.MILLISECONDS);\n+    }\n+\n+    private int currentNoAck() {\n+        return pendingCommits.values().stream().map(list -> list.size()).reduce((a, b) -> a + b).orElse(0);\n+    }\n+\n+    /**\n+     * Process the ACKs only if enable.auto.commit is false.\n+     * This will search events that are ACK and it will commit them to Kafka.\n+     * Those events that are committed will make the {@link KafkaMessage#ack()}\n+     * to complete.\n+     */\n+    private void processACK() {\n+        if (!autoCommit) {\n+            Map<TopicPartition, OffsetAndMetadata> offsets = new LinkedHashMap<>();\n+            List<KafkaMessage<K, V>> messagesToCommit = new LinkedList<>();\n+            // Commit highest offset + 1 of each partition that was ACK, and remove from pending\n+            for (Entry<TopicPartition, List<KafkaMessage<K, V>>> entry : pendingCommits.entrySet()) {\n+                // No need to sort it, offsets are consumed in order\n+                List<KafkaMessage<K, V>> byPartition = entry.getValue();\n+                Iterator<KafkaMessage<K, V>> iterator = byPartition.iterator();\n+                KafkaMessage<K, V> highest = null;\n+                while (iterator.hasNext()) {\n+                    KafkaMessage<K, V> element = iterator.next();\n+                    if (element.isAck()) {\n+                        messagesToCommit.add(element);\n+                        highest = element;\n+                        iterator.remove();\n+                    } else {\n+                        break;\n+                    }\n+                }\n+                if (highest != null) {\n+                    OffsetAndMetadata offset = new OffsetAndMetadata(highest.getPayload().offset() + 1);\n+                    LOGGER.fine(() -> String.format(\"Will commit %s %s\", entry.getKey(), offset));\n+                    offsets.put(entry.getKey(), offset);\n+                }\n+            }\n+            if (!messagesToCommit.isEmpty()) {\n+                Optional<RuntimeException> exception = commitInKafka(offsets);\n+                messagesToCommit.stream().forEach(message -> {\n+                    exception.ifPresentOrElse(\n+                            ex -> message.kafkaCommit().completeExceptionally(ex),\n+                            () -> message.kafkaCommit().complete(null));\n+                });\n+            }\n+        }\n+    }\n+\n+    private Optional<RuntimeException> commitInKafka(Map<TopicPartition, OffsetAndMetadata> offsets) {\n+        LOGGER.fine(() -> String.format(\"%s events to commit: \", offsets.size()));\n+        LOGGER.fine(() -> String.format(\"%s\", offsets));\n+        try {\n+            kafkaConsumer.commitSync(offsets);\n+            LOGGER.fine(() -> \"The commit was successful\");\n+            return Optional.empty();\n+        } catch (RuntimeException e) {\n+            LOGGER.log(Level.SEVERE, \"Unable to commit in Kafka \" + offsets, e);\n+            return Optional.of(e);\n+        }\n+    }\n+\n+    /**\n+     * Closes the connections to Kafka and stops to process new events.\n+     */\n+    @Override\n+    public void close() {\n+        // Stops pooling\n+        kafkaConsumer.wakeup();\n+        // Wait that current task finishes in case it is still running\n+        try {\n+            taskLock.lock();\n+            LOGGER.fine(() -> \"Pending ACKs: \" + pendingCommits.size());\n+            // Terminate waiting ACKs\n+            pendingCommits.values().stream().flatMap(List::stream)\n+            .forEach(message ->\n+            message.kafkaCommit().completeExceptionally(new TimeoutException(\"Aborted because KafkaPublisher is shutting down\")));\n+            kafkaConsumer.close();\n+            emiter.complete();\n+        } catch (RuntimeException e) {\n+            LOGGER.log(Level.SEVERE, \"Error closing KafkaPublisher\", e);\n+            emiter.fail(e);\n+        } finally {\n+            taskLock.unlock();\n+        }\n+        LOGGER.fine(() -> \"Closed\");\n+    }\n+\n+    //Move to messaging incoming connector\n+    protected void runInNewContext(Runnable runnable) {\n+        Context.Builder contextBuilder = Context.builder()\n+                .id(String.format(\"kafka-message-%s:\", UUID.randomUUID().toString()));\n+        Contexts.context().ifPresent(contextBuilder::parent);\n+        Contexts.runInContext(contextBuilder.build(), runnable);\n+    }\n+\n+    @Override\n+    public void subscribe(Subscriber<? super KafkaMessage<K, V>> subscriber) {\n+        emiter.subscribe(subscriber);\n+    }\n+\n+    /**\n+     * Blocks current thread until partitions are assigned, since when is consumer effectively ready to receive.\n+     *\n+     * @param timeout the maximum time to wait\n+     * @param unit    the time unit of the timeout argument\n+     * @throws java.lang.InterruptedException        if the current thread is interrupted while waiting\n+     * @throws java.util.concurrent.TimeoutException if the timeout is reached\n+     */\n+    void waitForPartitionAssigment(long timeout, TimeUnit unit) throws InterruptedException, TimeoutException {\n+        if (!partitionsAssignedLatch.await(timeout, unit)) {\n+            throw new TimeoutException(\"Timeout for subscription reached\");\n+        }\n+    }\n+\n+    /**\n+     * Creates a new instance of ReactiveKafkaPublisher given a scheduler and the configuration and it starts to publish.\n+     *\n+     * Note: after creating a KafkaPublisher you must always\n+     * {@link io.helidon.messaging.connectors.kafka.KafkaPublisher#close()} it to avoid resource leaks.\n+     *\n+     * @param <K> Key type\n+     * @param <V> Value type\n+     * @param scheduler It will trigger the task execution when\n+     * {@link io.helidon.messaging.connectors.kafka.KafkaPublisher#execute()} is invoked\n+     * @param config With the KafkaPublisher required parameters\n+     * @return A new instance of ReactiveKafkaPublisher\n+     */\n+    static <K, V> KafkaPublisher<K, V> build(ScheduledExecutorService scheduler, Config config){\n+        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n+        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n+        if (topics.isEmpty()) {\n+            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n+        }\n+        Consumer<K, V> kafkaConsumer = new KafkaConsumer<>(kafkaConfig);", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxNzU1Ng==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408317556", "bodyText": "The only allows factory method is create unless there is a good reason not to use it. I do not see a good reason, so please rename to create(Config) (even though this is package local only)", "author": "tomas-langer", "createdAt": "2020-04-14T17:39:28Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+/**\n+ * Reactive streams subscriber implementation.\n+ *\n+ * @param <T> kafka record value type\n+ */\n+class KafkaSubscriber<T> implements Subscriber<Message<T>> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaSubscriber.class.getName());\n+    private static final String BACKPRESSURE_SIZE_KEY = \"backpressure.size\";\n+    private static final long BACKPRESSURE_SIZE_DEFAULT = 5;\n+    private final long backpressure;\n+    private final AtomicLong backpressureCounter = new AtomicLong();\n+    private final BasicKafkaProducer<?, T> producer;\n+    private Subscription subscription;\n+\n+    private KafkaSubscriber(BasicKafkaProducer<?, T> producer, long backpressure){\n+        this.backpressure = backpressure;\n+        this.producer = producer;\n+    }\n+\n+    @Override\n+    public void onSubscribe(Subscription subscription) {\n+        if (this.subscription == null) {\n+            this.subscription = subscription;\n+            this.subscription.request(backpressure);\n+        } else {\n+            subscription.cancel();\n+        }\n+    }\n+\n+    @Override\n+    public void onNext(Message<T> message) {\n+        Objects.requireNonNull(message);\n+        producer.produceAsync(message.getPayload());\n+        message.ack();\n+        if (backpressureCounter.incrementAndGet() == backpressure) {\n+            backpressureCounter.set(0);\n+            subscription.request(backpressure);\n+        }\n+    }\n+\n+    @Override\n+    public void onError(Throwable t) {\n+        Objects.requireNonNull(t);\n+        LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n+        producer.close();\n+    }\n+\n+    @Override\n+    public void onComplete() {\n+        LOGGER.fine(\"Subscriber has finished\");\n+        producer.close();\n+    }\n+\n+    /**\n+     * Creates a new instance of KafkaSubscriber given the configuration.\n+     * Note: Every new instance of this type opens Kafka resources and it will be opened\n+     * till onComplete() or onError() is invoked.\n+     *\n+     * @param <T> The type to push\n+     * @param config With the KafkaSubscriber required parameters\n+     * @return A new KafkaSubscriber instance\n+     */\n+    static <T> KafkaSubscriber<T> build(Config config) {", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxNzgzMQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408317831", "bodyText": "Please do not do this.\nUse a builder if you want to specify details and fill them either manually or from config.", "author": "tomas-langer", "createdAt": "2020-04-14T17:39:56Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/KafkaSubscriber.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import io.helidon.config.Config;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+/**\n+ * Reactive streams subscriber implementation.\n+ *\n+ * @param <T> kafka record value type\n+ */\n+class KafkaSubscriber<T> implements Subscriber<Message<T>> {\n+\n+    private static final Logger LOGGER = Logger.getLogger(KafkaSubscriber.class.getName());\n+    private static final String BACKPRESSURE_SIZE_KEY = \"backpressure.size\";\n+    private static final long BACKPRESSURE_SIZE_DEFAULT = 5;\n+    private final long backpressure;\n+    private final AtomicLong backpressureCounter = new AtomicLong();\n+    private final BasicKafkaProducer<?, T> producer;\n+    private Subscription subscription;\n+\n+    private KafkaSubscriber(BasicKafkaProducer<?, T> producer, long backpressure){\n+        this.backpressure = backpressure;\n+        this.producer = producer;\n+    }\n+\n+    @Override\n+    public void onSubscribe(Subscription subscription) {\n+        if (this.subscription == null) {\n+            this.subscription = subscription;\n+            this.subscription.request(backpressure);\n+        } else {\n+            subscription.cancel();\n+        }\n+    }\n+\n+    @Override\n+    public void onNext(Message<T> message) {\n+        Objects.requireNonNull(message);\n+        producer.produceAsync(message.getPayload());\n+        message.ack();\n+        if (backpressureCounter.incrementAndGet() == backpressure) {\n+            backpressureCounter.set(0);\n+            subscription.request(backpressure);\n+        }\n+    }\n+\n+    @Override\n+    public void onError(Throwable t) {\n+        Objects.requireNonNull(t);\n+        LOGGER.log(Level.SEVERE, \"The Kafka subscription has failed\", t);\n+        producer.close();\n+    }\n+\n+    @Override\n+    public void onComplete() {\n+        LOGGER.fine(\"Subscriber has finished\");\n+        producer.close();\n+    }\n+\n+    /**\n+     * Creates a new instance of KafkaSubscriber given the configuration.\n+     * Note: Every new instance of this type opens Kafka resources and it will be opened\n+     * till onComplete() or onError() is invoked.\n+     *\n+     * @param <T> The type to push\n+     * @param config With the KafkaSubscriber required parameters\n+     * @return A new KafkaSubscriber instance\n+     */\n+    static <T> KafkaSubscriber<T> build(Config config) {\n+        Map<String, Object> kafkaConfig = HelidonToKafkaConfigParser.toMap(config);\n+        List<String> topics = HelidonToKafkaConfigParser.topicNameList(kafkaConfig);\n+        if (topics.isEmpty()) {\n+            throw new IllegalArgumentException(\"The topic is a required configuration value\");\n+        }\n+        long backpressure = config.get(BACKPRESSURE_SIZE_KEY).asLong().orElse(BACKPRESSURE_SIZE_DEFAULT);\n+        return new KafkaSubscriber<T>(new BasicKafkaProducer<>(topics, new KafkaProducer<>(kafkaConfig)), backpressure);\n+    }\n+\n+    // For tests", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxODI5Mw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408318293", "bodyText": "This seems not to do anything useful. Isn't the contract of ConsumerRebalancerListener a bit more complicated?", "author": "tomas-langer", "createdAt": "2020-04-14T17:40:43Z", "path": "messaging/connectors/kafka/src/main/java/io/helidon/messaging/connectors/kafka/PartitionsAssignedLatch.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.util.Collection;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.logging.Logger;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.common.TopicPartition;\n+\n+/**\n+ * Waiting latch for partition assigment, after that is consumer ready to receive.\n+ */\n+class PartitionsAssignedLatch extends CountDownLatch implements ConsumerRebalanceListener {", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODU5NDQ5NQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408594495", "bodyText": "From the point of view of reading new events, we don't care about the partitions because we don't specify them.\nBut this could affects pending commits. This is the current strategy when this happens:\n\nEvent is coming from partition A.\nUser is processing it.\nKafka revoke that partition.\nUser ack, commit is sent.\nKafka should throw CommitFailedException, because that partition doesn't exist.\nThe exception is sent to the ack, so user can decide. Note that publisher doesn't fail in this scenario, but the user can cancel the subscription if he wants to.\n\nOnce partition is up again, Kafka should send the message again (because it was not successfully committed).", "author": "jbescos", "createdAt": "2020-04-15T05:46:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxODI5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxOTA2OQ==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408319069", "bodyText": "Helidon MP implementations must not be required in an SE module.\nIf you want to split into SE/MP, then SE modules can only depend on other SE modules.\nSE modules can depend on MP APIs and SPIs of the specifications.", "author": "tomas-langer", "createdAt": "2020-04-14T17:41:55Z", "path": "messaging/connectors/kafka/src/main/java/module-info.java", "diffHunk": "@@ -0,0 +1,34 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+module io.helidon.microprofile.connectors.kafka {\n+    requires java.logging;\n+\n+    requires static cdi.api;\n+    requires static javax.inject;\n+    requires static java.activation;\n+    requires static kafka.clients;\n+    requires io.helidon.microprofile.config;", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMxOTMzNw==", "url": "https://github.com/oracle/helidon/pull/1510#discussion_r408319337", "bodyText": "This is a CDI bean, should not be part of SE impementation at all.", "author": "tomas-langer", "createdAt": "2020-04-14T17:42:21Z", "path": "messaging/connectors/kafka/src/test/java/io/helidon/messaging/connectors/kafka/AbstractSampleBean.java", "diffHunk": "@@ -0,0 +1,238 @@\n+/*\n+ * Copyright (c) 2020 Oracle and/or its affiliates. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.helidon.messaging.connectors.kafka;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.logging.Logger;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+\n+import io.helidon.messaging.connectors.kafka.KafkaMessage;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.eclipse.microprofile.reactive.messaging.Acknowledgment;\n+import org.eclipse.microprofile.reactive.messaging.Incoming;\n+import org.eclipse.microprofile.reactive.messaging.Message;\n+import org.eclipse.microprofile.reactive.messaging.Outgoing;\n+import org.eclipse.microprofile.reactive.streams.operators.ReactiveStreams;\n+import org.eclipse.microprofile.reactive.streams.operators.SubscriberBuilder;\n+import org.reactivestreams.Subscriber;\n+import org.reactivestreams.Subscription;\n+\n+/**\n+ * This class contains the outputs of the tests. In order to avoid that one test mess up in the results\n+ * of other tests (this could happen when some data is produced in one test and it is not committed),\n+ * there are many subclasses of AbstractSampleBean.\n+ */\n+abstract class AbstractSampleBean {", "originalCommit": "29ac5c1aa06b0c527a9dec6c1e8c8117ae1ee425", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "a5ee8f4ea70246d599e74c8967656108e723fb6e", "url": "https://github.com/oracle/helidon/commit/a5ee8f4ea70246d599e74c8967656108e723fb6e", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-14T18:42:50Z", "type": "forcePushed"}, {"oid": "b563ceb7de1d03c4d1f7b76fe3aae7a93d8381ac", "url": "https://github.com/oracle/helidon/commit/b563ceb7de1d03c4d1f7b76fe3aae7a93d8381ac", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-15T06:44:07Z", "type": "forcePushed"}, {"oid": "724b9f8a6ee57542b101efb18ee500be8f96aa7b", "url": "https://github.com/oracle/helidon/commit/724b9f8a6ee57542b101efb18ee500be8f96aa7b", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-15T09:17:11Z", "type": "forcePushed"}, {"oid": "819cb9c1a418302727f9f2d2dfffb59d350c951c", "url": "https://github.com/oracle/helidon/commit/819cb9c1a418302727f9f2d2dfffb59d350c951c", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-15T10:15:53Z", "type": "forcePushed"}, {"oid": "443cb90c00aa92d802da53ad4aff0e1fe5f5d6fe", "url": "https://github.com/oracle/helidon/commit/443cb90c00aa92d802da53ad4aff0e1fe5f5d6fe", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-15T16:30:56Z", "type": "forcePushed"}, {"oid": "127a89da95f26a491ef3bcc948d3c31a4ccbf46e", "url": "https://github.com/oracle/helidon/commit/127a89da95f26a491ef3bcc948d3c31a4ccbf46e", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-15T17:10:53Z", "type": "forcePushed"}, {"oid": "5fc90400ca8a0f51f772a2aaff0e4d21a6755c9d", "url": "https://github.com/oracle/helidon/commit/5fc90400ca8a0f51f772a2aaff0e4d21a6755c9d", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T05:43:59Z", "type": "forcePushed"}, {"oid": "1b20e99fa8a7f641f2152eb5e015f3eeab322aba", "url": "https://github.com/oracle/helidon/commit/1b20e99fa8a7f641f2152eb5e015f3eeab322aba", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T06:57:02Z", "type": "forcePushed"}, {"oid": "ccb99fc6d2a66eb0a61fd5b4a4fc5a68708bbf2e", "url": "https://github.com/oracle/helidon/commit/ccb99fc6d2a66eb0a61fd5b4a4fc5a68708bbf2e", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T08:22:05Z", "type": "forcePushed"}, {"oid": "040e496c48dfb131b358d5bc47394fdeeb0ab990", "url": "https://github.com/oracle/helidon/commit/040e496c48dfb131b358d5bc47394fdeeb0ab990", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T09:24:46Z", "type": "forcePushed"}, {"oid": "5164ac2f69b9548de389795d65fc47e06603fee5", "url": "https://github.com/oracle/helidon/commit/5164ac2f69b9548de389795d65fc47e06603fee5", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T10:11:13Z", "type": "forcePushed"}, {"oid": "f92986265ac9529f58a0d10e4e256624f4f6fe1b", "url": "https://github.com/oracle/helidon/commit/f92986265ac9529f58a0d10e4e256624f4f6fe1b", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T11:27:35Z", "type": "forcePushed"}, {"oid": "6ce484aeb96ee5891ce29899845a44cc5d67dc80", "url": "https://github.com/oracle/helidon/commit/6ce484aeb96ee5891ce29899845a44cc5d67dc80", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T11:31:34Z", "type": "forcePushed"}, {"oid": "5a1897e1adb0f07d4f20080c3dae54bc7afd5edf", "url": "https://github.com/oracle/helidon/commit/5a1897e1adb0f07d4f20080c3dae54bc7afd5edf", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-16T12:11:27Z", "type": "forcePushed"}, {"oid": "85dd94f6a91b16c06439440205939007d81c4011", "url": "https://github.com/oracle/helidon/commit/85dd94f6a91b16c06439440205939007d81c4011", "message": "Merge branch 'master' into kafkaConnector\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-17T05:38:12Z", "type": "forcePushed"}, {"oid": "3d22f6ce368e806ceea0047da247afc6460b84f3", "url": "https://github.com/oracle/helidon/commit/3d22f6ce368e806ceea0047da247afc6460b84f3", "message": "Remove public modifiers in TCK tests, and improve someEventsNoAckWithDifferentPartitions test\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-17T06:29:55Z", "type": "forcePushed"}, {"oid": "9f76d6cd40937d5622e165bf013edb08cfcf75ef", "url": "https://github.com/oracle/helidon/commit/9f76d6cd40937d5622e165bf013edb08cfcf75ef", "message": "Remove public modifiers in TCK tests, and improve someEventsNoAckWithDifferentPartitions test\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-17T06:44:22Z", "type": "forcePushed"}, {"oid": "907340606f71273dc743136f182a860bdb84d348", "url": "https://github.com/oracle/helidon/commit/907340606f71273dc743136f182a860bdb84d348", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-17T18:02:46Z", "type": "forcePushed"}, {"oid": "6a7489ad31cf9e88a550a002ccb7d3f24d67f1f1", "url": "https://github.com/oracle/helidon/commit/6a7489ad31cf9e88a550a002ccb7d3f24d67f1f1", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-17T18:09:09Z", "type": "forcePushed"}, {"oid": "c77a17bc4bb2db53a495bd3fefb7e9e3db06a189", "url": "https://github.com/oracle/helidon/commit/c77a17bc4bb2db53a495bd3fefb7e9e3db06a189", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-17T18:24:44Z", "type": "forcePushed"}, {"oid": "48eae481f9df3b5c50d586874f8f5610e2af0a3d", "url": "https://github.com/oracle/helidon/commit/48eae481f9df3b5c50d586874f8f5610e2af0a3d", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-20T09:01:08Z", "type": "forcePushed"}, {"oid": "c556cc8a85175caf5a60eb795a41cd425acb40b7", "url": "https://github.com/oracle/helidon/commit/c556cc8a85175caf5a60eb795a41cd425acb40b7", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-20T09:42:19Z", "type": "forcePushed"}, {"oid": "e17c7ab5db7ddb6861e52dd90796cf77806556ec", "url": "https://github.com/oracle/helidon/commit/e17c7ab5db7ddb6861e52dd90796cf77806556ec", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-21T10:11:04Z", "type": "forcePushed"}, {"oid": "3a896fa19cbc3a64ea69121d1bec080ce30389f8", "url": "https://github.com/oracle/helidon/commit/3a896fa19cbc3a64ea69121d1bec080ce30389f8", "message": "Kafka support\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:14:37Z", "type": "commit"}, {"oid": "b5e59d0868995c86edabe0588e078bc4c47e8660", "url": "https://github.com/oracle/helidon/commit/b5e59d0868995c86edabe0588e078bc4c47e8660", "message": "A few small changes.\n\nSigned-off-by: Tomas Langer <tomas.langer@oracle.com>", "committedDate": "2020-04-22T12:14:38Z", "type": "commit"}, {"oid": "0bae61f51a5d949b3b26503945948f198cfa2534", "url": "https://github.com/oracle/helidon/commit/0bae61f51a5d949b3b26503945948f198cfa2534", "message": "To trigger the build\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:14:38Z", "type": "commit"}, {"oid": "32be94abc930b1516e6eca952fd4e379619563c1", "url": "https://github.com/oracle/helidon/commit/32be94abc930b1516e6eca952fd4e379619563c1", "message": "TCK in KafkaConnector\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:15:13Z", "type": "commit"}, {"oid": "150c3e80e44aea0a5827b9d2bf2adca4e0b81477", "url": "https://github.com/oracle/helidon/commit/150c3e80e44aea0a5827b9d2bf2adca4e0b81477", "message": "Move tck tests inside kafka module\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:15:13Z", "type": "commit"}, {"oid": "c4558980f68943d3afb03efc198fa9ba94bf30cb", "url": "https://github.com/oracle/helidon/commit/c4558980f68943d3afb03efc198fa9ba94bf30cb", "message": "Refactorings and ACK fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:15:13Z", "type": "commit"}, {"oid": "430995f595845e9f88bdb7296f3ee72bf8a750c0", "url": "https://github.com/oracle/helidon/commit/430995f595845e9f88bdb7296f3ee72bf8a750c0", "message": "Move from microprofile to new module messaging\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:17:26Z", "type": "commit"}, {"oid": "7d68ce582ca13d11dedf930423920617870c62fd", "url": "https://github.com/oracle/helidon/commit/7d68ce582ca13d11dedf930423920617870c62fd", "message": "Improving the tests\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:17:26Z", "type": "commit"}, {"oid": "8ba7b9b2ae8db8031912e08aee34ea83d728ae0f", "url": "https://github.com/oracle/helidon/commit/8ba7b9b2ae8db8031912e08aee34ea83d728ae0f", "message": "Remove public modifiers in TCK tests, and improve someEventsNoAckWithDifferentPartitions test\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:17:26Z", "type": "commit"}, {"oid": "7081c593461d66f7eea754455b70d825a6a7cd96", "url": "https://github.com/oracle/helidon/commit/7081c593461d66f7eea754455b70d825a6a7cd96", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T12:24:13Z", "type": "forcePushed"}, {"oid": "8f82cfa76a66a82eda2f3e8d20a0acb864b744e6", "url": "https://github.com/oracle/helidon/commit/8f82cfa76a66a82eda2f3e8d20a0acb864b744e6", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T14:58:23Z", "type": "commit"}, {"oid": "8f82cfa76a66a82eda2f3e8d20a0acb864b744e6", "url": "https://github.com/oracle/helidon/commit/8f82cfa76a66a82eda2f3e8d20a0acb864b744e6", "message": "Tomas review comments iteration 1\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-22T14:58:23Z", "type": "forcePushed"}, {"oid": "620d5a451b6fea68673ac3defdaa7c5b9e03dd1a", "url": "https://github.com/oracle/helidon/commit/620d5a451b6fea68673ac3defdaa7c5b9e03dd1a", "message": "Copyright fixes\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-04-23T09:07:42Z", "type": "commit"}]}