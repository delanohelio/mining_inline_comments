{"pr_number": 3366, "pr_title": "[STORM-3388] Launch workers inside container using runc runtime", "pr_createdAt": "2020-12-21T22:03:00Z", "pr_url": "https://github.com/apache/storm/pull/3366", "timeline": [{"oid": "183b02dccbee3c4e62a388123b936690d44302bd", "url": "https://github.com/apache/storm/commit/183b02dccbee3c4e62a388123b936690d44302bd", "message": "[STORM-3388] Launch workers inside container using runc runtime", "committedDate": "2020-12-21T22:08:36Z", "type": "forcePushed"}, {"oid": "9861143465230cacb31cf67442e69815e5c10805", "url": "https://github.com/apache/storm/commit/9861143465230cacb31cf67442e69815e5c10805", "message": "[STORM-3388] Launch workers inside container using runc runtime", "committedDate": "2020-12-21T22:13:39Z", "type": "forcePushed"}, {"oid": "e3c37eaf1bfb77932bdf15c0631d5e8768410ebc", "url": "https://github.com/apache/storm/commit/e3c37eaf1bfb77932bdf15c0631d5e8768410ebc", "message": "[STORM-3388] Launch workers inside container using runc runtime", "committedDate": "2020-12-21T22:41:07Z", "type": "forcePushed"}, {"oid": "a92685ab214a332ce206c71e52878fc69a06019b", "url": "https://github.com/apache/storm/commit/a92685ab214a332ce206c71e52878fc69a06019b", "message": "cleanup and fixing travis build", "committedDate": "2020-12-22T04:36:32Z", "type": "forcePushed"}, {"oid": "690e02091244f33b32a1dc3c449413e6aff1d964", "url": "https://github.com/apache/storm/commit/690e02091244f33b32a1dc3c449413e6aff1d964", "message": "[STORM-3388] Launch workers inside container using runc runtime", "committedDate": "2020-12-22T18:55:02Z", "type": "commit"}, {"oid": "690e02091244f33b32a1dc3c449413e6aff1d964", "url": "https://github.com/apache/storm/commit/690e02091244f33b32a1dc3c449413e6aff1d964", "message": "[STORM-3388] Launch workers inside container using runc runtime", "committedDate": "2020-12-22T18:55:02Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY0NzA5Ng==", "url": "https://github.com/apache/storm/pull/3366#discussion_r547647096", "bodyText": "Is this supposed to be part of javadoc on parsing logic?", "author": "bipinprasad", "createdAt": "2020-12-23T05:09:58Z", "path": "external/storm-hdfs-oci/src/main/java/org/apache/storm/container/oci/LocalOrHdfsImageTagToManifestPlugin.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.container.oci;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.utils.HadoopLoginUtil;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class LocalOrHdfsImageTagToManifestPlugin implements OciImageTagToManifestPluginInterface {\n+    private static final Logger LOG = LoggerFactory.getLogger(LocalOrHdfsImageTagToManifestPlugin.class);\n+\n+    private Map<String, ImageManifest> manifestCache;\n+    private ObjectMapper objMapper;\n+    private Map<String, String> localImageToHashCache = new HashMap<>();\n+    private Map<String, String> hdfsImageToHashCache = new HashMap<>();\n+    private Map<String, Object> conf;\n+    private long hdfsModTime;\n+    private long localModTime;\n+    private String hdfsImageToHashFile;\n+    private String manifestDir;\n+    private String localImageTagToHashFile;\n+    private int ociCacheRefreshIntervalSecs;\n+    private long lastRefreshTime;\n+\n+    private static String LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX = \"storm.oci.local.or.hdfs.image.tag.to.manifest.plugin.\";\n+\n+    /**\n+     * The HDFS location where the oci image-tag-to-hash file exists.\n+     */\n+    private static String HDFS_OCI_IMAGE_TAG_TO_HASH_FILE =\n+        LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"hdfs.hash.file\";\n+\n+    /**\n+     * The local file system location where the oci image-tag-to-hash file exists.\n+     */\n+    private static String LOCAL_OCI_IMAGE_TAG_TO_HASH_FILE =\n+        LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"local.hash.file\";\n+\n+    /**\n+     * The interval in seconds between refreshing the oci image-Tag-to-hash cache.\n+     */\n+    private static String OCI_CACHE_REFRESH_INTERVAL =\n+        LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"cache.refresh.interval.secs\";\n+\n+    /**\n+     * The number of manifests to cache.\n+     */\n+    private static String OCI_NUM_MANIFESTS_TO_CACHE = LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"num.manifests.to.cache\";\n+\n+    private static int SHA256_HASH_LENGTH = 64;\n+\n+    private static String ALPHA_NUMERIC = \"[a-zA-Z0-9]+\";\n+\n+    @Override\n+    public void init(Map<String, Object> conf) throws IOException {\n+        this.conf = conf;\n+\n+        //login to hdfs\n+        HadoopLoginUtil.loginHadoop(conf);\n+\n+        localImageTagToHashFile = (String) conf.get(LOCAL_OCI_IMAGE_TAG_TO_HASH_FILE);\n+        if (localImageTagToHashFile == null) {\n+            LOG.debug(\"Failed to load local oci-image-to-hash file. Config not set\");\n+        }\n+        hdfsImageToHashFile = (String) conf.get(HDFS_OCI_IMAGE_TAG_TO_HASH_FILE);\n+        if (hdfsImageToHashFile == null) {\n+            LOG.debug(\"Failed to load HDFS oci-image-to-hash file. Config not set\");\n+        }\n+        if (hdfsImageToHashFile == null && localImageTagToHashFile == null) {\n+            throw new IllegalArgumentException(\"No valid image-tag-to-hash files\");\n+        }\n+        manifestDir = ObjectReader.getString(conf.get(DaemonConfig.STORM_OCI_IMAGE_HDFS_TOPLEVEL_DIR)) + \"/manifests/\";\n+        int numManifestsToCache = ObjectReader.getInt(conf.get(OCI_NUM_MANIFESTS_TO_CACHE), 10);\n+        this.objMapper = new ObjectMapper();\n+        this.manifestCache = new LruCache(numManifestsToCache, 0.75f);\n+        ociCacheRefreshIntervalSecs = ObjectReader.getInt(conf.get(OCI_CACHE_REFRESH_INTERVAL), 60);\n+    }\n+\n+    private boolean loadImageToHashFiles() throws IOException {\n+        boolean ret = false;\n+        try (BufferedReader localBr = getLocalImageToHashReader()) {\n+            Map<String, String> localImageToHash = readImageToHashFile(localBr);\n+            if (localImageToHash != null && !localImageToHash.equals(localImageToHashCache)) {\n+                localImageToHashCache = localImageToHash;\n+                LOG.info(\"Reloaded local image tag to hash cache\");\n+                ret = true;\n+            }\n+        }\n+\n+        try (BufferedReader hdfsBr = getHdfsImageToHashReader()) {\n+            Map<String, String> hdfsImageToHash = readImageToHashFile(hdfsBr);\n+            if (hdfsImageToHash != null && !hdfsImageToHash.equals(hdfsImageToHashCache)) {\n+                hdfsImageToHashCache = hdfsImageToHash;\n+                LOG.info(\"Reloaded hdfs image tag to hash cache\");\n+                ret = true;\n+            }\n+        }\n+        return ret;\n+    }\n+\n+    private BufferedReader getLocalImageToHashReader() throws IOException {\n+        if (localImageTagToHashFile == null) {\n+            LOG.debug(\"Did not load local image to hash file, file is null\");\n+            return null;\n+        }\n+\n+        File imageTagToHashFile = new File(localImageTagToHashFile);\n+        if (!imageTagToHashFile.exists()) {\n+            LOG.warn(\"Did not load local image to hash file, file doesn't exist\");\n+            return null;\n+        }\n+\n+        long newLocalModTime = imageTagToHashFile.lastModified();\n+        if (newLocalModTime == localModTime) {\n+            LOG.debug(\"Did not load local image to hash file, file is unmodified\");\n+            return null;\n+        }\n+        localModTime = newLocalModTime;\n+\n+        return new BufferedReader(new FileReader(imageTagToHashFile));\n+    }\n+\n+    private BufferedReader getHdfsImageToHashReader() throws IOException {\n+        if (hdfsImageToHashFile == null) {\n+            LOG.debug(\"Did not load hdfs image to hash file, file is null\");\n+            return null;\n+        }\n+\n+        Path imageToHash = new Path(hdfsImageToHashFile);\n+        FileSystem fs = imageToHash.getFileSystem(new Configuration());\n+        if (!fs.exists(imageToHash)) {\n+            String message = \"Could not load hdfs image to hash file, \" + hdfsImageToHashFile + \" doesn't exist\";\n+            LOG.error(message);\n+            throw new IOException(message);\n+        }\n+\n+        long newHdfsModTime = fs.getFileStatus(imageToHash).getModificationTime();\n+        if (newHdfsModTime == hdfsModTime) {\n+            LOG.debug(\"Did not load hdfs image to hash file, file is unmodified\");\n+            return null;\n+        }\n+        hdfsModTime = newHdfsModTime;\n+\n+        return new BufferedReader(new InputStreamReader(fs.open(imageToHash)));\n+    }\n+\n+    // You may specify multiple tags per hash all on the same line.\n+    // Comments are allowed using #. Anything after this character will not\n+    // be read\n+    // Example file:\n+    // foo/bar:current,fizz/gig:latest:123456789\n+    // #this/line:wont,be:parsed:2378590895\n+    //\n+    // This will map both foo/bar:current and fizz/gig:latest to 123456789", "originalCommit": "690e02091244f33b32a1dc3c449413e6aff1d964", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NzIyMjMzNw==", "url": "https://github.com/apache/storm/pull/3366#discussion_r667222337", "bodyText": "will convert this to javadoc", "author": "Ethanlm", "createdAt": "2021-07-09T21:35:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY0NzA5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY1MTg3NQ==", "url": "https://github.com/apache/storm/pull/3366#discussion_r547651875", "bodyText": "It will be good to know which file or hdfs url this error is in. Same comment for line 214.", "author": "bipinprasad", "createdAt": "2020-12-23T05:16:22Z", "path": "external/storm-hdfs-oci/src/main/java/org/apache/storm/container/oci/LocalOrHdfsImageTagToManifestPlugin.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.container.oci;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.utils.HadoopLoginUtil;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class LocalOrHdfsImageTagToManifestPlugin implements OciImageTagToManifestPluginInterface {\n+    private static final Logger LOG = LoggerFactory.getLogger(LocalOrHdfsImageTagToManifestPlugin.class);\n+\n+    private Map<String, ImageManifest> manifestCache;\n+    private ObjectMapper objMapper;\n+    private Map<String, String> localImageToHashCache = new HashMap<>();\n+    private Map<String, String> hdfsImageToHashCache = new HashMap<>();\n+    private Map<String, Object> conf;\n+    private long hdfsModTime;\n+    private long localModTime;\n+    private String hdfsImageToHashFile;\n+    private String manifestDir;\n+    private String localImageTagToHashFile;\n+    private int ociCacheRefreshIntervalSecs;\n+    private long lastRefreshTime;\n+\n+    private static String LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX = \"storm.oci.local.or.hdfs.image.tag.to.manifest.plugin.\";\n+\n+    /**\n+     * The HDFS location where the oci image-tag-to-hash file exists.\n+     */\n+    private static String HDFS_OCI_IMAGE_TAG_TO_HASH_FILE =\n+        LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"hdfs.hash.file\";\n+\n+    /**\n+     * The local file system location where the oci image-tag-to-hash file exists.\n+     */\n+    private static String LOCAL_OCI_IMAGE_TAG_TO_HASH_FILE =\n+        LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"local.hash.file\";\n+\n+    /**\n+     * The interval in seconds between refreshing the oci image-Tag-to-hash cache.\n+     */\n+    private static String OCI_CACHE_REFRESH_INTERVAL =\n+        LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"cache.refresh.interval.secs\";\n+\n+    /**\n+     * The number of manifests to cache.\n+     */\n+    private static String OCI_NUM_MANIFESTS_TO_CACHE = LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"num.manifests.to.cache\";\n+\n+    private static int SHA256_HASH_LENGTH = 64;\n+\n+    private static String ALPHA_NUMERIC = \"[a-zA-Z0-9]+\";\n+\n+    @Override\n+    public void init(Map<String, Object> conf) throws IOException {\n+        this.conf = conf;\n+\n+        //login to hdfs\n+        HadoopLoginUtil.loginHadoop(conf);\n+\n+        localImageTagToHashFile = (String) conf.get(LOCAL_OCI_IMAGE_TAG_TO_HASH_FILE);\n+        if (localImageTagToHashFile == null) {\n+            LOG.debug(\"Failed to load local oci-image-to-hash file. Config not set\");\n+        }\n+        hdfsImageToHashFile = (String) conf.get(HDFS_OCI_IMAGE_TAG_TO_HASH_FILE);\n+        if (hdfsImageToHashFile == null) {\n+            LOG.debug(\"Failed to load HDFS oci-image-to-hash file. Config not set\");\n+        }\n+        if (hdfsImageToHashFile == null && localImageTagToHashFile == null) {\n+            throw new IllegalArgumentException(\"No valid image-tag-to-hash files\");\n+        }\n+        manifestDir = ObjectReader.getString(conf.get(DaemonConfig.STORM_OCI_IMAGE_HDFS_TOPLEVEL_DIR)) + \"/manifests/\";\n+        int numManifestsToCache = ObjectReader.getInt(conf.get(OCI_NUM_MANIFESTS_TO_CACHE), 10);\n+        this.objMapper = new ObjectMapper();\n+        this.manifestCache = new LruCache(numManifestsToCache, 0.75f);\n+        ociCacheRefreshIntervalSecs = ObjectReader.getInt(conf.get(OCI_CACHE_REFRESH_INTERVAL), 60);\n+    }\n+\n+    private boolean loadImageToHashFiles() throws IOException {\n+        boolean ret = false;\n+        try (BufferedReader localBr = getLocalImageToHashReader()) {\n+            Map<String, String> localImageToHash = readImageToHashFile(localBr);\n+            if (localImageToHash != null && !localImageToHash.equals(localImageToHashCache)) {\n+                localImageToHashCache = localImageToHash;\n+                LOG.info(\"Reloaded local image tag to hash cache\");\n+                ret = true;\n+            }\n+        }\n+\n+        try (BufferedReader hdfsBr = getHdfsImageToHashReader()) {\n+            Map<String, String> hdfsImageToHash = readImageToHashFile(hdfsBr);\n+            if (hdfsImageToHash != null && !hdfsImageToHash.equals(hdfsImageToHashCache)) {\n+                hdfsImageToHashCache = hdfsImageToHash;\n+                LOG.info(\"Reloaded hdfs image tag to hash cache\");\n+                ret = true;\n+            }\n+        }\n+        return ret;\n+    }\n+\n+    private BufferedReader getLocalImageToHashReader() throws IOException {\n+        if (localImageTagToHashFile == null) {\n+            LOG.debug(\"Did not load local image to hash file, file is null\");\n+            return null;\n+        }\n+\n+        File imageTagToHashFile = new File(localImageTagToHashFile);\n+        if (!imageTagToHashFile.exists()) {\n+            LOG.warn(\"Did not load local image to hash file, file doesn't exist\");\n+            return null;\n+        }\n+\n+        long newLocalModTime = imageTagToHashFile.lastModified();\n+        if (newLocalModTime == localModTime) {\n+            LOG.debug(\"Did not load local image to hash file, file is unmodified\");\n+            return null;\n+        }\n+        localModTime = newLocalModTime;\n+\n+        return new BufferedReader(new FileReader(imageTagToHashFile));\n+    }\n+\n+    private BufferedReader getHdfsImageToHashReader() throws IOException {\n+        if (hdfsImageToHashFile == null) {\n+            LOG.debug(\"Did not load hdfs image to hash file, file is null\");\n+            return null;\n+        }\n+\n+        Path imageToHash = new Path(hdfsImageToHashFile);\n+        FileSystem fs = imageToHash.getFileSystem(new Configuration());\n+        if (!fs.exists(imageToHash)) {\n+            String message = \"Could not load hdfs image to hash file, \" + hdfsImageToHashFile + \" doesn't exist\";\n+            LOG.error(message);\n+            throw new IOException(message);\n+        }\n+\n+        long newHdfsModTime = fs.getFileStatus(imageToHash).getModificationTime();\n+        if (newHdfsModTime == hdfsModTime) {\n+            LOG.debug(\"Did not load hdfs image to hash file, file is unmodified\");\n+            return null;\n+        }\n+        hdfsModTime = newHdfsModTime;\n+\n+        return new BufferedReader(new InputStreamReader(fs.open(imageToHash)));\n+    }\n+\n+    // You may specify multiple tags per hash all on the same line.\n+    // Comments are allowed using #. Anything after this character will not\n+    // be read\n+    // Example file:\n+    // foo/bar:current,fizz/gig:latest:123456789\n+    // #this/line:wont,be:parsed:2378590895\n+    //\n+    // This will map both foo/bar:current and fizz/gig:latest to 123456789\n+    private static Map<String, String> readImageToHashFile(BufferedReader br) throws IOException {\n+        if (br == null) {\n+            return null;\n+        }\n+\n+        String line;\n+        Map<String, String> imageToHashCache = new HashMap<>();\n+        while ((line = br.readLine()) != null) {\n+            int index;\n+            index = line.indexOf(\"#\");\n+            if (index == 0) {\n+                continue;\n+            } else if (index != -1) {\n+                line = line.substring(0, index);\n+            }\n+\n+            index = line.lastIndexOf(\":\");\n+            if (index == -1) {\n+                LOG.warn(\"Malformed imageTagToManifest entry: \" + line);", "originalCommit": "690e02091244f33b32a1dc3c449413e6aff1d964", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY1NTMyOA==", "url": "https://github.com/apache/storm/pull/3366#discussion_r547655328", "bodyText": "The manifestPath seems to be a local file not HDFS.", "author": "bipinprasad", "createdAt": "2020-12-23T05:21:16Z", "path": "external/storm-hdfs-oci/src/main/java/org/apache/storm/container/oci/LocalOrHdfsImageTagToManifestPlugin.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.container.oci;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.utils.HadoopLoginUtil;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class LocalOrHdfsImageTagToManifestPlugin implements OciImageTagToManifestPluginInterface {\n+    private static final Logger LOG = LoggerFactory.getLogger(LocalOrHdfsImageTagToManifestPlugin.class);\n+\n+    private Map<String, ImageManifest> manifestCache;\n+    private ObjectMapper objMapper;\n+    private Map<String, String> localImageToHashCache = new HashMap<>();\n+    private Map<String, String> hdfsImageToHashCache = new HashMap<>();\n+    private Map<String, Object> conf;\n+    private long hdfsModTime;\n+    private long localModTime;\n+    private String hdfsImageToHashFile;\n+    private String manifestDir;\n+    private String localImageTagToHashFile;\n+    private int ociCacheRefreshIntervalSecs;\n+    private long lastRefreshTime;\n+\n+    private static String LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX = \"storm.oci.local.or.hdfs.image.tag.to.manifest.plugin.\";\n+\n+    /**\n+     * The HDFS location where the oci image-tag-to-hash file exists.\n+     */\n+    private static String HDFS_OCI_IMAGE_TAG_TO_HASH_FILE =\n+        LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"hdfs.hash.file\";\n+\n+    /**\n+     * The local file system location where the oci image-tag-to-hash file exists.\n+     */\n+    private static String LOCAL_OCI_IMAGE_TAG_TO_HASH_FILE =\n+        LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"local.hash.file\";\n+\n+    /**\n+     * The interval in seconds between refreshing the oci image-Tag-to-hash cache.\n+     */\n+    private static String OCI_CACHE_REFRESH_INTERVAL =\n+        LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"cache.refresh.interval.secs\";\n+\n+    /**\n+     * The number of manifests to cache.\n+     */\n+    private static String OCI_NUM_MANIFESTS_TO_CACHE = LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"num.manifests.to.cache\";\n+\n+    private static int SHA256_HASH_LENGTH = 64;\n+\n+    private static String ALPHA_NUMERIC = \"[a-zA-Z0-9]+\";\n+\n+    @Override\n+    public void init(Map<String, Object> conf) throws IOException {\n+        this.conf = conf;\n+\n+        //login to hdfs\n+        HadoopLoginUtil.loginHadoop(conf);\n+\n+        localImageTagToHashFile = (String) conf.get(LOCAL_OCI_IMAGE_TAG_TO_HASH_FILE);\n+        if (localImageTagToHashFile == null) {\n+            LOG.debug(\"Failed to load local oci-image-to-hash file. Config not set\");\n+        }\n+        hdfsImageToHashFile = (String) conf.get(HDFS_OCI_IMAGE_TAG_TO_HASH_FILE);\n+        if (hdfsImageToHashFile == null) {\n+            LOG.debug(\"Failed to load HDFS oci-image-to-hash file. Config not set\");\n+        }\n+        if (hdfsImageToHashFile == null && localImageTagToHashFile == null) {\n+            throw new IllegalArgumentException(\"No valid image-tag-to-hash files\");\n+        }\n+        manifestDir = ObjectReader.getString(conf.get(DaemonConfig.STORM_OCI_IMAGE_HDFS_TOPLEVEL_DIR)) + \"/manifests/\";\n+        int numManifestsToCache = ObjectReader.getInt(conf.get(OCI_NUM_MANIFESTS_TO_CACHE), 10);\n+        this.objMapper = new ObjectMapper();\n+        this.manifestCache = new LruCache(numManifestsToCache, 0.75f);\n+        ociCacheRefreshIntervalSecs = ObjectReader.getInt(conf.get(OCI_CACHE_REFRESH_INTERVAL), 60);\n+    }\n+\n+    private boolean loadImageToHashFiles() throws IOException {\n+        boolean ret = false;\n+        try (BufferedReader localBr = getLocalImageToHashReader()) {\n+            Map<String, String> localImageToHash = readImageToHashFile(localBr);\n+            if (localImageToHash != null && !localImageToHash.equals(localImageToHashCache)) {\n+                localImageToHashCache = localImageToHash;\n+                LOG.info(\"Reloaded local image tag to hash cache\");\n+                ret = true;\n+            }\n+        }\n+\n+        try (BufferedReader hdfsBr = getHdfsImageToHashReader()) {\n+            Map<String, String> hdfsImageToHash = readImageToHashFile(hdfsBr);\n+            if (hdfsImageToHash != null && !hdfsImageToHash.equals(hdfsImageToHashCache)) {\n+                hdfsImageToHashCache = hdfsImageToHash;\n+                LOG.info(\"Reloaded hdfs image tag to hash cache\");\n+                ret = true;\n+            }\n+        }\n+        return ret;\n+    }\n+\n+    private BufferedReader getLocalImageToHashReader() throws IOException {\n+        if (localImageTagToHashFile == null) {\n+            LOG.debug(\"Did not load local image to hash file, file is null\");\n+            return null;\n+        }\n+\n+        File imageTagToHashFile = new File(localImageTagToHashFile);\n+        if (!imageTagToHashFile.exists()) {\n+            LOG.warn(\"Did not load local image to hash file, file doesn't exist\");\n+            return null;\n+        }\n+\n+        long newLocalModTime = imageTagToHashFile.lastModified();\n+        if (newLocalModTime == localModTime) {\n+            LOG.debug(\"Did not load local image to hash file, file is unmodified\");\n+            return null;\n+        }\n+        localModTime = newLocalModTime;\n+\n+        return new BufferedReader(new FileReader(imageTagToHashFile));\n+    }\n+\n+    private BufferedReader getHdfsImageToHashReader() throws IOException {\n+        if (hdfsImageToHashFile == null) {\n+            LOG.debug(\"Did not load hdfs image to hash file, file is null\");\n+            return null;\n+        }\n+\n+        Path imageToHash = new Path(hdfsImageToHashFile);\n+        FileSystem fs = imageToHash.getFileSystem(new Configuration());\n+        if (!fs.exists(imageToHash)) {\n+            String message = \"Could not load hdfs image to hash file, \" + hdfsImageToHashFile + \" doesn't exist\";\n+            LOG.error(message);\n+            throw new IOException(message);\n+        }\n+\n+        long newHdfsModTime = fs.getFileStatus(imageToHash).getModificationTime();\n+        if (newHdfsModTime == hdfsModTime) {\n+            LOG.debug(\"Did not load hdfs image to hash file, file is unmodified\");\n+            return null;\n+        }\n+        hdfsModTime = newHdfsModTime;\n+\n+        return new BufferedReader(new InputStreamReader(fs.open(imageToHash)));\n+    }\n+\n+    // You may specify multiple tags per hash all on the same line.\n+    // Comments are allowed using #. Anything after this character will not\n+    // be read\n+    // Example file:\n+    // foo/bar:current,fizz/gig:latest:123456789\n+    // #this/line:wont,be:parsed:2378590895\n+    //\n+    // This will map both foo/bar:current and fizz/gig:latest to 123456789\n+    private static Map<String, String> readImageToHashFile(BufferedReader br) throws IOException {\n+        if (br == null) {\n+            return null;\n+        }\n+\n+        String line;\n+        Map<String, String> imageToHashCache = new HashMap<>();\n+        while ((line = br.readLine()) != null) {\n+            int index;\n+            index = line.indexOf(\"#\");\n+            if (index == 0) {\n+                continue;\n+            } else if (index != -1) {\n+                line = line.substring(0, index);\n+            }\n+\n+            index = line.lastIndexOf(\":\");\n+            if (index == -1) {\n+                LOG.warn(\"Malformed imageTagToManifest entry: \" + line);\n+                continue;\n+            }\n+            String imageTags = line.substring(0, index);\n+            String[] imageTagArray = imageTags.split(\",\");\n+            String hash = line.substring(index + 1);\n+\n+            if (!hash.matches(ALPHA_NUMERIC) || hash.length() != SHA256_HASH_LENGTH) {\n+                LOG.warn(\"Malformed image hash: \" + hash);\n+                continue;\n+            }\n+\n+            for (String imageTag : imageTagArray) {\n+                imageToHashCache.put(imageTag, hash);\n+            }\n+        }\n+        return imageToHashCache;\n+    }\n+\n+\n+    @Override\n+    public synchronized ImageManifest getManifestFromImageTag(String imageTag) throws IOException {\n+        String hash = getHashFromImageTag(imageTag);\n+        ImageManifest manifest = manifestCache.get(hash);\n+        if (manifest != null) {\n+            return manifest;\n+        }\n+        Path manifestPath = new Path(manifestDir + hash);\n+        FileSystem fs = manifestPath.getFileSystem(new Configuration());\n+        FSDataInputStream input;\n+        try {\n+            input = fs.open(manifestPath);\n+        } catch (IllegalArgumentException iae) {\n+            throw new IOException(\"Manifest file is not a valid HDFS file: \"\n+                + manifestPath.toString(), iae);", "originalCommit": "690e02091244f33b32a1dc3c449413e6aff1d964", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NzIyMzEyNw==", "url": "https://github.com/apache/storm/pull/3366#discussion_r667223127", "bodyText": "This is on HDFS.\nsee\nmanifestDir = ObjectReader.getString(conf.get(DaemonConfig.STORM_OCI_IMAGE_HDFS_TOPLEVEL_DIR)) + \"/manifests/\";", "author": "Ethanlm", "createdAt": "2021-07-09T21:37:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY1NTMyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY2Mzc4Mg==", "url": "https://github.com/apache/storm/pull/3366#discussion_r547663782", "bodyText": "does this \"final\" do anything here?", "author": "bipinprasad", "createdAt": "2020-12-23T05:32:55Z", "path": "storm-client/src/jvm/org/apache/storm/daemon/supervisor/ClientSupervisorUtils.java", "diffHunk": "@@ -61,10 +61,15 @@ static boolean doRequiredTopoFilesExist(Map<String, Object> conf, String stormId\n     }\n \n     public static int processLauncherAndWait(Map<String, Object> conf, String user, List<String> args,\n-                                             final Map<String, String> environment, final String logPreFix)\n+                                             final Map<String, String> environment, final String logPreFix) throws IOException {", "originalCommit": "690e02091244f33b32a1dc3c449413e6aff1d964", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NzIyMzg1MQ==", "url": "https://github.com/apache/storm/pull/3366#discussion_r667223851", "bodyText": "I think it prevents the variable to be changed inside the method. Don't think it is very important so just left it as it is", "author": "Ethanlm", "createdAt": "2021-07-09T21:39:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzY2Mzc4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQ5NTg4OQ==", "url": "https://github.com/apache/storm/pull/3366#discussion_r548495889", "bodyText": "Code duplicated in getConfigResource(). Should this be a method?", "author": "bipinprasad", "createdAt": "2020-12-24T11:09:23Z", "path": "external/storm-hdfs-oci/src/main/java/org/apache/storm/container/oci/HdfsManifestToResourcesPlugin.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.container.oci;\n+\n+import com.google.common.cache.CacheBuilder;\n+import com.google.common.cache.CacheLoader;\n+import com.google.common.cache.LoadingCache;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nonnull;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.utils.HadoopLoginUtil;\n+import org.apache.storm.utils.ObjectReader;\n+\n+public class HdfsManifestToResourcesPlugin implements OciManifestToResourcesPluginInterface {\n+\n+    private String layersDir;\n+    private String configDir;\n+    private FileSystem fs;\n+    private LoadingCache<Path, FileStatus> statCache;\n+\n+    private static String CONFIG_MEDIA_TYPE = \"application/vnd.docker.container.image.v1+json\";\n+\n+    private static String LAYER_TAR_GZIP_MEDIA_TYPE = \"application/vnd.docker.image.rootfs.diff.tar.gzip\";\n+\n+    private static String SHA_256 = \"sha256\";\n+\n+    private static String CONFIG_HASH_ALGORITHM = SHA_256;\n+\n+    private static String LAYER_HASH_ALGORITHM = SHA_256;\n+\n+    private static int SHA256_HASH_LENGTH = 64;\n+\n+    private static String ALPHA_NUMERIC = \"[a-zA-Z0-9]+\";\n+\n+    @Override\n+    public void init(Map<String, Object> conf) throws IOException {\n+\n+        //login to hdfs\n+        HadoopLoginUtil.loginHadoop(conf);\n+\n+        String topLevelDir = ObjectReader.getString(conf.get(DaemonConfig.STORM_OCI_IMAGE_HDFS_TOPLEVEL_DIR));\n+\n+        this.layersDir = topLevelDir + \"/layers/\";\n+        this.configDir = topLevelDir + \"/config/\";\n+\n+        this.fs = new Path(topLevelDir).getFileSystem(new Configuration());\n+\n+        CacheLoader<Path, FileStatus> cacheLoader =\n+            new CacheLoader<Path, FileStatus>() {\n+                @Override\n+                public FileStatus load(@Nonnull Path path) throws Exception {\n+                    return statBlob(path);\n+                }\n+            };\n+        this.statCache = CacheBuilder.newBuilder().maximumSize(30)\n+            .refreshAfterWrite(60, TimeUnit.MINUTES).build(cacheLoader);\n+    }\n+\n+    @Override\n+    public List<OciResource> getLayerResources(ImageManifest manifest) throws IOException {\n+        List<OciResource> ociResources = new ArrayList<>();\n+        for (ImageManifest.Blob blob : manifest.getLayers()) {\n+            String mediaType = blob.getMediaType();\n+            if (!mediaType.equals(LAYER_TAR_GZIP_MEDIA_TYPE)) {\n+                throw new IOException(\"Invalid config mediaType: \" + mediaType);\n+            }\n+\n+            String[] layerDigest = blob.getDigest().split(\":\", 2);\n+            String algorithm = layerDigest[0];\n+            if (!algorithm.equals(LAYER_HASH_ALGORITHM)) {\n+                throw new IOException(\"Invalid config digest algorithm: \" + algorithm);\n+            }\n+\n+            String hash = layerDigest[1];\n+            if (!hash.matches(ALPHA_NUMERIC) || hash.length() != SHA256_HASH_LENGTH) {\n+                throw new IOException(\"Malformed layer digest: \" + hash);\n+            }\n+", "originalCommit": "690e02091244f33b32a1dc3c449413e6aff1d964", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3MTkyMjYyMQ==", "url": "https://github.com/apache/storm/pull/3366#discussion_r671922621", "bodyText": "There are many variables involved. using one method is fine but might explore it as a follow-up", "author": "Ethanlm", "createdAt": "2021-07-19T00:29:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQ5NTg4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQ5NjQxMA==", "url": "https://github.com/apache/storm/pull/3366#discussion_r548496410", "bodyText": "These can be final", "author": "bipinprasad", "createdAt": "2020-12-24T11:11:41Z", "path": "external/storm-hdfs-oci/src/main/java/org/apache/storm/container/oci/HdfsManifestToResourcesPlugin.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.container.oci;\n+\n+import com.google.common.cache.CacheBuilder;\n+import com.google.common.cache.CacheLoader;\n+import com.google.common.cache.LoadingCache;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nonnull;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.utils.HadoopLoginUtil;\n+import org.apache.storm.utils.ObjectReader;\n+\n+public class HdfsManifestToResourcesPlugin implements OciManifestToResourcesPluginInterface {\n+\n+    private String layersDir;\n+    private String configDir;\n+    private FileSystem fs;\n+    private LoadingCache<Path, FileStatus> statCache;\n+\n+    private static String CONFIG_MEDIA_TYPE = \"application/vnd.docker.container.image.v1+json\";\n+\n+    private static String LAYER_TAR_GZIP_MEDIA_TYPE = \"application/vnd.docker.image.rootfs.diff.tar.gzip\";\n+\n+    private static String SHA_256 = \"sha256\";\n+\n+    private static String CONFIG_HASH_ALGORITHM = SHA_256;\n+\n+    private static String LAYER_HASH_ALGORITHM = SHA_256;\n+\n+    private static int SHA256_HASH_LENGTH = 64;\n+\n+    private static String ALPHA_NUMERIC = \"[a-zA-Z0-9]+\";", "originalCommit": "690e02091244f33b32a1dc3c449413e6aff1d964", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQ5NzM4MQ==", "url": "https://github.com/apache/storm/pull/3366#discussion_r548497381", "bodyText": "nit: return value is not checked for failure", "author": "bipinprasad", "createdAt": "2020-12-24T11:15:16Z", "path": "external/storm-hdfs-oci/src/main/java/org/apache/storm/container/oci/HdfsOciResourcesLocalizer.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.container.oci;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Map;\n+\n+import org.apache.commons.io.FileDeleteStrategy;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.utils.ConfigUtils;\n+import org.apache.storm.utils.HadoopLoginUtil;\n+import org.apache.storm.utils.ObjectReader;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HdfsOciResourcesLocalizer implements OciResourcesLocalizerInterface {\n+    private static final Logger LOG = LoggerFactory.getLogger(HdfsOciResourcesLocalizer.class);\n+    private static final int LOCALIZE_MAX_RETRY = 5;\n+    private String layersLocalDir;\n+    private String configLocalDir;\n+    private FileSystem fs;\n+\n+    /**\n+     * Initialization.\n+     * @param conf the storm conf.\n+     * @throws IOException on I/O exception\n+     */\n+    public void init(Map<String, Object> conf) throws IOException {\n+        //login to hdfs\n+        HadoopLoginUtil.loginHadoop(conf);\n+\n+        String resourcesLocalDir = ObjectReader.getString(conf.get(DaemonConfig.STORM_OCI_RESOURCES_LOCAL_DIR),\n+            ConfigUtils.supervisorLocalDir(conf) + \"/oci-resources\");\n+        FileUtils.forceMkdir(new File(resourcesLocalDir));\n+        this.layersLocalDir = resourcesLocalDir + \"/layers/\";\n+        this.configLocalDir = resourcesLocalDir + \"/config/\";\n+        String topLevelDir = ObjectReader.getString(conf.get(DaemonConfig.STORM_OCI_IMAGE_HDFS_TOPLEVEL_DIR));\n+        this.fs = new Path(topLevelDir).getFileSystem(new Configuration());\n+    }\n+\n+    /**\n+     * Download the resources from HDFS to local dir.\n+     * @param ociResource The oci resource to download\n+     * @return the destination of the oci resource\n+     * @throws IOException on I/O exception\n+     */\n+    public synchronized String localize(OciResource ociResource) throws IOException {\n+        if (ociResource == null) {\n+            return null;\n+        }\n+        File dst;\n+        switch (ociResource.getType()) {\n+            case CONFIG:\n+                dst = new File(this.configLocalDir, ociResource.getFileName());\n+                break;\n+            case LAYER:\n+                dst = new File(layersLocalDir, ociResource.getFileName());\n+                break;\n+            default:\n+                throw new IOException(\"unknown OciResourceType \" + ociResource.getType());\n+        }\n+\n+        if (dst.exists()) {\n+            LOG.info(\"{} already exists. Skip\", dst);\n+        } else {\n+            // create working dir, copy file here, and set readable, then move to final location.\n+            // this allows the operation to be atomic in case the supervisor dies.\n+            File workingDir = new File(dst.getParent() + \"/working\");\n+            workingDir.mkdir();", "originalCommit": "690e02091244f33b32a1dc3c449413e6aff1d964", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQ5NzQyNQ==", "url": "https://github.com/apache/storm/pull/3366#discussion_r548497425", "bodyText": "nit: return value is not checked for failure", "author": "bipinprasad", "createdAt": "2020-12-24T11:15:28Z", "path": "external/storm-hdfs-oci/src/main/java/org/apache/storm/container/oci/HdfsOciResourcesLocalizer.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.container.oci;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Map;\n+\n+import org.apache.commons.io.FileDeleteStrategy;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.utils.ConfigUtils;\n+import org.apache.storm.utils.HadoopLoginUtil;\n+import org.apache.storm.utils.ObjectReader;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HdfsOciResourcesLocalizer implements OciResourcesLocalizerInterface {\n+    private static final Logger LOG = LoggerFactory.getLogger(HdfsOciResourcesLocalizer.class);\n+    private static final int LOCALIZE_MAX_RETRY = 5;\n+    private String layersLocalDir;\n+    private String configLocalDir;\n+    private FileSystem fs;\n+\n+    /**\n+     * Initialization.\n+     * @param conf the storm conf.\n+     * @throws IOException on I/O exception\n+     */\n+    public void init(Map<String, Object> conf) throws IOException {\n+        //login to hdfs\n+        HadoopLoginUtil.loginHadoop(conf);\n+\n+        String resourcesLocalDir = ObjectReader.getString(conf.get(DaemonConfig.STORM_OCI_RESOURCES_LOCAL_DIR),\n+            ConfigUtils.supervisorLocalDir(conf) + \"/oci-resources\");\n+        FileUtils.forceMkdir(new File(resourcesLocalDir));\n+        this.layersLocalDir = resourcesLocalDir + \"/layers/\";\n+        this.configLocalDir = resourcesLocalDir + \"/config/\";\n+        String topLevelDir = ObjectReader.getString(conf.get(DaemonConfig.STORM_OCI_IMAGE_HDFS_TOPLEVEL_DIR));\n+        this.fs = new Path(topLevelDir).getFileSystem(new Configuration());\n+    }\n+\n+    /**\n+     * Download the resources from HDFS to local dir.\n+     * @param ociResource The oci resource to download\n+     * @return the destination of the oci resource\n+     * @throws IOException on I/O exception\n+     */\n+    public synchronized String localize(OciResource ociResource) throws IOException {\n+        if (ociResource == null) {\n+            return null;\n+        }\n+        File dst;\n+        switch (ociResource.getType()) {\n+            case CONFIG:\n+                dst = new File(this.configLocalDir, ociResource.getFileName());\n+                break;\n+            case LAYER:\n+                dst = new File(layersLocalDir, ociResource.getFileName());\n+                break;\n+            default:\n+                throw new IOException(\"unknown OciResourceType \" + ociResource.getType());\n+        }\n+\n+        if (dst.exists()) {\n+            LOG.info(\"{} already exists. Skip\", dst);\n+        } else {\n+            // create working dir, copy file here, and set readable, then move to final location.\n+            // this allows the operation to be atomic in case the supervisor dies.\n+            File workingDir = new File(dst.getParent() + \"/working\");\n+            workingDir.mkdir();\n+            File workingDst = new File(workingDir.getPath() + \"/\" + dst.getName());\n+\n+            LOG.info(\"Starting to copy {} from hdfs to {}\", ociResource.getPath(), workingDst.toString());\n+            copyFileLocallyWithRetry(ociResource, workingDst);\n+            LOG.info(\"Successfully finished copying {} from hdfs to {}\", ociResource.getPath(), workingDst.toString());\n+\n+            //set to readable by anyone\n+            boolean setReadable = workingDst.setReadable(true, false);\n+            if (!setReadable) {\n+                throw new IOException(\"Couldn't set \" + workingDst + \" to be world-readable\");\n+            }\n+            workingDst.renameTo(dst);", "originalCommit": "690e02091244f33b32a1dc3c449413e6aff1d964", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQ5ODcxOQ==", "url": "https://github.com/apache/storm/pull/3366#discussion_r548498719", "bodyText": "Can be final", "author": "bipinprasad", "createdAt": "2020-12-24T11:19:37Z", "path": "external/storm-hdfs-oci/src/main/java/org/apache/storm/container/oci/LocalOrHdfsImageTagToManifestPlugin.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.container.oci;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.utils.HadoopLoginUtil;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class LocalOrHdfsImageTagToManifestPlugin implements OciImageTagToManifestPluginInterface {\n+    private static final Logger LOG = LoggerFactory.getLogger(LocalOrHdfsImageTagToManifestPlugin.class);\n+\n+    private Map<String, ImageManifest> manifestCache;\n+    private ObjectMapper objMapper;\n+    private Map<String, String> localImageToHashCache = new HashMap<>();\n+    private Map<String, String> hdfsImageToHashCache = new HashMap<>();\n+    private Map<String, Object> conf;\n+    private long hdfsModTime;\n+    private long localModTime;\n+    private String hdfsImageToHashFile;\n+    private String manifestDir;\n+    private String localImageTagToHashFile;\n+    private int ociCacheRefreshIntervalSecs;\n+    private long lastRefreshTime;\n+\n+    private static String LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX = \"storm.oci.local.or.hdfs.image.tag.to.manifest.plugin.\";\n+\n+    /**\n+     * The HDFS location where the oci image-tag-to-hash file exists.\n+     */\n+    private static String HDFS_OCI_IMAGE_TAG_TO_HASH_FILE =\n+        LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"hdfs.hash.file\";\n+\n+    /**\n+     * The local file system location where the oci image-tag-to-hash file exists.\n+     */\n+    private static String LOCAL_OCI_IMAGE_TAG_TO_HASH_FILE =\n+        LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"local.hash.file\";\n+\n+    /**\n+     * The interval in seconds between refreshing the oci image-Tag-to-hash cache.\n+     */\n+    private static String OCI_CACHE_REFRESH_INTERVAL =\n+        LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"cache.refresh.interval.secs\";\n+\n+    /**\n+     * The number of manifests to cache.\n+     */\n+    private static String OCI_NUM_MANIFESTS_TO_CACHE = LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"num.manifests.to.cache\";\n+\n+    private static int SHA256_HASH_LENGTH = 64;\n+\n+    private static String ALPHA_NUMERIC = \"[a-zA-Z0-9]+\";", "originalCommit": "690e02091244f33b32a1dc3c449413e6aff1d964", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQ5OTA0NQ==", "url": "https://github.com/apache/storm/pull/3366#discussion_r548499045", "bodyText": "Class can be static", "author": "bipinprasad", "createdAt": "2020-12-24T11:20:46Z", "path": "external/storm-hdfs-oci/src/main/java/org/apache/storm/container/oci/LocalOrHdfsImageTagToManifestPlugin.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.container.oci;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.utils.HadoopLoginUtil;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class LocalOrHdfsImageTagToManifestPlugin implements OciImageTagToManifestPluginInterface {\n+    private static final Logger LOG = LoggerFactory.getLogger(LocalOrHdfsImageTagToManifestPlugin.class);\n+\n+    private Map<String, ImageManifest> manifestCache;\n+    private ObjectMapper objMapper;\n+    private Map<String, String> localImageToHashCache = new HashMap<>();\n+    private Map<String, String> hdfsImageToHashCache = new HashMap<>();\n+    private Map<String, Object> conf;\n+    private long hdfsModTime;\n+    private long localModTime;\n+    private String hdfsImageToHashFile;\n+    private String manifestDir;\n+    private String localImageTagToHashFile;\n+    private int ociCacheRefreshIntervalSecs;\n+    private long lastRefreshTime;\n+\n+    private static String LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX = \"storm.oci.local.or.hdfs.image.tag.to.manifest.plugin.\";\n+\n+    /**\n+     * The HDFS location where the oci image-tag-to-hash file exists.\n+     */\n+    private static String HDFS_OCI_IMAGE_TAG_TO_HASH_FILE =\n+        LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"hdfs.hash.file\";\n+\n+    /**\n+     * The local file system location where the oci image-tag-to-hash file exists.\n+     */\n+    private static String LOCAL_OCI_IMAGE_TAG_TO_HASH_FILE =\n+        LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"local.hash.file\";\n+\n+    /**\n+     * The interval in seconds between refreshing the oci image-Tag-to-hash cache.\n+     */\n+    private static String OCI_CACHE_REFRESH_INTERVAL =\n+        LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"cache.refresh.interval.secs\";\n+\n+    /**\n+     * The number of manifests to cache.\n+     */\n+    private static String OCI_NUM_MANIFESTS_TO_CACHE = LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"num.manifests.to.cache\";\n+\n+    private static int SHA256_HASH_LENGTH = 64;\n+\n+    private static String ALPHA_NUMERIC = \"[a-zA-Z0-9]+\";\n+\n+    @Override\n+    public void init(Map<String, Object> conf) throws IOException {\n+        this.conf = conf;\n+\n+        //login to hdfs\n+        HadoopLoginUtil.loginHadoop(conf);\n+\n+        localImageTagToHashFile = (String) conf.get(LOCAL_OCI_IMAGE_TAG_TO_HASH_FILE);\n+        if (localImageTagToHashFile == null) {\n+            LOG.debug(\"Failed to load local oci-image-to-hash file. Config not set\");\n+        }\n+        hdfsImageToHashFile = (String) conf.get(HDFS_OCI_IMAGE_TAG_TO_HASH_FILE);\n+        if (hdfsImageToHashFile == null) {\n+            LOG.debug(\"Failed to load HDFS oci-image-to-hash file. Config not set\");\n+        }\n+        if (hdfsImageToHashFile == null && localImageTagToHashFile == null) {\n+            throw new IllegalArgumentException(\"No valid image-tag-to-hash files\");\n+        }\n+        manifestDir = ObjectReader.getString(conf.get(DaemonConfig.STORM_OCI_IMAGE_HDFS_TOPLEVEL_DIR)) + \"/manifests/\";\n+        int numManifestsToCache = ObjectReader.getInt(conf.get(OCI_NUM_MANIFESTS_TO_CACHE), 10);\n+        this.objMapper = new ObjectMapper();\n+        this.manifestCache = new LruCache(numManifestsToCache, 0.75f);\n+        ociCacheRefreshIntervalSecs = ObjectReader.getInt(conf.get(OCI_CACHE_REFRESH_INTERVAL), 60);\n+    }\n+\n+    private boolean loadImageToHashFiles() throws IOException {\n+        boolean ret = false;\n+        try (BufferedReader localBr = getLocalImageToHashReader()) {\n+            Map<String, String> localImageToHash = readImageToHashFile(localBr);\n+            if (localImageToHash != null && !localImageToHash.equals(localImageToHashCache)) {\n+                localImageToHashCache = localImageToHash;\n+                LOG.info(\"Reloaded local image tag to hash cache\");\n+                ret = true;\n+            }\n+        }\n+\n+        try (BufferedReader hdfsBr = getHdfsImageToHashReader()) {\n+            Map<String, String> hdfsImageToHash = readImageToHashFile(hdfsBr);\n+            if (hdfsImageToHash != null && !hdfsImageToHash.equals(hdfsImageToHashCache)) {\n+                hdfsImageToHashCache = hdfsImageToHash;\n+                LOG.info(\"Reloaded hdfs image tag to hash cache\");\n+                ret = true;\n+            }\n+        }\n+        return ret;\n+    }\n+\n+    private BufferedReader getLocalImageToHashReader() throws IOException {\n+        if (localImageTagToHashFile == null) {\n+            LOG.debug(\"Did not load local image to hash file, file is null\");\n+            return null;\n+        }\n+\n+        File imageTagToHashFile = new File(localImageTagToHashFile);\n+        if (!imageTagToHashFile.exists()) {\n+            LOG.warn(\"Did not load local image to hash file, file doesn't exist\");\n+            return null;\n+        }\n+\n+        long newLocalModTime = imageTagToHashFile.lastModified();\n+        if (newLocalModTime == localModTime) {\n+            LOG.debug(\"Did not load local image to hash file, file is unmodified\");\n+            return null;\n+        }\n+        localModTime = newLocalModTime;\n+\n+        return new BufferedReader(new FileReader(imageTagToHashFile));\n+    }\n+\n+    private BufferedReader getHdfsImageToHashReader() throws IOException {\n+        if (hdfsImageToHashFile == null) {\n+            LOG.debug(\"Did not load hdfs image to hash file, file is null\");\n+            return null;\n+        }\n+\n+        Path imageToHash = new Path(hdfsImageToHashFile);\n+        FileSystem fs = imageToHash.getFileSystem(new Configuration());\n+        if (!fs.exists(imageToHash)) {\n+            String message = \"Could not load hdfs image to hash file, \" + hdfsImageToHashFile + \" doesn't exist\";\n+            LOG.error(message);\n+            throw new IOException(message);\n+        }\n+\n+        long newHdfsModTime = fs.getFileStatus(imageToHash).getModificationTime();\n+        if (newHdfsModTime == hdfsModTime) {\n+            LOG.debug(\"Did not load hdfs image to hash file, file is unmodified\");\n+            return null;\n+        }\n+        hdfsModTime = newHdfsModTime;\n+\n+        return new BufferedReader(new InputStreamReader(fs.open(imageToHash)));\n+    }\n+\n+    // You may specify multiple tags per hash all on the same line.\n+    // Comments are allowed using #. Anything after this character will not\n+    // be read\n+    // Example file:\n+    // foo/bar:current,fizz/gig:latest:123456789\n+    // #this/line:wont,be:parsed:2378590895\n+    //\n+    // This will map both foo/bar:current and fizz/gig:latest to 123456789\n+    private static Map<String, String> readImageToHashFile(BufferedReader br) throws IOException {\n+        if (br == null) {\n+            return null;\n+        }\n+\n+        String line;\n+        Map<String, String> imageToHashCache = new HashMap<>();\n+        while ((line = br.readLine()) != null) {\n+            int index;\n+            index = line.indexOf(\"#\");\n+            if (index == 0) {\n+                continue;\n+            } else if (index != -1) {\n+                line = line.substring(0, index);\n+            }\n+\n+            index = line.lastIndexOf(\":\");\n+            if (index == -1) {\n+                LOG.warn(\"Malformed imageTagToManifest entry: \" + line);\n+                continue;\n+            }\n+            String imageTags = line.substring(0, index);\n+            String[] imageTagArray = imageTags.split(\",\");\n+            String hash = line.substring(index + 1);\n+\n+            if (!hash.matches(ALPHA_NUMERIC) || hash.length() != SHA256_HASH_LENGTH) {\n+                LOG.warn(\"Malformed image hash: \" + hash);\n+                continue;\n+            }\n+\n+            for (String imageTag : imageTagArray) {\n+                imageToHashCache.put(imageTag, hash);\n+            }\n+        }\n+        return imageToHashCache;\n+    }\n+\n+\n+    @Override\n+    public synchronized ImageManifest getManifestFromImageTag(String imageTag) throws IOException {\n+        String hash = getHashFromImageTag(imageTag);\n+        ImageManifest manifest = manifestCache.get(hash);\n+        if (manifest != null) {\n+            return manifest;\n+        }\n+        Path manifestPath = new Path(manifestDir + hash);\n+        FileSystem fs = manifestPath.getFileSystem(new Configuration());\n+        FSDataInputStream input;\n+        try {\n+            input = fs.open(manifestPath);\n+        } catch (IllegalArgumentException iae) {\n+            throw new IOException(\"Manifest file is not a valid HDFS file: \"\n+                + manifestPath.toString(), iae);\n+        }\n+\n+        byte[] bytes = IOUtils.toByteArray(input);\n+        manifest = objMapper.readValue(bytes, ImageManifest.class);\n+\n+        manifestCache.put(hash, manifest);\n+        return manifest;\n+    }\n+\n+    @Override\n+    public synchronized String getHashFromImageTag(String imageTag) {\n+        String hash;\n+\n+        long currentTime = System.currentTimeMillis();\n+        if (currentTime - lastRefreshTime > Time.secsToMillis(ociCacheRefreshIntervalSecs)) {\n+            LOG.debug(\"Refreshing local and hdfs image-tag-to-hash cache\");\n+            try {\n+                boolean loaded = loadImageToHashFiles();\n+                //If this is the first time trying to load the files and yet it failed\n+                if (!loaded && lastRefreshTime == 0) {\n+                    throw new RuntimeException(\"Couldn't load any image-tag-to-hash-files\");\n+                }\n+                lastRefreshTime = currentTime;\n+            } catch (IOException e) {\n+                throw new RuntimeException(\"Couldn't load any image-tag-to-hash-files\", e);\n+            }\n+        }\n+\n+        // 1) Go to local file\n+        // 2) Go to HDFS\n+        // 3) Use tag as is/Assume tag is the hash\n+        if ((hash = localImageToHashCache.get(imageTag)) != null) {\n+            return hash;\n+        } else if ((hash = hdfsImageToHashCache.get(imageTag)) != null) {\n+            return hash;\n+        } else {\n+            return imageTag;\n+        }\n+    }\n+\n+    private class LruCache extends LinkedHashMap<String, ImageManifest> {", "originalCommit": "690e02091244f33b32a1dc3c449413e6aff1d964", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODU0MTAxNw==", "url": "https://github.com/apache/storm/pull/3366#discussion_r548541017", "bodyText": "containerCredentialsPath, localDirs and logDirs are unused vars.\nOther unused methods appear to be getPidFile(), getContainerScriptPath(), getReapLayerKeepCount(), getOciRuntimeConfig()", "author": "bipinprasad", "createdAt": "2020-12-24T14:12:17Z", "path": "storm-server/src/main/java/org/apache/storm/container/oci/OciContainerExecutorConfig.java", "diffHunk": "@@ -0,0 +1,1309 @@\n+/*\n+ *\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.storm.container.oci;\n+\n+import com.fasterxml.jackson.annotation.JsonInclude;\n+import com.fasterxml.jackson.annotation.JsonRawValue;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+@JsonInclude(JsonInclude.Include.NON_DEFAULT)\n+public class OciContainerExecutorConfig {\n+    private final String version;\n+    private final String username;\n+    private final String containerId;\n+    private final String pidFile;\n+    private final String containerScriptPath;\n+    private final List<OciLayer> layers;\n+    private final int reapLayerKeepCount;\n+    private final OciRuntimeConfig ociRuntimeConfig;\n+\n+    public OciContainerExecutorConfig() {\n+        this(null, null, null, null, null, null, 0, null);\n+    }\n+\n+    public OciContainerExecutorConfig(String username,\n+                                      String containerId,\n+                                      String pidFile, String containerScriptPath, String containerCredentialsPath,\n+                                      List<String> localDirs,\n+                                      List<String> logDirs, List<OciLayer> layers, int reapLayerKeepCount,", "originalCommit": "690e02091244f33b32a1dc3c449413e6aff1d964", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODU0MTg2MQ==", "url": "https://github.com/apache/storm/pull/3366#discussion_r548541861", "bodyText": "Unused constructor.", "author": "bipinprasad", "createdAt": "2020-12-24T14:16:03Z", "path": "storm-server/src/main/java/org/apache/storm/container/oci/OciContainerExecutorConfig.java", "diffHunk": "@@ -0,0 +1,1309 @@\n+/*\n+ *\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.storm.container.oci;\n+\n+import com.fasterxml.jackson.annotation.JsonInclude;\n+import com.fasterxml.jackson.annotation.JsonRawValue;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+@JsonInclude(JsonInclude.Include.NON_DEFAULT)\n+public class OciContainerExecutorConfig {\n+    private final String version;\n+    private final String username;\n+    private final String containerId;\n+    private final String pidFile;\n+    private final String containerScriptPath;\n+    private final List<OciLayer> layers;\n+    private final int reapLayerKeepCount;\n+    private final OciRuntimeConfig ociRuntimeConfig;\n+\n+    public OciContainerExecutorConfig() {\n+        this(null, null, null, null, null, null, 0, null);\n+    }\n+\n+    public OciContainerExecutorConfig(String username,\n+                                      String containerId,\n+                                      String pidFile, String containerScriptPath, String containerCredentialsPath,\n+                                      List<String> localDirs,\n+                                      List<String> logDirs, List<OciLayer> layers, int reapLayerKeepCount,\n+                                      OciRuntimeConfig ociRuntimeConfig) {\n+        this(\"0.1\", username, containerId, pidFile,\n+            containerScriptPath, layers, reapLayerKeepCount, ociRuntimeConfig);\n+    }\n+\n+    public OciContainerExecutorConfig(String version, String username,\n+                                      String containerId,\n+                                      String pidFile, String containerScriptPath,\n+                                      List<OciLayer> layers, int reapLayerKeepCount,\n+                                      OciRuntimeConfig ociRuntimeConfig) {\n+        this.version = version;\n+        this.username = username;\n+        this.containerId = containerId;\n+        this.pidFile = pidFile;\n+        this.containerScriptPath = containerScriptPath;\n+        this.layers = layers;\n+        this.reapLayerKeepCount = reapLayerKeepCount;\n+        this.ociRuntimeConfig = ociRuntimeConfig;\n+    }\n+\n+    public String getVersion() {\n+        return version;\n+    }\n+\n+    public String getUsername() {\n+        return username;\n+    }\n+\n+    public String getContainerId() {\n+        return containerId;\n+    }\n+\n+    public String getPidFile() {\n+        return pidFile;\n+    }\n+\n+    public String getContainerScriptPath() {\n+        return containerScriptPath;\n+    }\n+\n+    public List<OciLayer> getLayers() {\n+        return layers;\n+    }\n+\n+    public int getReapLayerKeepCount() {\n+        return reapLayerKeepCount;\n+    }\n+\n+    public OciRuntimeConfig getOciRuntimeConfig() {\n+        return ociRuntimeConfig;\n+    }\n+\n+    @JsonInclude(JsonInclude.Include.NON_DEFAULT)\n+    public static class OciLayer {\n+        private final String mediaType;\n+        private final String path;\n+\n+        public OciLayer(String mediaType, String path) {\n+            this.mediaType = mediaType;\n+            this.path = path;\n+        }\n+\n+        public OciLayer() {\n+            this(null, null);\n+        }\n+\n+        public String getMediaType() {\n+            return mediaType;\n+        }\n+\n+        public String getPath() {\n+            return path;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"OciLayer{\"\n+                + \"mediaType='\" + mediaType + '\\''\n+                + \", path='\" + path + '\\''\n+                + '}';\n+        }\n+    }\n+\n+    @JsonInclude(JsonInclude.Include.NON_DEFAULT)\n+    public static class OciRuntimeConfig {\n+        private final OciRootConfig root;\n+        private final List<OciMount> mounts;\n+        private final OciProcessConfig process;\n+        private final OciHooksConfig hooks;\n+        private final OciAnnotationsConfig annotations;\n+        private final OciLinuxConfig linux;\n+        private final String hostname;\n+\n+        public OciRuntimeConfig() {\n+            this(null, null, null, null, null, null, null);\n+        }\n+\n+        public OciRuntimeConfig(OciRootConfig root, List<OciMount> mounts,\n+                                OciProcessConfig process, String hostname, OciHooksConfig hooks, OciAnnotationsConfig annotations,\n+                                OciLinuxConfig linux) {\n+            this.root = root;\n+            this.mounts = mounts;\n+            this.process = process;\n+            this.hostname = hostname;\n+            this.hooks = hooks;\n+            this.annotations = annotations;\n+            this.linux = linux;\n+        }\n+\n+        public OciRootConfig getRoot() {\n+            return root;\n+        }\n+\n+        public List<OciMount> getMounts() {\n+            return mounts;\n+        }\n+\n+        public OciProcessConfig getProcess() {\n+            return process;\n+        }\n+\n+        public String getHostname() {\n+            return hostname;\n+        }\n+\n+        public OciHooksConfig getHooks() {\n+            return hooks;\n+        }\n+\n+        public OciAnnotationsConfig getAnnotations() {\n+            return annotations;\n+        }\n+\n+        public OciLinuxConfig getLinux() {\n+            return linux;\n+        }\n+\n+        @JsonInclude(JsonInclude.Include.NON_DEFAULT)\n+        static class OciRootConfig {\n+            private final String path;\n+            private final boolean readonly;\n+\n+            OciRootConfig(String path, boolean readonly) {\n+                this.path = path;\n+                this.readonly = readonly;\n+            }\n+\n+            OciRootConfig() {\n+                this(null, false);\n+            }\n+\n+            public String getPath() {\n+                return path;\n+            }\n+\n+            public boolean isReadonly() {\n+                return readonly;\n+            }\n+        }\n+\n+        @JsonInclude(JsonInclude.Include.NON_DEFAULT)\n+        static class OciMount {\n+            private final String destination;\n+            private final String type;\n+            private final String source;\n+            private final List<String> options;\n+\n+            OciMount(String destination, String type, String source, List<String> options) {\n+                this.destination = destination;\n+                this.type = type;\n+                this.source = source;\n+                this.options = options;\n+            }\n+\n+            OciMount(String destination, String source, List<String> options) {", "originalCommit": "690e02091244f33b32a1dc3c449413e6aff1d964", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODU0MjU1MQ==", "url": "https://github.com/apache/storm/pull/3366#discussion_r548542551", "bodyText": "Can these be final?", "author": "bipinprasad", "createdAt": "2020-12-24T14:18:44Z", "path": "storm-server/src/main/java/org/apache/storm/container/oci/RuncLibContainerManager.java", "diffHunk": "@@ -0,0 +1,631 @@\n+/*\n+ *\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.storm.container.oci;\n+\n+import static org.apache.storm.utils.ConfigUtils.FILE_SEPARATOR;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.FileReader;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.io.Reader;\n+import java.io.Writer;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.StormTimer;\n+import org.apache.storm.container.cgroup.CgroupUtils;\n+import org.apache.storm.container.cgroup.core.MemoryCore;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciLayer;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig.OciLinuxConfig;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig.OciMount;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig.OciProcessConfig;\n+import org.apache.storm.daemon.supervisor.ClientSupervisorUtils;\n+import org.apache.storm.daemon.supervisor.ExitCodeCallback;\n+import org.apache.storm.utils.ConfigUtils;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.ReflectionUtils;\n+import org.apache.storm.utils.ServerUtils;\n+import org.apache.storm.utils.Utils;\n+\n+import org.json.simple.JSONObject;\n+import org.json.simple.parser.JSONParser;\n+import org.json.simple.parser.ParseException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.yaml.snakeyaml.DumperOptions;\n+import org.yaml.snakeyaml.Yaml;\n+\n+public class RuncLibContainerManager extends OciContainerManager {\n+    private static final Logger LOG = LoggerFactory.getLogger(RuncLibContainerManager.class);\n+\n+    private OciImageTagToManifestPluginInterface imageTagToManifestPlugin;\n+    private OciManifestToResourcesPluginInterface manifestToResourcesPlugin;\n+    private OciResourcesLocalizerInterface ociResourcesLocalizer;\n+    private ObjectMapper mapper;\n+    private int layersToKeep;\n+    private String seccomp;\n+\n+    private static final String RESOLV_CONF = \"/etc/resolv.conf\";\n+    private static final String HOSTNAME = \"/etc/hostname\";\n+    private static final String HOSTS = \"/etc/hosts\";\n+    private static final String OCI_CONFIG_JSON = \"oci-config.json\";\n+\n+    private static final String SQUASHFS_MEDIA_TYPE = \"application/vnd.squashfs\";\n+\n+    //CPU CFS (Completely Fair Scheduler) period\n+    private static final long CPU_CFS_PERIOD_US = 100000;\n+\n+    private Map<String, Long> workerToContainerPid = new ConcurrentHashMap<>();\n+    private Map<String, ExitCodeCallback> workerToExitCallback = new ConcurrentHashMap<>();\n+    private Map<String, String> workerToUser = new ConcurrentHashMap<>();", "originalCommit": "690e02091244f33b32a1dc3c449413e6aff1d964", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODU0NzEyMw==", "url": "https://github.com/apache/storm/pull/3366#discussion_r548547123", "bodyText": "Could this multiplication on the right side cause an overflow - int * int * long  when the first two integers are being multiplied.", "author": "bipinprasad", "createdAt": "2020-12-24T14:36:56Z", "path": "storm-server/src/main/java/org/apache/storm/container/oci/RuncLibContainerManager.java", "diffHunk": "@@ -0,0 +1,631 @@\n+/*\n+ *\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.storm.container.oci;\n+\n+import static org.apache.storm.utils.ConfigUtils.FILE_SEPARATOR;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.FileReader;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.io.Reader;\n+import java.io.Writer;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.StormTimer;\n+import org.apache.storm.container.cgroup.CgroupUtils;\n+import org.apache.storm.container.cgroup.core.MemoryCore;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciLayer;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig.OciLinuxConfig;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig.OciMount;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig.OciProcessConfig;\n+import org.apache.storm.daemon.supervisor.ClientSupervisorUtils;\n+import org.apache.storm.daemon.supervisor.ExitCodeCallback;\n+import org.apache.storm.utils.ConfigUtils;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.ReflectionUtils;\n+import org.apache.storm.utils.ServerUtils;\n+import org.apache.storm.utils.Utils;\n+\n+import org.json.simple.JSONObject;\n+import org.json.simple.parser.JSONParser;\n+import org.json.simple.parser.ParseException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.yaml.snakeyaml.DumperOptions;\n+import org.yaml.snakeyaml.Yaml;\n+\n+public class RuncLibContainerManager extends OciContainerManager {\n+    private static final Logger LOG = LoggerFactory.getLogger(RuncLibContainerManager.class);\n+\n+    private OciImageTagToManifestPluginInterface imageTagToManifestPlugin;\n+    private OciManifestToResourcesPluginInterface manifestToResourcesPlugin;\n+    private OciResourcesLocalizerInterface ociResourcesLocalizer;\n+    private ObjectMapper mapper;\n+    private int layersToKeep;\n+    private String seccomp;\n+\n+    private static final String RESOLV_CONF = \"/etc/resolv.conf\";\n+    private static final String HOSTNAME = \"/etc/hostname\";\n+    private static final String HOSTS = \"/etc/hosts\";\n+    private static final String OCI_CONFIG_JSON = \"oci-config.json\";\n+\n+    private static final String SQUASHFS_MEDIA_TYPE = \"application/vnd.squashfs\";\n+\n+    //CPU CFS (Completely Fair Scheduler) period\n+    private static final long CPU_CFS_PERIOD_US = 100000;\n+\n+    private Map<String, Long> workerToContainerPid = new ConcurrentHashMap<>();\n+    private Map<String, ExitCodeCallback> workerToExitCallback = new ConcurrentHashMap<>();\n+    private Map<String, String> workerToUser = new ConcurrentHashMap<>();\n+    private StormTimer checkContainerAliveTimer;\n+\n+    @Override\n+    public void prepare(Map<String, Object> conf) throws IOException {\n+        super.prepare(conf);\n+\n+        imageTagToManifestPlugin = chooseImageTagToManifestPlugin();\n+        imageTagToManifestPlugin.init(conf);\n+\n+        manifestToResourcesPlugin = chooseManifestToResourcesPlugin();\n+        manifestToResourcesPlugin.init(conf);\n+\n+        ociResourcesLocalizer = chooseOciResourcesLocalizer();\n+        ociResourcesLocalizer.init(conf);\n+\n+        layersToKeep = ObjectReader.getInt(\n+                conf.get(DaemonConfig.STORM_OCI_LAYER_MOUNTS_TO_KEEP),\n+                100\n+        );\n+\n+        mapper = new ObjectMapper();\n+\n+        if (seccompJsonFile != null) {\n+            seccomp = new String(Files.readAllBytes(Paths.get(seccompJsonFile)));\n+        }\n+\n+        if (checkContainerAliveTimer == null) {\n+            checkContainerAliveTimer =\n+                new StormTimer(\"CheckRuncContainerAlive\", Utils.createDefaultUncaughtExceptionHandler());\n+            checkContainerAliveTimer\n+                .scheduleRecurring(0, (Integer) conf.get(DaemonConfig.SUPERVISOR_MONITOR_FREQUENCY_SECS), () -> {\n+                    try {\n+                        checkContainersAlive();\n+                    } catch (Exception e) {\n+                        //Ignore\n+                        LOG.debug(\"The CheckRuncContainerAlive thread has exception. Ignored\", e);\n+                    }\n+                });\n+        }\n+    }\n+\n+    private OciImageTagToManifestPluginInterface chooseImageTagToManifestPlugin() throws IllegalArgumentException {\n+        String pluginName = ObjectReader.getString(\n+                conf.get(DaemonConfig.STORM_OCI_IMAGE_TAG_TO_MANIFEST_PLUGIN)\n+        );\n+        LOG.info(\"imageTag-to-manifest Plugin is: {}\", pluginName);\n+        return ReflectionUtils.newInstance(pluginName);\n+    }\n+\n+    private OciManifestToResourcesPluginInterface chooseManifestToResourcesPlugin() throws IllegalArgumentException {\n+        String pluginName = ObjectReader.getString(\n+                conf.get(DaemonConfig.STORM_OCI_MANIFEST_TO_RESOURCES_PLUGIN)\n+        );\n+        LOG.info(\"manifest to resource Plugin is: {}\", pluginName);\n+        return ReflectionUtils.newInstance(pluginName);\n+    }\n+\n+    private OciResourcesLocalizerInterface chooseOciResourcesLocalizer()\n+        throws IllegalArgumentException {\n+        String pluginName = ObjectReader.getString(\n+                conf.get(DaemonConfig.STORM_OCI_RESOURCES_LOCALIZER)\n+        );\n+        LOG.info(\"oci resource localizer is: {}\", pluginName);\n+        return ReflectionUtils.newInstance(pluginName);\n+    }\n+\n+    //the container process ID in the process namespace of the host.\n+    private String containerPidFile(String workerId) {\n+        return ConfigUtils.workerArtifactsSymlink(conf, workerId) + FILE_SEPARATOR + \"container-\" + workerId + \".pid\";\n+    }\n+\n+    @Override\n+    public void launchWorkerProcess(String user, String topologyId,  Map<String, Object> topoConf,\n+                                    int port, String workerId,\n+                                    List<String> command, Map<String, String> env, String logPrefix,\n+                                    ExitCodeCallback processExitCallback, File targetDir) throws IOException {\n+\n+        String imageName = getImageName(topoConf);\n+        if (imageName == null) {\n+            LOG.error(\"Image name for {} is not configured properly; will not continue to launch the worker\", topologyId);\n+            return;\n+        }\n+\n+        //set container ID to port + worker ID\n+        String containerId = getContainerId(workerId, port);\n+\n+        //get manifest\n+        ImageManifest manifest = imageTagToManifestPlugin.getManifestFromImageTag(imageName);\n+        LOG.debug(\"workerId {}: Got manifest: {}\", workerId, manifest.toString());\n+\n+        //get layers metadata\n+        OciResource configResource = manifestToResourcesPlugin.getConfigResource(manifest);\n+        LOG.info(\"workerId {}: Got config metadata: {}\", workerId, configResource.toString());\n+\n+        saveRuncYaml(topologyId, port, containerId, imageName, configResource);\n+\n+        List<OciResource> layersResource = manifestToResourcesPlugin.getLayerResources(manifest);\n+        LOG.info(\"workerId {}: Got layers metadata: {}\", workerId, layersResource.toString());\n+\n+        //localize resource\n+        String configLocalPath = ociResourcesLocalizer.localize(configResource);\n+\n+        List<String> ociEnv = new ArrayList<>();\n+        List<String> args = new ArrayList<>();\n+\n+        ArrayList<OciLayer> layers = new ArrayList<>();\n+\n+        File file = new File(configLocalPath);\n+        //extract env\n+        List<String> imageEnv = extractImageEnv(file);\n+        if (imageEnv != null && !imageEnv.isEmpty()) {\n+            ociEnv.addAll(imageEnv);\n+        }\n+        for (Map.Entry<String, String> entry : env.entrySet()) {\n+            ociEnv.add(entry.getKey() + \"=\" + entry.getValue());\n+        }\n+        LOG.debug(\"workerId {}: ociEnv: {}\", workerId, ociEnv);\n+\n+        //extract entrypoint\n+        List<String> entrypoint = extractImageEntrypoint(file);\n+        if (entrypoint != null && !entrypoint.isEmpty()) {\n+            args.addAll(entrypoint);\n+        }\n+        LOG.debug(\"workerId {}: args: {}\", workerId, args);\n+\n+        //localize layers\n+        List<String> layersLocalPath = ociResourcesLocalizer.localize((layersResource));\n+        //compose layers\n+        for (String layerLocalPath : layersLocalPath) {\n+            OciLayer layer = new OciLayer(SQUASHFS_MEDIA_TYPE, layerLocalPath);\n+            layers.add(layer);\n+        }\n+        LOG.debug(\"workerId {}: layers: {}\", workerId, layers);\n+        ArrayList<OciMount> mounts = new ArrayList<>();\n+        setContainerMounts(mounts, topologyId, workerId, port);\n+        LOG.debug(\"workerId {}: mounts: {}\", workerId, mounts);\n+\n+        //calculate the cpusQuotas based on CPU_CFS_PERIOD and assigned CPU\n+        Long cpusQuotas = null;\n+        if (workerToCpu.containsKey(workerId)) {\n+            cpusQuotas = workerToCpu.get(workerId) * CPU_CFS_PERIOD_US / 100;\n+        }\n+\n+        Long memoryInBytes = null;\n+        if (workerToMemoryMb.containsKey(workerId)) {\n+            memoryInBytes = workerToMemoryMb.get(workerId) * 1024 * 1024L;", "originalCommit": "690e02091244f33b32a1dc3c449413e6aff1d964", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3MTkwMDU3MQ==", "url": "https://github.com/apache/storm/pull/3366#discussion_r671900571", "bodyText": "nice catch.", "author": "Ethanlm", "createdAt": "2021-07-18T21:32:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODU0NzEyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTQ5MzkwNA==", "url": "https://github.com/apache/storm/pull/3366#discussion_r549493904", "bodyText": "I did not find this referenced in the runc documentation.", "author": "agresch", "createdAt": "2020-12-28T21:35:37Z", "path": "external/storm-hdfs-oci/src/main/java/org/apache/storm/container/oci/LocalOrHdfsImageTagToManifestPlugin.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.container.oci;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.utils.HadoopLoginUtil;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class LocalOrHdfsImageTagToManifestPlugin implements OciImageTagToManifestPluginInterface {\n+    private static final Logger LOG = LoggerFactory.getLogger(LocalOrHdfsImageTagToManifestPlugin.class);\n+\n+    private Map<String, ImageManifest> manifestCache;\n+    private ObjectMapper objMapper;\n+    private Map<String, String> localImageToHashCache = new HashMap<>();\n+    private Map<String, String> hdfsImageToHashCache = new HashMap<>();\n+    private Map<String, Object> conf;\n+    private long hdfsModTime;\n+    private long localModTime;\n+    private String hdfsImageToHashFile;\n+    private String manifestDir;\n+    private String localImageTagToHashFile;\n+    private int ociCacheRefreshIntervalSecs;\n+    private long lastRefreshTime;\n+\n+    private static String LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX = \"storm.oci.local.or.hdfs.image.tag.to.manifest.plugin.\";\n+\n+    /**\n+     * The HDFS location where the oci image-tag-to-hash file exists.\n+     */\n+    private static String HDFS_OCI_IMAGE_TAG_TO_HASH_FILE =\n+        LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"hdfs.hash.file\";\n+\n+    /**\n+     * The local file system location where the oci image-tag-to-hash file exists.\n+     */\n+    private static String LOCAL_OCI_IMAGE_TAG_TO_HASH_FILE =", "originalCommit": "690e02091244f33b32a1dc3c449413e6aff1d964", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTQ5Mzk1MA==", "url": "https://github.com/apache/storm/pull/3366#discussion_r549493950", "bodyText": "I did not find this referenced in the runc documentation.", "author": "agresch", "createdAt": "2020-12-28T21:35:46Z", "path": "external/storm-hdfs-oci/src/main/java/org/apache/storm/container/oci/LocalOrHdfsImageTagToManifestPlugin.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.container.oci;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.utils.HadoopLoginUtil;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class LocalOrHdfsImageTagToManifestPlugin implements OciImageTagToManifestPluginInterface {\n+    private static final Logger LOG = LoggerFactory.getLogger(LocalOrHdfsImageTagToManifestPlugin.class);\n+\n+    private Map<String, ImageManifest> manifestCache;\n+    private ObjectMapper objMapper;\n+    private Map<String, String> localImageToHashCache = new HashMap<>();\n+    private Map<String, String> hdfsImageToHashCache = new HashMap<>();\n+    private Map<String, Object> conf;\n+    private long hdfsModTime;\n+    private long localModTime;\n+    private String hdfsImageToHashFile;\n+    private String manifestDir;\n+    private String localImageTagToHashFile;\n+    private int ociCacheRefreshIntervalSecs;\n+    private long lastRefreshTime;\n+\n+    private static String LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX = \"storm.oci.local.or.hdfs.image.tag.to.manifest.plugin.\";\n+\n+    /**\n+     * The HDFS location where the oci image-tag-to-hash file exists.\n+     */\n+    private static String HDFS_OCI_IMAGE_TAG_TO_HASH_FILE =\n+        LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"hdfs.hash.file\";\n+\n+    /**\n+     * The local file system location where the oci image-tag-to-hash file exists.\n+     */\n+    private static String LOCAL_OCI_IMAGE_TAG_TO_HASH_FILE =\n+        LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"local.hash.file\";\n+\n+    /**\n+     * The interval in seconds between refreshing the oci image-Tag-to-hash cache.\n+     */\n+    private static String OCI_CACHE_REFRESH_INTERVAL =", "originalCommit": "690e02091244f33b32a1dc3c449413e6aff1d964", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTQ5NDE2Ng==", "url": "https://github.com/apache/storm/pull/3366#discussion_r549494166", "bodyText": "I did not find this referenced in the runc documentation.", "author": "agresch", "createdAt": "2020-12-28T21:36:31Z", "path": "external/storm-hdfs-oci/src/main/java/org/apache/storm/container/oci/LocalOrHdfsImageTagToManifestPlugin.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.container.oci;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.utils.HadoopLoginUtil;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class LocalOrHdfsImageTagToManifestPlugin implements OciImageTagToManifestPluginInterface {\n+    private static final Logger LOG = LoggerFactory.getLogger(LocalOrHdfsImageTagToManifestPlugin.class);\n+\n+    private Map<String, ImageManifest> manifestCache;\n+    private ObjectMapper objMapper;\n+    private Map<String, String> localImageToHashCache = new HashMap<>();\n+    private Map<String, String> hdfsImageToHashCache = new HashMap<>();\n+    private Map<String, Object> conf;\n+    private long hdfsModTime;\n+    private long localModTime;\n+    private String hdfsImageToHashFile;\n+    private String manifestDir;\n+    private String localImageTagToHashFile;\n+    private int ociCacheRefreshIntervalSecs;\n+    private long lastRefreshTime;\n+\n+    private static String LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX = \"storm.oci.local.or.hdfs.image.tag.to.manifest.plugin.\";\n+\n+    /**\n+     * The HDFS location where the oci image-tag-to-hash file exists.\n+     */\n+    private static String HDFS_OCI_IMAGE_TAG_TO_HASH_FILE =\n+        LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"hdfs.hash.file\";\n+\n+    /**\n+     * The local file system location where the oci image-tag-to-hash file exists.\n+     */\n+    private static String LOCAL_OCI_IMAGE_TAG_TO_HASH_FILE =\n+        LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"local.hash.file\";\n+\n+    /**\n+     * The interval in seconds between refreshing the oci image-Tag-to-hash cache.\n+     */\n+    private static String OCI_CACHE_REFRESH_INTERVAL =\n+        LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"cache.refresh.interval.secs\";\n+\n+    /**\n+     * The number of manifests to cache.\n+     */\n+    private static String OCI_NUM_MANIFESTS_TO_CACHE = LOCAL_OR_HDFS_IMAGE_TAG_TO_MANIFEST_PLUGIN_PREFIX + \"num.manifests.to.cache\";", "originalCommit": "690e02091244f33b32a1dc3c449413e6aff1d964", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDU2OTUxNQ==", "url": "https://github.com/apache/storm/pull/3366#discussion_r550569515", "bodyText": "if for some reason this PR is not committed, this looks like a separate PR that should be addressed to get in", "author": "agresch", "createdAt": "2020-12-31T17:13:25Z", "path": "storm-server/src/main/java/org/apache/storm/container/oci/OciContainerManager.java", "diffHunk": "@@ -118,6 +118,8 @@ public void reserveResourcesForWorker(String workerId, Integer workerMemoryMb, I\n     public void releaseResourcesForWorker(String workerId) {\n         workerToCpu.remove(workerId);\n         workerToMemoryMb.remove(workerId);\n+        workerToCores.remove(workerId);", "originalCommit": "690e02091244f33b32a1dc3c449413e6aff1d964", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDU3NTg1Mg==", "url": "https://github.com/apache/storm/pull/3366#discussion_r550575852", "bodyText": "Is it expected that checkContainersAlive() throw an exception without a message?", "author": "agresch", "createdAt": "2020-12-31T17:21:34Z", "path": "storm-server/src/main/java/org/apache/storm/container/oci/RuncLibContainerManager.java", "diffHunk": "@@ -0,0 +1,631 @@\n+/*\n+ *\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.storm.container.oci;\n+\n+import static org.apache.storm.utils.ConfigUtils.FILE_SEPARATOR;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.FileReader;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.io.Reader;\n+import java.io.Writer;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.StormTimer;\n+import org.apache.storm.container.cgroup.CgroupUtils;\n+import org.apache.storm.container.cgroup.core.MemoryCore;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciLayer;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig.OciLinuxConfig;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig.OciMount;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig.OciProcessConfig;\n+import org.apache.storm.daemon.supervisor.ClientSupervisorUtils;\n+import org.apache.storm.daemon.supervisor.ExitCodeCallback;\n+import org.apache.storm.utils.ConfigUtils;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.ReflectionUtils;\n+import org.apache.storm.utils.ServerUtils;\n+import org.apache.storm.utils.Utils;\n+\n+import org.json.simple.JSONObject;\n+import org.json.simple.parser.JSONParser;\n+import org.json.simple.parser.ParseException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.yaml.snakeyaml.DumperOptions;\n+import org.yaml.snakeyaml.Yaml;\n+\n+public class RuncLibContainerManager extends OciContainerManager {\n+    private static final Logger LOG = LoggerFactory.getLogger(RuncLibContainerManager.class);\n+\n+    private OciImageTagToManifestPluginInterface imageTagToManifestPlugin;\n+    private OciManifestToResourcesPluginInterface manifestToResourcesPlugin;\n+    private OciResourcesLocalizerInterface ociResourcesLocalizer;\n+    private ObjectMapper mapper;\n+    private int layersToKeep;\n+    private String seccomp;\n+\n+    private static final String RESOLV_CONF = \"/etc/resolv.conf\";\n+    private static final String HOSTNAME = \"/etc/hostname\";\n+    private static final String HOSTS = \"/etc/hosts\";\n+    private static final String OCI_CONFIG_JSON = \"oci-config.json\";\n+\n+    private static final String SQUASHFS_MEDIA_TYPE = \"application/vnd.squashfs\";\n+\n+    //CPU CFS (Completely Fair Scheduler) period\n+    private static final long CPU_CFS_PERIOD_US = 100000;\n+\n+    private Map<String, Long> workerToContainerPid = new ConcurrentHashMap<>();\n+    private Map<String, ExitCodeCallback> workerToExitCallback = new ConcurrentHashMap<>();\n+    private Map<String, String> workerToUser = new ConcurrentHashMap<>();\n+    private StormTimer checkContainerAliveTimer;\n+\n+    @Override\n+    public void prepare(Map<String, Object> conf) throws IOException {\n+        super.prepare(conf);\n+\n+        imageTagToManifestPlugin = chooseImageTagToManifestPlugin();\n+        imageTagToManifestPlugin.init(conf);\n+\n+        manifestToResourcesPlugin = chooseManifestToResourcesPlugin();\n+        manifestToResourcesPlugin.init(conf);\n+\n+        ociResourcesLocalizer = chooseOciResourcesLocalizer();\n+        ociResourcesLocalizer.init(conf);\n+\n+        layersToKeep = ObjectReader.getInt(\n+                conf.get(DaemonConfig.STORM_OCI_LAYER_MOUNTS_TO_KEEP),\n+                100\n+        );\n+\n+        mapper = new ObjectMapper();\n+\n+        if (seccompJsonFile != null) {\n+            seccomp = new String(Files.readAllBytes(Paths.get(seccompJsonFile)));\n+        }\n+\n+        if (checkContainerAliveTimer == null) {\n+            checkContainerAliveTimer =\n+                new StormTimer(\"CheckRuncContainerAlive\", Utils.createDefaultUncaughtExceptionHandler());\n+            checkContainerAliveTimer\n+                .scheduleRecurring(0, (Integer) conf.get(DaemonConfig.SUPERVISOR_MONITOR_FREQUENCY_SECS), () -> {\n+                    try {\n+                        checkContainersAlive();\n+                    } catch (Exception e) {\n+                        //Ignore\n+                        LOG.debug(\"The CheckRuncContainerAlive thread has exception. Ignored\", e);", "originalCommit": "690e02091244f33b32a1dc3c449413e6aff1d964", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3MTkyODEyMw==", "url": "https://github.com/apache/storm/pull/3366#discussion_r671928123", "bodyText": "changing to warn", "author": "Ethanlm", "createdAt": "2021-07-19T01:00:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDU3NTg1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDU4NTM1OA==", "url": "https://github.com/apache/storm/pull/3366#discussion_r550585358", "bodyText": "looks like it should be \"container for workerId {}\"", "author": "agresch", "createdAt": "2020-12-31T17:33:39Z", "path": "storm-server/src/main/java/org/apache/storm/container/oci/RuncLibContainerManager.java", "diffHunk": "@@ -0,0 +1,631 @@\n+/*\n+ *\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.storm.container.oci;\n+\n+import static org.apache.storm.utils.ConfigUtils.FILE_SEPARATOR;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.FileReader;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.io.Reader;\n+import java.io.Writer;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.StormTimer;\n+import org.apache.storm.container.cgroup.CgroupUtils;\n+import org.apache.storm.container.cgroup.core.MemoryCore;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciLayer;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig.OciLinuxConfig;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig.OciMount;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig.OciProcessConfig;\n+import org.apache.storm.daemon.supervisor.ClientSupervisorUtils;\n+import org.apache.storm.daemon.supervisor.ExitCodeCallback;\n+import org.apache.storm.utils.ConfigUtils;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.ReflectionUtils;\n+import org.apache.storm.utils.ServerUtils;\n+import org.apache.storm.utils.Utils;\n+\n+import org.json.simple.JSONObject;\n+import org.json.simple.parser.JSONParser;\n+import org.json.simple.parser.ParseException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.yaml.snakeyaml.DumperOptions;\n+import org.yaml.snakeyaml.Yaml;\n+\n+public class RuncLibContainerManager extends OciContainerManager {\n+    private static final Logger LOG = LoggerFactory.getLogger(RuncLibContainerManager.class);\n+\n+    private OciImageTagToManifestPluginInterface imageTagToManifestPlugin;\n+    private OciManifestToResourcesPluginInterface manifestToResourcesPlugin;\n+    private OciResourcesLocalizerInterface ociResourcesLocalizer;\n+    private ObjectMapper mapper;\n+    private int layersToKeep;\n+    private String seccomp;\n+\n+    private static final String RESOLV_CONF = \"/etc/resolv.conf\";\n+    private static final String HOSTNAME = \"/etc/hostname\";\n+    private static final String HOSTS = \"/etc/hosts\";\n+    private static final String OCI_CONFIG_JSON = \"oci-config.json\";\n+\n+    private static final String SQUASHFS_MEDIA_TYPE = \"application/vnd.squashfs\";\n+\n+    //CPU CFS (Completely Fair Scheduler) period\n+    private static final long CPU_CFS_PERIOD_US = 100000;\n+\n+    private Map<String, Long> workerToContainerPid = new ConcurrentHashMap<>();\n+    private Map<String, ExitCodeCallback> workerToExitCallback = new ConcurrentHashMap<>();\n+    private Map<String, String> workerToUser = new ConcurrentHashMap<>();\n+    private StormTimer checkContainerAliveTimer;\n+\n+    @Override\n+    public void prepare(Map<String, Object> conf) throws IOException {\n+        super.prepare(conf);\n+\n+        imageTagToManifestPlugin = chooseImageTagToManifestPlugin();\n+        imageTagToManifestPlugin.init(conf);\n+\n+        manifestToResourcesPlugin = chooseManifestToResourcesPlugin();\n+        manifestToResourcesPlugin.init(conf);\n+\n+        ociResourcesLocalizer = chooseOciResourcesLocalizer();\n+        ociResourcesLocalizer.init(conf);\n+\n+        layersToKeep = ObjectReader.getInt(\n+                conf.get(DaemonConfig.STORM_OCI_LAYER_MOUNTS_TO_KEEP),\n+                100\n+        );\n+\n+        mapper = new ObjectMapper();\n+\n+        if (seccompJsonFile != null) {\n+            seccomp = new String(Files.readAllBytes(Paths.get(seccompJsonFile)));\n+        }\n+\n+        if (checkContainerAliveTimer == null) {\n+            checkContainerAliveTimer =\n+                new StormTimer(\"CheckRuncContainerAlive\", Utils.createDefaultUncaughtExceptionHandler());\n+            checkContainerAliveTimer\n+                .scheduleRecurring(0, (Integer) conf.get(DaemonConfig.SUPERVISOR_MONITOR_FREQUENCY_SECS), () -> {\n+                    try {\n+                        checkContainersAlive();\n+                    } catch (Exception e) {\n+                        //Ignore\n+                        LOG.debug(\"The CheckRuncContainerAlive thread has exception. Ignored\", e);\n+                    }\n+                });\n+        }\n+    }\n+\n+    private OciImageTagToManifestPluginInterface chooseImageTagToManifestPlugin() throws IllegalArgumentException {\n+        String pluginName = ObjectReader.getString(\n+                conf.get(DaemonConfig.STORM_OCI_IMAGE_TAG_TO_MANIFEST_PLUGIN)\n+        );\n+        LOG.info(\"imageTag-to-manifest Plugin is: {}\", pluginName);\n+        return ReflectionUtils.newInstance(pluginName);\n+    }\n+\n+    private OciManifestToResourcesPluginInterface chooseManifestToResourcesPlugin() throws IllegalArgumentException {\n+        String pluginName = ObjectReader.getString(\n+                conf.get(DaemonConfig.STORM_OCI_MANIFEST_TO_RESOURCES_PLUGIN)\n+        );\n+        LOG.info(\"manifest to resource Plugin is: {}\", pluginName);\n+        return ReflectionUtils.newInstance(pluginName);\n+    }\n+\n+    private OciResourcesLocalizerInterface chooseOciResourcesLocalizer()\n+        throws IllegalArgumentException {\n+        String pluginName = ObjectReader.getString(\n+                conf.get(DaemonConfig.STORM_OCI_RESOURCES_LOCALIZER)\n+        );\n+        LOG.info(\"oci resource localizer is: {}\", pluginName);\n+        return ReflectionUtils.newInstance(pluginName);\n+    }\n+\n+    //the container process ID in the process namespace of the host.\n+    private String containerPidFile(String workerId) {\n+        return ConfigUtils.workerArtifactsSymlink(conf, workerId) + FILE_SEPARATOR + \"container-\" + workerId + \".pid\";\n+    }\n+\n+    @Override\n+    public void launchWorkerProcess(String user, String topologyId,  Map<String, Object> topoConf,\n+                                    int port, String workerId,\n+                                    List<String> command, Map<String, String> env, String logPrefix,\n+                                    ExitCodeCallback processExitCallback, File targetDir) throws IOException {\n+\n+        String imageName = getImageName(topoConf);\n+        if (imageName == null) {\n+            LOG.error(\"Image name for {} is not configured properly; will not continue to launch the worker\", topologyId);\n+            return;\n+        }\n+\n+        //set container ID to port + worker ID\n+        String containerId = getContainerId(workerId, port);\n+\n+        //get manifest\n+        ImageManifest manifest = imageTagToManifestPlugin.getManifestFromImageTag(imageName);\n+        LOG.debug(\"workerId {}: Got manifest: {}\", workerId, manifest.toString());\n+\n+        //get layers metadata\n+        OciResource configResource = manifestToResourcesPlugin.getConfigResource(manifest);\n+        LOG.info(\"workerId {}: Got config metadata: {}\", workerId, configResource.toString());\n+\n+        saveRuncYaml(topologyId, port, containerId, imageName, configResource);\n+\n+        List<OciResource> layersResource = manifestToResourcesPlugin.getLayerResources(manifest);\n+        LOG.info(\"workerId {}: Got layers metadata: {}\", workerId, layersResource.toString());\n+\n+        //localize resource\n+        String configLocalPath = ociResourcesLocalizer.localize(configResource);\n+\n+        List<String> ociEnv = new ArrayList<>();\n+        List<String> args = new ArrayList<>();\n+\n+        ArrayList<OciLayer> layers = new ArrayList<>();\n+\n+        File file = new File(configLocalPath);\n+        //extract env\n+        List<String> imageEnv = extractImageEnv(file);\n+        if (imageEnv != null && !imageEnv.isEmpty()) {\n+            ociEnv.addAll(imageEnv);\n+        }\n+        for (Map.Entry<String, String> entry : env.entrySet()) {\n+            ociEnv.add(entry.getKey() + \"=\" + entry.getValue());\n+        }\n+        LOG.debug(\"workerId {}: ociEnv: {}\", workerId, ociEnv);\n+\n+        //extract entrypoint\n+        List<String> entrypoint = extractImageEntrypoint(file);\n+        if (entrypoint != null && !entrypoint.isEmpty()) {\n+            args.addAll(entrypoint);\n+        }\n+        LOG.debug(\"workerId {}: args: {}\", workerId, args);\n+\n+        //localize layers\n+        List<String> layersLocalPath = ociResourcesLocalizer.localize((layersResource));\n+        //compose layers\n+        for (String layerLocalPath : layersLocalPath) {\n+            OciLayer layer = new OciLayer(SQUASHFS_MEDIA_TYPE, layerLocalPath);\n+            layers.add(layer);\n+        }\n+        LOG.debug(\"workerId {}: layers: {}\", workerId, layers);\n+        ArrayList<OciMount> mounts = new ArrayList<>();\n+        setContainerMounts(mounts, topologyId, workerId, port);\n+        LOG.debug(\"workerId {}: mounts: {}\", workerId, mounts);\n+\n+        //calculate the cpusQuotas based on CPU_CFS_PERIOD and assigned CPU\n+        Long cpusQuotas = null;\n+        if (workerToCpu.containsKey(workerId)) {\n+            cpusQuotas = workerToCpu.get(workerId) * CPU_CFS_PERIOD_US / 100;\n+        }\n+\n+        Long memoryInBytes = null;\n+        if (workerToMemoryMb.containsKey(workerId)) {\n+            memoryInBytes = workerToMemoryMb.get(workerId) * 1024 * 1024L;\n+        }\n+        LOG.info(\"workerId {}: memoryInBytes set to {}; cpusQuotas set to {}\", workerId, memoryInBytes, cpusQuotas);\n+\n+        //<workerRoot>/<workerId>\n+        String workerDir = targetDir.getAbsolutePath();\n+        String workerScriptPath = ServerUtils.writeScript(workerDir, command, env, \"0027\");\n+\n+        args.add(\"bash\");\n+        args.add(workerScriptPath);\n+\n+        //The container PID (on the host) will be written to this file.\n+        String containerPidFilePath = containerPidFile(workerId);\n+\n+        OciProcessConfig processConfig = createOciProcessConfig(workerDir, ociEnv, args);\n+\n+        OciLinuxConfig linuxConfig =\n+            createOciLinuxConfig(cpusQuotas, memoryInBytes, cgroupParent + \"/\" + containerId, seccomp, workerId);\n+\n+        OciRuntimeConfig ociRuntimeConfig = new OciRuntimeConfig(null, mounts, processConfig, null,\n+                                                          null, null, linuxConfig);\n+\n+        OciContainerExecutorConfig ociContainerExecutorConfig =\n+            createOciContainerExecutorConfig(user, containerId, containerPidFilePath,\n+                                             workerScriptPath, null, null, null, layers, ociRuntimeConfig);\n+\n+        //launch the container using worker-launcher\n+        String executorConfigToJsonFile = writeOciExecutorConfigToJsonFile(mapper, ociContainerExecutorConfig, workerDir);\n+        LOG.info(\"workerId {}: oci-config.json file path: {}\", workerId, executorConfigToJsonFile);\n+\n+        List<String> cmdArgs = Arrays.asList(CmdType.RUN_OCI_CONTAINER.toString(), workerDir, executorConfigToJsonFile,\n+                                             ConfigUtils.workerArtifactsSymlink(conf, workerId));\n+\n+        // launch the oci container. waiting prevents possible race condition that could prevent cleanup of container\n+        int exitCode = ClientSupervisorUtils.processLauncherAndWait(conf, user, cmdArgs, env, logPrefix, targetDir);\n+        if (exitCode != 0) {\n+            LOG.error(\"launchWorkerProcess RuncCommand {} exited with code: {}\", \"LaunchWorker-\" + containerId, exitCode);\n+            throw new RuntimeException(\"launchWorkerProcess Failed to create Runc Container. ContainerId: \" + containerId);\n+        }\n+\n+        //Add to the watched list\n+        LOG.debug(\"Adding {} to the watched workers list\", workerId);\n+        workerToExitCallback.put(workerId, processExitCallback);\n+        workerToUser.put(workerId, user);\n+\n+    }\n+\n+    private void checkContainersAlive() {\n+        //Check if all watched workers are still alive\n+        workerToUser.forEach((workerId, user) -> {\n+            if (isContainerDead(workerId, user)) {\n+                invokeProcessExitCallback(workerId);\n+            }\n+        });\n+    }\n+\n+    private boolean isContainerDead(String workerId, String user) {\n+        boolean isDead = true;\n+        Long pid = getContainerPid(workerId);\n+        LOG.debug(\"Checking container {}, pid {}, user {}\", workerId, pid, user);\n+        //do nothing if pid is null.\n+        if (pid != null && user != null) {\n+            try {\n+                isDead = ServerUtils.areAllProcessesDead(conf, user, workerId, Collections.singleton(pid));\n+            } catch (IOException e) {\n+                //ignore\n+                LOG.debug(\"Error while checking if container is dead.\", e);\n+            }\n+        }\n+        return isDead;\n+    }\n+\n+    private void invokeProcessExitCallback(String workerId) {\n+        LOG.info(\"processExitCallback returned for workerId {}\", workerId);\n+        ExitCodeCallback processExitCallback = workerToExitCallback.get(workerId);\n+        if (processExitCallback != null) {\n+            processExitCallback.call(0);\n+        }\n+    }\n+\n+    private String getContainerId(String workerId, int port) throws IOException {\n+        if (port <= 0) { // when killing workers, we will have the workerId and a port of -1\n+            return getContainerIdFromOciJson(workerId);\n+        }\n+        return port + \"-\" + workerId;\n+    }\n+\n+    private String getContainerIdFromOciJson(String workerId) throws IOException {\n+        String ociJson = ConfigUtils.workerRoot(conf, workerId) + FILE_SEPARATOR + OCI_CONFIG_JSON;\n+        LOG.info(\"port unknown for workerId {}, looking up from {}\", workerId, ociJson);\n+        JSONParser parser = new JSONParser();\n+\n+        try (Reader reader = new FileReader(ociJson)) {\n+            JSONObject jsonObject = (JSONObject) parser.parse(reader);\n+            return (String) jsonObject.get(\"containerId\");\n+        } catch (ParseException e) {\n+            throw new IOException(\"Unable to parse {}\", e);\n+        }\n+    }\n+\n+    // save runc.yaml in artifacts dir so we can track which image the worker was launched with\n+    private void saveRuncYaml(String topologyId, int port, String containerId, String imageName, OciResource configResource) {\n+        String fname = String.format(\"runc-%s.yaml\", containerId);\n+        File file = new File(ConfigUtils.workerArtifactsRoot(conf, topologyId, port), fname);\n+        DumperOptions options = new DumperOptions();\n+        options.setIndent(2);\n+        options.setPrettyFlow(true);\n+        options.setDefaultFlowStyle(DumperOptions.FlowStyle.BLOCK);\n+        Yaml yaml = new Yaml(options);\n+        Map<String, Object> data = new HashMap<>();\n+        data.put(\"imageName\", imageName);\n+        data.put(\"manifest\", configResource.getFileName());\n+        data.put(\"configPath\", configResource.getPath());\n+        try (Writer writer = new FileWriter(file)) {\n+            yaml.dump(data, writer);\n+        } catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    private String writeOciExecutorConfigToJsonFile(ObjectMapper mapper, OciContainerExecutorConfig ociContainerExecutorConfig,\n+                                                    String workerDir) throws IOException {\n+        File cmdDir = new File(workerDir);\n+        if (!cmdDir.exists()) {\n+            throw new IOException(workerDir + \" doesn't exist\");\n+        }\n+\n+        File commandFile = new File(cmdDir + FILE_SEPARATOR + OCI_CONFIG_JSON);\n+        mapper.writeValue(commandFile, ociContainerExecutorConfig);\n+        return commandFile.getAbsolutePath();\n+    }\n+\n+    private void setContainerMounts(ArrayList<OciMount> mounts, String topologyId, String workerId, Integer port) throws IOException {\n+        //read-only bindmounts need to be added before read-write bindmounts otherwise read-write bindmounts may be overridden.\n+        for (String readonlyMount : readonlyBindmounts) {\n+            addOciMountLocation(mounts, readonlyMount, readonlyMount, false, false);\n+        }\n+\n+        for (String readwriteMount : readwriteBindmounts) {\n+            addOciMountLocation(mounts, readwriteMount, readwriteMount, false, true);\n+        }\n+\n+        addOciMountLocation(mounts, RESOLV_CONF, RESOLV_CONF, false, false);\n+        addOciMountLocation(mounts, HOSTNAME, HOSTNAME, false, false);\n+        addOciMountLocation(mounts, HOSTS, HOSTS, false, false);\n+        addOciMountLocation(mounts, nscdPath, nscdPath, false, false);\n+        addOciMountLocation(mounts, stormHome, stormHome, false, false);\n+        addOciMountLocation(mounts, cgroupRootPath, cgroupRootPath, false, false);\n+\n+        //set of locations to be bind mounted\n+        String supervisorLocalDir = ConfigUtils.supervisorLocalDir(conf);\n+        addOciMountLocation(mounts, supervisorLocalDir, supervisorLocalDir, false, false);\n+\n+        String workerRootDir = ConfigUtils.workerRoot(conf, workerId);\n+        addOciMountLocation(mounts, workerRootDir, workerRootDir, false, true);\n+\n+        String workerArtifactsRoot = ConfigUtils.workerArtifactsRoot(conf, topologyId, port);\n+        addOciMountLocation(mounts, workerArtifactsRoot, workerArtifactsRoot, false, true);\n+\n+        String workerUserFile = ConfigUtils.workerUserFile(conf, workerId);\n+        addOciMountLocation(mounts, workerUserFile, workerUserFile, false, true);\n+\n+        String sharedByTopologyDir = ConfigUtils.sharedByTopologyDir(conf, topologyId);\n+        addOciMountLocation(mounts, sharedByTopologyDir, sharedByTopologyDir, false, true);\n+\n+        String workerTmpRoot = ConfigUtils.workerTmpRoot(conf, workerId);\n+        addOciMountLocation(mounts, workerTmpRoot, TMP_DIR, false, true);\n+    }\n+\n+    private List<String> extractImageEnv(File config) throws IOException {\n+        JsonNode node = mapper.readTree(config);\n+        JsonNode envNode = node.path(\"config\").path(\"Env\");\n+        if (envNode.isMissingNode()) {\n+            return null;\n+        }\n+        return mapper.treeToValue(envNode, List.class);\n+    }\n+\n+    private List<String> extractImageEntrypoint(File config) throws IOException {\n+        JsonNode node = mapper.readTree(config);\n+        JsonNode entrypointNode = node.path(\"config\").path(\"Entrypoint\");\n+        if (entrypointNode.isMissingNode()) {\n+            return null;\n+        }\n+        return mapper.treeToValue(entrypointNode, List.class);\n+    }\n+\n+    private OciContainerExecutorConfig createOciContainerExecutorConfig(\n+            String username, String containerId, String pidFile,\n+            String containerScriptPath, String containerCredentialsPath,\n+            List<String> localDirs, List<String> logDirs,\n+            List<OciLayer> layers, OciRuntimeConfig ociRuntimeConfig) {\n+\n+        return new OciContainerExecutorConfig(username, containerId,\n+                pidFile, containerScriptPath, containerCredentialsPath,\n+                localDirs, logDirs, layers, layersToKeep, ociRuntimeConfig);\n+    }\n+\n+    private OciProcessConfig createOciProcessConfig(String cwd,\n+                                                    List<String> env, List<String> args) {\n+        return new OciProcessConfig(false, null, cwd, env,\n+                args, null, null, null, true, 0, null, null);\n+    }\n+\n+    private OciLinuxConfig createOciLinuxConfig(Long cpusQuotas, Long memInBytes,\n+                                                String cgroupsPath, String seccomp, String workerId) {\n+        OciLinuxConfig.Resources.Cpu cgroupCpu = null;\n+\n+        if (cpusQuotas != null) {\n+            cgroupCpu = new OciLinuxConfig.Resources.Cpu(0, cpusQuotas, CPU_CFS_PERIOD_US, 0, 0,\n+                    null, null);\n+\n+            if (workerToCores.containsKey(workerId)) {\n+                cgroupCpu.setCpus(StringUtils.join(workerToCores.get(workerId), \",\"));\n+                cgroupCpu.setMems(workerToMemoryZone.get(workerId));\n+            }\n+        }\n+\n+        OciLinuxConfig.Resources.Memory cgroupMem = null;\n+        if (memInBytes != null) {\n+            cgroupMem = new OciLinuxConfig.Resources.Memory(memInBytes, 0, 0, 0, 0, 0, false);\n+        }\n+\n+        OciLinuxConfig.Resources cgroupResources =\n+                new OciLinuxConfig.Resources(null, cgroupMem, cgroupCpu, null, null, null,\n+                        null, null);\n+\n+        return new OciLinuxConfig(null, null, null, null,\n+                cgroupsPath, cgroupResources, null, null, seccomp, null, null,\n+                null, null);\n+    }\n+\n+    private void addOciMountLocation(List<OciMount> mounts, String srcPath,\n+                                     String dstPath, boolean createSource, boolean isReadWrite) throws IOException {\n+        if (!createSource) {\n+            boolean sourceExists = new File(srcPath).exists();\n+            if (!sourceExists) {\n+                throw new IOException(\"SourcePath \" + srcPath + \" doesn't exit\");\n+            }\n+        }\n+\n+        ArrayList<String> options = new ArrayList<>();\n+        if (isReadWrite) {\n+            options.add(\"rw\");\n+        } else {\n+            options.add(\"ro\");\n+        }\n+        options.add(\"rbind\");\n+        options.add(\"rprivate\");\n+        mounts.add(new OciMount(dstPath, \"bind\", srcPath, options));\n+    }\n+\n+    @Override\n+    public long getMemoryUsage(String user, String workerId, int port) throws IOException {\n+        // \"/sys/fs/cgroup/memory/storm/containerId/\"\n+        String containerId = getContainerId(workerId, port);\n+        String memoryCgroupPath = memoryCgroupRootPath + File.separator  + containerId;\n+        MemoryCore memoryCore = new MemoryCore(memoryCgroupPath);\n+        LOG.debug(\"ContainerId {} : Got memory getPhysicalUsage {} from {}\", containerId, memoryCore.getPhysicalUsage(), memoryCgroupPath);\n+        return memoryCore.getPhysicalUsage();\n+    }\n+\n+    @Override\n+    public void kill(String user, String workerId) throws IOException {\n+        LOG.info(\"Killing {}\", workerId);\n+        Long pid = getContainerPid(workerId);\n+        if (pid != null) {\n+            signal(pid, 15, user);\n+        } else {\n+            LOG.warn(\"Trying to kill container {} but pidfile is not found\", workerId);\n+        }\n+    }\n+\n+    private void signal(long pid, int signal, String user) throws IOException {\n+        List<String> commands = Arrays.asList(\"signal\", String.valueOf(pid), String.valueOf(signal));\n+        String logPrefix = \"kill -\" + signal + \" \" + pid;\n+        ClientSupervisorUtils.processLauncherAndWait(conf, user, commands, null, logPrefix);\n+    }\n+\n+    @Override\n+    public void forceKill(String user, String workerId) throws IOException {\n+        LOG.debug(\"ForceKilling {}\", workerId);\n+        Long pid = getContainerPid(workerId);\n+        if (pid != null) {\n+            signal(pid, 9, user);\n+        } else {\n+            LOG.warn(\"Trying to forceKill container {} but pidfile is not found\", workerId);", "originalCommit": "690e02091244f33b32a1dc3c449413e6aff1d964", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDU4NzUyNQ==", "url": "https://github.com/apache/storm/pull/3366#discussion_r550587525", "bodyText": "should we release workerToExitCallback?", "author": "agresch", "createdAt": "2020-12-31T17:36:23Z", "path": "storm-server/src/main/java/org/apache/storm/container/oci/RuncLibContainerManager.java", "diffHunk": "@@ -0,0 +1,631 @@\n+/*\n+ *\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.storm.container.oci;\n+\n+import static org.apache.storm.utils.ConfigUtils.FILE_SEPARATOR;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.FileReader;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.io.Reader;\n+import java.io.Writer;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.StormTimer;\n+import org.apache.storm.container.cgroup.CgroupUtils;\n+import org.apache.storm.container.cgroup.core.MemoryCore;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciLayer;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig.OciLinuxConfig;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig.OciMount;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig.OciProcessConfig;\n+import org.apache.storm.daemon.supervisor.ClientSupervisorUtils;\n+import org.apache.storm.daemon.supervisor.ExitCodeCallback;\n+import org.apache.storm.utils.ConfigUtils;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.ReflectionUtils;\n+import org.apache.storm.utils.ServerUtils;\n+import org.apache.storm.utils.Utils;\n+\n+import org.json.simple.JSONObject;\n+import org.json.simple.parser.JSONParser;\n+import org.json.simple.parser.ParseException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.yaml.snakeyaml.DumperOptions;\n+import org.yaml.snakeyaml.Yaml;\n+\n+public class RuncLibContainerManager extends OciContainerManager {\n+    private static final Logger LOG = LoggerFactory.getLogger(RuncLibContainerManager.class);\n+\n+    private OciImageTagToManifestPluginInterface imageTagToManifestPlugin;\n+    private OciManifestToResourcesPluginInterface manifestToResourcesPlugin;\n+    private OciResourcesLocalizerInterface ociResourcesLocalizer;\n+    private ObjectMapper mapper;\n+    private int layersToKeep;\n+    private String seccomp;\n+\n+    private static final String RESOLV_CONF = \"/etc/resolv.conf\";\n+    private static final String HOSTNAME = \"/etc/hostname\";\n+    private static final String HOSTS = \"/etc/hosts\";\n+    private static final String OCI_CONFIG_JSON = \"oci-config.json\";\n+\n+    private static final String SQUASHFS_MEDIA_TYPE = \"application/vnd.squashfs\";\n+\n+    //CPU CFS (Completely Fair Scheduler) period\n+    private static final long CPU_CFS_PERIOD_US = 100000;\n+\n+    private Map<String, Long> workerToContainerPid = new ConcurrentHashMap<>();\n+    private Map<String, ExitCodeCallback> workerToExitCallback = new ConcurrentHashMap<>();\n+    private Map<String, String> workerToUser = new ConcurrentHashMap<>();\n+    private StormTimer checkContainerAliveTimer;\n+\n+    @Override\n+    public void prepare(Map<String, Object> conf) throws IOException {\n+        super.prepare(conf);\n+\n+        imageTagToManifestPlugin = chooseImageTagToManifestPlugin();\n+        imageTagToManifestPlugin.init(conf);\n+\n+        manifestToResourcesPlugin = chooseManifestToResourcesPlugin();\n+        manifestToResourcesPlugin.init(conf);\n+\n+        ociResourcesLocalizer = chooseOciResourcesLocalizer();\n+        ociResourcesLocalizer.init(conf);\n+\n+        layersToKeep = ObjectReader.getInt(\n+                conf.get(DaemonConfig.STORM_OCI_LAYER_MOUNTS_TO_KEEP),\n+                100\n+        );\n+\n+        mapper = new ObjectMapper();\n+\n+        if (seccompJsonFile != null) {\n+            seccomp = new String(Files.readAllBytes(Paths.get(seccompJsonFile)));\n+        }\n+\n+        if (checkContainerAliveTimer == null) {\n+            checkContainerAliveTimer =\n+                new StormTimer(\"CheckRuncContainerAlive\", Utils.createDefaultUncaughtExceptionHandler());\n+            checkContainerAliveTimer\n+                .scheduleRecurring(0, (Integer) conf.get(DaemonConfig.SUPERVISOR_MONITOR_FREQUENCY_SECS), () -> {\n+                    try {\n+                        checkContainersAlive();\n+                    } catch (Exception e) {\n+                        //Ignore\n+                        LOG.debug(\"The CheckRuncContainerAlive thread has exception. Ignored\", e);\n+                    }\n+                });\n+        }\n+    }\n+\n+    private OciImageTagToManifestPluginInterface chooseImageTagToManifestPlugin() throws IllegalArgumentException {\n+        String pluginName = ObjectReader.getString(\n+                conf.get(DaemonConfig.STORM_OCI_IMAGE_TAG_TO_MANIFEST_PLUGIN)\n+        );\n+        LOG.info(\"imageTag-to-manifest Plugin is: {}\", pluginName);\n+        return ReflectionUtils.newInstance(pluginName);\n+    }\n+\n+    private OciManifestToResourcesPluginInterface chooseManifestToResourcesPlugin() throws IllegalArgumentException {\n+        String pluginName = ObjectReader.getString(\n+                conf.get(DaemonConfig.STORM_OCI_MANIFEST_TO_RESOURCES_PLUGIN)\n+        );\n+        LOG.info(\"manifest to resource Plugin is: {}\", pluginName);\n+        return ReflectionUtils.newInstance(pluginName);\n+    }\n+\n+    private OciResourcesLocalizerInterface chooseOciResourcesLocalizer()\n+        throws IllegalArgumentException {\n+        String pluginName = ObjectReader.getString(\n+                conf.get(DaemonConfig.STORM_OCI_RESOURCES_LOCALIZER)\n+        );\n+        LOG.info(\"oci resource localizer is: {}\", pluginName);\n+        return ReflectionUtils.newInstance(pluginName);\n+    }\n+\n+    //the container process ID in the process namespace of the host.\n+    private String containerPidFile(String workerId) {\n+        return ConfigUtils.workerArtifactsSymlink(conf, workerId) + FILE_SEPARATOR + \"container-\" + workerId + \".pid\";\n+    }\n+\n+    @Override\n+    public void launchWorkerProcess(String user, String topologyId,  Map<String, Object> topoConf,\n+                                    int port, String workerId,\n+                                    List<String> command, Map<String, String> env, String logPrefix,\n+                                    ExitCodeCallback processExitCallback, File targetDir) throws IOException {\n+\n+        String imageName = getImageName(topoConf);\n+        if (imageName == null) {\n+            LOG.error(\"Image name for {} is not configured properly; will not continue to launch the worker\", topologyId);\n+            return;\n+        }\n+\n+        //set container ID to port + worker ID\n+        String containerId = getContainerId(workerId, port);\n+\n+        //get manifest\n+        ImageManifest manifest = imageTagToManifestPlugin.getManifestFromImageTag(imageName);\n+        LOG.debug(\"workerId {}: Got manifest: {}\", workerId, manifest.toString());\n+\n+        //get layers metadata\n+        OciResource configResource = manifestToResourcesPlugin.getConfigResource(manifest);\n+        LOG.info(\"workerId {}: Got config metadata: {}\", workerId, configResource.toString());\n+\n+        saveRuncYaml(topologyId, port, containerId, imageName, configResource);\n+\n+        List<OciResource> layersResource = manifestToResourcesPlugin.getLayerResources(manifest);\n+        LOG.info(\"workerId {}: Got layers metadata: {}\", workerId, layersResource.toString());\n+\n+        //localize resource\n+        String configLocalPath = ociResourcesLocalizer.localize(configResource);\n+\n+        List<String> ociEnv = new ArrayList<>();\n+        List<String> args = new ArrayList<>();\n+\n+        ArrayList<OciLayer> layers = new ArrayList<>();\n+\n+        File file = new File(configLocalPath);\n+        //extract env\n+        List<String> imageEnv = extractImageEnv(file);\n+        if (imageEnv != null && !imageEnv.isEmpty()) {\n+            ociEnv.addAll(imageEnv);\n+        }\n+        for (Map.Entry<String, String> entry : env.entrySet()) {\n+            ociEnv.add(entry.getKey() + \"=\" + entry.getValue());\n+        }\n+        LOG.debug(\"workerId {}: ociEnv: {}\", workerId, ociEnv);\n+\n+        //extract entrypoint\n+        List<String> entrypoint = extractImageEntrypoint(file);\n+        if (entrypoint != null && !entrypoint.isEmpty()) {\n+            args.addAll(entrypoint);\n+        }\n+        LOG.debug(\"workerId {}: args: {}\", workerId, args);\n+\n+        //localize layers\n+        List<String> layersLocalPath = ociResourcesLocalizer.localize((layersResource));\n+        //compose layers\n+        for (String layerLocalPath : layersLocalPath) {\n+            OciLayer layer = new OciLayer(SQUASHFS_MEDIA_TYPE, layerLocalPath);\n+            layers.add(layer);\n+        }\n+        LOG.debug(\"workerId {}: layers: {}\", workerId, layers);\n+        ArrayList<OciMount> mounts = new ArrayList<>();\n+        setContainerMounts(mounts, topologyId, workerId, port);\n+        LOG.debug(\"workerId {}: mounts: {}\", workerId, mounts);\n+\n+        //calculate the cpusQuotas based on CPU_CFS_PERIOD and assigned CPU\n+        Long cpusQuotas = null;\n+        if (workerToCpu.containsKey(workerId)) {\n+            cpusQuotas = workerToCpu.get(workerId) * CPU_CFS_PERIOD_US / 100;\n+        }\n+\n+        Long memoryInBytes = null;\n+        if (workerToMemoryMb.containsKey(workerId)) {\n+            memoryInBytes = workerToMemoryMb.get(workerId) * 1024 * 1024L;\n+        }\n+        LOG.info(\"workerId {}: memoryInBytes set to {}; cpusQuotas set to {}\", workerId, memoryInBytes, cpusQuotas);\n+\n+        //<workerRoot>/<workerId>\n+        String workerDir = targetDir.getAbsolutePath();\n+        String workerScriptPath = ServerUtils.writeScript(workerDir, command, env, \"0027\");\n+\n+        args.add(\"bash\");\n+        args.add(workerScriptPath);\n+\n+        //The container PID (on the host) will be written to this file.\n+        String containerPidFilePath = containerPidFile(workerId);\n+\n+        OciProcessConfig processConfig = createOciProcessConfig(workerDir, ociEnv, args);\n+\n+        OciLinuxConfig linuxConfig =\n+            createOciLinuxConfig(cpusQuotas, memoryInBytes, cgroupParent + \"/\" + containerId, seccomp, workerId);\n+\n+        OciRuntimeConfig ociRuntimeConfig = new OciRuntimeConfig(null, mounts, processConfig, null,\n+                                                          null, null, linuxConfig);\n+\n+        OciContainerExecutorConfig ociContainerExecutorConfig =\n+            createOciContainerExecutorConfig(user, containerId, containerPidFilePath,\n+                                             workerScriptPath, null, null, null, layers, ociRuntimeConfig);\n+\n+        //launch the container using worker-launcher\n+        String executorConfigToJsonFile = writeOciExecutorConfigToJsonFile(mapper, ociContainerExecutorConfig, workerDir);\n+        LOG.info(\"workerId {}: oci-config.json file path: {}\", workerId, executorConfigToJsonFile);\n+\n+        List<String> cmdArgs = Arrays.asList(CmdType.RUN_OCI_CONTAINER.toString(), workerDir, executorConfigToJsonFile,\n+                                             ConfigUtils.workerArtifactsSymlink(conf, workerId));\n+\n+        // launch the oci container. waiting prevents possible race condition that could prevent cleanup of container\n+        int exitCode = ClientSupervisorUtils.processLauncherAndWait(conf, user, cmdArgs, env, logPrefix, targetDir);\n+        if (exitCode != 0) {\n+            LOG.error(\"launchWorkerProcess RuncCommand {} exited with code: {}\", \"LaunchWorker-\" + containerId, exitCode);\n+            throw new RuntimeException(\"launchWorkerProcess Failed to create Runc Container. ContainerId: \" + containerId);\n+        }\n+\n+        //Add to the watched list\n+        LOG.debug(\"Adding {} to the watched workers list\", workerId);\n+        workerToExitCallback.put(workerId, processExitCallback);\n+        workerToUser.put(workerId, user);\n+\n+    }\n+\n+    private void checkContainersAlive() {\n+        //Check if all watched workers are still alive\n+        workerToUser.forEach((workerId, user) -> {\n+            if (isContainerDead(workerId, user)) {\n+                invokeProcessExitCallback(workerId);\n+            }\n+        });\n+    }\n+\n+    private boolean isContainerDead(String workerId, String user) {\n+        boolean isDead = true;\n+        Long pid = getContainerPid(workerId);\n+        LOG.debug(\"Checking container {}, pid {}, user {}\", workerId, pid, user);\n+        //do nothing if pid is null.\n+        if (pid != null && user != null) {\n+            try {\n+                isDead = ServerUtils.areAllProcessesDead(conf, user, workerId, Collections.singleton(pid));\n+            } catch (IOException e) {\n+                //ignore\n+                LOG.debug(\"Error while checking if container is dead.\", e);\n+            }\n+        }\n+        return isDead;\n+    }\n+\n+    private void invokeProcessExitCallback(String workerId) {\n+        LOG.info(\"processExitCallback returned for workerId {}\", workerId);\n+        ExitCodeCallback processExitCallback = workerToExitCallback.get(workerId);\n+        if (processExitCallback != null) {\n+            processExitCallback.call(0);\n+        }\n+    }\n+\n+    private String getContainerId(String workerId, int port) throws IOException {\n+        if (port <= 0) { // when killing workers, we will have the workerId and a port of -1\n+            return getContainerIdFromOciJson(workerId);\n+        }\n+        return port + \"-\" + workerId;\n+    }\n+\n+    private String getContainerIdFromOciJson(String workerId) throws IOException {\n+        String ociJson = ConfigUtils.workerRoot(conf, workerId) + FILE_SEPARATOR + OCI_CONFIG_JSON;\n+        LOG.info(\"port unknown for workerId {}, looking up from {}\", workerId, ociJson);\n+        JSONParser parser = new JSONParser();\n+\n+        try (Reader reader = new FileReader(ociJson)) {\n+            JSONObject jsonObject = (JSONObject) parser.parse(reader);\n+            return (String) jsonObject.get(\"containerId\");\n+        } catch (ParseException e) {\n+            throw new IOException(\"Unable to parse {}\", e);\n+        }\n+    }\n+\n+    // save runc.yaml in artifacts dir so we can track which image the worker was launched with\n+    private void saveRuncYaml(String topologyId, int port, String containerId, String imageName, OciResource configResource) {\n+        String fname = String.format(\"runc-%s.yaml\", containerId);\n+        File file = new File(ConfigUtils.workerArtifactsRoot(conf, topologyId, port), fname);\n+        DumperOptions options = new DumperOptions();\n+        options.setIndent(2);\n+        options.setPrettyFlow(true);\n+        options.setDefaultFlowStyle(DumperOptions.FlowStyle.BLOCK);\n+        Yaml yaml = new Yaml(options);\n+        Map<String, Object> data = new HashMap<>();\n+        data.put(\"imageName\", imageName);\n+        data.put(\"manifest\", configResource.getFileName());\n+        data.put(\"configPath\", configResource.getPath());\n+        try (Writer writer = new FileWriter(file)) {\n+            yaml.dump(data, writer);\n+        } catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    private String writeOciExecutorConfigToJsonFile(ObjectMapper mapper, OciContainerExecutorConfig ociContainerExecutorConfig,\n+                                                    String workerDir) throws IOException {\n+        File cmdDir = new File(workerDir);\n+        if (!cmdDir.exists()) {\n+            throw new IOException(workerDir + \" doesn't exist\");\n+        }\n+\n+        File commandFile = new File(cmdDir + FILE_SEPARATOR + OCI_CONFIG_JSON);\n+        mapper.writeValue(commandFile, ociContainerExecutorConfig);\n+        return commandFile.getAbsolutePath();\n+    }\n+\n+    private void setContainerMounts(ArrayList<OciMount> mounts, String topologyId, String workerId, Integer port) throws IOException {\n+        //read-only bindmounts need to be added before read-write bindmounts otherwise read-write bindmounts may be overridden.\n+        for (String readonlyMount : readonlyBindmounts) {\n+            addOciMountLocation(mounts, readonlyMount, readonlyMount, false, false);\n+        }\n+\n+        for (String readwriteMount : readwriteBindmounts) {\n+            addOciMountLocation(mounts, readwriteMount, readwriteMount, false, true);\n+        }\n+\n+        addOciMountLocation(mounts, RESOLV_CONF, RESOLV_CONF, false, false);\n+        addOciMountLocation(mounts, HOSTNAME, HOSTNAME, false, false);\n+        addOciMountLocation(mounts, HOSTS, HOSTS, false, false);\n+        addOciMountLocation(mounts, nscdPath, nscdPath, false, false);\n+        addOciMountLocation(mounts, stormHome, stormHome, false, false);\n+        addOciMountLocation(mounts, cgroupRootPath, cgroupRootPath, false, false);\n+\n+        //set of locations to be bind mounted\n+        String supervisorLocalDir = ConfigUtils.supervisorLocalDir(conf);\n+        addOciMountLocation(mounts, supervisorLocalDir, supervisorLocalDir, false, false);\n+\n+        String workerRootDir = ConfigUtils.workerRoot(conf, workerId);\n+        addOciMountLocation(mounts, workerRootDir, workerRootDir, false, true);\n+\n+        String workerArtifactsRoot = ConfigUtils.workerArtifactsRoot(conf, topologyId, port);\n+        addOciMountLocation(mounts, workerArtifactsRoot, workerArtifactsRoot, false, true);\n+\n+        String workerUserFile = ConfigUtils.workerUserFile(conf, workerId);\n+        addOciMountLocation(mounts, workerUserFile, workerUserFile, false, true);\n+\n+        String sharedByTopologyDir = ConfigUtils.sharedByTopologyDir(conf, topologyId);\n+        addOciMountLocation(mounts, sharedByTopologyDir, sharedByTopologyDir, false, true);\n+\n+        String workerTmpRoot = ConfigUtils.workerTmpRoot(conf, workerId);\n+        addOciMountLocation(mounts, workerTmpRoot, TMP_DIR, false, true);\n+    }\n+\n+    private List<String> extractImageEnv(File config) throws IOException {\n+        JsonNode node = mapper.readTree(config);\n+        JsonNode envNode = node.path(\"config\").path(\"Env\");\n+        if (envNode.isMissingNode()) {\n+            return null;\n+        }\n+        return mapper.treeToValue(envNode, List.class);\n+    }\n+\n+    private List<String> extractImageEntrypoint(File config) throws IOException {\n+        JsonNode node = mapper.readTree(config);\n+        JsonNode entrypointNode = node.path(\"config\").path(\"Entrypoint\");\n+        if (entrypointNode.isMissingNode()) {\n+            return null;\n+        }\n+        return mapper.treeToValue(entrypointNode, List.class);\n+    }\n+\n+    private OciContainerExecutorConfig createOciContainerExecutorConfig(\n+            String username, String containerId, String pidFile,\n+            String containerScriptPath, String containerCredentialsPath,\n+            List<String> localDirs, List<String> logDirs,\n+            List<OciLayer> layers, OciRuntimeConfig ociRuntimeConfig) {\n+\n+        return new OciContainerExecutorConfig(username, containerId,\n+                pidFile, containerScriptPath, containerCredentialsPath,\n+                localDirs, logDirs, layers, layersToKeep, ociRuntimeConfig);\n+    }\n+\n+    private OciProcessConfig createOciProcessConfig(String cwd,\n+                                                    List<String> env, List<String> args) {\n+        return new OciProcessConfig(false, null, cwd, env,\n+                args, null, null, null, true, 0, null, null);\n+    }\n+\n+    private OciLinuxConfig createOciLinuxConfig(Long cpusQuotas, Long memInBytes,\n+                                                String cgroupsPath, String seccomp, String workerId) {\n+        OciLinuxConfig.Resources.Cpu cgroupCpu = null;\n+\n+        if (cpusQuotas != null) {\n+            cgroupCpu = new OciLinuxConfig.Resources.Cpu(0, cpusQuotas, CPU_CFS_PERIOD_US, 0, 0,\n+                    null, null);\n+\n+            if (workerToCores.containsKey(workerId)) {\n+                cgroupCpu.setCpus(StringUtils.join(workerToCores.get(workerId), \",\"));\n+                cgroupCpu.setMems(workerToMemoryZone.get(workerId));\n+            }\n+        }\n+\n+        OciLinuxConfig.Resources.Memory cgroupMem = null;\n+        if (memInBytes != null) {\n+            cgroupMem = new OciLinuxConfig.Resources.Memory(memInBytes, 0, 0, 0, 0, 0, false);\n+        }\n+\n+        OciLinuxConfig.Resources cgroupResources =\n+                new OciLinuxConfig.Resources(null, cgroupMem, cgroupCpu, null, null, null,\n+                        null, null);\n+\n+        return new OciLinuxConfig(null, null, null, null,\n+                cgroupsPath, cgroupResources, null, null, seccomp, null, null,\n+                null, null);\n+    }\n+\n+    private void addOciMountLocation(List<OciMount> mounts, String srcPath,\n+                                     String dstPath, boolean createSource, boolean isReadWrite) throws IOException {\n+        if (!createSource) {\n+            boolean sourceExists = new File(srcPath).exists();\n+            if (!sourceExists) {\n+                throw new IOException(\"SourcePath \" + srcPath + \" doesn't exit\");\n+            }\n+        }\n+\n+        ArrayList<String> options = new ArrayList<>();\n+        if (isReadWrite) {\n+            options.add(\"rw\");\n+        } else {\n+            options.add(\"ro\");\n+        }\n+        options.add(\"rbind\");\n+        options.add(\"rprivate\");\n+        mounts.add(new OciMount(dstPath, \"bind\", srcPath, options));\n+    }\n+\n+    @Override\n+    public long getMemoryUsage(String user, String workerId, int port) throws IOException {\n+        // \"/sys/fs/cgroup/memory/storm/containerId/\"\n+        String containerId = getContainerId(workerId, port);\n+        String memoryCgroupPath = memoryCgroupRootPath + File.separator  + containerId;\n+        MemoryCore memoryCore = new MemoryCore(memoryCgroupPath);\n+        LOG.debug(\"ContainerId {} : Got memory getPhysicalUsage {} from {}\", containerId, memoryCore.getPhysicalUsage(), memoryCgroupPath);\n+        return memoryCore.getPhysicalUsage();\n+    }\n+\n+    @Override\n+    public void kill(String user, String workerId) throws IOException {\n+        LOG.info(\"Killing {}\", workerId);\n+        Long pid = getContainerPid(workerId);\n+        if (pid != null) {\n+            signal(pid, 15, user);\n+        } else {\n+            LOG.warn(\"Trying to kill container {} but pidfile is not found\", workerId);\n+        }\n+    }\n+\n+    private void signal(long pid, int signal, String user) throws IOException {\n+        List<String> commands = Arrays.asList(\"signal\", String.valueOf(pid), String.valueOf(signal));\n+        String logPrefix = \"kill -\" + signal + \" \" + pid;\n+        ClientSupervisorUtils.processLauncherAndWait(conf, user, commands, null, logPrefix);\n+    }\n+\n+    @Override\n+    public void forceKill(String user, String workerId) throws IOException {\n+        LOG.debug(\"ForceKilling {}\", workerId);\n+        Long pid = getContainerPid(workerId);\n+        if (pid != null) {\n+            signal(pid, 9, user);\n+        } else {\n+            LOG.warn(\"Trying to forceKill container {} but pidfile is not found\", workerId);\n+        }\n+    }\n+\n+    // return null if not found.\n+    private Long getContainerPid(String workerId) {\n+        Long pid = workerToContainerPid.get(workerId);\n+        if (pid == null) {\n+            String containerPidFilePath = containerPidFile(workerId);\n+            if (!new File(containerPidFilePath).exists()) {\n+                LOG.warn(\"{} doesn't exist\", containerPidFilePath);\n+            } else {\n+                try {\n+                    pid = Long.parseLong(CgroupUtils.readFileByLine(containerPidFilePath).get(0));\n+                    workerToContainerPid.put(workerId, pid);\n+                } catch (IOException e) {\n+                    LOG.warn(\"failed to read {}\", containerPidFilePath);\n+                }\n+            }\n+        }\n+        return pid;\n+    }\n+\n+    @Override\n+    public void releaseResourcesForWorker(String workerId) {\n+        super.releaseResourcesForWorker(workerId);\n+        workerToContainerPid.remove(workerId);", "originalCommit": "690e02091244f33b32a1dc3c449413e6aff1d964", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3MTg5NjE1MA==", "url": "https://github.com/apache/storm/pull/3366#discussion_r671896150", "bodyText": "yes it is done in cleanup.\nMoving workerToContainerPid.remove(workerId); to cleanup as it makes more sense to me", "author": "Ethanlm", "createdAt": "2021-07-18T20:53:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDU4NzUyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDU4ODk5Nw==", "url": "https://github.com/apache/storm/pull/3366#discussion_r550588997", "bodyText": "call releaseResourcesForWorker() instead?\nIt looks like releaseResourcesForWorker() is called before cleanup() already.\nDo we need both?\nI didn't see a javadoc for cleanup().  I'm fuzzy on the difference.  I am all for combining into one if we can.", "author": "agresch", "createdAt": "2020-12-31T17:38:22Z", "path": "storm-server/src/main/java/org/apache/storm/container/oci/RuncLibContainerManager.java", "diffHunk": "@@ -0,0 +1,631 @@\n+/*\n+ *\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.storm.container.oci;\n+\n+import static org.apache.storm.utils.ConfigUtils.FILE_SEPARATOR;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.FileReader;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.io.Reader;\n+import java.io.Writer;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.StormTimer;\n+import org.apache.storm.container.cgroup.CgroupUtils;\n+import org.apache.storm.container.cgroup.core.MemoryCore;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciLayer;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig.OciLinuxConfig;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig.OciMount;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig.OciProcessConfig;\n+import org.apache.storm.daemon.supervisor.ClientSupervisorUtils;\n+import org.apache.storm.daemon.supervisor.ExitCodeCallback;\n+import org.apache.storm.utils.ConfigUtils;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.ReflectionUtils;\n+import org.apache.storm.utils.ServerUtils;\n+import org.apache.storm.utils.Utils;\n+\n+import org.json.simple.JSONObject;\n+import org.json.simple.parser.JSONParser;\n+import org.json.simple.parser.ParseException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.yaml.snakeyaml.DumperOptions;\n+import org.yaml.snakeyaml.Yaml;\n+\n+public class RuncLibContainerManager extends OciContainerManager {\n+    private static final Logger LOG = LoggerFactory.getLogger(RuncLibContainerManager.class);\n+\n+    private OciImageTagToManifestPluginInterface imageTagToManifestPlugin;\n+    private OciManifestToResourcesPluginInterface manifestToResourcesPlugin;\n+    private OciResourcesLocalizerInterface ociResourcesLocalizer;\n+    private ObjectMapper mapper;\n+    private int layersToKeep;\n+    private String seccomp;\n+\n+    private static final String RESOLV_CONF = \"/etc/resolv.conf\";\n+    private static final String HOSTNAME = \"/etc/hostname\";\n+    private static final String HOSTS = \"/etc/hosts\";\n+    private static final String OCI_CONFIG_JSON = \"oci-config.json\";\n+\n+    private static final String SQUASHFS_MEDIA_TYPE = \"application/vnd.squashfs\";\n+\n+    //CPU CFS (Completely Fair Scheduler) period\n+    private static final long CPU_CFS_PERIOD_US = 100000;\n+\n+    private Map<String, Long> workerToContainerPid = new ConcurrentHashMap<>();\n+    private Map<String, ExitCodeCallback> workerToExitCallback = new ConcurrentHashMap<>();\n+    private Map<String, String> workerToUser = new ConcurrentHashMap<>();\n+    private StormTimer checkContainerAliveTimer;\n+\n+    @Override\n+    public void prepare(Map<String, Object> conf) throws IOException {\n+        super.prepare(conf);\n+\n+        imageTagToManifestPlugin = chooseImageTagToManifestPlugin();\n+        imageTagToManifestPlugin.init(conf);\n+\n+        manifestToResourcesPlugin = chooseManifestToResourcesPlugin();\n+        manifestToResourcesPlugin.init(conf);\n+\n+        ociResourcesLocalizer = chooseOciResourcesLocalizer();\n+        ociResourcesLocalizer.init(conf);\n+\n+        layersToKeep = ObjectReader.getInt(\n+                conf.get(DaemonConfig.STORM_OCI_LAYER_MOUNTS_TO_KEEP),\n+                100\n+        );\n+\n+        mapper = new ObjectMapper();\n+\n+        if (seccompJsonFile != null) {\n+            seccomp = new String(Files.readAllBytes(Paths.get(seccompJsonFile)));\n+        }\n+\n+        if (checkContainerAliveTimer == null) {\n+            checkContainerAliveTimer =\n+                new StormTimer(\"CheckRuncContainerAlive\", Utils.createDefaultUncaughtExceptionHandler());\n+            checkContainerAliveTimer\n+                .scheduleRecurring(0, (Integer) conf.get(DaemonConfig.SUPERVISOR_MONITOR_FREQUENCY_SECS), () -> {\n+                    try {\n+                        checkContainersAlive();\n+                    } catch (Exception e) {\n+                        //Ignore\n+                        LOG.debug(\"The CheckRuncContainerAlive thread has exception. Ignored\", e);\n+                    }\n+                });\n+        }\n+    }\n+\n+    private OciImageTagToManifestPluginInterface chooseImageTagToManifestPlugin() throws IllegalArgumentException {\n+        String pluginName = ObjectReader.getString(\n+                conf.get(DaemonConfig.STORM_OCI_IMAGE_TAG_TO_MANIFEST_PLUGIN)\n+        );\n+        LOG.info(\"imageTag-to-manifest Plugin is: {}\", pluginName);\n+        return ReflectionUtils.newInstance(pluginName);\n+    }\n+\n+    private OciManifestToResourcesPluginInterface chooseManifestToResourcesPlugin() throws IllegalArgumentException {\n+        String pluginName = ObjectReader.getString(\n+                conf.get(DaemonConfig.STORM_OCI_MANIFEST_TO_RESOURCES_PLUGIN)\n+        );\n+        LOG.info(\"manifest to resource Plugin is: {}\", pluginName);\n+        return ReflectionUtils.newInstance(pluginName);\n+    }\n+\n+    private OciResourcesLocalizerInterface chooseOciResourcesLocalizer()\n+        throws IllegalArgumentException {\n+        String pluginName = ObjectReader.getString(\n+                conf.get(DaemonConfig.STORM_OCI_RESOURCES_LOCALIZER)\n+        );\n+        LOG.info(\"oci resource localizer is: {}\", pluginName);\n+        return ReflectionUtils.newInstance(pluginName);\n+    }\n+\n+    //the container process ID in the process namespace of the host.\n+    private String containerPidFile(String workerId) {\n+        return ConfigUtils.workerArtifactsSymlink(conf, workerId) + FILE_SEPARATOR + \"container-\" + workerId + \".pid\";\n+    }\n+\n+    @Override\n+    public void launchWorkerProcess(String user, String topologyId,  Map<String, Object> topoConf,\n+                                    int port, String workerId,\n+                                    List<String> command, Map<String, String> env, String logPrefix,\n+                                    ExitCodeCallback processExitCallback, File targetDir) throws IOException {\n+\n+        String imageName = getImageName(topoConf);\n+        if (imageName == null) {\n+            LOG.error(\"Image name for {} is not configured properly; will not continue to launch the worker\", topologyId);\n+            return;\n+        }\n+\n+        //set container ID to port + worker ID\n+        String containerId = getContainerId(workerId, port);\n+\n+        //get manifest\n+        ImageManifest manifest = imageTagToManifestPlugin.getManifestFromImageTag(imageName);\n+        LOG.debug(\"workerId {}: Got manifest: {}\", workerId, manifest.toString());\n+\n+        //get layers metadata\n+        OciResource configResource = manifestToResourcesPlugin.getConfigResource(manifest);\n+        LOG.info(\"workerId {}: Got config metadata: {}\", workerId, configResource.toString());\n+\n+        saveRuncYaml(topologyId, port, containerId, imageName, configResource);\n+\n+        List<OciResource> layersResource = manifestToResourcesPlugin.getLayerResources(manifest);\n+        LOG.info(\"workerId {}: Got layers metadata: {}\", workerId, layersResource.toString());\n+\n+        //localize resource\n+        String configLocalPath = ociResourcesLocalizer.localize(configResource);\n+\n+        List<String> ociEnv = new ArrayList<>();\n+        List<String> args = new ArrayList<>();\n+\n+        ArrayList<OciLayer> layers = new ArrayList<>();\n+\n+        File file = new File(configLocalPath);\n+        //extract env\n+        List<String> imageEnv = extractImageEnv(file);\n+        if (imageEnv != null && !imageEnv.isEmpty()) {\n+            ociEnv.addAll(imageEnv);\n+        }\n+        for (Map.Entry<String, String> entry : env.entrySet()) {\n+            ociEnv.add(entry.getKey() + \"=\" + entry.getValue());\n+        }\n+        LOG.debug(\"workerId {}: ociEnv: {}\", workerId, ociEnv);\n+\n+        //extract entrypoint\n+        List<String> entrypoint = extractImageEntrypoint(file);\n+        if (entrypoint != null && !entrypoint.isEmpty()) {\n+            args.addAll(entrypoint);\n+        }\n+        LOG.debug(\"workerId {}: args: {}\", workerId, args);\n+\n+        //localize layers\n+        List<String> layersLocalPath = ociResourcesLocalizer.localize((layersResource));\n+        //compose layers\n+        for (String layerLocalPath : layersLocalPath) {\n+            OciLayer layer = new OciLayer(SQUASHFS_MEDIA_TYPE, layerLocalPath);\n+            layers.add(layer);\n+        }\n+        LOG.debug(\"workerId {}: layers: {}\", workerId, layers);\n+        ArrayList<OciMount> mounts = new ArrayList<>();\n+        setContainerMounts(mounts, topologyId, workerId, port);\n+        LOG.debug(\"workerId {}: mounts: {}\", workerId, mounts);\n+\n+        //calculate the cpusQuotas based on CPU_CFS_PERIOD and assigned CPU\n+        Long cpusQuotas = null;\n+        if (workerToCpu.containsKey(workerId)) {\n+            cpusQuotas = workerToCpu.get(workerId) * CPU_CFS_PERIOD_US / 100;\n+        }\n+\n+        Long memoryInBytes = null;\n+        if (workerToMemoryMb.containsKey(workerId)) {\n+            memoryInBytes = workerToMemoryMb.get(workerId) * 1024 * 1024L;\n+        }\n+        LOG.info(\"workerId {}: memoryInBytes set to {}; cpusQuotas set to {}\", workerId, memoryInBytes, cpusQuotas);\n+\n+        //<workerRoot>/<workerId>\n+        String workerDir = targetDir.getAbsolutePath();\n+        String workerScriptPath = ServerUtils.writeScript(workerDir, command, env, \"0027\");\n+\n+        args.add(\"bash\");\n+        args.add(workerScriptPath);\n+\n+        //The container PID (on the host) will be written to this file.\n+        String containerPidFilePath = containerPidFile(workerId);\n+\n+        OciProcessConfig processConfig = createOciProcessConfig(workerDir, ociEnv, args);\n+\n+        OciLinuxConfig linuxConfig =\n+            createOciLinuxConfig(cpusQuotas, memoryInBytes, cgroupParent + \"/\" + containerId, seccomp, workerId);\n+\n+        OciRuntimeConfig ociRuntimeConfig = new OciRuntimeConfig(null, mounts, processConfig, null,\n+                                                          null, null, linuxConfig);\n+\n+        OciContainerExecutorConfig ociContainerExecutorConfig =\n+            createOciContainerExecutorConfig(user, containerId, containerPidFilePath,\n+                                             workerScriptPath, null, null, null, layers, ociRuntimeConfig);\n+\n+        //launch the container using worker-launcher\n+        String executorConfigToJsonFile = writeOciExecutorConfigToJsonFile(mapper, ociContainerExecutorConfig, workerDir);\n+        LOG.info(\"workerId {}: oci-config.json file path: {}\", workerId, executorConfigToJsonFile);\n+\n+        List<String> cmdArgs = Arrays.asList(CmdType.RUN_OCI_CONTAINER.toString(), workerDir, executorConfigToJsonFile,\n+                                             ConfigUtils.workerArtifactsSymlink(conf, workerId));\n+\n+        // launch the oci container. waiting prevents possible race condition that could prevent cleanup of container\n+        int exitCode = ClientSupervisorUtils.processLauncherAndWait(conf, user, cmdArgs, env, logPrefix, targetDir);\n+        if (exitCode != 0) {\n+            LOG.error(\"launchWorkerProcess RuncCommand {} exited with code: {}\", \"LaunchWorker-\" + containerId, exitCode);\n+            throw new RuntimeException(\"launchWorkerProcess Failed to create Runc Container. ContainerId: \" + containerId);\n+        }\n+\n+        //Add to the watched list\n+        LOG.debug(\"Adding {} to the watched workers list\", workerId);\n+        workerToExitCallback.put(workerId, processExitCallback);\n+        workerToUser.put(workerId, user);\n+\n+    }\n+\n+    private void checkContainersAlive() {\n+        //Check if all watched workers are still alive\n+        workerToUser.forEach((workerId, user) -> {\n+            if (isContainerDead(workerId, user)) {\n+                invokeProcessExitCallback(workerId);\n+            }\n+        });\n+    }\n+\n+    private boolean isContainerDead(String workerId, String user) {\n+        boolean isDead = true;\n+        Long pid = getContainerPid(workerId);\n+        LOG.debug(\"Checking container {}, pid {}, user {}\", workerId, pid, user);\n+        //do nothing if pid is null.\n+        if (pid != null && user != null) {\n+            try {\n+                isDead = ServerUtils.areAllProcessesDead(conf, user, workerId, Collections.singleton(pid));\n+            } catch (IOException e) {\n+                //ignore\n+                LOG.debug(\"Error while checking if container is dead.\", e);\n+            }\n+        }\n+        return isDead;\n+    }\n+\n+    private void invokeProcessExitCallback(String workerId) {\n+        LOG.info(\"processExitCallback returned for workerId {}\", workerId);\n+        ExitCodeCallback processExitCallback = workerToExitCallback.get(workerId);\n+        if (processExitCallback != null) {\n+            processExitCallback.call(0);\n+        }\n+    }\n+\n+    private String getContainerId(String workerId, int port) throws IOException {\n+        if (port <= 0) { // when killing workers, we will have the workerId and a port of -1\n+            return getContainerIdFromOciJson(workerId);\n+        }\n+        return port + \"-\" + workerId;\n+    }\n+\n+    private String getContainerIdFromOciJson(String workerId) throws IOException {\n+        String ociJson = ConfigUtils.workerRoot(conf, workerId) + FILE_SEPARATOR + OCI_CONFIG_JSON;\n+        LOG.info(\"port unknown for workerId {}, looking up from {}\", workerId, ociJson);\n+        JSONParser parser = new JSONParser();\n+\n+        try (Reader reader = new FileReader(ociJson)) {\n+            JSONObject jsonObject = (JSONObject) parser.parse(reader);\n+            return (String) jsonObject.get(\"containerId\");\n+        } catch (ParseException e) {\n+            throw new IOException(\"Unable to parse {}\", e);\n+        }\n+    }\n+\n+    // save runc.yaml in artifacts dir so we can track which image the worker was launched with\n+    private void saveRuncYaml(String topologyId, int port, String containerId, String imageName, OciResource configResource) {\n+        String fname = String.format(\"runc-%s.yaml\", containerId);\n+        File file = new File(ConfigUtils.workerArtifactsRoot(conf, topologyId, port), fname);\n+        DumperOptions options = new DumperOptions();\n+        options.setIndent(2);\n+        options.setPrettyFlow(true);\n+        options.setDefaultFlowStyle(DumperOptions.FlowStyle.BLOCK);\n+        Yaml yaml = new Yaml(options);\n+        Map<String, Object> data = new HashMap<>();\n+        data.put(\"imageName\", imageName);\n+        data.put(\"manifest\", configResource.getFileName());\n+        data.put(\"configPath\", configResource.getPath());\n+        try (Writer writer = new FileWriter(file)) {\n+            yaml.dump(data, writer);\n+        } catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    private String writeOciExecutorConfigToJsonFile(ObjectMapper mapper, OciContainerExecutorConfig ociContainerExecutorConfig,\n+                                                    String workerDir) throws IOException {\n+        File cmdDir = new File(workerDir);\n+        if (!cmdDir.exists()) {\n+            throw new IOException(workerDir + \" doesn't exist\");\n+        }\n+\n+        File commandFile = new File(cmdDir + FILE_SEPARATOR + OCI_CONFIG_JSON);\n+        mapper.writeValue(commandFile, ociContainerExecutorConfig);\n+        return commandFile.getAbsolutePath();\n+    }\n+\n+    private void setContainerMounts(ArrayList<OciMount> mounts, String topologyId, String workerId, Integer port) throws IOException {\n+        //read-only bindmounts need to be added before read-write bindmounts otherwise read-write bindmounts may be overridden.\n+        for (String readonlyMount : readonlyBindmounts) {\n+            addOciMountLocation(mounts, readonlyMount, readonlyMount, false, false);\n+        }\n+\n+        for (String readwriteMount : readwriteBindmounts) {\n+            addOciMountLocation(mounts, readwriteMount, readwriteMount, false, true);\n+        }\n+\n+        addOciMountLocation(mounts, RESOLV_CONF, RESOLV_CONF, false, false);\n+        addOciMountLocation(mounts, HOSTNAME, HOSTNAME, false, false);\n+        addOciMountLocation(mounts, HOSTS, HOSTS, false, false);\n+        addOciMountLocation(mounts, nscdPath, nscdPath, false, false);\n+        addOciMountLocation(mounts, stormHome, stormHome, false, false);\n+        addOciMountLocation(mounts, cgroupRootPath, cgroupRootPath, false, false);\n+\n+        //set of locations to be bind mounted\n+        String supervisorLocalDir = ConfigUtils.supervisorLocalDir(conf);\n+        addOciMountLocation(mounts, supervisorLocalDir, supervisorLocalDir, false, false);\n+\n+        String workerRootDir = ConfigUtils.workerRoot(conf, workerId);\n+        addOciMountLocation(mounts, workerRootDir, workerRootDir, false, true);\n+\n+        String workerArtifactsRoot = ConfigUtils.workerArtifactsRoot(conf, topologyId, port);\n+        addOciMountLocation(mounts, workerArtifactsRoot, workerArtifactsRoot, false, true);\n+\n+        String workerUserFile = ConfigUtils.workerUserFile(conf, workerId);\n+        addOciMountLocation(mounts, workerUserFile, workerUserFile, false, true);\n+\n+        String sharedByTopologyDir = ConfigUtils.sharedByTopologyDir(conf, topologyId);\n+        addOciMountLocation(mounts, sharedByTopologyDir, sharedByTopologyDir, false, true);\n+\n+        String workerTmpRoot = ConfigUtils.workerTmpRoot(conf, workerId);\n+        addOciMountLocation(mounts, workerTmpRoot, TMP_DIR, false, true);\n+    }\n+\n+    private List<String> extractImageEnv(File config) throws IOException {\n+        JsonNode node = mapper.readTree(config);\n+        JsonNode envNode = node.path(\"config\").path(\"Env\");\n+        if (envNode.isMissingNode()) {\n+            return null;\n+        }\n+        return mapper.treeToValue(envNode, List.class);\n+    }\n+\n+    private List<String> extractImageEntrypoint(File config) throws IOException {\n+        JsonNode node = mapper.readTree(config);\n+        JsonNode entrypointNode = node.path(\"config\").path(\"Entrypoint\");\n+        if (entrypointNode.isMissingNode()) {\n+            return null;\n+        }\n+        return mapper.treeToValue(entrypointNode, List.class);\n+    }\n+\n+    private OciContainerExecutorConfig createOciContainerExecutorConfig(\n+            String username, String containerId, String pidFile,\n+            String containerScriptPath, String containerCredentialsPath,\n+            List<String> localDirs, List<String> logDirs,\n+            List<OciLayer> layers, OciRuntimeConfig ociRuntimeConfig) {\n+\n+        return new OciContainerExecutorConfig(username, containerId,\n+                pidFile, containerScriptPath, containerCredentialsPath,\n+                localDirs, logDirs, layers, layersToKeep, ociRuntimeConfig);\n+    }\n+\n+    private OciProcessConfig createOciProcessConfig(String cwd,\n+                                                    List<String> env, List<String> args) {\n+        return new OciProcessConfig(false, null, cwd, env,\n+                args, null, null, null, true, 0, null, null);\n+    }\n+\n+    private OciLinuxConfig createOciLinuxConfig(Long cpusQuotas, Long memInBytes,\n+                                                String cgroupsPath, String seccomp, String workerId) {\n+        OciLinuxConfig.Resources.Cpu cgroupCpu = null;\n+\n+        if (cpusQuotas != null) {\n+            cgroupCpu = new OciLinuxConfig.Resources.Cpu(0, cpusQuotas, CPU_CFS_PERIOD_US, 0, 0,\n+                    null, null);\n+\n+            if (workerToCores.containsKey(workerId)) {\n+                cgroupCpu.setCpus(StringUtils.join(workerToCores.get(workerId), \",\"));\n+                cgroupCpu.setMems(workerToMemoryZone.get(workerId));\n+            }\n+        }\n+\n+        OciLinuxConfig.Resources.Memory cgroupMem = null;\n+        if (memInBytes != null) {\n+            cgroupMem = new OciLinuxConfig.Resources.Memory(memInBytes, 0, 0, 0, 0, 0, false);\n+        }\n+\n+        OciLinuxConfig.Resources cgroupResources =\n+                new OciLinuxConfig.Resources(null, cgroupMem, cgroupCpu, null, null, null,\n+                        null, null);\n+\n+        return new OciLinuxConfig(null, null, null, null,\n+                cgroupsPath, cgroupResources, null, null, seccomp, null, null,\n+                null, null);\n+    }\n+\n+    private void addOciMountLocation(List<OciMount> mounts, String srcPath,\n+                                     String dstPath, boolean createSource, boolean isReadWrite) throws IOException {\n+        if (!createSource) {\n+            boolean sourceExists = new File(srcPath).exists();\n+            if (!sourceExists) {\n+                throw new IOException(\"SourcePath \" + srcPath + \" doesn't exit\");\n+            }\n+        }\n+\n+        ArrayList<String> options = new ArrayList<>();\n+        if (isReadWrite) {\n+            options.add(\"rw\");\n+        } else {\n+            options.add(\"ro\");\n+        }\n+        options.add(\"rbind\");\n+        options.add(\"rprivate\");\n+        mounts.add(new OciMount(dstPath, \"bind\", srcPath, options));\n+    }\n+\n+    @Override\n+    public long getMemoryUsage(String user, String workerId, int port) throws IOException {\n+        // \"/sys/fs/cgroup/memory/storm/containerId/\"\n+        String containerId = getContainerId(workerId, port);\n+        String memoryCgroupPath = memoryCgroupRootPath + File.separator  + containerId;\n+        MemoryCore memoryCore = new MemoryCore(memoryCgroupPath);\n+        LOG.debug(\"ContainerId {} : Got memory getPhysicalUsage {} from {}\", containerId, memoryCore.getPhysicalUsage(), memoryCgroupPath);\n+        return memoryCore.getPhysicalUsage();\n+    }\n+\n+    @Override\n+    public void kill(String user, String workerId) throws IOException {\n+        LOG.info(\"Killing {}\", workerId);\n+        Long pid = getContainerPid(workerId);\n+        if (pid != null) {\n+            signal(pid, 15, user);\n+        } else {\n+            LOG.warn(\"Trying to kill container {} but pidfile is not found\", workerId);\n+        }\n+    }\n+\n+    private void signal(long pid, int signal, String user) throws IOException {\n+        List<String> commands = Arrays.asList(\"signal\", String.valueOf(pid), String.valueOf(signal));\n+        String logPrefix = \"kill -\" + signal + \" \" + pid;\n+        ClientSupervisorUtils.processLauncherAndWait(conf, user, commands, null, logPrefix);\n+    }\n+\n+    @Override\n+    public void forceKill(String user, String workerId) throws IOException {\n+        LOG.debug(\"ForceKilling {}\", workerId);\n+        Long pid = getContainerPid(workerId);\n+        if (pid != null) {\n+            signal(pid, 9, user);\n+        } else {\n+            LOG.warn(\"Trying to forceKill container {} but pidfile is not found\", workerId);\n+        }\n+    }\n+\n+    // return null if not found.\n+    private Long getContainerPid(String workerId) {\n+        Long pid = workerToContainerPid.get(workerId);\n+        if (pid == null) {\n+            String containerPidFilePath = containerPidFile(workerId);\n+            if (!new File(containerPidFilePath).exists()) {\n+                LOG.warn(\"{} doesn't exist\", containerPidFilePath);\n+            } else {\n+                try {\n+                    pid = Long.parseLong(CgroupUtils.readFileByLine(containerPidFilePath).get(0));\n+                    workerToContainerPid.put(workerId, pid);\n+                } catch (IOException e) {\n+                    LOG.warn(\"failed to read {}\", containerPidFilePath);\n+                }\n+            }\n+        }\n+        return pid;\n+    }\n+\n+    @Override\n+    public void releaseResourcesForWorker(String workerId) {\n+        super.releaseResourcesForWorker(workerId);\n+        workerToContainerPid.remove(workerId);\n+    }\n+\n+    /**\n+     * The container terminates if any process inside the container dies.\n+     * So we only need to check if the initial process is alive or not.\n+     * @param user the user that the processes are running as\n+     * @param workerId the id of the worker to kill\n+     * @return true if all processes are dead; false otherwise\n+     * @throws IOException on I/O exception\n+     */\n+    @Override\n+    public boolean areAllProcessesDead(String user, String workerId) throws IOException {\n+        boolean areAllDead = isContainerDead(workerId, user);\n+        LOG.debug(\"WorkerId {}: Checking areAllProcessesDead: {}\", workerId, areAllDead);\n+        return areAllDead;\n+    }\n+\n+    @Override\n+    public void cleanup(String user, String workerId, int port) throws IOException {\n+        LOG.debug(\"clean up worker {}\", workerId);\n+        try {\n+            String containerId = getContainerId(workerId, port);\n+            List<String> commands = Arrays.asList(CmdType.REAP_OCI_CONTAINER.toString(), containerId, String.valueOf(layersToKeep));\n+            String logPrefix = \"Worker Process \" + workerId;\n+            int result = ClientSupervisorUtils.processLauncherAndWait(conf, user, commands, null, logPrefix);\n+            if (result != 0) {\n+                LOG.warn(\"Failed cleaning up RuncWorker {}\", workerId);\n+            }\n+        } catch (FileNotFoundException e) {\n+            // This could happen if we had an IOException and failed launching the worker.\n+            // We need to continue on in order for the worker directory to get cleaned up.\n+            LOG.error(\"Failed to find container id for {} ({}), unable to reap container\", workerId, e.getMessage());\n+        }\n+        //remove from the watched list\n+        LOG.debug(\"Removing {} from the watched workers list\", workerId);\n+        workerToUser.remove(workerId);", "originalCommit": "690e02091244f33b32a1dc3c449413e6aff1d964", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3MTg5ODMwMA==", "url": "https://github.com/apache/storm/pull/3366#discussion_r671898300", "bodyText": "makes sense. keeping cleanup and removing releaseResourcesForWorker", "author": "Ethanlm", "createdAt": "2021-07-18T21:12:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDU4ODk5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3NTYzODcxNA==", "url": "https://github.com/apache/storm/pull/3366#discussion_r675638714", "bodyText": "Thanks for addressing this.", "author": "agresch", "createdAt": "2021-07-23T15:09:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDU4ODk5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDU5MzI2Ng==", "url": "https://github.com/apache/storm/pull/3366#discussion_r550593266", "bodyText": "Counldn't spelling", "author": "agresch", "createdAt": "2020-12-31T17:44:22Z", "path": "storm-server/src/main/java/org/apache/storm/container/oci/RuncLibContainerManager.java", "diffHunk": "@@ -0,0 +1,631 @@\n+/*\n+ *\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.storm.container.oci;\n+\n+import static org.apache.storm.utils.ConfigUtils.FILE_SEPARATOR;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.FileReader;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.io.Reader;\n+import java.io.Writer;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.storm.DaemonConfig;\n+import org.apache.storm.StormTimer;\n+import org.apache.storm.container.cgroup.CgroupUtils;\n+import org.apache.storm.container.cgroup.core.MemoryCore;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciLayer;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig.OciLinuxConfig;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig.OciMount;\n+import org.apache.storm.container.oci.OciContainerExecutorConfig.OciRuntimeConfig.OciProcessConfig;\n+import org.apache.storm.daemon.supervisor.ClientSupervisorUtils;\n+import org.apache.storm.daemon.supervisor.ExitCodeCallback;\n+import org.apache.storm.utils.ConfigUtils;\n+import org.apache.storm.utils.ObjectReader;\n+import org.apache.storm.utils.ReflectionUtils;\n+import org.apache.storm.utils.ServerUtils;\n+import org.apache.storm.utils.Utils;\n+\n+import org.json.simple.JSONObject;\n+import org.json.simple.parser.JSONParser;\n+import org.json.simple.parser.ParseException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.yaml.snakeyaml.DumperOptions;\n+import org.yaml.snakeyaml.Yaml;\n+\n+public class RuncLibContainerManager extends OciContainerManager {\n+    private static final Logger LOG = LoggerFactory.getLogger(RuncLibContainerManager.class);\n+\n+    private OciImageTagToManifestPluginInterface imageTagToManifestPlugin;\n+    private OciManifestToResourcesPluginInterface manifestToResourcesPlugin;\n+    private OciResourcesLocalizerInterface ociResourcesLocalizer;\n+    private ObjectMapper mapper;\n+    private int layersToKeep;\n+    private String seccomp;\n+\n+    private static final String RESOLV_CONF = \"/etc/resolv.conf\";\n+    private static final String HOSTNAME = \"/etc/hostname\";\n+    private static final String HOSTS = \"/etc/hosts\";\n+    private static final String OCI_CONFIG_JSON = \"oci-config.json\";\n+\n+    private static final String SQUASHFS_MEDIA_TYPE = \"application/vnd.squashfs\";\n+\n+    //CPU CFS (Completely Fair Scheduler) period\n+    private static final long CPU_CFS_PERIOD_US = 100000;\n+\n+    private Map<String, Long> workerToContainerPid = new ConcurrentHashMap<>();\n+    private Map<String, ExitCodeCallback> workerToExitCallback = new ConcurrentHashMap<>();\n+    private Map<String, String> workerToUser = new ConcurrentHashMap<>();\n+    private StormTimer checkContainerAliveTimer;\n+\n+    @Override\n+    public void prepare(Map<String, Object> conf) throws IOException {\n+        super.prepare(conf);\n+\n+        imageTagToManifestPlugin = chooseImageTagToManifestPlugin();\n+        imageTagToManifestPlugin.init(conf);\n+\n+        manifestToResourcesPlugin = chooseManifestToResourcesPlugin();\n+        manifestToResourcesPlugin.init(conf);\n+\n+        ociResourcesLocalizer = chooseOciResourcesLocalizer();\n+        ociResourcesLocalizer.init(conf);\n+\n+        layersToKeep = ObjectReader.getInt(\n+                conf.get(DaemonConfig.STORM_OCI_LAYER_MOUNTS_TO_KEEP),\n+                100\n+        );\n+\n+        mapper = new ObjectMapper();\n+\n+        if (seccompJsonFile != null) {\n+            seccomp = new String(Files.readAllBytes(Paths.get(seccompJsonFile)));\n+        }\n+\n+        if (checkContainerAliveTimer == null) {\n+            checkContainerAliveTimer =\n+                new StormTimer(\"CheckRuncContainerAlive\", Utils.createDefaultUncaughtExceptionHandler());\n+            checkContainerAliveTimer\n+                .scheduleRecurring(0, (Integer) conf.get(DaemonConfig.SUPERVISOR_MONITOR_FREQUENCY_SECS), () -> {\n+                    try {\n+                        checkContainersAlive();\n+                    } catch (Exception e) {\n+                        //Ignore\n+                        LOG.debug(\"The CheckRuncContainerAlive thread has exception. Ignored\", e);\n+                    }\n+                });\n+        }\n+    }\n+\n+    private OciImageTagToManifestPluginInterface chooseImageTagToManifestPlugin() throws IllegalArgumentException {\n+        String pluginName = ObjectReader.getString(\n+                conf.get(DaemonConfig.STORM_OCI_IMAGE_TAG_TO_MANIFEST_PLUGIN)\n+        );\n+        LOG.info(\"imageTag-to-manifest Plugin is: {}\", pluginName);\n+        return ReflectionUtils.newInstance(pluginName);\n+    }\n+\n+    private OciManifestToResourcesPluginInterface chooseManifestToResourcesPlugin() throws IllegalArgumentException {\n+        String pluginName = ObjectReader.getString(\n+                conf.get(DaemonConfig.STORM_OCI_MANIFEST_TO_RESOURCES_PLUGIN)\n+        );\n+        LOG.info(\"manifest to resource Plugin is: {}\", pluginName);\n+        return ReflectionUtils.newInstance(pluginName);\n+    }\n+\n+    private OciResourcesLocalizerInterface chooseOciResourcesLocalizer()\n+        throws IllegalArgumentException {\n+        String pluginName = ObjectReader.getString(\n+                conf.get(DaemonConfig.STORM_OCI_RESOURCES_LOCALIZER)\n+        );\n+        LOG.info(\"oci resource localizer is: {}\", pluginName);\n+        return ReflectionUtils.newInstance(pluginName);\n+    }\n+\n+    //the container process ID in the process namespace of the host.\n+    private String containerPidFile(String workerId) {\n+        return ConfigUtils.workerArtifactsSymlink(conf, workerId) + FILE_SEPARATOR + \"container-\" + workerId + \".pid\";\n+    }\n+\n+    @Override\n+    public void launchWorkerProcess(String user, String topologyId,  Map<String, Object> topoConf,\n+                                    int port, String workerId,\n+                                    List<String> command, Map<String, String> env, String logPrefix,\n+                                    ExitCodeCallback processExitCallback, File targetDir) throws IOException {\n+\n+        String imageName = getImageName(topoConf);\n+        if (imageName == null) {\n+            LOG.error(\"Image name for {} is not configured properly; will not continue to launch the worker\", topologyId);\n+            return;\n+        }\n+\n+        //set container ID to port + worker ID\n+        String containerId = getContainerId(workerId, port);\n+\n+        //get manifest\n+        ImageManifest manifest = imageTagToManifestPlugin.getManifestFromImageTag(imageName);\n+        LOG.debug(\"workerId {}: Got manifest: {}\", workerId, manifest.toString());\n+\n+        //get layers metadata\n+        OciResource configResource = manifestToResourcesPlugin.getConfigResource(manifest);\n+        LOG.info(\"workerId {}: Got config metadata: {}\", workerId, configResource.toString());\n+\n+        saveRuncYaml(topologyId, port, containerId, imageName, configResource);\n+\n+        List<OciResource> layersResource = manifestToResourcesPlugin.getLayerResources(manifest);\n+        LOG.info(\"workerId {}: Got layers metadata: {}\", workerId, layersResource.toString());\n+\n+        //localize resource\n+        String configLocalPath = ociResourcesLocalizer.localize(configResource);\n+\n+        List<String> ociEnv = new ArrayList<>();\n+        List<String> args = new ArrayList<>();\n+\n+        ArrayList<OciLayer> layers = new ArrayList<>();\n+\n+        File file = new File(configLocalPath);\n+        //extract env\n+        List<String> imageEnv = extractImageEnv(file);\n+        if (imageEnv != null && !imageEnv.isEmpty()) {\n+            ociEnv.addAll(imageEnv);\n+        }\n+        for (Map.Entry<String, String> entry : env.entrySet()) {\n+            ociEnv.add(entry.getKey() + \"=\" + entry.getValue());\n+        }\n+        LOG.debug(\"workerId {}: ociEnv: {}\", workerId, ociEnv);\n+\n+        //extract entrypoint\n+        List<String> entrypoint = extractImageEntrypoint(file);\n+        if (entrypoint != null && !entrypoint.isEmpty()) {\n+            args.addAll(entrypoint);\n+        }\n+        LOG.debug(\"workerId {}: args: {}\", workerId, args);\n+\n+        //localize layers\n+        List<String> layersLocalPath = ociResourcesLocalizer.localize((layersResource));\n+        //compose layers\n+        for (String layerLocalPath : layersLocalPath) {\n+            OciLayer layer = new OciLayer(SQUASHFS_MEDIA_TYPE, layerLocalPath);\n+            layers.add(layer);\n+        }\n+        LOG.debug(\"workerId {}: layers: {}\", workerId, layers);\n+        ArrayList<OciMount> mounts = new ArrayList<>();\n+        setContainerMounts(mounts, topologyId, workerId, port);\n+        LOG.debug(\"workerId {}: mounts: {}\", workerId, mounts);\n+\n+        //calculate the cpusQuotas based on CPU_CFS_PERIOD and assigned CPU\n+        Long cpusQuotas = null;\n+        if (workerToCpu.containsKey(workerId)) {\n+            cpusQuotas = workerToCpu.get(workerId) * CPU_CFS_PERIOD_US / 100;\n+        }\n+\n+        Long memoryInBytes = null;\n+        if (workerToMemoryMb.containsKey(workerId)) {\n+            memoryInBytes = workerToMemoryMb.get(workerId) * 1024 * 1024L;\n+        }\n+        LOG.info(\"workerId {}: memoryInBytes set to {}; cpusQuotas set to {}\", workerId, memoryInBytes, cpusQuotas);\n+\n+        //<workerRoot>/<workerId>\n+        String workerDir = targetDir.getAbsolutePath();\n+        String workerScriptPath = ServerUtils.writeScript(workerDir, command, env, \"0027\");\n+\n+        args.add(\"bash\");\n+        args.add(workerScriptPath);\n+\n+        //The container PID (on the host) will be written to this file.\n+        String containerPidFilePath = containerPidFile(workerId);\n+\n+        OciProcessConfig processConfig = createOciProcessConfig(workerDir, ociEnv, args);\n+\n+        OciLinuxConfig linuxConfig =\n+            createOciLinuxConfig(cpusQuotas, memoryInBytes, cgroupParent + \"/\" + containerId, seccomp, workerId);\n+\n+        OciRuntimeConfig ociRuntimeConfig = new OciRuntimeConfig(null, mounts, processConfig, null,\n+                                                          null, null, linuxConfig);\n+\n+        OciContainerExecutorConfig ociContainerExecutorConfig =\n+            createOciContainerExecutorConfig(user, containerId, containerPidFilePath,\n+                                             workerScriptPath, null, null, null, layers, ociRuntimeConfig);\n+\n+        //launch the container using worker-launcher\n+        String executorConfigToJsonFile = writeOciExecutorConfigToJsonFile(mapper, ociContainerExecutorConfig, workerDir);\n+        LOG.info(\"workerId {}: oci-config.json file path: {}\", workerId, executorConfigToJsonFile);\n+\n+        List<String> cmdArgs = Arrays.asList(CmdType.RUN_OCI_CONTAINER.toString(), workerDir, executorConfigToJsonFile,\n+                                             ConfigUtils.workerArtifactsSymlink(conf, workerId));\n+\n+        // launch the oci container. waiting prevents possible race condition that could prevent cleanup of container\n+        int exitCode = ClientSupervisorUtils.processLauncherAndWait(conf, user, cmdArgs, env, logPrefix, targetDir);\n+        if (exitCode != 0) {\n+            LOG.error(\"launchWorkerProcess RuncCommand {} exited with code: {}\", \"LaunchWorker-\" + containerId, exitCode);\n+            throw new RuntimeException(\"launchWorkerProcess Failed to create Runc Container. ContainerId: \" + containerId);\n+        }\n+\n+        //Add to the watched list\n+        LOG.debug(\"Adding {} to the watched workers list\", workerId);\n+        workerToExitCallback.put(workerId, processExitCallback);\n+        workerToUser.put(workerId, user);\n+\n+    }\n+\n+    private void checkContainersAlive() {\n+        //Check if all watched workers are still alive\n+        workerToUser.forEach((workerId, user) -> {\n+            if (isContainerDead(workerId, user)) {\n+                invokeProcessExitCallback(workerId);\n+            }\n+        });\n+    }\n+\n+    private boolean isContainerDead(String workerId, String user) {\n+        boolean isDead = true;\n+        Long pid = getContainerPid(workerId);\n+        LOG.debug(\"Checking container {}, pid {}, user {}\", workerId, pid, user);\n+        //do nothing if pid is null.\n+        if (pid != null && user != null) {\n+            try {\n+                isDead = ServerUtils.areAllProcessesDead(conf, user, workerId, Collections.singleton(pid));\n+            } catch (IOException e) {\n+                //ignore\n+                LOG.debug(\"Error while checking if container is dead.\", e);\n+            }\n+        }\n+        return isDead;\n+    }\n+\n+    private void invokeProcessExitCallback(String workerId) {\n+        LOG.info(\"processExitCallback returned for workerId {}\", workerId);\n+        ExitCodeCallback processExitCallback = workerToExitCallback.get(workerId);\n+        if (processExitCallback != null) {\n+            processExitCallback.call(0);\n+        }\n+    }\n+\n+    private String getContainerId(String workerId, int port) throws IOException {\n+        if (port <= 0) { // when killing workers, we will have the workerId and a port of -1\n+            return getContainerIdFromOciJson(workerId);\n+        }\n+        return port + \"-\" + workerId;\n+    }\n+\n+    private String getContainerIdFromOciJson(String workerId) throws IOException {\n+        String ociJson = ConfigUtils.workerRoot(conf, workerId) + FILE_SEPARATOR + OCI_CONFIG_JSON;\n+        LOG.info(\"port unknown for workerId {}, looking up from {}\", workerId, ociJson);\n+        JSONParser parser = new JSONParser();\n+\n+        try (Reader reader = new FileReader(ociJson)) {\n+            JSONObject jsonObject = (JSONObject) parser.parse(reader);\n+            return (String) jsonObject.get(\"containerId\");\n+        } catch (ParseException e) {\n+            throw new IOException(\"Unable to parse {}\", e);\n+        }\n+    }\n+\n+    // save runc.yaml in artifacts dir so we can track which image the worker was launched with\n+    private void saveRuncYaml(String topologyId, int port, String containerId, String imageName, OciResource configResource) {\n+        String fname = String.format(\"runc-%s.yaml\", containerId);\n+        File file = new File(ConfigUtils.workerArtifactsRoot(conf, topologyId, port), fname);\n+        DumperOptions options = new DumperOptions();\n+        options.setIndent(2);\n+        options.setPrettyFlow(true);\n+        options.setDefaultFlowStyle(DumperOptions.FlowStyle.BLOCK);\n+        Yaml yaml = new Yaml(options);\n+        Map<String, Object> data = new HashMap<>();\n+        data.put(\"imageName\", imageName);\n+        data.put(\"manifest\", configResource.getFileName());\n+        data.put(\"configPath\", configResource.getPath());\n+        try (Writer writer = new FileWriter(file)) {\n+            yaml.dump(data, writer);\n+        } catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    private String writeOciExecutorConfigToJsonFile(ObjectMapper mapper, OciContainerExecutorConfig ociContainerExecutorConfig,\n+                                                    String workerDir) throws IOException {\n+        File cmdDir = new File(workerDir);\n+        if (!cmdDir.exists()) {\n+            throw new IOException(workerDir + \" doesn't exist\");\n+        }\n+\n+        File commandFile = new File(cmdDir + FILE_SEPARATOR + OCI_CONFIG_JSON);\n+        mapper.writeValue(commandFile, ociContainerExecutorConfig);\n+        return commandFile.getAbsolutePath();\n+    }\n+\n+    private void setContainerMounts(ArrayList<OciMount> mounts, String topologyId, String workerId, Integer port) throws IOException {\n+        //read-only bindmounts need to be added before read-write bindmounts otherwise read-write bindmounts may be overridden.\n+        for (String readonlyMount : readonlyBindmounts) {\n+            addOciMountLocation(mounts, readonlyMount, readonlyMount, false, false);\n+        }\n+\n+        for (String readwriteMount : readwriteBindmounts) {\n+            addOciMountLocation(mounts, readwriteMount, readwriteMount, false, true);\n+        }\n+\n+        addOciMountLocation(mounts, RESOLV_CONF, RESOLV_CONF, false, false);\n+        addOciMountLocation(mounts, HOSTNAME, HOSTNAME, false, false);\n+        addOciMountLocation(mounts, HOSTS, HOSTS, false, false);\n+        addOciMountLocation(mounts, nscdPath, nscdPath, false, false);\n+        addOciMountLocation(mounts, stormHome, stormHome, false, false);\n+        addOciMountLocation(mounts, cgroupRootPath, cgroupRootPath, false, false);\n+\n+        //set of locations to be bind mounted\n+        String supervisorLocalDir = ConfigUtils.supervisorLocalDir(conf);\n+        addOciMountLocation(mounts, supervisorLocalDir, supervisorLocalDir, false, false);\n+\n+        String workerRootDir = ConfigUtils.workerRoot(conf, workerId);\n+        addOciMountLocation(mounts, workerRootDir, workerRootDir, false, true);\n+\n+        String workerArtifactsRoot = ConfigUtils.workerArtifactsRoot(conf, topologyId, port);\n+        addOciMountLocation(mounts, workerArtifactsRoot, workerArtifactsRoot, false, true);\n+\n+        String workerUserFile = ConfigUtils.workerUserFile(conf, workerId);\n+        addOciMountLocation(mounts, workerUserFile, workerUserFile, false, true);\n+\n+        String sharedByTopologyDir = ConfigUtils.sharedByTopologyDir(conf, topologyId);\n+        addOciMountLocation(mounts, sharedByTopologyDir, sharedByTopologyDir, false, true);\n+\n+        String workerTmpRoot = ConfigUtils.workerTmpRoot(conf, workerId);\n+        addOciMountLocation(mounts, workerTmpRoot, TMP_DIR, false, true);\n+    }\n+\n+    private List<String> extractImageEnv(File config) throws IOException {\n+        JsonNode node = mapper.readTree(config);\n+        JsonNode envNode = node.path(\"config\").path(\"Env\");\n+        if (envNode.isMissingNode()) {\n+            return null;\n+        }\n+        return mapper.treeToValue(envNode, List.class);\n+    }\n+\n+    private List<String> extractImageEntrypoint(File config) throws IOException {\n+        JsonNode node = mapper.readTree(config);\n+        JsonNode entrypointNode = node.path(\"config\").path(\"Entrypoint\");\n+        if (entrypointNode.isMissingNode()) {\n+            return null;\n+        }\n+        return mapper.treeToValue(entrypointNode, List.class);\n+    }\n+\n+    private OciContainerExecutorConfig createOciContainerExecutorConfig(\n+            String username, String containerId, String pidFile,\n+            String containerScriptPath, String containerCredentialsPath,\n+            List<String> localDirs, List<String> logDirs,\n+            List<OciLayer> layers, OciRuntimeConfig ociRuntimeConfig) {\n+\n+        return new OciContainerExecutorConfig(username, containerId,\n+                pidFile, containerScriptPath, containerCredentialsPath,\n+                localDirs, logDirs, layers, layersToKeep, ociRuntimeConfig);\n+    }\n+\n+    private OciProcessConfig createOciProcessConfig(String cwd,\n+                                                    List<String> env, List<String> args) {\n+        return new OciProcessConfig(false, null, cwd, env,\n+                args, null, null, null, true, 0, null, null);\n+    }\n+\n+    private OciLinuxConfig createOciLinuxConfig(Long cpusQuotas, Long memInBytes,\n+                                                String cgroupsPath, String seccomp, String workerId) {\n+        OciLinuxConfig.Resources.Cpu cgroupCpu = null;\n+\n+        if (cpusQuotas != null) {\n+            cgroupCpu = new OciLinuxConfig.Resources.Cpu(0, cpusQuotas, CPU_CFS_PERIOD_US, 0, 0,\n+                    null, null);\n+\n+            if (workerToCores.containsKey(workerId)) {\n+                cgroupCpu.setCpus(StringUtils.join(workerToCores.get(workerId), \",\"));\n+                cgroupCpu.setMems(workerToMemoryZone.get(workerId));\n+            }\n+        }\n+\n+        OciLinuxConfig.Resources.Memory cgroupMem = null;\n+        if (memInBytes != null) {\n+            cgroupMem = new OciLinuxConfig.Resources.Memory(memInBytes, 0, 0, 0, 0, 0, false);\n+        }\n+\n+        OciLinuxConfig.Resources cgroupResources =\n+                new OciLinuxConfig.Resources(null, cgroupMem, cgroupCpu, null, null, null,\n+                        null, null);\n+\n+        return new OciLinuxConfig(null, null, null, null,\n+                cgroupsPath, cgroupResources, null, null, seccomp, null, null,\n+                null, null);\n+    }\n+\n+    private void addOciMountLocation(List<OciMount> mounts, String srcPath,\n+                                     String dstPath, boolean createSource, boolean isReadWrite) throws IOException {\n+        if (!createSource) {\n+            boolean sourceExists = new File(srcPath).exists();\n+            if (!sourceExists) {\n+                throw new IOException(\"SourcePath \" + srcPath + \" doesn't exit\");\n+            }\n+        }\n+\n+        ArrayList<String> options = new ArrayList<>();\n+        if (isReadWrite) {\n+            options.add(\"rw\");\n+        } else {\n+            options.add(\"ro\");\n+        }\n+        options.add(\"rbind\");\n+        options.add(\"rprivate\");\n+        mounts.add(new OciMount(dstPath, \"bind\", srcPath, options));\n+    }\n+\n+    @Override\n+    public long getMemoryUsage(String user, String workerId, int port) throws IOException {\n+        // \"/sys/fs/cgroup/memory/storm/containerId/\"\n+        String containerId = getContainerId(workerId, port);\n+        String memoryCgroupPath = memoryCgroupRootPath + File.separator  + containerId;\n+        MemoryCore memoryCore = new MemoryCore(memoryCgroupPath);\n+        LOG.debug(\"ContainerId {} : Got memory getPhysicalUsage {} from {}\", containerId, memoryCore.getPhysicalUsage(), memoryCgroupPath);\n+        return memoryCore.getPhysicalUsage();\n+    }\n+\n+    @Override\n+    public void kill(String user, String workerId) throws IOException {\n+        LOG.info(\"Killing {}\", workerId);\n+        Long pid = getContainerPid(workerId);\n+        if (pid != null) {\n+            signal(pid, 15, user);\n+        } else {\n+            LOG.warn(\"Trying to kill container {} but pidfile is not found\", workerId);\n+        }\n+    }\n+\n+    private void signal(long pid, int signal, String user) throws IOException {\n+        List<String> commands = Arrays.asList(\"signal\", String.valueOf(pid), String.valueOf(signal));\n+        String logPrefix = \"kill -\" + signal + \" \" + pid;\n+        ClientSupervisorUtils.processLauncherAndWait(conf, user, commands, null, logPrefix);\n+    }\n+\n+    @Override\n+    public void forceKill(String user, String workerId) throws IOException {\n+        LOG.debug(\"ForceKilling {}\", workerId);\n+        Long pid = getContainerPid(workerId);\n+        if (pid != null) {\n+            signal(pid, 9, user);\n+        } else {\n+            LOG.warn(\"Trying to forceKill container {} but pidfile is not found\", workerId);\n+        }\n+    }\n+\n+    // return null if not found.\n+    private Long getContainerPid(String workerId) {\n+        Long pid = workerToContainerPid.get(workerId);\n+        if (pid == null) {\n+            String containerPidFilePath = containerPidFile(workerId);\n+            if (!new File(containerPidFilePath).exists()) {\n+                LOG.warn(\"{} doesn't exist\", containerPidFilePath);\n+            } else {\n+                try {\n+                    pid = Long.parseLong(CgroupUtils.readFileByLine(containerPidFilePath).get(0));\n+                    workerToContainerPid.put(workerId, pid);\n+                } catch (IOException e) {\n+                    LOG.warn(\"failed to read {}\", containerPidFilePath);\n+                }\n+            }\n+        }\n+        return pid;\n+    }\n+\n+    @Override\n+    public void releaseResourcesForWorker(String workerId) {\n+        super.releaseResourcesForWorker(workerId);\n+        workerToContainerPid.remove(workerId);\n+    }\n+\n+    /**\n+     * The container terminates if any process inside the container dies.\n+     * So we only need to check if the initial process is alive or not.\n+     * @param user the user that the processes are running as\n+     * @param workerId the id of the worker to kill\n+     * @return true if all processes are dead; false otherwise\n+     * @throws IOException on I/O exception\n+     */\n+    @Override\n+    public boolean areAllProcessesDead(String user, String workerId) throws IOException {\n+        boolean areAllDead = isContainerDead(workerId, user);\n+        LOG.debug(\"WorkerId {}: Checking areAllProcessesDead: {}\", workerId, areAllDead);\n+        return areAllDead;\n+    }\n+\n+    @Override\n+    public void cleanup(String user, String workerId, int port) throws IOException {\n+        LOG.debug(\"clean up worker {}\", workerId);\n+        try {\n+            String containerId = getContainerId(workerId, port);\n+            List<String> commands = Arrays.asList(CmdType.REAP_OCI_CONTAINER.toString(), containerId, String.valueOf(layersToKeep));\n+            String logPrefix = \"Worker Process \" + workerId;\n+            int result = ClientSupervisorUtils.processLauncherAndWait(conf, user, commands, null, logPrefix);\n+            if (result != 0) {\n+                LOG.warn(\"Failed cleaning up RuncWorker {}\", workerId);\n+            }\n+        } catch (FileNotFoundException e) {\n+            // This could happen if we had an IOException and failed launching the worker.\n+            // We need to continue on in order for the worker directory to get cleaned up.\n+            LOG.error(\"Failed to find container id for {} ({}), unable to reap container\", workerId, e.getMessage());\n+        }\n+        //remove from the watched list\n+        LOG.debug(\"Removing {} from the watched workers list\", workerId);\n+        workerToUser.remove(workerId);\n+        workerToExitCallback.remove(workerId);\n+    }\n+\n+    /**\n+     * Run profiling command in the container.\n+     * @param user the user that the worker is running as\n+     * @param workerId the id of the worker\n+     * @param command the command to run.\n+     *                The profiler to be used is configured in worker-launcher.cfg.\n+     * @param env the environment to run the command\n+     * @param logPrefix the prefix to include in the logs\n+     * @param targetDir the working directory to run the command in\n+     * @return true if the command succeeds, false otherwise.\n+     * @throws IOException on I/O exception\n+     * @throws InterruptedException if interrupted\n+     */\n+    @Override\n+    public boolean runProfilingCommand(String user, String workerId, List<String> command, Map<String, String> env,\n+                                       String logPrefix, File targetDir) throws IOException, InterruptedException {\n+        String workerDir = targetDir.getAbsolutePath();\n+\n+        String profilingArgs = StringUtils.join(command, \" \");\n+\n+        //run nsenter\n+        String nsenterScriptPath = writeToCommandFile(workerDir, profilingArgs, \"profile\");\n+\n+        Long containerPid = getContainerPid(workerId);\n+        if (containerPid == null) {\n+            LOG.error(\"Counldn't get container PID for the worker {}. Skip profiling\", workerId);", "originalCommit": "690e02091244f33b32a1dc3c449413e6aff1d964", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "7cbd29b95829e24bdde780b0458477e75c5fdb82", "url": "https://github.com/apache/storm/commit/7cbd29b95829e24bdde780b0458477e75c5fdb82", "message": "[STORM-3388] change Indentation of docker-to-squash.py to 4 spaces", "committedDate": "2021-01-11T22:51:22Z", "type": "commit"}, {"oid": "f904159d2a451ef156731a8fc4747d5f1d286ac0", "url": "https://github.com/apache/storm/commit/f904159d2a451ef156731a8fc4747d5f1d286ac0", "message": "[STORM-3388] address review comments on docker-to-squash.py", "committedDate": "2021-07-09T21:16:16Z", "type": "commit"}, {"oid": "10d13fd1e88ad84b2b082eb4738c7554a5390971", "url": "https://github.com/apache/storm/commit/10d13fd1e88ad84b2b082eb4738c7554a5390971", "message": "[STORM-3388] fix typo", "committedDate": "2021-07-09T21:19:31Z", "type": "commit"}, {"oid": "de2afc8fdc053d732a7f74bf045aa21c88a717fb", "url": "https://github.com/apache/storm/commit/de2afc8fdc053d732a7f74bf045aa21c88a717fb", "message": "[STORM-3388] use javadoc; add final modifier, add static", "committedDate": "2021-07-12T20:10:55Z", "type": "commit"}, {"oid": "5fc077dfa5631f28063b65efd6a770380db8b2ce", "url": "https://github.com/apache/storm/commit/5fc077dfa5631f28063b65efd6a770380db8b2ce", "message": "[STORM-3388] check return value of mkdir and rename operations", "committedDate": "2021-07-12T21:13:04Z", "type": "commit"}, {"oid": "b477ba006e3b7acc277400c82b9b9037c899ec50", "url": "https://github.com/apache/storm/commit/b477ba006e3b7acc277400c82b9b9037c899ec50", "message": "nit on configuration.c", "committedDate": "2021-07-13T19:33:53Z", "type": "commit"}, {"oid": "9f208bf24e8ae667e7ec8c0a669db2b0d6f1de0f", "url": "https://github.com/apache/storm/commit/9f208bf24e8ae667e7ec8c0a669db2b0d6f1de0f", "message": "remove unnecessary code in utils/file-utils.c", "committedDate": "2021-07-13T20:29:43Z", "type": "commit"}, {"oid": "9dd11141b08a947ee2732e77f85d9b8123494e59", "url": "https://github.com/apache/storm/commit/9dd11141b08a947ee2732e77f85d9b8123494e59", "message": "doc update", "committedDate": "2021-07-14T20:19:40Z", "type": "commit"}, {"oid": "248ff3ae2b08d685b9a11dc7268e3caa90f0aa54", "url": "https://github.com/apache/storm/commit/248ff3ae2b08d685b9a11dc7268e3caa90f0aa54", "message": "more nit fix", "committedDate": "2021-07-14T20:22:20Z", "type": "commit"}, {"oid": "ceec321df96ba25ec82f42aa6c343761eaac0199", "url": "https://github.com/apache/storm/commit/ceec321df96ba25ec82f42aa6c343761eaac0199", "message": "[STORM-3388] address comments in java code", "committedDate": "2021-07-18T21:36:38Z", "type": "commit"}, {"oid": "53ff164d2db1c52eb433aaaa7843dc019fe9319a", "url": "https://github.com/apache/storm/commit/53ff164d2db1c52eb433aaaa7843dc019fe9319a", "message": "[STORM-3388] address comments in java code", "committedDate": "2021-07-18T21:54:50Z", "type": "commit"}, {"oid": "f3286cf838a7ab117897ae30e2cedd6b23706e72", "url": "https://github.com/apache/storm/commit/f3286cf838a7ab117897ae30e2cedd6b23706e72", "message": "[STORM-3388] address comments in main.c", "committedDate": "2021-07-18T23:59:01Z", "type": "commit"}, {"oid": "0b80c1597d2ddaeff31cdf5cf1af370c70703785", "url": "https://github.com/apache/storm/commit/0b80c1597d2ddaeff31cdf5cf1af370c70703785", "message": "[STORM-3388] address comments in hdfs-oci", "committedDate": "2021-07-19T00:36:46Z", "type": "commit"}, {"oid": "dcad06769f2235d2624101da03c2ac1c02e9c1e7", "url": "https://github.com/apache/storm/commit/dcad06769f2235d2624101da03c2ac1c02e9c1e7", "message": "[STORM-3388] make storm_user_info generic", "committedDate": "2021-07-19T00:41:03Z", "type": "commit"}, {"oid": "6aa4a8dd901ff8987d089661052d6b95f34565c9", "url": "https://github.com/apache/storm/commit/6aa4a8dd901ff8987d089661052d6b95f34565c9", "message": "[STORM-3388] prepend ERROR to error messages", "committedDate": "2021-07-19T00:55:34Z", "type": "commit"}, {"oid": "9704f6f6540f61c803aafce8854e9de035a2b199", "url": "https://github.com/apache/storm/commit/9704f6f6540f61c803aafce8854e9de035a2b199", "message": "[STORM-3388] change log level to warn", "committedDate": "2021-07-19T01:01:49Z", "type": "commit"}, {"oid": "6b9f785b0876b85cb57c43324a1ee0252276d08e", "url": "https://github.com/apache/storm/commit/6b9f785b0876b85cb57c43324a1ee0252276d08e", "message": "[STORM-3388] change ERROR to warn", "committedDate": "2021-07-19T14:28:33Z", "type": "commit"}, {"oid": "3b5f64c31183f2b5fd8c1995c5c02c6f189d524a", "url": "https://github.com/apache/storm/commit/3b5f64c31183f2b5fd8c1995c5c02c6f189d524a", "message": "[STORM-3388] address review comments", "committedDate": "2021-08-06T14:50:10Z", "type": "commit"}]}