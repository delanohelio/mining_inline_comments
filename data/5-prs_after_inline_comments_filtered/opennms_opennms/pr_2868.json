{"pr_number": 2868, "pr_title": "NMS-10586: Sink API: Persistent Off-Heap Storage", "pr_createdAt": "2020-01-14T17:56:32Z", "pr_url": "https://github.com/OpenNMS/opennms/pull/2868", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUyMTk5MA==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r366521990", "bodyText": "We can use the standard logger for these instead of the RATE_LIMITED_LOGGER. The RATE_LIMITED_LOGGER is intended to be used to avoid flooding the log with error messages.", "author": "j-white", "createdAt": "2020-01-14T19:12:25Z", "path": "core/ipc/sink/common/src/main/java/org/opennms/core/ipc/sink/common/AsyncDispatcherImpl.java", "diffHunk": "@@ -67,226 +65,162 @@\n \n     private static final Logger LOG = LoggerFactory.getLogger(AsyncDispatcherImpl.class);\n     private final SyncDispatcher<S> syncDispatcher;\n-    private OffHeapAdapter offHeapAdapter;\n-    private ExecutorService offHeapAdapterExecutor = Executors.newSingleThreadExecutor();\n     private final AsyncPolicy asyncPolicy;\n-    private OffHeapQueue offHeapQueue;\n-    private SinkModule<S,T> sinkModule;\n-    private DispatcherState<W,S,T> state;\n-    private boolean useOffHeap = false;\n+    private final Counter droppedCounter;\n     \n-    final RateLimitedLog rateLimittedLogger = RateLimitedLog\n+    private final DispatchQueue<S> dispatchQueue;\n+    private final Map<String, CompletableFuture<DispatchStatus>> futureMap = new ConcurrentHashMap<>();\n+    private final AtomicInteger activeDispatchers = new AtomicInteger(0);\n+    \n+    private final RateLimitedLog RATE_LIMITED_LOGGER = RateLimitedLog\n             .withRateLimit(LOG)\n-            .maxRate(5).every(Duration.standardSeconds(30))\n+            .maxRate(5).every(Duration.standardSeconds(5))\n             .build();\n \n-    final LinkedBlockingQueue<Runnable> queue;\n-    final ExecutorService executor;\n+    private final ExecutorService executor;\n \n     public AsyncDispatcherImpl(DispatcherState<W, S, T> state, AsyncPolicy asyncPolicy,\n-            SyncDispatcher<S> syncDispatcher) {\n+                               SyncDispatcher<S> syncDispatcher) {\n         Objects.requireNonNull(state);\n         Objects.requireNonNull(asyncPolicy);\n-        this.syncDispatcher = Objects.requireNonNull(syncDispatcher);\n+        Objects.requireNonNull(syncDispatcher);\n+        this.syncDispatcher = syncDispatcher;\n         this.asyncPolicy = asyncPolicy;\n-        this.state = state;\n-        sinkModule = state.getModule();\n-        if (OffHeapServiceLoader.isOffHeapEnabled()) {\n-            offHeapQueue = OffHeapServiceLoader.getOffHeapQueue();\n-            if (offHeapQueue != null) {\n-                useOffHeap = true;\n-                LOG.info(\"Offheap storage enabled for sink module, {}\", sinkModule.getId());\n-            }\n-        }\n-        \n-        final RejectedExecutionHandler rejectedExecutionHandler;\n-        if (asyncPolicy.isBlockWhenFull()) {\n-            // This queue ensures that calling thread is blocked when the queue is full\n-            // See the implementation of OfferBlockingQueue for details\n-            queue = new OfferBlockingQueue<>(asyncPolicy.getQueueSize());\n-            rejectedExecutionHandler = new ThreadPoolExecutor.AbortPolicy();\n+        SinkModule<S, T> sinkModule = state.getModule();\n+        Optional<DispatchQueueFactory> factory = DispatchQueueServiceLoader.getDispatchQueueFactory();\n+\n+        if (factory.isPresent()) {\n+            LOG.debug(\"Using queue from factory\");\n+            dispatchQueue = factory.get().getQueue(asyncPolicy, sinkModule.getId(),\n+                    sinkModule::marshalSingleMessage, sinkModule::unmarshalSingleMessage);\n         } else {\n-            queue = new LinkedBlockingQueue<Runnable>(asyncPolicy.getQueueSize());\n-            // Reject and increase the dropped counter when the queue is full\n-            final Counter droppedCounter = state.getMetrics().counter(MetricRegistry.name(state.getModule().getId(), \"dropped\"));\n-            rejectedExecutionHandler = new RejectedExecutionHandler() {\n-                @Override\n-                public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {\n-                    droppedCounter.inc();\n-                    throw new RejectedExecutionException(\"Task \" + r.toString() +\n-                            \" rejected from \" +\n-                            e.toString());\n-                }\n-            };\n+            int size = asyncPolicy.getQueueSize();\n+            LOG.debug(\"Using default in memory queue of size {}\", size);\n+            dispatchQueue = new DefaultQueue<>(size);\n         }\n \n-        state.getMetrics().register(MetricRegistry.name(state.getModule().getId(), \"queue-size\"), new Gauge<Integer>() {\n-            @Override\n-            public Integer getValue() {\n-                return queue.size();\n-            }\n-        });\n+        state.getMetrics().register(MetricRegistry.name(state.getModule().getId(), \"queue-size\"),\n+                (Gauge<Integer>) activeDispatchers::get);\n+\n+        droppedCounter = state.getMetrics().counter(MetricRegistry.name(state.getModule().getId(), \"dropped\"));\n \n-        executor = new ThreadPoolExecutor(\n-                asyncPolicy.getNumThreads(),\n-                asyncPolicy.getNumThreads(),\n-                1000L,\n-                TimeUnit.MILLISECONDS,\n-                queue,\n-                new LogPreservingThreadFactory(SystemInfoUtils.DEFAULT_INSTANCE_ID + \".Sink.AsyncDispatcher.\" + state.getModule().getId(), Integer.MAX_VALUE),\n-                rejectedExecutionHandler\n-            );\n+        executor = Executors.newFixedThreadPool(asyncPolicy.getNumThreads(),\n+                new LogPreservingThreadFactory(SystemInfoUtils.DEFAULT_INSTANCE_ID + \".Sink.AsyncDispatcher.\" +\n+                        state.getModule().getId(), Integer.MAX_VALUE));\n+        startDrainingQueue();\n     }\n \n-    /**\n-     * When used in a ThreadPoolExecutor, this queue will block calls to\n-     * {@link ThreadPoolExecutor#execute(Runnable)} when the queue is full.\n-     * This is done by overriding calls to {@link LinkedBlockingQueue#offer(Object)}\n-     * with calls to {@link LinkedBlockingQueue#put(Object)}, but comes with the caveat\n-     * that executor must be built with <code>corePoolSize == maxPoolSize</code>.\n-     * In the context of the {@link AsyncDispatcherImpl}, this is an acceptable caveat,\n-     * since we enforce the matching pool sizes.\n-     *\n-     * There are alternative ways of solving this problem, for example we could use a\n-     * {@link RejectedExecutionHandler} to achieve similar behavior, and allow\n-     * for <code>corePoolSize < maxPoolSize</code>, but not for <code>corePoolSize==0</code>.\n-     *\n-     * For further discussions on this topic see:\n-     *   http://stackoverflow.com/a/3518588\n-     *   http://stackoverflow.com/a/32123535\n-     *\n-     * If the implementation is changed, make sure that that executor is built accordingly.\n-     */\n-    \n-    private static class OfferBlockingQueue<E> extends LinkedBlockingQueue<E> {\n-        private static final long serialVersionUID = 1L;\n+    private void dispatchFromQueue() {\n+        while (true) {\n+            try {\n+                RATE_LIMITED_LOGGER.trace(\"Asking dispatch queue for the next entry...\");", "originalCommit": "4ca6d5c20d6504263741240580fd45a07215616a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUzNDYxMQ==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r366534611", "bodyText": "Ok should I apply this to the trace logging in QueueFileOffHeapDispatchQueue as well?", "author": "mattixtech", "createdAt": "2020-01-14T19:38:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUyMTk5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUzNjYxMQ==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r366536611", "bodyText": "Yeah, I would do the same there too.", "author": "j-white", "createdAt": "2020-01-14T19:43:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUyMTk5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUyMzIzMg==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r366523232", "bodyText": "TODO", "author": "j-white", "createdAt": "2020-01-14T19:15:07Z", "path": "core/ipc/sink/off-heap/src/main/java/org/opennms/core/ipc/sink/offheap/QueueFileOffHeapDispatchQueueFactory.java", "diffHunk": "@@ -0,0 +1,117 @@\n+/*******************************************************************************\n+ * This file is part of OpenNMS(R).\n+ *\n+ * Copyright (C) 2020 The OpenNMS Group, Inc.\n+ * OpenNMS(R) is Copyright (C) 1999-2020 The OpenNMS Group, Inc.\n+ *\n+ * OpenNMS(R) is a registered trademark of The OpenNMS Group, Inc.\n+ *\n+ * OpenNMS(R) is free software: you can redistribute it and/or modify\n+ * it under the terms of the GNU Affero General Public License as published\n+ * by the Free Software Foundation, either version 3 of the License,\n+ * or (at your option) any later version.\n+ *\n+ * OpenNMS(R) is distributed in the hope that it will be useful,\n+ * but WITHOUT ANY WARRANTY; without even the implied warranty of\n+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+ * GNU Affero General Public License for more details.\n+ *\n+ * You should have received a copy of the GNU Affero General Public License\n+ * along with OpenNMS(R).  If not, see:\n+ *      http://www.gnu.org/licenses/\n+ *\n+ * For more information contact:\n+ *     OpenNMS(R) Licensing <license@opennms.org>\n+ *     http://www.opennms.org/\n+ *     http://www.opennms.com/\n+ *******************************************************************************/\n+\n+package org.opennms.core.ipc.sink.offheap;\n+\n+import java.io.IOException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.function.Function;\n+\n+import org.opennms.core.ipc.sink.api.AsyncPolicy;\n+import org.opennms.core.ipc.sink.api.DispatchQueue;\n+import org.opennms.core.ipc.sink.api.DispatchQueueFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class QueueFileOffHeapDispatchQueueFactory implements DispatchQueueFactory {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(QueueFileOffHeapDispatchQueueFactory.class);\n+\n+    private final int inMemoryEntrySize;\n+    private final long offHeapSize;\n+    private final int batchSize;\n+    private final Path baseFilePath;\n+\n+    private final Map<String, DispatchQueue<?>> queues = new ConcurrentHashMap<>();\n+\n+    public QueueFileOffHeapDispatchQueueFactory(int inMemoryEntrySize, int batchSize, String offHeapSize,\n+                                                String baseFilePath) {\n+        this.inMemoryEntrySize = inMemoryEntrySize;\n+        this.batchSize = batchSize;\n+        this.offHeapSize = convertToBytes(offHeapSize);\n+\n+        if (baseFilePath == null || baseFilePath.length() == 0) {\n+            this.baseFilePath = Paths.get(System.getProperty(\"karaf.data\"));\n+        } else {\n+            this.baseFilePath = Paths.get(baseFilePath);\n+        }\n+\n+        LOG.info(\"DispatchQueue factory initialized with on-heap size: {}, batch size: {}, off-heap size: {}, \" +\n+                        \"and file path: {}\", this.inMemoryEntrySize, this.batchSize, this.offHeapSize,\n+                this.baseFilePath);\n+    }\n+\n+    @Override\n+    public <T> DispatchQueue<T> getQueue(AsyncPolicy asyncPolicy, String moduleName, Function<T, byte[]> serializer,\n+                                         Function<byte[], T> deserializer) {\n+        if (asyncPolicy.getNumThreads() > inMemoryEntrySize) {\n+            throw new IllegalArgumentException(\"The in memory queue size must be greater than or equal to the number\" +\n+                    \" of consuming threads\");\n+        }\n+\n+        return (DispatchQueue<T>) queues.computeIfAbsent(moduleName, (k) -> {\n+            try {\n+                // TODO: Configurable", "originalCommit": "4ca6d5c20d6504263741240580fd45a07215616a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUyMzU0NQ==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r366523545", "bodyText": "What's the expect behavior for unknown suffixes?", "author": "j-white", "createdAt": "2020-01-14T19:15:48Z", "path": "core/ipc/sink/off-heap/src/main/java/org/opennms/core/ipc/sink/offheap/QueueFileOffHeapDispatchQueueFactory.java", "diffHunk": "@@ -0,0 +1,117 @@\n+/*******************************************************************************\n+ * This file is part of OpenNMS(R).\n+ *\n+ * Copyright (C) 2020 The OpenNMS Group, Inc.\n+ * OpenNMS(R) is Copyright (C) 1999-2020 The OpenNMS Group, Inc.\n+ *\n+ * OpenNMS(R) is a registered trademark of The OpenNMS Group, Inc.\n+ *\n+ * OpenNMS(R) is free software: you can redistribute it and/or modify\n+ * it under the terms of the GNU Affero General Public License as published\n+ * by the Free Software Foundation, either version 3 of the License,\n+ * or (at your option) any later version.\n+ *\n+ * OpenNMS(R) is distributed in the hope that it will be useful,\n+ * but WITHOUT ANY WARRANTY; without even the implied warranty of\n+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+ * GNU Affero General Public License for more details.\n+ *\n+ * You should have received a copy of the GNU Affero General Public License\n+ * along with OpenNMS(R).  If not, see:\n+ *      http://www.gnu.org/licenses/\n+ *\n+ * For more information contact:\n+ *     OpenNMS(R) Licensing <license@opennms.org>\n+ *     http://www.opennms.org/\n+ *     http://www.opennms.com/\n+ *******************************************************************************/\n+\n+package org.opennms.core.ipc.sink.offheap;\n+\n+import java.io.IOException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.function.Function;\n+\n+import org.opennms.core.ipc.sink.api.AsyncPolicy;\n+import org.opennms.core.ipc.sink.api.DispatchQueue;\n+import org.opennms.core.ipc.sink.api.DispatchQueueFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class QueueFileOffHeapDispatchQueueFactory implements DispatchQueueFactory {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(QueueFileOffHeapDispatchQueueFactory.class);\n+\n+    private final int inMemoryEntrySize;\n+    private final long offHeapSize;\n+    private final int batchSize;\n+    private final Path baseFilePath;\n+\n+    private final Map<String, DispatchQueue<?>> queues = new ConcurrentHashMap<>();\n+\n+    public QueueFileOffHeapDispatchQueueFactory(int inMemoryEntrySize, int batchSize, String offHeapSize,\n+                                                String baseFilePath) {\n+        this.inMemoryEntrySize = inMemoryEntrySize;\n+        this.batchSize = batchSize;\n+        this.offHeapSize = convertToBytes(offHeapSize);\n+\n+        if (baseFilePath == null || baseFilePath.length() == 0) {\n+            this.baseFilePath = Paths.get(System.getProperty(\"karaf.data\"));\n+        } else {\n+            this.baseFilePath = Paths.get(baseFilePath);\n+        }\n+\n+        LOG.info(\"DispatchQueue factory initialized with on-heap size: {}, batch size: {}, off-heap size: {}, \" +\n+                        \"and file path: {}\", this.inMemoryEntrySize, this.batchSize, this.offHeapSize,\n+                this.baseFilePath);\n+    }\n+\n+    @Override\n+    public <T> DispatchQueue<T> getQueue(AsyncPolicy asyncPolicy, String moduleName, Function<T, byte[]> serializer,\n+                                         Function<byte[], T> deserializer) {\n+        if (asyncPolicy.getNumThreads() > inMemoryEntrySize) {\n+            throw new IllegalArgumentException(\"The in memory queue size must be greater than or equal to the number\" +\n+                    \" of consuming threads\");\n+        }\n+\n+        return (DispatchQueue<T>) queues.computeIfAbsent(moduleName, (k) -> {\n+            try {\n+                // TODO: Configurable\n+                return new QueueFileOffHeapDispatchQueue<>(serializer, deserializer, k, baseFilePath,\n+                        inMemoryEntrySize, batchSize,\n+                        offHeapSize);\n+            } catch (IOException e) {\n+                throw new RuntimeException(e);\n+            }\n+        });\n+    }\n+\n+    private static long convertToBytes(String sizeWithSuffix) {\n+        if (sizeWithSuffix == null || sizeWithSuffix.length() == 0) {\n+            return 0;\n+        }\n+\n+        String suffix = sizeWithSuffix.substring(sizeWithSuffix.length() - 2).toLowerCase();\n+        String sizeValue = sizeWithSuffix.substring(0, sizeWithSuffix.length() - 2);\n+\n+        double value = Long.parseLong(sizeValue);\n+        long bytes = 0;\n+\n+        switch (suffix) {\n+            case \"kb\":\n+                bytes = (long) value * 1024;\n+                break;\n+            case \"mb\":\n+                bytes = (long) value * 1024 * 1024;\n+                break;\n+            case \"gb\":\n+                bytes = (long) value * 1024 * 1024 * 1024;\n+                break;", "originalCommit": "4ca6d5c20d6504263741240580fd45a07215616a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUzMDc3OA==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r366530778", "bodyText": "Updated to throw an error.", "author": "mattixtech", "createdAt": "2020-01-14T19:30:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUyMzU0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUyMzc3Ng==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r366523776", "bodyText": "Will cause error if length is 1", "author": "j-white", "createdAt": "2020-01-14T19:16:18Z", "path": "core/ipc/sink/off-heap/src/main/java/org/opennms/core/ipc/sink/offheap/QueueFileOffHeapDispatchQueueFactory.java", "diffHunk": "@@ -0,0 +1,117 @@\n+/*******************************************************************************\n+ * This file is part of OpenNMS(R).\n+ *\n+ * Copyright (C) 2020 The OpenNMS Group, Inc.\n+ * OpenNMS(R) is Copyright (C) 1999-2020 The OpenNMS Group, Inc.\n+ *\n+ * OpenNMS(R) is a registered trademark of The OpenNMS Group, Inc.\n+ *\n+ * OpenNMS(R) is free software: you can redistribute it and/or modify\n+ * it under the terms of the GNU Affero General Public License as published\n+ * by the Free Software Foundation, either version 3 of the License,\n+ * or (at your option) any later version.\n+ *\n+ * OpenNMS(R) is distributed in the hope that it will be useful,\n+ * but WITHOUT ANY WARRANTY; without even the implied warranty of\n+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+ * GNU Affero General Public License for more details.\n+ *\n+ * You should have received a copy of the GNU Affero General Public License\n+ * along with OpenNMS(R).  If not, see:\n+ *      http://www.gnu.org/licenses/\n+ *\n+ * For more information contact:\n+ *     OpenNMS(R) Licensing <license@opennms.org>\n+ *     http://www.opennms.org/\n+ *     http://www.opennms.com/\n+ *******************************************************************************/\n+\n+package org.opennms.core.ipc.sink.offheap;\n+\n+import java.io.IOException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.function.Function;\n+\n+import org.opennms.core.ipc.sink.api.AsyncPolicy;\n+import org.opennms.core.ipc.sink.api.DispatchQueue;\n+import org.opennms.core.ipc.sink.api.DispatchQueueFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class QueueFileOffHeapDispatchQueueFactory implements DispatchQueueFactory {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(QueueFileOffHeapDispatchQueueFactory.class);\n+\n+    private final int inMemoryEntrySize;\n+    private final long offHeapSize;\n+    private final int batchSize;\n+    private final Path baseFilePath;\n+\n+    private final Map<String, DispatchQueue<?>> queues = new ConcurrentHashMap<>();\n+\n+    public QueueFileOffHeapDispatchQueueFactory(int inMemoryEntrySize, int batchSize, String offHeapSize,\n+                                                String baseFilePath) {\n+        this.inMemoryEntrySize = inMemoryEntrySize;\n+        this.batchSize = batchSize;\n+        this.offHeapSize = convertToBytes(offHeapSize);\n+\n+        if (baseFilePath == null || baseFilePath.length() == 0) {\n+            this.baseFilePath = Paths.get(System.getProperty(\"karaf.data\"));\n+        } else {\n+            this.baseFilePath = Paths.get(baseFilePath);\n+        }\n+\n+        LOG.info(\"DispatchQueue factory initialized with on-heap size: {}, batch size: {}, off-heap size: {}, \" +\n+                        \"and file path: {}\", this.inMemoryEntrySize, this.batchSize, this.offHeapSize,\n+                this.baseFilePath);\n+    }\n+\n+    @Override\n+    public <T> DispatchQueue<T> getQueue(AsyncPolicy asyncPolicy, String moduleName, Function<T, byte[]> serializer,\n+                                         Function<byte[], T> deserializer) {\n+        if (asyncPolicy.getNumThreads() > inMemoryEntrySize) {\n+            throw new IllegalArgumentException(\"The in memory queue size must be greater than or equal to the number\" +\n+                    \" of consuming threads\");\n+        }\n+\n+        return (DispatchQueue<T>) queues.computeIfAbsent(moduleName, (k) -> {\n+            try {\n+                // TODO: Configurable\n+                return new QueueFileOffHeapDispatchQueue<>(serializer, deserializer, k, baseFilePath,\n+                        inMemoryEntrySize, batchSize,\n+                        offHeapSize);\n+            } catch (IOException e) {\n+                throw new RuntimeException(e);\n+            }\n+        });\n+    }\n+\n+    private static long convertToBytes(String sizeWithSuffix) {\n+        if (sizeWithSuffix == null || sizeWithSuffix.length() == 0) {\n+            return 0;\n+        }\n+\n+        String suffix = sizeWithSuffix.substring(sizeWithSuffix.length() - 2).toLowerCase();", "originalCommit": "4ca6d5c20d6504263741240580fd45a07215616a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUyMzk4NA==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r366523984", "bodyText": "Might be a good idea to trim trailing white-space off of this too", "author": "j-white", "createdAt": "2020-01-14T19:16:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUyMzc3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUzMTcwNA==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r366531704", "bodyText": "Catching that error and logging now.", "author": "mattixtech", "createdAt": "2020-01-14T19:32:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUyMzc3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUyNjAxMQ==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r366526011", "bodyText": "Update copyright year.", "author": "bouff", "createdAt": "2020-01-14T19:20:54Z", "path": "core/ipc/sink/api/src/main/java/org/opennms/core/ipc/sink/api/AsyncDispatcher.java", "diffHunk": "@@ -40,13 +40,12 @@\n public interface AsyncDispatcher<S extends Message> extends AutoCloseable {", "originalCommit": "4ca6d5c20d6504263741240580fd45a07215616a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjUzMTA2OQ==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r366531069", "bodyText": "Copyright year for this one as well.", "author": "bouff", "createdAt": "2020-01-14T19:31:20Z", "path": "core/ipc/sink/api/src/main/java/org/opennms/core/ipc/sink/api/WriteFailedException.java", "diffHunk": "@@ -35,4 +35,9 @@\n     public WriteFailedException(String message) {\n         super(message);\n     }\n+", "originalCommit": "4ca6d5c20d6504263741240580fd45a07215616a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjU2MDg0MQ==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r366560841", "bodyText": "Perhaps add a test case to check return value of the future (dispatch, queued).  No biggie though.", "author": "bouff", "createdAt": "2020-01-14T20:39:33Z", "path": "core/ipc/sink/off-heap/src/test/java/org/opennms/core/ipc/sink/offheap/QueueFileOffHeapDispatchQueueTest.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*******************************************************************************\n+ * This file is part of OpenNMS(R).\n+ *\n+ * Copyright (C) 2020 The OpenNMS Group, Inc.\n+ * OpenNMS(R) is Copyright (C) 1999-2020 The OpenNMS Group, Inc.\n+ *\n+ * OpenNMS(R) is a registered trademark of The OpenNMS Group, Inc.\n+ *\n+ * OpenNMS(R) is free software: you can redistribute it and/or modify\n+ * it under the terms of the GNU Affero General Public License as published\n+ * by the Free Software Foundation, either version 3 of the License,\n+ * or (at your option) any later version.\n+ *\n+ * OpenNMS(R) is distributed in the hope that it will be useful,\n+ * but WITHOUT ANY WARRANTY; without even the implied warranty of\n+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+ * GNU Affero General Public License for more details.\n+ *\n+ * You should have received a copy of the GNU Affero General Public License\n+ * along with OpenNMS(R).  If not, see:\n+ *      http://www.gnu.org/licenses/\n+ *\n+ * For more information contact:\n+ *     OpenNMS(R) Licensing <license@opennms.org>\n+ *     http://www.opennms.org/\n+ *     http://www.opennms.com/\n+ *******************************************************************************/\n+\n+package org.opennms.core.ipc.sink.offheap;\n+\n+import static com.jayway.awaitility.Awaitility.await;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.nullValue;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.Objects;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.opennms.core.ipc.sink.api.DispatchQueue;\n+import org.opennms.core.ipc.sink.api.WriteFailedException;\n+\n+import com.jayway.awaitility.core.ConditionTimeoutException;\n+\n+public class QueueFileOffHeapDispatchQueueTest {", "originalCommit": "4ca6d5c20d6504263741240580fd45a07215616a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjYxNzk2Ng==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r366617966", "bodyText": "Updated one of the test cases to pay attention to the return type.", "author": "mattixtech", "createdAt": "2020-01-14T23:00:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjU2MDg0MQ=="}], "type": "inlineReview"}, {"oid": "1825b2d49ad85ad7a650161eafa1b2a5bb1fbd5c", "url": "https://github.com/OpenNMS/opennms/commit/1825b2d49ad85ad7a650161eafa1b2a5bb1fbd5c", "message": "Review feedback 2", "committedDate": "2020-01-15T15:36:47Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Njk4ODk1Ng==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r366988956", "bodyText": "This should be either LOG.trace or remove message from the log if you want to keep it in debug.", "author": "cgorantla", "createdAt": "2020-01-15T16:52:00Z", "path": "core/ipc/sink/off-heap/src/main/java/org/opennms/core/ipc/sink/offheap/QueueFileOffHeapDispatchQueue.java", "diffHunk": "@@ -0,0 +1,537 @@\n+/*******************************************************************************\n+ * This file is part of OpenNMS(R).\n+ *\n+ * Copyright (C) 2020 The OpenNMS Group, Inc.\n+ * OpenNMS(R) is Copyright (C) 1999-2020 The OpenNMS Group, Inc.\n+ *\n+ * OpenNMS(R) is a registered trademark of The OpenNMS Group, Inc.\n+ *\n+ * OpenNMS(R) is free software: you can redistribute it and/or modify\n+ * it under the terms of the GNU Affero General Public License as published\n+ * by the Free Software Foundation, either version 3 of the License,\n+ * or (at your option) any later version.\n+ *\n+ * OpenNMS(R) is distributed in the hope that it will be useful,\n+ * but WITHOUT ANY WARRANTY; without even the implied warranty of\n+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+ * GNU Affero General Public License for more details.\n+ *\n+ * You should have received a copy of the GNU Affero General Public License\n+ * along with OpenNMS(R).  If not, see:\n+ *      http://www.gnu.org/licenses/\n+ *\n+ * For more information contact:\n+ *     OpenNMS(R) Licensing <license@opennms.org>\n+ *     http://www.opennms.org/\n+ *     http://www.opennms.com/\n+ *******************************************************************************/\n+\n+package org.opennms.core.ipc.sink.offheap;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.io.Serializable;\n+import java.lang.reflect.InvocationTargetException;\n+import java.lang.reflect.Method;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ArrayBlockingQueue;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ForkJoinPool;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import org.joda.time.Duration;\n+import org.opennms.core.ipc.sink.api.DispatchQueue;\n+import org.opennms.core.ipc.sink.api.WriteFailedException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.squareup.tape2.QueueFile;\n+import com.swrve.ratelimitedlogger.RateLimitedLog;\n+\n+/**\n+ * A {@link DispatchQueue} that first attempts to queue items in memory and upon overflowing the allocated in-memory\n+ * queue writes items \"off heap\" directly to disk via a file. The in-memory queue is volatile and if the process\n+ * crashes its contents are lost. The contents written to disk are durable and in the event of a crash will be reloaded.\n+ * <p>\n+ * This queue can be configured to only queue to memory by specifying the maximum off-heap size of 0. Using this\n+ * configuration causes {@link #enqueue} to block when the in-memory queue fills up rather than writing to the off-heap\n+ * queue.\n+ * <p>\n+ * Before queued items are written to disk they are first accumulated in a batch to limit the number of discrete writes\n+ * we make to disk. The batched items are considered part of the in-memory portion of the queue and are also volatile.\n+ *\n+ * @param <T> the type being queued\n+ */\n+public class QueueFileOffHeapDispatchQueue<T> implements DispatchQueue<T> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(QueueFileOffHeapDispatchQueue.class);\n+    private final RateLimitedLog RATE_LIMITED_LOGGER = RateLimitedLog\n+            .withRateLimit(LOG)\n+            .maxRate(5)\n+            .every(Duration.standardSeconds(30))\n+            .build();\n+\n+    // This must match the size of the HEADER_LENGTH in QueueFile's Element class\n+    //\n+    // We could pull this out of that class reflectively but it seemed easier to just hard code it here\n+    private static final int SERIALIZED_OBJECT_HEADER_SIZE_IN_BYTES = 4;\n+    private static final String FILE_EXTENSION = \".fifo\";\n+\n+    private final Function<T, byte[]> serializer;\n+    private final Function<byte[], T> deserializer;\n+    private final String moduleName;\n+    private final BlockingQueue<Map.Entry<String, T>> inMemoryQueue;\n+\n+    // Note the queue is not thread safe so access should be synchronized\n+    private final QueueFile offHeapQueue;\n+    private final long maxFileSizeInBytes;\n+    private final int batchSize;\n+    private final Batch batch;\n+\n+    private final ForkJoinPool serdesPool = new ForkJoinPool(\n+            Math.max(Runtime.getRuntime().availableProcessors() - 1, 1));\n+\n+    // Used to guard access to the offHeapQueue and batch data structures\n+    private final Object offHeapMutex = new Object();\n+    private final FileCapacityLatch fileCapacityLatch = new FileCapacityLatch();\n+\n+    private final Method usedBytesMethod;\n+\n+    public QueueFileOffHeapDispatchQueue(Function<T, byte[]> serializer, Function<byte[], T> deserializer,\n+                                         String moduleName, Path filePath, int inMemoryQueueSize, int batchSize,\n+                                         long maxFileSizeInBytes) throws IOException {\n+        Objects.requireNonNull(serializer);\n+        Objects.requireNonNull(deserializer);\n+        Objects.requireNonNull(moduleName);\n+\n+        if (inMemoryQueueSize < 1) {\n+            throw new IllegalArgumentException(\"In memory queue size must be greater than 0\");\n+        }\n+\n+        if (inMemoryQueueSize % batchSize != 0) {\n+            throw new IllegalArgumentException(\"In memory queue size must be a multiple of batch size\");\n+        }\n+\n+        if (maxFileSizeInBytes < 0) {\n+            throw new IllegalArgumentException(\"Max file size must be either 0 or a positive integer\");\n+        }\n+\n+        this.serializer = serializer;\n+        this.deserializer = deserializer;\n+        this.moduleName = moduleName;\n+        this.batchSize = batchSize;\n+        this.maxFileSizeInBytes = maxFileSizeInBytes;\n+        batch = new Batch(batchSize);\n+\n+        inMemoryQueue = new ArrayBlockingQueue<>(inMemoryQueueSize, true);\n+\n+        // Setting the max file size to 0 or less will disable the off-heap portion of this queue\n+        if (maxFileSizeInBytes > 0) {\n+            Objects.requireNonNull(filePath);\n+            File file = Paths.get(filePath.toString(), moduleName + FILE_EXTENSION).toFile();\n+\n+            QueueFile qf;\n+            try {\n+                qf = new QueueFile.Builder(file).build();\n+            } catch (Exception e) {\n+                LOG.warn(\"Exception while loading queue file\", e);\n+\n+                if (file.delete()) {\n+                    qf = new QueueFile.Builder(file).build();\n+                } else {\n+                    throw new IOException(\"Could delete corrupted queue file \" + file.getAbsolutePath());\n+                }\n+            }\n+            offHeapQueue = qf;\n+\n+            // QueueFile unfortunately does not expose its file size usage publicly so we need to access it reflectively\n+            try {\n+                usedBytesMethod = offHeapQueue.getClass().getDeclaredMethod(\"usedBytes\");\n+                usedBytesMethod.setAccessible(true);\n+            } catch (NoSuchMethodException e) {\n+                LOG.warn(\"Could not instantiate queue\", e);\n+                throw new RuntimeException(e);\n+            }\n+\n+            checkFileSize();\n+        } else {\n+            offHeapQueue = null;\n+            usedBytesMethod = null;\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    public long checkFileSize() {\n+        try {\n+            long fileSize = (long) usedBytesMethod.invoke(offHeapQueue);\n+            fileCapacityLatch.setCurrentCapacityBytes(maxFileSizeInBytes - fileSize);\n+            LOG.trace(\"Checked file size for module {} and got result {} bytes\", moduleName,\n+                    fileSize);\n+\n+            return fileSize;\n+        } catch (IllegalAccessException | InvocationTargetException e) {\n+            RATE_LIMITED_LOGGER.warn(\"Failed to check file size for module {}\", moduleName, e);\n+            return 0;\n+        }\n+    }\n+\n+    /**\n+     * When enqueueing we prefer the in-memory queue unless the file based queue is already utilized. If that fails\n+     * (because it is full) we then enqueue via the file based queue provided it is not currently full and has been\n+     * configured. If the file based queue is full or not configured we block and wait for capacity.\n+     * <p>\n+     * We only write to the file based queue when we have a full batch ready. The batch container is then emptied after\n+     * being written to disk.\n+     */\n+    @Override\n+    public synchronized EnqueueResult enqueue(T message, String key) throws WriteFailedException {\n+        Map.Entry<String, T> msgEntry = new AbstractMap.SimpleImmutableEntry<>(key, message);\n+        \n+        LOG.debug(\"Attempting to enqueue {} with key {} into queue with current size {}\", message, key, getSize());", "originalCommit": "1825b2d49ad85ad7a650161eafa1b2a5bb1fbd5c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Njk5NzAzNQ==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r366997035", "bodyText": "This method is also used in source code.", "author": "cgorantla", "createdAt": "2020-01-15T17:06:57Z", "path": "core/ipc/sink/off-heap/src/main/java/org/opennms/core/ipc/sink/offheap/QueueFileOffHeapDispatchQueue.java", "diffHunk": "@@ -0,0 +1,537 @@\n+/*******************************************************************************\n+ * This file is part of OpenNMS(R).\n+ *\n+ * Copyright (C) 2020 The OpenNMS Group, Inc.\n+ * OpenNMS(R) is Copyright (C) 1999-2020 The OpenNMS Group, Inc.\n+ *\n+ * OpenNMS(R) is a registered trademark of The OpenNMS Group, Inc.\n+ *\n+ * OpenNMS(R) is free software: you can redistribute it and/or modify\n+ * it under the terms of the GNU Affero General Public License as published\n+ * by the Free Software Foundation, either version 3 of the License,\n+ * or (at your option) any later version.\n+ *\n+ * OpenNMS(R) is distributed in the hope that it will be useful,\n+ * but WITHOUT ANY WARRANTY; without even the implied warranty of\n+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+ * GNU Affero General Public License for more details.\n+ *\n+ * You should have received a copy of the GNU Affero General Public License\n+ * along with OpenNMS(R).  If not, see:\n+ *      http://www.gnu.org/licenses/\n+ *\n+ * For more information contact:\n+ *     OpenNMS(R) Licensing <license@opennms.org>\n+ *     http://www.opennms.org/\n+ *     http://www.opennms.com/\n+ *******************************************************************************/\n+\n+package org.opennms.core.ipc.sink.offheap;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.io.Serializable;\n+import java.lang.reflect.InvocationTargetException;\n+import java.lang.reflect.Method;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ArrayBlockingQueue;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ForkJoinPool;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import org.joda.time.Duration;\n+import org.opennms.core.ipc.sink.api.DispatchQueue;\n+import org.opennms.core.ipc.sink.api.WriteFailedException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.squareup.tape2.QueueFile;\n+import com.swrve.ratelimitedlogger.RateLimitedLog;\n+\n+/**\n+ * A {@link DispatchQueue} that first attempts to queue items in memory and upon overflowing the allocated in-memory\n+ * queue writes items \"off heap\" directly to disk via a file. The in-memory queue is volatile and if the process\n+ * crashes its contents are lost. The contents written to disk are durable and in the event of a crash will be reloaded.\n+ * <p>\n+ * This queue can be configured to only queue to memory by specifying the maximum off-heap size of 0. Using this\n+ * configuration causes {@link #enqueue} to block when the in-memory queue fills up rather than writing to the off-heap\n+ * queue.\n+ * <p>\n+ * Before queued items are written to disk they are first accumulated in a batch to limit the number of discrete writes\n+ * we make to disk. The batched items are considered part of the in-memory portion of the queue and are also volatile.\n+ *\n+ * @param <T> the type being queued\n+ */\n+public class QueueFileOffHeapDispatchQueue<T> implements DispatchQueue<T> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(QueueFileOffHeapDispatchQueue.class);\n+    private final RateLimitedLog RATE_LIMITED_LOGGER = RateLimitedLog\n+            .withRateLimit(LOG)\n+            .maxRate(5)\n+            .every(Duration.standardSeconds(30))\n+            .build();\n+\n+    // This must match the size of the HEADER_LENGTH in QueueFile's Element class\n+    //\n+    // We could pull this out of that class reflectively but it seemed easier to just hard code it here\n+    private static final int SERIALIZED_OBJECT_HEADER_SIZE_IN_BYTES = 4;\n+    private static final String FILE_EXTENSION = \".fifo\";\n+\n+    private final Function<T, byte[]> serializer;\n+    private final Function<byte[], T> deserializer;\n+    private final String moduleName;\n+    private final BlockingQueue<Map.Entry<String, T>> inMemoryQueue;\n+\n+    // Note the queue is not thread safe so access should be synchronized\n+    private final QueueFile offHeapQueue;\n+    private final long maxFileSizeInBytes;\n+    private final int batchSize;\n+    private final Batch batch;\n+\n+    private final ForkJoinPool serdesPool = new ForkJoinPool(\n+            Math.max(Runtime.getRuntime().availableProcessors() - 1, 1));\n+\n+    // Used to guard access to the offHeapQueue and batch data structures\n+    private final Object offHeapMutex = new Object();\n+    private final FileCapacityLatch fileCapacityLatch = new FileCapacityLatch();\n+\n+    private final Method usedBytesMethod;\n+\n+    public QueueFileOffHeapDispatchQueue(Function<T, byte[]> serializer, Function<byte[], T> deserializer,\n+                                         String moduleName, Path filePath, int inMemoryQueueSize, int batchSize,\n+                                         long maxFileSizeInBytes) throws IOException {\n+        Objects.requireNonNull(serializer);\n+        Objects.requireNonNull(deserializer);\n+        Objects.requireNonNull(moduleName);\n+\n+        if (inMemoryQueueSize < 1) {\n+            throw new IllegalArgumentException(\"In memory queue size must be greater than 0\");\n+        }\n+\n+        if (inMemoryQueueSize % batchSize != 0) {\n+            throw new IllegalArgumentException(\"In memory queue size must be a multiple of batch size\");\n+        }\n+\n+        if (maxFileSizeInBytes < 0) {\n+            throw new IllegalArgumentException(\"Max file size must be either 0 or a positive integer\");\n+        }\n+\n+        this.serializer = serializer;\n+        this.deserializer = deserializer;\n+        this.moduleName = moduleName;\n+        this.batchSize = batchSize;\n+        this.maxFileSizeInBytes = maxFileSizeInBytes;\n+        batch = new Batch(batchSize);\n+\n+        inMemoryQueue = new ArrayBlockingQueue<>(inMemoryQueueSize, true);\n+\n+        // Setting the max file size to 0 or less will disable the off-heap portion of this queue\n+        if (maxFileSizeInBytes > 0) {\n+            Objects.requireNonNull(filePath);\n+            File file = Paths.get(filePath.toString(), moduleName + FILE_EXTENSION).toFile();\n+\n+            QueueFile qf;\n+            try {\n+                qf = new QueueFile.Builder(file).build();\n+            } catch (Exception e) {\n+                LOG.warn(\"Exception while loading queue file\", e);\n+\n+                if (file.delete()) {\n+                    qf = new QueueFile.Builder(file).build();\n+                } else {\n+                    throw new IOException(\"Could delete corrupted queue file \" + file.getAbsolutePath());\n+                }\n+            }\n+            offHeapQueue = qf;\n+\n+            // QueueFile unfortunately does not expose its file size usage publicly so we need to access it reflectively\n+            try {\n+                usedBytesMethod = offHeapQueue.getClass().getDeclaredMethod(\"usedBytes\");\n+                usedBytesMethod.setAccessible(true);\n+            } catch (NoSuchMethodException e) {\n+                LOG.warn(\"Could not instantiate queue\", e);\n+                throw new RuntimeException(e);\n+            }\n+\n+            checkFileSize();\n+        } else {\n+            offHeapQueue = null;\n+            usedBytesMethod = null;\n+        }\n+    }\n+\n+    @VisibleForTesting", "originalCommit": "1825b2d49ad85ad7a650161eafa1b2a5bb1fbd5c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzEzODM1MA==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r367138350", "bodyText": "The only other class that uses it is a test class. It would normally be private otherwise so the annotation is there to explain why the access modifier is not private.", "author": "mattixtech", "createdAt": "2020-01-15T22:21:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Njk5NzAzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzA1NDUyNw==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r367054527", "bodyText": "It is also good to have a test case that shows that order is maintained while in the batch. With batch having 100 different messages and getting them back in order.", "author": "cgorantla", "createdAt": "2020-01-15T19:08:17Z", "path": "core/ipc/sink/off-heap/src/test/java/org/opennms/core/ipc/sink/offheap/QueueFileOffHeapDispatchQueueTest.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/*******************************************************************************\n+ * This file is part of OpenNMS(R).\n+ *\n+ * Copyright (C) 2020 The OpenNMS Group, Inc.\n+ * OpenNMS(R) is Copyright (C) 1999-2020 The OpenNMS Group, Inc.\n+ *\n+ * OpenNMS(R) is a registered trademark of The OpenNMS Group, Inc.\n+ *\n+ * OpenNMS(R) is free software: you can redistribute it and/or modify\n+ * it under the terms of the GNU Affero General Public License as published\n+ * by the Free Software Foundation, either version 3 of the License,\n+ * or (at your option) any later version.\n+ *\n+ * OpenNMS(R) is distributed in the hope that it will be useful,\n+ * but WITHOUT ANY WARRANTY; without even the implied warranty of\n+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+ * GNU Affero General Public License for more details.\n+ *\n+ * You should have received a copy of the GNU Affero General Public License\n+ * along with OpenNMS(R).  If not, see:\n+ *      http://www.gnu.org/licenses/\n+ *\n+ * For more information contact:\n+ *     OpenNMS(R) Licensing <license@opennms.org>\n+ *     http://www.opennms.org/\n+ *     http://www.opennms.com/\n+ *******************************************************************************/\n+\n+package org.opennms.core.ipc.sink.offheap;\n+\n+import static com.jayway.awaitility.Awaitility.await;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.CoreMatchers.nullValue;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.Objects;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.opennms.core.ipc.sink.api.DispatchQueue;\n+import org.opennms.core.ipc.sink.api.WriteFailedException;\n+\n+import com.jayway.awaitility.core.ConditionTimeoutException;\n+\n+public class QueueFileOffHeapDispatchQueueTest {\n+\n+    @Rule\n+    public TemporaryFolder folder = new TemporaryFolder();\n+\n+    @Test\n+    public void canQueueAndDequeue() throws IOException, WriteFailedException, InterruptedException {", "originalCommit": "1825b2d49ad85ad7a650161eafa1b2a5bb1fbd5c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzEyNzU0Ng==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r367127546", "bodyText": "Will do.", "author": "mattixtech", "createdAt": "2020-01-15T21:54:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzA1NDUyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzA2MDA3Nw==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r367060077", "bodyText": "This should never happen I guess, may be a warning if happens.", "author": "cgorantla", "createdAt": "2020-01-15T19:20:25Z", "path": "core/ipc/sink/common/src/main/java/org/opennms/core/ipc/sink/common/AsyncDispatcherImpl.java", "diffHunk": "@@ -67,226 +65,162 @@\n \n     private static final Logger LOG = LoggerFactory.getLogger(AsyncDispatcherImpl.class);\n     private final SyncDispatcher<S> syncDispatcher;\n-    private OffHeapAdapter offHeapAdapter;\n-    private ExecutorService offHeapAdapterExecutor = Executors.newSingleThreadExecutor();\n     private final AsyncPolicy asyncPolicy;\n-    private OffHeapQueue offHeapQueue;\n-    private SinkModule<S,T> sinkModule;\n-    private DispatcherState<W,S,T> state;\n-    private boolean useOffHeap = false;\n+    private final Counter droppedCounter;\n     \n-    final RateLimitedLog rateLimittedLogger = RateLimitedLog\n+    private final DispatchQueue<S> dispatchQueue;\n+    private final Map<String, CompletableFuture<DispatchStatus>> futureMap = new ConcurrentHashMap<>();\n+    private final AtomicInteger activeDispatchers = new AtomicInteger(0);\n+    \n+    private final RateLimitedLog RATE_LIMITED_LOGGER = RateLimitedLog\n             .withRateLimit(LOG)\n             .maxRate(5).every(Duration.standardSeconds(30))\n             .build();\n \n-    final LinkedBlockingQueue<Runnable> queue;\n-    final ExecutorService executor;\n+    private final ExecutorService executor;\n \n     public AsyncDispatcherImpl(DispatcherState<W, S, T> state, AsyncPolicy asyncPolicy,\n-            SyncDispatcher<S> syncDispatcher) {\n+                               SyncDispatcher<S> syncDispatcher) {\n         Objects.requireNonNull(state);\n         Objects.requireNonNull(asyncPolicy);\n-        this.syncDispatcher = Objects.requireNonNull(syncDispatcher);\n+        Objects.requireNonNull(syncDispatcher);\n+        this.syncDispatcher = syncDispatcher;\n         this.asyncPolicy = asyncPolicy;\n-        this.state = state;\n-        sinkModule = state.getModule();\n-        if (OffHeapServiceLoader.isOffHeapEnabled()) {\n-            offHeapQueue = OffHeapServiceLoader.getOffHeapQueue();\n-            if (offHeapQueue != null) {\n-                useOffHeap = true;\n-                LOG.info(\"Offheap storage enabled for sink module, {}\", sinkModule.getId());\n-            }\n-        }\n-        \n-        final RejectedExecutionHandler rejectedExecutionHandler;\n-        if (asyncPolicy.isBlockWhenFull()) {\n-            // This queue ensures that calling thread is blocked when the queue is full\n-            // See the implementation of OfferBlockingQueue for details\n-            queue = new OfferBlockingQueue<>(asyncPolicy.getQueueSize());\n-            rejectedExecutionHandler = new ThreadPoolExecutor.AbortPolicy();\n+        SinkModule<S, T> sinkModule = state.getModule();\n+        Optional<DispatchQueueFactory> factory = DispatchQueueServiceLoader.getDispatchQueueFactory();\n+\n+        if (factory.isPresent()) {\n+            LOG.debug(\"Using queue from factory\");\n+            dispatchQueue = factory.get().getQueue(asyncPolicy, sinkModule.getId(),\n+                    sinkModule::marshalSingleMessage, sinkModule::unmarshalSingleMessage);\n         } else {\n-            queue = new LinkedBlockingQueue<Runnable>(asyncPolicy.getQueueSize());\n-            // Reject and increase the dropped counter when the queue is full\n-            final Counter droppedCounter = state.getMetrics().counter(MetricRegistry.name(state.getModule().getId(), \"dropped\"));\n-            rejectedExecutionHandler = new RejectedExecutionHandler() {\n-                @Override\n-                public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {\n-                    droppedCounter.inc();\n-                    throw new RejectedExecutionException(\"Task \" + r.toString() +\n-                            \" rejected from \" +\n-                            e.toString());\n-                }\n-            };\n+            int size = asyncPolicy.getQueueSize();\n+            LOG.debug(\"Using default in memory queue of size {}\", size);\n+            dispatchQueue = new DefaultQueue<>(size);\n         }\n \n-        state.getMetrics().register(MetricRegistry.name(state.getModule().getId(), \"queue-size\"), new Gauge<Integer>() {\n-            @Override\n-            public Integer getValue() {\n-                return queue.size();\n-            }\n-        });\n+        state.getMetrics().register(MetricRegistry.name(state.getModule().getId(), \"queue-size\"),\n+                (Gauge<Integer>) activeDispatchers::get);\n+\n+        droppedCounter = state.getMetrics().counter(MetricRegistry.name(state.getModule().getId(), \"dropped\"));\n \n-        executor = new ThreadPoolExecutor(\n-                asyncPolicy.getNumThreads(),\n-                asyncPolicy.getNumThreads(),\n-                1000L,\n-                TimeUnit.MILLISECONDS,\n-                queue,\n-                new LogPreservingThreadFactory(SystemInfoUtils.DEFAULT_INSTANCE_ID + \".Sink.AsyncDispatcher.\" + state.getModule().getId(), Integer.MAX_VALUE),\n-                rejectedExecutionHandler\n-            );\n+        executor = Executors.newFixedThreadPool(asyncPolicy.getNumThreads(),\n+                new LogPreservingThreadFactory(SystemInfoUtils.DEFAULT_INSTANCE_ID + \".Sink.AsyncDispatcher.\" +\n+                        state.getModule().getId(), Integer.MAX_VALUE));\n+        startDrainingQueue();\n     }\n \n-    /**\n-     * When used in a ThreadPoolExecutor, this queue will block calls to\n-     * {@link ThreadPoolExecutor#execute(Runnable)} when the queue is full.\n-     * This is done by overriding calls to {@link LinkedBlockingQueue#offer(Object)}\n-     * with calls to {@link LinkedBlockingQueue#put(Object)}, but comes with the caveat\n-     * that executor must be built with <code>corePoolSize == maxPoolSize</code>.\n-     * In the context of the {@link AsyncDispatcherImpl}, this is an acceptable caveat,\n-     * since we enforce the matching pool sizes.\n-     *\n-     * There are alternative ways of solving this problem, for example we could use a\n-     * {@link RejectedExecutionHandler} to achieve similar behavior, and allow\n-     * for <code>corePoolSize < maxPoolSize</code>, but not for <code>corePoolSize==0</code>.\n-     *\n-     * For further discussions on this topic see:\n-     *   http://stackoverflow.com/a/3518588\n-     *   http://stackoverflow.com/a/32123535\n-     *\n-     * If the implementation is changed, make sure that that executor is built accordingly.\n-     */\n-    \n-    private static class OfferBlockingQueue<E> extends LinkedBlockingQueue<E> {\n-        private static final long serialVersionUID = 1L;\n+    private void dispatchFromQueue() {\n+        while (true) {\n+            try {\n+                LOG.trace(\"Asking dispatch queue for the next entry...\");\n+                Map.Entry<String, S> messageEntry = dispatchQueue.dequeue();\n+                LOG.trace(\"Received message entry from dispatch queue {}\", messageEntry);\n+                activeDispatchers.incrementAndGet();\n+                LOG.trace(\"Sending message {} via sync dispatcher\", messageEntry);\n+                syncDispatcher.send(messageEntry.getValue());\n+                LOG.trace(\"Successfully sent message {}\", messageEntry);\n+\n+                if (messageEntry.getKey() != null) {\n+                    LOG.trace(\"Attempting to complete future for message {}\", messageEntry);\n+                    CompletableFuture<DispatchStatus> messageFuture = futureMap.remove(messageEntry.getKey());\n+\n+                    if (messageFuture != null) {\n+                        messageFuture.complete(DispatchStatus.DISPATCHED);\n+                        LOG.trace(\"Completed future for message {}\", messageEntry);\n+                    } else {\n+                        RATE_LIMITED_LOGGER.warn(\"No future found for message {}\", messageEntry);\n+                    }\n+                } else {\n+                    LOG.trace(\"Dequeued an entry with a null key\");", "originalCommit": "1825b2d49ad85ad7a650161eafa1b2a5bb1fbd5c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzEzODc3MQ==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r367138771", "bodyText": "This will happen any time we get an off-heap entry since we don't bother keeping keys for them as we don't associate a future with them.", "author": "mattixtech", "createdAt": "2020-01-15T22:22:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzA2MDA3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzA3NTc4NA==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r367075784", "bodyText": "may be add newId here in the message or put it in RATE_LIMITED_LOGGER", "author": "cgorantla", "createdAt": "2020-01-15T19:54:13Z", "path": "core/ipc/sink/common/src/main/java/org/opennms/core/ipc/sink/common/AsyncDispatcherImpl.java", "diffHunk": "@@ -67,226 +65,162 @@\n \n     private static final Logger LOG = LoggerFactory.getLogger(AsyncDispatcherImpl.class);\n     private final SyncDispatcher<S> syncDispatcher;\n-    private OffHeapAdapter offHeapAdapter;\n-    private ExecutorService offHeapAdapterExecutor = Executors.newSingleThreadExecutor();\n     private final AsyncPolicy asyncPolicy;\n-    private OffHeapQueue offHeapQueue;\n-    private SinkModule<S,T> sinkModule;\n-    private DispatcherState<W,S,T> state;\n-    private boolean useOffHeap = false;\n+    private final Counter droppedCounter;\n     \n-    final RateLimitedLog rateLimittedLogger = RateLimitedLog\n+    private final DispatchQueue<S> dispatchQueue;\n+    private final Map<String, CompletableFuture<DispatchStatus>> futureMap = new ConcurrentHashMap<>();\n+    private final AtomicInteger activeDispatchers = new AtomicInteger(0);\n+    \n+    private final RateLimitedLog RATE_LIMITED_LOGGER = RateLimitedLog\n             .withRateLimit(LOG)\n             .maxRate(5).every(Duration.standardSeconds(30))\n             .build();\n \n-    final LinkedBlockingQueue<Runnable> queue;\n-    final ExecutorService executor;\n+    private final ExecutorService executor;\n \n     public AsyncDispatcherImpl(DispatcherState<W, S, T> state, AsyncPolicy asyncPolicy,\n-            SyncDispatcher<S> syncDispatcher) {\n+                               SyncDispatcher<S> syncDispatcher) {\n         Objects.requireNonNull(state);\n         Objects.requireNonNull(asyncPolicy);\n-        this.syncDispatcher = Objects.requireNonNull(syncDispatcher);\n+        Objects.requireNonNull(syncDispatcher);\n+        this.syncDispatcher = syncDispatcher;\n         this.asyncPolicy = asyncPolicy;\n-        this.state = state;\n-        sinkModule = state.getModule();\n-        if (OffHeapServiceLoader.isOffHeapEnabled()) {\n-            offHeapQueue = OffHeapServiceLoader.getOffHeapQueue();\n-            if (offHeapQueue != null) {\n-                useOffHeap = true;\n-                LOG.info(\"Offheap storage enabled for sink module, {}\", sinkModule.getId());\n-            }\n-        }\n-        \n-        final RejectedExecutionHandler rejectedExecutionHandler;\n-        if (asyncPolicy.isBlockWhenFull()) {\n-            // This queue ensures that calling thread is blocked when the queue is full\n-            // See the implementation of OfferBlockingQueue for details\n-            queue = new OfferBlockingQueue<>(asyncPolicy.getQueueSize());\n-            rejectedExecutionHandler = new ThreadPoolExecutor.AbortPolicy();\n+        SinkModule<S, T> sinkModule = state.getModule();\n+        Optional<DispatchQueueFactory> factory = DispatchQueueServiceLoader.getDispatchQueueFactory();\n+\n+        if (factory.isPresent()) {\n+            LOG.debug(\"Using queue from factory\");\n+            dispatchQueue = factory.get().getQueue(asyncPolicy, sinkModule.getId(),\n+                    sinkModule::marshalSingleMessage, sinkModule::unmarshalSingleMessage);\n         } else {\n-            queue = new LinkedBlockingQueue<Runnable>(asyncPolicy.getQueueSize());\n-            // Reject and increase the dropped counter when the queue is full\n-            final Counter droppedCounter = state.getMetrics().counter(MetricRegistry.name(state.getModule().getId(), \"dropped\"));\n-            rejectedExecutionHandler = new RejectedExecutionHandler() {\n-                @Override\n-                public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {\n-                    droppedCounter.inc();\n-                    throw new RejectedExecutionException(\"Task \" + r.toString() +\n-                            \" rejected from \" +\n-                            e.toString());\n-                }\n-            };\n+            int size = asyncPolicy.getQueueSize();\n+            LOG.debug(\"Using default in memory queue of size {}\", size);\n+            dispatchQueue = new DefaultQueue<>(size);\n         }\n \n-        state.getMetrics().register(MetricRegistry.name(state.getModule().getId(), \"queue-size\"), new Gauge<Integer>() {\n-            @Override\n-            public Integer getValue() {\n-                return queue.size();\n-            }\n-        });\n+        state.getMetrics().register(MetricRegistry.name(state.getModule().getId(), \"queue-size\"),\n+                (Gauge<Integer>) activeDispatchers::get);\n+\n+        droppedCounter = state.getMetrics().counter(MetricRegistry.name(state.getModule().getId(), \"dropped\"));\n \n-        executor = new ThreadPoolExecutor(\n-                asyncPolicy.getNumThreads(),\n-                asyncPolicy.getNumThreads(),\n-                1000L,\n-                TimeUnit.MILLISECONDS,\n-                queue,\n-                new LogPreservingThreadFactory(SystemInfoUtils.DEFAULT_INSTANCE_ID + \".Sink.AsyncDispatcher.\" + state.getModule().getId(), Integer.MAX_VALUE),\n-                rejectedExecutionHandler\n-            );\n+        executor = Executors.newFixedThreadPool(asyncPolicy.getNumThreads(),\n+                new LogPreservingThreadFactory(SystemInfoUtils.DEFAULT_INSTANCE_ID + \".Sink.AsyncDispatcher.\" +\n+                        state.getModule().getId(), Integer.MAX_VALUE));\n+        startDrainingQueue();\n     }\n \n-    /**\n-     * When used in a ThreadPoolExecutor, this queue will block calls to\n-     * {@link ThreadPoolExecutor#execute(Runnable)} when the queue is full.\n-     * This is done by overriding calls to {@link LinkedBlockingQueue#offer(Object)}\n-     * with calls to {@link LinkedBlockingQueue#put(Object)}, but comes with the caveat\n-     * that executor must be built with <code>corePoolSize == maxPoolSize</code>.\n-     * In the context of the {@link AsyncDispatcherImpl}, this is an acceptable caveat,\n-     * since we enforce the matching pool sizes.\n-     *\n-     * There are alternative ways of solving this problem, for example we could use a\n-     * {@link RejectedExecutionHandler} to achieve similar behavior, and allow\n-     * for <code>corePoolSize < maxPoolSize</code>, but not for <code>corePoolSize==0</code>.\n-     *\n-     * For further discussions on this topic see:\n-     *   http://stackoverflow.com/a/3518588\n-     *   http://stackoverflow.com/a/32123535\n-     *\n-     * If the implementation is changed, make sure that that executor is built accordingly.\n-     */\n-    \n-    private static class OfferBlockingQueue<E> extends LinkedBlockingQueue<E> {\n-        private static final long serialVersionUID = 1L;\n+    private void dispatchFromQueue() {\n+        while (true) {\n+            try {\n+                LOG.trace(\"Asking dispatch queue for the next entry...\");\n+                Map.Entry<String, S> messageEntry = dispatchQueue.dequeue();\n+                LOG.trace(\"Received message entry from dispatch queue {}\", messageEntry);\n+                activeDispatchers.incrementAndGet();\n+                LOG.trace(\"Sending message {} via sync dispatcher\", messageEntry);\n+                syncDispatcher.send(messageEntry.getValue());\n+                LOG.trace(\"Successfully sent message {}\", messageEntry);\n+\n+                if (messageEntry.getKey() != null) {\n+                    LOG.trace(\"Attempting to complete future for message {}\", messageEntry);\n+                    CompletableFuture<DispatchStatus> messageFuture = futureMap.remove(messageEntry.getKey());\n+\n+                    if (messageFuture != null) {\n+                        messageFuture.complete(DispatchStatus.DISPATCHED);\n+                        LOG.trace(\"Completed future for message {}\", messageEntry);\n+                    } else {\n+                        RATE_LIMITED_LOGGER.warn(\"No future found for message {}\", messageEntry);\n+                    }\n+                } else {\n+                    LOG.trace(\"Dequeued an entry with a null key\");\n+                }\n \n-        public OfferBlockingQueue(int capacity) {\n-            super(capacity);\n+                activeDispatchers.decrementAndGet();\n+            } catch (InterruptedException e) {\n+                break;\n+            } catch (Exception e) {\n+                RATE_LIMITED_LOGGER.warn(\"Encountered exception while taking from dispatch queue\", e);\n+            }\n         }\n+    }\n \n-        @Override\n-        public boolean offer(E e) {\n-            try {\n-                put(e);\n-                return true;\n-            } catch (InterruptedException ie) {\n-                Thread.currentThread().interrupt();\n-            }\n-            return false;\n+    private void startDrainingQueue() {\n+        for (int i = 0; i < asyncPolicy.getNumThreads(); i++) {\n+            executor.execute(this::dispatchFromQueue);\n         }\n     }\n \n     @Override\n-    public CompletableFuture<S> send(S message) {\n-         \n-        // Check if OffHeap is enabled and if local queue is full or if OffHeap not Empty then write message to OffHeap.\n-        if (useOffHeap && (asyncPolicy.getQueueSize() == getQueueSize() ||\n-                ((offHeapAdapter != null) && !offHeapAdapter.isOffHeapEmpty()))) {\n-            // Start drain thread before the first write to OffHeapQueue.\n-            if (offHeapAdapter == null) {\n-                this.offHeapAdapter = new OffHeapAdapter();\n-                offHeapAdapterExecutor.execute(offHeapAdapter);\n-                LOG.info(\"started drain thread for {}\", sinkModule.getId());\n-            }\n-            try {\n-                return offHeapAdapter.writeMessage(message);\n-            } catch (WriteFailedException e) {\n-                rateLimittedLogger.error(\"OffHeap write failed \", e);\n-            }\n+    public CompletableFuture<DispatchStatus> send(S message) {\n+        CompletableFuture<DispatchStatus> sendFuture = new CompletableFuture<>();\n+\n+        if (!asyncPolicy.isBlockWhenFull() && dispatchQueue.isFull()) {\n+            droppedCounter.inc();\n+            sendFuture.completeExceptionally(new RuntimeException(\"Dispatch queue full\"));\n+            return sendFuture;\n         }\n+\n         try {\n-            return CompletableFuture.supplyAsync(() -> {\n-                syncDispatcher.send(message);\n-                return message;\n-            }, executor);\n-        } catch (RejectedExecutionException ree) {\n-            final CompletableFuture<S> future = new CompletableFuture<>();\n-            future.completeExceptionally(ree);\n-            return future;\n+            String newId = UUID.randomUUID().toString();\n+            DispatchQueue.EnqueueResult result = dispatchQueue.enqueue(message, newId);\n+            \n+            LOG.trace(\"Result of enqueueing was {}\", result);", "originalCommit": "1825b2d49ad85ad7a650161eafa1b2a5bb1fbd5c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzA3NjgxOA==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r367076818", "bodyText": "since the message could be large, put these traces  under if(LOG.isTraceEnabled) or remove the message from the log.", "author": "cgorantla", "createdAt": "2020-01-15T19:56:25Z", "path": "core/ipc/sink/common/src/main/java/org/opennms/core/ipc/sink/common/AsyncDispatcherImpl.java", "diffHunk": "@@ -67,226 +65,162 @@\n \n     private static final Logger LOG = LoggerFactory.getLogger(AsyncDispatcherImpl.class);\n     private final SyncDispatcher<S> syncDispatcher;\n-    private OffHeapAdapter offHeapAdapter;\n-    private ExecutorService offHeapAdapterExecutor = Executors.newSingleThreadExecutor();\n     private final AsyncPolicy asyncPolicy;\n-    private OffHeapQueue offHeapQueue;\n-    private SinkModule<S,T> sinkModule;\n-    private DispatcherState<W,S,T> state;\n-    private boolean useOffHeap = false;\n+    private final Counter droppedCounter;\n     \n-    final RateLimitedLog rateLimittedLogger = RateLimitedLog\n+    private final DispatchQueue<S> dispatchQueue;\n+    private final Map<String, CompletableFuture<DispatchStatus>> futureMap = new ConcurrentHashMap<>();\n+    private final AtomicInteger activeDispatchers = new AtomicInteger(0);\n+    \n+    private final RateLimitedLog RATE_LIMITED_LOGGER = RateLimitedLog\n             .withRateLimit(LOG)\n             .maxRate(5).every(Duration.standardSeconds(30))\n             .build();\n \n-    final LinkedBlockingQueue<Runnable> queue;\n-    final ExecutorService executor;\n+    private final ExecutorService executor;\n \n     public AsyncDispatcherImpl(DispatcherState<W, S, T> state, AsyncPolicy asyncPolicy,\n-            SyncDispatcher<S> syncDispatcher) {\n+                               SyncDispatcher<S> syncDispatcher) {\n         Objects.requireNonNull(state);\n         Objects.requireNonNull(asyncPolicy);\n-        this.syncDispatcher = Objects.requireNonNull(syncDispatcher);\n+        Objects.requireNonNull(syncDispatcher);\n+        this.syncDispatcher = syncDispatcher;\n         this.asyncPolicy = asyncPolicy;\n-        this.state = state;\n-        sinkModule = state.getModule();\n-        if (OffHeapServiceLoader.isOffHeapEnabled()) {\n-            offHeapQueue = OffHeapServiceLoader.getOffHeapQueue();\n-            if (offHeapQueue != null) {\n-                useOffHeap = true;\n-                LOG.info(\"Offheap storage enabled for sink module, {}\", sinkModule.getId());\n-            }\n-        }\n-        \n-        final RejectedExecutionHandler rejectedExecutionHandler;\n-        if (asyncPolicy.isBlockWhenFull()) {\n-            // This queue ensures that calling thread is blocked when the queue is full\n-            // See the implementation of OfferBlockingQueue for details\n-            queue = new OfferBlockingQueue<>(asyncPolicy.getQueueSize());\n-            rejectedExecutionHandler = new ThreadPoolExecutor.AbortPolicy();\n+        SinkModule<S, T> sinkModule = state.getModule();\n+        Optional<DispatchQueueFactory> factory = DispatchQueueServiceLoader.getDispatchQueueFactory();\n+\n+        if (factory.isPresent()) {\n+            LOG.debug(\"Using queue from factory\");\n+            dispatchQueue = factory.get().getQueue(asyncPolicy, sinkModule.getId(),\n+                    sinkModule::marshalSingleMessage, sinkModule::unmarshalSingleMessage);\n         } else {\n-            queue = new LinkedBlockingQueue<Runnable>(asyncPolicy.getQueueSize());\n-            // Reject and increase the dropped counter when the queue is full\n-            final Counter droppedCounter = state.getMetrics().counter(MetricRegistry.name(state.getModule().getId(), \"dropped\"));\n-            rejectedExecutionHandler = new RejectedExecutionHandler() {\n-                @Override\n-                public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {\n-                    droppedCounter.inc();\n-                    throw new RejectedExecutionException(\"Task \" + r.toString() +\n-                            \" rejected from \" +\n-                            e.toString());\n-                }\n-            };\n+            int size = asyncPolicy.getQueueSize();\n+            LOG.debug(\"Using default in memory queue of size {}\", size);\n+            dispatchQueue = new DefaultQueue<>(size);\n         }\n \n-        state.getMetrics().register(MetricRegistry.name(state.getModule().getId(), \"queue-size\"), new Gauge<Integer>() {\n-            @Override\n-            public Integer getValue() {\n-                return queue.size();\n-            }\n-        });\n+        state.getMetrics().register(MetricRegistry.name(state.getModule().getId(), \"queue-size\"),\n+                (Gauge<Integer>) activeDispatchers::get);\n+\n+        droppedCounter = state.getMetrics().counter(MetricRegistry.name(state.getModule().getId(), \"dropped\"));\n \n-        executor = new ThreadPoolExecutor(\n-                asyncPolicy.getNumThreads(),\n-                asyncPolicy.getNumThreads(),\n-                1000L,\n-                TimeUnit.MILLISECONDS,\n-                queue,\n-                new LogPreservingThreadFactory(SystemInfoUtils.DEFAULT_INSTANCE_ID + \".Sink.AsyncDispatcher.\" + state.getModule().getId(), Integer.MAX_VALUE),\n-                rejectedExecutionHandler\n-            );\n+        executor = Executors.newFixedThreadPool(asyncPolicy.getNumThreads(),\n+                new LogPreservingThreadFactory(SystemInfoUtils.DEFAULT_INSTANCE_ID + \".Sink.AsyncDispatcher.\" +\n+                        state.getModule().getId(), Integer.MAX_VALUE));\n+        startDrainingQueue();\n     }\n \n-    /**\n-     * When used in a ThreadPoolExecutor, this queue will block calls to\n-     * {@link ThreadPoolExecutor#execute(Runnable)} when the queue is full.\n-     * This is done by overriding calls to {@link LinkedBlockingQueue#offer(Object)}\n-     * with calls to {@link LinkedBlockingQueue#put(Object)}, but comes with the caveat\n-     * that executor must be built with <code>corePoolSize == maxPoolSize</code>.\n-     * In the context of the {@link AsyncDispatcherImpl}, this is an acceptable caveat,\n-     * since we enforce the matching pool sizes.\n-     *\n-     * There are alternative ways of solving this problem, for example we could use a\n-     * {@link RejectedExecutionHandler} to achieve similar behavior, and allow\n-     * for <code>corePoolSize < maxPoolSize</code>, but not for <code>corePoolSize==0</code>.\n-     *\n-     * For further discussions on this topic see:\n-     *   http://stackoverflow.com/a/3518588\n-     *   http://stackoverflow.com/a/32123535\n-     *\n-     * If the implementation is changed, make sure that that executor is built accordingly.\n-     */\n-    \n-    private static class OfferBlockingQueue<E> extends LinkedBlockingQueue<E> {\n-        private static final long serialVersionUID = 1L;\n+    private void dispatchFromQueue() {\n+        while (true) {\n+            try {\n+                LOG.trace(\"Asking dispatch queue for the next entry...\");\n+                Map.Entry<String, S> messageEntry = dispatchQueue.dequeue();\n+                LOG.trace(\"Received message entry from dispatch queue {}\", messageEntry);", "originalCommit": "1825b2d49ad85ad7a650161eafa1b2a5bb1fbd5c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzEzOTQ2OQ==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r367139469", "bodyText": "AFAIK the toString() will only be called if trace is enabled so adding the conditional around the message won't make a difference.", "author": "mattixtech", "createdAt": "2020-01-15T22:24:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzA3NjgxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzExNjgwMw==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r367116803", "bodyText": "I'm not sure if I understand this code completely.   If there is only one thread that does dequeue the order should be maintained. Would like to see a test case.", "author": "cgorantla", "createdAt": "2020-01-15T21:29:27Z", "path": "core/ipc/sink/off-heap/src/main/java/org/opennms/core/ipc/sink/offheap/QueueFileOffHeapDispatchQueue.java", "diffHunk": "@@ -0,0 +1,537 @@\n+/*******************************************************************************\n+ * This file is part of OpenNMS(R).\n+ *\n+ * Copyright (C) 2020 The OpenNMS Group, Inc.\n+ * OpenNMS(R) is Copyright (C) 1999-2020 The OpenNMS Group, Inc.\n+ *\n+ * OpenNMS(R) is a registered trademark of The OpenNMS Group, Inc.\n+ *\n+ * OpenNMS(R) is free software: you can redistribute it and/or modify\n+ * it under the terms of the GNU Affero General Public License as published\n+ * by the Free Software Foundation, either version 3 of the License,\n+ * or (at your option) any later version.\n+ *\n+ * OpenNMS(R) is distributed in the hope that it will be useful,\n+ * but WITHOUT ANY WARRANTY; without even the implied warranty of\n+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+ * GNU Affero General Public License for more details.\n+ *\n+ * You should have received a copy of the GNU Affero General Public License\n+ * along with OpenNMS(R).  If not, see:\n+ *      http://www.gnu.org/licenses/\n+ *\n+ * For more information contact:\n+ *     OpenNMS(R) Licensing <license@opennms.org>\n+ *     http://www.opennms.org/\n+ *     http://www.opennms.com/\n+ *******************************************************************************/\n+\n+package org.opennms.core.ipc.sink.offheap;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.io.Serializable;\n+import java.lang.reflect.InvocationTargetException;\n+import java.lang.reflect.Method;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ArrayBlockingQueue;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ForkJoinPool;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import org.joda.time.Duration;\n+import org.opennms.core.ipc.sink.api.DispatchQueue;\n+import org.opennms.core.ipc.sink.api.WriteFailedException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.squareup.tape2.QueueFile;\n+import com.swrve.ratelimitedlogger.RateLimitedLog;\n+\n+/**\n+ * A {@link DispatchQueue} that first attempts to queue items in memory and upon overflowing the allocated in-memory\n+ * queue writes items \"off heap\" directly to disk via a file. The in-memory queue is volatile and if the process\n+ * crashes its contents are lost. The contents written to disk are durable and in the event of a crash will be reloaded.\n+ * <p>\n+ * This queue can be configured to only queue to memory by specifying the maximum off-heap size of 0. Using this\n+ * configuration causes {@link #enqueue} to block when the in-memory queue fills up rather than writing to the off-heap\n+ * queue.\n+ * <p>\n+ * Before queued items are written to disk they are first accumulated in a batch to limit the number of discrete writes\n+ * we make to disk. The batched items are considered part of the in-memory portion of the queue and are also volatile.\n+ *\n+ * @param <T> the type being queued\n+ */\n+public class QueueFileOffHeapDispatchQueue<T> implements DispatchQueue<T> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(QueueFileOffHeapDispatchQueue.class);\n+    private final RateLimitedLog RATE_LIMITED_LOGGER = RateLimitedLog\n+            .withRateLimit(LOG)\n+            .maxRate(5)\n+            .every(Duration.standardSeconds(30))\n+            .build();\n+\n+    // This must match the size of the HEADER_LENGTH in QueueFile's Element class\n+    //\n+    // We could pull this out of that class reflectively but it seemed easier to just hard code it here\n+    private static final int SERIALIZED_OBJECT_HEADER_SIZE_IN_BYTES = 4;\n+    private static final String FILE_EXTENSION = \".fifo\";\n+\n+    private final Function<T, byte[]> serializer;\n+    private final Function<byte[], T> deserializer;\n+    private final String moduleName;\n+    private final BlockingQueue<Map.Entry<String, T>> inMemoryQueue;\n+\n+    // Note the queue is not thread safe so access should be synchronized\n+    private final QueueFile offHeapQueue;\n+    private final long maxFileSizeInBytes;\n+    private final int batchSize;\n+    private final Batch batch;\n+\n+    private final ForkJoinPool serdesPool = new ForkJoinPool(\n+            Math.max(Runtime.getRuntime().availableProcessors() - 1, 1));\n+\n+    // Used to guard access to the offHeapQueue and batch data structures\n+    private final Object offHeapMutex = new Object();\n+    private final FileCapacityLatch fileCapacityLatch = new FileCapacityLatch();\n+\n+    private final Method usedBytesMethod;\n+\n+    public QueueFileOffHeapDispatchQueue(Function<T, byte[]> serializer, Function<byte[], T> deserializer,\n+                                         String moduleName, Path filePath, int inMemoryQueueSize, int batchSize,\n+                                         long maxFileSizeInBytes) throws IOException {\n+        Objects.requireNonNull(serializer);\n+        Objects.requireNonNull(deserializer);\n+        Objects.requireNonNull(moduleName);\n+\n+        if (inMemoryQueueSize < 1) {\n+            throw new IllegalArgumentException(\"In memory queue size must be greater than 0\");\n+        }\n+\n+        if (inMemoryQueueSize % batchSize != 0) {\n+            throw new IllegalArgumentException(\"In memory queue size must be a multiple of batch size\");\n+        }\n+\n+        if (maxFileSizeInBytes < 0) {\n+            throw new IllegalArgumentException(\"Max file size must be either 0 or a positive integer\");\n+        }\n+\n+        this.serializer = serializer;\n+        this.deserializer = deserializer;\n+        this.moduleName = moduleName;\n+        this.batchSize = batchSize;\n+        this.maxFileSizeInBytes = maxFileSizeInBytes;\n+        batch = new Batch(batchSize);\n+\n+        inMemoryQueue = new ArrayBlockingQueue<>(inMemoryQueueSize, true);\n+\n+        // Setting the max file size to 0 or less will disable the off-heap portion of this queue\n+        if (maxFileSizeInBytes > 0) {\n+            Objects.requireNonNull(filePath);\n+            File file = Paths.get(filePath.toString(), moduleName + FILE_EXTENSION).toFile();\n+\n+            QueueFile qf;\n+            try {\n+                qf = new QueueFile.Builder(file).build();\n+            } catch (Exception e) {\n+                LOG.warn(\"Exception while loading queue file\", e);\n+\n+                if (file.delete()) {\n+                    qf = new QueueFile.Builder(file).build();\n+                } else {\n+                    throw new IOException(\"Could delete corrupted queue file \" + file.getAbsolutePath());\n+                }\n+            }\n+            offHeapQueue = qf;\n+\n+            // QueueFile unfortunately does not expose its file size usage publicly so we need to access it reflectively\n+            try {\n+                usedBytesMethod = offHeapQueue.getClass().getDeclaredMethod(\"usedBytes\");\n+                usedBytesMethod.setAccessible(true);\n+            } catch (NoSuchMethodException e) {\n+                LOG.warn(\"Could not instantiate queue\", e);\n+                throw new RuntimeException(e);\n+            }\n+\n+            checkFileSize();\n+        } else {\n+            offHeapQueue = null;\n+            usedBytesMethod = null;\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    public long checkFileSize() {\n+        try {\n+            long fileSize = (long) usedBytesMethod.invoke(offHeapQueue);\n+            fileCapacityLatch.setCurrentCapacityBytes(maxFileSizeInBytes - fileSize);\n+            LOG.trace(\"Checked file size for module {} and got result {} bytes\", moduleName,\n+                    fileSize);\n+\n+            return fileSize;\n+        } catch (IllegalAccessException | InvocationTargetException e) {\n+            RATE_LIMITED_LOGGER.warn(\"Failed to check file size for module {}\", moduleName, e);\n+            return 0;\n+        }\n+    }\n+\n+    /**\n+     * When enqueueing we prefer the in-memory queue unless the file based queue is already utilized. If that fails\n+     * (because it is full) we then enqueue via the file based queue provided it is not currently full and has been\n+     * configured. If the file based queue is full or not configured we block and wait for capacity.\n+     * <p>\n+     * We only write to the file based queue when we have a full batch ready. The batch container is then emptied after\n+     * being written to disk.\n+     */\n+    @Override\n+    public synchronized EnqueueResult enqueue(T message, String key) throws WriteFailedException {\n+        Map.Entry<String, T> msgEntry = new AbstractMap.SimpleImmutableEntry<>(key, message);\n+        \n+        LOG.debug(\"Attempting to enqueue {} with key {} into queue with current size {}\", message, key, getSize());\n+\n+        // Off-heap queueing is not enabled so queue directly to memory\n+        if (offHeapQueue == null) {\n+            LOG.trace(\"Enqueueing {} with key {} in-memory since there is no off-heap queue \" +\n+                    \"configured\", message, key);\n+\n+            try {\n+                inMemoryQueue.put(msgEntry);\n+            } catch (InterruptedException e) {\n+                throw new WriteFailedException(e);\n+            }\n+\n+            return EnqueueResult.IMMEDIATE;\n+        }\n+\n+        // Off-heap queueing is enabled but we haven't started using it yet so continue trying to fill the in-memory\n+        // queue\n+        int size = 0;\n+        byte[] serializedBatch = null;\n+        synchronized (offHeapMutex) {\n+            if (offHeapQueue.size() <= 0 && batch.isEmpty()) {\n+                // If the in-memory queue is full, this offer will fail and we will fall through below to the off-heap\n+                // queueing logic\n+                boolean inMemoryQueueHadSpace = inMemoryQueue.offer(msgEntry);\n+\n+                if (inMemoryQueueHadSpace) {\n+                    LOG.trace(\"Enqueueing {} with key {} in-memory\", message, key);\n+\n+                    return EnqueueResult.IMMEDIATE;\n+                }\n+            }\n+\n+            // The in-memory queue is either full or there is already message in the batch or off-heap so we continue to\n+            // batch\n+            LOG.trace(\"Batching message {} with key {} for off-heap queue\", message, key);\n+            batch.add(message);\n+\n+            if (batch.isFull()) {\n+                LOG.trace(\"Flushing batch off-heap\");\n+\n+                try {\n+                    serializedBatch = batch.toSerializedBatchAndClear();\n+                } catch (Exception e) {\n+                    RATE_LIMITED_LOGGER.warn(\"Failed to flush to off-heap\", e);\n+                    throw new WriteFailedException(e);\n+                }\n+\n+                size = serializedBatch.length + SERIALIZED_OBJECT_HEADER_SIZE_IN_BYTES;\n+                fileCapacityLatch.markFlushNeeded();\n+            }\n+        }\n+\n+        if (serializedBatch != null) {\n+            try {\n+                // This is a critical blocking call and it has to be done outside the context of any shared lock with\n+                // dequeue() otherwise it will cause a deadlock\n+                //\n+                // After unblocking we will pick up the lock again and double check that we still need to flush and then\n+                // perform that while holding the lock\n+                fileCapacityLatch.waitForCapacity(size);\n+\n+                synchronized (offHeapMutex) {\n+                    if (!fileCapacityLatch.isFlushNeeded()) {\n+                        return EnqueueResult.DEFERRED;\n+                    }\n+\n+                    try {\n+                        offHeapQueue.add(serializedBatch);\n+\n+                        // Since we just wrote to disk, we need to check the file again to record the current capacity\n+                        checkFileSize();\n+                    } catch (IOException e) {\n+                        throw new WriteFailedException(e);\n+                    }\n+                }\n+            } catch (InterruptedException e) {\n+                throw new WriteFailedException(e);\n+            }\n+        }\n+\n+        return EnqueueResult.DEFERRED;\n+    }\n+\n+    /**\n+     * On every call to dequeue, if the off-heap queue is configured, we check the file for an entry and drain it to the\n+     * in-memory queue provided there is room. We then take exclusively from the head of the in-memory queue which\n+     * ensures ordering with respect to the two discrete queues.\n+     * <p>\n+     * After completely draining the queue on disk we check the existing batch for entries and drain them next.\n+     */\n+    @Override\n+    public Map.Entry<String, T> dequeue() throws InterruptedException {\n+        LOG.debug(\"Dequeueing an entry from queue with current size {}\", getSize());\n+\n+        // If off-heap queueing is enabled we need to first check if there is anything to read off-heap\n+        if (offHeapQueue != null) {\n+            synchronized (offHeapMutex) {\n+                // Try to move a batch from the off-heap queue to the in-memory queue\n+                if (offHeapQueue.size() > 0 && inMemoryQueue.remainingCapacity() >= batchSize) {\n+                    LOG.trace(\"Found an entry off-heap and there was room in-memory, moving it\");\n+\n+                    try {\n+                        byte[] entry = offHeapQueue.peek();\n+                        if (entry != null) {\n+                            offHeapQueue.remove();\n+\n+                            try {\n+                                inMemoryQueue.addAll(unbatchSerializedBatch(new SerializedBatch(entry)));", "originalCommit": "1825b2d49ad85ad7a650161eafa1b2a5bb1fbd5c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzEyNzQ1OA==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r367127458", "bodyText": "Good idea, I will add a test case to verify the ordering. Since batches are synchronized the order of contents should be guaranteed to be in the same order.\nLet me know if you want me to elaborate on how anything works.", "author": "mattixtech", "createdAt": "2020-01-15T21:54:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzExNjgwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzEzOTMxNw==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r367139317", "bodyText": "Adding a test case with some data on file, some data in memory would be sufficient. Then I can play with that and understand the logic better.", "author": "cgorantla", "createdAt": "2020-01-15T22:23:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzExNjgwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzE1MjY1OQ==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r367152659", "bodyText": "Looks like there was an issue with the ordering, will fix. Thanks for catching this.", "author": "mattixtech", "createdAt": "2020-01-15T23:00:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzExNjgwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzUwMjM4Nw==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r367502387", "bodyText": "If I understand this correctly,  If dequeue is called from two different threads, it would pull two different batches of messages in parallel then order may not be maintained when they dispatch in two different threads.", "author": "cgorantla", "createdAt": "2020-01-16T16:00:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzExNjgwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzYzNjcyMg==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r367636722", "bodyText": "All calls to dequeue end up taking from the inMemoryQueue data structure and ordering is ensured there by guarding manipulation of that queue with a mutex. It is not possible for two threads to be simultaneously retrieving two different batches at once.\nThis means that entries are always dequeued in order.\nHowever it is still possible to dispatch entries in a different order since there is multiple consuming threads calling dequeue since all the threads ultimately race to finish dispatching. If we need that to be guaranteed to be in order we need make sure there is only one thread consuming from the queue which will likely drastically impact the performance of the AsyncDispatcher.\nNote that that race condition existed in the previous implementation so this PR is not violating any previously established ordering guarantee.", "author": "mattixtech", "createdAt": "2020-01-16T20:36:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzExNjgwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY2MTg2Ng==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r367661866", "bodyText": "All calls to dequeue end up taking from the inMemoryQueue data structure and ordering is ensured there by guarding manipulation of that queue with a mutex. It is not possible for two threads to be simultaneously retrieving two different batches at once.\nThis means that entries are always dequeued in order.\nHowever it is still possible to dispatch entries in a different order since there is multiple consuming threads calling dequeue since all the threads ultimately race to finish dispatching. If we need that to be guaranteed to be in order we need make sure there is only one thread consuming from the queue which will likely drastically impact the performance of the AsyncDispatcher.\nNote that that race condition existed in the previous implementation so this PR is not violating any previously established ordering guarantee.\n\nGot it. I think it does maintain order. Can we modify test case for verifying that multiple threads doing dequeue also maintains order. Probably synchronize both dequeue and putting elements back.\nI think we don't need to worry about dispatching in order since even after pushing them to kafka, there is no guarantee in order.", "author": "cgorantla", "createdAt": "2020-01-16T21:34:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzExNjgwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY4OTI5NA==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r367689294", "bodyText": "Multiple threads calling dequeue won't necessarily be in order since there is a race condition on which thread will get to take() from the queue first. This should be acceptable since we don't have an ordering guarantee on the consumption side anyways (dispatching).\nThe queue itself will dispense items in the correct order but there is no way of knowing the order the threads will actually dequeue in so there really isn't any predictable behaviour to test without synchronizing the entire dequeue & process task.\nAt that point we are effectively testing the single threaded consumer case which I already added the test case for.", "author": "mattixtech", "createdAt": "2020-01-16T22:46:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzExNjgwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY5NjA2NQ==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r367696065", "bodyText": "yeah, the dequeue method is not synchronized. That's fine then.", "author": "cgorantla", "createdAt": "2020-01-16T23:08:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzExNjgwMw=="}], "type": "inlineReview"}, {"oid": "3d612385a83e66958c518f69cb1e2f98af480271", "url": "https://github.com/OpenNMS/opennms/commit/3d612385a83e66958c518f69cb1e2f98af480271", "message": "Initial version of testms generated by JHipster-6.6.0 with blueprints: kotlin-1.4.0", "committedDate": "2020-01-17T18:50:25Z", "type": "forcePushed"}, {"oid": "d13a046b32568f268530975b096936ee35afe84d", "url": "https://github.com/OpenNMS/opennms/commit/d13a046b32568f268530975b096936ee35afe84d", "message": "Additional test case to test production/consumption in parallel on separate threads", "committedDate": "2020-01-17T20:39:22Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE3MTMwMw==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r368171303", "bodyText": "Consider using asyncPolicy.getQueueSize() as the default inMemoryQueueSize for the specific module. We can extend AsyncPolicy to set offHeapSize then everything is associated with module", "author": "cgorantla", "createdAt": "2020-01-17T23:01:53Z", "path": "core/ipc/sink/common/src/main/java/org/opennms/core/ipc/sink/common/AsyncDispatcherImpl.java", "diffHunk": "@@ -67,226 +65,162 @@\n \n     private static final Logger LOG = LoggerFactory.getLogger(AsyncDispatcherImpl.class);\n     private final SyncDispatcher<S> syncDispatcher;\n-    private OffHeapAdapter offHeapAdapter;\n-    private ExecutorService offHeapAdapterExecutor = Executors.newSingleThreadExecutor();\n     private final AsyncPolicy asyncPolicy;\n-    private OffHeapQueue offHeapQueue;\n-    private SinkModule<S,T> sinkModule;\n-    private DispatcherState<W,S,T> state;\n-    private boolean useOffHeap = false;\n+    private final Counter droppedCounter;\n     \n-    final RateLimitedLog rateLimittedLogger = RateLimitedLog\n+    private final DispatchQueue<S> dispatchQueue;\n+    private final Map<String, CompletableFuture<DispatchStatus>> futureMap = new ConcurrentHashMap<>();\n+    private final AtomicInteger activeDispatchers = new AtomicInteger(0);\n+    \n+    private final RateLimitedLog RATE_LIMITED_LOGGER = RateLimitedLog\n             .withRateLimit(LOG)\n             .maxRate(5).every(Duration.standardSeconds(30))\n             .build();\n \n-    final LinkedBlockingQueue<Runnable> queue;\n-    final ExecutorService executor;\n+    private final ExecutorService executor;\n \n     public AsyncDispatcherImpl(DispatcherState<W, S, T> state, AsyncPolicy asyncPolicy,\n-            SyncDispatcher<S> syncDispatcher) {\n+                               SyncDispatcher<S> syncDispatcher) {\n         Objects.requireNonNull(state);\n         Objects.requireNonNull(asyncPolicy);\n-        this.syncDispatcher = Objects.requireNonNull(syncDispatcher);\n+        Objects.requireNonNull(syncDispatcher);\n+        this.syncDispatcher = syncDispatcher;\n         this.asyncPolicy = asyncPolicy;\n-        this.state = state;\n-        sinkModule = state.getModule();\n-        if (OffHeapServiceLoader.isOffHeapEnabled()) {\n-            offHeapQueue = OffHeapServiceLoader.getOffHeapQueue();\n-            if (offHeapQueue != null) {\n-                useOffHeap = true;\n-                LOG.info(\"Offheap storage enabled for sink module, {}\", sinkModule.getId());\n-            }\n-        }\n-        \n-        final RejectedExecutionHandler rejectedExecutionHandler;\n-        if (asyncPolicy.isBlockWhenFull()) {\n-            // This queue ensures that calling thread is blocked when the queue is full\n-            // See the implementation of OfferBlockingQueue for details\n-            queue = new OfferBlockingQueue<>(asyncPolicy.getQueueSize());\n-            rejectedExecutionHandler = new ThreadPoolExecutor.AbortPolicy();\n+        SinkModule<S, T> sinkModule = state.getModule();\n+        Optional<DispatchQueueFactory> factory = DispatchQueueServiceLoader.getDispatchQueueFactory();\n+\n+        if (factory.isPresent()) {\n+            LOG.debug(\"Using queue from factory\");\n+            dispatchQueue = factory.get().getQueue(asyncPolicy, sinkModule.getId(),\n+                    sinkModule::marshalSingleMessage, sinkModule::unmarshalSingleMessage);\n         } else {\n-            queue = new LinkedBlockingQueue<Runnable>(asyncPolicy.getQueueSize());", "originalCommit": "d13a046b32568f268530975b096936ee35afe84d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODYwNjE0Mw==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r368606143", "bodyText": "It should be easy to re-use the queue size value but what about the batch size and max file size? Do they belong in the XML configuration for the module? They are implementation specific details for this queueing strategy. Do you think it makes sense to move those into the per-module XML as well?\nAnother thing to think about... the original meaning of the queue size setting is not really applicable in the case of off-heap storage. For instance if the queue gets filled up it doesn't mean we are going to start rejecting or blocking, we will just be going off-heap. This was the original reasoning why I didn't re-use the existing config.", "author": "mattixtech", "createdAt": "2020-01-20T15:29:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE3MTMwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc3NjkwMQ==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r368776901", "bodyText": "Yeah, I was thinking that it makes sense to have module specific configuration for off-heap as well so that we can change off-heap size based on needs of module.\nBut looking at configuration more closely, it may be confusing to add batchSize and queueSize in module configuration as they are also used for aggregation.  Let's keep them separate for now.", "author": "cgorantla", "createdAt": "2020-01-21T01:32:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE3MTMwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTEyMTE1Ng==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r369121156", "bodyText": "Ok thanks for the feedback. I think ultimately what we would want is an aggregate max file size instead of a per-module file size so the user could choose to use a max of XGB across all modules. To keep it simple I just went with the fixed file size  per module for now.", "author": "mattixtech", "createdAt": "2020-01-21T16:53:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE3MTMwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE3MTY2NA==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r368171664", "bodyText": "I think the exception should be  Could not delete", "author": "cgorantla", "createdAt": "2020-01-17T23:03:28Z", "path": "core/ipc/sink/off-heap/src/main/java/org/opennms/core/ipc/sink/offheap/QueueFileOffHeapDispatchQueue.java", "diffHunk": "@@ -0,0 +1,560 @@\n+/*******************************************************************************\n+ * This file is part of OpenNMS(R).\n+ *\n+ * Copyright (C) 2020 The OpenNMS Group, Inc.\n+ * OpenNMS(R) is Copyright (C) 1999-2020 The OpenNMS Group, Inc.\n+ *\n+ * OpenNMS(R) is a registered trademark of The OpenNMS Group, Inc.\n+ *\n+ * OpenNMS(R) is free software: you can redistribute it and/or modify\n+ * it under the terms of the GNU Affero General Public License as published\n+ * by the Free Software Foundation, either version 3 of the License,\n+ * or (at your option) any later version.\n+ *\n+ * OpenNMS(R) is distributed in the hope that it will be useful,\n+ * but WITHOUT ANY WARRANTY; without even the implied warranty of\n+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+ * GNU Affero General Public License for more details.\n+ *\n+ * You should have received a copy of the GNU Affero General Public License\n+ * along with OpenNMS(R).  If not, see:\n+ *      http://www.gnu.org/licenses/\n+ *\n+ * For more information contact:\n+ *     OpenNMS(R) Licensing <license@opennms.org>\n+ *     http://www.opennms.org/\n+ *     http://www.opennms.com/\n+ *******************************************************************************/\n+\n+package org.opennms.core.ipc.sink.offheap;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.io.Serializable;\n+import java.lang.reflect.InvocationTargetException;\n+import java.lang.reflect.Method;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ArrayBlockingQueue;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ForkJoinPool;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import org.joda.time.Duration;\n+import org.opennms.core.ipc.sink.api.DispatchQueue;\n+import org.opennms.core.ipc.sink.api.WriteFailedException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.squareup.tape2.QueueFile;\n+import com.swrve.ratelimitedlogger.RateLimitedLog;\n+\n+/**\n+ * A {@link DispatchQueue} that first attempts to queue items in memory and upon overflowing the allocated in-memory\n+ * queue writes items \"off heap\" directly to disk via a file. The in-memory queue is volatile and if the process\n+ * crashes its contents are lost. The contents written to disk are durable and in the event of a crash will be reloaded.\n+ * <p>\n+ * This queue can be configured to only queue to memory by specifying the maximum off-heap size of 0. Using this\n+ * configuration causes {@link #enqueue} to block when the in-memory queue fills up rather than writing to the off-heap\n+ * queue.\n+ * <p>\n+ * Before queued items are written to disk they are first accumulated in a batch to limit the number of discrete writes\n+ * we make to disk. The batched items are considered part of the in-memory portion of the queue and are also volatile.\n+ *\n+ * @param <T> the type being queued\n+ */\n+public class QueueFileOffHeapDispatchQueue<T> implements DispatchQueue<T> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(QueueFileOffHeapDispatchQueue.class);\n+    private final RateLimitedLog RATE_LIMITED_LOGGER = RateLimitedLog\n+            .withRateLimit(LOG)\n+            .maxRate(5)\n+            .every(Duration.standardSeconds(30))\n+            .build();\n+\n+    // This must match the size of the HEADER_LENGTH in QueueFile's Element class\n+    //\n+    // We could pull this out of that class reflectively but it seemed easier to just hard code it here\n+    private static final int SERIALIZED_OBJECT_HEADER_SIZE_IN_BYTES = 4;\n+    private static final String FILE_EXTENSION = \".fifo\";\n+\n+    private final Function<T, byte[]> serializer;\n+    private final Function<byte[], T> deserializer;\n+    private final String moduleName;\n+    private final BlockingQueue<Map.Entry<String, T>> inMemoryQueue;\n+\n+    // Note the queue is not thread safe so access should be synchronized\n+    private final QueueFile offHeapQueue;\n+    private final long maxFileSizeInBytes;\n+    private final int batchSize;\n+    private final Batch batch;\n+\n+    private final ForkJoinPool serdesPool = new ForkJoinPool(\n+            Math.max(Runtime.getRuntime().availableProcessors() - 1, 1));\n+\n+    // Used to guard access to the offHeapQueue and batch data structures\n+    private final Lock offHeapLock = new ReentrantLock(true);\n+    // Used to ensure only one thread can be enqueing at a time\n+    private final Lock enqueueLock = new ReentrantLock(true);\n+    private final FileCapacityLatch fileCapacityLatch = new FileCapacityLatch();\n+\n+    private final Method usedBytesMethod;\n+\n+    public QueueFileOffHeapDispatchQueue(Function<T, byte[]> serializer, Function<byte[], T> deserializer,\n+                                         String moduleName, Path filePath, int inMemoryQueueSize, int batchSize,\n+                                         long maxFileSizeInBytes) throws IOException {\n+        Objects.requireNonNull(serializer);\n+        Objects.requireNonNull(deserializer);\n+        Objects.requireNonNull(moduleName);\n+\n+        if (inMemoryQueueSize < 1) {\n+            throw new IllegalArgumentException(\"In memory queue size must be greater than 0\");\n+        }\n+\n+        if (inMemoryQueueSize % batchSize != 0) {\n+            throw new IllegalArgumentException(\"In memory queue size must be a multiple of batch size\");\n+        }\n+\n+        if (maxFileSizeInBytes < 0) {\n+            throw new IllegalArgumentException(\"Max file size must be either 0 or a positive integer\");\n+        }\n+\n+        this.serializer = serializer;\n+        this.deserializer = deserializer;\n+        this.moduleName = moduleName;\n+        this.batchSize = batchSize;\n+        this.maxFileSizeInBytes = maxFileSizeInBytes;\n+        batch = new Batch(batchSize);\n+\n+        inMemoryQueue = new ArrayBlockingQueue<>(inMemoryQueueSize, true);\n+\n+        // Setting the max file size to 0 or less will disable the off-heap portion of this queue\n+        if (maxFileSizeInBytes > 0) {\n+            Objects.requireNonNull(filePath);\n+            File file = Paths.get(filePath.toString(), moduleName + FILE_EXTENSION).toFile();\n+\n+            QueueFile qf;\n+            try {\n+                qf = new QueueFile.Builder(file).build();\n+            } catch (Exception e) {\n+                LOG.warn(\"Exception while loading queue file\", e);\n+\n+                if (file.delete()) {\n+                    qf = new QueueFile.Builder(file).build();\n+                } else {\n+                    throw new IOException(\"Could delete corrupted queue file \" + file.getAbsolutePath());", "originalCommit": "d13a046b32568f268530975b096936ee35afe84d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODYwMjg5Nw==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r368602897", "bodyText": "Yep it should be.", "author": "mattixtech", "createdAt": "2020-01-20T15:23:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE3MTY2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM2OTc5OA==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r370369798", "bodyText": "I think you have to do null check on the reference before passing it to  context.getService.  In the previous case, it used to check this only when off-heap feature is enabled config is present.", "author": "cgorantla", "createdAt": "2020-01-23T21:41:20Z", "path": "core/ipc/sink/common/src/main/java/org/opennms/core/ipc/sink/offheap/DispatchQueueServiceLoader.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*******************************************************************************\n+ * This file is part of OpenNMS(R).\n+ *\n+ * Copyright (C) 2018 The OpenNMS Group, Inc.\n+ * OpenNMS(R) is Copyright (C) 1999-2018 The OpenNMS Group, Inc.\n+ *\n+ * OpenNMS(R) is a registered trademark of The OpenNMS Group, Inc.\n+ *\n+ * OpenNMS(R) is free software: you can redistribute it and/or modify\n+ * it under the terms of the GNU Affero General Public License as published\n+ * by the Free Software Foundation, either version 3 of the License,\n+ * or (at your option) any later version.\n+ *\n+ * OpenNMS(R) is distributed in the hope that it will be useful,\n+ * but WITHOUT ANY WARRANTY; without even the implied warranty of\n+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+ * GNU Affero General Public License for more details.\n+ *\n+ * You should have received a copy of the GNU Affero General Public License\n+ * along with OpenNMS(R).  If not, see:\n+ *      http://www.gnu.org/licenses/\n+ *\n+ * For more information contact:\n+ *     OpenNMS(R) Licensing <license@opennms.org>\n+ *     http://www.opennms.org/\n+ *     http://www.opennms.com/\n+ *******************************************************************************/\n+\n+package org.opennms.core.ipc.sink.offheap;\n+\n+import java.util.Optional;\n+\n+import org.opennms.core.ipc.sink.api.DispatchQueueFactory;\n+import org.osgi.framework.BundleContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+\n+public class DispatchQueueServiceLoader {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(DispatchQueueServiceLoader.class);\n+    private static BundleContext context;\n+    private static volatile DispatchQueueFactory dispatchQueueFactory;\n+\n+    public BundleContext getBundleContext() {\n+        return context;\n+    }\n+\n+    public static void setBundleContext(BundleContext bundleContext) {\n+        context = bundleContext;\n+    }\n+\n+    public static Optional<DispatchQueueFactory> getDispatchQueueFactory() {\n+        if (dispatchQueueFactory != null) {\n+            return Optional.of(dispatchQueueFactory);\n+        }\n+\n+        if (context != null) {\n+            try {\n+                dispatchQueueFactory = context.getService(context.getServiceReference(DispatchQueueFactory.class));", "originalCommit": "0a582a9767ba891d972843347f45d4c9ef7863ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM3MDMxMQ==", "url": "https://github.com/OpenNMS/opennms/pull/2868#discussion_r370370311", "bodyText": "Yeah you are right. I'll add that too. I think it is still preferably to install the feature on sentinel additionally so we use the same impl.", "author": "mattixtech", "createdAt": "2020-01-23T21:42:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM2OTc5OA=="}], "type": "inlineReview"}, {"oid": "7bb6413c20dcccf7874c9cd480a2441c76953747", "url": "https://github.com/OpenNMS/opennms/commit/7bb6413c20dcccf7874c9cd480a2441c76953747", "message": "Sink API: Persistent Off-Heap Storage\n\nAdd off-heap storage implementation using a fifo file.", "committedDate": "2020-01-23T22:02:36Z", "type": "commit"}, {"oid": "960cacd1c9e9d8299b8c2bb40122fb928b826f3a", "url": "https://github.com/OpenNMS/opennms/commit/960cacd1c9e9d8299b8c2bb40122fb928b826f3a", "message": "Review feedback and update test to wait for threads", "committedDate": "2020-01-23T22:02:43Z", "type": "commit"}, {"oid": "4adfbca387f9b2889ea20567a85755183e5c8881", "url": "https://github.com/OpenNMS/opennms/commit/4adfbca387f9b2889ea20567a85755183e5c8881", "message": "Update more log messages", "committedDate": "2020-01-23T22:02:43Z", "type": "commit"}, {"oid": "adf1f7f145d8c701f616eb2c4e52e04e3d893ef0", "url": "https://github.com/OpenNMS/opennms/commit/adf1f7f145d8c701f616eb2c4e52e04e3d893ef0", "message": "Update copyrights", "committedDate": "2020-01-23T22:02:45Z", "type": "commit"}, {"oid": "7f11e6a5edc0a6985518c30efbbdf241e5f7ee7d", "url": "https://github.com/OpenNMS/opennms/commit/7f11e6a5edc0a6985518c30efbbdf241e5f7ee7d", "message": "Review feedback 2", "committedDate": "2020-01-23T22:02:46Z", "type": "commit"}, {"oid": "fd34b2b11685926881be8563d95d181b62e35667", "url": "https://github.com/OpenNMS/opennms/commit/fd34b2b11685926881be8563d95d181b62e35667", "message": "More review feedback", "committedDate": "2020-01-23T22:02:47Z", "type": "commit"}, {"oid": "8b0e1bbd16a27062871af00bd7f30424ff7b16d2", "url": "https://github.com/OpenNMS/opennms/commit/8b0e1bbd16a27062871af00bd7f30424ff7b16d2", "message": "Additional test case to test production/consumption in parallel on separate threads", "committedDate": "2020-01-23T22:02:49Z", "type": "commit"}, {"oid": "3b3dd20bc6fa2cc61682ee375228be42f5cacb7a", "url": "https://github.com/OpenNMS/opennms/commit/3b3dd20bc6fa2cc61682ee375228be42f5cacb7a", "message": "Fix exception message", "committedDate": "2020-01-23T22:02:55Z", "type": "commit"}, {"oid": "9dcc257c5e729334ea4f2ae661b8fce02ce202bd", "url": "https://github.com/OpenNMS/opennms/commit/9dcc257c5e729334ea4f2ae661b8fce02ce202bd", "message": "Install the feature on sentinel too...", "committedDate": "2020-01-23T22:02:56Z", "type": "commit"}, {"oid": "029acc34322865fee996929e9239c1d8a3207161", "url": "https://github.com/OpenNMS/opennms/commit/029acc34322865fee996929e9239c1d8a3207161", "message": "Add null checking for service ref and fix fair locking in queue impl", "committedDate": "2020-01-23T22:02:58Z", "type": "commit"}, {"oid": "029acc34322865fee996929e9239c1d8a3207161", "url": "https://github.com/OpenNMS/opennms/commit/029acc34322865fee996929e9239c1d8a3207161", "message": "Add null checking for service ref and fix fair locking in queue impl", "committedDate": "2020-01-23T22:02:58Z", "type": "forcePushed"}]}