{"pr_number": 5329, "pr_title": "GEODE-8029: Allow OplogEntryIdSet to Overflow", "pr_createdAt": "2020-06-30T09:58:32Z", "pr_url": "https://github.com/apache/geode/pull/5329", "timeline": [{"oid": "3a59e4f8bca536610428134b82d0816a11bcee0f", "url": "https://github.com/apache/geode/commit/3a59e4f8bca536610428134b82d0816a11bcee0f", "message": "GEODE-8029: Allow OplogEntryIdSet to Overflow\n\nDo not delete drf files during member startup as that should be only\ndone by the compactor thread. Instead, allow the OplogEntryIdSet to\ngrow over the default capacity and log a warning message instructing\nthe user to manually compact the disk-stores.\n\n- Added unit tests.\n- Replaced usages of 'junit.Assert' by 'assertj'.\n- Modified DiskStoreImpl.deadRecordCount to return long instead of int.\n- Added internal overflow implementation to the OplogEntryIdSet so it can\n  grow above the default limit.", "committedDate": "2020-06-30T09:50:43Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUxODg2OQ==", "url": "https://github.com/apache/geode/pull/5329#discussion_r448518869", "bodyText": "change \"my\" to \"may\"", "author": "dschneider-pivotal", "createdAt": "2020-07-01T17:40:06Z", "path": "geode-core/src/main/java/org/apache/geode/internal/cache/Oplog.java", "diffHunk": "@@ -937,17 +937,11 @@ void initAfterRecovery(boolean offline) {\n         // this.crf.raf.seek(this.crf.currSize);\n       } else if (!offline) {\n         // drf exists but crf has been deleted (because it was empty).\n+        // I don't think the drf needs to be opened. It is only used during\n+        // recovery.\n+        // At some point the compacter my identify that it can be deleted.", "originalCommit": "3a59e4f8bca536610428134b82d0816a11bcee0f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODg5ODA2OQ==", "url": "https://github.com/apache/geode/pull/5329#discussion_r448898069", "bodyText": "Thanks @dschneider-pivotal!. I'll fix the type and create another ticket for the long term solution.", "author": "jujoramos", "createdAt": "2020-07-02T10:14:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUxODg2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU2MDM3MQ==", "url": "https://github.com/apache/geode/pull/5329#discussion_r448560371", "bodyText": "\"Too many entries\", this is referring to deleted entries, right? In that case how about changing it\nto \"large number of deleted entries\"?", "author": "agingade", "createdAt": "2020-07-01T19:03:16Z", "path": "geode-core/src/main/java/org/apache/geode/internal/cache/DiskStoreImpl.java", "diffHunk": "@@ -3517,33 +3518,86 @@ public String toString() {\n   }\n \n   /**\n-   * Set of OplogEntryIds (longs). Memory is optimized by using an int[] for ids in the unsigned int\n-   * range.\n+   * Set of OplogEntryIds (longs).\n+   * Memory is optimized by using an int[] for ids in the unsigned int range.\n+   * By default we can't have more than 805306401 ids for a load factor of 0.75, the internal lists\n+   * are used to overcome this limit, allowing the disk-store to recover successfully (the internal\n+   * class is **only** used during recovery to read all deleted entries).\n    */\n   static class OplogEntryIdSet {\n-    private final IntOpenHashSet ints = new IntOpenHashSet((int) INVALID_ID);\n-    private final LongOpenHashSet longs = new LongOpenHashSet((int) INVALID_ID);\n+    private final List<IntOpenHashSet> allInts;\n+    private final List<LongOpenHashSet> allLongs;\n+    private final AtomicReference<IntOpenHashSet> currentInts;\n+    private final AtomicReference<LongOpenHashSet> currentLongs;\n+\n+    // For testing purposes only.\n+    @VisibleForTesting\n+    OplogEntryIdSet(List<IntOpenHashSet> allInts, List<LongOpenHashSet> allLongs) {\n+      this.allInts = allInts;\n+      this.currentInts = new AtomicReference<>(this.allInts.get(0));\n+\n+      this.allLongs = allLongs;\n+      this.currentLongs = new AtomicReference<>(this.allLongs.get(0));\n+    }\n+\n+    public OplogEntryIdSet() {\n+      IntOpenHashSet intHashSet = new IntOpenHashSet((int) INVALID_ID);\n+      this.allInts = new ArrayList<>();\n+      this.allInts.add(intHashSet);\n+      this.currentInts = new AtomicReference<>(intHashSet);\n+\n+      LongOpenHashSet longHashSet = new LongOpenHashSet((int) INVALID_ID);\n+      this.allLongs = new ArrayList<>();\n+      this.allLongs.add(longHashSet);\n+      this.currentLongs = new AtomicReference<>(longHashSet);\n+    }\n \n     public void add(long id) {\n       if (id == 0) {\n         throw new IllegalArgumentException();\n-      } else if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n-        this.ints.add((int) id);\n-      } else {\n-        this.longs.add(id);\n+      }\n+\n+      try {\n+        if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n+          this.currentInts.get().add((int) id);\n+        } else {\n+          this.currentLongs.get().add(id);\n+        }\n+      } catch (IllegalArgumentException illegalArgumentException) {\n+        // See GEODE-8029.\n+        // Too many entries on the accumulated drf files, overflow and continue.\n+        logger.warn(\n+            \"There are too many entries within the disk-store, please execute an offline compaction.\",", "originalCommit": "3a59e4f8bca536610428134b82d0816a11bcee0f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU2MDkwMg==", "url": "https://github.com/apache/geode/pull/5329#discussion_r448560902", "bodyText": "Do we need to pass \"illegalArgumentExcpetion\" to warning message? It may not be useful for end user.", "author": "agingade", "createdAt": "2020-07-01T19:04:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU2MDM3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODg5ODMyMA==", "url": "https://github.com/apache/geode/pull/5329#discussion_r448898320", "bodyText": "\ud83d\udc4d", "author": "jujoramos", "createdAt": "2020-07-02T10:14:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU2MDM3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU3NzcwMg==", "url": "https://github.com/apache/geode/pull/5329#discussion_r448577702", "bodyText": "@dschneider-pivotal\nIs there a scenario where the same ID is added many times? In that case, earlier we had only one id set, and there was no chance of duplicate. Now with the new approach, the same ID could be present in multiple id sets.\nOne of the caller of add is \"offlineCompact()\" its iterating over live entries and calling add...Will there be two live entries with the same id?", "author": "agingade", "createdAt": "2020-07-01T19:40:47Z", "path": "geode-core/src/main/java/org/apache/geode/internal/cache/DiskStoreImpl.java", "diffHunk": "@@ -3517,33 +3518,86 @@ public String toString() {\n   }\n \n   /**\n-   * Set of OplogEntryIds (longs). Memory is optimized by using an int[] for ids in the unsigned int\n-   * range.\n+   * Set of OplogEntryIds (longs).\n+   * Memory is optimized by using an int[] for ids in the unsigned int range.\n+   * By default we can't have more than 805306401 ids for a load factor of 0.75, the internal lists\n+   * are used to overcome this limit, allowing the disk-store to recover successfully (the internal\n+   * class is **only** used during recovery to read all deleted entries).\n    */\n   static class OplogEntryIdSet {\n-    private final IntOpenHashSet ints = new IntOpenHashSet((int) INVALID_ID);\n-    private final LongOpenHashSet longs = new LongOpenHashSet((int) INVALID_ID);\n+    private final List<IntOpenHashSet> allInts;\n+    private final List<LongOpenHashSet> allLongs;\n+    private final AtomicReference<IntOpenHashSet> currentInts;\n+    private final AtomicReference<LongOpenHashSet> currentLongs;\n+\n+    // For testing purposes only.\n+    @VisibleForTesting\n+    OplogEntryIdSet(List<IntOpenHashSet> allInts, List<LongOpenHashSet> allLongs) {\n+      this.allInts = allInts;\n+      this.currentInts = new AtomicReference<>(this.allInts.get(0));\n+\n+      this.allLongs = allLongs;\n+      this.currentLongs = new AtomicReference<>(this.allLongs.get(0));\n+    }\n+\n+    public OplogEntryIdSet() {\n+      IntOpenHashSet intHashSet = new IntOpenHashSet((int) INVALID_ID);\n+      this.allInts = new ArrayList<>();\n+      this.allInts.add(intHashSet);\n+      this.currentInts = new AtomicReference<>(intHashSet);\n+\n+      LongOpenHashSet longHashSet = new LongOpenHashSet((int) INVALID_ID);\n+      this.allLongs = new ArrayList<>();\n+      this.allLongs.add(longHashSet);\n+      this.currentLongs = new AtomicReference<>(longHashSet);\n+    }\n \n     public void add(long id) {\n       if (id == 0) {\n         throw new IllegalArgumentException();\n-      } else if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n-        this.ints.add((int) id);\n-      } else {\n-        this.longs.add(id);\n+      }\n+\n+      try {\n+        if (id > 0 && id <= 0x00000000FFFFFFFFL) {\n+          this.currentInts.get().add((int) id);\n+        } else {\n+          this.currentLongs.get().add(id);\n+        }\n+      } catch (IllegalArgumentException illegalArgumentException) {\n+        // See GEODE-8029.\n+        // Too many entries on the accumulated drf files, overflow and continue.\n+        logger.warn(\n+            \"There are too many entries within the disk-store, please execute an offline compaction.\",\n+            illegalArgumentException);\n+\n+        // Overflow to the next [Int|Long]OpenHashSet and continue.\n+        if (id > 0 && id <= 0x00000000FFFFFFFFL) {", "originalCommit": "3a59e4f8bca536610428134b82d0816a11bcee0f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODYxNTIzOQ==", "url": "https://github.com/apache/geode/pull/5329#discussion_r448615239", "bodyText": "I don't think so. The ids are unique and should only be deleted once from a drf. If this did happen it would just cause the size to be larger than it should be. So you could review the code that uses the size of this class.\nYou could prevent this from having each add first call contains which shouldn't slow down the add too much.", "author": "dschneider-pivotal", "createdAt": "2020-07-01T21:04:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU3NzcwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODkwNzYxMQ==", "url": "https://github.com/apache/geode/pull/5329#discussion_r448907611", "bodyText": "@agingade: I think we're good on this regard, ids are unique and the actual size of the OplogEntryIdSet is only used by offline-validate / offline-compact commands. That is, if we ever report a bigger number for deadRecordCount (set through OplogEntryIdSet.size()) because there are duplicated ids (shouldn't happen), the user would know that the disk-store should be compacted anyways.\nThe real problem would happen if we ever report there are not compact-able entries when there actually are, which is not the case here.", "author": "jujoramos", "createdAt": "2020-07-02T10:33:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU3NzcwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE3ODcyMg==", "url": "https://github.com/apache/geode/pull/5329#discussion_r449178722", "bodyText": "Thanks for the clarification.", "author": "agingade", "createdAt": "2020-07-02T17:49:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU3NzcwMg=="}], "type": "inlineReview"}, {"oid": "604346c7c89d2dccbf88d2cc65343818912de35e", "url": "https://github.com/apache/geode/commit/604346c7c89d2dccbf88d2cc65343818912de35e", "message": "- Changes requested by reviewers.", "committedDate": "2020-07-02T10:37:02Z", "type": "commit"}]}