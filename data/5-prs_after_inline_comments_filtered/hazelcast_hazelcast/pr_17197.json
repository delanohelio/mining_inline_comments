{"pr_number": 17197, "pr_title": "Parallelize partition replica migrations", "pr_createdAt": "2020-07-09T13:11:09Z", "pr_url": "https://github.com/hazelcast/hazelcast/pull/17197", "timeline": [{"oid": "624690d24ebe3be55010eb44849b7bbee3c92a98", "url": "https://github.com/hazelcast/hazelcast/commit/624690d24ebe3be55010eb44849b7bbee3c92a98", "message": "Parallelize partition replica migrations\n\nWith thousands of partitions and/or with large amount of data (a few TB),\nrebalancing partitions after adding or removing a member takes\nsignificant amount of time (up to hours).\n\nBy parallelizing migrations, it's possible to reduce this time\nto a few mins by utilizing network bandwidth. That way cluster will\nstabilize in a shorter time. But there's a trade-off; using\nmost of the network bandwidth for migration data may increase\nthe latency of concurrent read/write operations.\n\nIn summary:\n\n- Replaced the global partition table version with per-partition\nversions.\n- Parallelized replica migrations of independent partitions.", "committedDate": "2020-07-09T13:58:06Z", "type": "forcePushed"}, {"oid": "91ac03c34f4ae278bbcc7590be12a829d16c5802", "url": "https://github.com/hazelcast/hazelcast/commit/91ac03c34f4ae278bbcc7590be12a829d16c5802", "message": "Parallelize partition replica migrations\n\nWith thousands of partitions and/or with large amount of data (a few TB),\nrebalancing partitions after adding or removing a member takes\nsignificant amount of time (up to hours).\n\nBy parallelizing migrations, it's possible to reduce this time\nto a few mins by utilizing network bandwidth. That way cluster will\nstabilize in a shorter time. But there's a trade-off; using\nmost of the network bandwidth for migration data may increase\nthe latency of concurrent read/write operations.\n\nIn summary:\n\n- Replaced the global partition table version with per-partition\nversions.\n- Parallelized replica migrations of independent partitions.", "committedDate": "2020-08-10T11:45:24Z", "type": "forcePushed"}, {"oid": "8a23b6f1898d33bc6c27468e52f7b3d8e78e9e91", "url": "https://github.com/hazelcast/hazelcast/commit/8a23b6f1898d33bc6c27468e52f7b3d8e78e9e91", "message": "Parallelize partition replica migrations\n\nWith thousands of partitions and/or with large amount of data (a few TB),\nrebalancing partitions after adding or removing a member takes\nsignificant amount of time (up to hours).\n\nBy parallelizing migrations, it's possible to reduce this time\nto a few mins by utilizing network bandwidth. That way cluster will\nstabilize in a shorter time. But there's a trade-off; using\nmost of the network bandwidth for migration data may increase\nthe latency of concurrent read/write operations.\n\nIn summary:\n\n- Replaced the global partition table version with per-partition\nversions.\n- Parallelized replica migrations of independent partitions.", "committedDate": "2020-08-11T08:46:25Z", "type": "forcePushed"}, {"oid": "fa223ac0d6c487e16940496f7173d5c35546ee9a", "url": "https://github.com/hazelcast/hazelcast/commit/fa223ac0d6c487e16940496f7173d5c35546ee9a", "message": "Parallelize partition replica migrations\n\nWith thousands of partitions and/or with large amount of data (a few TB),\nrebalancing partitions after adding or removing a member takes\nsignificant amount of time (up to hours).\n\nBy parallelizing migrations, it's possible to reduce this time\nto a few mins by utilizing network bandwidth. That way cluster will\nstabilize in a shorter time. But there's a trade-off; using\nmost of the network bandwidth for migration data may increase\nthe latency of concurrent read/write operations.\n\nIn summary:\n\n- Replaced the global partition table version with per-partition\nversions.\n- Parallelized replica migrations of independent partitions.", "committedDate": "2020-08-11T10:04:09Z", "type": "forcePushed"}, {"oid": "cd95e5aced863bda0595e3320769ddb7691d726c", "url": "https://github.com/hazelcast/hazelcast/commit/cd95e5aced863bda0595e3320769ddb7691d726c", "message": "Parallelize partition replica migrations\n\nWith thousands of partitions and/or with large amount of data (a few TB),\nrebalancing partitions after adding or removing a member takes\nsignificant amount of time (up to hours).\n\nBy parallelizing migrations, it's possible to reduce this time\nto a few mins by utilizing network bandwidth. That way cluster will\nstabilize in a shorter time. But there's a trade-off; using\nmost of the network bandwidth for migration data may increase\nthe latency of concurrent read/write operations.\n\nIn summary:\n\n- Replaced the global partition table version with per-partition\nversions.\n- Parallelized replica migrations of independent partitions.", "committedDate": "2020-08-19T09:21:45Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyNTYyMQ==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472925621", "bodyText": "it can be better to document when this partitionTableVersion can be deleted from code.", "author": "ahmetmircik", "createdAt": "2020-08-19T10:26:09Z", "path": "hazelcast/src/main/java/com/hazelcast/client/impl/ClusterViewListenerService.java", "diffHunk": "@@ -57,6 +60,12 @@\n     private final boolean advancedNetworkConfigEnabled;\n     private final AtomicBoolean pushScheduled = new AtomicBoolean();\n     private final CoalescingDelayedTrigger delayedPartitionUpdateTrigger;\n+    // This is an emulation of the pre-4.1 partition state version.\n+    // We will increment this version if a partition table change is detected\n+    // while sending partition table to the client.\n+    private final AtomicInteger partitionTableVersion = new AtomicInteger();", "originalCommit": "cd95e5aced863bda0595e3320769ddb7691d726c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzODIwNg==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472938206", "bodyText": "Most probably it will stay until the next major version of the client protocol. I'll add this.", "author": "mdogan", "createdAt": "2020-08-19T10:49:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyNTYyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyNjUwMg==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472926502", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            logFailure(connection, partitionStateVersion, current, \"response state version is old\");\n          \n          \n            \n                            logFailure(connection, partitionStateVersion, current, \"response partition state version is old\");", "author": "ahmetmircik", "createdAt": "2020-08-19T10:27:46Z", "path": "hazelcast/src/main/java/com/hazelcast/client/impl/spi/impl/ClientPartitionServiceImpl.java", "diffHunk": "@@ -106,10 +106,9 @@ private boolean shouldBeApplied(Connection connection, Collection<Map.Entry<UUID\n             }\n             return true;\n         }\n-        if (partitionStateVersion <= current.partitionSateVersion) {\n+        if (partitionStateVersion == current.partitionSateVersion) {\n             if (logger.isFinestEnabled()) {\n-                logFailure(connection, partitionStateVersion, current,\n-                        \"response state version is old\");\n+                logFailure(connection, partitionStateVersion, current, \"response state version is old\");", "originalCommit": "cd95e5aced863bda0595e3320769ddb7691d726c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyNzgwMw==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472927803", "bodyText": "can't we remove partitionStateStamp from code-base and continue to use partitionStateVersion instead? It also helps hot-restart partial restart case.", "author": "ahmetmircik", "createdAt": "2020-08-19T10:30:09Z", "path": "hazelcast/src/main/java/com/hazelcast/internal/cluster/impl/ClusterStateManager.java", "diffHunk": "@@ -271,17 +272,22 @@ private void validateClusterVersionChange(Version newClusterVersion) {\n         }\n     }\n \n-    private void checkMigrationsAndPartitionStateVersion(ClusterStateChange stateChange, int partitionStateVersion) {\n-        final InternalPartitionService partitionService = node.getPartitionService();\n-        final int thisPartitionStateVersion = partitionService.getPartitionStateVersion();\n+    private void checkMigrationsAndPartitionStateStamp(ClusterStateChange stateChange, long partitionStateStamp) {\n+        InternalPartitionService partitionService = node.getPartitionService();\n+        long thisPartitionStateStamp;\n+        if (clusterVersion.isGreaterOrEqual(Versions.V4_1)) {\n+            thisPartitionStateStamp = partitionService.getPartitionStateStamp();\n+        } else {\n+            thisPartitionStateStamp = partitionService.getPartitionStateVersion();", "originalCommit": "cd95e5aced863bda0595e3320769ddb7691d726c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjk0MzMxMw==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472943313", "bodyText": "Problem is partitionStateVersion indicates a monotonic order between different partition states. But with the new mechanism, there's no such an order, members may receive partition updates in different order but still can have the same version. partitionStateStamp tries to solve this problem.", "author": "mdogan", "createdAt": "2020-08-19T10:59:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyNzgwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyOTk3NQ==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472929975", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                boolean isPartitionStateInitialized();\n          \n          \n            \n                boolean isPartitionTableInitialized();", "author": "ahmetmircik", "createdAt": "2020-08-19T10:34:08Z", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/InternalPartitionService.java", "diffHunk": "@@ -79,6 +79,11 @@\n      */\n     void memberRemoved(Member member);\n \n+    /**\n+     * Returns whether partition table is initialized or not.\n+     */\n+    boolean isPartitionStateInitialized();", "originalCommit": "cd95e5aced863bda0595e3320769ddb7691d726c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjk0MzU4OA==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472943588", "bodyText": "I'll remove this method, as it's not used apart from a test.", "author": "mdogan", "createdAt": "2020-08-19T11:00:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyOTk3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzMDY1OA==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472930658", "bodyText": "can we have a short javaDoc for what is encodedPartitionTable?", "author": "ahmetmircik", "createdAt": "2020-08-19T10:35:22Z", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/PartitionRuntimeState.java", "diffHunk": "@@ -34,27 +38,42 @@\n import static com.hazelcast.internal.serialization.impl.SerializationUtil.writeNullableCollection;\n import static com.hazelcast.internal.util.StringUtil.LINE_SEPARATOR;\n \n-public final class PartitionRuntimeState implements IdentifiedDataSerializable {\n+public final class PartitionRuntimeState implements IdentifiedDataSerializable, Versioned {\n \n-    private PartitionReplica[] replicas;\n-    private int[][] minimizedPartitionTable;\n+    private PartitionReplica[] allReplicas;\n+    private int[][] encodedPartitionTable;", "originalCommit": "cd95e5aced863bda0595e3320769ddb7691d726c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzNDk4OA==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472934988", "bodyText": "are these commented lines leftover?", "author": "ahmetmircik", "createdAt": "2020-08-19T10:43:23Z", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/operation/PromotionCommitOperation.java", "diffHunk": "@@ -180,6 +195,46 @@ private CallStatus beforePromotion() {\n         return CallStatus.VOID;\n     }\n \n+    private CallStatus alreadyAppliedAllPromotions() {\n+        getLogger().warning(\"Already applied all promotions to the partition state. Promotion state stamp: \"\n+                + partitionState.getStamp());\n+        InternalPartitionServiceImpl partitionService = getService();\n+        partitionService.getMigrationManager().releasePromotionPermit();\n+        success = true;\n+        return CallStatus.RESPONSE;\n+    }\n+\n+    //RU_COMPAT_4_0\n+    private void filterAlreadyAppliedPromotions() {\n+        if (getNodeEngine().getClusterService().getClusterVersion().isUnknownOrLessOrEqual(Versions.V4_0)) {\n+            return;\n+        }\n+        InternalPartitionServiceImpl partitionService = getService();\n+        PartitionStateManager stateManager = partitionService.getPartitionStateManager();\n+        Iterator<MigrationInfo> iter = promotions.iterator();\n+        while (iter.hasNext()) {\n+            MigrationInfo promotion = iter.next();\n+            InternalPartitionImpl partition = stateManager.getPartitionImpl(promotion.getPartitionId());\n+\n+            if (partition.version() >= promotion.getFinalPartitionVersion()) {\n+                // ? we can't assume this, since partition might be further updated...\n+//                if (promotion.getDestination().equals(partition.getOwnerReplicaOrNull())) {", "originalCommit": "cd95e5aced863bda0595e3320769ddb7691d726c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjk0ODM5Mw==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472948393", "bodyText": "yes \ud83d\ude03, I'll remove them.", "author": "mdogan", "createdAt": "2020-08-19T11:09:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzNDk4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzNTE5Mw==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472935193", "bodyText": "TODO?", "author": "ahmetmircik", "createdAt": "2020-08-19T10:43:47Z", "path": "hazelcast/src/main/java/com/hazelcast/spi/impl/operationservice/impl/Invocation.java", "diffHunk": "@@ -283,6 +283,7 @@ final void initInvocationTarget() throws Exception {\n             if (previousTargetMember != null) {\n                 // If a target member was found earlier but current target member is null\n                 // then it means a member left.\n+                // TODO: not always true ??", "originalCommit": "cd95e5aced863bda0595e3320769ddb7691d726c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjk1MDkwOA==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472950908", "bodyText": "hmm this is a left-over comment from a CP testing, not relevant for this PR. but still the comment is true...", "author": "mdogan", "createdAt": "2020-08-19T11:14:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzNTE5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzNjYwOA==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472936608", "bodyText": "leftover?", "author": "ahmetmircik", "createdAt": "2020-08-19T10:46:33Z", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/impl/PartitionStateManager.java", "diffHunk": "@@ -214,16 +225,20 @@ void setInitialState(PartitionTableView partitionTable) {\n         PartitionReplica localReplica = PartitionReplica.from(node.getLocalMember());\n         for (int partitionId = 0; partitionId < partitionCount; partitionId++) {\n             InternalPartitionImpl partition = partitions[partitionId];\n-            PartitionReplica[] replicas = partitionTable.getReplicas(partitionId);\n-            if (!foundReplica && replicas != null) {\n+            InternalPartition newPartition = partitionTable.getPartition(partitionId);\n+            if (!foundReplica && newPartition != null) {\n                 for (int i = 0; i < InternalPartition.MAX_REPLICA_COUNT; i++) {\n-                    foundReplica |= replicas[i] != null;\n+                    foundReplica |= newPartition.getReplica(i) != null;\n                 }\n             }\n             partition.reset(localReplica);\n-            partition.setInitialReplicas(replicas);\n+//            partition.setInitialReplicas(replicas);", "originalCommit": "cd95e5aced863bda0595e3320769ddb7691d726c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "869cd7f44a82f572cf57e40e6d5e791fd62af919", "url": "https://github.com/hazelcast/hazelcast/commit/869cd7f44a82f572cf57e40e6d5e791fd62af919", "message": "Parallelize partition replica migrations\n\nWith thousands of partitions and/or with large amount of data (a few TB),\nrebalancing partitions after adding or removing a member takes\nsignificant amount of time (up to hours).\n\nBy parallelizing migrations, it's possible to reduce this time\nto a few mins by utilizing network bandwidth. That way cluster will\nstabilize in a shorter time. But there's a trade-off; using\nmost of the network bandwidth for migration data may increase\nthe latency of concurrent read/write operations.\n\nIn summary:\n\n- Replaced the global partition table version with per-partition\nversions.\n- Parallelized replica migrations of independent partitions.", "committedDate": "2020-08-19T11:32:19Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjI5MjY2Mw==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r476292663", "bodyText": "is the equals comparison valid for both RU-compatibility and 4.1-version-stamp modes?", "author": "vbekiaris", "createdAt": "2020-08-25T09:01:08Z", "path": "hazelcast/src/main/java/com/hazelcast/client/impl/spi/impl/ClientPartitionServiceImpl.java", "diffHunk": "@@ -106,10 +106,9 @@ private boolean shouldBeApplied(Connection connection, Collection<Map.Entry<UUID\n             }\n             return true;\n         }\n-        if (partitionStateVersion <= current.partitionSateVersion) {\n+        if (partitionStateVersion == current.partitionSateVersion) {", "originalCommit": "869cd7f44a82f572cf57e40e6d5e791fd62af919", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODIzOTUzOQ==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r478239539", "bodyText": "hmm.. good catch! I think I should revert this change back. I was planning to send partition stamp to the clients but couldn't find a way because of client compatibility requirement.", "author": "mdogan", "createdAt": "2020-08-27T08:16:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjI5MjY2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjMyODA3OA==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r476328078", "bodyText": "can you clarify what \"update on the partition\" means in this context? (maybe I 'm just silly, but it feels like this statement could be misinterpreted as \"data-in-the-partition version\", similar to replica versions we use for anti-entropy)", "author": "vbekiaris", "createdAt": "2020-08-25T09:56:33Z", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/IPartition.java", "diffHunk": "@@ -98,4 +98,13 @@\n      * @return {@code true} if address is owner or backup, {@code false} otherwise\n      */\n     boolean isOwnerOrBackup(Address address);\n+\n+    /**\n+     * Returns the version of the partition.\n+     * Partition version is incremented by one on each update\n+     * on the partition.", "originalCommit": "869cd7f44a82f572cf57e40e6d5e791fd62af919", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI0MTQxNA==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r478241414", "bodyText": "\ud83d\udc4d I'll elaborate more.", "author": "mdogan", "createdAt": "2020-08-27T08:19:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjMyODA3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjM3MjM0MA==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r476372340", "bodyText": "add an // RU_COMPAT comment here so the class can be located & renamed in next major version?", "author": "vbekiaris", "createdAt": "2020-08-25T11:23:10Z", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/PartitionStateVersionMismatchException.java", "diffHunk": "@@ -19,17 +19,19 @@\n import com.hazelcast.core.HazelcastException;\n \n /**\n- * Thrown when local partition-state version doesn't match the version\n+ * Thrown when local partition stamp doesn't match the stamp\n  * of master member while running a migration/replication operation.\n+ * <p>\n+ * PartitionStateVersionMismatchException name is kept to provide rolling upgrade\n+ * guarantees. Otherwise this class would be renamed to PartitionStateStampMismatchException.", "originalCommit": "869cd7f44a82f572cf57e40e6d5e791fd62af919", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjM5MzcyMQ==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r476393721", "bodyText": "Aways returning false from ReadonlyInternalPartition#isLocal() method seems unexpected. If the information is not available, is it probably best to throw an UnsupportedOperationException?", "author": "vbekiaris", "createdAt": "2020-08-25T12:00:49Z", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/ReadonlyInternalPartition.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.internal.partition;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+\n+/**\n+ * Readonly/immutable implementation of {@link InternalPartition} interface.\n+ */\n+public class ReadonlyInternalPartition extends AbstractInternalPartition {\n+\n+    private final PartitionReplica[] replicas;\n+    private final int version;\n+\n+    @SuppressFBWarnings(\"EI_EXPOSE_REP2\")\n+    public ReadonlyInternalPartition(PartitionReplica[] replicas, int partitionId, int version) {\n+        super(partitionId);\n+        this.replicas = replicas;\n+        this.version = version;\n+    }\n+\n+    public ReadonlyInternalPartition(InternalPartition partition) {\n+        super(partition.getPartitionId());\n+        this.replicas = partition.getReplicasCopy();\n+        this.version = partition.version();\n+    }\n+\n+    @Override\n+    public boolean isLocal() {\n+        return false;", "originalCommit": "869cd7f44a82f572cf57e40e6d5e791fd62af919", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzEwODkwMQ==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r477108901", "bodyText": "Could we keep the elapsedMigrationTime & totalElapsedMigrationTime rendered in all cases so it is visible in \"All migration tasks have been completed\" log entry? IMHO it's useful info", "author": "vbekiaris", "createdAt": "2020-08-26T07:59:40Z", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/impl/MigrationStats.java", "diffHunk": "@@ -208,11 +208,10 @@ public String formatToString(boolean detailed) {\n             s.append(\", elapsedMigrationOperationTime=\").append(getElapsedMigrationOperationTime()).append(\"ms\")\n                     .append(\", totalElapsedMigrationOperationTime=\").append(getTotalElapsedMigrationOperationTime()).append(\"ms\")\n                     .append(\", elapsedDestinationCommitTime=\").append(getElapsedDestinationCommitTime()).append(\"ms\")\n-                    .append(\", totalElapsedDestinationCommitTime=\").append(getTotalElapsedDestinationCommitTime()).append(\"ms\");\n+                    .append(\", totalElapsedDestinationCommitTime=\").append(getTotalElapsedDestinationCommitTime()).append(\"ms\")\n+                    .append(\", elapsedMigrationTime=\").append(getElapsedMigrationTime()).append(\"ms\")", "originalCommit": "869cd7f44a82f572cf57e40e6d5e791fd62af919", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI2OTYzMQ==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r478269631", "bodyText": "These numbers show the total elapsed time of the individual migrations. But when migrations are concurrent, they might be misleading. Because they don't show the absolute elapsed time between start and complete. Still I can revert this change, if you think it's useful...", "author": "mdogan", "createdAt": "2020-08-27T09:06:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzEwODkwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI3NjgyNA==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r478276824", "bodyText": "oh right, so with concurrent migrations it will be more like \"cpu time\" rather than wall clock time. Let's leave this only in detailed == true case since it can be confusing.", "author": "vbekiaris", "createdAt": "2020-08-27T09:18:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzEwODkwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE3NDI5Mw==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r477174293", "bodyText": "can we document the special 0 value in getStamp() method javadoc (and maybe define it as a constant?) ?", "author": "vbekiaris", "createdAt": "2020-08-26T09:43:23Z", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/impl/PartitionReplicaStateChecker.java", "diffHunk": "@@ -82,6 +82,10 @@ public PartitionServiceState getPartitionServiceState() {\n             return FETCHING_PARTITION_TABLE;\n         }\n \n+        if (partitionStateManager.getStamp() != 0 && !partitionStateManager.isInitialized()) {", "originalCommit": "869cd7f44a82f572cf57e40e6d5e791fd62af919", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzIzOTA4MQ==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r477239081", "bodyText": "is this change in log level intentional? It looks like it can result in logging migration failures during shutdown at WARNING level.", "author": "vbekiaris", "createdAt": "2020-08-26T11:48:21Z", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/operation/MigrationRequestOperation.java", "diffHunk": "@@ -277,13 +264,7 @@ private void logThrowable(Throwable t) {\n         if (throwableToLog instanceof ExecutionException) {\n             throwableToLog = throwableToLog.getCause() != null ? throwableToLog.getCause() : throwableToLog;\n         }\n-        Level level = getLogLevel(throwableToLog);\n-        getLogger().log(level, throwableToLog.getMessage(), throwableToLog);\n-    }\n-\n-    private Level getLogLevel(Throwable e) {\n-        return (e instanceof MemberLeftException || e instanceof InterruptedException)\n-                || !getNodeEngine().isRunning() ? Level.INFO : Level.WARNING;\n+        getLogger().warning(\"Failure while executing \" + migrationInfo, throwableToLog);", "originalCommit": "869cd7f44a82f572cf57e40e6d5e791fd62af919", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI3NDg3MA==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r478274870", "bodyText": "It was for analyzing a failure. I'll revert back.", "author": "mdogan", "createdAt": "2020-08-27T09:15:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzIzOTA4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzI1NTA3OA==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r477255078", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Maximum number of partition migrations to be executed on a member.\n          \n          \n            \n                 * Maximum number of partition migrations to be executed concurrently on a member.", "author": "vbekiaris", "createdAt": "2020-08-26T12:18:09Z", "path": "hazelcast/src/main/java/com/hazelcast/spi/properties/ClusterProperty.java", "diffHunk": "@@ -652,8 +652,24 @@ private int getWhenNoSSLDetected() {\n             = new HazelcastProperty(\"hazelcast.partition.table.send.interval\", 15, SECONDS);\n     public static final HazelcastProperty PARTITION_BACKUP_SYNC_INTERVAL\n             = new HazelcastProperty(\"hazelcast.partition.backup.sync.interval\", 30, SECONDS);\n+    /**\n+     * Maximum number of partition migrations to be executed on a member.", "originalCommit": "869cd7f44a82f572cf57e40e6d5e791fd62af919", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzI1NjY5Nw==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r477256697", "bodyText": "If there is an associated gh issue to address what needs to be done, I 'd rather we don't add yet another todo in the codebase :-)", "author": "vbekiaris", "createdAt": "2020-08-26T12:20:52Z", "path": "hazelcast/src/main/java/com/hazelcast/spi/impl/operationservice/impl/Invocation.java", "diffHunk": "@@ -283,6 +283,7 @@ final void initInvocationTarget() throws Exception {\n             if (previousTargetMember != null) {\n                 // If a target member was found earlier but current target member is null\n                 // then it means a member left.\n+                // TODO: Above comment is not always true for RaftInvocation.", "originalCommit": "869cd7f44a82f572cf57e40e6d5e791fd62af919", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI3NjQ2Ng==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r478276466", "bodyText": "I'll create an issue. \ud83d\udc4d", "author": "mdogan", "createdAt": "2020-08-27T09:18:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzI1NjY5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI5Njk0NQ==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r478296945", "bodyText": "#17420", "author": "mdogan", "createdAt": "2020-08-27T09:52:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzI1NjY5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzI3NzU2NA==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r477277564", "bodyText": "checkstyle \ud83d\ude09\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                if (t instanceof  OperationTimeoutException || t.getCause() instanceof OperationTimeoutException) {\n          \n          \n            \n                                if (t instanceof OperationTimeoutException || t.getCause() instanceof OperationTimeoutException) {", "author": "vbekiaris", "createdAt": "2020-08-26T12:55:11Z", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/impl/MigrationManager.java", "diffHunk": "@@ -423,6 +408,86 @@ private boolean commitMigrationToDestination(MigrationInfo migration) {\n         return false;\n     }\n \n+    /**\n+     * Sends a {@link MigrationCommitOperation} to the destination and returns {@code true} if the new partition state\n+     * was applied on the destination.\n+     */\n+    @SuppressWarnings({\"checkstyle:npathcomplexity\", \"checkstyle:cyclomaticcomplexity\", \"checkstyle:methodlength\"})\n+    private CompletionStage<Boolean> commitMigrationToDestinationAsync(MigrationInfo migration) {\n+        PartitionReplica destination = migration.getDestination();\n+\n+        if (destination.isIdentical(node.getLocalMember())) {\n+            if (logger.isFinestEnabled()) {\n+                logger.finest(\"Shortcutting migration commit, since destination is master. -> \" + migration);\n+            }\n+            return CompletableFuture.completedFuture(Boolean.TRUE);\n+        }\n+\n+        Member member = node.getClusterService().getMember(destination.address(), destination.uuid());\n+        if (member == null) {\n+            logger.warning(\"Cannot commit \" + migration + \". Destination \" + destination + \" is not a member anymore\");\n+            return CompletableFuture.completedFuture(Boolean.FALSE);\n+        }\n+\n+        try {\n+            if (logger.isFinestEnabled()) {\n+                logger.finest(\"Sending migration commit operation to \" + destination + \" for \" + migration);\n+            }\n+            migration.setStatus(MigrationStatus.SUCCESS);\n+            UUID destinationUuid = member.getUuid();\n+\n+            MigrationCommitOperation operation = new MigrationCommitOperation(migration, destinationUuid);\n+            InvocationFuture<Boolean> future = nodeEngine.getOperationService()\n+                    .createInvocationBuilder(SERVICE_NAME, operation, destination.address())\n+                    .setTryCount(Integer.MAX_VALUE)\n+                    .setCallTimeout(memberHeartbeatTimeoutMillis).invoke();\n+\n+            final String successResult = \"SUCCESS\";\n+            final String failureResult = \"FAIL\";\n+            final String retryResult = \"RETRY\";\n+\n+            return future.handle((done, t) -> {\n+                // Inspect commit result;\n+                // - if there's an exception, either retry or fail\n+                // - if result is true then success, otherwise failure\n+                logger.fine(\"Migration commit response received -> \" + migration + \", success: \" + done + \", failure: \" + t);\n+                if (t != null) {\n+                    logMigrationCommitFailure(migration, t);\n+                    if (t instanceof  OperationTimeoutException || t.getCause() instanceof OperationTimeoutException) {", "originalCommit": "869cd7f44a82f572cf57e40e6d5e791fd62af919", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzI4OTY1Nw==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r477289657", "bodyText": "refactor to static final constants? Also, we could use a more succinct result encoding, like plain ints?", "author": "vbekiaris", "createdAt": "2020-08-26T13:14:01Z", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/impl/MigrationManager.java", "diffHunk": "@@ -423,6 +408,86 @@ private boolean commitMigrationToDestination(MigrationInfo migration) {\n         return false;\n     }\n \n+    /**\n+     * Sends a {@link MigrationCommitOperation} to the destination and returns {@code true} if the new partition state\n+     * was applied on the destination.\n+     */\n+    @SuppressWarnings({\"checkstyle:npathcomplexity\", \"checkstyle:cyclomaticcomplexity\", \"checkstyle:methodlength\"})\n+    private CompletionStage<Boolean> commitMigrationToDestinationAsync(MigrationInfo migration) {\n+        PartitionReplica destination = migration.getDestination();\n+\n+        if (destination.isIdentical(node.getLocalMember())) {\n+            if (logger.isFinestEnabled()) {\n+                logger.finest(\"Shortcutting migration commit, since destination is master. -> \" + migration);\n+            }\n+            return CompletableFuture.completedFuture(Boolean.TRUE);\n+        }\n+\n+        Member member = node.getClusterService().getMember(destination.address(), destination.uuid());\n+        if (member == null) {\n+            logger.warning(\"Cannot commit \" + migration + \". Destination \" + destination + \" is not a member anymore\");\n+            return CompletableFuture.completedFuture(Boolean.FALSE);\n+        }\n+\n+        try {\n+            if (logger.isFinestEnabled()) {\n+                logger.finest(\"Sending migration commit operation to \" + destination + \" for \" + migration);\n+            }\n+            migration.setStatus(MigrationStatus.SUCCESS);\n+            UUID destinationUuid = member.getUuid();\n+\n+            MigrationCommitOperation operation = new MigrationCommitOperation(migration, destinationUuid);\n+            InvocationFuture<Boolean> future = nodeEngine.getOperationService()\n+                    .createInvocationBuilder(SERVICE_NAME, operation, destination.address())\n+                    .setTryCount(Integer.MAX_VALUE)\n+                    .setCallTimeout(memberHeartbeatTimeoutMillis).invoke();\n+\n+            final String successResult = \"SUCCESS\";\n+            final String failureResult = \"FAIL\";\n+            final String retryResult = \"RETRY\";", "originalCommit": "869cd7f44a82f572cf57e40e6d5e791fd62af919", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI3ODA3Nw==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r478278077", "bodyText": "\ud83d\udc4d I was logging the responses while testing, that's why I've used string values. I'll convert to ints.", "author": "mdogan", "createdAt": "2020-08-27T09:20:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzI4OTY1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzM5MDUyNQ==", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r477390525", "bodyText": "minor: could also use the static factory methods from InternalCompletableFuture for completed futures", "author": "vbekiaris", "createdAt": "2020-08-26T15:28:29Z", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/impl/MigrationManager.java", "diffHunk": "@@ -947,17 +1048,529 @@ public void migrate(PartitionReplica source, int sourceCurrentReplicaIndex, int\n                 }\n             }\n         }\n+    }\n+\n+    class MigrationPlanTask implements MigrationRunnable {\n+        /** List of migration queues per-partition */\n+        private final List<Queue<MigrationInfo>> migrationQs;\n+        /**\n+         * Queue for completed migrations.\n+         * It will be processed concurrently while migrations are running.\n+         * */\n+        private final BlockingQueue<MigrationInfo> completed;\n+        /**\n+         * Set of currently migrating partition IDs.\n+         * It's illegal to have concurrent migrations on the same partition.\n+         */\n+        private final Set<Integer> migratingPartitions = new HashSet<>();\n+        /**\n+         * Map of endpoint -> migration-count.\n+         * Only {@link #maxParallelMigrations} number of migrations are allowed on a single member.\n+         */\n+        private final Map<Address, Integer> endpoint2MigrationCount = new HashMap<>();\n+        private int ongoingMigrationCount;\n+        private boolean failed;\n+        private volatile boolean aborted;\n+\n+        MigrationPlanTask(List<Queue<MigrationInfo>> migrationQs) {\n+            this.migrationQs = migrationQs;\n+            this.completed = new ArrayBlockingQueue<>(migrationQs.size());\n+        }\n+\n+        @Override\n+        public void run() {\n+            migrationCount.set(migrationQs.stream().mapToInt(Collection::size).sum());\n+\n+            while (true) {\n+                MigrationInfo migration = next();\n+                if (migration == null) {\n+                    break;\n+                }\n+\n+                if (failed | aborted) {\n+                    break;\n+                }\n+\n+                onStart(migration);\n+\n+                try {\n+                    CompletionStage<Boolean> f = new AsyncMigrationTask(migration).run();\n+                    f.thenRun(() -> {\n+                        logger.fine(\"AsyncMigrationTask completed: \" + migration);\n+                        boolean offered = completed.offer(migration);\n+                        assert offered : \"Failed to offer completed migration: \" + migration;\n+                    });\n+                } catch (Throwable e) {\n+                    logger.warning(\"AsyncMigrationTask failed: \" + migration, e);\n+                    boolean offered = completed.offer(migration);\n+                    assert offered : \"Failed to offer completed migration: \" + migration;\n+                }\n+\n+                if (!migrationDelay()) {\n+                    break;\n+                }\n+            }\n+\n+            waitOngoingMigrations();\n+\n+            if (failed || aborted) {\n+                logger.info(\"Rebalance process was \" + (failed ? \" failed\" : \"aborted\")\n+                        + \". Ignoring remaining migrations. Will recalculate the new migration plan. (\"\n+                        + stats.formatToString(logger.isFineEnabled()) + \")\");\n+                migrationCount.set(0);\n+                migrationQs.clear();\n+            } else {\n+                logger.info(\"All migration tasks have been completed. (\" + stats.formatToString(logger.isFineEnabled()) + \")\");\n+            }\n+        }\n+\n+        private void onStart(MigrationInfo migration) {\n+            boolean added = migratingPartitions.add(migration.getPartitionId());\n+            assert added : \"Couldn't add partitionId to migrating partitions set: \" + migration;\n+\n+            BiFunction<Address, Integer, Integer> inc = (address, current) -> current != null ? current + 1 : 1;\n+\n+            int count = endpoint2MigrationCount.compute(migration.getDestinationAddress(), inc);\n+            assert count > 0 && count <= maxParallelMigrations : \"Count: \" + count + \" -> \" + migration;\n+\n+            count = endpoint2MigrationCount.compute(sourceAddress(migration), inc);\n+            assert count > 0 && count <= maxParallelMigrations : \"Count: \" + count + \" -> \" + migration;\n \n+            ongoingMigrationCount++;\n+            migrationCount.decrementAndGet();\n+        }\n+\n+        private void onComplete(MigrationInfo migration) {\n+            boolean removed = migratingPartitions.remove(migration.getPartitionId());\n+            assert removed : \"Couldn't remove partitionId from migrating partitions set: \" + migration;\n+\n+            BiFunction<Address, Integer, Integer> dec = (address, current) -> current != null ? current - 1 : -1;\n+\n+            long count = endpoint2MigrationCount.compute(migration.getDestinationAddress(), dec);\n+            assert count >= 0 && count < maxParallelMigrations : \"Count: \" + count + \" -> \" + migration;\n+\n+            count = endpoint2MigrationCount.compute(sourceAddress(migration), dec);\n+            assert count >= 0 && count < maxParallelMigrations : \"Count: \" + count + \" -> \" + migration;\n+\n+            if (migration.getStatus() != MigrationStatus.SUCCESS) {\n+                failed = true;\n+            }\n+\n+            ongoingMigrationCount--;\n+        }\n+\n+        private boolean processCompleted() {\n+            boolean ok = false;\n+            MigrationInfo migration;\n+            while ((migration = completed.poll()) != null) {\n+                onComplete(migration);\n+                ok = true;\n+            }\n+            return ok;\n+        }\n+\n+        private MigrationInfo next() {\n+            MigrationInfo m;\n+            while ((m = next0()) == null) {\n+                if (migrationQs.isEmpty()) {\n+                    break;\n+                }\n+\n+                if (!processCompleted()) {\n+                    try {\n+                        MigrationInfo migration = completed.take();\n+                        onComplete(migration);\n+                    } catch (InterruptedException e) {\n+                        onInterrupted(e);\n+                        break;\n+                    }\n+                }\n+\n+                if (failed | aborted) {\n+                    break;\n+                }\n+            }\n+            return m;\n+        }\n+\n+        private MigrationInfo next0() {\n+            Iterator<Queue<MigrationInfo>> iter = migrationQs.iterator();\n+            while (iter.hasNext()) {\n+                Queue<MigrationInfo> q = iter.next();\n+                if (q.isEmpty()) {\n+                    iter.remove();\n+                    continue;\n+                }\n+\n+                if (!select(q.peek())) {\n+                    continue;\n+                }\n+\n+                return q.poll();\n+            }\n+            return null;\n+        }\n+\n+        private boolean select(MigrationInfo m) {\n+            if (m == null) {\n+                return true;\n+            }\n+\n+            if (migratingPartitions.contains(m.getPartitionId())) {\n+                return false;\n+            }\n+            if (endpoint2MigrationCount.getOrDefault(m.getDestinationAddress(), 0) == maxParallelMigrations) {\n+                return false;\n+            }\n+            return endpoint2MigrationCount.getOrDefault(sourceAddress(m), 0) < maxParallelMigrations;\n+        }\n+\n+        private Address sourceAddress(MigrationInfo m) {\n+            if (m.getSourceCurrentReplicaIndex() == 0) {\n+                return m.getSourceAddress();\n+            }\n+            InternalPartitionImpl partition = partitionStateManager.getPartitionImpl(m.getPartitionId());\n+            return partition.getOwnerOrNull();\n+        }\n+\n+        private boolean migrationDelay() {\n+            if (partitionMigrationInterval > 0) {\n+                try {\n+                    Thread.sleep(partitionMigrationInterval);\n+                } catch (InterruptedException e) {\n+                    onInterrupted(e);\n+                    return false;\n+                }\n+            }\n+            return true;\n+        }\n+\n+        private void waitOngoingMigrations() {\n+            boolean interrupted = false;\n+            while (ongoingMigrationCount > 0) {\n+                try {\n+                    MigrationInfo migration = completed.take();\n+                    onComplete(migration);\n+                } catch (InterruptedException ignored) {\n+                    interrupted = true;\n+                }\n+            }\n+            if (interrupted) {\n+                Thread.currentThread().interrupt();\n+            }\n+        }\n+\n+        private void onInterrupted(InterruptedException e) {\n+            logger.info(\"MigrationProcessTask is interrupted! Ignoring remaining migrations...\", e);\n+            Thread.currentThread().interrupt();\n+            abort();\n+        }\n+\n+        void abort() {\n+            aborted = true;\n+        }\n+    }\n+\n+    /**\n+     * Invoked on the master node to migrate a partition (excluding promotions).\n+     * It will execute the {@link MigrationRequestOperation} on the partition owner.\n+     */\n+    private class AsyncMigrationTask {\n+        private final MigrationInfo migration;\n+\n+        AsyncMigrationTask(MigrationInfo migration) {\n+            this.migration = migration;\n+            migration.setMaster(node.getThisAddress());\n+        }\n+\n+        CompletionStage<Boolean> run() {\n+            if (!partitionService.isLocalMemberMaster()) {\n+                return CompletableFuture.completedFuture(Boolean.FALSE);\n+            }\n+\n+            if (migration.getSource() == null\n+                    && migration.getDestinationCurrentReplicaIndex() > 0\n+                    && migration.getDestinationNewReplicaIndex() == 0) {\n+\n+                throw new IllegalStateException(\"Promotion migrations should be handled by \"\n+                        + RepairPartitionTableTask.class.getSimpleName() + \" -> \" + migration);\n+            }\n+\n+            Member partitionOwner = checkMigrationParticipantsAndGetPartitionOwner();\n+            if (partitionOwner == null) {\n+                return CompletableFuture.completedFuture(Boolean.FALSE);\n+            }\n+\n+            return executeMigrateOperation(partitionOwner);\n+        }\n+\n+        private void beforeMigration() {\n+            migration.setInitialPartitionVersion(partitionStateManager.getPartitionVersion(migration.getPartitionId()));\n+            migrationInterceptor.onMigrationStart(MigrationParticipant.MASTER, migration);\n+            if (logger.isFineEnabled()) {\n+                logger.fine(\"Starting Migration: \" + migration);\n+            }\n+        }\n+\n+        /**\n+         * Checks if the partition owner is not {@code null}, the source and destinations are still members and returns the owner.\n+         * Returns {@code null} and reschedules the {@link ControlTask} if the checks failed.\n+         */\n+        private Member checkMigrationParticipantsAndGetPartitionOwner() {\n+            Member partitionOwner = getPartitionOwner();\n+            if (partitionOwner == null) {\n+                logger.fine(\"Partition owner is null. Ignoring \" + migration);\n+                triggerRepartitioningAfterMigrationFailure();\n+                return null;\n+            }\n+            if (migration.getSource() != null) {\n+                PartitionReplica source = migration.getSource();\n+                if (node.getClusterService().getMember(source.address(), source.uuid()) == null) {\n+                    logger.fine(\"Source is not a member anymore. Ignoring \" + migration);\n+                    triggerRepartitioningAfterMigrationFailure();\n+                    return null;\n+                }\n+            }\n+            PartitionReplica destination = migration.getDestination();\n+            if (node.getClusterService().getMember(destination.address(), destination.uuid()) == null) {\n+                logger.fine(\"Destination is not a member anymore. Ignoring \" + migration);\n+                triggerRepartitioningAfterMigrationFailure();\n+                return null;\n+            }\n+            return partitionOwner;\n+        }\n+\n+        /** Returns the partition owner or {@code null} if it is not set. */\n+        private Member getPartitionOwner() {\n+            InternalPartitionImpl partition = partitionStateManager.getPartitionImpl(migration.getPartitionId());\n+            PartitionReplica owner = partition.getOwnerReplicaOrNull();\n+            if (owner == null) {\n+                if (migration.isValid()) {\n+                    logger.severe(\"Skipping migration! Partition owner is not set! -> partitionId=\"\n+                            + migration.getPartitionId()\n+                            + \", \" + partition + \" -VS- \" + migration);\n+                }\n+                return null;\n+            }\n+            return node.getClusterService().getMember(owner.address(), owner.uuid());\n+        }\n+\n+        /**\n+         * Sends a {@link MigrationRequestOperation} to the {@code fromMember} and returns the migration result if the\n+         * migration was successful.\n+         */\n+        @SuppressWarnings({\"checkstyle:npathcomplexity\", \"checkstyle:cyclomaticcomplexity\"})\n+        private CompletionStage<Boolean> executeMigrateOperation(Member fromMember) {\n+            long start = Timer.nanos();\n+            CompletableFuture<Boolean> future;\n+            try {\n+                beforeMigration();\n+\n+                List<MigrationInfo> completedMigrations = getCompletedMigrations(migration.getPartitionId());\n+                Operation op = new MigrationRequestOperation(migration, completedMigrations, 0, fragmentedMigrationEnabled);\n+                future = nodeEngine.getOperationService()\n+                        .createInvocationBuilder(SERVICE_NAME, op, fromMember.getAddress())\n+                        .setCallTimeout(partitionMigrationTimeout)\n+                        .invoke();\n+            } catch (Throwable t) {\n+                Level level = migration.isValid() ? Level.WARNING : Level.FINE;\n+                logger.log(level, \"Error during \" + migration, t);\n+                future = new CompletableFuture<>();\n+                future.completeExceptionally(t);", "originalCommit": "869cd7f44a82f572cf57e40e6d5e791fd62af919", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "c4bfc82475ce0b112b2ed904a4a38d23457c5789", "url": "https://github.com/hazelcast/hazelcast/commit/c4bfc82475ce0b112b2ed904a4a38d23457c5789", "message": "Parallelize partition replica migrations\n\nWith thousands of partitions and/or with large amount of data (a few TB),\nrebalancing partitions after adding or removing a member takes\nsignificant amount of time (up to hours).\n\nBy parallelizing migrations, it's possible to reduce this time\nto a few mins by utilizing network bandwidth. That way cluster will\nstabilize in a shorter time. But there's a trade-off; using\nmost of the network bandwidth for migration data may increase\nthe latency of concurrent read/write operations.\n\nIn summary:\n\n- Replaced the global partition table version with per-partition\nversions.\n- Parallelized replica migrations of independent partitions.", "committedDate": "2020-08-27T09:59:14Z", "type": "forcePushed"}, {"oid": "916fa4c0a51867b56e94359e2a5202ad092912df", "url": "https://github.com/hazelcast/hazelcast/commit/916fa4c0a51867b56e94359e2a5202ad092912df", "message": "Parallelize partition replica migrations\n\nWith thousands of partitions and/or with large amount of data (a few TB),\nrebalancing partitions after adding or removing a member takes\nsignificant amount of time (up to hours).\n\nBy parallelizing migrations, it's possible to reduce this time\nto a few mins by utilizing network bandwidth. That way cluster will\nstabilize in a shorter time. But there's a trade-off; using\nmost of the network bandwidth for migration data may increase\nthe latency of concurrent read/write operations.\n\nIn summary:\n\n- Replaced the global partition table version with per-partition\nversions.\n- Parallelized replica migrations of independent partitions.", "committedDate": "2020-08-27T10:02:26Z", "type": "forcePushed"}, {"oid": "6f74b1206b315961dee144fa563146559ca6205c", "url": "https://github.com/hazelcast/hazelcast/commit/6f74b1206b315961dee144fa563146559ca6205c", "message": "Parallelize partition replica migrations\n\nWith thousands of partitions and/or with large amount of data (a few TB),\nrebalancing partitions after adding or removing a member takes\nsignificant amount of time (up to hours).\n\nBy parallelizing migrations, it's possible to reduce this time\nto a few mins by utilizing network bandwidth. That way cluster will\nstabilize in a shorter time. But there's a trade-off; using\nmost of the network bandwidth for migration data may increase\nthe latency of concurrent read/write operations.\n\nIn summary:\n\n- Replaced the global partition table version with per-partition\nversions.\n- Parallelized replica migrations of independent partitions.", "committedDate": "2020-08-27T14:23:43Z", "type": "forcePushed"}, {"oid": "7455c6693a37b3636af106e043e095087c6d7100", "url": "https://github.com/hazelcast/hazelcast/commit/7455c6693a37b3636af106e043e095087c6d7100", "message": "Parallelize partition replica migrations\n\nWith thousands of partitions and/or with large amount of data (a few TB),\nrebalancing partitions after adding or removing a member takes\nsignificant amount of time (up to hours).\n\nBy parallelizing migrations, it's possible to reduce this time\nto a few mins by utilizing network bandwidth. That way cluster will\nstabilize in a shorter time. But there's a trade-off; using\nmost of the network bandwidth for migration data may increase\nthe latency of concurrent read/write operations.\n\nIn summary:\n\n- Replaced the global partition table version with per-partition\nversions.\n- Parallelized replica migrations of independent partitions.", "committedDate": "2020-08-31T08:56:32Z", "type": "commit"}, {"oid": "7455c6693a37b3636af106e043e095087c6d7100", "url": "https://github.com/hazelcast/hazelcast/commit/7455c6693a37b3636af106e043e095087c6d7100", "message": "Parallelize partition replica migrations\n\nWith thousands of partitions and/or with large amount of data (a few TB),\nrebalancing partitions after adding or removing a member takes\nsignificant amount of time (up to hours).\n\nBy parallelizing migrations, it's possible to reduce this time\nto a few mins by utilizing network bandwidth. That way cluster will\nstabilize in a shorter time. But there's a trade-off; using\nmost of the network bandwidth for migration data may increase\nthe latency of concurrent read/write operations.\n\nIn summary:\n\n- Replaced the global partition table version with per-partition\nversions.\n- Parallelized replica migrations of independent partitions.", "committedDate": "2020-08-31T08:56:32Z", "type": "forcePushed"}]}