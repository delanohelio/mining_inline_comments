{"pr_number": 1026, "pr_title": "Updated Kafka + SQL storage variant, re-using existing kafka storage utils/data model", "pr_createdAt": "2020-11-23T20:45:00Z", "pr_url": "https://github.com/Apicurio/apicurio-registry/pull/1026", "timeline": [{"oid": "20d2060799eb9b5106f5ac68d750df02ddbc0753", "url": "https://github.com/Apicurio/apicurio-registry/commit/20d2060799eb9b5106f5ac68d750df02ddbc0753", "message": "Added a Kafka+SQL storage variant", "committedDate": "2020-11-10T15:50:23Z", "type": "commit"}, {"oid": "e76c3d89b5474aaca31e1043e555fd52a7517e67", "url": "https://github.com/Apicurio/apicurio-registry/commit/e76c3d89b5474aaca31e1043e555fd52a7517e67", "message": "introduced overlays for application.properties to avoid putting all properties in one file", "committedDate": "2020-11-11T03:27:54Z", "type": "commit"}, {"oid": "31f2098241371f2a9a112085a4ef22313cb95c0d", "url": "https://github.com/Apicurio/apicurio-registry/commit/31f2098241371f2a9a112085a4ef22313cb95c0d", "message": "Fixed the ksql tests - they all pass!", "committedDate": "2020-11-11T15:47:20Z", "type": "commit"}, {"oid": "5863a973be143260d39fd23d43235d02bfa0b908", "url": "https://github.com/Apicurio/apicurio-registry/commit/5863a973be143260d39fd23d43235d02bfa0b908", "message": "added some logging to the merge properties mojo", "committedDate": "2020-11-11T18:10:10Z", "type": "commit"}, {"oid": "402db21e3f677ee00e55f72ecd9b5c2f522a336c", "url": "https://github.com/Apicurio/apicurio-registry/commit/402db21e3f677ee00e55f72ecd9b5c2f522a336c", "message": "push the UUID into the payload and make the kafka message key the artifactId to ensure ordering", "committedDate": "2020-11-13T14:07:00Z", "type": "commit"}, {"oid": "1b874f3c5741436132dd79c98fecbf074cf166f4", "url": "https://github.com/Apicurio/apicurio-registry/commit/1b874f3c5741436132dd79c98fecbf074cf166f4", "message": "fix selectArtifactMetaDataByGlobalId query bug and add reproducer test", "committedDate": "2020-11-15T18:17:38Z", "type": "commit"}, {"oid": "f4240cb599cb48542cdfd9d5ec89ae503018ed14", "url": "https://github.com/Apicurio/apicurio-registry/commit/f4240cb599cb48542cdfd9d5ec89ae503018ed14", "message": "Merge pull request #10 from famartinrh/learning/kafka-sql/fix-sql-bug\n\nfix selectArtifactMetaDataByGlobalId query bug and add reproducer test", "committedDate": "2020-11-16T17:21:44Z", "type": "commit"}, {"oid": "371a49d80cd69b27971f994052b25137a41ca0fe", "url": "https://github.com/Apicurio/apicurio-registry/commit/371a49d80cd69b27971f994052b25137a41ca0fe", "message": "some tweaks based on perf testing", "committedDate": "2020-11-23T13:57:30Z", "type": "commit"}, {"oid": "66105cf4acfb03b1a3a0b555b9f01d502a6be006", "url": "https://github.com/Apicurio/apicurio-registry/commit/66105cf4acfb03b1a3a0b555b9f01d502a6be006", "message": "kafka + sql storage variant, reusing streams variant datamodel (#1012)\n\n* kafka + sql storage variant, reusing streams variant datamodel\r\n\r\n* ksql - integration tests\r\n\r\n* fix streams storage", "committedDate": "2020-11-23T14:03:50Z", "type": "commit"}, {"oid": "7054b23454ad60b554f1551fdd8644d8e01b429f", "url": "https://github.com/Apicurio/apicurio-registry/commit/7054b23454ad60b554f1551fdd8644d8e01b429f", "message": "Merge branch 'learning/kafka-sql' of github.com:Apicurio/apicurio-registry into learning/kafka-sql", "committedDate": "2020-11-23T14:05:09Z", "type": "commit"}, {"oid": "004edd311ab0f3344981a78d45304a5a21c07338", "url": "https://github.com/Apicurio/apicurio-registry/commit/004edd311ab0f3344981a78d45304a5a21c07338", "message": "Merged changes from master", "committedDate": "2020-11-23T14:59:08Z", "type": "commit"}, {"oid": "163a77df9be8df20e4241cc29bf6d9ed5d35a71a", "url": "https://github.com/Apicurio/apicurio-registry/commit/163a77df9be8df20e4241cc29bf6d9ed5d35a71a", "message": "fixed some bugs in the ksql modified impl, and modified some tests to handle new async behavior", "committedDate": "2020-11-23T20:20:44Z", "type": "commit"}, {"oid": "df0c0720564eb90b4167468327f19cbc4798e24c", "url": "https://github.com/Apicurio/apicurio-registry/commit/df0c0720564eb90b4167468327f19cbc4798e24c", "message": "minor TODO", "committedDate": "2020-11-23T20:25:43Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTI4OTM2MA==", "url": "https://github.com/Apicurio/apicurio-registry/pull/1026#discussion_r529289360", "bodyText": "Maybe a more concrete error message here?", "author": "carlesarnal", "createdAt": "2020-11-24T08:31:42Z", "path": "storage/ksql/src/main/java/io/apicurio/registry/storage/impl/ksql/KafkaSqlCoordinator.java", "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Copyright 2020 Red Hat\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.apicurio.registry.storage.impl.ksql;\n+\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+\n+import javax.enterprise.context.ApplicationScoped;\n+\n+import io.apicurio.registry.types.RegistryException;\n+\n+/**\n+ * Coordinates \"write\" responses across threads in the Kafka-SQL storage implementation.  Basically this is used\n+ * to communicate between the Kafka consumer thread and the waiting HTTP/API thread, where the HTTP thread is\n+ * waiting for an operation to be completed by the Kafka consumer thread.\n+ *\n+ * @author eric.wittmann@gmail.com\n+ */\n+@ApplicationScoped\n+public class KafkaSqlCoordinator {\n+\n+    private static final Object NULL = new Object();\n+    private Map<UUID, CountDownLatch> latches = new ConcurrentHashMap<>();\n+    private Map<UUID, Object> returnValues = new ConcurrentHashMap<>();\n+\n+    /**\n+     * Creates a UUID for a single operation.\n+     */\n+    public UUID createUUID() {\n+        UUID uuid = UUID.randomUUID();\n+        latches.put(uuid, new CountDownLatch(1));\n+        return uuid;\n+    }\n+\n+    /**\n+     * Waits for a response to the operation with the given UUID. There is a countdown latch for each operation.  The\n+     * caller waiting for the response will wait for the countdown to happen and then proceed.  We also remove\n+     * the latch from the Map here since it's not needed anymore.\n+     *\n+     * @param uuid\n+     * @throws InterruptedException\n+     */\n+    public Object waitForResponse(UUID uuid) {\n+        // TODO timeout should be configurable\n+        try {\n+            latches.get(uuid).await(30, TimeUnit.SECONDS);\n+\n+            Object rval = returnValues.remove(uuid);\n+            if (rval == NULL) {\n+                return null;\n+            } else if (rval instanceof RegistryException) {\n+                throw (RegistryException) rval;\n+            }\n+            return rval;\n+        } catch (InterruptedException e) {\n+          throw new RegistryException(e);", "originalCommit": "df0c0720564eb90b4167468327f19cbc4798e24c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc2OTk1NA==", "url": "https://github.com/Apicurio/apicurio-registry/pull/1026#discussion_r529769954", "bodyText": "Good idea.  I'll do this.", "author": "EricWittmann", "createdAt": "2020-11-24T17:55:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTI4OTM2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTMwNDU4Ng==", "url": "https://github.com/Apicurio/apicurio-registry/pull/1026#discussion_r529304586", "bodyText": "I don't see how the artifact version is being handled in this storage. I know that the -1 is what the streams storage is sending at this point, but then the StreamsTopologyProvider updates that version appropriately. I don't see something that replaces that functionality here.", "author": "carlesarnal", "createdAt": "2020-11-24T08:55:35Z", "path": "storage/ksql/src/main/java/io/apicurio/registry/storage/impl/ksql/KafkaSqlRegistryStorage.java", "diffHunk": "@@ -0,0 +1,597 @@\n+/*\n+ * Copyright 2020 Red Hat\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.apicurio.registry.storage.impl.ksql;\n+\n+import static io.apicurio.registry.metrics.MetricIDs.STORAGE_CONCURRENT_OPERATION_COUNT;\n+import static io.apicurio.registry.metrics.MetricIDs.STORAGE_CONCURRENT_OPERATION_COUNT_DESC;\n+import static io.apicurio.registry.metrics.MetricIDs.STORAGE_GROUP_TAG;\n+import static io.apicurio.registry.metrics.MetricIDs.STORAGE_OPERATION_COUNT;\n+import static io.apicurio.registry.metrics.MetricIDs.STORAGE_OPERATION_COUNT_DESC;\n+import static io.apicurio.registry.metrics.MetricIDs.STORAGE_OPERATION_TIME;\n+import static io.apicurio.registry.metrics.MetricIDs.STORAGE_OPERATION_TIME_DESC;\n+import static org.eclipse.microprofile.metrics.MetricUnits.MILLISECONDS;\n+\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.EnumSet;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.SortedSet;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.function.Function;\n+import javax.annotation.PreDestroy;\n+import javax.enterprise.context.ApplicationScoped;\n+import javax.enterprise.event.Observes;\n+import javax.inject.Inject;\n+import javax.inject.Named;\n+\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.header.Header;\n+import org.apache.kafka.common.header.internals.RecordHeader;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.eclipse.microprofile.config.inject.ConfigProperty;\n+import org.eclipse.microprofile.metrics.annotation.ConcurrentGauge;\n+import org.eclipse.microprofile.metrics.annotation.Counted;\n+import org.eclipse.microprofile.metrics.annotation.Timed;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import io.apicurio.registry.content.ContentHandle;\n+import io.apicurio.registry.logging.Logged;\n+import io.apicurio.registry.metrics.PersistenceExceptionLivenessApply;\n+import io.apicurio.registry.metrics.PersistenceTimeoutReadinessApply;\n+import io.apicurio.registry.rest.beans.ArtifactSearchResults;\n+import io.apicurio.registry.rest.beans.SearchOver;\n+import io.apicurio.registry.rest.beans.SortOrder;\n+import io.apicurio.registry.rest.beans.VersionSearchResults;\n+import io.apicurio.registry.storage.ArtifactAlreadyExistsException;\n+import io.apicurio.registry.storage.ArtifactMetaDataDto;\n+import io.apicurio.registry.storage.ArtifactNotFoundException;\n+import io.apicurio.registry.storage.ArtifactStateExt;\n+import io.apicurio.registry.storage.ArtifactVersionMetaDataDto;\n+import io.apicurio.registry.storage.EditableArtifactMetaDataDto;\n+import io.apicurio.registry.storage.RegistryStorage;\n+import io.apicurio.registry.storage.RegistryStorageException;\n+import io.apicurio.registry.storage.RuleAlreadyExistsException;\n+import io.apicurio.registry.storage.RuleConfigurationDto;\n+import io.apicurio.registry.storage.RuleNotFoundException;\n+import io.apicurio.registry.storage.StoredArtifact;\n+import io.apicurio.registry.storage.VersionNotFoundException;\n+import io.apicurio.registry.storage.impl.AbstractRegistryStorage;\n+import io.apicurio.registry.storage.impl.ksql.sql.KafkaSQLSink;\n+import io.apicurio.registry.storage.proto.Str;\n+import io.apicurio.registry.types.ArtifactState;\n+import io.apicurio.registry.types.ArtifactType;\n+import io.apicurio.registry.types.RuleType;\n+import io.apicurio.registry.util.DtoUtil;\n+import io.apicurio.registry.utils.ConcurrentUtil;\n+import io.apicurio.registry.utils.kafka.AsyncProducer;\n+import io.apicurio.registry.utils.kafka.ProducerActions;\n+import io.apicurio.registry.utils.kafka.ProtoSerde;\n+import io.apicurio.registry.utils.kafka.Submitter;\n+import io.quarkus.runtime.StartupEvent;\n+\n+/**\n+ * An implementation of a registry storage that extends the basic SQL storage but federates 'write' operations\n+ * to other nodes in a cluster using a Kafka topic.  As a result, all reads are performed locally but all\n+ * writes are published to a topic for consumption by all nodes.\n+ *\n+ * @author eric.wittmann@gmail.com\n+ */\n+@ApplicationScoped\n+@PersistenceExceptionLivenessApply\n+@PersistenceTimeoutReadinessApply\n+@Counted(name = STORAGE_OPERATION_COUNT, description = STORAGE_OPERATION_COUNT_DESC, tags = {\"group=\" + STORAGE_GROUP_TAG, \"metric=\" + STORAGE_OPERATION_COUNT})\n+@ConcurrentGauge(name = STORAGE_CONCURRENT_OPERATION_COUNT, description = STORAGE_CONCURRENT_OPERATION_COUNT_DESC, tags = {\"group=\" + STORAGE_GROUP_TAG, \"metric=\" + STORAGE_CONCURRENT_OPERATION_COUNT})\n+@Timed(name = STORAGE_OPERATION_TIME, description = STORAGE_OPERATION_TIME_DESC, tags = {\"group=\" + STORAGE_GROUP_TAG, \"metric=\" + STORAGE_OPERATION_TIME}, unit = MILLISECONDS)\n+@Logged\n+@SuppressWarnings(\"unchecked\")\n+public class KafkaSqlRegistryStorage extends AbstractRegistryStorage {\n+\n+    private static final Logger log = LoggerFactory.getLogger(KafkaSqlRegistryStorage.class);\n+\n+    /* Fake global rules as an artifact */\n+    public static final String GLOBAL_RULES_ID = \"__GLOBAL_RULES__\";\n+\n+    @Inject\n+    KafkaSqlCoordinator coordinator;\n+\n+    @Inject\n+    KafkaSQLSink kafkaSqlSink;\n+\n+    @Inject\n+    @Named(\"SQLRegistryStorage\")\n+    RegistryStorage sqlStorage;\n+\n+    @Inject\n+    @ConfigProperty(name = \"registry.ksql.globalRuleKey\", defaultValue = \"__global_rule\")\n+    String globalRuleKey;\n+\n+    @Inject\n+    @ConfigProperty(name = \"registry.ksql.bootstrap.servers\")\n+    String bootstrapServers;\n+\n+    @Inject\n+    @ConfigProperty(name = \"registry.ksql.topic\", defaultValue = \"storage-topic\")\n+    String topic;\n+\n+    @Inject\n+    @ConfigProperty(name = \"registry.ksql.consumer.startupLag\", defaultValue = \"1000\")\n+    Integer startupLag;\n+\n+    @Inject\n+    @ConfigProperty(name = \"registry.ksql.consumer.poll.timeout\", defaultValue = \"1000\")\n+    Integer pollTimeout;\n+\n+    private boolean stopped = true;\n+    private ProducerActions<String, Str.StorageValue> storageProducer;\n+    private KafkaConsumer<String, Str.StorageValue> consumer;\n+    private Submitter<UUID> submitter;\n+\n+    void onConstruct(@Observes StartupEvent ev) {\n+        log.info(\"Using Kafka-SQL storage.\");\n+        // Start the Kafka Consumer thread\n+        consumer = createKafkaConsumer();\n+        startConsumerThread(consumer);\n+\n+        storageProducer = createKafkaProducer();\n+        submitter = new Submitter<UUID>(this::send);\n+    }\n+\n+    @PreDestroy\n+    void onDestroy() {\n+        stopped = true;\n+    }\n+\n+    private CompletableFuture<UUID> send(Str.StorageValue value) {\n+        UUID requestId = coordinator.createUUID();\n+        RecordHeader h = new RecordHeader(\"req\", requestId.toString().getBytes());\n+        ProducerRecord<String, Str.StorageValue> record = new ProducerRecord<>(\n+            topic,\n+            0,\n+            value.getArtifactId(), // MUST be set\n+            value,\n+            Collections.singletonList(h)\n+        );\n+        return storageProducer.apply(record).thenApply(rm -> requestId);\n+    }\n+\n+    /**\n+     * Start the KSQL Kafka consumer thread which is responsible for subscribing to the kafka topic,\n+     * consuming JournalRecord entries found on that topic, and applying those journal entries to\n+     * the internal data model.\n+     * @param consumer\n+     */\n+    private void startConsumerThread(final KafkaConsumer<String, Str.StorageValue> consumer) {\n+        log.info(\"Starting KSQL consumer thread on topic: {}\", topic);\n+        log.info(\"Bootstrap servers: \" + bootstrapServers);\n+        Runnable runner = () -> {\n+            log.info(\"KSQL consumer thread startup lag: {}\", startupLag);\n+\n+            try {\n+                // Startup lag\n+                try { Thread.sleep(startupLag); } catch (InterruptedException e) { }\n+\n+                log.info(\"Subscribing to {}\", topic);\n+\n+                // Subscribe to the journal topic\n+                Collection<String> topics = Collections.singleton(topic);\n+                consumer.subscribe(topics);\n+\n+                // Main consumer loop\n+                while (!stopped) {\n+                    final ConsumerRecords<String, Str.StorageValue> records = consumer.poll(Duration.ofMillis(pollTimeout));\n+                    if (records != null && !records.isEmpty()) {\n+                        log.debug(\"Consuming {} journal records.\", records.count());\n+                        records.forEach(record -> {\n+\n+                            UUID req = Optional.ofNullable(record.headers().headers(\"req\"))\n+                                .map(Iterable::iterator)\n+                                .map(it -> {\n+                                    return it.hasNext() ? it.next() : null;\n+                                })\n+                                .map(Header::value)\n+                                .map(String::new)\n+                                .map(UUID::fromString)\n+                                .orElse(null);\n+\n+                            String artifactId = record.key();\n+                            Str.StorageValue storageAction = record.value();\n+\n+                            // TODO instead of processing the journal record directly on the consumer thread, instead queue them and have *another* thread process the queue\n+                            kafkaSqlSink.processStorageAction(req, artifactId, storageAction);\n+                        });\n+                    }\n+                }\n+            } finally {\n+                consumer.close();\n+            }\n+        };\n+        stopped = false;\n+        Thread thread = new Thread(runner);\n+        thread.setDaemon(true);\n+        thread.setName(\"KSQL Kafka Consumer Thread\");\n+        thread.start();\n+    }\n+\n+    /**\n+     * Creates the Kafka producer.\n+     */\n+    private ProducerActions<String, Str.StorageValue> createKafkaProducer() {\n+        // TODO properties should be injected similar to StreamsRegistryConfiguration#storageProducer\n+        Properties props = new Properties();\n+\n+        // Configure kafka settings\n+        props.putIfAbsent(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n+        props.putIfAbsent(ProducerConfig.CLIENT_ID_CONFIG, \"Producer-\" + topic);\n+        props.putIfAbsent(ProducerConfig.ACKS_CONFIG, \"all\");\n+\n+        // Create the Kafka producer\n+        return new AsyncProducer<String, Str.StorageValue>(props,\n+                Serdes.String().serializer(),\n+                ProtoSerde.parsedWith(Str.StorageValue.parser()));\n+    }\n+\n+    /**\n+     * Creates the Kafka consumer.\n+     */\n+    private KafkaConsumer<String, Str.StorageValue> createKafkaConsumer() {\n+        // TODO properties should be injected similar to StreamsRegistryConfiguration#storageProducer\n+        Properties props = new Properties();\n+\n+        props.putIfAbsent(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n+        props.putIfAbsent(ConsumerConfig.GROUP_ID_CONFIG, UUID.randomUUID().toString());\n+        props.putIfAbsent(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"true\");\n+        props.putIfAbsent(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, \"1000\");\n+        props.putIfAbsent(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n+\n+        // Create the Kafka Consumer\n+        KafkaConsumer<String, Str.StorageValue> consumer = new KafkaConsumer<>(props,\n+                Serdes.String().deserializer(),\n+                ProtoSerde.parsedWith(Str.StorageValue.parser()));\n+        return consumer;\n+    }\n+\n+    private void updateArtifactState(ArtifactState currentState, String artifactId, Integer version, ArtifactState state) {\n+        ArtifactStateExt.applyState(\n+            s ->  {\n+                UUID reqId = ConcurrentUtil.get(submitter.submitState(artifactId, version.longValue(), s));\n+                coordinator.waitForResponse(reqId);\n+            },\n+            currentState,\n+            state\n+        );\n+    }\n+\n+    //TODO implement is Ready and is alive checking if the state is fully updated\n+\n+    @Override\n+    public void updateArtifactState(String artifactId, ArtifactState state) {\n+        ArtifactMetaDataDto metadata = sqlStorage.getArtifactMetaData(artifactId);\n+        updateArtifactState(metadata.getState(), artifactId, metadata.getVersion(), state);\n+    }\n+\n+    @Override\n+    public void updateArtifactState(String artifactId, ArtifactState state, Integer version) {\n+        ArtifactVersionMetaDataDto metadata = sqlStorage.getArtifactVersionMetaData(artifactId, version);\n+        updateArtifactState(metadata.getState(), artifactId, version, state);\n+    }\n+\n+    @Override\n+    public CompletionStage<ArtifactMetaDataDto> createArtifact(String artifactId, ArtifactType artifactType, ContentHandle content) throws ArtifactAlreadyExistsException, RegistryStorageException {\n+\n+        try {\n+            sqlStorage.getArtifactMetaData(artifactId);\n+            throw new ArtifactAlreadyExistsException(artifactId);\n+        } catch (ArtifactNotFoundException e) {\n+            //ignored\n+            //artifact does not exist, we can create it\n+        }\n+\n+        return submitter\n+                .submitArtifact(Str.ActionType.CREATE, artifactId, -1, artifactType, content.bytes())\n+                .thenCompose(reqId -> {\n+                    return (CompletionStage<ArtifactMetaDataDto>) coordinator.waitForResponse(reqId);\n+                });\n+    }\n+\n+    @Override\n+    public CompletionStage<ArtifactMetaDataDto> createArtifactWithMetadata(String artifactId, ArtifactType artifactType, \n+            ContentHandle content, EditableArtifactMetaDataDto metaData) throws ArtifactAlreadyExistsException, RegistryStorageException {\n+        return createArtifact(artifactId, artifactType, content)\n+            .thenCompose(amdd -> submitter.submitMetadata(Str.ActionType.UPDATE, artifactId, -1, metaData.getName(), metaData.getDescription(), metaData.getLabels(), metaData.getProperties())\n+                .thenApply(v -> DtoUtil.setEditableMetaDataInArtifact(amdd, metaData)));\n+    }\n+\n+    @Override\n+    public SortedSet<Long> deleteArtifact(String artifactId) throws ArtifactNotFoundException, RegistryStorageException {\n+\n+        //to verify artifact exists\n+        //TODO implement a low level storage api that provides methods like, exists, ...\n+        sqlStorage.getArtifactMetaData(artifactId);\n+\n+        UUID reqId = ConcurrentUtil.get(submitter.submitArtifact(Str.ActionType.DELETE, artifactId, -1, null, null));\n+        SortedSet<Long> versionIds = (SortedSet<Long>) coordinator.waitForResponse(reqId);\n+\n+        return versionIds;\n+    }\n+\n+    @Override\n+    public StoredArtifact getArtifact(String artifactId) throws ArtifactNotFoundException, RegistryStorageException {\n+        return sqlStorage.getArtifact(artifactId);\n+    }\n+\n+    @Override\n+    public CompletionStage<ArtifactMetaDataDto> updateArtifact(String artifactId, ArtifactType artifactType, ContentHandle content) throws ArtifactNotFoundException, RegistryStorageException {\n+\n+        try {\n+            sqlStorage.getArtifactMetaData(artifactId);\n+        } catch (ArtifactNotFoundException e) {\n+            throw e;\n+        }\n+\n+        return submitter", "originalCommit": "df0c0720564eb90b4167468327f19cbc4798e24c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc3MzM2Mw==", "url": "https://github.com/Apicurio/apicurio-registry/pull/1026#discussion_r529773363", "bodyText": "The version is created when the SQL layer creates the row in the versions table.  I believe the try-catch is purely a fail-fast check for when the artifact doesn't exist.\nThe way this will work is that a Kafka message with the artifactId and a \"create version\" action (and the content) will be published.  Then all replicas will consume that message and perform the action, which is to create a new version for the given artifactId in the DB.  At that point the version ID will be generated and communicated back to the original thread via the coordinator.\nDoes that explanation make sense?", "author": "EricWittmann", "createdAt": "2020-11-24T18:00:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTMwNDU4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTc3OTYwOQ==", "url": "https://github.com/Apicurio/apicurio-registry/pull/1026#discussion_r529779609", "bodyText": "Yes, it does. Thanks, now I see the query and its usage in the CommonSqlStatements.", "author": "carlesarnal", "createdAt": "2020-11-24T18:10:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTMwNDU4Ng=="}], "type": "inlineReview"}, {"oid": "8e43971ec8747efcc4e0bd9bc2a98a01eeaf466e", "url": "https://github.com/Apicurio/apicurio-registry/commit/8e43971ec8747efcc4e0bd9bc2a98a01eeaf466e", "message": "run integration tests and fix storage bug (#1028)", "committedDate": "2020-11-24T13:47:16Z", "type": "commit"}, {"oid": "3ea5801e522e2291c3b49fd6e989e6cc199f13b1", "url": "https://github.com/Apicurio/apicurio-registry/commit/3ea5801e522e2291c3b49fd6e989e6cc199f13b1", "message": "fix ksql storage - error create/update artifact with metadata (#1029)", "committedDate": "2020-11-24T15:48:31Z", "type": "commit"}, {"oid": "463252b201e1f8e8cb0c5622437490abbe80434f", "url": "https://github.com/Apicurio/apicurio-registry/commit/463252b201e1f8e8cb0c5622437490abbe80434f", "message": "update after some PR feedback", "committedDate": "2020-11-24T18:09:28Z", "type": "commit"}, {"oid": "9d01a5bad70f0b151eb3e5f9400b2192a7ab4678", "url": "https://github.com/Apicurio/apicurio-registry/commit/9d01a5bad70f0b151eb3e5f9400b2192a7ab4678", "message": "remove some debug methods", "committedDate": "2020-11-24T18:10:24Z", "type": "commit"}, {"oid": "2b2621e3361372ba3c76ca37a9e38769a5c62063", "url": "https://github.com/Apicurio/apicurio-registry/commit/2b2621e3361372ba3c76ca37a9e38769a5c62063", "message": "Merge remote-tracking branch 'upstream/master' into learning/kafka-sql", "committedDate": "2020-11-24T18:10:49Z", "type": "commit"}, {"oid": "dda622ace5e3b9d237b8111dbc8ef7dfc6d908bc", "url": "https://github.com/Apicurio/apicurio-registry/commit/dda622ace5e3b9d237b8111dbc8ef7dfc6d908bc", "message": "updated the perftest readme", "committedDate": "2020-11-24T18:12:38Z", "type": "commit"}]}