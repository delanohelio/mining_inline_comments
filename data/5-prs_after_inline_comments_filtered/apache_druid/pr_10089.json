{"pr_number": 10089, "pr_title": "Fix native batch range partition segment sizing", "pr_createdAt": "2020-06-27T05:12:56Z", "pr_url": "https://github.com/apache/druid/pull/10089", "timeline": [{"oid": "d437ffddfca9d536ea0798bb9859ce674941a93b", "url": "https://github.com/apache/druid/commit/d437ffddfca9d536ea0798bb9859ce674941a93b", "message": "Fix native batch range partition segment sizing\n\nFixes #10057.\n\nNative batch range partitioning was only considering the partition\ndimension value when grouping rows instead of using all of the row's\npartition values. Thus, for schemas with multiple dimensions, the rollup\nwas overestimated, which would cause too many dimension values to be\npacked into the same range partition. The resulting segments would then\nbe overly large (and not honor the target or max partition sizes).\n\nMain changes:\n\n- PartialDimensionDistributionTask: Consider all dimension values when\n  grouping row\n\n- RangePartitionMultiPhaseParallelIndexingTest: Regression test by\n  having input with rows that should roll up and rows that should not\n  roll up", "committedDate": "2020-06-27T04:01:52Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzIyMDg1MA==", "url": "https://github.com/apache/druid/pull/10089#discussion_r447220850", "bodyText": "I haven't fully looked into the logic of determineDistribution, but a warning: this isn't the right way to get the list of dimension names that will end up in the finished product, since it won't work properly in schemaless dimensions mode. So determineDistribution may need to accept the DimensionsSpec itself and do the same logic that the index generator code itself would do.\nWatch out for the queryGranularity as well, which is part of the rollup key. It looks like determineDistribution isn't currently inspecting it (it comes from granularitySpec.getQueryGranularity()).", "author": "gianm", "createdAt": "2020-06-29T20:03:23Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialDimensionDistributionTask.java", "diffHunk": "@@ -223,17 +224,19 @@ public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n \n     try (\n         CloseableIterator<InputRow> inputRowIterator = inputSourceReader.read();\n-        HandlingInputRowIterator iterator = new RangePartitionIndexTaskInputRowIteratorBuilder(partitionDimension, SKIP_NULL)\n-            .delegate(inputRowIterator)\n-            .granularitySpec(granularitySpec)\n-            .nullRowRunnable(IndexTaskInputRowIteratorBuilder.NOOP_RUNNABLE)\n-            .absentBucketIntervalConsumer(IndexTaskInputRowIteratorBuilder.NOOP_CONSUMER)\n-            .build()\n+        HandlingInputRowIterator iterator =\n+            new RangePartitionIndexTaskInputRowIteratorBuilder(partitionDimension, SKIP_NULL)\n+                .delegate(inputRowIterator)\n+                .granularitySpec(granularitySpec)\n+                .nullRowRunnable(IndexTaskInputRowIteratorBuilder.NOOP_RUNNABLE)\n+                .absentBucketIntervalConsumer(IndexTaskInputRowIteratorBuilder.NOOP_CONSUMER)\n+                .build()\n     ) {\n       Map<Interval, StringDistribution> distribution = determineDistribution(\n           iterator,\n           granularitySpec,\n           partitionDimension,\n+          dataSchema.getDimensionsSpec().getDimensionNames(),", "originalCommit": "d437ffddfca9d536ea0798bb9859ce674941a93b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzIzNDE2MA==", "url": "https://github.com/apache/druid/pull/10089#discussion_r447234160", "bodyText": "Maybe use Rows.toGroupKey?", "author": "gianm", "createdAt": "2020-06-29T20:28:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzIyMDg1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI1ODgzNA==", "url": "https://github.com/apache/druid/pull/10089#discussion_r447258834", "bodyText": "Changed to use Rows.toGroupKey(). The code was already using queryGranualrity as part of the rollup key.", "author": "ccaominh", "createdAt": "2020-06-29T21:16:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzIyMDg1MA=="}], "type": "inlineReview"}, {"oid": "45758c8eae13ba053b09e0757aeec0a7f4b7082e", "url": "https://github.com/apache/druid/commit/45758c8eae13ba053b09e0757aeec0a7f4b7082e", "message": "Use hadoop & native hash ingestion row group key", "committedDate": "2020-06-29T21:14:24Z", "type": "commit"}]}