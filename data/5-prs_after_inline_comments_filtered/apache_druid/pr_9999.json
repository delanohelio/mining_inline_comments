{"pr_number": 9999, "pr_title": "Optimize protobuf parsing for flatten data", "pr_createdAt": "2020-06-08T04:10:51Z", "pr_url": "https://github.com/apache/druid/pull/9999", "timeline": [{"oid": "396826119a39fe8efa5bcfcea8274d50925796a8", "url": "https://github.com/apache/druid/commit/396826119a39fe8efa5bcfcea8274d50925796a8", "message": "optimize for protobuf parsing", "committedDate": "2020-06-08T02:37:52Z", "type": "commit"}, {"oid": "cccd904827ef7e08f3d043b7544ff6ba4d779a12", "url": "https://github.com/apache/druid/commit/cccd904827ef7e08f3d043b7544ff6ba4d779a12", "message": "fix import error and maven dependency", "committedDate": "2020-06-08T08:26:53Z", "type": "commit"}, {"oid": "a7e5e23f39cf2d2306b7647bca28bac04423bb34", "url": "https://github.com/apache/druid/commit/a7e5e23f39cf2d2306b7647bca28bac04423bb34", "message": "add unit test in protobufInputrowParserTest for flatten data", "committedDate": "2020-06-08T12:04:51Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgyMzgyNg==", "url": "https://github.com/apache/druid/pull/9999#discussion_r439823826", "bodyText": "I suggest refactoring these modifications to reduce code duplications.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                Map<String, Object> record;\n          \n          \n            \n            \n          \n          \n            \n                if (parseSpec instanceof JSONParseSpec && ((JSONParseSpec) parseSpec).getFlattenSpec().getFields().isEmpty()) {\n          \n          \n            \n                  try {\n          \n          \n            \n                    DynamicMessage message = DynamicMessage.parseFrom(descriptor, ByteString.copyFrom(input));\n          \n          \n            \n                    record = CollectionUtils.mapKeys(message.getAllFields(), k -> k.getJsonName());\n          \n          \n            \n                  }\n          \n          \n            \n                  catch (InvalidProtocolBufferException ex) {\n          \n          \n            \n                    throw new ParseException(ex, \"Protobuf message could not be parsed\");\n          \n          \n            \n                  }\n          \n          \n            \n                } else {\n          \n          \n            \n                  try {\n          \n          \n            \n                    DynamicMessage message = DynamicMessage.parseFrom(descriptor, ByteString.copyFrom(input));\n          \n          \n            \n                    String json = JsonFormat.printer().print(message);\n          \n          \n            \n                    record = parser.parseToMap(json);\n          \n          \n            \n                  }\n          \n          \n            \n                  catch (InvalidProtocolBufferException e) {\n          \n          \n            \n                    throw new ParseException(e, \"Protobuf message could not be parsed\");\n          \n          \n            \n                  }\n          \n          \n            \n                try {\n          \n          \n            \n                  DynamicMessage message = DynamicMessage.parseFrom(descriptor, ByteString.copyFrom(input));\n          \n          \n            \n                }\n          \n          \n            \n                catch (InvalidProtocolBufferException ex) {\n          \n          \n            \n                  throw new ParseException(ex, \"Protobuf message could not be parsed\");\n          \n          \n            \n                }\n          \n          \n            \n            \n          \n          \n            \n                Map<String, Object> record;\n          \n          \n            \n                if (parseSpec instanceof JSONParseSpec && ((JSONParseSpec) parseSpec).getFlattenSpec().getFields().isEmpty()) {\n          \n          \n            \n                  record = CollectionUtils.mapKeys(message.getAllFields(), k -> k.getJsonName());\n          \n          \n            \n                } else {\n          \n          \n            \n                  String json = JsonFormat.printer().print(message);\n          \n          \n            \n                  record = parser.parseToMap(json);\n          \n          \n            \n                }", "author": "liran-funaro", "createdAt": "2020-06-14T12:15:53Z", "path": "extensions-core/protobuf-extensions/src/main/java/org/apache/druid/data/input/protobuf/ProtobufInputRowParser.java", "diffHunk": "@@ -100,16 +102,27 @@ void initDescriptor()\n       parser = parseSpec.makeParser();\n       initDescriptor();\n     }\n-    String json;\n-    try {\n-      DynamicMessage message = DynamicMessage.parseFrom(descriptor, ByteString.copyFrom(input));\n-      json = JsonFormat.printer().print(message);\n-    }\n-    catch (InvalidProtocolBufferException e) {\n-      throw new ParseException(e, \"Protobuf message could not be parsed\");\n+    Map<String, Object> record;\n+\n+    if (parseSpec instanceof JSONParseSpec && ((JSONParseSpec) parseSpec).getFlattenSpec().getFields().isEmpty()) {\n+      try {\n+        DynamicMessage message = DynamicMessage.parseFrom(descriptor, ByteString.copyFrom(input));\n+        record = CollectionUtils.mapKeys(message.getAllFields(), k -> k.getJsonName());\n+      }\n+      catch (InvalidProtocolBufferException ex) {\n+        throw new ParseException(ex, \"Protobuf message could not be parsed\");\n+      }\n+    } else {\n+      try {\n+        DynamicMessage message = DynamicMessage.parseFrom(descriptor, ByteString.copyFrom(input));\n+        String json = JsonFormat.printer().print(message);\n+        record = parser.parseToMap(json);\n+      }\n+      catch (InvalidProtocolBufferException e) {\n+        throw new ParseException(e, \"Protobuf message could not be parsed\");\n+      }", "originalCommit": "a7e5e23f39cf2d2306b7647bca28bac04423bb34", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgyNDA2Mg==", "url": "https://github.com/apache/druid/pull/9999#discussion_r439824062", "bodyText": "I don't think the benchmark class should have a main method. It seems useful for debugging, but I don't think it should exist in the master branch.", "author": "liran-funaro", "createdAt": "2020-06-14T12:19:36Z", "path": "benchmarks/src/test/java/org/apache/druid/benchmark/ProtobufParserBenchmark.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.benchmark;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.io.Files;\n+import org.apache.druid.common.config.NullHandling;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.impl.DimensionsSpec;\n+import org.apache.druid.data.input.impl.JSONParseSpec;\n+import org.apache.druid.data.input.impl.ParseSpec;\n+import org.apache.druid.data.input.impl.StringDimensionSchema;\n+import org.apache.druid.data.input.impl.TimestampSpec;\n+import org.apache.druid.data.input.protobuf.ProtobufInputRowParser;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldSpec;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldType;\n+import org.apache.druid.java.util.common.parsers.JSONPathSpec;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.infra.Blackhole;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.TimeUnit;\n+\n+@State(Scope.Benchmark)\n+@Fork(value = 1)\n+@Warmup(iterations = 10)\n+@Measurement(iterations = 25)\n+public class ProtobufParserBenchmark\n+{\n+  @Param({\"75000\"})\n+  private int rowsPerSegment;\n+\n+  private static final Logger log = new Logger(ProtobufParserBenchmark.class);\n+\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n+  private ParseSpec nestedParseSpec;\n+  private ProtobufInputRowParser nestedParser;\n+  private ParseSpec flattenParseSpec;\n+  private ProtobufInputRowParser flattenParser;\n+  private byte[] protoInputs;\n+  private String protoFilePath;\n+\n+  @Setup\n+  public void setup()\n+  {\n+    log.info(\"SETUP CALLED AT \" + +System.currentTimeMillis());\n+\n+    nestedParseSpec = new JSONParseSpec(\n+                new TimestampSpec(\"timestamp\", \"iso\", null),\n+                new DimensionsSpec(Lists.newArrayList(\n+                        new StringDimensionSchema(\"event\"),\n+                        new StringDimensionSchema(\"id\"),\n+                        new StringDimensionSchema(\"someOtherId\"),\n+                        new StringDimensionSchema(\"isValid\")\n+                ), null, null),\n+                new JSONPathSpec(\n+                        true,\n+                        Lists.newArrayList(\n+                                new JSONPathFieldSpec(JSONPathFieldType.ROOT, \"eventType\", \"eventType\"),\n+                                new JSONPathFieldSpec(JSONPathFieldType.PATH, \"foobar\", \"$.foo.bar\"),\n+                                new JSONPathFieldSpec(JSONPathFieldType.PATH, \"bar0\", \"$.bar[0].bar\")\n+                        )\n+                ),\n+                null,\n+                null\n+    );\n+\n+    flattenParseSpec = new JSONParseSpec(\n+            new TimestampSpec(\"timestamp\", \"iso\", null),\n+            new DimensionsSpec(Lists.newArrayList(\n+                    new StringDimensionSchema(\"event\"),\n+                    new StringDimensionSchema(\"id\"),\n+                    new StringDimensionSchema(\"someOtherId\"),\n+                    new StringDimensionSchema(\"isValid\")\n+            ), null, null),\n+            null,\n+            null,\n+            null\n+    );\n+\n+    protoFilePath = \"ProtoFile\";\n+    protoInputs = getProtoInputs(protoFilePath);\n+    nestedParser = new ProtobufInputRowParser(nestedParseSpec, \"prototest.desc\", \"ProtoTestEvent\");\n+    flattenParser = new ProtobufInputRowParser(flattenParseSpec, \"prototest.desc\", \"ProtoTestEvent\");\n+  }\n+\n+  @Benchmark\n+  @BenchmarkMode(Mode.AverageTime)\n+  @OutputTimeUnit(TimeUnit.MICROSECONDS)\n+  public void consumeFlattenData(Blackhole blackhole)\n+  {\n+    for (int i = 0; i < rowsPerSegment; i++) {\n+      InputRow row = flattenParser.parseBatch(ByteBuffer.wrap(protoInputs)).get(0);\n+      blackhole.consume(row);\n+    }\n+  }\n+\n+  @Benchmark\n+  @BenchmarkMode(Mode.AverageTime)\n+  @OutputTimeUnit(TimeUnit.MICROSECONDS)\n+  public void consumeNestedData(Blackhole blackhole)\n+  {\n+    for (int i = 0; i < rowsPerSegment; i++) {\n+      InputRow row = nestedParser.parseBatch(ByteBuffer.wrap(protoInputs)).get(0);\n+      blackhole.consume(row);\n+    }\n+\n+  }\n+  private byte[] getProtoInputs(String fileName)\n+  {\n+    String filePath = this.getClass().getClassLoader().getResource(fileName).getPath();\n+    byte[] bytes = null;\n+    try {\n+      File file = new File(filePath);\n+      bytes = new byte[(int) file.length()];\n+      bytes = Files.toByteArray(file);\n+    }\n+    catch (FileNotFoundException e) {\n+      log.error(\"Cannot find the file: \" + filePath);\n+      e.printStackTrace();\n+    }\n+    catch (IOException e) {\n+      e.printStackTrace();\n+    }\n+    return bytes;\n+  }\n+\n+  public static void main(String[] args) throws RunnerException\n+  {\n+    Options opt = new OptionsBuilder()\n+        .include(ProtobufParserBenchmark.class.getSimpleName())\n+        .build();\n+    new Runner(opt).run();", "originalCommit": "a7e5e23f39cf2d2306b7647bca28bac04423bb34", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDIxNjIwMQ==", "url": "https://github.com/apache/druid/pull/9999#discussion_r440216201", "bodyText": "Yes you're right~ It is just for debugging and should be removed.", "author": "xhl0726", "createdAt": "2020-06-15T14:27:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgyNDA2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgyNDQxMQ==", "url": "https://github.com/apache/druid/pull/9999#discussion_r439824411", "bodyText": "I wonder what is the effect of ByteBuffer.wrap() on the measured performance.\nIs there a reason the wrapping is done inside the loop and not in the setup phase?", "author": "liran-funaro", "createdAt": "2020-06-14T12:23:14Z", "path": "benchmarks/src/test/java/org/apache/druid/benchmark/ProtobufParserBenchmark.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.benchmark;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.io.Files;\n+import org.apache.druid.common.config.NullHandling;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.impl.DimensionsSpec;\n+import org.apache.druid.data.input.impl.JSONParseSpec;\n+import org.apache.druid.data.input.impl.ParseSpec;\n+import org.apache.druid.data.input.impl.StringDimensionSchema;\n+import org.apache.druid.data.input.impl.TimestampSpec;\n+import org.apache.druid.data.input.protobuf.ProtobufInputRowParser;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldSpec;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldType;\n+import org.apache.druid.java.util.common.parsers.JSONPathSpec;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.infra.Blackhole;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.TimeUnit;\n+\n+@State(Scope.Benchmark)\n+@Fork(value = 1)\n+@Warmup(iterations = 10)\n+@Measurement(iterations = 25)\n+public class ProtobufParserBenchmark\n+{\n+  @Param({\"75000\"})\n+  private int rowsPerSegment;\n+\n+  private static final Logger log = new Logger(ProtobufParserBenchmark.class);\n+\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n+  private ParseSpec nestedParseSpec;\n+  private ProtobufInputRowParser nestedParser;\n+  private ParseSpec flattenParseSpec;\n+  private ProtobufInputRowParser flattenParser;\n+  private byte[] protoInputs;\n+  private String protoFilePath;\n+\n+  @Setup\n+  public void setup()\n+  {\n+    log.info(\"SETUP CALLED AT \" + +System.currentTimeMillis());\n+\n+    nestedParseSpec = new JSONParseSpec(\n+                new TimestampSpec(\"timestamp\", \"iso\", null),\n+                new DimensionsSpec(Lists.newArrayList(\n+                        new StringDimensionSchema(\"event\"),\n+                        new StringDimensionSchema(\"id\"),\n+                        new StringDimensionSchema(\"someOtherId\"),\n+                        new StringDimensionSchema(\"isValid\")\n+                ), null, null),\n+                new JSONPathSpec(\n+                        true,\n+                        Lists.newArrayList(\n+                                new JSONPathFieldSpec(JSONPathFieldType.ROOT, \"eventType\", \"eventType\"),\n+                                new JSONPathFieldSpec(JSONPathFieldType.PATH, \"foobar\", \"$.foo.bar\"),\n+                                new JSONPathFieldSpec(JSONPathFieldType.PATH, \"bar0\", \"$.bar[0].bar\")\n+                        )\n+                ),\n+                null,\n+                null\n+    );\n+\n+    flattenParseSpec = new JSONParseSpec(\n+            new TimestampSpec(\"timestamp\", \"iso\", null),\n+            new DimensionsSpec(Lists.newArrayList(\n+                    new StringDimensionSchema(\"event\"),\n+                    new StringDimensionSchema(\"id\"),\n+                    new StringDimensionSchema(\"someOtherId\"),\n+                    new StringDimensionSchema(\"isValid\")\n+            ), null, null),\n+            null,\n+            null,\n+            null\n+    );\n+\n+    protoFilePath = \"ProtoFile\";\n+    protoInputs = getProtoInputs(protoFilePath);\n+    nestedParser = new ProtobufInputRowParser(nestedParseSpec, \"prototest.desc\", \"ProtoTestEvent\");\n+    flattenParser = new ProtobufInputRowParser(flattenParseSpec, \"prototest.desc\", \"ProtoTestEvent\");\n+  }\n+\n+  @Benchmark\n+  @BenchmarkMode(Mode.AverageTime)\n+  @OutputTimeUnit(TimeUnit.MICROSECONDS)\n+  public void consumeFlattenData(Blackhole blackhole)\n+  {\n+    for (int i = 0; i < rowsPerSegment; i++) {\n+      InputRow row = flattenParser.parseBatch(ByteBuffer.wrap(protoInputs)).get(0);", "originalCommit": "a7e5e23f39cf2d2306b7647bca28bac04423bb34", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDIyMTA2MQ==", "url": "https://github.com/apache/druid/pull/9999#discussion_r440221061", "bodyText": "Yeah your suggestion would make the loop clearer.  However, when I put the ByteBuffer.wrap() out of the loop, some parsing error occured. Since there is only one line in the input file ( I used the loop to monitor read many lines), the buffer would be empty after reading once. In another word, reading from a bytes.Buffer consumes the bytes that were read. This means if you try to read again, those will not be returned. When i = 0, it works. When i>0, it would report a parsing error due to the pos of the ByteBuffer allocated has been moved to the end. That's the reason why I put it in the loop. If you have any better solution to that, just tell me to make the loop easier to understand.", "author": "xhl0726", "createdAt": "2020-06-15T14:34:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgyNDQxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc1MDk3Ng==", "url": "https://github.com/apache/druid/pull/9999#discussion_r444750976", "bodyText": "I see. It makes sense. It could be solved by buff.pos(0), but since ByteBuffer.wrap() only instantiate a single object with O(1) complexity, I don't think it would make much of a difference.", "author": "liran-funaro", "createdAt": "2020-06-24T09:03:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgyNDQxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgyNDY4Ng==", "url": "https://github.com/apache/druid/pull/9999#discussion_r439824686", "bodyText": "Is this logging necessary? It seems useful for debugging, but I don't think it should exist in the master branch.", "author": "liran-funaro", "createdAt": "2020-06-14T12:26:47Z", "path": "benchmarks/src/test/java/org/apache/druid/benchmark/ProtobufParserBenchmark.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.benchmark;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.io.Files;\n+import org.apache.druid.common.config.NullHandling;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.impl.DimensionsSpec;\n+import org.apache.druid.data.input.impl.JSONParseSpec;\n+import org.apache.druid.data.input.impl.ParseSpec;\n+import org.apache.druid.data.input.impl.StringDimensionSchema;\n+import org.apache.druid.data.input.impl.TimestampSpec;\n+import org.apache.druid.data.input.protobuf.ProtobufInputRowParser;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldSpec;\n+import org.apache.druid.java.util.common.parsers.JSONPathFieldType;\n+import org.apache.druid.java.util.common.parsers.JSONPathSpec;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.infra.Blackhole;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.concurrent.TimeUnit;\n+\n+@State(Scope.Benchmark)\n+@Fork(value = 1)\n+@Warmup(iterations = 10)\n+@Measurement(iterations = 25)\n+public class ProtobufParserBenchmark\n+{\n+  @Param({\"75000\"})\n+  private int rowsPerSegment;\n+\n+  private static final Logger log = new Logger(ProtobufParserBenchmark.class);\n+\n+  static {\n+    NullHandling.initializeForTests();\n+  }\n+\n+  private ParseSpec nestedParseSpec;\n+  private ProtobufInputRowParser nestedParser;\n+  private ParseSpec flattenParseSpec;\n+  private ProtobufInputRowParser flattenParser;\n+  private byte[] protoInputs;\n+  private String protoFilePath;\n+\n+  @Setup\n+  public void setup()\n+  {\n+    log.info(\"SETUP CALLED AT \" + +System.currentTimeMillis());", "originalCommit": "a7e5e23f39cf2d2306b7647bca28bac04423bb34", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDIzMDM2OA==", "url": "https://github.com/apache/druid/pull/9999#discussion_r440230368", "bodyText": "Yes it can be removed. Thanks for pointing it out~", "author": "xhl0726", "createdAt": "2020-06-15T14:47:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTgyNDY4Ng=="}], "type": "inlineReview"}, {"oid": "95df02f8681cb66b3a0392d8dab64522e6aa4249", "url": "https://github.com/apache/druid/commit/95df02f8681cb66b3a0392d8dab64522e6aa4249", "message": "solve code duplication (remove the log and main())", "committedDate": "2020-06-16T12:51:35Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc0MDU4OA==", "url": "https://github.com/apache/druid/pull/9999#discussion_r444740588", "bodyText": "nit: unnecessary println, though there are others in this test file prior to this change so no worries", "author": "clintropolis", "createdAt": "2020-06-24T08:46:12Z", "path": "extensions-core/protobuf-extensions/src/test/java/org/apache/druid/data/input/protobuf/ProtobufInputRowParserTest.java", "diffHunk": "@@ -177,6 +191,45 @@ public void testParse() throws Exception\n     Assert.assertEquals(816.0F, row.getMetric(\"someLongColumn\").floatValue(), 0.0);\n   }\n \n+  @Test\n+  public void testParseFlattenData() throws Exception\n+  {\n+    //configure parser with desc file\n+    ProtobufInputRowParser parser = new ProtobufInputRowParser(flattenParseSpec, \"prototest.desc\", \"ProtoTestEvent\");\n+\n+    //create binary of proto test event\n+    DateTime dateTime = new DateTime(2012, 7, 12, 9, 30, ISOChronology.getInstanceUTC());\n+    ProtoTestEventWrapper.ProtoTestEvent event = ProtoTestEventWrapper.ProtoTestEvent.newBuilder()\n+            .setDescription(\"description\")\n+            .setEventType(ProtoTestEventWrapper.ProtoTestEvent.EventCategory.CATEGORY_ONE)\n+            .setId(4711L)\n+            .setIsValid(true)\n+            .setSomeOtherId(4712)\n+            .setTimestamp(dateTime.toString())\n+            .setSomeFloatColumn(47.11F)\n+            .setSomeIntColumn(815)\n+            .setSomeLongColumn(816L)\n+            .build();\n+\n+    ByteArrayOutputStream out = new ByteArrayOutputStream();\n+    event.writeTo(out);\n+\n+    InputRow row = parser.parseBatch(ByteBuffer.wrap(out.toByteArray())).get(0);\n+    System.out.println(row);", "originalCommit": "95df02f8681cb66b3a0392d8dab64522e6aa4249", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc0MjkwNQ==", "url": "https://github.com/apache/druid/pull/9999#discussion_r444742905", "bodyText": "nit: should this be named flatParseSpec since it is for flat data and will not flatten the data since it has a null flattenSpec?", "author": "clintropolis", "createdAt": "2020-06-24T08:49:59Z", "path": "extensions-core/protobuf-extensions/src/test/java/org/apache/druid/data/input/protobuf/ProtobufInputRowParserTest.java", "diffHunk": "@@ -76,6 +77,19 @@ public void setUp()\n         null\n     );\n \n+    flattenParseSpec = new JSONParseSpec(", "originalCommit": "95df02f8681cb66b3a0392d8dab64522e6aa4249", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc5MjUyMA==", "url": "https://github.com/apache/druid/pull/9999#discussion_r444792520", "bodyText": "yeah you're right.", "author": "xhl0726", "createdAt": "2020-06-24T10:17:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc0MjkwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc0MzUzNw==", "url": "https://github.com/apache/druid/pull/9999#discussion_r444743537", "bodyText": "nit: same suggestion about naming, should this be testParseFlatData since it isn't actively flattening nested data?", "author": "clintropolis", "createdAt": "2020-06-24T08:51:00Z", "path": "extensions-core/protobuf-extensions/src/test/java/org/apache/druid/data/input/protobuf/ProtobufInputRowParserTest.java", "diffHunk": "@@ -177,6 +191,45 @@ public void testParse() throws Exception\n     Assert.assertEquals(816.0F, row.getMetric(\"someLongColumn\").floatValue(), 0.0);\n   }\n \n+  @Test\n+  public void testParseFlattenData() throws Exception", "originalCommit": "95df02f8681cb66b3a0392d8dab64522e6aa4249", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc5NjkxNw==", "url": "https://github.com/apache/druid/pull/9999#discussion_r444796917", "bodyText": "Your suggestion makes sense. I'll fix it soon.", "author": "xhl0726", "createdAt": "2020-06-24T10:25:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc0MzUzNw=="}], "type": "inlineReview"}, {"oid": "178191f0a45d24256952aeaa461929413e234c91", "url": "https://github.com/apache/druid/commit/178191f0a45d24256952aeaa461929413e234c91", "message": "rename 'flatten' to 'flat' to make it clearer", "committedDate": "2020-06-24T10:21:39Z", "type": "commit"}]}