{"pr_number": 9356, "pr_title": "Add Azure config options for segment prefix and max listing length", "pr_createdAt": "2020-02-13T01:53:16Z", "pr_url": "https://github.com/apache/druid/pull/9356", "timeline": [{"oid": "abfbead11446c3b0b9be6e71592e2808de0e1592", "url": "https://github.com/apache/druid/commit/abfbead11446c3b0b9be6e71592e2808de0e1592", "message": "Add Azure config options for segment prefix and max listing length\n\nAdded configuration options to allow the user to specify the prefix\nwithin the segment container to store the segment files. Also\nadded a configuration option to allow the user to specify the\nmaximum number of input files to stream for each iteration.", "committedDate": "2020-02-13T01:50:09Z", "type": "commit"}, {"oid": "9dcb61e6917fad6ef0e4bd43ec7c63c6dc7c3e66", "url": "https://github.com/apache/druid/commit/9dcb61e6917fad6ef0e4bd43ec7c63c6dc7c3e66", "message": "* Fix test failures", "committedDate": "2020-02-13T23:05:52Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgzNDg2OQ==", "url": "https://github.com/apache/druid/pull/9356#discussion_r380834869", "bodyText": "Why Nonnull when the previous one is annotated with 'NotNull'?\nAlso is prefix a required config? Why is assigned to an empty string?\nPerhaps using a @JsonCreator constructor with Precondition checks will make it clearer what is required in each field\n @JsonCreator\n  public AzureDataSegmentConfig(\n      ...\n      @JsonProperty(\"prefix\") String prefix)\n  {\n    this.prefix = Preconditions.checkState(!StringUtils.isEmpty(prefix), \"prefix must be non empty\");\n    ...\n  }\n\nThen you don't need all the setters", "author": "suneet-s", "createdAt": "2020-02-18T17:49:35Z", "path": "extensions-contrib/azure-extensions/src/main/java/org/apache/druid/storage/azure/AzureDataSegmentConfig.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.storage.azure;\n+\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import javax.annotation.Nonnull;\n+import javax.validation.constraints.Min;\n+import javax.validation.constraints.NotNull;\n+\n+/**\n+ * Stores the configuration for segments written to Azure deep storage\n+ */\n+public class AzureDataSegmentConfig\n+{\n+  @JsonProperty\n+  @NotNull\n+  private String container;\n+\n+  @JsonProperty\n+  @Nonnull", "originalCommit": "9dcb61e6917fad6ef0e4bd43ec7c63c6dc7c3e66", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDg0NDM0Nw==", "url": "https://github.com/apache/druid/pull/9356#discussion_r380844347", "bodyText": "prefix is not required. Before adding this option segments were written to the root directory within the segment container specified, in a directory named after the datasource. Do we want to change the behavior here and specify a non empty default prefix? I'm not sure how this change would affect users already using the azure extension whose data is already written, will we not be able to find the segment data in this case?\nI will fix to @NotNull", "author": "zachjsh", "createdAt": "2020-02-18T18:07:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgzNDg2OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTU4MzgyNQ==", "url": "https://github.com/apache/druid/pull/9356#discussion_r381583825", "bodyText": "fixed", "author": "zachjsh", "createdAt": "2020-02-19T22:29:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgzNDg2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgzNTk5MA==", "url": "https://github.com/apache/druid/pull/9356#discussion_r380835990", "bodyText": "What if prefix ends with a / Is there a util that will build the path with only one separator at the end? Is there any harm if the path ends with two /", "author": "suneet-s", "createdAt": "2020-02-18T17:51:38Z", "path": "extensions-contrib/azure-extensions/src/main/java/org/apache/druid/storage/azure/AzureDataSegmentPusher.java", "diffHunk": "@@ -70,11 +73,12 @@ public String getPathForHadoop(String dataSource)\n   public String getPathForHadoop()\n   {\n     String hadoopPath = StringUtils.format(\n-        \"%s://%s@%s.%s/\",\n-        AzureDataSegmentPuller.AZURE_STORAGE_HADOOP_PROTOCOL,\n-        config.getContainer(),\n-        config.getAccount(),\n-        AzureDataSegmentPuller.AZURE_STORAGE_HOST_ADDRESS\n+        \"%s://%s@%s.%s/%s\",\n+        AzureUtils.AZURE_STORAGE_HADOOP_PROTOCOL,\n+        segmentConfig.getContainer(),\n+        accountConfig.getAccount(),\n+        AzureUtils.AZURE_STORAGE_HOST_ADDRESS,\n+        segmentConfig.getPrefix().isEmpty() ? \"\" : segmentConfig.getPrefix() + '/'", "originalCommit": "9dcb61e6917fad6ef0e4bd43ec7c63c6dc7c3e66", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTU4MzkyMQ==", "url": "https://github.com/apache/druid/pull/9356#discussion_r381583921", "bodyText": "good catch! fixed", "author": "zachjsh", "createdAt": "2020-02-19T22:29:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgzNTk5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgzNjg3MA==", "url": "https://github.com/apache/druid/pull/9356#discussion_r380836870", "bodyText": "org.apache.commons.lang.StringUtils.isEmpty(prefix)", "author": "suneet-s", "createdAt": "2020-02-18T17:53:16Z", "path": "extensions-contrib/azure-extensions/src/main/java/org/apache/druid/storage/azure/AzureDataSegmentPusher.java", "diffHunk": "@@ -85,7 +89,10 @@ public String getPathForHadoop()\n   @Override\n   public String getStorageDir(DataSegment dataSegment, boolean useUniquePath)\n   {\n+    String prefix = segmentConfig.getPrefix();\n+    boolean prefixIsNullOrEmpty = (prefix == null || prefix.isEmpty());", "originalCommit": "9dcb61e6917fad6ef0e4bd43ec7c63c6dc7c3e66", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTU4NDA0Ng==", "url": "https://github.com/apache/druid/pull/9356#discussion_r381584046", "bodyText": "fixed", "author": "zachjsh", "createdAt": "2020-02-19T22:30:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgzNjg3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgzODA5NA==", "url": "https://github.com/apache/druid/pull/9356#discussion_r380838094", "bodyText": "test for unraveling a stacktrace. Should we check an unlimited depth?\nThis also changes the current behavior where if the top level throwable was a \"retryable\" exception, we'd retry, but with this change if a StorageException is caused by a RuntimeException we won't retry. Is this intentional?", "author": "suneet-s", "createdAt": "2020-02-18T17:55:35Z", "path": "extensions-contrib/azure-extensions/src/main/java/org/apache/druid/storage/azure/AzureUtils.java", "diffHunk": "@@ -39,16 +39,25 @@\n   @VisibleForTesting\n   static final String AZURE_STORAGE_HOST_ADDRESS = \"blob.core.windows.net\";\n \n+  // The azure storage hadoop access pattern is:\n+  // wasb[s]://<containername>@<accountname>.blob.core.windows.net/<path>\n+  // (from https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-use-blob-storage)\n+  static final String AZURE_STORAGE_HADOOP_PROTOCOL = \"wasbs\";\n+\n   public static final Predicate<Throwable> AZURE_RETRY = e -> {\n-    if (e instanceof URISyntaxException) {\n+    Throwable t = e;\n+    for (Throwable t2 = e.getCause(); t2 != null; t2 = t2.getCause()) {\n+      t = t2;\n+    }", "originalCommit": "9dcb61e6917fad6ef0e4bd43ec7c63c6dc7c3e66", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk1OTY3Mw==", "url": "https://github.com/apache/druid/pull/9356#discussion_r380959673", "bodyText": "I think the below if clauses should be checked in the above for loop.", "author": "jihoonson", "createdAt": "2020-02-18T21:57:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgzODA5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTU4NDEwNg==", "url": "https://github.com/apache/druid/pull/9356#discussion_r381584106", "bodyText": "fixed", "author": "zachjsh", "createdAt": "2020-02-19T22:30:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgzODA5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDg0NDAxMQ==", "url": "https://github.com/apache/druid/pull/9356#discussion_r380844011", "bodyText": "nit: looks like this is repeated in multiple tests, maybe move to a helper function?", "author": "suneet-s", "createdAt": "2020-02-18T18:07:10Z", "path": "extensions-core/google-extensions/src/test/java/org/apache/druid/data/input/google/GoogleCloudStorageInputSourceTest.java", "diffHunk": "@@ -169,14 +175,18 @@ public void testWithPrefixesSplit() throws IOException\n   public void testReader() throws IOException\n   {\n     EasyMock.reset(STORAGE);\n+    EasyMock.reset(CONFIG);\n     addExpectedPrefixObjects(PREFIXES.get(0), ImmutableList.of(EXPECTED_URIS.get(0)));\n     addExpectedGetObjectMock(EXPECTED_URIS.get(0));\n     addExpectedPrefixObjects(PREFIXES.get(1), ImmutableList.of(EXPECTED_URIS.get(1)));\n     addExpectedGetObjectMock(EXPECTED_URIS.get(1));\n+    EasyMock.expect(CONFIG.getMaxListingLength()).andReturn(EXPECTED_MAX_LISTING_LENGTH);\n     EasyMock.replay(STORAGE);\n+    EasyMock.replay(CONFIG);", "originalCommit": "9dcb61e6917fad6ef0e4bd43ec7c63c6dc7c3e66", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDg0NDQ0Nw==", "url": "https://github.com/apache/druid/pull/9356#discussion_r380844447", "bodyText": "nit: MAX_LISTING_LENGTH since we're mocking the maxListingLength() to this value", "author": "suneet-s", "createdAt": "2020-02-18T18:07:58Z", "path": "extensions-core/google-extensions/src/test/java/org/apache/druid/data/input/google/GoogleCloudStorageInputSourceTest.java", "diffHunk": "@@ -66,8 +67,9 @@\n \n public class GoogleCloudStorageInputSourceTest extends InitializedNullHandlingTest\n {\n-  private static final long EXPECTED_MAX_LISTING_LENGTH = 1024L;\n+  private static final int EXPECTED_MAX_LISTING_LENGTH = 10;", "originalCommit": "9dcb61e6917fad6ef0e4bd43ec7c63c6dc7c3e66", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTU4NDE1OA==", "url": "https://github.com/apache/druid/pull/9356#discussion_r381584158", "bodyText": "fixed", "author": "zachjsh", "createdAt": "2020-02-19T22:30:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDg0NDQ0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk1MjU5NQ==", "url": "https://github.com/apache/druid/pull/9356#discussion_r380952595", "bodyText": "Should the log level be error instead of warn?", "author": "jihoonson", "createdAt": "2020-02-18T21:43:16Z", "path": "extensions-contrib/azure-extensions/src/main/java/org/apache/druid/storage/azure/AzureByteSource.java", "diffHunk": "@@ -63,12 +63,12 @@ public InputStream openStream(long offset) throws IOException\n       return azureStorage.getBlobInputStream(offset, containerName, blobPath);\n     }\n     catch (StorageException | URISyntaxException e) {\n-      log.warn(\"Exception when opening stream to azure resource, containerName: %s, blobPath: %s, Error: %s\",\n-               containerName, blobPath, e.getMessage()\n-      );\n       if (AzureUtils.AZURE_RETRY.apply(e)) {\n         throw new IOException(\"Recoverable exception\", e);\n       }\n+      log.warn(\"Exception when opening stream to azure resource, containerName: %s, blobPath: %s, Error: %s\",", "originalCommit": "9dcb61e6917fad6ef0e4bd43ec7c63c6dc7c3e66", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTU4NDIzMg==", "url": "https://github.com/apache/druid/pull/9356#discussion_r381584232", "bodyText": "fixed", "author": "zachjsh", "createdAt": "2020-02-19T22:30:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk1MjU5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk1NTYxNQ==", "url": "https://github.com/apache/druid/pull/9356#discussion_r380955615", "bodyText": "I think this should be in a separate class rather than being in the class for deep storage configuration. I would suggest to add a new class AzureReadConfig (I think there could be a better name) that has the new configuration only, so that we can add more read-related configurations in the future.", "author": "jihoonson", "createdAt": "2020-02-18T21:49:04Z", "path": "extensions-contrib/azure-extensions/src/main/java/org/apache/druid/storage/azure/AzureDataSegmentConfig.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.storage.azure;\n+\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import javax.annotation.Nonnull;\n+import javax.validation.constraints.Min;\n+import javax.validation.constraints.NotNull;\n+\n+/**\n+ * Stores the configuration for segments written to Azure deep storage\n+ */\n+public class AzureDataSegmentConfig\n+{\n+  @JsonProperty\n+  @NotNull\n+  private String container;\n+\n+  @JsonProperty\n+  @Nonnull\n+  private String prefix = \"\";\n+\n+  @JsonProperty\n+  @Min(1)\n+  private int maxListingLength = 1024;", "originalCommit": "9dcb61e6917fad6ef0e4bd43ec7c63c6dc7c3e66", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk2MTQ3Mg==", "url": "https://github.com/apache/druid/pull/9356#discussion_r380961472", "bodyText": "Same for other cloud storage types.", "author": "jihoonson", "createdAt": "2020-02-18T22:00:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk1NTYxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk3ODU5Nw==", "url": "https://github.com/apache/druid/pull/9356#discussion_r380978597", "bodyText": "Also please add docs for the new configurations.", "author": "jihoonson", "createdAt": "2020-02-18T22:40:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk1NTYxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTAwOTI0Mw==", "url": "https://github.com/apache/druid/pull/9356#discussion_r381009243", "bodyText": "How about AzureInputDataConfig? And similar classes for AWS and Google", "author": "zachjsh", "createdAt": "2020-02-19T00:09:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk1NTYxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTU4NDI5Mg==", "url": "https://github.com/apache/druid/pull/9356#discussion_r381584292", "bodyText": "fixed", "author": "zachjsh", "createdAt": "2020-02-19T22:30:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk1NTYxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk2MDQ0NQ==", "url": "https://github.com/apache/druid/pull/9356#discussion_r380960445", "bodyText": "This variable is not used anymore.", "author": "jihoonson", "createdAt": "2020-02-18T21:58:51Z", "path": "extensions-core/google-extensions/src/main/java/org/apache/druid/data/input/google/GoogleCloudStorageInputSource.java", "diffHunk": "@@ -43,17 +44,20 @@\n   private static final int MAX_LISTING_LENGTH = 1024;", "originalCommit": "9dcb61e6917fad6ef0e4bd43ec7c63c6dc7c3e66", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTU4NDM2MA==", "url": "https://github.com/apache/druid/pull/9356#discussion_r381584360", "bodyText": "fixed", "author": "zachjsh", "createdAt": "2020-02-19T22:30:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk2MDQ0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk2OTMyNQ==", "url": "https://github.com/apache/druid/pull/9356#discussion_r380969325", "bodyText": "MAX_LISTING_LENGTH is defined in the parent class (CloudObjectInputSource) and is not used anymore. Please remove it.", "author": "jihoonson", "createdAt": "2020-02-18T22:17:56Z", "path": "extensions-core/s3-extensions/src/main/java/org/apache/druid/data/input/s3/S3InputSource.java", "diffHunk": "@@ -87,6 +91,6 @@ public String toString()\n \n   private Iterable<S3ObjectSummary> getIterableObjectsFromPrefixes()\n   {\n-    return () -> S3Utils.objectSummaryIterator(s3Client, getPrefixes(), MAX_LISTING_LENGTH);\n+    return () -> S3Utils.objectSummaryIterator(s3Client, getPrefixes(), segmentPusherConfig.getMaxListingLength());", "originalCommit": "9dcb61e6917fad6ef0e4bd43ec7c63c6dc7c3e66", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTU4NDQzMA==", "url": "https://github.com/apache/druid/pull/9356#discussion_r381584430", "bodyText": "fixed", "author": "zachjsh", "createdAt": "2020-02-19T22:30:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk2OTMyNQ=="}], "type": "inlineReview"}, {"oid": "7c27c2605bfacfaf3dd5c39ec3193a3b38b1495e", "url": "https://github.com/apache/druid/commit/7c27c2605bfacfaf3dd5c39ec3193a3b38b1495e", "message": "* Address review comments", "committedDate": "2020-02-19T22:27:59Z", "type": "commit"}, {"oid": "4b3e9f31f63cec172c051612b0e98dad2ab2ec1e", "url": "https://github.com/apache/druid/commit/4b3e9f31f63cec172c051612b0e98dad2ab2ec1e", "message": "* add dependency explicitly to pom", "committedDate": "2020-02-20T00:57:09Z", "type": "commit"}, {"oid": "b1e46e1e0cf4520416b9e515d3becf9b4b3930b9", "url": "https://github.com/apache/druid/commit/b1e46e1e0cf4520416b9e515d3becf9b4b3930b9", "message": "* update docs", "committedDate": "2020-02-21T00:17:48Z", "type": "commit"}, {"oid": "6725f96ca7bab59afae144153b4602e6d224475e", "url": "https://github.com/apache/druid/commit/6725f96ca7bab59afae144153b4602e6d224475e", "message": "* Address review comments", "committedDate": "2020-02-21T18:58:05Z", "type": "commit"}, {"oid": "187a6f9e9c83997b8cab4fbf3ace22db0a465ad6", "url": "https://github.com/apache/druid/commit/187a6f9e9c83997b8cab4fbf3ace22db0a465ad6", "message": "* Address review comments", "committedDate": "2020-02-21T19:12:47Z", "type": "commit"}]}