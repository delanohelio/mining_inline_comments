{"pr_number": 10213, "pr_title": "Avoid deletion of load/drop entry from CuratorLoadQueuePeon in case of load timeout", "pr_createdAt": "2020-07-24T19:03:21Z", "pr_url": "https://github.com/apache/druid/pull/10213", "timeline": [{"oid": "4e0a9955d006b87de03afd1abc2ff39e3665dec5", "url": "https://github.com/apache/druid/commit/4e0a9955d006b87de03afd1abc2ff39e3665dec5", "message": "Skip queue removal on timeout", "committedDate": "2020-07-24T18:40:57Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTAyMjk4OQ==", "url": "https://github.com/apache/druid/pull/10213#discussion_r581022989", "bodyText": "I think it would be worth clarifying this log message to indicate that for load operations, that while the coordinator has given up, the historical might still process and load the requested segments. Maybe something like \"Load segments operation timed out, %s was never removed! Abandoning attempt, (but these segments might still be loaded)\". I guess it would need to adjust message based on whether it was a load or drop.", "author": "clintropolis", "createdAt": "2021-02-23T13:12:53Z", "path": "server/src/main/java/org/apache/druid/server/coordinator/CuratorLoadQueuePeon.java", "diffHunk": "@@ -282,14 +297,14 @@ public void run()\n           () -> {\n             try {\n               if (curator.checkExists().forPath(path) != null) {\n-                failAssign(segmentHolder, new ISE(\"%s was never removed! Failing this operation!\", path));\n+                failAssign(segmentHolder, true, new ISE(\"%s was never removed! Failing this operation!\", path));", "originalCommit": "4e0a9955d006b87de03afd1abc2ff39e3665dec5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Njc3MDMyNQ==", "url": "https://github.com/apache/druid/pull/10213#discussion_r586770325", "bodyText": "I've modified the message here. Please let me know if this works.", "author": "a2l007", "createdAt": "2021-03-03T20:52:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTAyMjk4OQ=="}], "type": "inlineReview"}, {"oid": "92154de91a287368521f99e498ddd2c98d6a756a", "url": "https://github.com/apache/druid/commit/92154de91a287368521f99e498ddd2c98d6a756a", "message": "Merge branch 'master' of https://github.com/druid-io/druid into zkerror", "committedDate": "2021-03-03T17:49:44Z", "type": "commit"}, {"oid": "6c06ef2f3e6f96ca41743aacdf3dba6af0453cfd", "url": "https://github.com/apache/druid/commit/6c06ef2f3e6f96ca41743aacdf3dba6af0453cfd", "message": "Clarify error", "committedDate": "2021-03-03T20:48:13Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTExNjMzMQ==", "url": "https://github.com/apache/druid/pull/10213#discussion_r589116331", "bodyText": "I'm not sure why we don't emit exceptions currently (using EmittingLogger.makeAlert()), but should we? At least for the segment loading timeout error, it would be nice to emit those errors so that cluster operators can notice there is something going wrong with segment loading.", "author": "jihoonson", "createdAt": "2021-03-08T00:50:20Z", "path": "server/src/main/java/org/apache/druid/server/coordinator/CuratorLoadQueuePeon.java", "diffHunk": "@@ -361,21 +386,33 @@ private void entryRemoved(SegmentHolder segmentHolder, String path)\n     );\n   }\n \n-  private void failAssign(SegmentHolder segmentHolder)\n+  private void failAssign(SegmentHolder segmentHolder, boolean handleTimeout)\n   {\n-    failAssign(segmentHolder, null);\n+    failAssign(segmentHolder, handleTimeout, null);\n   }\n \n-  private void failAssign(SegmentHolder segmentHolder, Exception e)\n+  private void failAssign(SegmentHolder segmentHolder, boolean handleTimeout, Exception e)\n   {\n     if (e != null) {\n       log.error(e, \"Server[%s], throwable caught when submitting [%s].\", basePath, segmentHolder);", "originalCommit": "6c06ef2f3e6f96ca41743aacdf3dba6af0453cfd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MDUyODY4MA==", "url": "https://github.com/apache/druid/pull/10213#discussion_r590528680", "bodyText": "Alerting sounds like a good idea, but my concern is that since the alert would happen per segment, a slowness on the historical side can generate a large number of alerts for a fairly large cluster. What do you think?", "author": "a2l007", "createdAt": "2021-03-09T16:33:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTExNjMzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MDUyOTAzNA==", "url": "https://github.com/apache/druid/pull/10213#discussion_r590529034", "bodyText": "Also as a followup PR I was planning to add the timedOut segment list to the /druid/coordinator/v1/loadqueue along with some docs about its usage in understanding the cluster behavior.", "author": "a2l007", "createdAt": "2021-03-09T16:34:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTExNjMzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDU2MjYzMw==", "url": "https://github.com/apache/druid/pull/10213#discussion_r594562633", "bodyText": "Alerting sounds like a good idea, but my concern is that since the alert would happen per segment, a slowness on the historical side can generate a large number of alerts for a fairly large cluster. What do you think?\n\nI think it's a valid concern. We may be able to emit those exceptions in bulk if they are thrown in a short time frame. I believe this should be done in a separate PR even if we want, and thus my comment is not a blocker for this PR.\n\nAlso as a followup PR I was planning to add the timedOut segment list to the /druid/coordinator/v1/loadqueue along with some docs about its usage in understanding the cluster behavior.\n\nThanks. It sounds good to me.", "author": "jihoonson", "createdAt": "2021-03-15T17:57:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTExNjMzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTExNjU1Mg==", "url": "https://github.com/apache/druid/pull/10213#discussion_r589116552", "bodyText": "loadingSegments is not just a set of segments loading anymore. Please add some javadoc in SegmentReplicantLookup about this.", "author": "jihoonson", "createdAt": "2021-03-08T00:51:51Z", "path": "server/src/main/java/org/apache/druid/server/coordinator/SegmentReplicantLookup.java", "diffHunk": "@@ -59,7 +59,10 @@ public static SegmentReplicantLookup make(DruidCluster cluster)\n           if (numReplicants == null) {\n             numReplicants = 0;\n           }\n-          loadingSegments.put(segment.getId(), server.getTier(), numReplicants + 1);\n+          // Timed out segments need to be replicated in another server for faster availability\n+          if (!serverHolder.getPeon().getTimedOutSegments().contains(segment)) {\n+            loadingSegments.put(segment.getId(), server.getTier(), numReplicants + 1);", "originalCommit": "6c06ef2f3e6f96ca41743aacdf3dba6af0453cfd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTEyMDIxMg==", "url": "https://github.com/apache/druid/pull/10213#discussion_r589120212", "bodyText": "As @himanshug pointed out in #10193 (comment), there could be two types of slow segment loading.\n\nThere are a few historicals being slow in segment loading in the cluster. This can be caused by unbalanced load queues or some intermittent failures.\nHistoricals are OK, but ingestion might outpace the ability to load segments.\n\nThis particular change in SegmentReplicantLookup could help in the former case, but make things worse in the latter case. In an extreme case, all historicals could have the same set of timed-out segments in their load queue. This might be still OK though, because, if that's the case, Druid cannot get out of that state by itself anyway. The system administrator should add more historicals or use more threads for parallel segment loading. However, we should provide relevant data so that system administrators can tell what's happening. I left another comment about emitting exceptions to provide such data.", "author": "jihoonson", "createdAt": "2021-03-08T01:14:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTExNjU1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MDUzNTYyMQ==", "url": "https://github.com/apache/druid/pull/10213#discussion_r590535621", "bodyText": "@jihoonson @himanshug Would it make sense to make the replication behavior user configurable?  We could have a dynamic config like replicateAfterLoadTimeout which would control whether the segments would be attempted to be replicated to a different historical in case of a load timeout to the current historical. The default could be true but a cluster operator can set this to false if they wish to avoid the additional churn and know the historicals are OK and it would eventually load the segments.", "author": "a2l007", "createdAt": "2021-03-09T16:41:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTExNjU1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5MTg0NTIyOQ==", "url": "https://github.com/apache/druid/pull/10213#discussion_r591845229", "bodyText": "Adding a config seems reasonable to me \ud83d\udc4d", "author": "clintropolis", "createdAt": "2021-03-10T20:23:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTExNjU1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDU2Mjk4NQ==", "url": "https://github.com/apache/druid/pull/10213#discussion_r594562985", "bodyText": "It sounds good to me too.", "author": "jihoonson", "createdAt": "2021-03-15T17:58:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTExNjU1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NTQ3MTE2Mg==", "url": "https://github.com/apache/druid/pull/10213#discussion_r595471162", "bodyText": "Added a config. I've set replicateAfterLoadTimeout to false as the default I feel it might be better to preserve the existing behaviour and admins need to be aware of this property's behavior before setting it to true. Let me know what you think.", "author": "a2l007", "createdAt": "2021-03-16T19:15:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTExNjU1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NjI4NTc5Ng==", "url": "https://github.com/apache/druid/pull/10213#discussion_r596285796", "bodyText": "It sounds good to me to preserve the existing behavior by default.", "author": "jihoonson", "createdAt": "2021-03-17T18:33:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTExNjU1Mg=="}], "type": "inlineReview"}, {"oid": "a56f5bf98ebcb35fd71f01bbfc0a7bd160c0e330", "url": "https://github.com/apache/druid/commit/a56f5bf98ebcb35fd71f01bbfc0a7bd160c0e330", "message": "Merge branch 'master' of https://github.com/druid-io/druid into zkerror", "committedDate": "2021-03-09T15:09:04Z", "type": "commit"}, {"oid": "99f249e3d4228a96e8d58dc6fbed97faabc28b15", "url": "https://github.com/apache/druid/commit/99f249e3d4228a96e8d58dc6fbed97faabc28b15", "message": "Merge branch 'master' of https://github.com/druid-io/druid into zkerror", "committedDate": "2021-03-15T16:57:43Z", "type": "commit"}, {"oid": "9dbf5b56fbfdbe36a2e5dbbb0fe037808d9b3d1f", "url": "https://github.com/apache/druid/commit/9dbf5b56fbfdbe36a2e5dbbb0fe037808d9b3d1f", "message": "Add new config to control replication", "committedDate": "2021-03-16T19:06:13Z", "type": "commit"}]}