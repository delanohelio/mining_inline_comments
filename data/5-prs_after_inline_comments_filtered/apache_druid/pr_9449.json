{"pr_number": 9449, "pr_title": "Add Sql InputSource", "pr_createdAt": "2020-03-02T21:31:01Z", "pr_url": "https://github.com/apache/druid/pull/9449", "timeline": [{"oid": "64b24758bb91ff2174e80e24cd068b3987cc2ca3", "url": "https://github.com/apache/druid/commit/64b24758bb91ff2174e80e24cd068b3987cc2ca3", "message": "Add Sql InputSource", "committedDate": "2020-03-02T21:26:06Z", "type": "commit"}, {"oid": "61f62fe01a64534a485b0f8d9176ca6f18186b55", "url": "https://github.com/apache/druid/commit/61f62fe01a64534a485b0f8d9176ca6f18186b55", "message": "Add spelling", "committedDate": "2020-03-02T22:16:16Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjU0OTk0MA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r416549940", "bodyText": "can this be part of new module as InputSource seems to replacement for firehose related interfaces ?", "author": "pjain1", "createdAt": "2020-04-28T11:50:33Z", "path": "server/src/main/java/org/apache/druid/guice/FirehoseModule.java", "diffHunk": "@@ -58,7 +59,8 @@ public void configure(Binder binder)\n                 new NamedType(CombiningFirehoseFactory.class, \"combining\"),\n                 new NamedType(FixedCountFirehoseFactory.class, \"fixedCount\"),\n                 new NamedType(SqlFirehoseFactory.class, \"sql\"),\n-                new NamedType(InlineFirehoseFactory.class, \"inline\")\n+                new NamedType(InlineFirehoseFactory.class, \"inline\"),\n+                new NamedType(SqlInputSource.class, \"sql\")", "originalCommit": "61f62fe01a64534a485b0f8d9176ca6f18186b55", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM0OTIxNQ==", "url": "https://github.com/apache/druid/pull/9449#discussion_r420349215", "bodyText": "Thanks. Extracted it to a separate module.", "author": "a2l007", "createdAt": "2020-05-05T19:20:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjU0OTk0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM1MzE4Nw==", "url": "https://github.com/apache/druid/pull/9449#discussion_r420353187", "bodyText": "Thanks, however I would have just called it InputSourceModule and made it available to all processes by adding it in makeInjectorWithModules of Initialization.java similar to FirehoseModule.", "author": "pjain1", "createdAt": "2020-05-05T19:27:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjU0OTk0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDM2MTQ1NA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r420361454", "bodyText": "I can refactor the module name, but I believe this module wouldn't be needed in the broker, historical and router processes since its specific to indexing.", "author": "a2l007", "createdAt": "2020-05-05T19:42:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjU0OTk0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDUzNTI0Mg==", "url": "https://github.com/apache/druid/pull/9449#discussion_r420535242", "bodyText": "ok", "author": "pjain1", "createdAt": "2020-05-06T04:11:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjU0OTk0MA=="}], "type": "inlineReview"}, {"oid": "23a7c414b97cad0c871de331a8339c22333ace0c", "url": "https://github.com/apache/druid/commit/23a7c414b97cad0c871de331a8339c22333ace0c", "message": "Merge branch 'master' of https://github.com/druid-io/druid into sqlinputsource", "committedDate": "2020-05-05T16:24:30Z", "type": "commit"}, {"oid": "d1185e49933fea9baf4e4b28c8d43417e7c543b0", "url": "https://github.com/apache/druid/commit/d1185e49933fea9baf4e4b28c8d43417e7c543b0", "message": "Use separate DruidModule", "committedDate": "2020-05-05T19:18:46Z", "type": "commit"}, {"oid": "7f9743b112d910d50e6348ddb098dd1532650027", "url": "https://github.com/apache/druid/commit/7f9743b112d910d50e6348ddb098dd1532650027", "message": "Change module name", "committedDate": "2020-05-05T20:05:20Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgxOTQ1Mg==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421819452", "bodyText": "\ud83d\udc4d", "author": "jihoonson", "createdAt": "2020-05-07T21:59:17Z", "path": "core/src/main/java/org/apache/druid/java/util/common/parsers/CloseableIterator.java", "diffHunk": "@@ -66,10 +66,10 @@ public void close() throws IOException\n \n     return new CloseableIterator<R>()\n     {\n-      CloseableIterator<R> iterator = findNextIeteratorIfNecessary();\n+      CloseableIterator<R> iterator = findNextIteratorIfNecessary();", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgyNjQxOQ==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421826419", "bodyText": "Could you add a comment on why we fetch all result in local storage first? I remember this is to avoid holding database connections for too long time. It would help other developers.", "author": "jihoonson", "createdAt": "2020-05-07T22:16:47Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlReader.java", "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.type.TypeReference;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.IntermediateRowParsingReader;\n+import org.apache.druid.data.input.impl.MapInputRowParser;\n+import org.apache.druid.data.input.impl.prefetch.JsonIterator;\n+import org.apache.druid.java.util.common.io.Closer;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.java.util.common.parsers.ParseException;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SqlReader extends IntermediateRowParsingReader<Map<String, Object>>\n+{\n+  private final InputRowSchema inputRowSchema;\n+  private final SqlEntity source;\n+  private final File temporaryDirectory;\n+  private final ObjectMapper objectMapper;\n+\n+\n+  SqlReader(\n+      InputRowSchema inputRowSchema,\n+      InputEntity source,\n+      File temporaryDirectory,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.inputRowSchema = inputRowSchema;\n+    this.source = (SqlEntity) source;\n+    this.temporaryDirectory = temporaryDirectory;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  @Override\n+  protected CloseableIterator<Map<String, Object>> intermediateRowIterator() throws IOException\n+  {\n+    final Closer closer = Closer.create();\n+    final InputEntity.CleanableFile resultFile = closer.register(source.fetch(temporaryDirectory, null));", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgyNzAzNw==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421827037", "bodyText": "nit: this doesn't have to be done in this PR, but how about making JsonIterator a CloseableIterator? It already implements Iterator and Closeable so it would be pretty simple.", "author": "jihoonson", "createdAt": "2020-05-07T22:18:31Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlReader.java", "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.type.TypeReference;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.IntermediateRowParsingReader;\n+import org.apache.druid.data.input.impl.MapInputRowParser;\n+import org.apache.druid.data.input.impl.prefetch.JsonIterator;\n+import org.apache.druid.java.util.common.io.Closer;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.java.util.common.parsers.ParseException;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SqlReader extends IntermediateRowParsingReader<Map<String, Object>>\n+{\n+  private final InputRowSchema inputRowSchema;\n+  private final SqlEntity source;\n+  private final File temporaryDirectory;\n+  private final ObjectMapper objectMapper;\n+\n+\n+  SqlReader(\n+      InputRowSchema inputRowSchema,\n+      InputEntity source,\n+      File temporaryDirectory,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.inputRowSchema = inputRowSchema;\n+    this.source = (SqlEntity) source;\n+    this.temporaryDirectory = temporaryDirectory;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  @Override\n+  protected CloseableIterator<Map<String, Object>> intermediateRowIterator() throws IOException\n+  {\n+    final Closer closer = Closer.create();\n+    final InputEntity.CleanableFile resultFile = closer.register(source.fetch(temporaryDirectory, null));\n+    FileInputStream inputStream = new FileInputStream(resultFile.file());\n+    JsonIterator<Map<String, Object>> jsonIterator = new JsonIterator<>(new TypeReference<Map<String, Object>>()", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzNDc4MA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421834780", "bodyText": "Why did you choose this package for the sql ingestion classes?\nOther implementations of InputSource lives under druid-core not druid-server.\nAnd while we're on the subject of packages - have you thought about making this an extension?", "author": "suneet-s", "createdAt": "2020-05-07T22:39:54Z", "path": "server/src/main/java/org/apache/druid/metadata/input/InputSourceModule.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5MzE2NA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r432593164", "bodyText": "This feature has a dependency on the SQLFirehoseDatabaseConnector which resides in druid-server. This can be refactored to druid-core in a follow up PR as it might involve changes to the SqlFirehoseFactory  as well.\nIn order to use this InputSource, it has to be paired with a metadata storage extension that provides connectorConfig support. Making this itself an extension would require multiple extensions to be loaded for a single InputSource, which may not be user-friendly.", "author": "a2l007", "createdAt": "2020-05-29T16:17:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzNDc4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzNTIzMQ==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421835231", "bodyText": "javadocs please - same on all the new classes. Also this module name sounds very broad - have you thought about scoping this to a SqlInputModule ? Once we know the right package structure, I think the name of the module will make more sense.", "author": "suneet-s", "createdAt": "2020-05-07T22:41:12Z", "path": "server/src/main/java/org/apache/druid/metadata/input/InputSourceModule.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.databind.Module;\n+import com.fasterxml.jackson.databind.jsontype.NamedType;\n+import com.fasterxml.jackson.databind.module.SimpleModule;\n+import com.google.common.collect.ImmutableList;\n+import com.google.inject.Binder;\n+import org.apache.druid.initialization.DruidModule;\n+\n+import java.util.List;\n+\n+public class InputSourceModule implements DruidModule", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5MzQ2MQ==", "url": "https://github.com/apache/druid/pull/9449#discussion_r432593461", "bodyText": "This module is intended to support additional input sources in the future such as replacements for ClippedFirehoseFactory and CombiningFirehoseFactory, which is why the module name is rather broad.", "author": "a2l007", "createdAt": "2020-05-29T16:17:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzNTIzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzNzYzNg==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421837636", "bodyText": "Can you write a ModuleTest that verifies all the injections are made correctly?", "author": "suneet-s", "createdAt": "2020-05-07T22:47:51Z", "path": "server/src/main/java/org/apache/druid/metadata/input/InputSourceModule.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.databind.Module;\n+import com.fasterxml.jackson.databind.jsontype.NamedType;\n+import com.fasterxml.jackson.databind.module.SimpleModule;\n+import com.google.common.collect.ImmutableList;\n+import com.google.inject.Binder;\n+import org.apache.druid.initialization.DruidModule;\n+\n+import java.util.List;\n+\n+public class InputSourceModule implements DruidModule\n+{\n+  @Override\n+  public List<? extends Module> getJacksonModules()", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5MzcyNQ==", "url": "https://github.com/apache/druid/pull/9449#discussion_r432593725", "bodyText": "I believe it is being tested in SqlInputSourceTest.\nCould you please provide an example of what type of test you\u2019re looking for?", "author": "a2l007", "createdAt": "2020-05-29T16:18:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzNzYzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2Njc0NQ==", "url": "https://github.com/apache/druid/pull/9449#discussion_r434766745", "bodyText": "I was thinking of a test where we verify that the \"sql\" named type is registered to the SqlInputSource class correctly. Here's an example of a module test I've added for a custom extension https://github.com/implydata/indexed-table-loader/blob/master/src/test/java/io/imply/druid/indextable/loader/IndexedTableLoaderDruidModuleTest.java", "author": "suneet-s", "createdAt": "2020-06-03T18:22:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzNzYzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzODY5Mg==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421838692", "bodyText": "Why can't open() do this for you? Callers of the interface won't know about the inner workings of each implementation, so if we have a remediation, we should do this automatically.\nInject a Supplier in to this class so you can get a temp dir, this will also make it easier to test.", "author": "suneet-s", "createdAt": "2020-05-07T22:50:55Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = sqlFirehoseDatabaseConnector;\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5NDA2Ng==", "url": "https://github.com/apache/druid/pull/9449#discussion_r432594066", "bodyText": "The contract for InputStream.open defines that it needs to be an InputStream directly on the input entity. Since, there isn\u2019t a direct way to do this and in the interest of not maintaining the sql connection open for a long time, SqlInputSource isn\u2019t supporting open().", "author": "a2l007", "createdAt": "2020-05-29T16:18:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzODY5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgzOTY4Mg==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421839682", "bodyText": "Is this possible? The annotations indicate that this should always be NonNull\nIf this can be null, then I think it should be checked at the constructor - right now, there is the possiblilty that we create temp files that are never cleaned up if an exception is thrown on this line.", "author": "suneet-s", "createdAt": "2020-05-07T22:53:56Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = sqlFirehoseDatabaseConnector;\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    Preconditions.checkNotNull(sqlFirehoseDatabaseConnector, \"SQL Metadata Connector not configured!\");", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTg0MDYxMQ==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421840611", "bodyText": "Can you add more comments in here for what this is trying to do", "author": "suneet-s", "createdAt": "2020-05-07T22:56:46Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = sqlFirehoseDatabaseConnector;\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    Preconditions.checkNotNull(sqlFirehoseDatabaseConnector, \"SQL Metadata Connector not configured!\");\n+    try (FileOutputStream fos = new FileOutputStream(tempFile)) {\n+      final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);\n+      sqlFirehoseDatabaseConnector.retryWithHandle(\n+          (handle) -> {\n+            ResultIterator<Map<String, Object>> resultIterator = handle.createQuery(\n+                sql\n+            ).map(\n+                (index, r, ctx) -> {\n+                  Map<String, Object> resultRow = foldCase ? new CaseFoldedMap() : new HashMap<>();\n+                  ResultSetMetaData resultMetadata;\n+                  try {\n+                    resultMetadata = r.getMetaData();\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to obtain metadata from result set\", e, ctx);\n+                  }\n+                  try {\n+                    for (int i = 1; i <= resultMetadata.getColumnCount(); i++) {\n+                      String key = resultMetadata.getColumnName(i);\n+                      String alias = resultMetadata.getColumnLabel(i);\n+                      Object value = r.getObject(i);\n+                      resultRow.put(alias != null ? alias : key, value);\n+                    }\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to access specific metadata from \" +\n+                                                 \"result set metadata\", e, ctx);\n+                  }\n+                  return resultRow;", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTg0MjEwNw==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421842107", "bodyText": "Are there any size restrictions here? What happens if I try to ingest a very large sql output? How big do the indexing machines need to be? How long can the db connection be kept open so that we can keep writing records to the temp file? Are there any logs that indicate what I need to do operationally to support this?", "author": "suneet-s", "createdAt": "2020-05-07T23:01:17Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = sqlFirehoseDatabaseConnector;\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    Preconditions.checkNotNull(sqlFirehoseDatabaseConnector, \"SQL Metadata Connector not configured!\");\n+    try (FileOutputStream fos = new FileOutputStream(tempFile)) {\n+      final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);\n+      sqlFirehoseDatabaseConnector.retryWithHandle(\n+          (handle) -> {\n+            ResultIterator<Map<String, Object>> resultIterator = handle.createQuery(\n+                sql\n+            ).map(\n+                (index, r, ctx) -> {\n+                  Map<String, Object> resultRow = foldCase ? new CaseFoldedMap() : new HashMap<>();\n+                  ResultSetMetaData resultMetadata;\n+                  try {\n+                    resultMetadata = r.getMetaData();\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to obtain metadata from result set\", e, ctx);\n+                  }\n+                  try {\n+                    for (int i = 1; i <= resultMetadata.getColumnCount(); i++) {\n+                      String key = resultMetadata.getColumnName(i);\n+                      String alias = resultMetadata.getColumnLabel(i);\n+                      Object value = r.getObject(i);\n+                      resultRow.put(alias != null ? alias : key, value);\n+                    }\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to access specific metadata from \" +\n+                                                 \"result set metadata\", e, ctx);\n+                  }\n+                  return resultRow;\n+                }\n+            ).iterator();\n+            jg.writeStartArray();\n+            while (resultIterator.hasNext()) {\n+              jg.writeObject(resultIterator.next());\n+            }\n+            jg.writeEndArray();\n+            jg.close();", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5NDQzMw==", "url": "https://github.com/apache/druid/pull/9449#discussion_r432594433", "bodyText": "Yeah this is something that I\u2019d like to fix in a follow up PR. There isn\u2019t a way to restrict the file sizes presently  and I feel size limiting on fetched files should be supported at the InputEntity interface level. I have added some info in the docs related to the size limit for now.", "author": "a2l007", "createdAt": "2020-05-29T16:19:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTg0MjEwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTg0Mjg5Mw==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421842893", "bodyText": "Do we need null checks on all the json provided fields? Or is that handled by some annotation I'm not seeing?\nWhat is the default boolean value if foldCase is not specified?", "author": "suneet-s", "createdAt": "2020-05-07T23:03:52Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlInputSource.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.AbstractInputSource;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.InputSourceReader;\n+import org.apache.druid.data.input.InputSplit;\n+import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.data.input.impl.InputEntityIteratingReader;\n+import org.apache.druid.data.input.impl.SplittableInputSource;\n+import org.apache.druid.guice.annotations.Smile;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Stream;\n+\n+public class SqlInputSource extends AbstractInputSource implements SplittableInputSource<String>\n+{\n+  private final List<String> sqls;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final ObjectMapper objectMapper;\n+  private final boolean foldCase;\n+\n+  @JsonCreator\n+  public SqlInputSource(\n+      @JsonProperty(\"sqls\") List<String> sqls,\n+      @JsonProperty(\"foldCase\") boolean foldCase,\n+      @JsonProperty(\"database\") SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      @JacksonInject @Smile ObjectMapper objectMapper\n+  )\n+  {\n+    Preconditions.checkArgument(sqls.size() > 0, \"No SQL queries provided\");", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5NjAzMQ==", "url": "https://github.com/apache/druid/pull/9449#discussion_r432596031", "bodyText": "Added null checks. Default for foldcase is false", "author": "a2l007", "createdAt": "2020-05-29T16:22:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTg0Mjg5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTg0MzYxNw==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421843617", "bodyText": "EqualsVerifier test please for equals and hashcode", "author": "suneet-s", "createdAt": "2020-05-07T23:06:05Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlInputSource.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.AbstractInputSource;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.InputSourceReader;\n+import org.apache.druid.data.input.InputSplit;\n+import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.data.input.impl.InputEntityIteratingReader;\n+import org.apache.druid.data.input.impl.SplittableInputSource;\n+import org.apache.druid.guice.annotations.Smile;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Stream;\n+\n+public class SqlInputSource extends AbstractInputSource implements SplittableInputSource<String>\n+{\n+  private final List<String> sqls;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final ObjectMapper objectMapper;\n+  private final boolean foldCase;\n+\n+  @JsonCreator\n+  public SqlInputSource(\n+      @JsonProperty(\"sqls\") List<String> sqls,\n+      @JsonProperty(\"foldCase\") boolean foldCase,\n+      @JsonProperty(\"database\") SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      @JacksonInject @Smile ObjectMapper objectMapper\n+  )\n+  {\n+    Preconditions.checkArgument(sqls.size() > 0, \"No SQL queries provided\");\n+\n+    this.sqls = sqls;\n+    this.foldCase = foldCase;\n+    this.sqlFirehoseDatabaseConnector = sqlFirehoseDatabaseConnector;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  @JsonProperty\n+  public List<String> getSqls()\n+  {\n+    return sqls;\n+  }\n+\n+  @JsonProperty\n+  public boolean isFoldCase()\n+  {\n+    return foldCase;\n+  }\n+\n+  @JsonProperty(\"database\")\n+  public SQLFirehoseDatabaseConnector getSQLFirehoseDatabaseConnector()\n+  {\n+    return sqlFirehoseDatabaseConnector;\n+  }\n+\n+  @Override\n+  public Stream<InputSplit<String>> createSplits(InputFormat inputFormat, @Nullable SplitHintSpec splitHintSpec)\n+  {\n+    return sqls.stream().map(InputSplit::new);\n+  }\n+\n+  @Override\n+  public int estimateNumSplits(InputFormat inputFormat, @Nullable SplitHintSpec splitHintSpec)\n+  {\n+    return sqls.size();\n+  }\n+\n+  @Override\n+  public SplittableInputSource<String> withSplit(InputSplit<String> split)\n+  {\n+    return new SqlInputSource(\n+        Collections.singletonList(split.get()),\n+        foldCase,\n+        sqlFirehoseDatabaseConnector,\n+        objectMapper\n+    );\n+  }\n+\n+  @Override\n+  protected InputSourceReader fixedFormatReader(InputRowSchema inputRowSchema, @Nullable File temporaryDirectory)\n+  {\n+    final SqlInputFormat inputFormat = new SqlInputFormat(objectMapper);\n+    return new InputEntityIteratingReader(\n+        inputRowSchema,\n+        inputFormat,\n+        createSplits(inputFormat, null)\n+            .map(split -> new SqlEntity(split.get(), sqlFirehoseDatabaseConnector, foldCase, objectMapper)).iterator(),\n+        temporaryDirectory\n+    );\n+  }\n+\n+  @Override\n+  public boolean needsFormat()\n+  {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o)\n+  {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+    SqlInputSource that = (SqlInputSource) o;\n+    return foldCase == that.foldCase &&\n+           sqls.equals(that.sqls) &&\n+           sqlFirehoseDatabaseConnector.equals(that.sqlFirehoseDatabaseConnector) &&\n+           objectMapper.equals(that.objectMapper);\n+  }\n+\n+  @Override\n+  public int hashCode()\n+  {\n+    return Objects.hash(sqls, sqlFirehoseDatabaseConnector, objectMapper, foldCase);\n+  }", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTg0OTI5Mw==", "url": "https://github.com/apache/druid/pull/9449#discussion_r421849293", "bodyText": "Can you add unit tests for this class please - same with SqlInputFormat", "author": "suneet-s", "createdAt": "2020-05-07T23:23:29Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = sqlFirehoseDatabaseConnector;\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    Preconditions.checkNotNull(sqlFirehoseDatabaseConnector, \"SQL Metadata Connector not configured!\");\n+    try (FileOutputStream fos = new FileOutputStream(tempFile)) {\n+      final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);\n+      sqlFirehoseDatabaseConnector.retryWithHandle(\n+          (handle) -> {\n+            ResultIterator<Map<String, Object>> resultIterator = handle.createQuery(\n+                sql\n+            ).map(\n+                (index, r, ctx) -> {\n+                  Map<String, Object> resultRow = foldCase ? new CaseFoldedMap() : new HashMap<>();\n+                  ResultSetMetaData resultMetadata;\n+                  try {\n+                    resultMetadata = r.getMetaData();\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to obtain metadata from result set\", e, ctx);\n+                  }\n+                  try {\n+                    for (int i = 1; i <= resultMetadata.getColumnCount(); i++) {\n+                      String key = resultMetadata.getColumnName(i);\n+                      String alias = resultMetadata.getColumnLabel(i);\n+                      Object value = r.getObject(i);\n+                      resultRow.put(alias != null ? alias : key, value);\n+                    }\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to access specific metadata from \" +\n+                                                 \"result set metadata\", e, ctx);\n+                  }\n+                  return resultRow;\n+                }\n+            ).iterator();\n+            jg.writeStartArray();\n+            while (resultIterator.hasNext()) {\n+              jg.writeObject(resultIterator.next());\n+            }\n+            jg.writeEndArray();\n+            jg.close();\n+            return null;\n+          },\n+          (exception) -> {\n+            final boolean isStatementException = exception instanceof StatementException ||\n+                                                 (exception instanceof CallbackFailedException\n+                                                  && exception.getCause() instanceof StatementException);\n+            return sqlFirehoseDatabaseConnector.isTransientException(exception) && !(isStatementException);\n+          }\n+      );\n+    }\n+    return new CleanableFile()\n+    {\n+      @Override\n+      public File file()\n+      {\n+        return tempFile;\n+      }\n+\n+      @Override\n+      public void close()\n+      {\n+        if (!tempFile.delete()) {\n+          LOG.warn(\"Failed to remove file[%s]\", tempFile.getAbsolutePath());\n+        }\n+      }\n+    };\n+  }", "originalCommit": "7f9743b112d910d50e6348ddb098dd1532650027", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b379d198cb83fd3706c1c19bf3ed0cb777a951c6", "url": "https://github.com/apache/druid/commit/b379d198cb83fd3706c1c19bf3ed0cb777a951c6", "message": "Merge branch 'master' of https://github.com/druid-io/druid into sqlinputsource", "committedDate": "2020-05-25T12:52:52Z", "type": "commit"}, {"oid": "633323490f39fc279a874ae72d549c6e34762211", "url": "https://github.com/apache/druid/commit/633323490f39fc279a874ae72d549c6e34762211", "message": "Fix docs", "committedDate": "2020-05-28T22:18:49Z", "type": "commit"}, {"oid": "af4d5fab2495c0ee7c461d693053a2fa38c15f12", "url": "https://github.com/apache/druid/commit/af4d5fab2495c0ee7c461d693053a2fa38c15f12", "message": "Merge branch 'master' of https://github.com/druid-io/druid into sqlinputsource", "committedDate": "2020-05-28T22:18:54Z", "type": "commit"}, {"oid": "0328eaded9f4b85649316bfca72a79e5b7ed1db9", "url": "https://github.com/apache/druid/commit/0328eaded9f4b85649316bfca72a79e5b7ed1db9", "message": "Use sqltestutils for tests", "committedDate": "2020-05-29T16:13:07Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg5OTgwOA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r432899808", "bodyText": "The tempFile will not be deleted if an exception is thrown in any lines above. We should catch all exceptions and delete the file properly.", "author": "jihoonson", "createdAt": "2020-05-31T01:04:07Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+/**\n+ * Represents a rdbms based input resource and knows how to read query results from the resource using SQL queries.\n+ */\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = Preconditions.checkNotNull(\n+        sqlFirehoseDatabaseConnector,\n+        \"SQL Metadata Connector not configured!\"\n+    );\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    try (FileOutputStream fos = new FileOutputStream(tempFile)) {\n+      final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);\n+\n+      // Execute the sql query and lazily retrieve the results into the file in json format.\n+      // foldCase is useful to handle differences in case sensitivity behavior across databases.\n+      sqlFirehoseDatabaseConnector.retryWithHandle(\n+          (handle) -> {\n+            ResultIterator<Map<String, Object>> resultIterator = handle.createQuery(\n+                sql\n+            ).map(\n+                (index, r, ctx) -> {\n+                  Map<String, Object> resultRow = foldCase ? new CaseFoldedMap() : new HashMap<>();\n+                  ResultSetMetaData resultMetadata;\n+                  try {\n+                    resultMetadata = r.getMetaData();\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to obtain metadata from result set\", e, ctx);\n+                  }\n+                  try {\n+                    for (int i = 1; i <= resultMetadata.getColumnCount(); i++) {\n+                      String key = resultMetadata.getColumnName(i);\n+                      String alias = resultMetadata.getColumnLabel(i);\n+                      Object value = r.getObject(i);\n+                      resultRow.put(alias != null ? alias : key, value);\n+                    }\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to access specific metadata from \" +\n+                                                 \"result set metadata\", e, ctx);\n+                  }\n+                  return resultRow;\n+                }\n+            ).iterator();\n+            jg.writeStartArray();\n+            while (resultIterator.hasNext()) {\n+              jg.writeObject(resultIterator.next());\n+            }\n+            jg.writeEndArray();\n+            jg.close();\n+            return null;\n+          },\n+          (exception) -> {\n+            final boolean isStatementException = exception instanceof StatementException ||\n+                                                 (exception instanceof CallbackFailedException\n+                                                  && exception.getCause() instanceof StatementException);\n+            return sqlFirehoseDatabaseConnector.isTransientException(exception) && !(isStatementException);\n+          }\n+      );\n+    }\n+    return new CleanableFile()", "originalCommit": "0328eaded9f4b85649316bfca72a79e5b7ed1db9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg5OTg5NQ==", "url": "https://github.com/apache/druid/pull/9449#discussion_r432899895", "bodyText": "Thanks for making the JsonIterator a CloseableIterator. Now you can return jsonIterator directly and remove this.", "author": "jihoonson", "createdAt": "2020-05-31T01:05:59Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlReader.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.type.TypeReference;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.IntermediateRowParsingReader;\n+import org.apache.druid.data.input.impl.MapInputRowParser;\n+import org.apache.druid.data.input.impl.prefetch.JsonIterator;\n+import org.apache.druid.java.util.common.io.Closer;\n+import org.apache.druid.java.util.common.parsers.CloseableIterator;\n+import org.apache.druid.java.util.common.parsers.ParseException;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Reader exclusively for {@link SqlEntity}\n+ */\n+public class SqlReader extends IntermediateRowParsingReader<Map<String, Object>>\n+{\n+  private final InputRowSchema inputRowSchema;\n+  private final SqlEntity source;\n+  private final File temporaryDirectory;\n+  private final ObjectMapper objectMapper;\n+\n+\n+  SqlReader(\n+      InputRowSchema inputRowSchema,\n+      InputEntity source,\n+      File temporaryDirectory,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.inputRowSchema = inputRowSchema;\n+    this.source = (SqlEntity) source;\n+    this.temporaryDirectory = temporaryDirectory;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  @Override\n+  protected CloseableIterator<Map<String, Object>> intermediateRowIterator() throws IOException\n+  {\n+    final Closer closer = Closer.create();\n+    //The results are fetched into local storage as this avoids having to keep a persistent database connection for a long time\n+    final InputEntity.CleanableFile resultFile = closer.register(source.fetch(temporaryDirectory, null));\n+    FileInputStream inputStream = new FileInputStream(resultFile.file());\n+    JsonIterator<Map<String, Object>> jsonIterator = new JsonIterator<>(new TypeReference<Map<String, Object>>()\n+    {\n+    }, inputStream, closer, objectMapper);\n+    return new CloseableIterator<Map<String, Object>>()", "originalCommit": "0328eaded9f4b85649316bfca72a79e5b7ed1db9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjkwMDIyMA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r432900220", "bodyText": "Please move jg to the above try-resource clause so that i can be closed safely.", "author": "jihoonson", "createdAt": "2020-05-31T01:12:53Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+/**\n+ * Represents a rdbms based input resource and knows how to read query results from the resource using SQL queries.\n+ */\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = Preconditions.checkNotNull(\n+        sqlFirehoseDatabaseConnector,\n+        \"SQL Metadata Connector not configured!\"\n+    );\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    try (FileOutputStream fos = new FileOutputStream(tempFile)) {\n+      final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);", "originalCommit": "0328eaded9f4b85649316bfca72a79e5b7ed1db9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "03de8c0e6a40c61ed9e9399766bc633886d4682a", "url": "https://github.com/apache/druid/commit/03de8c0e6a40c61ed9e9399766bc633886d4682a", "message": "Merge branch 'master' of https://github.com/druid-io/druid into sqlinputsource", "committedDate": "2020-06-01T13:41:38Z", "type": "commit"}, {"oid": "62dc00b2eb9597e33a2bdb37642b94197d36f3f9", "url": "https://github.com/apache/druid/commit/62dc00b2eb9597e33a2bdb37642b94197d36f3f9", "message": "Add additional tests", "committedDate": "2020-06-01T20:16:21Z", "type": "commit"}, {"oid": "7869ea92f09682f0c29d6ab30fc5a09b9cde0bbc", "url": "https://github.com/apache/druid/commit/7869ea92f09682f0c29d6ab30fc5a09b9cde0bbc", "message": "Merge branch 'master' of https://github.com/druid-io/druid into sqlinputsource", "committedDate": "2020-06-01T20:16:33Z", "type": "commit"}, {"oid": "53cb148570ccb48903a3f6d587287bdc9220f0fc", "url": "https://github.com/apache/druid/commit/53cb148570ccb48903a3f6d587287bdc9220f0fc", "message": "Fix inspection", "committedDate": "2020-06-01T21:14:36Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc3ODU4Nw==", "url": "https://github.com/apache/druid/pull/9449#discussion_r434778587", "bodyText": "Sorry I missed this earlier: Why did we chose json serialization? What happens if the object returned from the result set can not be serialized by the json generator?", "author": "suneet-s", "createdAt": "2020-06-03T18:43:20Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+/**\n+ * Represents a rdbms based input resource and knows how to read query results from the resource using SQL queries.\n+ */\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = Preconditions.checkNotNull(\n+        sqlFirehoseDatabaseConnector,\n+        \"SQL Metadata Connector not configured!\"\n+    );\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  /**\n+   * Executes a SQL query on the specified database and fetches the result into the given file.\n+   * The result file is deleted if the query execution or the file write fails.\n+   *\n+   * @param sql                          The SQL query to be executed\n+   * @param sqlFirehoseDatabaseConnector The database connector\n+   * @param objectMapper                 An object mapper, used for deserialization\n+   * @param foldCase                     A boolean flag used to enable or disabling case sensitivity while handling database column names\n+   *\n+   * @return A {@link InputEntity.CleanableFile} object that wraps the file containing the SQL results\n+   */\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    try (FileOutputStream fos = new FileOutputStream(tempFile);\n+         final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);) {\n+\n+      // Execute the sql query and lazily retrieve the results into the file in json format.\n+      // foldCase is useful to handle differences in case sensitivity behavior across databases.\n+      sqlFirehoseDatabaseConnector.retryWithHandle(\n+          (handle) -> {\n+            ResultIterator<Map<String, Object>> resultIterator = handle.createQuery(\n+                sql\n+            ).map(\n+                (index, r, ctx) -> {\n+                  Map<String, Object> resultRow = foldCase ? new CaseFoldedMap() : new HashMap<>();\n+                  ResultSetMetaData resultMetadata;\n+                  try {\n+                    resultMetadata = r.getMetaData();\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to obtain metadata from result set\", e, ctx);\n+                  }\n+                  try {\n+                    for (int i = 1; i <= resultMetadata.getColumnCount(); i++) {\n+                      String key = resultMetadata.getColumnName(i);\n+                      String alias = resultMetadata.getColumnLabel(i);\n+                      Object value = r.getObject(i);\n+                      resultRow.put(alias != null ? alias : key, value);\n+                    }\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to access specific metadata from \" +\n+                                                 \"result set metadata\", e, ctx);\n+                  }\n+                  return resultRow;\n+                }\n+            ).iterator();\n+            jg.writeStartArray();\n+            while (resultIterator.hasNext()) {\n+              jg.writeObject(resultIterator.next());", "originalCommit": "53cb148570ccb48903a3f6d587287bdc9220f0fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQyMjQ5MA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r435422490", "bodyText": "The design was made to align with one of the multiple use cases where events from json sources were imported into RDBMS and this inputsource would be used to index off of that. The data types supported by file formats would work with this Inputsource as well. Going forward I\u2019d like to take a stab at evaluating if this result file can be hooked up to read from one of the TextReader implementations. The tricky part right now is that the intermediate row parsing logic for SqlReader is completely different compared to the TextReader one.", "author": "a2l007", "createdAt": "2020-06-04T17:21:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc3ODU4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc4MDIwMA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r434780200", "bodyText": "It looks like this logic is duplicated from SQLMetadataStorageActionHandler#isStatementException. Can you consolidate them please", "author": "suneet-s", "createdAt": "2020-06-03T18:46:15Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+/**\n+ * Represents a rdbms based input resource and knows how to read query results from the resource using SQL queries.\n+ */\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = Preconditions.checkNotNull(\n+        sqlFirehoseDatabaseConnector,\n+        \"SQL Metadata Connector not configured!\"\n+    );\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  /**\n+   * Executes a SQL query on the specified database and fetches the result into the given file.\n+   * The result file is deleted if the query execution or the file write fails.\n+   *\n+   * @param sql                          The SQL query to be executed\n+   * @param sqlFirehoseDatabaseConnector The database connector\n+   * @param objectMapper                 An object mapper, used for deserialization\n+   * @param foldCase                     A boolean flag used to enable or disabling case sensitivity while handling database column names\n+   *\n+   * @return A {@link InputEntity.CleanableFile} object that wraps the file containing the SQL results\n+   */\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    try (FileOutputStream fos = new FileOutputStream(tempFile);\n+         final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);) {\n+\n+      // Execute the sql query and lazily retrieve the results into the file in json format.\n+      // foldCase is useful to handle differences in case sensitivity behavior across databases.\n+      sqlFirehoseDatabaseConnector.retryWithHandle(\n+          (handle) -> {\n+            ResultIterator<Map<String, Object>> resultIterator = handle.createQuery(\n+                sql\n+            ).map(\n+                (index, r, ctx) -> {\n+                  Map<String, Object> resultRow = foldCase ? new CaseFoldedMap() : new HashMap<>();\n+                  ResultSetMetaData resultMetadata;\n+                  try {\n+                    resultMetadata = r.getMetaData();\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to obtain metadata from result set\", e, ctx);\n+                  }\n+                  try {\n+                    for (int i = 1; i <= resultMetadata.getColumnCount(); i++) {\n+                      String key = resultMetadata.getColumnName(i);\n+                      String alias = resultMetadata.getColumnLabel(i);\n+                      Object value = r.getObject(i);\n+                      resultRow.put(alias != null ? alias : key, value);\n+                    }\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to access specific metadata from \" +\n+                                                 \"result set metadata\", e, ctx);\n+                  }\n+                  return resultRow;\n+                }\n+            ).iterator();\n+            jg.writeStartArray();\n+            while (resultIterator.hasNext()) {\n+              jg.writeObject(resultIterator.next());\n+            }\n+            jg.writeEndArray();\n+            jg.close();\n+            return null;\n+          },\n+          (exception) -> {\n+            final boolean isStatementException = exception instanceof StatementException ||", "originalCommit": "53cb148570ccb48903a3f6d587287bdc9220f0fc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc4NDQxMA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r434784410", "bodyText": "This breaks the contract of HashMap which allows the key to be null. StringUtils.toLowerCase(null) will throw an NPE", "author": "suneet-s", "createdAt": "2020-06-03T18:54:06Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlEntity.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.core.JsonGenerator;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.InputEntity;\n+import org.apache.druid.java.util.common.StringUtils;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+import org.skife.jdbi.v2.ResultIterator;\n+import org.skife.jdbi.v2.exceptions.CallbackFailedException;\n+import org.skife.jdbi.v2.exceptions.ResultSetException;\n+import org.skife.jdbi.v2.exceptions.StatementException;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.sql.ResultSetMetaData;\n+import java.sql.SQLException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+/**\n+ * Represents a rdbms based input resource and knows how to read query results from the resource using SQL queries.\n+ */\n+public class SqlEntity implements InputEntity\n+{\n+  private static final Logger LOG = new Logger(SqlEntity.class);\n+\n+  private final String sql;\n+  private final ObjectMapper objectMapper;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final boolean foldCase;\n+\n+  public SqlEntity(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      boolean foldCase,\n+      ObjectMapper objectMapper\n+  )\n+  {\n+    this.sql = sql;\n+    this.sqlFirehoseDatabaseConnector = Preconditions.checkNotNull(\n+        sqlFirehoseDatabaseConnector,\n+        \"SQL Metadata Connector not configured!\"\n+    );\n+    this.foldCase = foldCase;\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  public String getSql()\n+  {\n+    return sql;\n+  }\n+\n+  @Nullable\n+  @Override\n+  public URI getUri()\n+  {\n+    return null;\n+  }\n+\n+  @Override\n+  public InputStream open()\n+  {\n+    throw new UnsupportedOperationException(\"Please use fetch() instead\");\n+  }\n+\n+  @Override\n+  public CleanableFile fetch(File temporaryDirectory, byte[] fetchBuffer) throws IOException\n+  {\n+    final File tempFile = File.createTempFile(\"druid-sql-entity\", \".tmp\", temporaryDirectory);\n+    return openCleanableFile(sql, sqlFirehoseDatabaseConnector, objectMapper, foldCase, tempFile);\n+\n+  }\n+\n+  /**\n+   * Executes a SQL query on the specified database and fetches the result into the given file.\n+   * The result file is deleted if the query execution or the file write fails.\n+   *\n+   * @param sql                          The SQL query to be executed\n+   * @param sqlFirehoseDatabaseConnector The database connector\n+   * @param objectMapper                 An object mapper, used for deserialization\n+   * @param foldCase                     A boolean flag used to enable or disabling case sensitivity while handling database column names\n+   *\n+   * @return A {@link InputEntity.CleanableFile} object that wraps the file containing the SQL results\n+   */\n+\n+  public static CleanableFile openCleanableFile(\n+      String sql,\n+      SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      ObjectMapper objectMapper,\n+      boolean foldCase,\n+      File tempFile\n+  )\n+      throws IOException\n+  {\n+    try (FileOutputStream fos = new FileOutputStream(tempFile);\n+         final JsonGenerator jg = objectMapper.getFactory().createGenerator(fos);) {\n+\n+      // Execute the sql query and lazily retrieve the results into the file in json format.\n+      // foldCase is useful to handle differences in case sensitivity behavior across databases.\n+      sqlFirehoseDatabaseConnector.retryWithHandle(\n+          (handle) -> {\n+            ResultIterator<Map<String, Object>> resultIterator = handle.createQuery(\n+                sql\n+            ).map(\n+                (index, r, ctx) -> {\n+                  Map<String, Object> resultRow = foldCase ? new CaseFoldedMap() : new HashMap<>();\n+                  ResultSetMetaData resultMetadata;\n+                  try {\n+                    resultMetadata = r.getMetaData();\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to obtain metadata from result set\", e, ctx);\n+                  }\n+                  try {\n+                    for (int i = 1; i <= resultMetadata.getColumnCount(); i++) {\n+                      String key = resultMetadata.getColumnName(i);\n+                      String alias = resultMetadata.getColumnLabel(i);\n+                      Object value = r.getObject(i);\n+                      resultRow.put(alias != null ? alias : key, value);\n+                    }\n+                  }\n+                  catch (SQLException e) {\n+                    throw new ResultSetException(\"Unable to access specific metadata from \" +\n+                                                 \"result set metadata\", e, ctx);\n+                  }\n+                  return resultRow;\n+                }\n+            ).iterator();\n+            jg.writeStartArray();\n+            while (resultIterator.hasNext()) {\n+              jg.writeObject(resultIterator.next());\n+            }\n+            jg.writeEndArray();\n+            jg.close();\n+            return null;\n+          },\n+          (exception) -> {\n+            final boolean isStatementException = exception instanceof StatementException ||\n+                                                 (exception instanceof CallbackFailedException\n+                                                  && exception.getCause() instanceof StatementException);\n+            return sqlFirehoseDatabaseConnector.isTransientException(exception) && !(isStatementException);\n+          }\n+      );\n+      return new CleanableFile()\n+      {\n+        @Override\n+        public File file()\n+        {\n+          return tempFile;\n+        }\n+\n+        @Override\n+        public void close()\n+        {\n+          if (!tempFile.delete()) {\n+            LOG.warn(\"Failed to remove file[%s]\", tempFile.getAbsolutePath());\n+          }\n+        }\n+      };\n+    }\n+    catch (Exception e) {\n+      if (!tempFile.delete()) {\n+        LOG.warn(\"Failed to remove file[%s]\", tempFile.getAbsolutePath());\n+      }\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  private static class CaseFoldedMap extends HashMap<String, Object>\n+  {\n+    public static final long serialVersionUID = 1L;\n+\n+    @Override\n+    public Object get(Object obj)\n+    {\n+      return super.get(StringUtils.toLowerCase((String) obj));", "originalCommit": "53cb148570ccb48903a3f6d587287bdc9220f0fc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc4NzI4NA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r434787284", "bodyText": "Does this need to check either inputFormat or splitHintSpec to create the Stream of InputSplits? SImilar comment for function below. Not sure if I understand how these should be used.", "author": "suneet-s", "createdAt": "2020-06-03T18:59:25Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlInputSource.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.AbstractInputSource;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.InputSourceReader;\n+import org.apache.druid.data.input.InputSplit;\n+import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.data.input.impl.InputEntityIteratingReader;\n+import org.apache.druid.data.input.impl.SplittableInputSource;\n+import org.apache.druid.guice.annotations.Smile;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Stream;\n+\n+public class SqlInputSource extends AbstractInputSource implements SplittableInputSource<String>\n+{\n+  private final List<String> sqls;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final ObjectMapper objectMapper;\n+  private final boolean foldCase;\n+\n+  @JsonCreator\n+  public SqlInputSource(\n+      @JsonProperty(\"sqls\") List<String> sqls,\n+      @JsonProperty(\"foldCase\") boolean foldCase,\n+      @JsonProperty(\"database\") SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      @JacksonInject @Smile ObjectMapper objectMapper\n+  )\n+  {\n+    Preconditions.checkArgument(sqls.size() > 0, \"No SQL queries provided\");\n+\n+    this.sqls = sqls;\n+    this.foldCase = foldCase;\n+    this.sqlFirehoseDatabaseConnector = Preconditions.checkNotNull(\n+        sqlFirehoseDatabaseConnector,\n+        \"SQL Metadata Connector not configured!\"\n+    );\n+    this.objectMapper = objectMapper;\n+  }\n+\n+  @JsonProperty\n+  public List<String> getSqls()\n+  {\n+    return sqls;\n+  }\n+\n+  @JsonProperty\n+  public boolean isFoldCase()\n+  {\n+    return foldCase;\n+  }\n+\n+  @JsonProperty(\"database\")\n+  public SQLFirehoseDatabaseConnector getSQLFirehoseDatabaseConnector()\n+  {\n+    return sqlFirehoseDatabaseConnector;\n+  }\n+\n+  @Override\n+  public Stream<InputSplit<String>> createSplits(InputFormat inputFormat, @Nullable SplitHintSpec splitHintSpec)\n+  {\n+    return sqls.stream().map(InputSplit::new);", "originalCommit": "53cb148570ccb48903a3f6d587287bdc9220f0fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQyMjgzNg==", "url": "https://github.com/apache/druid/pull/9449#discussion_r435422836", "bodyText": "This inputsource does not support custom input formats as defined in SqlInputSource.needsFormat(). The splits are performed based on the number of sql queries and therefore the splitHintSpec is not used as well.", "author": "a2l007", "createdAt": "2020-06-04T17:21:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc4NzI4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc5Njg5NQ==", "url": "https://github.com/apache/druid/pull/9449#discussion_r434796895", "bodyText": "Why do we need a smile object mapper instead of just the default object mapper", "author": "suneet-s", "createdAt": "2020-06-03T19:17:29Z", "path": "server/src/main/java/org/apache/druid/metadata/input/SqlInputSource.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.metadata.input;\n+\n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.data.input.AbstractInputSource;\n+import org.apache.druid.data.input.InputFormat;\n+import org.apache.druid.data.input.InputRowSchema;\n+import org.apache.druid.data.input.InputSourceReader;\n+import org.apache.druid.data.input.InputSplit;\n+import org.apache.druid.data.input.SplitHintSpec;\n+import org.apache.druid.data.input.impl.InputEntityIteratingReader;\n+import org.apache.druid.data.input.impl.SplittableInputSource;\n+import org.apache.druid.guice.annotations.Smile;\n+import org.apache.druid.metadata.SQLFirehoseDatabaseConnector;\n+\n+import javax.annotation.Nullable;\n+import java.io.File;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Stream;\n+\n+public class SqlInputSource extends AbstractInputSource implements SplittableInputSource<String>\n+{\n+  private final List<String> sqls;\n+  private final SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector;\n+  private final ObjectMapper objectMapper;\n+  private final boolean foldCase;\n+\n+  @JsonCreator\n+  public SqlInputSource(\n+      @JsonProperty(\"sqls\") List<String> sqls,\n+      @JsonProperty(\"foldCase\") boolean foldCase,\n+      @JsonProperty(\"database\") SQLFirehoseDatabaseConnector sqlFirehoseDatabaseConnector,\n+      @JacksonInject @Smile ObjectMapper objectMapper", "originalCommit": "53cb148570ccb48903a3f6d587287bdc9220f0fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQyMzg0OA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r435423848", "bodyText": "Smile would be a better option here as it is more compact and would have less overhead while the query results are written to file.", "author": "a2l007", "createdAt": "2020-06-04T17:23:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc5Njg5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTY0MDQwMA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r435640400", "bodyText": "ah yeah this makes sense. I saw the constructors in SqlEntity and SqlInputFormat use an ObjectMapper that wasn't annotated with @Smile so I was confused why only the InputSource needed a smile object mapper. It's because this is the only guice injected ObjectMapper, the other constructors are not guicified, so they also use the smile ObjectMapper", "author": "suneet-s", "createdAt": "2020-06-05T01:21:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc5Njg5NQ=="}], "type": "inlineReview"}, {"oid": "eefc29262357e2e4bb5025b0e4a13bcb64931e8a", "url": "https://github.com/apache/druid/commit/eefc29262357e2e4bb5025b0e4a13bcb64931e8a", "message": "Add module test", "committedDate": "2020-06-04T17:18:29Z", "type": "commit"}, {"oid": "dd9d5bb96c010790725c081ae2d0efab15ae49b0", "url": "https://github.com/apache/druid/commit/dd9d5bb96c010790725c081ae2d0efab15ae49b0", "message": "Merge branch 'master' of https://github.com/druid-io/druid into sqlinputsource", "committedDate": "2020-06-04T17:18:54Z", "type": "commit"}, {"oid": "fe731f78c8ffb97a9d072e33da392cf4a783cc2c", "url": "https://github.com/apache/druid/commit/fe731f78c8ffb97a9d072e33da392cf4a783cc2c", "message": "Fix md in docs", "committedDate": "2020-06-04T18:09:23Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTY0MTU5MA==", "url": "https://github.com/apache/druid/pull/9449#discussion_r435641590", "bodyText": "nit: @VisibleForTesting is no longer true.", "author": "suneet-s", "createdAt": "2020-06-05T01:26:38Z", "path": "server/src/main/java/org/apache/druid/metadata/SQLMetadataStorageActionHandler.java", "diffHunk": "@@ -174,7 +174,7 @@ public void insert(\n   }\n \n   @VisibleForTesting", "originalCommit": "fe731f78c8ffb97a9d072e33da392cf4a783cc2c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "08b662647de8ad752194399561ff5102df492547", "url": "https://github.com/apache/druid/commit/08b662647de8ad752194399561ff5102df492547", "message": "Merge branch 'master' of https://github.com/druid-io/druid into sqlinputsource", "committedDate": "2020-06-05T13:27:37Z", "type": "commit"}, {"oid": "9550f53ef3616ad7cb996a60782b462a706e3e07", "url": "https://github.com/apache/druid/commit/9550f53ef3616ad7cb996a60782b462a706e3e07", "message": "Remove annotation", "committedDate": "2020-06-05T14:07:46Z", "type": "commit"}]}