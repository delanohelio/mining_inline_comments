{"pr_number": 587, "pr_title": "GH #570 : How to maintain message order and avoid message duplication with the idempotent producer", "pr_createdAt": "2020-10-12T00:02:14Z", "pr_url": "https://github.com/confluentinc/kafka-tutorials/pull/587", "timeline": [{"oid": "62f32b59641d91cf391289894d23ca5fcd726cda", "url": "https://github.com/confluentinc/kafka-tutorials/commit/62f32b59641d91cf391289894d23ca5fcd726cda", "message": "GH-570: DEVX-2108: initial commit for message ordering", "committedDate": "2020-10-05T01:43:15Z", "type": "commit"}, {"oid": "40fc6b8ada53342386b7a2733b7dc5241ce0fa01", "url": "https://github.com/confluentinc/kafka-tutorials/commit/40fc6b8ada53342386b7a2733b7dc5241ce0fa01", "message": "Rename ksql folder to kafka folder", "committedDate": "2020-10-05T01:49:04Z", "type": "commit"}, {"oid": "79b413c8eb336b5811a1114ea087dd66ea4af6f9", "url": "https://github.com/confluentinc/kafka-tutorials/commit/79b413c8eb336b5811a1114ea087dd66ea4af6f9", "message": "Create original topic with 2 partitions", "committedDate": "2020-10-05T01:52:28Z", "type": "commit"}, {"oid": "abf99e610c8d103ce477bfc03667eb46077412da", "url": "https://github.com/confluentinc/kafka-tutorials/commit/abf99e610c8d103ce477bfc03667eb46077412da", "message": "Clean up introduction", "committedDate": "2020-10-05T01:59:02Z", "type": "commit"}, {"oid": "59faa2b74fb78893c12b7ad44859459b81543a41", "url": "https://github.com/confluentinc/kafka-tutorials/commit/59faa2b74fb78893c12b7ad44859459b81543a41", "message": "Remove unnecessary files (part 1)", "committedDate": "2020-10-05T02:16:46Z", "type": "commit"}, {"oid": "7994fad79e44df32b623f63c04e8f342e1de8232", "url": "https://github.com/confluentinc/kafka-tutorials/commit/7994fad79e44df32b623f63c04e8f342e1de8232", "message": "acks=all", "committedDate": "2020-10-05T02:31:17Z", "type": "commit"}, {"oid": "6ecd04b8c8c0d79b9773915c00898517a480ff8d", "url": "https://github.com/confluentinc/kafka-tutorials/commit/6ecd04b8c8c0d79b9773915c00898517a480ff8d", "message": "Remove max.in.flight.requests.per.connection", "committedDate": "2020-10-05T02:33:55Z", "type": "commit"}, {"oid": "c0a1a006a8590f67c631fcf28f2eb277883e4c1f", "url": "https://github.com/confluentinc/kafka-tutorials/commit/c0a1a006a8590f67c631fcf28f2eb277883e4c1f", "message": "Add note about max.in.flight.requests.per.connection", "committedDate": "2020-10-09T13:37:31Z", "type": "commit"}, {"oid": "4e8df1cb3c7b9f5611f4da66e8189ae71e6c0a76", "url": "https://github.com/confluentinc/kafka-tutorials/commit/4e8df1cb3c7b9f5611f4da66e8189ae71e6c0a76", "message": "Tweak", "committedDate": "2020-10-09T13:47:53Z", "type": "commit"}, {"oid": "993467159c190c8e66023047d17c7e7cce038034", "url": "https://github.com/confluentinc/kafka-tutorials/commit/993467159c190c8e66023047d17c7e7cce038034", "message": "Add new steps", "committedDate": "2020-10-09T19:01:11Z", "type": "commit"}, {"oid": "2967d9a544ca3fea0aa04f3c795ee5ea1c756ede", "url": "https://github.com/confluentinc/kafka-tutorials/commit/2967d9a544ca3fea0aa04f3c795ee5ea1c756ede", "message": "Checkpoint", "committedDate": "2020-10-10T00:21:55Z", "type": "commit"}, {"oid": "39efd72bea941f3aab923bf8c67b6fbe3e935802", "url": "https://github.com/confluentinc/kafka-tutorials/commit/39efd72bea941f3aab923bf8c67b6fbe3e935802", "message": "Rename topic from topic1 to myTopic", "committedDate": "2020-10-10T00:47:33Z", "type": "commit"}, {"oid": "bc729babfe05c77008d1569757d29aaca5feaf9f", "url": "https://github.com/confluentinc/kafka-tutorials/commit/bc729babfe05c77008d1569757d29aaca5feaf9f", "message": "Cleanup", "committedDate": "2020-10-10T01:26:55Z", "type": "commit"}, {"oid": "530f3cf8121dda260bae77df4075b5efd6429fc5", "url": "https://github.com/confluentinc/kafka-tutorials/commit/530f3cf8121dda260bae77df4075b5efd6429fc5", "message": "Tweak", "committedDate": "2020-10-10T01:37:32Z", "type": "commit"}, {"oid": "321355f80d6eaf7fde592980e24b2487ebc1c2ae", "url": "https://github.com/confluentinc/kafka-tutorials/commit/321355f80d6eaf7fde592980e24b2487ebc1c2ae", "message": "Accurate outputs", "committedDate": "2020-10-10T02:24:37Z", "type": "commit"}, {"oid": "01449cc056c9a62f97f4514fb7d9aa9a5a6981b0", "url": "https://github.com/confluentinc/kafka-tutorials/commit/01449cc056c9a62f97f4514fb7d9aa9a5a6981b0", "message": "Markup", "committedDate": "2020-10-10T02:45:43Z", "type": "commit"}, {"oid": "766d8674d2c4dea3b0c8930b8acc50983cbd52cc", "url": "https://github.com/confluentinc/kafka-tutorials/commit/766d8674d2c4dea3b0c8930b8acc50983cbd52cc", "message": "Change container criteria", "committedDate": "2020-10-10T03:10:38Z", "type": "commit"}, {"oid": "54f3cde716e829995894c63ba4610218c2ec930d", "url": "https://github.com/confluentinc/kafka-tutorials/commit/54f3cde716e829995894c63ba4610218c2ec930d", "message": "Minor tweaks; add file for answer", "committedDate": "2020-10-10T12:46:02Z", "type": "commit"}, {"oid": "d9901b61c6a8a3bf122df8a4f5e6fd90e4f3a0dc", "url": "https://github.com/confluentinc/kafka-tutorials/commit/d9901b61c6a8a3bf122df8a4f5e6fd90e4f3a0dc", "message": "Working on a clean make", "committedDate": "2020-10-11T23:10:59Z", "type": "commit"}, {"oid": "445be4c82ab50c8abb6be792afcb4e9d9e035e0c", "url": "https://github.com/confluentinc/kafka-tutorials/commit/445be4c82ab50c8abb6be792afcb4e9d9e035e0c", "message": "website and make succeess", "committedDate": "2020-10-12T00:01:02Z", "type": "commit"}, {"oid": "c803f45d4ab021f0f9f793924358056407895717", "url": "https://github.com/confluentinc/kafka-tutorials/commit/c803f45d4ab021f0f9f793924358056407895717", "message": "Merge branch 'master' into GH-570", "committedDate": "2020-10-12T00:02:41Z", "type": "commit"}, {"oid": "31fa6d59c731ad8f4282d3a16f16f65465c5ecdd", "url": "https://github.com/confluentinc/kafka-tutorials/commit/31fa6d59c731ad8f4282d3a16f16f65465c5ecdd", "message": "Update .semaphore/semaphore.yml KSQL -> Kafka", "committedDate": "2020-10-12T00:09:06Z", "type": "commit"}, {"oid": "a804e847e31454bc432f3edc10e8376b9384b415", "url": "https://github.com/confluentinc/kafka-tutorials/commit/a804e847e31454bc432f3edc10e8376b9384b415", "message": "Add KT to settings.gradle", "committedDate": "2020-10-12T00:11:37Z", "type": "commit"}, {"oid": "78f7a60fa79c2b31e6399c1f747b020a474747b4", "url": "https://github.com/confluentinc/kafka-tutorials/commit/78f7a60fa79c2b31e6399c1f747b020a474747b4", "message": "Retitle card", "committedDate": "2020-10-12T00:15:57Z", "type": "commit"}, {"oid": "d91f85eb54eda05dc74a1f2e6b8cbfae934ff539", "url": "https://github.com/confluentinc/kafka-tutorials/commit/d91f85eb54eda05dc74a1f2e6b8cbfae934ff539", "message": "Tweak", "committedDate": "2020-10-12T00:18:26Z", "type": "commit"}, {"oid": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa", "url": "https://github.com/confluentinc/kafka-tutorials/commit/f8e8cd9dcaea79f5aced11317803abd8fd9c7daa", "message": "Tweak", "committedDate": "2020-10-12T00:20:46Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDI4OA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503030288", "bodyText": "Maybe it's intentional, but NO-KEY is a key, and will result in all records being sent to the same partition (vs null, which would result in using the uniform sticky partitioner instead)", "author": "mikebin", "createdAt": "2020-10-12T04:17:22Z", "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";", "originalCommit": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI0Nzk2OA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503247968", "bodyText": "@mikebin good point.  Not intentional in that this code is duplicated from kafka-producer-application (cc: @bbejeck ) but in that case, IIUC the partitioner didn't matter.\nSo stepping back, what is the desirable behavior for this tutorial if there is no key?  I think a true null and round robin to use both partitions would better demonstrate ordering on a per-partition basis.  If you agree, I'll modify the code accordingly.  WDYT?", "author": "ybyzek", "createdAt": "2020-10-12T12:02:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDI4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzMxMjIxMg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503312212", "bodyText": "null makes sense if no key is provided, although in this specific tutorial, keys are provided in the input data, so only if the user experiments with their own data would that code be relevant.", "author": "mikebin", "createdAt": "2020-10-12T13:53:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDI4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzMyMjg4Mg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503322882", "bodyText": "I've tested this and it seems that the default partitioner behavior is, as described in https://github.com/apache/kafka/blob/2.6/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java#L30:\n\nIf no partition or key is present choose the sticky partition that changes when the batch is full\n\nNote that it changes when the batch is full, and as it turns out, with the current application all the null-keyed messages get put into the same batch so they all land in the same partition anyway.  If we wanted to force a different behavior, we could set batch.size=0 to get more batches and get some kind of distribution, but that conflates different subjects in one KT and could lead to bad copy/paste.\nSo I'm thinking to change the code to allow no keys such that a real null key gets written, but we just need to understand that it doesn't necessarily mean they will round robin because they will likely land in the same batch.\nWDYT?", "author": "ybyzek", "createdAt": "2020-10-12T14:10:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDI4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzMzMDI1OA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503330258", "bodyText": "SGTM", "author": "mikebin", "createdAt": "2020-10-12T14:22:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDI4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg0MDI4NA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r505840284", "bodyText": "The \"NO-KEY\" was intentional to provide a default key if a record didn't have one after splitting the line.  Probably poorly worded.", "author": "bbejeck", "createdAt": "2020-10-15T20:53:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDI4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDQwMA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503030400", "bodyText": "Not required, but doesn't hurt to explicitly set", "author": "mikebin", "createdAt": "2020-10-12T04:17:56Z", "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);\n+    }\n+\n+    public void shutdown() {\n+        producer.close();\n+    }\n+\n+    public static Properties loadProperties(String fileName) throws IOException {\n+        final Properties envProps = new Properties();\n+        final FileInputStream input = new FileInputStream(fileName);\n+        envProps.load(input);\n+        input.close();\n+\n+        return envProps;\n+    }\n+\n+    public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                              final String fileName) {\n+        System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+        metadata.forEach(m -> {\n+            try {\n+                final RecordMetadata recordMetadata = m.get();\n+                System.out.println(\"Record written to offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp() + \" partition \" + recordMetadata.partition());\n+            } catch (InterruptedException | ExecutionException e) {\n+                if (e instanceof InterruptedException) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            }\n+        });\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length < 2) {\n+            throw new IllegalArgumentException(\n+                    \"This program takes two arguments: the path to an environment configuration file and\" +\n+                            \"the path to the file with records to send\");\n+        }\n+\n+        final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+\n+        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\n+        props.put(ProducerConfig.ACKS_CONFIG, \"all\");", "originalCommit": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI4ODM5NA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503288394", "bodyText": "@mikebin revised copy available, WDYT?", "author": "ybyzek", "createdAt": "2020-10-12T13:15:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDQwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzMxMjg1Nw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503312857", "bodyText": "LGTM", "author": "mikebin", "createdAt": "2020-10-12T13:54:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDQwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDQ4OA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503030488", "bodyText": "Since a shutdown hook is configured, this additional call to shutdown isn't needed.", "author": "mikebin", "createdAt": "2020-10-12T04:18:25Z", "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);\n+    }\n+\n+    public void shutdown() {\n+        producer.close();\n+    }\n+\n+    public static Properties loadProperties(String fileName) throws IOException {\n+        final Properties envProps = new Properties();\n+        final FileInputStream input = new FileInputStream(fileName);\n+        envProps.load(input);\n+        input.close();\n+\n+        return envProps;\n+    }\n+\n+    public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                              final String fileName) {\n+        System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+        metadata.forEach(m -> {\n+            try {\n+                final RecordMetadata recordMetadata = m.get();\n+                System.out.println(\"Record written to offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp() + \" partition \" + recordMetadata.partition());\n+            } catch (InterruptedException | ExecutionException e) {\n+                if (e instanceof InterruptedException) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            }\n+        });\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length < 2) {\n+            throw new IllegalArgumentException(\n+                    \"This program takes two arguments: the path to an environment configuration file and\" +\n+                            \"the path to the file with records to send\");\n+        }\n+\n+        final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+\n+        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\n+        props.put(ProducerConfig.ACKS_CONFIG, \"all\");\n+\n+        props.put(ProducerConfig.CLIENT_ID_CONFIG, \"myApp\");\n+        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+\n+        final String topic = props.getProperty(\"output.topic.name\");\n+        final Producer<String, String> producer = new KafkaProducer<>(props);\n+        final KafkaProducerApplication producerApp = new KafkaProducerApplication(producer, topic);\n+\n+        // Attach shutdown handler to catch Control-C.\n+        Runtime.getRuntime().addShutdownHook(new Thread(producerApp::shutdown));\n+        String filePath = args[1];\n+        try {\n+            List<String> linesToProduce = Files.readAllLines(Paths.get(filePath));\n+            List<Future<RecordMetadata>> metadata = linesToProduce.stream()\n+                    .filter(l -> !l.trim().isEmpty())\n+                    .map(producerApp::produce)\n+                    .collect(Collectors.toList());\n+            producerApp.printMetadata(metadata, filePath);\n+\n+        } catch (IOException e) {\n+            System.err.println(String.format(\"Error reading file %s due to %s\", filePath, e));\n+        }\n+        producerApp.shutdown();", "originalCommit": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI1MDExOA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503250118", "bodyText": "@mikebin , ok, I can validate and remove the call to shutdown().\n@bbejeck similar to the previous comment, this code is also present in other KTs. Assuming you agree with this removal, do you have a preference whether this PR addresses those KTs or should I file a separate GH issue?", "author": "ybyzek", "createdAt": "2020-10-12T12:07:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDQ4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTgzMTE4NQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r505831185", "bodyText": "@mikebin  good catch\n\n@bbejeck similar to the previous comment, this code is also present in other KTs. Assuming you agree with this removal, do you have a preference whether this PR addresses those KTs or should I file a separate GH issue?\n\nthat was an oversight on my part during refactoring, I'll clean it up in a separate PR\nEDIT: When I originally developed the tutorial it would continue running until the user did a CTRL+C hence the shutdown hook.  But later I refactored to produce the records in the file and shutdown.\nSo I'd say put the producerApp.shutdown() back in and remove the shutdown hook, I've done the same in the corresponding tutorial.", "author": "bbejeck", "createdAt": "2020-10-15T20:43:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDQ4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY2Mzk4Mw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r509663983", "bodyText": "AI for self: mirror changes that @bbejeck did in #594", "author": "ybyzek", "createdAt": "2020-10-21T20:23:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDQ4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDE2Njg0NQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r510166845", "bodyText": "Addressed by 44f2b42", "author": "ybyzek", "createdAt": "2020-10-22T13:36:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMDQ4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTIzMw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503031233", "bodyText": "It's probably more common to provide a callback to asynchronously handle acks, instead of handling the Future returned by send directly", "author": "mikebin", "createdAt": "2020-10-12T04:21:57Z", "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);", "originalCommit": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzM2NTk3NA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503365974", "bodyText": "cc: @bbejeck also borrowed from the other KT.  Any leaning on how to fix and where?", "author": "ybyzek", "createdAt": "2020-10-12T15:17:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTIzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDE2Nzc4NQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r510167785", "bodyText": "@bbejeck did you have a leaning on whether to leave this as-is, or whether to implement Callback?", "author": "ybyzek", "createdAt": "2020-10-22T13:37:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTIzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTA0MDg3Mw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r511040873", "bodyText": "Thanks for discussing @bbejeck , as we agreed, I will add Callbacks just to this KT and no impact to other KTs", "author": "ybyzek", "createdAt": "2020-10-23T17:44:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTIzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTU4OTEyNA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r511589124", "bodyText": "@bbejeck @mikebin 4214a91 implements the callback", "author": "ybyzek", "createdAt": "2020-10-25T12:17:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTIzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMzIyMw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503033223", "bodyText": "This could be simplified to:\nSystem.err.printf(\"Error reading file %s due to %s%n\", filePath, e);", "author": "mikebin", "createdAt": "2020-10-12T04:31:37Z", "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,111 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public Future<RecordMetadata> produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = \"NO-KEY\";\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        return producer.send(producerRecord);\n+    }\n+\n+    public void shutdown() {\n+        producer.close();\n+    }\n+\n+    public static Properties loadProperties(String fileName) throws IOException {\n+        final Properties envProps = new Properties();\n+        final FileInputStream input = new FileInputStream(fileName);\n+        envProps.load(input);\n+        input.close();\n+\n+        return envProps;\n+    }\n+\n+    public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                              final String fileName) {\n+        System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+        metadata.forEach(m -> {\n+            try {\n+                final RecordMetadata recordMetadata = m.get();\n+                System.out.println(\"Record written to offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp() + \" partition \" + recordMetadata.partition());\n+            } catch (InterruptedException | ExecutionException e) {\n+                if (e instanceof InterruptedException) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            }\n+        });\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length < 2) {\n+            throw new IllegalArgumentException(\n+                    \"This program takes two arguments: the path to an environment configuration file and\" +\n+                            \"the path to the file with records to send\");\n+        }\n+\n+        final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+\n+        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\n+        props.put(ProducerConfig.ACKS_CONFIG, \"all\");\n+\n+        props.put(ProducerConfig.CLIENT_ID_CONFIG, \"myApp\");\n+        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+\n+        final String topic = props.getProperty(\"output.topic.name\");\n+        final Producer<String, String> producer = new KafkaProducer<>(props);\n+        final KafkaProducerApplication producerApp = new KafkaProducerApplication(producer, topic);\n+\n+        // Attach shutdown handler to catch Control-C.\n+        Runtime.getRuntime().addShutdownHook(new Thread(producerApp::shutdown));\n+        String filePath = args[1];\n+        try {\n+            List<String> linesToProduce = Files.readAllLines(Paths.get(filePath));\n+            List<Future<RecordMetadata>> metadata = linesToProduce.stream()\n+                    .filter(l -> !l.trim().isEmpty())\n+                    .map(producerApp::produce)\n+                    .collect(Collectors.toList());\n+            producerApp.printMetadata(metadata, filePath);\n+\n+        } catch (IOException e) {\n+            System.err.println(String.format(\"Error reading file %s due to %s\", filePath, e));", "originalCommit": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI0OTM2NA==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503249364", "bodyText": "@mikebin , sure, I can make this simplification.\n@bbejeck this code is also present in kafka-producer-application KT and kafka-producer-application-callback KT.  Assuming you agree with this simplification, do you have a preference whether this PR addresses those KTs or should I file a separate GH issue?", "author": "ybyzek", "createdAt": "2020-10-12T12:05:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMzIyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTgzMTk4Nw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r505831987", "bodyText": "same as above I'll put it in the same PR", "author": "bbejeck", "createdAt": "2020-10-15T20:44:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMzIyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY2NDI1Mw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r509664253", "bodyText": "AI for self: mirror changes that @bbejeck did in #594", "author": "ybyzek", "createdAt": "2020-10-21T20:23:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMzIyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDE2Njc0MQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r510166741", "bodyText": "Addressed by 44f2b42", "author": "ybyzek", "createdAt": "2020-10-22T13:35:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMzIyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMzUzOQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503033539", "bodyText": "Is this intended to show up somewhere in the tutorial? I didn't see it on staging. Maybe it's only for automated internal testing with Semaphore?", "author": "mikebin", "createdAt": "2020-10-12T04:33:12Z", "path": "_includes/tutorials/message-ordering/kafka/code/src/test/java/io/confluent/developer/KafkaProducerApplicationTest.java", "diffHunk": "@@ -0,0 +1,47 @@\n+package io.confluent.developer;", "originalCommit": "f8e8cd9dcaea79f5aced11317803abd8fd9c7daa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzM3MTIzNQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r503371235", "bodyText": "Hmmm, not sure about this one.", "author": "ybyzek", "createdAt": "2020-10-12T15:26:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMzUzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTg0MzE3NQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r505843175", "bodyText": "The project this KT was cloned from has a different test section that uses a unit test.   Part of the build runs only the unit tests via gradle and not the harness runner, so it might be a good idea to leave it in.", "author": "bbejeck", "createdAt": "2020-10-15T20:57:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMzUzOQ=="}], "type": "inlineReview"}, {"oid": "c41a2f7d46e417f51e81f1a7623959f1ba446302", "url": "https://github.com/confluentinc/kafka-tutorials/commit/c41a2f7d46e417f51e81f1a7623959f1ba446302", "message": "Replace kafka-run-class kafka.tools.DumpLogSegments with kafka-dump-log", "committedDate": "2020-10-12T12:23:18Z", "type": "commit"}, {"oid": "0bf4585c706dbe2f3f54692a1b3d05903278d7b3", "url": "https://github.com/confluentinc/kafka-tutorials/commit/0bf4585c706dbe2f3f54692a1b3d05903278d7b3", "message": "Mention retries; remove additional reference to DumpLogSegments; fix grammar", "committedDate": "2020-10-12T12:51:48Z", "type": "commit"}, {"oid": "2b8744b67920f3de5ec1345cc8a61a5fb1a035f2", "url": "https://github.com/confluentinc/kafka-tutorials/commit/2b8744b67920f3de5ec1345cc8a61a5fb1a035f2", "message": "Tweak", "committedDate": "2020-10-12T12:55:45Z", "type": "commit"}, {"oid": "a1845b6d2febd7bfc2afb693a599cefdee14f91b", "url": "https://github.com/confluentinc/kafka-tutorials/commit/a1845b6d2febd7bfc2afb693a599cefdee14f91b", "message": "Update answer; update description of parameters", "committedDate": "2020-10-12T13:14:51Z", "type": "commit"}, {"oid": "f3ce318705e67a17902ebecfe205e8539d338180", "url": "https://github.com/confluentinc/kafka-tutorials/commit/f3ce318705e67a17902ebecfe205e8539d338180", "message": "Write real null key if none is specified", "committedDate": "2020-10-12T14:13:11Z", "type": "commit"}, {"oid": "1eac90b90b32c61fd673ebc2a1979e368f8d4489", "url": "https://github.com/confluentinc/kafka-tutorials/commit/1eac90b90b32c61fd673ebc2a1979e368f8d4489", "message": "Shout out to transactions", "committedDate": "2020-10-12T14:38:22Z", "type": "commit"}, {"oid": "12e0b49c39a1035b172e96a1c1835347b5900090", "url": "https://github.com/confluentinc/kafka-tutorials/commit/12e0b49c39a1035b172e96a1c1835347b5900090", "message": "Remove unnecessary lines from build.gradle", "committedDate": "2020-10-12T15:21:56Z", "type": "commit"}, {"oid": "f03c5e4dc979d99bed65cc0477d45ecff9e99b72", "url": "https://github.com/confluentinc/kafka-tutorials/commit/f03c5e4dc979d99bed65cc0477d45ecff9e99b72", "message": "Use kafka-clients and explicitly add jackson", "committedDate": "2020-10-12T15:40:25Z", "type": "commit"}, {"oid": "8cd2c4be3492fc5e29cfe14624b7feb0b214dfeb", "url": "https://github.com/confluentinc/kafka-tutorials/commit/8cd2c4be3492fc5e29cfe14624b7feb0b214dfeb", "message": "Tweak introduction", "committedDate": "2020-10-12T17:38:46Z", "type": "commit"}, {"oid": "83cf59fb1a67cca371c2bbce1f0d4ca3fa77c686", "url": "https://github.com/confluentinc/kafka-tutorials/commit/83cf59fb1a67cca371c2bbce1f0d4ca3fa77c686", "message": "Change back to kafka-streams to avoid error in test", "committedDate": "2020-10-13T00:03:35Z", "type": "commit"}, {"oid": "fe8636c038bd624991f94950fbc066bd55a92b4c", "url": "https://github.com/confluentinc/kafka-tutorials/commit/fe8636c038bd624991f94950fbc066bd55a92b4c", "message": "Unit test passes", "committedDate": "2020-10-13T00:05:49Z", "type": "commit"}, {"oid": "9429d36e8bc4f19e60f600168da3421dec2b338e", "url": "https://github.com/confluentinc/kafka-tutorials/commit/9429d36e8bc4f19e60f600168da3421dec2b338e", "message": "Merge branch 'master' into GH-570", "committedDate": "2020-10-22T13:28:56Z", "type": "commit"}, {"oid": "44f2b420df1bc802781c0dc7e9c8fde322691f3b", "url": "https://github.com/confluentinc/kafka-tutorials/commit/44f2b420df1bc802781c0dc7e9c8fde322691f3b", "message": "Remove addShutdownHook; update error message to System.err.printf", "committedDate": "2020-10-22T13:35:32Z", "type": "commit"}, {"oid": "4214a91f008525b00d78f89479dcca72cfd89fde", "url": "https://github.com/confluentinc/kafka-tutorials/commit/4214a91f008525b00d78f89479dcca72cfd89fde", "message": "Implement Callback() and update output", "committedDate": "2020-10-25T12:16:43Z", "type": "commit"}, {"oid": "5191ad0e70aa2532565ad6b0ebd29c7c9b1e9ead", "url": "https://github.com/confluentinc/kafka-tutorials/commit/5191ad0e70aa2532565ad6b0ebd29c7c9b1e9ead", "message": "Grammar fix", "committedDate": "2020-10-25T21:50:54Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjkyMDkzNw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r512920937", "bodyText": "I'd probably write this as a lambda instead of an anonymous inner class (more concise, and implicitly gives you access to the producerRecord inside the callback), but not critical.", "author": "mikebin", "createdAt": "2020-10-27T18:10:59Z", "path": "_includes/tutorials/message-ordering/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,103 @@\n+package io.confluent.developer;\n+\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+\n+public class KafkaProducerApplication {\n+\n+    private final Producer<String, String> producer;\n+    final String outTopic;\n+\n+    public KafkaProducerApplication(final Producer<String, String> producer,\n+                                    final String topic) {\n+        this.producer = producer;\n+        outTopic = topic;\n+    }\n+\n+    public void produce(final String message) {\n+        final String[] parts = message.split(\"-\");\n+        final String key, value;\n+        if (parts.length > 1) {\n+            key = parts[0];\n+            value = parts[1];\n+        } else {\n+            key = null;\n+            value = parts[0];\n+        }\n+        final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+        producer.send(producerRecord,\n+            new Callback() {", "originalCommit": "5191ad0e70aa2532565ad6b0ebd29c7c9b1e9ead", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjk0Nzk3MQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r512947971", "bodyText": "@mikebin very cool suggestion .\n@mikebin  and @bbejeck WDYT about the implementation 1d8572b ?", "author": "ybyzek", "createdAt": "2020-10-27T18:53:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjkyMDkzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjk1MTQ5NQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/587#discussion_r512951495", "bodyText": "LGTM!", "author": "mikebin", "createdAt": "2020-10-27T18:59:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjkyMDkzNw=="}], "type": "inlineReview"}, {"oid": "1d8572ba985096b03078355bb948487e8837bb47", "url": "https://github.com/confluentinc/kafka-tutorials/commit/1d8572ba985096b03078355bb948487e8837bb47", "message": "Convert callback to lambda", "committedDate": "2020-10-27T18:51:56Z", "type": "commit"}, {"oid": "03f07fe17d7de9f988b2d0c1ef1f31962a7a1031", "url": "https://github.com/confluentinc/kafka-tutorials/commit/03f07fe17d7de9f988b2d0c1ef1f31962a7a1031", "message": "Tweak", "committedDate": "2020-10-28T13:32:29Z", "type": "commit"}]}