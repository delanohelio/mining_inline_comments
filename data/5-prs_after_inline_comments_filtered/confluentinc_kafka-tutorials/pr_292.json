{"pr_number": 292, "pr_title": "Use ValueTransformerWithKey instead of Transformer", "pr_createdAt": "2020-03-19T15:26:15Z", "pr_url": "https://github.com/confluentinc/kafka-tutorials/pull/292", "timeline": [{"oid": "8befd46de67fb932368e2a2fc22f03ffae2394be", "url": "https://github.com/confluentinc/kafka-tutorials/commit/8befd46de67fb932368e2a2fc22f03ffae2394be", "message": "use ValueTransformerWithKey instead of Transformer", "committedDate": "2020-03-19T15:16:08Z", "type": "commit"}, {"oid": "57793a4fa13ccfe22d4037c4df462fcf062c31ab", "url": "https://github.com/confluentinc/kafka-tutorials/commit/57793a4fa13ccfe22d4037c4df462fcf062c31ab", "message": "update the tutoral", "committedDate": "2020-04-07T12:05:18Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDk1MTA0Mg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/292#discussion_r404951042", "bodyText": "Suggestion - I know this is pre-existing code, but can we update close() to close(Duration.ofSeconds(5))", "author": "bbejeck", "createdAt": "2020-04-07T16:35:25Z", "path": "_includes/tutorials/finding-distinct/kstreams/code/src/main/java/io/confluent/developer/FindDistinctEvents.java", "diffHunk": "@@ -30,224 +30,227 @@\n \n public class FindDistinctEvents {\n \n-  private static final String storeName = \"eventId-store\";\n-\n-  /**\n-   * Discards duplicate click events from the input stream by ip address\n-   * <p>\n-   * Duplicate records are detected based on ip address\n-   * The transformer remembers known ip addresses within an associated window state\n-   * store, which automatically purges/expires IPs from the store after a certain amount of\n-   * time has passed to prevent the store from growing indefinitely.\n-   * <p>\n-   * Note: This code is for demonstration purposes and was not tested for production usage.\n-   */\n-  private static class DeduplicationTransformer<K, V, E> implements Transformer<K, V, KeyValue<K, V>> {\n-\n-    private ProcessorContext context;\n+    private static final String storeName = \"eventId-store\";\n \n     /**\n-     * Key: ip address\n-     * Value: timestamp (event-time) of the corresponding event when the event ID was seen for the\n-     * first time\n+     * Discards duplicate click events from the input stream by ip address\n+     * <p>\n+     * Duplicate records are detected based on ip address\n+     * The transformer remembers known ip addresses within an associated window state\n+     * store, which automatically purges/expires IPs from the store after a certain amount of\n+     * time has passed to prevent the store from growing indefinitely.\n+     * <p>\n+     * Note: This code is for demonstration purposes and was not tested for production usage.\n      */\n-    private WindowStore<E, Long> eventIdStore;\n+    private static class DeduplicationTransformer<K, V, E> implements ValueTransformerWithKey<K, V, V> {\n+\n+        private ProcessorContext context;\n+\n+        /**\n+         * Key: ip address\n+         * Value: timestamp (event-time) of the corresponding event when the event ID was seen for the\n+         * first time\n+         */\n+        private WindowStore<E, Long> eventIdStore;\n+\n+        private final long leftDurationMs;\n+        private final long rightDurationMs;\n+\n+        private final KeyValueMapper<K, V, E> idExtractor;\n+\n+        /**\n+         * @param maintainDurationPerEventInMs how long to \"remember\" a known ip address\n+         *                                     during the time of which any incoming duplicates\n+         *                                     will be dropped, thereby de-duplicating the\n+         *                                     input.\n+         * @param idExtractor                  extracts a unique identifier from a record by which we de-duplicate input\n+         *                                     records; if it returns null, the record will not be considered for\n+         *                                     de-duping but forwarded as-is.\n+         */\n+        DeduplicationTransformer(final long maintainDurationPerEventInMs, final KeyValueMapper<K, V, E> idExtractor) {\n+            if (maintainDurationPerEventInMs < 1) {\n+                throw new IllegalArgumentException(\"maintain duration per event must be >= 1\");\n+            }\n+            leftDurationMs = maintainDurationPerEventInMs / 2;\n+            rightDurationMs = maintainDurationPerEventInMs - leftDurationMs;\n+            this.idExtractor = idExtractor;\n+        }\n \n-    private final long leftDurationMs;\n-    private final long rightDurationMs;\n+        @Override\n+        @SuppressWarnings(\"unchecked\")\n+        public void init(final ProcessorContext context) {\n+            this.context = context;\n+            eventIdStore = (WindowStore<E, Long>) context.getStateStore(storeName);\n+        }\n \n-    private final KeyValueMapper<K, V, E> idExtractor;\n+        @Override\n+        public V transform(final K key, final V value) {\n+            final E eventId = idExtractor.apply(key, value);\n+            if (eventId == null) {\n+                return value;\n+            } else {\n+                final V output;\n+                if (isDuplicate(eventId)) {\n+                    output = null;\n+                    updateTimestampOfExistingEventToPreventExpiry(eventId, context.timestamp());\n+                } else {\n+                    output = value;\n+                    rememberNewEvent(eventId, context.timestamp());\n+                }\n+                return output;\n+            }\n+        }\n \n-    /**\n-     * @param maintainDurationPerEventInMs how long to \"remember\" a known ip address\n-     *                                     during the time of which any incoming duplicates\n-     *                                     will be dropped, thereby de-duplicating the\n-     *                                     input.\n-     * @param idExtractor                  extracts a unique identifier from a record by which we de-duplicate input\n-     *                                     records; if it returns null, the record will not be considered for\n-     *                                     de-duping but forwarded as-is.\n-     */\n-    DeduplicationTransformer(final long maintainDurationPerEventInMs, final KeyValueMapper<K, V, E> idExtractor) {\n-      if (maintainDurationPerEventInMs < 1) {\n-        throw new IllegalArgumentException(\"maintain duration per event must be >= 1\");\n-      }\n-      leftDurationMs = maintainDurationPerEventInMs / 2;\n-      rightDurationMs = maintainDurationPerEventInMs - leftDurationMs;\n-      this.idExtractor = idExtractor;\n-    }\n+        private boolean isDuplicate(final E eventId) {\n+            final long eventTime = context.timestamp();\n+            final WindowStoreIterator<Long> timeIterator = eventIdStore.fetch(\n+                    eventId,\n+                    eventTime - leftDurationMs,\n+                    eventTime + rightDurationMs);\n+            final boolean isDuplicate = timeIterator.hasNext();\n+            timeIterator.close();\n+            return isDuplicate;\n+        }\n \n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    public void init(final ProcessorContext context) {\n-      this.context = context;\n-      eventIdStore = (WindowStore<E, Long>) context.getStateStore(storeName);\n-    }\n+        private void updateTimestampOfExistingEventToPreventExpiry(final E eventId, final long newTimestamp) {\n+            eventIdStore.put(eventId, newTimestamp, newTimestamp);\n+        }\n \n-    public KeyValue<K, V> transform(final K key, final V value) {\n-      final E eventId = idExtractor.apply(key, value);\n-      if (eventId == null) {\n-        return KeyValue.pair(key, value);\n-      } else {\n-        final KeyValue<K, V> output;\n-        if (isDuplicate(eventId)) {\n-          output = null;\n-          updateTimestampOfExistingEventToPreventExpiry(eventId, context.timestamp());\n-        } else {\n-          output = KeyValue.pair(key, value);\n-          rememberNewEvent(eventId, context.timestamp());\n+        private void rememberNewEvent(final E eventId, final long timestamp) {\n+            eventIdStore.put(eventId, timestamp, timestamp);\n         }\n-        return output;\n-      }\n+\n+        @Override\n+        public void close() {\n+            // Note: The store should NOT be closed manually here via `eventIdStore.close()`!\n+            // The Kafka Streams API will automatically close stores when necessary.\n+        }\n+\n     }\n \n-    private boolean isDuplicate(final E eventId) {\n-      final long eventTime = context.timestamp();\n-      final WindowStoreIterator<Long> timeIterator = eventIdStore.fetch(\n-              eventId,\n-              eventTime - leftDurationMs,\n-              eventTime + rightDurationMs);\n-      final boolean isDuplicate = timeIterator.hasNext();\n-      timeIterator.close();\n-      return isDuplicate;\n+    private SpecificAvroSerde<Click> buildClicksSerde(final Properties envProps) {\n+        final SpecificAvroSerde<Click> serde = new SpecificAvroSerde<>();\n+        Map<String, String> config = new HashMap<>();\n+        config.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));\n+        serde.configure(config, false);\n+        return serde;\n     }\n \n-    private void updateTimestampOfExistingEventToPreventExpiry(final E eventId, final long newTimestamp) {\n-      eventIdStore.put(eventId, newTimestamp, newTimestamp);\n+    public Topology buildTopology(Properties envProps,\n+                                  final SpecificAvroSerde<Click> clicksSerde) {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+\n+        final String inputTopic = envProps.getProperty(\"input.topic.name\");\n+        final String outputTopic = envProps.getProperty(\"output.topic.name\");\n+\n+        // How long we \"remember\" an event.  During this time, any incoming duplicates of the event\n+        // will be, well, dropped, thereby de-duplicating the input data.\n+        //\n+        // The actual value depends on your use case.  To reduce memory and disk usage, you could\n+        // decrease the size to purge old windows more frequently at the cost of potentially missing out\n+        // on de-duplicating late-arriving records.\n+        final Duration windowSize = Duration.ofMinutes(2);\n+\n+        // retention period must be at least window size -- for this use case, we don't need a longer retention period\n+        // and thus just use the window size as retention time\n+        final Duration retentionPeriod = windowSize;\n+\n+        final StoreBuilder<WindowStore<String, Long>> dedupStoreBuilder = Stores.windowStoreBuilder(\n+                Stores.persistentWindowStore(storeName,\n+                        retentionPeriod,\n+                        windowSize,\n+                        false\n+                ),\n+                Serdes.String(),\n+                Serdes.Long());\n+\n+        builder.addStateStore(dedupStoreBuilder);\n+\n+        builder\n+                .stream(inputTopic, Consumed.with(Serdes.String(), clicksSerde))\n+                .transformValues(() -> new DeduplicationTransformer<>(windowSize.toMillis(), (key, value) -> value.getIp()), storeName)\n+                .filter((k, v) -> v != null)\n+                .to(outputTopic, Produced.with(Serdes.String(), clicksSerde));\n+\n+        return builder.build();\n     }\n \n-    private void rememberNewEvent(final E eventId, final long timestamp) {\n-      eventIdStore.put(eventId, timestamp, timestamp);\n+    public void createTopics(Properties envProps) {\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(\"bootstrap.servers\", envProps.getProperty(\"bootstrap.servers\"));\n+        AdminClient client = AdminClient.create(config);\n+\n+        List<NewTopic> topics = new ArrayList<>();\n+        topics.add(new NewTopic(\n+                envProps.getProperty(\"input.topic.name\"),\n+                Integer.parseInt(envProps.getProperty(\"input.topic.partitions\")),\n+                Short.parseShort(envProps.getProperty(\"input.topic.replication.factor\"))));\n+        topics.add(new NewTopic(\n+                envProps.getProperty(\"output.topic.name\"),\n+                Integer.parseInt(envProps.getProperty(\"output.topic.partitions\")),\n+                Short.parseShort(envProps.getProperty(\"output.topic.replication.factor\"))));\n+\n+        client.createTopics(topics);\n+        client.close();\n     }\n \n-    @Override\n-    public void close() {\n-      // Note: The store should NOT be closed manually here via `eventIdStore.close()`!\n-      // The Kafka Streams API will automatically close stores when necessary.\n+    public static Properties buildStreamsProperties(Properties envProps) {\n+        Properties props = new Properties();\n+\n+        props.put(StreamsConfig.APPLICATION_ID_CONFIG, envProps.getProperty(\"application.id\"));\n+        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, envProps.getProperty(\"bootstrap.servers\"));\n+        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n+        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n+        props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));\n+        props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n+\n+        return props;\n     }\n \n-  }\n-\n-  private SpecificAvroSerde<Click> buildClicksSerde(final Properties envProps) {\n-    final SpecificAvroSerde<Click> serde = new SpecificAvroSerde<>();\n-    Map<String, String> config = new HashMap<>();\n-    config.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));\n-    serde.configure(config, false);\n-    return serde;\n-  }\n-\n-  public Topology buildTopology(Properties envProps,\n-                                final SpecificAvroSerde<Click> clicksSerde) {\n-    final StreamsBuilder builder = new StreamsBuilder();\n-\n-    final String inputTopic = envProps.getProperty(\"input.topic.name\");\n-    final String outputTopic = envProps.getProperty(\"output.topic.name\");\n-\n-    // How long we \"remember\" an event.  During this time, any incoming duplicates of the event\n-    // will be, well, dropped, thereby de-duplicating the input data.\n-    //\n-    // The actual value depends on your use case.  To reduce memory and disk usage, you could\n-    // decrease the size to purge old windows more frequently at the cost of potentially missing out\n-    // on de-duplicating late-arriving records.\n-    final Duration windowSize = Duration.ofMinutes(2);\n-\n-    // retention period must be at least window size -- for this use case, we don't need a longer retention period\n-    // and thus just use the window size as retention time\n-    final Duration retentionPeriod = windowSize;\n-\n-    final StoreBuilder<WindowStore<String, Long>> dedupStoreBuilder = Stores.windowStoreBuilder(\n-            Stores.persistentWindowStore(storeName,\n-                    retentionPeriod,\n-                    windowSize,\n-                    false\n-            ),\n-            Serdes.String(),\n-            Serdes.Long());\n-\n-    builder.addStateStore(dedupStoreBuilder);\n-\n-    builder\n-      .stream(inputTopic, Consumed.with(Serdes.String(), clicksSerde))\n-      .transform( () -> new DeduplicationTransformer<>(windowSize.toMillis(), (key, value) -> value.getIp()), storeName)\n-      .to(outputTopic, Produced.with(Serdes.String(), clicksSerde));\n-\n-    return builder.build();\n-  }\n-\n-  public void createTopics(Properties envProps) {\n-    Map<String, Object> config = new HashMap<>();\n-    config.put(\"bootstrap.servers\", envProps.getProperty(\"bootstrap.servers\"));\n-    AdminClient client = AdminClient.create(config);\n-\n-    List<NewTopic> topics = new ArrayList<>();\n-    topics.add(new NewTopic(\n-        envProps.getProperty(\"input.topic.name\"),\n-        Integer.parseInt(envProps.getProperty(\"input.topic.partitions\")),\n-        Short.parseShort(envProps.getProperty(\"input.topic.replication.factor\"))));\n-    topics.add(new NewTopic(\n-        envProps.getProperty(\"output.topic.name\"),\n-        Integer.parseInt(envProps.getProperty(\"output.topic.partitions\")),\n-        Short.parseShort(envProps.getProperty(\"output.topic.replication.factor\"))));\n-\n-    client.createTopics(topics);\n-    client.close();\n-  }\n-\n-  public static Properties buildStreamsProperties(Properties envProps) {\n-    Properties props = new Properties();\n-\n-    props.put(StreamsConfig.APPLICATION_ID_CONFIG, envProps.getProperty(\"application.id\"));\n-    props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, envProps.getProperty(\"bootstrap.servers\"));\n-    props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n-    props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n-    props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, envProps.getProperty(\"schema.registry.url\"));\n-    props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n-\n-    return props;\n-  }\n-  public static Properties loadEnvProperties(String fileName) throws IOException {\n-    Properties envProps = new Properties();\n-    FileInputStream input = new FileInputStream(fileName);\n-    envProps.load(input);\n-    input.close();\n-\n-    return envProps;\n-  }\n-\n-  public static void main(String[] args) throws IOException {\n-    if (args.length < 1) {\n-      throw new IllegalArgumentException(\n-          \"This program takes one argument: the path to an environment configuration file.\");\n+    public static Properties loadEnvProperties(String fileName) throws IOException {\n+        Properties envProps = new Properties();\n+        FileInputStream input = new FileInputStream(fileName);\n+        envProps.load(input);\n+        input.close();\n+\n+        return envProps;\n     }\n \n-    new FindDistinctEvents().runRecipe(args[0]);\n-  }\n-\n-  private void runRecipe(final String configPath) throws IOException {\n-    Properties envProps = this.loadEnvProperties(configPath);\n-    Properties streamProps = this.buildStreamsProperties(envProps);\n-\n-    Topology topology = this.buildTopology(envProps, this.buildClicksSerde(envProps));\n-    this.createTopics(envProps);\n-\n-    final KafkaStreams streams = new KafkaStreams(topology, streamProps);\n-    final CountDownLatch latch = new CountDownLatch(1);\n-\n-    // Attach shutdown handler to catch Control-C.\n-    Runtime.getRuntime().addShutdownHook(new Thread(\"streams-shutdown-hook\") {\n-      @Override\n-      public void run() {\n-        streams.close();\n-        latch.countDown();\n-      }\n-    });\n-\n-    try {\n-      streams.start();\n-      latch.await();\n-    } catch (Throwable e) {\n-      System.exit(1);\n+    public static void main(String[] args) throws IOException {\n+        if (args.length < 1) {\n+            throw new IllegalArgumentException(\n+                    \"This program takes one argument: the path to an environment configuration file.\");\n+        }\n+\n+        new FindDistinctEvents().runRecipe(args[0]);\n     }\n-    System.exit(0);\n \n-  }\n+    private void runRecipe(final String configPath) throws IOException {\n+        Properties envProps = this.loadEnvProperties(configPath);\n+        Properties streamProps = this.buildStreamsProperties(envProps);\n+\n+        Topology topology = this.buildTopology(envProps, this.buildClicksSerde(envProps));\n+        this.createTopics(envProps);\n+\n+        final KafkaStreams streams = new KafkaStreams(topology, streamProps);\n+        final CountDownLatch latch = new CountDownLatch(1);\n+\n+        // Attach shutdown handler to catch Control-C.\n+        Runtime.getRuntime().addShutdownHook(new Thread(\"streams-shutdown-hook\") {\n+            @Override\n+            public void run() {\n+                streams.close();", "originalCommit": "57793a4fa13ccfe22d4037c4df462fcf062c31ab", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "7db2a3b6b55f941b4e60b2417cb41f1fbd99baed", "url": "https://github.com/confluentinc/kafka-tutorials/commit/7db2a3b6b55f941b4e60b2417cb41f1fbd99baed", "message": "code review corrections", "committedDate": "2020-04-07T17:15:45Z", "type": "commit"}, {"oid": "cd52f0f8393aea0611201cfa567dda3f7516cc39", "url": "https://github.com/confluentinc/kafka-tutorials/commit/cd52f0f8393aea0611201cfa567dda3f7516cc39", "message": "revert to 2.3.0", "committedDate": "2020-04-08T21:13:47Z", "type": "commit"}]}