{"pr_number": 491, "pr_title": "DEVX-1942: Create first producer application", "pr_createdAt": "2020-07-30T21:58:33Z", "pr_url": "https://github.com/confluentinc/kafka-tutorials/pull/491", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxNzI4Mg==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463317282", "bodyText": "Why is this implemented with the user being prompted to provide path name, instead of just an argument appended to java -jar build/libs/kafka-producer-application-standalone-0.0.1.jar configuration/dev.properties?  FYC, the latter has better UX", "author": "ybyzek", "createdAt": "2020-07-30T22:59:49Z", "path": "_includes/tutorials/kafka-producer-application/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,109 @@\n+package io.confluent.developer;\n+\n+\n+import java.io.BufferedReader;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,\n+                                  final String topic) {\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+  public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"#\");\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+    return producer.send(producerRecord);\n+  }\n+\n+  public void shutdown() {\n+    producer.close();\n+  }\n+\n+  public static Properties loadProperties(String fileName) throws IOException {\n+    final Properties envProps = new Properties();\n+    final FileInputStream input = new FileInputStream(fileName);\n+    envProps.load(input);\n+    input.close();\n+\n+    return envProps;\n+  }\n+\n+  public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                            final String fileName) {\n+    System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+         metadata.forEach(m -> {\n+           try {\n+             final RecordMetadata recordMetadata = m.get();\n+             System.out.println(\"Offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp());\n+           } catch (InterruptedException | ExecutionException e) {\n+               if (e instanceof  InterruptedException) {\n+                  Thread.currentThread().interrupt();\n+               }\n+           }\n+         });\n+  }\n+\n+  public static void main(String[] args) throws Exception {\n+    if (args.length < 1) {\n+      throw new IllegalArgumentException(\n+          \"This program takes one argument: the path to an environment configuration file.\");\n+    }\n+\n+    final Properties props = KafkaProducerApplication.loadProperties(args[0]);\n+    final String topic = props.getProperty(\"output.topic.name\");\n+    final Producer<String, String> producer = new KafkaProducer<>(props);\n+    final KafkaProducerApplication producerApp = new KafkaProducerApplication(producer, topic);\n+\n+    // Attach shutdown handler to catch Control-C.\n+    Runtime.getRuntime().addShutdownHook(new Thread(producerApp::shutdown));\n+\n+    try (final BufferedReader stdinReader = new BufferedReader(new InputStreamReader(System.in))) {\n+      String filePath;\n+      System.out.println(\"Enter the file path to publish records > \");", "originalCommit": "b0db1b51adc885823de57d076c40f40fb33171c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTUzNDM2OQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469534369", "bodyText": "I thought about the same thing, but I wanted the user to be able to experiment and process additional files if desired.", "author": "bbejeck", "createdAt": "2020-08-12T20:41:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxNzI4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDE0MzM1NQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r470143355", "bodyText": "If a user wants to process additional files, could they just run it again?", "author": "ybyzek", "createdAt": "2020-08-13T17:57:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxNzI4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxODE4Mw==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r463318183", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                         System.out.println(\"Offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp());\n          \n          \n            \n                         System.out.println(\"Record written to offset \" + recordMetadata.offset() + \" and timestamp \" + recordMetadata.timestamp());", "author": "ybyzek", "createdAt": "2020-07-30T23:02:45Z", "path": "_includes/tutorials/kafka-producer-application/kafka/code/src/main/java/io/confluent/developer/KafkaProducerApplication.java", "diffHunk": "@@ -0,0 +1,109 @@\n+package io.confluent.developer;\n+\n+\n+import java.io.BufferedReader;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+public class KafkaProducerApplication {\n+\n+  private final Producer<String, String> producer;\n+  final String outTopic;\n+\n+  public KafkaProducerApplication(final Producer<String, String> producer,\n+                                  final String topic) {\n+    this.producer = producer;\n+    outTopic = topic;\n+  }\n+\n+  public Future<RecordMetadata> produce(final String message) {\n+    final String[] parts = message.split(\"#\");\n+    final String key, value;\n+    if (parts.length > 1) {\n+      key = parts[0];\n+      value = parts[1];\n+    } else {\n+      key = \"NO-KEY\";\n+      value = parts[0];\n+    }\n+    final ProducerRecord<String, String> producerRecord = new ProducerRecord<>(outTopic, key, value);\n+    return producer.send(producerRecord);\n+  }\n+\n+  public void shutdown() {\n+    producer.close();\n+  }\n+\n+  public static Properties loadProperties(String fileName) throws IOException {\n+    final Properties envProps = new Properties();\n+    final FileInputStream input = new FileInputStream(fileName);\n+    envProps.load(input);\n+    input.close();\n+\n+    return envProps;\n+  }\n+\n+  public void printMetadata(final Collection<Future<RecordMetadata>> metadata,\n+                            final String fileName) {\n+    System.out.println(\"Offsets and timestamps committed in batch from \" + fileName);\n+         metadata.forEach(m -> {\n+           try {\n+             final RecordMetadata recordMetadata = m.get();\n+             System.out.println(\"Offset \" + recordMetadata.offset() + \" timestamp \" + recordMetadata.timestamp());", "originalCommit": "b0db1b51adc885823de57d076c40f40fb33171c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTUzNDQ3MQ==", "url": "https://github.com/confluentinc/kafka-tutorials/pull/491#discussion_r469534471", "bodyText": "ack", "author": "bbejeck", "createdAt": "2020-08-12T20:42:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzMxODE4Mw=="}], "type": "inlineReview"}, {"oid": "3f3bcbfa803e72dbb74de32c9aa0130de4534419", "url": "https://github.com/confluentinc/kafka-tutorials/commit/3f3bcbfa803e72dbb74de32c9aa0130de4534419", "message": "Updates for comments; rebased for merge conflicts", "committedDate": "2020-08-12T20:38:34Z", "type": "forcePushed"}, {"oid": "7c9bdc89e6c17d6be39a96f001cc551c1162cefe", "url": "https://github.com/confluentinc/kafka-tutorials/commit/7c9bdc89e6c17d6be39a96f001cc551c1162cefe", "message": "Rebased with master, added new tutorial to settings.gradle", "committedDate": "2020-08-12T22:13:52Z", "type": "forcePushed"}, {"oid": "f44e1abbe853853e673a6b7cbb01c694edb55397", "url": "https://github.com/confluentinc/kafka-tutorials/commit/f44e1abbe853853e673a6b7cbb01c694edb55397", "message": "initial commit for producer application", "committedDate": "2020-08-13T19:23:35Z", "type": "commit"}, {"oid": "b586f7c575088f0931c06b2148230ffc8e816ad0", "url": "https://github.com/confluentinc/kafka-tutorials/commit/b586f7c575088f0931c06b2148230ffc8e816ad0", "message": "added dev steps for producer application", "committedDate": "2020-08-13T19:23:35Z", "type": "commit"}, {"oid": "e25406254728ce0424f5e6f6d61db7af532d5ebe", "url": "https://github.com/confluentinc/kafka-tutorials/commit/e25406254728ce0424f5e6f6d61db7af532d5ebe", "message": "tutorial running from end-to-end", "committedDate": "2020-08-13T19:23:35Z", "type": "commit"}, {"oid": "947122d3eeb76f531f8350d9075dd3c1fdce1032", "url": "https://github.com/confluentinc/kafka-tutorials/commit/947122d3eeb76f531f8350d9075dd3c1fdce1032", "message": "fix semphore entry", "committedDate": "2020-08-13T19:23:36Z", "type": "commit"}, {"oid": "8c685747a25cc1beaddce512e92c35bf2748243f", "url": "https://github.com/confluentinc/kafka-tutorials/commit/8c685747a25cc1beaddce512e92c35bf2748243f", "message": "Updates for comments; rebased for merge conflicts", "committedDate": "2020-08-13T19:23:36Z", "type": "commit"}, {"oid": "12f8ae84fff809c6d0cb570286eaa13341394104", "url": "https://github.com/confluentinc/kafka-tutorials/commit/12f8ae84fff809c6d0cb570286eaa13341394104", "message": "Rebased with master, added new tutorial to settings.gradle", "committedDate": "2020-08-13T19:23:36Z", "type": "commit"}, {"oid": "b91ec35fc12f730fd5f935bdabd989d791667f70", "url": "https://github.com/confluentinc/kafka-tutorials/commit/b91ec35fc12f730fd5f935bdabd989d791667f70", "message": "updates for comments from @ybyzek", "committedDate": "2020-08-13T19:23:36Z", "type": "commit"}, {"oid": "dce77ec559dce21beb98b20f7672ff7192c4735a", "url": "https://github.com/confluentinc/kafka-tutorials/commit/dce77ec559dce21beb98b20f7672ff7192c4735a", "message": "further updates per comments", "committedDate": "2020-08-13T21:21:31Z", "type": "commit"}, {"oid": "dce77ec559dce21beb98b20f7672ff7192c4735a", "url": "https://github.com/confluentinc/kafka-tutorials/commit/dce77ec559dce21beb98b20f7672ff7192c4735a", "message": "further updates per comments", "committedDate": "2020-08-13T21:21:31Z", "type": "forcePushed"}]}