{"pr_number": 5116, "pr_title": "Add Azure Data Lake Gen2 connector for PinotFS", "pr_createdAt": "2020-03-05T09:58:05Z", "pr_url": "https://github.com/apache/pinot/pull/5116", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ1NzM3NA==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r388457374", "bodyText": "Not needed for the PR, but we should think about supporting encrypted ACCESS_KEY in the config, as opposed to plain text.", "author": "mayankshriv", "createdAt": "2020-03-05T17:46:24Z", "path": "pinot-plugins/pinot-file-system/pinot-adls/src/main/java/org/apache/pinot/plugin/filesystem/AzureGen2PinotFS.java", "diffHunk": "@@ -0,0 +1,447 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.plugin.filesystem;\n+\n+import com.azure.core.http.rest.PagedIterable;\n+import com.azure.core.util.Context;\n+import com.azure.storage.blob.BlobClient;\n+import com.azure.storage.blob.BlobServiceClient;\n+import com.azure.storage.blob.BlobServiceClientBuilder;\n+import com.azure.storage.common.StorageSharedKeyCredential;\n+import com.azure.storage.common.Utility;\n+import com.azure.storage.file.datalake.DataLakeFileClient;\n+import com.azure.storage.file.datalake.DataLakeFileSystemClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClientBuilder;\n+import com.azure.storage.file.datalake.models.DataLakeRequestConditions;\n+import com.azure.storage.file.datalake.models.DataLakeStorageException;\n+import com.azure.storage.file.datalake.models.ListPathsOptions;\n+import com.azure.storage.file.datalake.models.PathHttpHeaders;\n+import com.azure.storage.file.datalake.models.PathItem;\n+import com.azure.storage.file.datalake.models.PathProperties;\n+import java.io.ByteArrayInputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.net.URLDecoder;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.security.MessageDigest;\n+import java.security.NoSuchAlgorithmException;\n+import java.sql.Timestamp;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.util.Arrays;\n+import java.util.Map;\n+import org.apache.commons.configuration.Configuration;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Azure Data Lake Storage Gen2 implementation for the PinotFS interface.\n+ */\n+public class AzureGen2PinotFS extends PinotFS {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(AzureGen2PinotFS.class);\n+\n+  private static final String ACCOUNT_NAME = \"accountName\";\n+  private static final String ACCESS_KEY = \"accessKey\";\n+  private static final String FILE_SYSTEM_NAME = \"fileSystemName\";\n+\n+  private static final String HTTPS_URL_PREFIX = \"https://\";\n+  private static final String DIRECTORY_DELIMITER = \"/\";\n+\n+  private static final String AZURE_STORAGE_DNS_SUFFIX = \".dfs.core.windows.net\";\n+  private static final String AZURE_BLOB_DNS_SUFFIX = \".blob.core.windows.net\";\n+  private static final String PATH_ALREADY_EXISTS_ERROR_CODE = \"PathAlreadyExists\";\n+  private static final String IS_DIRECTORY_KEY = \"hdi_isfolder\";\n+\n+  private static final int NOT_FOUND_STATUS_CODE = 404;\n+  private static final int ALREADY_EXISTS_STATUS_CODE = 409;\n+\n+  // Azure Data Lake Gen2's block size is 4MB\n+  private static final int BUFFER_SIZE = 4 * 1024 * 1024;\n+\n+  private DataLakeFileSystemClient _fileSystemClient;\n+  private BlobServiceClient _blobServiceClient;\n+\n+  @Override\n+  public void init(Configuration config) {\n+    // Azure storage account name\n+    String accountName = config.getString(ACCOUNT_NAME);\n+    String accessKey = config.getString(ACCESS_KEY);", "originalCommit": "6d71f9802a47e9c8515296b50bc00987cf9485d1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODUwMjU3Nw==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r388502577", "bodyText": "Existing AzurePinotFS also gets the config as a string. I guess that our internal restli framework handles encryption of the ACCESS_KEY. I will add TODO comment", "author": "snleee", "createdAt": "2020-03-05T19:09:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ1NzM3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ1Nzg5Mg==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r388457892", "bodyText": "Should this be true or false? Don't recall what the interface expects, but good to check.", "author": "mayankshriv", "createdAt": "2020-03-05T17:47:29Z", "path": "pinot-plugins/pinot-file-system/pinot-adls/src/main/java/org/apache/pinot/plugin/filesystem/AzureGen2PinotFS.java", "diffHunk": "@@ -0,0 +1,447 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.plugin.filesystem;\n+\n+import com.azure.core.http.rest.PagedIterable;\n+import com.azure.core.util.Context;\n+import com.azure.storage.blob.BlobClient;\n+import com.azure.storage.blob.BlobServiceClient;\n+import com.azure.storage.blob.BlobServiceClientBuilder;\n+import com.azure.storage.common.StorageSharedKeyCredential;\n+import com.azure.storage.common.Utility;\n+import com.azure.storage.file.datalake.DataLakeFileClient;\n+import com.azure.storage.file.datalake.DataLakeFileSystemClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClientBuilder;\n+import com.azure.storage.file.datalake.models.DataLakeRequestConditions;\n+import com.azure.storage.file.datalake.models.DataLakeStorageException;\n+import com.azure.storage.file.datalake.models.ListPathsOptions;\n+import com.azure.storage.file.datalake.models.PathHttpHeaders;\n+import com.azure.storage.file.datalake.models.PathItem;\n+import com.azure.storage.file.datalake.models.PathProperties;\n+import java.io.ByteArrayInputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.net.URLDecoder;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.security.MessageDigest;\n+import java.security.NoSuchAlgorithmException;\n+import java.sql.Timestamp;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.util.Arrays;\n+import java.util.Map;\n+import org.apache.commons.configuration.Configuration;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Azure Data Lake Storage Gen2 implementation for the PinotFS interface.\n+ */\n+public class AzureGen2PinotFS extends PinotFS {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(AzureGen2PinotFS.class);\n+\n+  private static final String ACCOUNT_NAME = \"accountName\";\n+  private static final String ACCESS_KEY = \"accessKey\";\n+  private static final String FILE_SYSTEM_NAME = \"fileSystemName\";\n+\n+  private static final String HTTPS_URL_PREFIX = \"https://\";\n+  private static final String DIRECTORY_DELIMITER = \"/\";\n+\n+  private static final String AZURE_STORAGE_DNS_SUFFIX = \".dfs.core.windows.net\";\n+  private static final String AZURE_BLOB_DNS_SUFFIX = \".blob.core.windows.net\";\n+  private static final String PATH_ALREADY_EXISTS_ERROR_CODE = \"PathAlreadyExists\";\n+  private static final String IS_DIRECTORY_KEY = \"hdi_isfolder\";\n+\n+  private static final int NOT_FOUND_STATUS_CODE = 404;\n+  private static final int ALREADY_EXISTS_STATUS_CODE = 409;\n+\n+  // Azure Data Lake Gen2's block size is 4MB\n+  private static final int BUFFER_SIZE = 4 * 1024 * 1024;\n+\n+  private DataLakeFileSystemClient _fileSystemClient;\n+  private BlobServiceClient _blobServiceClient;\n+\n+  @Override\n+  public void init(Configuration config) {\n+    // Azure storage account name\n+    String accountName = config.getString(ACCOUNT_NAME);\n+    String accessKey = config.getString(ACCESS_KEY);\n+    String fileSystemName = config.getString(FILE_SYSTEM_NAME);\n+    String dfsServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_STORAGE_DNS_SUFFIX;\n+    String blobServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_BLOB_DNS_SUFFIX;\n+\n+    StorageSharedKeyCredential sharedKeyCredential = new StorageSharedKeyCredential(accountName, accessKey);\n+\n+    DataLakeServiceClient serviceClient = new DataLakeServiceClientBuilder().credential(sharedKeyCredential)\n+        .endpoint(dfsServiceEndpointUrl)\n+        .buildClient();\n+\n+    _blobServiceClient =\n+        new BlobServiceClientBuilder().credential(sharedKeyCredential).endpoint(blobServiceEndpointUrl).buildClient();\n+    _fileSystemClient = serviceClient.getFileSystemClient(fileSystemName);\n+    LOGGER.info(\"AzureGen2PinotFS is initialized (accountName={}, fileSystemName={}, dfsServiceEndpointUrl={}, \"\n+        + \"blobServiceEndpointUrl={})\", accountName, fileSystemName, dfsServiceEndpointUrl, blobServiceEndpointUrl);\n+  }\n+\n+  @Override\n+  public boolean mkdir(URI uri) throws IOException {\n+    LOGGER.info(\"mkdir is called with uri='{}'\", uri);\n+    try {\n+      // By default, create directory call will overwrite if the path already exists. Setting IfNoneMatch = \"*\" to\n+      // prevent overwrite. https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/create\n+      DataLakeRequestConditions requestConditions = new DataLakeRequestConditions().setIfNoneMatch(\"*\");\n+      _fileSystemClient.createDirectoryWithResponse(convertUriToAzureStylePath(uri), null, null, null, null,\n+          requestConditions, null, null);\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      // If the path already exists, doing nothing and return true\n+      if (e.getStatusCode() == ALREADY_EXISTS_STATUS_CODE && e.getErrorCode().equals(PATH_ALREADY_EXISTS_ERROR_CODE)) {\n+        return true;", "originalCommit": "6d71f9802a47e9c8515296b50bc00987cf9485d1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODUwNDE4MA==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r388504180", "bodyText": "I'm explicitly following pinotFS interface.\n  /**\n   * Creates a new directory. If parent directories are not created, it will create them.\n   * If the directory exists, it will return true without doing anything.\n   * @return true if mkdir is successful\n   * @throws IOException on IO failure\n   */", "author": "snleee", "createdAt": "2020-03-05T19:12:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ1Nzg5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ1ODEwNw==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r388458107", "bodyText": "Given that rename is implemented as copy and delete, this message will be logged for each segment push (we already have too many messages per segment push). Consider if this can be a debug message, or merged with other messages (in case of errors).", "author": "mayankshriv", "createdAt": "2020-03-05T17:47:55Z", "path": "pinot-plugins/pinot-file-system/pinot-adls/src/main/java/org/apache/pinot/plugin/filesystem/AzureGen2PinotFS.java", "diffHunk": "@@ -0,0 +1,447 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.plugin.filesystem;\n+\n+import com.azure.core.http.rest.PagedIterable;\n+import com.azure.core.util.Context;\n+import com.azure.storage.blob.BlobClient;\n+import com.azure.storage.blob.BlobServiceClient;\n+import com.azure.storage.blob.BlobServiceClientBuilder;\n+import com.azure.storage.common.StorageSharedKeyCredential;\n+import com.azure.storage.common.Utility;\n+import com.azure.storage.file.datalake.DataLakeFileClient;\n+import com.azure.storage.file.datalake.DataLakeFileSystemClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClientBuilder;\n+import com.azure.storage.file.datalake.models.DataLakeRequestConditions;\n+import com.azure.storage.file.datalake.models.DataLakeStorageException;\n+import com.azure.storage.file.datalake.models.ListPathsOptions;\n+import com.azure.storage.file.datalake.models.PathHttpHeaders;\n+import com.azure.storage.file.datalake.models.PathItem;\n+import com.azure.storage.file.datalake.models.PathProperties;\n+import java.io.ByteArrayInputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.net.URLDecoder;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.security.MessageDigest;\n+import java.security.NoSuchAlgorithmException;\n+import java.sql.Timestamp;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.util.Arrays;\n+import java.util.Map;\n+import org.apache.commons.configuration.Configuration;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Azure Data Lake Storage Gen2 implementation for the PinotFS interface.\n+ */\n+public class AzureGen2PinotFS extends PinotFS {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(AzureGen2PinotFS.class);\n+\n+  private static final String ACCOUNT_NAME = \"accountName\";\n+  private static final String ACCESS_KEY = \"accessKey\";\n+  private static final String FILE_SYSTEM_NAME = \"fileSystemName\";\n+\n+  private static final String HTTPS_URL_PREFIX = \"https://\";\n+  private static final String DIRECTORY_DELIMITER = \"/\";\n+\n+  private static final String AZURE_STORAGE_DNS_SUFFIX = \".dfs.core.windows.net\";\n+  private static final String AZURE_BLOB_DNS_SUFFIX = \".blob.core.windows.net\";\n+  private static final String PATH_ALREADY_EXISTS_ERROR_CODE = \"PathAlreadyExists\";\n+  private static final String IS_DIRECTORY_KEY = \"hdi_isfolder\";\n+\n+  private static final int NOT_FOUND_STATUS_CODE = 404;\n+  private static final int ALREADY_EXISTS_STATUS_CODE = 409;\n+\n+  // Azure Data Lake Gen2's block size is 4MB\n+  private static final int BUFFER_SIZE = 4 * 1024 * 1024;\n+\n+  private DataLakeFileSystemClient _fileSystemClient;\n+  private BlobServiceClient _blobServiceClient;\n+\n+  @Override\n+  public void init(Configuration config) {\n+    // Azure storage account name\n+    String accountName = config.getString(ACCOUNT_NAME);\n+    String accessKey = config.getString(ACCESS_KEY);\n+    String fileSystemName = config.getString(FILE_SYSTEM_NAME);\n+    String dfsServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_STORAGE_DNS_SUFFIX;\n+    String blobServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_BLOB_DNS_SUFFIX;\n+\n+    StorageSharedKeyCredential sharedKeyCredential = new StorageSharedKeyCredential(accountName, accessKey);\n+\n+    DataLakeServiceClient serviceClient = new DataLakeServiceClientBuilder().credential(sharedKeyCredential)\n+        .endpoint(dfsServiceEndpointUrl)\n+        .buildClient();\n+\n+    _blobServiceClient =\n+        new BlobServiceClientBuilder().credential(sharedKeyCredential).endpoint(blobServiceEndpointUrl).buildClient();\n+    _fileSystemClient = serviceClient.getFileSystemClient(fileSystemName);\n+    LOGGER.info(\"AzureGen2PinotFS is initialized (accountName={}, fileSystemName={}, dfsServiceEndpointUrl={}, \"\n+        + \"blobServiceEndpointUrl={})\", accountName, fileSystemName, dfsServiceEndpointUrl, blobServiceEndpointUrl);\n+  }\n+\n+  @Override\n+  public boolean mkdir(URI uri) throws IOException {\n+    LOGGER.info(\"mkdir is called with uri='{}'\", uri);\n+    try {\n+      // By default, create directory call will overwrite if the path already exists. Setting IfNoneMatch = \"*\" to\n+      // prevent overwrite. https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/create\n+      DataLakeRequestConditions requestConditions = new DataLakeRequestConditions().setIfNoneMatch(\"*\");\n+      _fileSystemClient.createDirectoryWithResponse(convertUriToAzureStylePath(uri), null, null, null, null,\n+          requestConditions, null, null);\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      // If the path already exists, doing nothing and return true\n+      if (e.getStatusCode() == ALREADY_EXISTS_STATUS_CODE && e.getErrorCode().equals(PATH_ALREADY_EXISTS_ERROR_CODE)) {\n+        return true;\n+      }\n+      LOGGER.error(\"Exception thrown while calling mkdir.\", e);\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean delete(URI segmentUri, boolean forceDelete) throws IOException {\n+    LOGGER.info(\"delete is called with segmentUri='{}', forceDelete='{}'\", segmentUri, forceDelete);", "originalCommit": "6d71f9802a47e9c8515296b50bc00987cf9485d1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE1MTMwNQ==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r389151305", "bodyText": "i will change all the logs to debug level.", "author": "snleee", "createdAt": "2020-03-06T21:24:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ1ODEwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ1OTIzNA==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r388459234", "bodyText": "What's the expected performance of listFiles? We just need to ensure there's at least one file, and not list all.", "author": "mayankshriv", "createdAt": "2020-03-05T17:50:02Z", "path": "pinot-plugins/pinot-file-system/pinot-adls/src/main/java/org/apache/pinot/plugin/filesystem/AzureGen2PinotFS.java", "diffHunk": "@@ -0,0 +1,447 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.plugin.filesystem;\n+\n+import com.azure.core.http.rest.PagedIterable;\n+import com.azure.core.util.Context;\n+import com.azure.storage.blob.BlobClient;\n+import com.azure.storage.blob.BlobServiceClient;\n+import com.azure.storage.blob.BlobServiceClientBuilder;\n+import com.azure.storage.common.StorageSharedKeyCredential;\n+import com.azure.storage.common.Utility;\n+import com.azure.storage.file.datalake.DataLakeFileClient;\n+import com.azure.storage.file.datalake.DataLakeFileSystemClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClientBuilder;\n+import com.azure.storage.file.datalake.models.DataLakeRequestConditions;\n+import com.azure.storage.file.datalake.models.DataLakeStorageException;\n+import com.azure.storage.file.datalake.models.ListPathsOptions;\n+import com.azure.storage.file.datalake.models.PathHttpHeaders;\n+import com.azure.storage.file.datalake.models.PathItem;\n+import com.azure.storage.file.datalake.models.PathProperties;\n+import java.io.ByteArrayInputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.net.URLDecoder;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.security.MessageDigest;\n+import java.security.NoSuchAlgorithmException;\n+import java.sql.Timestamp;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.util.Arrays;\n+import java.util.Map;\n+import org.apache.commons.configuration.Configuration;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Azure Data Lake Storage Gen2 implementation for the PinotFS interface.\n+ */\n+public class AzureGen2PinotFS extends PinotFS {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(AzureGen2PinotFS.class);\n+\n+  private static final String ACCOUNT_NAME = \"accountName\";\n+  private static final String ACCESS_KEY = \"accessKey\";\n+  private static final String FILE_SYSTEM_NAME = \"fileSystemName\";\n+\n+  private static final String HTTPS_URL_PREFIX = \"https://\";\n+  private static final String DIRECTORY_DELIMITER = \"/\";\n+\n+  private static final String AZURE_STORAGE_DNS_SUFFIX = \".dfs.core.windows.net\";\n+  private static final String AZURE_BLOB_DNS_SUFFIX = \".blob.core.windows.net\";\n+  private static final String PATH_ALREADY_EXISTS_ERROR_CODE = \"PathAlreadyExists\";\n+  private static final String IS_DIRECTORY_KEY = \"hdi_isfolder\";\n+\n+  private static final int NOT_FOUND_STATUS_CODE = 404;\n+  private static final int ALREADY_EXISTS_STATUS_CODE = 409;\n+\n+  // Azure Data Lake Gen2's block size is 4MB\n+  private static final int BUFFER_SIZE = 4 * 1024 * 1024;\n+\n+  private DataLakeFileSystemClient _fileSystemClient;\n+  private BlobServiceClient _blobServiceClient;\n+\n+  @Override\n+  public void init(Configuration config) {\n+    // Azure storage account name\n+    String accountName = config.getString(ACCOUNT_NAME);\n+    String accessKey = config.getString(ACCESS_KEY);\n+    String fileSystemName = config.getString(FILE_SYSTEM_NAME);\n+    String dfsServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_STORAGE_DNS_SUFFIX;\n+    String blobServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_BLOB_DNS_SUFFIX;\n+\n+    StorageSharedKeyCredential sharedKeyCredential = new StorageSharedKeyCredential(accountName, accessKey);\n+\n+    DataLakeServiceClient serviceClient = new DataLakeServiceClientBuilder().credential(sharedKeyCredential)\n+        .endpoint(dfsServiceEndpointUrl)\n+        .buildClient();\n+\n+    _blobServiceClient =\n+        new BlobServiceClientBuilder().credential(sharedKeyCredential).endpoint(blobServiceEndpointUrl).buildClient();\n+    _fileSystemClient = serviceClient.getFileSystemClient(fileSystemName);\n+    LOGGER.info(\"AzureGen2PinotFS is initialized (accountName={}, fileSystemName={}, dfsServiceEndpointUrl={}, \"\n+        + \"blobServiceEndpointUrl={})\", accountName, fileSystemName, dfsServiceEndpointUrl, blobServiceEndpointUrl);\n+  }\n+\n+  @Override\n+  public boolean mkdir(URI uri) throws IOException {\n+    LOGGER.info(\"mkdir is called with uri='{}'\", uri);\n+    try {\n+      // By default, create directory call will overwrite if the path already exists. Setting IfNoneMatch = \"*\" to\n+      // prevent overwrite. https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/create\n+      DataLakeRequestConditions requestConditions = new DataLakeRequestConditions().setIfNoneMatch(\"*\");\n+      _fileSystemClient.createDirectoryWithResponse(convertUriToAzureStylePath(uri), null, null, null, null,\n+          requestConditions, null, null);\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      // If the path already exists, doing nothing and return true\n+      if (e.getStatusCode() == ALREADY_EXISTS_STATUS_CODE && e.getErrorCode().equals(PATH_ALREADY_EXISTS_ERROR_CODE)) {\n+        return true;\n+      }\n+      LOGGER.error(\"Exception thrown while calling mkdir.\", e);\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean delete(URI segmentUri, boolean forceDelete) throws IOException {\n+    LOGGER.info(\"delete is called with segmentUri='{}', forceDelete='{}'\", segmentUri, forceDelete);\n+    try {\n+      boolean isDirectory = isDirectory(segmentUri);\n+      if (isDirectory && listFiles(segmentUri, false).length > 0 && !forceDelete) {", "originalCommit": "6d71f9802a47e9c8515296b50bc00987cf9485d1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE1MTU2OQ==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r389151569", "bodyText": "I will run some testing on all the functions and will share the number", "author": "snleee", "createdAt": "2020-03-06T21:25:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ1OTIzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ2MTg4MQ==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r388461881", "bodyText": "Do we need atomic moves? That was one of the key benefits of ADLS Gen2 over ABS, if I am not mistaken.\nWould be good to link the issue you created, here.", "author": "mayankshriv", "createdAt": "2020-03-05T17:55:01Z", "path": "pinot-plugins/pinot-file-system/pinot-adls/src/main/java/org/apache/pinot/plugin/filesystem/AzureGen2PinotFS.java", "diffHunk": "@@ -0,0 +1,447 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.plugin.filesystem;\n+\n+import com.azure.core.http.rest.PagedIterable;\n+import com.azure.core.util.Context;\n+import com.azure.storage.blob.BlobClient;\n+import com.azure.storage.blob.BlobServiceClient;\n+import com.azure.storage.blob.BlobServiceClientBuilder;\n+import com.azure.storage.common.StorageSharedKeyCredential;\n+import com.azure.storage.common.Utility;\n+import com.azure.storage.file.datalake.DataLakeFileClient;\n+import com.azure.storage.file.datalake.DataLakeFileSystemClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClientBuilder;\n+import com.azure.storage.file.datalake.models.DataLakeRequestConditions;\n+import com.azure.storage.file.datalake.models.DataLakeStorageException;\n+import com.azure.storage.file.datalake.models.ListPathsOptions;\n+import com.azure.storage.file.datalake.models.PathHttpHeaders;\n+import com.azure.storage.file.datalake.models.PathItem;\n+import com.azure.storage.file.datalake.models.PathProperties;\n+import java.io.ByteArrayInputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.net.URLDecoder;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.security.MessageDigest;\n+import java.security.NoSuchAlgorithmException;\n+import java.sql.Timestamp;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.util.Arrays;\n+import java.util.Map;\n+import org.apache.commons.configuration.Configuration;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Azure Data Lake Storage Gen2 implementation for the PinotFS interface.\n+ */\n+public class AzureGen2PinotFS extends PinotFS {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(AzureGen2PinotFS.class);\n+\n+  private static final String ACCOUNT_NAME = \"accountName\";\n+  private static final String ACCESS_KEY = \"accessKey\";\n+  private static final String FILE_SYSTEM_NAME = \"fileSystemName\";\n+\n+  private static final String HTTPS_URL_PREFIX = \"https://\";\n+  private static final String DIRECTORY_DELIMITER = \"/\";\n+\n+  private static final String AZURE_STORAGE_DNS_SUFFIX = \".dfs.core.windows.net\";\n+  private static final String AZURE_BLOB_DNS_SUFFIX = \".blob.core.windows.net\";\n+  private static final String PATH_ALREADY_EXISTS_ERROR_CODE = \"PathAlreadyExists\";\n+  private static final String IS_DIRECTORY_KEY = \"hdi_isfolder\";\n+\n+  private static final int NOT_FOUND_STATUS_CODE = 404;\n+  private static final int ALREADY_EXISTS_STATUS_CODE = 409;\n+\n+  // Azure Data Lake Gen2's block size is 4MB\n+  private static final int BUFFER_SIZE = 4 * 1024 * 1024;\n+\n+  private DataLakeFileSystemClient _fileSystemClient;\n+  private BlobServiceClient _blobServiceClient;\n+\n+  @Override\n+  public void init(Configuration config) {\n+    // Azure storage account name\n+    String accountName = config.getString(ACCOUNT_NAME);\n+    String accessKey = config.getString(ACCESS_KEY);\n+    String fileSystemName = config.getString(FILE_SYSTEM_NAME);\n+    String dfsServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_STORAGE_DNS_SUFFIX;\n+    String blobServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_BLOB_DNS_SUFFIX;\n+\n+    StorageSharedKeyCredential sharedKeyCredential = new StorageSharedKeyCredential(accountName, accessKey);\n+\n+    DataLakeServiceClient serviceClient = new DataLakeServiceClientBuilder().credential(sharedKeyCredential)\n+        .endpoint(dfsServiceEndpointUrl)\n+        .buildClient();\n+\n+    _blobServiceClient =\n+        new BlobServiceClientBuilder().credential(sharedKeyCredential).endpoint(blobServiceEndpointUrl).buildClient();\n+    _fileSystemClient = serviceClient.getFileSystemClient(fileSystemName);\n+    LOGGER.info(\"AzureGen2PinotFS is initialized (accountName={}, fileSystemName={}, dfsServiceEndpointUrl={}, \"\n+        + \"blobServiceEndpointUrl={})\", accountName, fileSystemName, dfsServiceEndpointUrl, blobServiceEndpointUrl);\n+  }\n+\n+  @Override\n+  public boolean mkdir(URI uri) throws IOException {\n+    LOGGER.info(\"mkdir is called with uri='{}'\", uri);\n+    try {\n+      // By default, create directory call will overwrite if the path already exists. Setting IfNoneMatch = \"*\" to\n+      // prevent overwrite. https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/create\n+      DataLakeRequestConditions requestConditions = new DataLakeRequestConditions().setIfNoneMatch(\"*\");\n+      _fileSystemClient.createDirectoryWithResponse(convertUriToAzureStylePath(uri), null, null, null, null,\n+          requestConditions, null, null);\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      // If the path already exists, doing nothing and return true\n+      if (e.getStatusCode() == ALREADY_EXISTS_STATUS_CODE && e.getErrorCode().equals(PATH_ALREADY_EXISTS_ERROR_CODE)) {\n+        return true;\n+      }\n+      LOGGER.error(\"Exception thrown while calling mkdir.\", e);\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean delete(URI segmentUri, boolean forceDelete) throws IOException {\n+    LOGGER.info(\"delete is called with segmentUri='{}', forceDelete='{}'\", segmentUri, forceDelete);\n+    try {\n+      boolean isDirectory = isDirectory(segmentUri);\n+      if (isDirectory && listFiles(segmentUri, false).length > 0 && !forceDelete) {\n+        return false;\n+      }\n+\n+      String path = convertUriToAzureStylePath(segmentUri);\n+      if (isDirectory) {\n+        _fileSystemClient.deleteDirectoryWithResponse(path, true, null, null, Context.NONE).getValue();\n+      } else {\n+        _fileSystemClient.deleteFile(path);\n+      }\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean doMove(URI srcUri, URI dstUri) throws IOException {\n+    LOGGER.info(\"doMove is called with srcUri='{}', dstUri='{}'\", srcUri, dstUri);\n+    try {\n+      // TODO: currently, azure-sdk has a bug in \"rename\" when the path includes some special characters that gets", "originalCommit": "6d71f9802a47e9c8515296b50bc00987cf9485d1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE1NzMyNA==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r389157324", "bodyText": "the comment already have the issue linked.\nAs for atomic moves, pinotFS interface doesn't require us to implement move() as an atomic operation.", "author": "snleee", "createdAt": "2020-03-06T21:39:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ2MTg4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ2MjkzNA==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r388462934", "bodyText": "Same here, let's try to consolidate logging per segment push.", "author": "mayankshriv", "createdAt": "2020-03-05T17:56:53Z", "path": "pinot-plugins/pinot-file-system/pinot-adls/src/main/java/org/apache/pinot/plugin/filesystem/AzureGen2PinotFS.java", "diffHunk": "@@ -0,0 +1,447 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.plugin.filesystem;\n+\n+import com.azure.core.http.rest.PagedIterable;\n+import com.azure.core.util.Context;\n+import com.azure.storage.blob.BlobClient;\n+import com.azure.storage.blob.BlobServiceClient;\n+import com.azure.storage.blob.BlobServiceClientBuilder;\n+import com.azure.storage.common.StorageSharedKeyCredential;\n+import com.azure.storage.common.Utility;\n+import com.azure.storage.file.datalake.DataLakeFileClient;\n+import com.azure.storage.file.datalake.DataLakeFileSystemClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClientBuilder;\n+import com.azure.storage.file.datalake.models.DataLakeRequestConditions;\n+import com.azure.storage.file.datalake.models.DataLakeStorageException;\n+import com.azure.storage.file.datalake.models.ListPathsOptions;\n+import com.azure.storage.file.datalake.models.PathHttpHeaders;\n+import com.azure.storage.file.datalake.models.PathItem;\n+import com.azure.storage.file.datalake.models.PathProperties;\n+import java.io.ByteArrayInputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.net.URLDecoder;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.security.MessageDigest;\n+import java.security.NoSuchAlgorithmException;\n+import java.sql.Timestamp;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.util.Arrays;\n+import java.util.Map;\n+import org.apache.commons.configuration.Configuration;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Azure Data Lake Storage Gen2 implementation for the PinotFS interface.\n+ */\n+public class AzureGen2PinotFS extends PinotFS {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(AzureGen2PinotFS.class);\n+\n+  private static final String ACCOUNT_NAME = \"accountName\";\n+  private static final String ACCESS_KEY = \"accessKey\";\n+  private static final String FILE_SYSTEM_NAME = \"fileSystemName\";\n+\n+  private static final String HTTPS_URL_PREFIX = \"https://\";\n+  private static final String DIRECTORY_DELIMITER = \"/\";\n+\n+  private static final String AZURE_STORAGE_DNS_SUFFIX = \".dfs.core.windows.net\";\n+  private static final String AZURE_BLOB_DNS_SUFFIX = \".blob.core.windows.net\";\n+  private static final String PATH_ALREADY_EXISTS_ERROR_CODE = \"PathAlreadyExists\";\n+  private static final String IS_DIRECTORY_KEY = \"hdi_isfolder\";\n+\n+  private static final int NOT_FOUND_STATUS_CODE = 404;\n+  private static final int ALREADY_EXISTS_STATUS_CODE = 409;\n+\n+  // Azure Data Lake Gen2's block size is 4MB\n+  private static final int BUFFER_SIZE = 4 * 1024 * 1024;\n+\n+  private DataLakeFileSystemClient _fileSystemClient;\n+  private BlobServiceClient _blobServiceClient;\n+\n+  @Override\n+  public void init(Configuration config) {\n+    // Azure storage account name\n+    String accountName = config.getString(ACCOUNT_NAME);\n+    String accessKey = config.getString(ACCESS_KEY);\n+    String fileSystemName = config.getString(FILE_SYSTEM_NAME);\n+    String dfsServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_STORAGE_DNS_SUFFIX;\n+    String blobServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_BLOB_DNS_SUFFIX;\n+\n+    StorageSharedKeyCredential sharedKeyCredential = new StorageSharedKeyCredential(accountName, accessKey);\n+\n+    DataLakeServiceClient serviceClient = new DataLakeServiceClientBuilder().credential(sharedKeyCredential)\n+        .endpoint(dfsServiceEndpointUrl)\n+        .buildClient();\n+\n+    _blobServiceClient =\n+        new BlobServiceClientBuilder().credential(sharedKeyCredential).endpoint(blobServiceEndpointUrl).buildClient();\n+    _fileSystemClient = serviceClient.getFileSystemClient(fileSystemName);\n+    LOGGER.info(\"AzureGen2PinotFS is initialized (accountName={}, fileSystemName={}, dfsServiceEndpointUrl={}, \"\n+        + \"blobServiceEndpointUrl={})\", accountName, fileSystemName, dfsServiceEndpointUrl, blobServiceEndpointUrl);\n+  }\n+\n+  @Override\n+  public boolean mkdir(URI uri) throws IOException {\n+    LOGGER.info(\"mkdir is called with uri='{}'\", uri);\n+    try {\n+      // By default, create directory call will overwrite if the path already exists. Setting IfNoneMatch = \"*\" to\n+      // prevent overwrite. https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/create\n+      DataLakeRequestConditions requestConditions = new DataLakeRequestConditions().setIfNoneMatch(\"*\");\n+      _fileSystemClient.createDirectoryWithResponse(convertUriToAzureStylePath(uri), null, null, null, null,\n+          requestConditions, null, null);\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      // If the path already exists, doing nothing and return true\n+      if (e.getStatusCode() == ALREADY_EXISTS_STATUS_CODE && e.getErrorCode().equals(PATH_ALREADY_EXISTS_ERROR_CODE)) {\n+        return true;\n+      }\n+      LOGGER.error(\"Exception thrown while calling mkdir.\", e);\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean delete(URI segmentUri, boolean forceDelete) throws IOException {\n+    LOGGER.info(\"delete is called with segmentUri='{}', forceDelete='{}'\", segmentUri, forceDelete);\n+    try {\n+      boolean isDirectory = isDirectory(segmentUri);\n+      if (isDirectory && listFiles(segmentUri, false).length > 0 && !forceDelete) {\n+        return false;\n+      }\n+\n+      String path = convertUriToAzureStylePath(segmentUri);\n+      if (isDirectory) {\n+        _fileSystemClient.deleteDirectoryWithResponse(path, true, null, null, Context.NONE).getValue();\n+      } else {\n+        _fileSystemClient.deleteFile(path);\n+      }\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean doMove(URI srcUri, URI dstUri) throws IOException {\n+    LOGGER.info(\"doMove is called with srcUri='{}', dstUri='{}'\", srcUri, dstUri);\n+    try {\n+      // TODO: currently, azure-sdk has a bug in \"rename\" when the path includes some special characters that gets\n+      // changed during the url encoding (e.g '%' -> '%25', ' ' -> '%20')\n+      // https://github.com/Azure/azure-sdk-for-java/issues/8761\n+//      DataLakeDirectoryClient directoryClient =\n+//          _fileSystemClient.getDirectoryClient(convertUriToAzureStylePath(srcUri));\n+//      directoryClient.rename(null, convertUriToAzureStylePath(dstUri));\n+      copy(srcUri, dstUri);\n+      delete(srcUri, true);\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean copy(URI srcUri, URI dstUri) throws IOException {\n+    LOGGER.info(\"copy is called with srcUri='{}', dstUri='{}'\", srcUri, dstUri);", "originalCommit": "6d71f9802a47e9c8515296b50bc00987cf9485d1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE1MTY5Mw==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r389151693", "bodyText": "changed to debug", "author": "snleee", "createdAt": "2020-03-06T21:25:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ2MjkzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ2NDg2MQ==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r388464861", "bodyText": "Would be good to publish some performance numbers, for such operations as copy, listFiles, etc.", "author": "mayankshriv", "createdAt": "2020-03-05T17:59:24Z", "path": "pinot-plugins/pinot-file-system/pinot-adls/src/main/java/org/apache/pinot/plugin/filesystem/AzureGen2PinotFS.java", "diffHunk": "@@ -0,0 +1,447 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.plugin.filesystem;\n+\n+import com.azure.core.http.rest.PagedIterable;\n+import com.azure.core.util.Context;\n+import com.azure.storage.blob.BlobClient;\n+import com.azure.storage.blob.BlobServiceClient;\n+import com.azure.storage.blob.BlobServiceClientBuilder;\n+import com.azure.storage.common.StorageSharedKeyCredential;\n+import com.azure.storage.common.Utility;\n+import com.azure.storage.file.datalake.DataLakeFileClient;\n+import com.azure.storage.file.datalake.DataLakeFileSystemClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClientBuilder;\n+import com.azure.storage.file.datalake.models.DataLakeRequestConditions;\n+import com.azure.storage.file.datalake.models.DataLakeStorageException;\n+import com.azure.storage.file.datalake.models.ListPathsOptions;\n+import com.azure.storage.file.datalake.models.PathHttpHeaders;\n+import com.azure.storage.file.datalake.models.PathItem;\n+import com.azure.storage.file.datalake.models.PathProperties;\n+import java.io.ByteArrayInputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.net.URLDecoder;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.security.MessageDigest;\n+import java.security.NoSuchAlgorithmException;\n+import java.sql.Timestamp;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.util.Arrays;\n+import java.util.Map;\n+import org.apache.commons.configuration.Configuration;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Azure Data Lake Storage Gen2 implementation for the PinotFS interface.\n+ */\n+public class AzureGen2PinotFS extends PinotFS {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(AzureGen2PinotFS.class);\n+\n+  private static final String ACCOUNT_NAME = \"accountName\";\n+  private static final String ACCESS_KEY = \"accessKey\";\n+  private static final String FILE_SYSTEM_NAME = \"fileSystemName\";\n+\n+  private static final String HTTPS_URL_PREFIX = \"https://\";\n+  private static final String DIRECTORY_DELIMITER = \"/\";\n+\n+  private static final String AZURE_STORAGE_DNS_SUFFIX = \".dfs.core.windows.net\";\n+  private static final String AZURE_BLOB_DNS_SUFFIX = \".blob.core.windows.net\";\n+  private static final String PATH_ALREADY_EXISTS_ERROR_CODE = \"PathAlreadyExists\";\n+  private static final String IS_DIRECTORY_KEY = \"hdi_isfolder\";\n+\n+  private static final int NOT_FOUND_STATUS_CODE = 404;\n+  private static final int ALREADY_EXISTS_STATUS_CODE = 409;\n+\n+  // Azure Data Lake Gen2's block size is 4MB\n+  private static final int BUFFER_SIZE = 4 * 1024 * 1024;\n+\n+  private DataLakeFileSystemClient _fileSystemClient;\n+  private BlobServiceClient _blobServiceClient;\n+\n+  @Override\n+  public void init(Configuration config) {\n+    // Azure storage account name\n+    String accountName = config.getString(ACCOUNT_NAME);\n+    String accessKey = config.getString(ACCESS_KEY);\n+    String fileSystemName = config.getString(FILE_SYSTEM_NAME);\n+    String dfsServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_STORAGE_DNS_SUFFIX;\n+    String blobServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_BLOB_DNS_SUFFIX;\n+\n+    StorageSharedKeyCredential sharedKeyCredential = new StorageSharedKeyCredential(accountName, accessKey);\n+\n+    DataLakeServiceClient serviceClient = new DataLakeServiceClientBuilder().credential(sharedKeyCredential)\n+        .endpoint(dfsServiceEndpointUrl)\n+        .buildClient();\n+\n+    _blobServiceClient =\n+        new BlobServiceClientBuilder().credential(sharedKeyCredential).endpoint(blobServiceEndpointUrl).buildClient();\n+    _fileSystemClient = serviceClient.getFileSystemClient(fileSystemName);\n+    LOGGER.info(\"AzureGen2PinotFS is initialized (accountName={}, fileSystemName={}, dfsServiceEndpointUrl={}, \"\n+        + \"blobServiceEndpointUrl={})\", accountName, fileSystemName, dfsServiceEndpointUrl, blobServiceEndpointUrl);\n+  }\n+\n+  @Override\n+  public boolean mkdir(URI uri) throws IOException {\n+    LOGGER.info(\"mkdir is called with uri='{}'\", uri);\n+    try {\n+      // By default, create directory call will overwrite if the path already exists. Setting IfNoneMatch = \"*\" to\n+      // prevent overwrite. https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/create\n+      DataLakeRequestConditions requestConditions = new DataLakeRequestConditions().setIfNoneMatch(\"*\");\n+      _fileSystemClient.createDirectoryWithResponse(convertUriToAzureStylePath(uri), null, null, null, null,\n+          requestConditions, null, null);\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      // If the path already exists, doing nothing and return true\n+      if (e.getStatusCode() == ALREADY_EXISTS_STATUS_CODE && e.getErrorCode().equals(PATH_ALREADY_EXISTS_ERROR_CODE)) {\n+        return true;\n+      }\n+      LOGGER.error(\"Exception thrown while calling mkdir.\", e);\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean delete(URI segmentUri, boolean forceDelete) throws IOException {\n+    LOGGER.info(\"delete is called with segmentUri='{}', forceDelete='{}'\", segmentUri, forceDelete);\n+    try {\n+      boolean isDirectory = isDirectory(segmentUri);\n+      if (isDirectory && listFiles(segmentUri, false).length > 0 && !forceDelete) {\n+        return false;\n+      }\n+\n+      String path = convertUriToAzureStylePath(segmentUri);\n+      if (isDirectory) {\n+        _fileSystemClient.deleteDirectoryWithResponse(path, true, null, null, Context.NONE).getValue();\n+      } else {\n+        _fileSystemClient.deleteFile(path);\n+      }\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean doMove(URI srcUri, URI dstUri) throws IOException {\n+    LOGGER.info(\"doMove is called with srcUri='{}', dstUri='{}'\", srcUri, dstUri);\n+    try {\n+      // TODO: currently, azure-sdk has a bug in \"rename\" when the path includes some special characters that gets\n+      // changed during the url encoding (e.g '%' -> '%25', ' ' -> '%20')\n+      // https://github.com/Azure/azure-sdk-for-java/issues/8761\n+//      DataLakeDirectoryClient directoryClient =\n+//          _fileSystemClient.getDirectoryClient(convertUriToAzureStylePath(srcUri));\n+//      directoryClient.rename(null, convertUriToAzureStylePath(dstUri));\n+      copy(srcUri, dstUri);\n+      delete(srcUri, true);\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean copy(URI srcUri, URI dstUri) throws IOException {\n+    LOGGER.info(\"copy is called with srcUri='{}', dstUri='{}'\", srcUri, dstUri);\n+    // If src and dst are the same, do nothing.\n+    if (srcUri.equals(dstUri)) {\n+      return true;\n+    }\n+\n+    // Remove the destination directory or file\n+    if (exists(dstUri)) {\n+      delete(dstUri, true);\n+    }\n+\n+    if (!isDirectory(srcUri)) {\n+      // If source is a file, we can simply copy the file from src to dst\n+      return copySrcToDst(srcUri, dstUri);\n+    } else {\n+      // In case we are copying a directory, we need to recursively look into the directory and copy all the files and\n+      // directories accordingly\n+      try {\n+        boolean copySucceeded = false;\n+        Path srcPath = Paths.get(srcUri.getPath());\n+        for (String path : listFiles(srcUri, true)) {\n+          // Compute the src path for the given path\n+          URI currentSrc =\n+              new URI(srcUri.getScheme(), srcUri.getHost(), convertAzureStylePathToUriStylePath(path), null);\n+\n+          // Compute the destination path for the current path.\n+          String relativeSrcPath = srcPath.relativize(Paths.get(convertAzureStylePathToUriStylePath(path))).toString();\n+          String newDstPath = Paths.get(dstUri.getPath(), relativeSrcPath).toString();\n+          URI newDst = new URI(dstUri.getScheme(), dstUri.getHost(), newDstPath, null);\n+\n+          if (isDirectory(currentSrc)) {\n+            // If src is directory, create one.\n+            if (!mkdir(newDst)) {\n+              return false;\n+            }\n+          } else {\n+            // If src is a file, we need to copy.\n+            copySucceeded |= copySrcToDst(currentSrc, newDst);\n+          }\n+        }\n+        return copySucceeded;\n+      } catch (DataLakeStorageException | URISyntaxException e) {\n+        throw new IOException(e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public boolean exists(URI fileUri) throws IOException {\n+    try {\n+      _fileSystemClient.getDirectoryClient(convertUriToAzureStylePath(fileUri)).getProperties();\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      if (e.getStatusCode() == NOT_FOUND_STATUS_CODE) {\n+        return false;\n+      }\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public long length(URI fileUri) throws IOException {\n+    try {\n+      PathProperties pathProperties =\n+          _fileSystemClient.getDirectoryClient(convertUriToAzureStylePath(fileUri)).getProperties();\n+      return pathProperties.getFileSize();\n+    } catch (DataLakeStorageException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public String[] listFiles(URI fileUri, boolean recursive) throws IOException {\n+    LOGGER.info(\"listFiles is called with fileUri='{}', recursive='{}'\", fileUri, recursive);\n+    try {\n+      ListPathsOptions options =\n+          new ListPathsOptions().setPath(convertUriToAzureStylePath(fileUri)).setRecursive(recursive);\n+      PagedIterable<PathItem> iter = _fileSystemClient.listPaths(options, null);\n+      return iter.stream().map(PathItem::getName).toArray(String[]::new);\n+    } catch (DataLakeStorageException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public void copyToLocalFile(URI srcUri, File dstFile) throws Exception {", "originalCommit": "6d71f9802a47e9c8515296b50bc00987cf9485d1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE1MTc3MQ==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r389151771", "bodyText": "same as above", "author": "snleee", "createdAt": "2020-03-06T21:25:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ2NDg2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ5MzE3OA==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r388493178", "bodyText": "Would be good to have perf check here, given that we don't do such checks for other PinotFS implementations.", "author": "mayankshriv", "createdAt": "2020-03-05T18:53:34Z", "path": "pinot-plugins/pinot-file-system/pinot-adls/src/main/java/org/apache/pinot/plugin/filesystem/AzureGen2PinotFS.java", "diffHunk": "@@ -0,0 +1,447 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.plugin.filesystem;\n+\n+import com.azure.core.http.rest.PagedIterable;\n+import com.azure.core.util.Context;\n+import com.azure.storage.blob.BlobClient;\n+import com.azure.storage.blob.BlobServiceClient;\n+import com.azure.storage.blob.BlobServiceClientBuilder;\n+import com.azure.storage.common.StorageSharedKeyCredential;\n+import com.azure.storage.common.Utility;\n+import com.azure.storage.file.datalake.DataLakeFileClient;\n+import com.azure.storage.file.datalake.DataLakeFileSystemClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClientBuilder;\n+import com.azure.storage.file.datalake.models.DataLakeRequestConditions;\n+import com.azure.storage.file.datalake.models.DataLakeStorageException;\n+import com.azure.storage.file.datalake.models.ListPathsOptions;\n+import com.azure.storage.file.datalake.models.PathHttpHeaders;\n+import com.azure.storage.file.datalake.models.PathItem;\n+import com.azure.storage.file.datalake.models.PathProperties;\n+import java.io.ByteArrayInputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.net.URLDecoder;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.security.MessageDigest;\n+import java.security.NoSuchAlgorithmException;\n+import java.sql.Timestamp;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.util.Arrays;\n+import java.util.Map;\n+import org.apache.commons.configuration.Configuration;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Azure Data Lake Storage Gen2 implementation for the PinotFS interface.\n+ */\n+public class AzureGen2PinotFS extends PinotFS {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(AzureGen2PinotFS.class);\n+\n+  private static final String ACCOUNT_NAME = \"accountName\";\n+  private static final String ACCESS_KEY = \"accessKey\";\n+  private static final String FILE_SYSTEM_NAME = \"fileSystemName\";\n+\n+  private static final String HTTPS_URL_PREFIX = \"https://\";\n+  private static final String DIRECTORY_DELIMITER = \"/\";\n+\n+  private static final String AZURE_STORAGE_DNS_SUFFIX = \".dfs.core.windows.net\";\n+  private static final String AZURE_BLOB_DNS_SUFFIX = \".blob.core.windows.net\";\n+  private static final String PATH_ALREADY_EXISTS_ERROR_CODE = \"PathAlreadyExists\";\n+  private static final String IS_DIRECTORY_KEY = \"hdi_isfolder\";\n+\n+  private static final int NOT_FOUND_STATUS_CODE = 404;\n+  private static final int ALREADY_EXISTS_STATUS_CODE = 409;\n+\n+  // Azure Data Lake Gen2's block size is 4MB\n+  private static final int BUFFER_SIZE = 4 * 1024 * 1024;\n+\n+  private DataLakeFileSystemClient _fileSystemClient;\n+  private BlobServiceClient _blobServiceClient;\n+\n+  @Override\n+  public void init(Configuration config) {\n+    // Azure storage account name\n+    String accountName = config.getString(ACCOUNT_NAME);\n+    String accessKey = config.getString(ACCESS_KEY);\n+    String fileSystemName = config.getString(FILE_SYSTEM_NAME);\n+    String dfsServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_STORAGE_DNS_SUFFIX;\n+    String blobServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_BLOB_DNS_SUFFIX;\n+\n+    StorageSharedKeyCredential sharedKeyCredential = new StorageSharedKeyCredential(accountName, accessKey);\n+\n+    DataLakeServiceClient serviceClient = new DataLakeServiceClientBuilder().credential(sharedKeyCredential)\n+        .endpoint(dfsServiceEndpointUrl)\n+        .buildClient();\n+\n+    _blobServiceClient =\n+        new BlobServiceClientBuilder().credential(sharedKeyCredential).endpoint(blobServiceEndpointUrl).buildClient();\n+    _fileSystemClient = serviceClient.getFileSystemClient(fileSystemName);\n+    LOGGER.info(\"AzureGen2PinotFS is initialized (accountName={}, fileSystemName={}, dfsServiceEndpointUrl={}, \"\n+        + \"blobServiceEndpointUrl={})\", accountName, fileSystemName, dfsServiceEndpointUrl, blobServiceEndpointUrl);\n+  }\n+\n+  @Override\n+  public boolean mkdir(URI uri) throws IOException {\n+    LOGGER.info(\"mkdir is called with uri='{}'\", uri);\n+    try {\n+      // By default, create directory call will overwrite if the path already exists. Setting IfNoneMatch = \"*\" to\n+      // prevent overwrite. https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/create\n+      DataLakeRequestConditions requestConditions = new DataLakeRequestConditions().setIfNoneMatch(\"*\");\n+      _fileSystemClient.createDirectoryWithResponse(convertUriToAzureStylePath(uri), null, null, null, null,\n+          requestConditions, null, null);\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      // If the path already exists, doing nothing and return true\n+      if (e.getStatusCode() == ALREADY_EXISTS_STATUS_CODE && e.getErrorCode().equals(PATH_ALREADY_EXISTS_ERROR_CODE)) {\n+        return true;\n+      }\n+      LOGGER.error(\"Exception thrown while calling mkdir.\", e);\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean delete(URI segmentUri, boolean forceDelete) throws IOException {\n+    LOGGER.info(\"delete is called with segmentUri='{}', forceDelete='{}'\", segmentUri, forceDelete);\n+    try {\n+      boolean isDirectory = isDirectory(segmentUri);\n+      if (isDirectory && listFiles(segmentUri, false).length > 0 && !forceDelete) {\n+        return false;\n+      }\n+\n+      String path = convertUriToAzureStylePath(segmentUri);\n+      if (isDirectory) {\n+        _fileSystemClient.deleteDirectoryWithResponse(path, true, null, null, Context.NONE).getValue();\n+      } else {\n+        _fileSystemClient.deleteFile(path);\n+      }\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean doMove(URI srcUri, URI dstUri) throws IOException {\n+    LOGGER.info(\"doMove is called with srcUri='{}', dstUri='{}'\", srcUri, dstUri);\n+    try {\n+      // TODO: currently, azure-sdk has a bug in \"rename\" when the path includes some special characters that gets\n+      // changed during the url encoding (e.g '%' -> '%25', ' ' -> '%20')\n+      // https://github.com/Azure/azure-sdk-for-java/issues/8761\n+//      DataLakeDirectoryClient directoryClient =\n+//          _fileSystemClient.getDirectoryClient(convertUriToAzureStylePath(srcUri));\n+//      directoryClient.rename(null, convertUriToAzureStylePath(dstUri));\n+      copy(srcUri, dstUri);\n+      delete(srcUri, true);\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean copy(URI srcUri, URI dstUri) throws IOException {\n+    LOGGER.info(\"copy is called with srcUri='{}', dstUri='{}'\", srcUri, dstUri);\n+    // If src and dst are the same, do nothing.\n+    if (srcUri.equals(dstUri)) {\n+      return true;\n+    }\n+\n+    // Remove the destination directory or file\n+    if (exists(dstUri)) {\n+      delete(dstUri, true);\n+    }\n+\n+    if (!isDirectory(srcUri)) {\n+      // If source is a file, we can simply copy the file from src to dst\n+      return copySrcToDst(srcUri, dstUri);\n+    } else {\n+      // In case we are copying a directory, we need to recursively look into the directory and copy all the files and\n+      // directories accordingly\n+      try {\n+        boolean copySucceeded = false;\n+        Path srcPath = Paths.get(srcUri.getPath());\n+        for (String path : listFiles(srcUri, true)) {\n+          // Compute the src path for the given path\n+          URI currentSrc =\n+              new URI(srcUri.getScheme(), srcUri.getHost(), convertAzureStylePathToUriStylePath(path), null);\n+\n+          // Compute the destination path for the current path.\n+          String relativeSrcPath = srcPath.relativize(Paths.get(convertAzureStylePathToUriStylePath(path))).toString();\n+          String newDstPath = Paths.get(dstUri.getPath(), relativeSrcPath).toString();\n+          URI newDst = new URI(dstUri.getScheme(), dstUri.getHost(), newDstPath, null);\n+\n+          if (isDirectory(currentSrc)) {\n+            // If src is directory, create one.\n+            if (!mkdir(newDst)) {\n+              return false;\n+            }\n+          } else {\n+            // If src is a file, we need to copy.\n+            copySucceeded |= copySrcToDst(currentSrc, newDst);\n+          }\n+        }\n+        return copySucceeded;\n+      } catch (DataLakeStorageException | URISyntaxException e) {\n+        throw new IOException(e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public boolean exists(URI fileUri) throws IOException {\n+    try {\n+      _fileSystemClient.getDirectoryClient(convertUriToAzureStylePath(fileUri)).getProperties();\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      if (e.getStatusCode() == NOT_FOUND_STATUS_CODE) {\n+        return false;\n+      }\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public long length(URI fileUri) throws IOException {\n+    try {\n+      PathProperties pathProperties =\n+          _fileSystemClient.getDirectoryClient(convertUriToAzureStylePath(fileUri)).getProperties();\n+      return pathProperties.getFileSize();\n+    } catch (DataLakeStorageException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public String[] listFiles(URI fileUri, boolean recursive) throws IOException {\n+    LOGGER.info(\"listFiles is called with fileUri='{}', recursive='{}'\", fileUri, recursive);\n+    try {\n+      ListPathsOptions options =\n+          new ListPathsOptions().setPath(convertUriToAzureStylePath(fileUri)).setRecursive(recursive);\n+      PagedIterable<PathItem> iter = _fileSystemClient.listPaths(options, null);\n+      return iter.stream().map(PathItem::getName).toArray(String[]::new);\n+    } catch (DataLakeStorageException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public void copyToLocalFile(URI srcUri, File dstFile) throws Exception {\n+    LOGGER.info(\"copyToLocalFile is called with srcUri='{}', dstFile='{}'\", srcUri, dstFile);\n+    if (dstFile.exists()) {\n+      if (dstFile.isDirectory()) {\n+        FileUtils.deleteDirectory(dstFile);\n+      } else {\n+        FileUtils.deleteQuietly(dstFile);\n+      }\n+    }\n+\n+    MessageDigest md5File = MessageDigest.getInstance(\"MD5\");\n+    int bytesRead;\n+    byte[] buffer = new byte[BUFFER_SIZE];\n+    try (InputStream inputStream = open(srcUri)) {\n+      try (OutputStream outputStream = new FileOutputStream(dstFile)) {\n+        while ((bytesRead = inputStream.read(buffer)) != -1) {\n+          outputStream.write(buffer, 0, bytesRead);\n+          md5File.update(buffer, 0, bytesRead);", "originalCommit": "6d71f9802a47e9c8515296b50bc00987cf9485d1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE1MTkyMw==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r389151923", "bodyText": "will check md5 hash compute time but i think that this should be very quick", "author": "snleee", "createdAt": "2020-03-06T21:26:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ5MzE3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ5NTMxMw==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r388495313", "bodyText": "Seems like this part of the code can be refactored, since there is one more place doing something similar?", "author": "mayankshriv", "createdAt": "2020-03-05T18:57:07Z", "path": "pinot-plugins/pinot-file-system/pinot-adls/src/main/java/org/apache/pinot/plugin/filesystem/AzureGen2PinotFS.java", "diffHunk": "@@ -0,0 +1,447 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.plugin.filesystem;\n+\n+import com.azure.core.http.rest.PagedIterable;\n+import com.azure.core.util.Context;\n+import com.azure.storage.blob.BlobClient;\n+import com.azure.storage.blob.BlobServiceClient;\n+import com.azure.storage.blob.BlobServiceClientBuilder;\n+import com.azure.storage.common.StorageSharedKeyCredential;\n+import com.azure.storage.common.Utility;\n+import com.azure.storage.file.datalake.DataLakeFileClient;\n+import com.azure.storage.file.datalake.DataLakeFileSystemClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClientBuilder;\n+import com.azure.storage.file.datalake.models.DataLakeRequestConditions;\n+import com.azure.storage.file.datalake.models.DataLakeStorageException;\n+import com.azure.storage.file.datalake.models.ListPathsOptions;\n+import com.azure.storage.file.datalake.models.PathHttpHeaders;\n+import com.azure.storage.file.datalake.models.PathItem;\n+import com.azure.storage.file.datalake.models.PathProperties;\n+import java.io.ByteArrayInputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.net.URLDecoder;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.security.MessageDigest;\n+import java.security.NoSuchAlgorithmException;\n+import java.sql.Timestamp;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.util.Arrays;\n+import java.util.Map;\n+import org.apache.commons.configuration.Configuration;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Azure Data Lake Storage Gen2 implementation for the PinotFS interface.\n+ */\n+public class AzureGen2PinotFS extends PinotFS {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(AzureGen2PinotFS.class);\n+\n+  private static final String ACCOUNT_NAME = \"accountName\";\n+  private static final String ACCESS_KEY = \"accessKey\";\n+  private static final String FILE_SYSTEM_NAME = \"fileSystemName\";\n+\n+  private static final String HTTPS_URL_PREFIX = \"https://\";\n+  private static final String DIRECTORY_DELIMITER = \"/\";\n+\n+  private static final String AZURE_STORAGE_DNS_SUFFIX = \".dfs.core.windows.net\";\n+  private static final String AZURE_BLOB_DNS_SUFFIX = \".blob.core.windows.net\";\n+  private static final String PATH_ALREADY_EXISTS_ERROR_CODE = \"PathAlreadyExists\";\n+  private static final String IS_DIRECTORY_KEY = \"hdi_isfolder\";\n+\n+  private static final int NOT_FOUND_STATUS_CODE = 404;\n+  private static final int ALREADY_EXISTS_STATUS_CODE = 409;\n+\n+  // Azure Data Lake Gen2's block size is 4MB\n+  private static final int BUFFER_SIZE = 4 * 1024 * 1024;\n+\n+  private DataLakeFileSystemClient _fileSystemClient;\n+  private BlobServiceClient _blobServiceClient;\n+\n+  @Override\n+  public void init(Configuration config) {\n+    // Azure storage account name\n+    String accountName = config.getString(ACCOUNT_NAME);\n+    String accessKey = config.getString(ACCESS_KEY);\n+    String fileSystemName = config.getString(FILE_SYSTEM_NAME);\n+    String dfsServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_STORAGE_DNS_SUFFIX;\n+    String blobServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_BLOB_DNS_SUFFIX;\n+\n+    StorageSharedKeyCredential sharedKeyCredential = new StorageSharedKeyCredential(accountName, accessKey);\n+\n+    DataLakeServiceClient serviceClient = new DataLakeServiceClientBuilder().credential(sharedKeyCredential)\n+        .endpoint(dfsServiceEndpointUrl)\n+        .buildClient();\n+\n+    _blobServiceClient =\n+        new BlobServiceClientBuilder().credential(sharedKeyCredential).endpoint(blobServiceEndpointUrl).buildClient();\n+    _fileSystemClient = serviceClient.getFileSystemClient(fileSystemName);\n+    LOGGER.info(\"AzureGen2PinotFS is initialized (accountName={}, fileSystemName={}, dfsServiceEndpointUrl={}, \"\n+        + \"blobServiceEndpointUrl={})\", accountName, fileSystemName, dfsServiceEndpointUrl, blobServiceEndpointUrl);\n+  }\n+\n+  @Override\n+  public boolean mkdir(URI uri) throws IOException {\n+    LOGGER.info(\"mkdir is called with uri='{}'\", uri);\n+    try {\n+      // By default, create directory call will overwrite if the path already exists. Setting IfNoneMatch = \"*\" to\n+      // prevent overwrite. https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/create\n+      DataLakeRequestConditions requestConditions = new DataLakeRequestConditions().setIfNoneMatch(\"*\");\n+      _fileSystemClient.createDirectoryWithResponse(convertUriToAzureStylePath(uri), null, null, null, null,\n+          requestConditions, null, null);\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      // If the path already exists, doing nothing and return true\n+      if (e.getStatusCode() == ALREADY_EXISTS_STATUS_CODE && e.getErrorCode().equals(PATH_ALREADY_EXISTS_ERROR_CODE)) {\n+        return true;\n+      }\n+      LOGGER.error(\"Exception thrown while calling mkdir.\", e);\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean delete(URI segmentUri, boolean forceDelete) throws IOException {\n+    LOGGER.info(\"delete is called with segmentUri='{}', forceDelete='{}'\", segmentUri, forceDelete);\n+    try {\n+      boolean isDirectory = isDirectory(segmentUri);\n+      if (isDirectory && listFiles(segmentUri, false).length > 0 && !forceDelete) {\n+        return false;\n+      }\n+\n+      String path = convertUriToAzureStylePath(segmentUri);\n+      if (isDirectory) {\n+        _fileSystemClient.deleteDirectoryWithResponse(path, true, null, null, Context.NONE).getValue();\n+      } else {\n+        _fileSystemClient.deleteFile(path);\n+      }\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean doMove(URI srcUri, URI dstUri) throws IOException {\n+    LOGGER.info(\"doMove is called with srcUri='{}', dstUri='{}'\", srcUri, dstUri);\n+    try {\n+      // TODO: currently, azure-sdk has a bug in \"rename\" when the path includes some special characters that gets\n+      // changed during the url encoding (e.g '%' -> '%25', ' ' -> '%20')\n+      // https://github.com/Azure/azure-sdk-for-java/issues/8761\n+//      DataLakeDirectoryClient directoryClient =\n+//          _fileSystemClient.getDirectoryClient(convertUriToAzureStylePath(srcUri));\n+//      directoryClient.rename(null, convertUriToAzureStylePath(dstUri));\n+      copy(srcUri, dstUri);\n+      delete(srcUri, true);\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean copy(URI srcUri, URI dstUri) throws IOException {\n+    LOGGER.info(\"copy is called with srcUri='{}', dstUri='{}'\", srcUri, dstUri);\n+    // If src and dst are the same, do nothing.\n+    if (srcUri.equals(dstUri)) {\n+      return true;\n+    }\n+\n+    // Remove the destination directory or file\n+    if (exists(dstUri)) {\n+      delete(dstUri, true);\n+    }\n+\n+    if (!isDirectory(srcUri)) {\n+      // If source is a file, we can simply copy the file from src to dst\n+      return copySrcToDst(srcUri, dstUri);\n+    } else {\n+      // In case we are copying a directory, we need to recursively look into the directory and copy all the files and\n+      // directories accordingly\n+      try {\n+        boolean copySucceeded = false;\n+        Path srcPath = Paths.get(srcUri.getPath());\n+        for (String path : listFiles(srcUri, true)) {\n+          // Compute the src path for the given path\n+          URI currentSrc =\n+              new URI(srcUri.getScheme(), srcUri.getHost(), convertAzureStylePathToUriStylePath(path), null);\n+\n+          // Compute the destination path for the current path.\n+          String relativeSrcPath = srcPath.relativize(Paths.get(convertAzureStylePathToUriStylePath(path))).toString();\n+          String newDstPath = Paths.get(dstUri.getPath(), relativeSrcPath).toString();\n+          URI newDst = new URI(dstUri.getScheme(), dstUri.getHost(), newDstPath, null);\n+\n+          if (isDirectory(currentSrc)) {\n+            // If src is directory, create one.\n+            if (!mkdir(newDst)) {\n+              return false;\n+            }\n+          } else {\n+            // If src is a file, we need to copy.\n+            copySucceeded |= copySrcToDst(currentSrc, newDst);\n+          }\n+        }\n+        return copySucceeded;\n+      } catch (DataLakeStorageException | URISyntaxException e) {\n+        throw new IOException(e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public boolean exists(URI fileUri) throws IOException {\n+    try {\n+      _fileSystemClient.getDirectoryClient(convertUriToAzureStylePath(fileUri)).getProperties();\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      if (e.getStatusCode() == NOT_FOUND_STATUS_CODE) {\n+        return false;\n+      }\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public long length(URI fileUri) throws IOException {\n+    try {\n+      PathProperties pathProperties =\n+          _fileSystemClient.getDirectoryClient(convertUriToAzureStylePath(fileUri)).getProperties();\n+      return pathProperties.getFileSize();\n+    } catch (DataLakeStorageException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public String[] listFiles(URI fileUri, boolean recursive) throws IOException {\n+    LOGGER.info(\"listFiles is called with fileUri='{}', recursive='{}'\", fileUri, recursive);\n+    try {\n+      ListPathsOptions options =\n+          new ListPathsOptions().setPath(convertUriToAzureStylePath(fileUri)).setRecursive(recursive);\n+      PagedIterable<PathItem> iter = _fileSystemClient.listPaths(options, null);\n+      return iter.stream().map(PathItem::getName).toArray(String[]::new);\n+    } catch (DataLakeStorageException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public void copyToLocalFile(URI srcUri, File dstFile) throws Exception {\n+    LOGGER.info(\"copyToLocalFile is called with srcUri='{}', dstFile='{}'\", srcUri, dstFile);\n+    if (dstFile.exists()) {\n+      if (dstFile.isDirectory()) {\n+        FileUtils.deleteDirectory(dstFile);\n+      } else {\n+        FileUtils.deleteQuietly(dstFile);\n+      }\n+    }\n+\n+    MessageDigest md5File = MessageDigest.getInstance(\"MD5\");\n+    int bytesRead;\n+    byte[] buffer = new byte[BUFFER_SIZE];\n+    try (InputStream inputStream = open(srcUri)) {\n+      try (OutputStream outputStream = new FileOutputStream(dstFile)) {\n+        while ((bytesRead = inputStream.read(buffer)) != -1) {\n+          outputStream.write(buffer, 0, bytesRead);\n+          md5File.update(buffer, 0, bytesRead);\n+        }\n+      }\n+    }\n+\n+    // If MD5 hash is available as part of path properties, verify it with the local file\n+    DataLakeFileClient fileClient = _fileSystemClient.getFileClient(convertUriToAzureStylePath(srcUri));\n+    byte[] md5FromMetadata = fileClient.getProperties().getContentMd5();\n+    if (md5FromMetadata != null && md5FromMetadata.length > 0) {\n+      byte[] md5FromLocalFile = md5File.digest();\n+      if (!Arrays.equals(md5FromLocalFile, md5FromMetadata)) {\n+        // Clean up the corrupted file\n+        FileUtils.deleteQuietly(dstFile);\n+        throw new IOException(\"Computed MD5 and MD5 from metadata do not match\");\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void copyFromLocalFile(File srcFile, URI dstUri) throws Exception {\n+    LOGGER.info(\"copyFromLocalFile is called with srcFile='{}', dstUri='{}'\", srcFile, dstUri);\n+    try (InputStream fileInputStream = new FileInputStream(srcFile)) {\n+      copyInputStreamToDst(fileInputStream, dstUri);\n+    }\n+  }\n+\n+  @Override\n+  public boolean isDirectory(URI uri) throws IOException {\n+    try {\n+      PathProperties pathProperties = getPathProperties(uri);\n+      Map<String, String> metadata = pathProperties.getMetadata();\n+      // TODO: need to find the other ways to check the directory if it becomes available. listFiles API returns\n+      // PathInfo, which includes \"isDirectory\" field; however, there's no API available for fetching PathInfo directly\n+      // from target uri.\n+      return Boolean.valueOf(metadata.get(IS_DIRECTORY_KEY));\n+    } catch (DataLakeStorageException e) {\n+      throw new IOException(\"Failed while checking isDirectory for : \" + uri, e);\n+    }\n+  }\n+\n+  @Override\n+  public long lastModified(URI uri) throws IOException {\n+    try {\n+      PathProperties pathProperties = getPathProperties(uri);\n+      OffsetDateTime offsetDateTime = pathProperties.getLastModified();\n+      Timestamp timestamp = Timestamp.valueOf(offsetDateTime.atZoneSameInstant(ZoneOffset.UTC).toLocalDateTime());\n+      return timestamp.getTime();\n+    } catch (DataLakeStorageException e) {\n+      throw new IOException(\"Failed while checking lastModified time for : \" + uri, e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean touch(URI uri) throws IOException {\n+    // The following data lake gen2 API provides a way to update file properties including last modified time.\n+    // https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/update\n+    // However, action = \"setProperties\" is available in REST API but not available in Java SDK yet.\n+    //\n+    // For now, directly use Blob service's API to get the same effect.\n+    // https://docs.microsoft.com/en-us/rest/api/storageservices/set-file-properties\n+    try {\n+      DataLakeFileClient fileClient = _fileSystemClient.getFileClient(convertUriToAzureStylePath(uri));\n+      PathProperties pathProperties = fileClient.getProperties();\n+      fileClient.setHttpHeaders(getPathHttpHeaders(pathProperties));\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public InputStream open(URI uri) throws IOException {\n+    // Use Blob API since read() function from Data Lake Client currently takes \"OutputStream\" as an input and\n+    // flush bytes to an output stream. This needs to be piped back into input stream to implement this function.\n+    // On the other hand, Blob API directly allow you to open the input stream.\n+    BlobClient blobClient = _blobServiceClient.getBlobContainerClient(_fileSystemClient.getFileSystemName())\n+        .getBlobClient(convertUriToAzureStylePath(uri));\n+\n+    return blobClient.openInputStream();\n+    // Another approach is to download the file to the local disk to a temp path and return the file input stream. In\n+    // this case, we need to override \"close()\" and delete temp file.\n+  }\n+\n+  private boolean copySrcToDst(URI srcUri, URI dstUri) throws IOException {\n+    try (InputStream inputStream = open(srcUri)) {\n+      return copyInputStreamToDst(inputStream, dstUri);\n+    }\n+  }\n+\n+  /**\n+   * Helper function to copy input stream to destination URI.\n+   *\n+   * NOTE: the caller has to close the input stream.\n+   *\n+   * @param inputStream input stream that will be written to dstUri\n+   * @param dstUri destination URI\n+   * @return true if the copy succeeds\n+   */\n+  private boolean copyInputStreamToDst(InputStream inputStream, URI dstUri) throws IOException {\n+    int bytesRead;\n+    long totalBytesRead = 0;\n+    byte[] buffer = new byte[BUFFER_SIZE];\n+    DataLakeFileClient fileClient = _fileSystemClient.createFile(convertUriToAzureStylePath(dstUri));\n+    try {\n+      MessageDigest md5File = MessageDigest.getInstance(\"MD5\");", "originalCommit": "6d71f9802a47e9c8515296b50bc00987cf9485d1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE1OTMzOA==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r389159338", "bodyText": "discussed offline. resolving this.", "author": "snleee", "createdAt": "2020-03-06T21:44:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ5NTMxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODU2NzI3NA==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r388567274", "bodyText": "It'd be good to log the path here in error level.", "author": "jackjlli", "createdAt": "2020-03-05T21:10:17Z", "path": "pinot-plugins/pinot-file-system/pinot-adls/src/main/java/org/apache/pinot/plugin/filesystem/AzureGen2PinotFS.java", "diffHunk": "@@ -0,0 +1,447 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.plugin.filesystem;\n+\n+import com.azure.core.http.rest.PagedIterable;\n+import com.azure.core.util.Context;\n+import com.azure.storage.blob.BlobClient;\n+import com.azure.storage.blob.BlobServiceClient;\n+import com.azure.storage.blob.BlobServiceClientBuilder;\n+import com.azure.storage.common.StorageSharedKeyCredential;\n+import com.azure.storage.common.Utility;\n+import com.azure.storage.file.datalake.DataLakeFileClient;\n+import com.azure.storage.file.datalake.DataLakeFileSystemClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClientBuilder;\n+import com.azure.storage.file.datalake.models.DataLakeRequestConditions;\n+import com.azure.storage.file.datalake.models.DataLakeStorageException;\n+import com.azure.storage.file.datalake.models.ListPathsOptions;\n+import com.azure.storage.file.datalake.models.PathHttpHeaders;\n+import com.azure.storage.file.datalake.models.PathItem;\n+import com.azure.storage.file.datalake.models.PathProperties;\n+import java.io.ByteArrayInputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.net.URLDecoder;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.security.MessageDigest;\n+import java.security.NoSuchAlgorithmException;\n+import java.sql.Timestamp;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.util.Arrays;\n+import java.util.Map;\n+import org.apache.commons.configuration.Configuration;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Azure Data Lake Storage Gen2 implementation for the PinotFS interface.\n+ */\n+public class AzureGen2PinotFS extends PinotFS {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(AzureGen2PinotFS.class);\n+\n+  private static final String ACCOUNT_NAME = \"accountName\";\n+  private static final String ACCESS_KEY = \"accessKey\";\n+  private static final String FILE_SYSTEM_NAME = \"fileSystemName\";\n+\n+  private static final String HTTPS_URL_PREFIX = \"https://\";\n+  private static final String DIRECTORY_DELIMITER = \"/\";\n+\n+  private static final String AZURE_STORAGE_DNS_SUFFIX = \".dfs.core.windows.net\";\n+  private static final String AZURE_BLOB_DNS_SUFFIX = \".blob.core.windows.net\";\n+  private static final String PATH_ALREADY_EXISTS_ERROR_CODE = \"PathAlreadyExists\";\n+  private static final String IS_DIRECTORY_KEY = \"hdi_isfolder\";\n+\n+  private static final int NOT_FOUND_STATUS_CODE = 404;\n+  private static final int ALREADY_EXISTS_STATUS_CODE = 409;\n+\n+  // Azure Data Lake Gen2's block size is 4MB\n+  private static final int BUFFER_SIZE = 4 * 1024 * 1024;\n+\n+  private DataLakeFileSystemClient _fileSystemClient;\n+  private BlobServiceClient _blobServiceClient;\n+\n+  @Override\n+  public void init(Configuration config) {\n+    // Azure storage account name\n+    String accountName = config.getString(ACCOUNT_NAME);\n+    String accessKey = config.getString(ACCESS_KEY);\n+    String fileSystemName = config.getString(FILE_SYSTEM_NAME);\n+    String dfsServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_STORAGE_DNS_SUFFIX;\n+    String blobServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_BLOB_DNS_SUFFIX;\n+\n+    StorageSharedKeyCredential sharedKeyCredential = new StorageSharedKeyCredential(accountName, accessKey);\n+\n+    DataLakeServiceClient serviceClient = new DataLakeServiceClientBuilder().credential(sharedKeyCredential)\n+        .endpoint(dfsServiceEndpointUrl)\n+        .buildClient();\n+\n+    _blobServiceClient =\n+        new BlobServiceClientBuilder().credential(sharedKeyCredential).endpoint(blobServiceEndpointUrl).buildClient();\n+    _fileSystemClient = serviceClient.getFileSystemClient(fileSystemName);\n+    LOGGER.info(\"AzureGen2PinotFS is initialized (accountName={}, fileSystemName={}, dfsServiceEndpointUrl={}, \"\n+        + \"blobServiceEndpointUrl={})\", accountName, fileSystemName, dfsServiceEndpointUrl, blobServiceEndpointUrl);\n+  }\n+\n+  @Override\n+  public boolean mkdir(URI uri) throws IOException {\n+    LOGGER.info(\"mkdir is called with uri='{}'\", uri);\n+    try {\n+      // By default, create directory call will overwrite if the path already exists. Setting IfNoneMatch = \"*\" to\n+      // prevent overwrite. https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/create\n+      DataLakeRequestConditions requestConditions = new DataLakeRequestConditions().setIfNoneMatch(\"*\");\n+      _fileSystemClient.createDirectoryWithResponse(convertUriToAzureStylePath(uri), null, null, null, null,\n+          requestConditions, null, null);\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      // If the path already exists, doing nothing and return true\n+      if (e.getStatusCode() == ALREADY_EXISTS_STATUS_CODE && e.getErrorCode().equals(PATH_ALREADY_EXISTS_ERROR_CODE)) {\n+        return true;\n+      }\n+      LOGGER.error(\"Exception thrown while calling mkdir.\", e);", "originalCommit": "6d71f9802a47e9c8515296b50bc00987cf9485d1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE1MzQzMg==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r389153432", "bodyText": "added uri path as part of this log", "author": "snleee", "createdAt": "2020-03-06T21:29:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODU2NzI3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODU2OTkwMw==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r388569903", "bodyText": "According to the interface description of mkdir, it will either return true or throw an exception. Thus, it won't return false, right?", "author": "jackjlli", "createdAt": "2020-03-05T21:15:58Z", "path": "pinot-plugins/pinot-file-system/pinot-adls/src/main/java/org/apache/pinot/plugin/filesystem/AzureGen2PinotFS.java", "diffHunk": "@@ -0,0 +1,447 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.plugin.filesystem;\n+\n+import com.azure.core.http.rest.PagedIterable;\n+import com.azure.core.util.Context;\n+import com.azure.storage.blob.BlobClient;\n+import com.azure.storage.blob.BlobServiceClient;\n+import com.azure.storage.blob.BlobServiceClientBuilder;\n+import com.azure.storage.common.StorageSharedKeyCredential;\n+import com.azure.storage.common.Utility;\n+import com.azure.storage.file.datalake.DataLakeFileClient;\n+import com.azure.storage.file.datalake.DataLakeFileSystemClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClientBuilder;\n+import com.azure.storage.file.datalake.models.DataLakeRequestConditions;\n+import com.azure.storage.file.datalake.models.DataLakeStorageException;\n+import com.azure.storage.file.datalake.models.ListPathsOptions;\n+import com.azure.storage.file.datalake.models.PathHttpHeaders;\n+import com.azure.storage.file.datalake.models.PathItem;\n+import com.azure.storage.file.datalake.models.PathProperties;\n+import java.io.ByteArrayInputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.net.URLDecoder;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.security.MessageDigest;\n+import java.security.NoSuchAlgorithmException;\n+import java.sql.Timestamp;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.util.Arrays;\n+import java.util.Map;\n+import org.apache.commons.configuration.Configuration;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Azure Data Lake Storage Gen2 implementation for the PinotFS interface.\n+ */\n+public class AzureGen2PinotFS extends PinotFS {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(AzureGen2PinotFS.class);\n+\n+  private static final String ACCOUNT_NAME = \"accountName\";\n+  private static final String ACCESS_KEY = \"accessKey\";\n+  private static final String FILE_SYSTEM_NAME = \"fileSystemName\";\n+\n+  private static final String HTTPS_URL_PREFIX = \"https://\";\n+  private static final String DIRECTORY_DELIMITER = \"/\";\n+\n+  private static final String AZURE_STORAGE_DNS_SUFFIX = \".dfs.core.windows.net\";\n+  private static final String AZURE_BLOB_DNS_SUFFIX = \".blob.core.windows.net\";\n+  private static final String PATH_ALREADY_EXISTS_ERROR_CODE = \"PathAlreadyExists\";\n+  private static final String IS_DIRECTORY_KEY = \"hdi_isfolder\";\n+\n+  private static final int NOT_FOUND_STATUS_CODE = 404;\n+  private static final int ALREADY_EXISTS_STATUS_CODE = 409;\n+\n+  // Azure Data Lake Gen2's block size is 4MB\n+  private static final int BUFFER_SIZE = 4 * 1024 * 1024;\n+\n+  private DataLakeFileSystemClient _fileSystemClient;\n+  private BlobServiceClient _blobServiceClient;\n+\n+  @Override\n+  public void init(Configuration config) {\n+    // Azure storage account name\n+    String accountName = config.getString(ACCOUNT_NAME);\n+    String accessKey = config.getString(ACCESS_KEY);\n+    String fileSystemName = config.getString(FILE_SYSTEM_NAME);\n+    String dfsServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_STORAGE_DNS_SUFFIX;\n+    String blobServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_BLOB_DNS_SUFFIX;\n+\n+    StorageSharedKeyCredential sharedKeyCredential = new StorageSharedKeyCredential(accountName, accessKey);\n+\n+    DataLakeServiceClient serviceClient = new DataLakeServiceClientBuilder().credential(sharedKeyCredential)\n+        .endpoint(dfsServiceEndpointUrl)\n+        .buildClient();\n+\n+    _blobServiceClient =\n+        new BlobServiceClientBuilder().credential(sharedKeyCredential).endpoint(blobServiceEndpointUrl).buildClient();\n+    _fileSystemClient = serviceClient.getFileSystemClient(fileSystemName);\n+    LOGGER.info(\"AzureGen2PinotFS is initialized (accountName={}, fileSystemName={}, dfsServiceEndpointUrl={}, \"\n+        + \"blobServiceEndpointUrl={})\", accountName, fileSystemName, dfsServiceEndpointUrl, blobServiceEndpointUrl);\n+  }\n+\n+  @Override\n+  public boolean mkdir(URI uri) throws IOException {\n+    LOGGER.info(\"mkdir is called with uri='{}'\", uri);\n+    try {\n+      // By default, create directory call will overwrite if the path already exists. Setting IfNoneMatch = \"*\" to\n+      // prevent overwrite. https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/create\n+      DataLakeRequestConditions requestConditions = new DataLakeRequestConditions().setIfNoneMatch(\"*\");\n+      _fileSystemClient.createDirectoryWithResponse(convertUriToAzureStylePath(uri), null, null, null, null,\n+          requestConditions, null, null);\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      // If the path already exists, doing nothing and return true\n+      if (e.getStatusCode() == ALREADY_EXISTS_STATUS_CODE && e.getErrorCode().equals(PATH_ALREADY_EXISTS_ERROR_CODE)) {\n+        return true;\n+      }\n+      LOGGER.error(\"Exception thrown while calling mkdir.\", e);\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean delete(URI segmentUri, boolean forceDelete) throws IOException {\n+    LOGGER.info(\"delete is called with segmentUri='{}', forceDelete='{}'\", segmentUri, forceDelete);\n+    try {\n+      boolean isDirectory = isDirectory(segmentUri);\n+      if (isDirectory && listFiles(segmentUri, false).length > 0 && !forceDelete) {\n+        return false;\n+      }\n+\n+      String path = convertUriToAzureStylePath(segmentUri);\n+      if (isDirectory) {\n+        _fileSystemClient.deleteDirectoryWithResponse(path, true, null, null, Context.NONE).getValue();\n+      } else {\n+        _fileSystemClient.deleteFile(path);\n+      }\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean doMove(URI srcUri, URI dstUri) throws IOException {\n+    LOGGER.info(\"doMove is called with srcUri='{}', dstUri='{}'\", srcUri, dstUri);\n+    try {\n+      // TODO: currently, azure-sdk has a bug in \"rename\" when the path includes some special characters that gets\n+      // changed during the url encoding (e.g '%' -> '%25', ' ' -> '%20')\n+      // https://github.com/Azure/azure-sdk-for-java/issues/8761\n+//      DataLakeDirectoryClient directoryClient =\n+//          _fileSystemClient.getDirectoryClient(convertUriToAzureStylePath(srcUri));\n+//      directoryClient.rename(null, convertUriToAzureStylePath(dstUri));\n+      copy(srcUri, dstUri);\n+      delete(srcUri, true);\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean copy(URI srcUri, URI dstUri) throws IOException {\n+    LOGGER.info(\"copy is called with srcUri='{}', dstUri='{}'\", srcUri, dstUri);\n+    // If src and dst are the same, do nothing.\n+    if (srcUri.equals(dstUri)) {\n+      return true;\n+    }\n+\n+    // Remove the destination directory or file\n+    if (exists(dstUri)) {\n+      delete(dstUri, true);\n+    }\n+\n+    if (!isDirectory(srcUri)) {\n+      // If source is a file, we can simply copy the file from src to dst\n+      return copySrcToDst(srcUri, dstUri);\n+    } else {\n+      // In case we are copying a directory, we need to recursively look into the directory and copy all the files and\n+      // directories accordingly\n+      try {\n+        boolean copySucceeded = false;\n+        Path srcPath = Paths.get(srcUri.getPath());\n+        for (String path : listFiles(srcUri, true)) {\n+          // Compute the src path for the given path\n+          URI currentSrc =\n+              new URI(srcUri.getScheme(), srcUri.getHost(), convertAzureStylePathToUriStylePath(path), null);\n+\n+          // Compute the destination path for the current path.\n+          String relativeSrcPath = srcPath.relativize(Paths.get(convertAzureStylePathToUriStylePath(path))).toString();\n+          String newDstPath = Paths.get(dstUri.getPath(), relativeSrcPath).toString();\n+          URI newDst = new URI(dstUri.getScheme(), dstUri.getHost(), newDstPath, null);\n+\n+          if (isDirectory(currentSrc)) {\n+            // If src is directory, create one.\n+            if (!mkdir(newDst)) {", "originalCommit": "6d71f9802a47e9c8515296b50bc00987cf9485d1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE1NTU0NQ==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r389155545", "bodyText": "changed to mkdir(newDst)", "author": "snleee", "createdAt": "2020-03-06T21:35:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODU2OTkwMw=="}], "type": "inlineReview"}, {"oid": "797cf03a1a451c02731f6b2c9613561a487c6bc9", "url": "https://github.com/apache/pinot/commit/797cf03a1a451c02731f6b2c9613561a487c6bc9", "message": "Add Azure Data Lake Gen2 connector for PinotFS\n\n1. Testing have been done by attaching ADLS Gen2 to the local deployment.\n2. move() is implemented by copy & delete because of azure sdk issue with rename() API.\n   https://github.com/Azure/azure-sdk-for-java/issues/8761", "committedDate": "2020-03-06T21:46:15Z", "type": "forcePushed"}, {"oid": "6112a995d8a271518609c1a37717526b64803aea", "url": "https://github.com/apache/pinot/commit/6112a995d8a271518609c1a37717526b64803aea", "message": "Add Azure Data Lake Gen2 connector for PinotFS\n\n1. Testing have been done by attaching ADLS Gen2 to the local deployment.\n2. move() is implemented by copy & delete because of azure sdk issue with rename() API.\n   https://github.com/Azure/azure-sdk-for-java/issues/8761", "committedDate": "2020-03-10T01:29:38Z", "type": "forcePushed"}, {"oid": "4d2a8ff50b2dd64cc8ba1239d6d37219656222aa", "url": "https://github.com/apache/pinot/commit/4d2a8ff50b2dd64cc8ba1239d6d37219656222aa", "message": "Add Azure Data Lake Gen2 connector for PinotFS\n\n1. Testing have been done by attaching ADLS Gen2 to the local deployment.\n2. move() is implemented by copy & delete because of azure sdk issue with rename() API.\n   https://github.com/Azure/azure-sdk-for-java/issues/8761", "committedDate": "2020-03-10T02:20:02Z", "type": "forcePushed"}, {"oid": "23a6b2d73881c3e257494e11ec60c0bbfeb0e45c", "url": "https://github.com/apache/pinot/commit/23a6b2d73881c3e257494e11ec60c0bbfeb0e45c", "message": "Add Azure Data Lake Gen2 connector for PinotFS\n\n1. Testing have been done by attaching ADLS Gen2 to the local deployment.\n2. move() is implemented by copy & delete because of azure sdk issue with rename() API.\n   https://github.com/Azure/azure-sdk-for-java/issues/8761", "committedDate": "2020-03-10T17:51:31Z", "type": "forcePushed"}, {"oid": "67feb1023436f47280a5f3f48c4dfac697847e86", "url": "https://github.com/apache/pinot/commit/67feb1023436f47280a5f3f48c4dfac697847e86", "message": "Add Azure Data Lake Gen2 connector for PinotFS\n\n1. Testing have been done by attaching ADLS Gen2 to the local deployment.\n2. move() is implemented by copy & delete because of azure sdk issue with rename() API.\n   https://github.com/Azure/azure-sdk-for-java/issues/8761", "committedDate": "2020-03-12T01:35:55Z", "type": "commit"}, {"oid": "67feb1023436f47280a5f3f48c4dfac697847e86", "url": "https://github.com/apache/pinot/commit/67feb1023436f47280a5f3f48c4dfac697847e86", "message": "Add Azure Data Lake Gen2 connector for PinotFS\n\n1. Testing have been done by attaching ADLS Gen2 to the local deployment.\n2. move() is implemented by copy & delete because of azure sdk issue with rename() API.\n   https://github.com/Azure/azure-sdk-for-java/issues/8761", "committedDate": "2020-03-12T01:35:55Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTc5MjAzOQ==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r391792039", "bodyText": "why is this an error log?", "author": "mcvsubbu", "createdAt": "2020-03-12T17:50:21Z", "path": "pinot-plugins/pinot-file-system/pinot-adls/src/main/java/org/apache/pinot/plugin/filesystem/AzureGen2PinotFS.java", "diffHunk": "@@ -0,0 +1,460 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.plugin.filesystem;\n+\n+import com.azure.core.http.rest.PagedIterable;\n+import com.azure.core.util.Context;\n+import com.azure.storage.blob.BlobClient;\n+import com.azure.storage.blob.BlobServiceClient;\n+import com.azure.storage.blob.BlobServiceClientBuilder;\n+import com.azure.storage.common.StorageSharedKeyCredential;\n+import com.azure.storage.common.Utility;\n+import com.azure.storage.file.datalake.DataLakeDirectoryClient;\n+import com.azure.storage.file.datalake.DataLakeFileClient;\n+import com.azure.storage.file.datalake.DataLakeFileSystemClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClientBuilder;\n+import com.azure.storage.file.datalake.models.DataLakeRequestConditions;\n+import com.azure.storage.file.datalake.models.DataLakeStorageException;\n+import com.azure.storage.file.datalake.models.ListPathsOptions;\n+import com.azure.storage.file.datalake.models.PathHttpHeaders;\n+import com.azure.storage.file.datalake.models.PathItem;\n+import com.azure.storage.file.datalake.models.PathProperties;\n+import java.io.ByteArrayInputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.security.MessageDigest;\n+import java.security.NoSuchAlgorithmException;\n+import java.sql.Timestamp;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.util.Arrays;\n+import java.util.Map;\n+import org.apache.commons.configuration.Configuration;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Azure Data Lake Storage Gen2 implementation for the PinotFS interface.\n+ */\n+public class AzureGen2PinotFS extends PinotFS {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(AzureGen2PinotFS.class);\n+\n+  private static final String ACCOUNT_NAME = \"accountName\";\n+  private static final String ACCESS_KEY = \"accessKey\";\n+  private static final String FILE_SYSTEM_NAME = \"fileSystemName\";\n+  private static final String ENABLE_CHECKSUM = \"enableChecksum\";\n+\n+  private static final String HTTPS_URL_PREFIX = \"https://\";\n+\n+  private static final String AZURE_STORAGE_DNS_SUFFIX = \".dfs.core.windows.net\";\n+  private static final String AZURE_BLOB_DNS_SUFFIX = \".blob.core.windows.net\";\n+  private static final String PATH_ALREADY_EXISTS_ERROR_CODE = \"PathAlreadyExists\";\n+  private static final String IS_DIRECTORY_KEY = \"hdi_isfolder\";\n+\n+  private static final int NOT_FOUND_STATUS_CODE = 404;\n+  private static final int ALREADY_EXISTS_STATUS_CODE = 409;\n+\n+  // Azure Data Lake Gen2's block size is 4MB\n+  private static final int BUFFER_SIZE = 4 * 1024 * 1024;\n+\n+  private DataLakeFileSystemClient _fileSystemClient;\n+  private BlobServiceClient _blobServiceClient;\n+\n+  // If enabled, pinotFS implementation will guarantee that the bits you've read are the same as the ones you wrote.\n+  // However, there's some overhead in computing hash. (Adds roughly 3 seconds for 1GB file)\n+  private boolean _enableChecksum;\n+\n+  @Override\n+  public void init(Configuration config) {\n+    _enableChecksum = config.getBoolean(ENABLE_CHECKSUM, false);\n+\n+    // Azure storage account name\n+    String accountName = config.getString(ACCOUNT_NAME);\n+\n+    // TODO: consider to add the encryption of the following config\n+    String accessKey = config.getString(ACCESS_KEY);\n+    String fileSystemName = config.getString(FILE_SYSTEM_NAME);\n+\n+    String dfsServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_STORAGE_DNS_SUFFIX;\n+    String blobServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_BLOB_DNS_SUFFIX;\n+\n+    StorageSharedKeyCredential sharedKeyCredential = new StorageSharedKeyCredential(accountName, accessKey);\n+\n+    DataLakeServiceClient serviceClient = new DataLakeServiceClientBuilder().credential(sharedKeyCredential)\n+        .endpoint(dfsServiceEndpointUrl)\n+        .buildClient();\n+\n+    _blobServiceClient =\n+        new BlobServiceClientBuilder().credential(sharedKeyCredential).endpoint(blobServiceEndpointUrl).buildClient();\n+    _fileSystemClient = serviceClient.getFileSystemClient(fileSystemName);\n+    LOGGER.error(\"AzureGen2PinotFS is initialized (accountName={}, fileSystemName={}, dfsServiceEndpointUrl={}, \"", "originalCommit": "67feb1023436f47280a5f3f48c4dfac697847e86", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkxNTQyMA==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r391915420", "bodyText": "good catch. I was debugging and forgot to turn it back.", "author": "snleee", "createdAt": "2020-03-12T21:41:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTc5MjAzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTg1MTU2Mg==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r391851562", "bodyText": "It'd be good to print the error status code here.", "author": "jackjlli", "createdAt": "2020-03-12T19:44:46Z", "path": "pinot-plugins/pinot-file-system/pinot-adls/src/main/java/org/apache/pinot/plugin/filesystem/AzureGen2PinotFS.java", "diffHunk": "@@ -0,0 +1,460 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.plugin.filesystem;\n+\n+import com.azure.core.http.rest.PagedIterable;\n+import com.azure.core.util.Context;\n+import com.azure.storage.blob.BlobClient;\n+import com.azure.storage.blob.BlobServiceClient;\n+import com.azure.storage.blob.BlobServiceClientBuilder;\n+import com.azure.storage.common.StorageSharedKeyCredential;\n+import com.azure.storage.common.Utility;\n+import com.azure.storage.file.datalake.DataLakeDirectoryClient;\n+import com.azure.storage.file.datalake.DataLakeFileClient;\n+import com.azure.storage.file.datalake.DataLakeFileSystemClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClientBuilder;\n+import com.azure.storage.file.datalake.models.DataLakeRequestConditions;\n+import com.azure.storage.file.datalake.models.DataLakeStorageException;\n+import com.azure.storage.file.datalake.models.ListPathsOptions;\n+import com.azure.storage.file.datalake.models.PathHttpHeaders;\n+import com.azure.storage.file.datalake.models.PathItem;\n+import com.azure.storage.file.datalake.models.PathProperties;\n+import java.io.ByteArrayInputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.security.MessageDigest;\n+import java.security.NoSuchAlgorithmException;\n+import java.sql.Timestamp;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.util.Arrays;\n+import java.util.Map;\n+import org.apache.commons.configuration.Configuration;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Azure Data Lake Storage Gen2 implementation for the PinotFS interface.\n+ */\n+public class AzureGen2PinotFS extends PinotFS {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(AzureGen2PinotFS.class);\n+\n+  private static final String ACCOUNT_NAME = \"accountName\";\n+  private static final String ACCESS_KEY = \"accessKey\";\n+  private static final String FILE_SYSTEM_NAME = \"fileSystemName\";\n+  private static final String ENABLE_CHECKSUM = \"enableChecksum\";\n+\n+  private static final String HTTPS_URL_PREFIX = \"https://\";\n+\n+  private static final String AZURE_STORAGE_DNS_SUFFIX = \".dfs.core.windows.net\";\n+  private static final String AZURE_BLOB_DNS_SUFFIX = \".blob.core.windows.net\";\n+  private static final String PATH_ALREADY_EXISTS_ERROR_CODE = \"PathAlreadyExists\";\n+  private static final String IS_DIRECTORY_KEY = \"hdi_isfolder\";\n+\n+  private static final int NOT_FOUND_STATUS_CODE = 404;\n+  private static final int ALREADY_EXISTS_STATUS_CODE = 409;\n+\n+  // Azure Data Lake Gen2's block size is 4MB\n+  private static final int BUFFER_SIZE = 4 * 1024 * 1024;\n+\n+  private DataLakeFileSystemClient _fileSystemClient;\n+  private BlobServiceClient _blobServiceClient;\n+\n+  // If enabled, pinotFS implementation will guarantee that the bits you've read are the same as the ones you wrote.\n+  // However, there's some overhead in computing hash. (Adds roughly 3 seconds for 1GB file)\n+  private boolean _enableChecksum;\n+\n+  @Override\n+  public void init(Configuration config) {\n+    _enableChecksum = config.getBoolean(ENABLE_CHECKSUM, false);\n+\n+    // Azure storage account name\n+    String accountName = config.getString(ACCOUNT_NAME);\n+\n+    // TODO: consider to add the encryption of the following config\n+    String accessKey = config.getString(ACCESS_KEY);\n+    String fileSystemName = config.getString(FILE_SYSTEM_NAME);\n+\n+    String dfsServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_STORAGE_DNS_SUFFIX;\n+    String blobServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_BLOB_DNS_SUFFIX;\n+\n+    StorageSharedKeyCredential sharedKeyCredential = new StorageSharedKeyCredential(accountName, accessKey);\n+\n+    DataLakeServiceClient serviceClient = new DataLakeServiceClientBuilder().credential(sharedKeyCredential)\n+        .endpoint(dfsServiceEndpointUrl)\n+        .buildClient();\n+\n+    _blobServiceClient =\n+        new BlobServiceClientBuilder().credential(sharedKeyCredential).endpoint(blobServiceEndpointUrl).buildClient();\n+    _fileSystemClient = serviceClient.getFileSystemClient(fileSystemName);\n+    LOGGER.error(\"AzureGen2PinotFS is initialized (accountName={}, fileSystemName={}, dfsServiceEndpointUrl={}, \"\n+            + \"blobServiceEndpointUrl={}, enableChecksum={})\", accountName, fileSystemName, dfsServiceEndpointUrl,\n+        blobServiceEndpointUrl, _enableChecksum);\n+  }\n+\n+  @Override\n+  public boolean mkdir(URI uri) throws IOException {\n+    LOGGER.debug(\"mkdir is called with uri='{}'\", uri);\n+    try {\n+      // By default, create directory call will overwrite if the path already exists. Setting IfNoneMatch = \"*\" to\n+      // prevent overwrite. https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/create\n+      DataLakeRequestConditions requestConditions = new DataLakeRequestConditions().setIfNoneMatch(\"*\");\n+      _fileSystemClient.createDirectoryWithResponse(AzurePinotFSUtil.convertUriToUrlEncodedAzureStylePath(uri), null,\n+          null, null, null, requestConditions, null, null);\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      // If the path already exists, doing nothing and return true\n+      if (e.getStatusCode() == ALREADY_EXISTS_STATUS_CODE && e.getErrorCode().equals(PATH_ALREADY_EXISTS_ERROR_CODE)) {\n+        return true;\n+      }\n+      LOGGER.error(\"Exception thrown while calling mkdir (uri = {})\", uri, e);", "originalCommit": "67feb1023436f47280a5f3f48c4dfac697847e86", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjUxMjc4Nw==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r392512787", "bodyText": "I'm including e, which is exception object to the log. This should include the status code information as part of the exception stack. Do you think it's better to add status code explicitly along with uri?", "author": "snleee", "createdAt": "2020-03-13T22:26:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTg1MTU2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjUxNDA3Nw==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r392514077", "bodyText": "Not sure whether the status code is included in the exception, but it'd be good to show it in the log. :)", "author": "jackjlli", "createdAt": "2020-03-13T22:31:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTg1MTU2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTg1MzE4Nw==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r391853187", "bodyText": "What if part of the files failed? It will still return true, right?", "author": "jackjlli", "createdAt": "2020-03-12T19:48:17Z", "path": "pinot-plugins/pinot-file-system/pinot-adls/src/main/java/org/apache/pinot/plugin/filesystem/AzureGen2PinotFS.java", "diffHunk": "@@ -0,0 +1,460 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.plugin.filesystem;\n+\n+import com.azure.core.http.rest.PagedIterable;\n+import com.azure.core.util.Context;\n+import com.azure.storage.blob.BlobClient;\n+import com.azure.storage.blob.BlobServiceClient;\n+import com.azure.storage.blob.BlobServiceClientBuilder;\n+import com.azure.storage.common.StorageSharedKeyCredential;\n+import com.azure.storage.common.Utility;\n+import com.azure.storage.file.datalake.DataLakeDirectoryClient;\n+import com.azure.storage.file.datalake.DataLakeFileClient;\n+import com.azure.storage.file.datalake.DataLakeFileSystemClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClientBuilder;\n+import com.azure.storage.file.datalake.models.DataLakeRequestConditions;\n+import com.azure.storage.file.datalake.models.DataLakeStorageException;\n+import com.azure.storage.file.datalake.models.ListPathsOptions;\n+import com.azure.storage.file.datalake.models.PathHttpHeaders;\n+import com.azure.storage.file.datalake.models.PathItem;\n+import com.azure.storage.file.datalake.models.PathProperties;\n+import java.io.ByteArrayInputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.security.MessageDigest;\n+import java.security.NoSuchAlgorithmException;\n+import java.sql.Timestamp;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.util.Arrays;\n+import java.util.Map;\n+import org.apache.commons.configuration.Configuration;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Azure Data Lake Storage Gen2 implementation for the PinotFS interface.\n+ */\n+public class AzureGen2PinotFS extends PinotFS {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(AzureGen2PinotFS.class);\n+\n+  private static final String ACCOUNT_NAME = \"accountName\";\n+  private static final String ACCESS_KEY = \"accessKey\";\n+  private static final String FILE_SYSTEM_NAME = \"fileSystemName\";\n+  private static final String ENABLE_CHECKSUM = \"enableChecksum\";\n+\n+  private static final String HTTPS_URL_PREFIX = \"https://\";\n+\n+  private static final String AZURE_STORAGE_DNS_SUFFIX = \".dfs.core.windows.net\";\n+  private static final String AZURE_BLOB_DNS_SUFFIX = \".blob.core.windows.net\";\n+  private static final String PATH_ALREADY_EXISTS_ERROR_CODE = \"PathAlreadyExists\";\n+  private static final String IS_DIRECTORY_KEY = \"hdi_isfolder\";\n+\n+  private static final int NOT_FOUND_STATUS_CODE = 404;\n+  private static final int ALREADY_EXISTS_STATUS_CODE = 409;\n+\n+  // Azure Data Lake Gen2's block size is 4MB\n+  private static final int BUFFER_SIZE = 4 * 1024 * 1024;\n+\n+  private DataLakeFileSystemClient _fileSystemClient;\n+  private BlobServiceClient _blobServiceClient;\n+\n+  // If enabled, pinotFS implementation will guarantee that the bits you've read are the same as the ones you wrote.\n+  // However, there's some overhead in computing hash. (Adds roughly 3 seconds for 1GB file)\n+  private boolean _enableChecksum;\n+\n+  @Override\n+  public void init(Configuration config) {\n+    _enableChecksum = config.getBoolean(ENABLE_CHECKSUM, false);\n+\n+    // Azure storage account name\n+    String accountName = config.getString(ACCOUNT_NAME);\n+\n+    // TODO: consider to add the encryption of the following config\n+    String accessKey = config.getString(ACCESS_KEY);\n+    String fileSystemName = config.getString(FILE_SYSTEM_NAME);\n+\n+    String dfsServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_STORAGE_DNS_SUFFIX;\n+    String blobServiceEndpointUrl = HTTPS_URL_PREFIX + accountName + AZURE_BLOB_DNS_SUFFIX;\n+\n+    StorageSharedKeyCredential sharedKeyCredential = new StorageSharedKeyCredential(accountName, accessKey);\n+\n+    DataLakeServiceClient serviceClient = new DataLakeServiceClientBuilder().credential(sharedKeyCredential)\n+        .endpoint(dfsServiceEndpointUrl)\n+        .buildClient();\n+\n+    _blobServiceClient =\n+        new BlobServiceClientBuilder().credential(sharedKeyCredential).endpoint(blobServiceEndpointUrl).buildClient();\n+    _fileSystemClient = serviceClient.getFileSystemClient(fileSystemName);\n+    LOGGER.error(\"AzureGen2PinotFS is initialized (accountName={}, fileSystemName={}, dfsServiceEndpointUrl={}, \"\n+            + \"blobServiceEndpointUrl={}, enableChecksum={})\", accountName, fileSystemName, dfsServiceEndpointUrl,\n+        blobServiceEndpointUrl, _enableChecksum);\n+  }\n+\n+  @Override\n+  public boolean mkdir(URI uri) throws IOException {\n+    LOGGER.debug(\"mkdir is called with uri='{}'\", uri);\n+    try {\n+      // By default, create directory call will overwrite if the path already exists. Setting IfNoneMatch = \"*\" to\n+      // prevent overwrite. https://docs.microsoft.com/en-us/rest/api/storageservices/datalakestoragegen2/path/create\n+      DataLakeRequestConditions requestConditions = new DataLakeRequestConditions().setIfNoneMatch(\"*\");\n+      _fileSystemClient.createDirectoryWithResponse(AzurePinotFSUtil.convertUriToUrlEncodedAzureStylePath(uri), null,\n+          null, null, null, requestConditions, null, null);\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      // If the path already exists, doing nothing and return true\n+      if (e.getStatusCode() == ALREADY_EXISTS_STATUS_CODE && e.getErrorCode().equals(PATH_ALREADY_EXISTS_ERROR_CODE)) {\n+        return true;\n+      }\n+      LOGGER.error(\"Exception thrown while calling mkdir (uri = {})\", uri, e);\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean delete(URI segmentUri, boolean forceDelete) throws IOException {\n+    LOGGER.debug(\"delete is called with segmentUri='{}', forceDelete='{}'\", segmentUri, forceDelete);\n+    try {\n+      boolean isDirectory = isDirectory(segmentUri);\n+      if (isDirectory && listFiles(segmentUri, false).length > 0 && !forceDelete) {\n+        return false;\n+      }\n+\n+      String path = AzurePinotFSUtil.convertUriToUrlEncodedAzureStylePath(segmentUri);\n+      if (isDirectory) {\n+        _fileSystemClient.deleteDirectoryWithResponse(path, true, null, null, Context.NONE).getValue();\n+      } else {\n+        _fileSystemClient.deleteFile(path);\n+      }\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean doMove(URI srcUri, URI dstUri) throws IOException {\n+    LOGGER.debug(\"doMove is called with srcUri='{}', dstUri='{}'\", srcUri, dstUri);\n+    try {\n+      // TODO: currently, azure-sdk has a bug in \"rename\" when the path includes some special characters that gets\n+      // changed during the url encoding (e.g '%' -> '%25', ' ' -> '%20')\n+      // https://github.com/Azure/azure-sdk-for-java/issues/8761\n+      DataLakeDirectoryClient directoryClient =\n+          _fileSystemClient.getDirectoryClient(AzurePinotFSUtil.convertUriToUrlEncodedAzureStylePath(srcUri));\n+      directoryClient.rename(null, AzurePinotFSUtil.convertUriToUrlEncodedAzureStylePath(dstUri));\n+      return true;\n+    } catch (DataLakeStorageException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean copy(URI srcUri, URI dstUri) throws IOException {\n+    LOGGER.debug(\"copy is called with srcUri='{}', dstUri='{}'\", srcUri, dstUri);\n+    // If src and dst are the same, do nothing.\n+    if (srcUri.equals(dstUri)) {\n+      return true;\n+    }\n+\n+    // Remove the destination directory or file\n+    if (exists(dstUri)) {\n+      delete(dstUri, true);\n+    }\n+\n+    if (!isDirectory(srcUri)) {\n+      // If source is a file, we can simply copy the file from src to dst\n+      return copySrcToDst(srcUri, dstUri);\n+    } else {\n+      // In case we are copying a directory, we need to recursively look into the directory and copy all the files and\n+      // directories accordingly\n+      try {\n+        boolean copySucceeded = false;\n+        Path srcPath = Paths.get(srcUri.getPath());\n+        for (String path : listFiles(srcUri, true)) {\n+          // Compute the src path for the given path\n+          URI currentSrc = new URI(srcUri.getScheme(), srcUri.getHost(), path, null);\n+\n+          // Compute the destination path for the current path.\n+          String relativeSrcPath = srcPath.relativize(Paths.get(path)).toString();\n+          String newDstPath = Paths.get(dstUri.getPath(), relativeSrcPath).toString();\n+          URI newDst = new URI(dstUri.getScheme(), dstUri.getHost(), newDstPath, null);\n+\n+          if (isDirectory(currentSrc)) {\n+            // If src is directory, create one.\n+            mkdir(newDst);\n+          } else {\n+            // If src is a file, we need to copy.\n+            copySucceeded |= copySrcToDst(currentSrc, newDst);", "originalCommit": "67feb1023436f47280a5f3f48c4dfac697847e86", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjUxMjMwNg==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r392512306", "bodyText": "good catch :) I updated. I also made the change to GcsPinotFS, which had the similar issue.", "author": "snleee", "createdAt": "2020-03-13T22:24:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTg1MzE4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTg1NjQyNQ==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r391856425", "bodyText": "Can you add a test for testing the case when the scheme doesn't match then throw the exception?", "author": "jackjlli", "createdAt": "2020-03-12T19:55:12Z", "path": "pinot-spi/src/main/java/org/apache/pinot/spi/filesystem/PinotFS.java", "diffHunk": "@@ -103,8 +106,13 @@ public boolean move(URI srcUri, URI dstUri, boolean overwrite)\n       }\n     } else {\n       // ensures the parent path of dst exists.\n-      URI parentUri = Paths.get(dstUri).getParent().toUri();\n-      mkdir(parentUri);\n+      try {\n+        Path parentPath = Paths.get(dstUri.getPath()).getParent();", "originalCommit": "67feb1023436f47280a5f3f48c4dfac697847e86", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjUxOTU4MQ==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r392519581", "bodyText": "added a test to cover this case to LocalPinotFSTest", "author": "snleee", "createdAt": "2020-03-13T22:54:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTg1NjQyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTg1NzUzNg==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r391857536", "bodyText": "Do we have any test for this class? We've noticed the code coverage is only 50% though.", "author": "jackjlli", "createdAt": "2020-03-12T19:57:02Z", "path": "pinot-plugins/pinot-file-system/pinot-adls/src/main/java/org/apache/pinot/plugin/filesystem/AzureGen2PinotFS.java", "diffHunk": "@@ -0,0 +1,460 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.plugin.filesystem;\n+\n+import com.azure.core.http.rest.PagedIterable;\n+import com.azure.core.util.Context;\n+import com.azure.storage.blob.BlobClient;\n+import com.azure.storage.blob.BlobServiceClient;\n+import com.azure.storage.blob.BlobServiceClientBuilder;\n+import com.azure.storage.common.StorageSharedKeyCredential;\n+import com.azure.storage.common.Utility;\n+import com.azure.storage.file.datalake.DataLakeDirectoryClient;\n+import com.azure.storage.file.datalake.DataLakeFileClient;\n+import com.azure.storage.file.datalake.DataLakeFileSystemClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClient;\n+import com.azure.storage.file.datalake.DataLakeServiceClientBuilder;\n+import com.azure.storage.file.datalake.models.DataLakeRequestConditions;\n+import com.azure.storage.file.datalake.models.DataLakeStorageException;\n+import com.azure.storage.file.datalake.models.ListPathsOptions;\n+import com.azure.storage.file.datalake.models.PathHttpHeaders;\n+import com.azure.storage.file.datalake.models.PathItem;\n+import com.azure.storage.file.datalake.models.PathProperties;\n+import java.io.ByteArrayInputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.security.MessageDigest;\n+import java.security.NoSuchAlgorithmException;\n+import java.sql.Timestamp;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.util.Arrays;\n+import java.util.Map;\n+import org.apache.commons.configuration.Configuration;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Azure Data Lake Storage Gen2 implementation for the PinotFS interface.\n+ */\n+public class AzureGen2PinotFS extends PinotFS {", "originalCommit": "67feb1023436f47280a5f3f48c4dfac697847e86", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjUyMDYxMg==", "url": "https://github.com/apache/pinot/pull/5116#discussion_r392520612", "bodyText": "I don't see a good way to test. I've been testing this by hooking this up to the live ADLS Gen2. One way is to mock every single Azure SDK API that i'm calling using Mockhito but this doesn't really check much.\nAnother potential approach is to create the integration test by incorporating Azurite https://github.com/Azure/Azurite, which is Azure storage service emulator. But, this doesn't support Azure Datalake Gen2.\nBy the way, I did verify all the functions by hooking up the live Data Lake Gen2.", "author": "snleee", "createdAt": "2020-03-13T22:58:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTg1NzUzNg=="}], "type": "inlineReview"}, {"oid": "a9ec33c09fc67b581b227dbdca2ab49b985a1ae2", "url": "https://github.com/apache/pinot/commit/a9ec33c09fc67b581b227dbdca2ab49b985a1ae2", "message": "Addressing comments", "committedDate": "2020-03-17T00:12:52Z", "type": "commit"}, {"oid": "a9ec33c09fc67b581b227dbdca2ab49b985a1ae2", "url": "https://github.com/apache/pinot/commit/a9ec33c09fc67b581b227dbdca2ab49b985a1ae2", "message": "Addressing comments", "committedDate": "2020-03-17T00:12:52Z", "type": "forcePushed"}]}