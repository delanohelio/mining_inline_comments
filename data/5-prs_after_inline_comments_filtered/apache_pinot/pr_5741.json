{"pr_number": 5741, "pr_title": "Move lambda expression to inner function in pinot-spark", "pr_createdAt": "2020-07-23T09:34:11Z", "pr_url": "https://github.com/apache/pinot/pull/5741", "timeline": [{"oid": "4b6eb03d04b0b25ebf4a67d76c8856c8d8c805a3", "url": "https://github.com/apache/pinot/commit/4b6eb03d04b0b25ebf4a67d76c8856c8d8c805a3", "message": "Move lambda expression to inner function in pinot-spark", "committedDate": "2020-07-23T10:45:59Z", "type": "forcePushed"}, {"oid": "aaf69d8d26f1cb6ecebcad8660c95bd139eb24e6", "url": "https://github.com/apache/pinot/commit/aaf69d8d26f1cb6ecebcad8660c95bd139eb24e6", "message": "Move lambda expression to inner function in pinot-spark", "committedDate": "2020-07-23T10:50:08Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQ2NzM5Nw==", "url": "https://github.com/apache/pinot/pull/5741#discussion_r459467397", "bodyText": "I can't tell if the original or the new formatting adheres to the Pinot Style, please ensure the new one does.", "author": "mayankshriv", "createdAt": "2020-07-23T13:55:06Z", "path": "pinot-plugins/pinot-batch-ingestion/pinot-batch-ingestion-spark/src/main/java/org/apache/pinot/plugin/ingestion/batch/spark/SparkSegmentGenerationJobRunner.java", "diffHunk": "@@ -209,93 +210,99 @@ public void run()\n               .get(PLUGINS_INCLUDE_PROPERTY_NAME) : null;\n       final URI finalInputDirURI = inputDirURI;\n       final URI finalOutputDirURI = (stagingDirURI == null) ? outputDirURI : stagingDirURI;\n-      pathRDD.foreach(pathAndIdx -> {\n-        for (PinotFSSpec pinotFSSpec : _spec.getPinotFSSpecs()) {\n-          PinotFSFactory.register(pinotFSSpec.getScheme(), pinotFSSpec.getClassName(), new PinotConfiguration(pinotFSSpec));\n-        }\n-        PinotFS finalOutputDirFS = PinotFSFactory.create(finalOutputDirURI.getScheme());\n-        String[] splits = pathAndIdx.split(\" \");\n-        String path = splits[0];\n-        int idx = Integer.valueOf(splits[1]);\n-        // Load Pinot Plugins copied from Distributed cache.\n-        File localPluginsTarFile = new File(PINOT_PLUGINS_TAR_GZ);\n-        if (localPluginsTarFile.exists()) {\n-          File pluginsDirFile = new File(PINOT_PLUGINS_DIR + \"-\" + idx);\n-          try {\n-            TarGzCompressionUtils.untar(localPluginsTarFile, pluginsDirFile);\n-          } catch (Exception e) {\n-            LOGGER.error(\"Failed to untar local Pinot plugins tarball file [{}]\", localPluginsTarFile, e);\n-            throw new RuntimeException(e);\n+      // Prevent using lambda expression in Spark to avoid potential serialization exceptions, use inner function instead.\n+      pathRDD.foreach(new VoidFunction<String>() {\n+        @Override\n+        public void call(String pathAndIdx)\n+            throws Exception {\n+          PluginManager.get().init();\n+          for (PinotFSSpec pinotFSSpec : _spec.getPinotFSSpecs()) {\n+            PinotFSFactory.register(pinotFSSpec.getScheme(), pinotFSSpec.getClassName(), new PinotConfiguration(pinotFSSpec));\n           }\n-          LOGGER.info(\"Trying to set System Property: [{}={}]\", PLUGINS_DIR_PROPERTY_NAME,\n-              pluginsDirFile.getAbsolutePath());\n-          System.setProperty(PLUGINS_DIR_PROPERTY_NAME, pluginsDirFile.getAbsolutePath());\n-          if (pluginsInclude != null) {\n-            LOGGER.info(\"Trying to set System Property: [{}={}]\", PLUGINS_INCLUDE_PROPERTY_NAME, pluginsInclude);\n-            System.setProperty(PLUGINS_INCLUDE_PROPERTY_NAME, pluginsInclude);\n+          PinotFS finalOutputDirFS = PinotFSFactory.create(finalOutputDirURI.getScheme());\n+          String[] splits = pathAndIdx.split(\" \");\n+          String path = splits[0];\n+          int idx = Integer.valueOf(splits[1]);\n+          // Load Pinot Plugins copied from Distributed cache.\n+          File localPluginsTarFile = new File(PINOT_PLUGINS_TAR_GZ);\n+          if (localPluginsTarFile.exists()) {\n+            File pluginsDirFile = new File(PINOT_PLUGINS_DIR + \"-\" + idx);\n+            try {\n+              TarGzCompressionUtils.untar(localPluginsTarFile, pluginsDirFile);\n+            } catch (Exception e) {\n+              LOGGER.error(\"Failed to untar local Pinot plugins tarball file [{}]\", localPluginsTarFile, e);\n+              throw new RuntimeException(e);\n+            }\n+            LOGGER.info(\"Trying to set System Property: [{}={}]\", PLUGINS_DIR_PROPERTY_NAME,\n+                pluginsDirFile.getAbsolutePath());\n+            System.setProperty(PLUGINS_DIR_PROPERTY_NAME, pluginsDirFile.getAbsolutePath());\n+            if (pluginsInclude != null) {\n+              LOGGER.info(\"Trying to set System Property: [{}={}]\", PLUGINS_INCLUDE_PROPERTY_NAME, pluginsInclude);\n+              System.setProperty(PLUGINS_INCLUDE_PROPERTY_NAME, pluginsInclude);\n+            }\n+            LOGGER.info(\"Pinot plugins System Properties are set at [{}], plugins includes [{}]\",\n+                System.getProperty(PLUGINS_DIR_PROPERTY_NAME), System.getProperty(PLUGINS_INCLUDE_PROPERTY_NAME));\n+          } else {\n+            LOGGER.warn(\"Cannot find local Pinot plugins tar file at [{}]\", localPluginsTarFile.getAbsolutePath());\n+          }\n+          URI inputFileURI = URI.create(path);\n+          if (inputFileURI.getScheme() == null) {\n+            inputFileURI =\n+                new URI(finalInputDirURI.getScheme(), inputFileURI.getSchemeSpecificPart(), inputFileURI.getFragment());\n           }\n-          LOGGER.info(\"Pinot plugins System Properties are set at [{}], plugins includes [{}]\",\n-              System.getProperty(PLUGINS_DIR_PROPERTY_NAME), System.getProperty(PLUGINS_INCLUDE_PROPERTY_NAME));\n-        } else {\n-          LOGGER.warn(\"Cannot find local Pinot plugins tar file at [{}]\", localPluginsTarFile.getAbsolutePath());\n-        }\n-        URI inputFileURI = URI.create(path);\n-        if (inputFileURI.getScheme() == null) {\n-          inputFileURI =\n-              new URI(finalInputDirURI.getScheme(), inputFileURI.getSchemeSpecificPart(), inputFileURI.getFragment());\n-        }\n \n-        //create localTempDir for input and output\n-        File localTempDir = new File(FileUtils.getTempDirectory(), \"pinot-\" + UUID.randomUUID());\n-        File localInputTempDir = new File(localTempDir, \"input\");\n-        FileUtils.forceMkdir(localInputTempDir);\n-        File localOutputTempDir = new File(localTempDir, \"output\");\n-        FileUtils.forceMkdir(localOutputTempDir);\n+          //create localTempDir for input and output", "originalCommit": "aaf69d8d26f1cb6ecebcad8660c95bd139eb24e6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY3MTYxOQ==", "url": "https://github.com/apache/pinot/pull/5741#discussion_r459671619", "bodyText": "updated!", "author": "xiangfu0", "createdAt": "2020-07-23T19:15:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQ2NzM5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQ2ODAyNg==", "url": "https://github.com/apache/pinot/pull/5741#discussion_r459468026", "bodyText": "Thanks for fixing this, how to prevent this from happening in future? Is this unit testable?", "author": "mayankshriv", "createdAt": "2020-07-23T13:55:40Z", "path": "pinot-plugins/pinot-batch-ingestion/pinot-batch-ingestion-spark/src/main/java/org/apache/pinot/plugin/ingestion/batch/spark/SparkSegmentGenerationJobRunner.java", "diffHunk": "@@ -209,93 +210,99 @@ public void run()\n               .get(PLUGINS_INCLUDE_PROPERTY_NAME) : null;\n       final URI finalInputDirURI = inputDirURI;\n       final URI finalOutputDirURI = (stagingDirURI == null) ? outputDirURI : stagingDirURI;\n-      pathRDD.foreach(pathAndIdx -> {\n-        for (PinotFSSpec pinotFSSpec : _spec.getPinotFSSpecs()) {\n-          PinotFSFactory.register(pinotFSSpec.getScheme(), pinotFSSpec.getClassName(), new PinotConfiguration(pinotFSSpec));\n-        }\n-        PinotFS finalOutputDirFS = PinotFSFactory.create(finalOutputDirURI.getScheme());\n-        String[] splits = pathAndIdx.split(\" \");\n-        String path = splits[0];\n-        int idx = Integer.valueOf(splits[1]);\n-        // Load Pinot Plugins copied from Distributed cache.\n-        File localPluginsTarFile = new File(PINOT_PLUGINS_TAR_GZ);\n-        if (localPluginsTarFile.exists()) {\n-          File pluginsDirFile = new File(PINOT_PLUGINS_DIR + \"-\" + idx);\n-          try {\n-            TarGzCompressionUtils.untar(localPluginsTarFile, pluginsDirFile);\n-          } catch (Exception e) {\n-            LOGGER.error(\"Failed to untar local Pinot plugins tarball file [{}]\", localPluginsTarFile, e);\n-            throw new RuntimeException(e);\n+      // Prevent using lambda expression in Spark to avoid potential serialization exceptions, use inner function instead.", "originalCommit": "aaf69d8d26f1cb6ecebcad8660c95bd139eb24e6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTYzMDQ3OA==", "url": "https://github.com/apache/pinot/pull/5741#discussion_r459630478", "bodyText": "This is not happening in every spark version/cluster, likely to be a bug in Spark, but we need to accommodate it .\nWe hit this issue until some users reported it.\nI don't have a good way to prevent it apart from code review :(", "author": "xiangfu0", "createdAt": "2020-07-23T18:00:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQ2ODAyNg=="}], "type": "inlineReview"}, {"oid": "9d1759aa7e1b58855dc8e72c59b15b1281993c21", "url": "https://github.com/apache/pinot/commit/9d1759aa7e1b58855dc8e72c59b15b1281993c21", "message": "Move lambda expression to inner function in pinot-spark", "committedDate": "2020-07-23T19:11:39Z", "type": "commit"}, {"oid": "9d1759aa7e1b58855dc8e72c59b15b1281993c21", "url": "https://github.com/apache/pinot/commit/9d1759aa7e1b58855dc8e72c59b15b1281993c21", "message": "Move lambda expression to inner function in pinot-spark", "committedDate": "2020-07-23T19:11:39Z", "type": "forcePushed"}]}