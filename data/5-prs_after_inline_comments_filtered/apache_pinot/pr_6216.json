{"pr_number": 6216, "pr_title": "Nested Field (JSON) Indexing ", "pr_createdAt": "2020-11-01T01:15:58Z", "pr_url": "https://github.com/apache/pinot/pull/6216", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1NzU3NA==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515657574", "bodyText": "clean these up", "author": "yupeng9", "createdAt": "2020-11-01T18:51:53Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/operator/filter/JSONMatchFilterOperator.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.operator.filter;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.docidsets.BitmapDocIdSet;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.InPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.NotEqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.NotInPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.segment.creator.impl.inv.JSONIndexCreator;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.roaringbitmap.buffer.ImmutableRoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+\n+@SuppressWarnings(\"rawtypes\")\n+public class JSONMatchFilterOperator extends BaseFilterOperator {\n+  private static final String OPERATOR_NAME = \"JSONMatchFilterOperator\";\n+\n+  private final JSONIndexReader _jsonIndexReader;\n+  private final int _numDocs;\n+  private String _column;\n+  private final FilterContext _filterContext;\n+\n+  public JSONMatchFilterOperator(String column, FilterContext filterContext, JSONIndexReader jsonIndexReader,\n+      int numDocs) {\n+    _column = column;\n+    _filterContext = filterContext;\n+    _jsonIndexReader = jsonIndexReader;\n+    _numDocs = numDocs;\n+  }\n+\n+  @Override\n+  protected FilterBlock getNextBlock() {\n+    ImmutableRoaringBitmap flattenedDocIds = process(_filterContext);\n+//    System.out.println(\"flattenedDocIds = \" + flattenedDocIds);", "originalCommit": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1NzcxNw==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515657717", "bodyText": "add a handling of default", "author": "yupeng9", "createdAt": "2020-11-01T18:53:17Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/operator/filter/JSONMatchFilterOperator.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.operator.filter;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.docidsets.BitmapDocIdSet;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.InPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.NotEqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.NotInPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.segment.creator.impl.inv.JSONIndexCreator;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.roaringbitmap.buffer.ImmutableRoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+\n+@SuppressWarnings(\"rawtypes\")\n+public class JSONMatchFilterOperator extends BaseFilterOperator {\n+  private static final String OPERATOR_NAME = \"JSONMatchFilterOperator\";\n+\n+  private final JSONIndexReader _jsonIndexReader;\n+  private final int _numDocs;\n+  private String _column;\n+  private final FilterContext _filterContext;\n+\n+  public JSONMatchFilterOperator(String column, FilterContext filterContext, JSONIndexReader jsonIndexReader,\n+      int numDocs) {\n+    _column = column;\n+    _filterContext = filterContext;\n+    _jsonIndexReader = jsonIndexReader;\n+    _numDocs = numDocs;\n+  }\n+\n+  @Override\n+  protected FilterBlock getNextBlock() {\n+    ImmutableRoaringBitmap flattenedDocIds = process(_filterContext);\n+//    System.out.println(\"flattenedDocIds = \" + flattenedDocIds);\n+    MutableRoaringBitmap rootDocIds = new MutableRoaringBitmap();\n+    Iterator<Integer> iterator = flattenedDocIds.iterator();\n+    while (iterator.hasNext()) {\n+      int flattenedDocId = iterator.next();\n+      rootDocIds.add(_jsonIndexReader.getRootDocId(flattenedDocId));\n+    }\n+//    System.out.println(\"rootDocIds = \" + rootDocIds);\n+\n+    return new FilterBlock(new BitmapDocIdSet(rootDocIds, _numDocs));\n+  }\n+\n+  private MutableRoaringBitmap process(FilterContext filterContext) {\n+    List<FilterContext> children = _filterContext.getChildren();\n+    MutableRoaringBitmap resultBitmap = null;\n+\n+    switch (filterContext.getType()) {\n+      case AND:\n+        for (FilterContext child : children) {\n+          if (resultBitmap == null) {\n+            resultBitmap = process(child);\n+          } else {\n+            resultBitmap.and(process(child));\n+          }\n+        }\n+        break;\n+      case OR:\n+        for (FilterContext child : children) {\n+          if (resultBitmap == null) {\n+            resultBitmap = process(child);\n+          } else {\n+            resultBitmap.or(process(child));\n+          }\n+        }\n+        break;\n+      case PREDICATE:\n+        Predicate predicate = filterContext.getPredicate();\n+        Predicate newPredicate = null;\n+        switch (predicate.getType()) {\n+\n+          case EQ:\n+            EqPredicate eqPredicate = (EqPredicate) predicate;\n+            newPredicate = new EqPredicate(ExpressionContext.forIdentifier(_column),\n+                eqPredicate.getLhs().getIdentifier() + JSONIndexCreator.POSTING_LIST_KEY_SEPARATOR + eqPredicate\n+                    .getValue());\n+            break;\n+          case NOT_EQ:\n+            NotEqPredicate nEqPredicate = (NotEqPredicate) predicate;\n+            newPredicate = new NotEqPredicate(ExpressionContext.forIdentifier(_column),\n+                nEqPredicate.getLhs().getIdentifier() + JSONIndexCreator.POSTING_LIST_KEY_SEPARATOR\n+                    + nEqPredicate.getValue());\n+            break;\n+          case IN:\n+            InPredicate inPredicate = (InPredicate) predicate;\n+            List<String> newInValues = inPredicate.getValues().stream().map(\n+                value -> inPredicate.getLhs().getIdentifier() + JSONIndexCreator.POSTING_LIST_KEY_SEPARATOR\n+                    + value).collect(Collectors.toList());\n+            newPredicate = new InPredicate(ExpressionContext.forIdentifier(_column), newInValues);\n+            break;\n+          case NOT_IN:\n+            NotInPredicate notInPredicate = (NotInPredicate) predicate;\n+            List<String> newNotInValues = notInPredicate.getValues().stream().map(\n+                value -> notInPredicate.getLhs().getIdentifier() + JSONIndexCreator.POSTING_LIST_KEY_SEPARATOR\n+                    + value).collect(Collectors.toList());\n+            newPredicate = new InPredicate(ExpressionContext.forIdentifier(_column), newNotInValues);\n+            break;\n+          case IS_NULL:\n+            newPredicate = predicate;\n+            break;\n+          case IS_NOT_NULL:\n+            newPredicate = predicate;\n+            break;\n+          case RANGE:\n+          case REGEXP_LIKE:\n+          case TEXT_MATCH:\n+            throw new UnsupportedOperationException(\"JSON Match does not support RANGE, REGEXP or TEXTMATCH\");", "originalCommit": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1Nzc1Mw==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515657753", "bodyText": "revert", "author": "yupeng9", "createdAt": "2020-11-01T18:53:36Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/operator/transform/function/CastTransformFunction.java", "diffHunk": "@@ -30,7 +30,7 @@\n import org.apache.pinot.core.plan.DocIdSetPlanNode;\n \n \n-public class CastTransformFunction extends BaseTransformFunction {\n+  public class CastTransformFunction extends BaseTransformFunction {", "originalCommit": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1Nzk5MQ==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515657991", "bodyText": "add some javadoc, plz", "author": "yupeng9", "createdAt": "2020-11-01T18:55:28Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {", "originalCommit": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2MDA3MA==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515660070", "bodyText": "also need to ramp up test coverage of this class", "author": "yupeng9", "createdAt": "2020-11-01T19:14:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1Nzk5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MTc4Nw==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515671787", "bodyText": "+1 to both above.", "author": "mayankshriv", "createdAt": "2020-11-01T21:04:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1Nzk5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1ODEzMg==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515658132", "bodyText": "define these as constants", "author": "yupeng9", "createdAt": "2020-11-01T18:56:43Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {\n+\n+  //separator used to join the key and value to create posting list key\n+  public static String POSTING_LIST_KEY_SEPARATOR = \"|\";\n+  static int FLUSH_THRESHOLD = 50_000;\n+  static int VERSION = 1;\n+  private final File flattenedDocId2RootDocIdMappingFile;\n+  private final File postingListFile;\n+  private File dictionaryheaderFile;\n+  private File dictionaryOffsetFile;\n+  private File dictionaryFile;\n+  private File invertedIndexOffsetFile;\n+  private File invertedIndexFile;\n+  private File outputIndexFile;\n+\n+  private int docId = 0;\n+  private int numFlatennedDocId = 0;\n+  int chunkId = 0;\n+\n+  private DataOutputStream postingListWriter;\n+  private DataOutputStream flattenedDocId2RootDocIdWriter;\n+\n+  Map<String, List<Integer>> postingListMap = new TreeMap<>();\n+  List<Integer> flattenedDocIdList = new ArrayList<>();\n+  List<Integer> postingListChunkOffsets = new ArrayList<>();\n+  List<Integer> chunkLengths = new ArrayList<>();\n+  private FieldSpec fieldSpec;\n+\n+  public JSONIndexCreator(File indexDir, FieldSpec fieldSpec)\n+      throws IOException {\n+    this.fieldSpec = fieldSpec;\n+    System.out.println(\"indexDir = \" + indexDir);\n+\n+    String name = fieldSpec.getName();\n+    postingListFile = new File(indexDir + name + \"_postingList.buf\");\n+    postingListWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(postingListFile)));\n+    postingListChunkOffsets.add(postingListWriter.size());\n+\n+    dictionaryheaderFile = new File(indexDir, name + \"_dictionaryHeader.buf\");", "originalCommit": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1OTA0Mw==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515659043", "bodyText": "do you think this header is future-proof? Will it ever evolve and change?", "author": "yupeng9", "createdAt": "2020-11-01T19:05:23Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {\n+\n+  //separator used to join the key and value to create posting list key\n+  public static String POSTING_LIST_KEY_SEPARATOR = \"|\";\n+  static int FLUSH_THRESHOLD = 50_000;\n+  static int VERSION = 1;\n+  private final File flattenedDocId2RootDocIdMappingFile;\n+  private final File postingListFile;\n+  private File dictionaryheaderFile;\n+  private File dictionaryOffsetFile;\n+  private File dictionaryFile;\n+  private File invertedIndexOffsetFile;\n+  private File invertedIndexFile;\n+  private File outputIndexFile;\n+\n+  private int docId = 0;\n+  private int numFlatennedDocId = 0;\n+  int chunkId = 0;\n+\n+  private DataOutputStream postingListWriter;\n+  private DataOutputStream flattenedDocId2RootDocIdWriter;\n+\n+  Map<String, List<Integer>> postingListMap = new TreeMap<>();\n+  List<Integer> flattenedDocIdList = new ArrayList<>();\n+  List<Integer> postingListChunkOffsets = new ArrayList<>();\n+  List<Integer> chunkLengths = new ArrayList<>();\n+  private FieldSpec fieldSpec;\n+\n+  public JSONIndexCreator(File indexDir, FieldSpec fieldSpec)\n+      throws IOException {\n+    this.fieldSpec = fieldSpec;\n+    System.out.println(\"indexDir = \" + indexDir);\n+\n+    String name = fieldSpec.getName();\n+    postingListFile = new File(indexDir + name + \"_postingList.buf\");\n+    postingListWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(postingListFile)));\n+    postingListChunkOffsets.add(postingListWriter.size());\n+\n+    dictionaryheaderFile = new File(indexDir, name + \"_dictionaryHeader.buf\");\n+    dictionaryOffsetFile = new File(indexDir, name + \"_dictionaryOffset.buf\");\n+    dictionaryFile = new File(indexDir, name + \"_dictionary.buf\");\n+    invertedIndexOffsetFile = new File(indexDir, name + \"_invertedIndexOffset.buf\");\n+    invertedIndexFile = new File(indexDir, name + \"_invertedIndex.buf\");\n+    flattenedDocId2RootDocIdMappingFile = new File(indexDir, name + \"_flattenedDocId.buf\");\n+    flattenedDocId2RootDocIdWriter =\n+        new DataOutputStream(new BufferedOutputStream(new FileOutputStream(flattenedDocId2RootDocIdMappingFile)));\n+\n+    //output file\n+    outputIndexFile = new File(indexDir, name + JSON_INDEX_FILE_EXTENSION);\n+  }\n+\n+  public void add(byte[] data)\n+      throws IOException {\n+\n+    JsonNode jsonNode = new ObjectMapper().readTree(data);\n+    List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+    for (Map<String, String> map : flattenedMapList) {\n+      //\n+      for (Map.Entry<String, String> entry : map.entrySet()) {\n+        //handle key posting list\n+        String key = entry.getKey();\n+\n+        List<Integer> keyPostingList = postingListMap.get(key);\n+        if (keyPostingList == null) {\n+          keyPostingList = new ArrayList<>();\n+          postingListMap.put(key, keyPostingList);\n+        }\n+        keyPostingList.add(numFlatennedDocId);\n+\n+        //handle keyvalue posting list\n+        String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+        List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+        if (keyValuePostingList == null) {\n+          keyValuePostingList = new ArrayList<>();\n+          postingListMap.put(keyValue, keyValuePostingList);\n+        }\n+        keyValuePostingList.add(numFlatennedDocId);\n+      }\n+      //flattenedDocId2RootDocIdMapping\n+      flattenedDocIdList.add(docId);\n+\n+      numFlatennedDocId++;\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  /**\n+   * Multi value\n+   * @param dataArray\n+   * @param length\n+   * @throws IOException\n+   */\n+  public void add(byte[][] dataArray, int length)\n+      throws IOException {\n+\n+    for (int i = 0; i < length; i++) {\n+      byte[] data = dataArray[i];\n+      JsonNode jsonNode = new ObjectMapper().readTree(data);\n+      List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+      for (Map<String, String> map : flattenedMapList) {\n+        //\n+        for (Map.Entry<String, String> entry : map.entrySet()) {\n+          //handle key posting list\n+          String key = entry.getKey();\n+\n+          List<Integer> keyPostingList = postingListMap.get(key);\n+          if (keyPostingList == null) {\n+            keyPostingList = new ArrayList<>();\n+            postingListMap.put(key, keyPostingList);\n+          }\n+          keyPostingList.add(numFlatennedDocId);\n+\n+          //handle keyvalue posting list\n+          String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+          List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+          if (keyValuePostingList == null) {\n+            keyValuePostingList = new ArrayList<>();\n+            postingListMap.put(keyValue, keyValuePostingList);\n+          }\n+          keyValuePostingList.add(numFlatennedDocId);\n+        }\n+        //flattenedDocId2RootDocIdMapping\n+        flattenedDocIdList.add(numFlatennedDocId);\n+\n+        numFlatennedDocId++;\n+      }\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  public void seal()\n+      throws IOException {\n+\n+    flush();\n+\n+    flattenedDocId2RootDocIdWriter.close();\n+    postingListWriter.close();\n+\n+    //key posting list merging\n+    System.out.println(\"InvertedIndex\");\n+    System.out.println(\"=================\");\n+\n+    int maxKeyLength = createInvertedIndex(postingListFile, postingListChunkOffsets, chunkLengths);\n+    System.out.println(\"=================\");\n+\n+    int flattenedDocid = 0;\n+    DataInputStream flattenedDocId2RootDocIdReader =\n+        new DataInputStream(new BufferedInputStream(new FileInputStream(flattenedDocId2RootDocIdMappingFile)));\n+    int[] rootDocIdArray = new int[numFlatennedDocId];\n+    while (flattenedDocid < numFlatennedDocId) {\n+      rootDocIdArray[flattenedDocid++] = flattenedDocId2RootDocIdReader.readInt();\n+    }\n+    System.out.println(\"FlattenedDocId  to RootDocId Mapping = \");\n+    System.out.println(Arrays.toString(rootDocIdArray));\n+\n+    //PUT all contents into one file\n+\n+    //header\n+    // version + maxDictionaryLength + [store the offsets + length for each one (dictionary offset file, dictionaryFile, index offset file, index file, flattened docId to rootDocId file)]\n+    long headerSize = 2 * Integer.BYTES + 6 * 2 * Long.BYTES;", "originalCommit": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MjUwMg==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515672502", "bodyText": "Perhaps adding an offset to payload/data-start makes header expandable for future and helps with compatibility.", "author": "mayankshriv", "createdAt": "2020-11-01T21:10:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1OTA0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTI0OTMxNg==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r541249316", "bodyText": "As long as the version is included in the header, we can always evolve. The reader factory can check the version and create the proper reader for the version", "author": "Jackie-Jiang", "createdAt": "2020-12-11T20:23:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1OTA0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1OTQyMA==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515659420", "bodyText": "how do you handle the key that contains .?", "author": "yupeng9", "createdAt": "2020-11-01T19:09:10Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {\n+\n+  //separator used to join the key and value to create posting list key\n+  public static String POSTING_LIST_KEY_SEPARATOR = \"|\";\n+  static int FLUSH_THRESHOLD = 50_000;\n+  static int VERSION = 1;\n+  private final File flattenedDocId2RootDocIdMappingFile;\n+  private final File postingListFile;\n+  private File dictionaryheaderFile;\n+  private File dictionaryOffsetFile;\n+  private File dictionaryFile;\n+  private File invertedIndexOffsetFile;\n+  private File invertedIndexFile;\n+  private File outputIndexFile;\n+\n+  private int docId = 0;\n+  private int numFlatennedDocId = 0;\n+  int chunkId = 0;\n+\n+  private DataOutputStream postingListWriter;\n+  private DataOutputStream flattenedDocId2RootDocIdWriter;\n+\n+  Map<String, List<Integer>> postingListMap = new TreeMap<>();\n+  List<Integer> flattenedDocIdList = new ArrayList<>();\n+  List<Integer> postingListChunkOffsets = new ArrayList<>();\n+  List<Integer> chunkLengths = new ArrayList<>();\n+  private FieldSpec fieldSpec;\n+\n+  public JSONIndexCreator(File indexDir, FieldSpec fieldSpec)\n+      throws IOException {\n+    this.fieldSpec = fieldSpec;\n+    System.out.println(\"indexDir = \" + indexDir);\n+\n+    String name = fieldSpec.getName();\n+    postingListFile = new File(indexDir + name + \"_postingList.buf\");\n+    postingListWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(postingListFile)));\n+    postingListChunkOffsets.add(postingListWriter.size());\n+\n+    dictionaryheaderFile = new File(indexDir, name + \"_dictionaryHeader.buf\");\n+    dictionaryOffsetFile = new File(indexDir, name + \"_dictionaryOffset.buf\");\n+    dictionaryFile = new File(indexDir, name + \"_dictionary.buf\");\n+    invertedIndexOffsetFile = new File(indexDir, name + \"_invertedIndexOffset.buf\");\n+    invertedIndexFile = new File(indexDir, name + \"_invertedIndex.buf\");\n+    flattenedDocId2RootDocIdMappingFile = new File(indexDir, name + \"_flattenedDocId.buf\");\n+    flattenedDocId2RootDocIdWriter =\n+        new DataOutputStream(new BufferedOutputStream(new FileOutputStream(flattenedDocId2RootDocIdMappingFile)));\n+\n+    //output file\n+    outputIndexFile = new File(indexDir, name + JSON_INDEX_FILE_EXTENSION);\n+  }\n+\n+  public void add(byte[] data)\n+      throws IOException {\n+\n+    JsonNode jsonNode = new ObjectMapper().readTree(data);\n+    List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+    for (Map<String, String> map : flattenedMapList) {\n+      //\n+      for (Map.Entry<String, String> entry : map.entrySet()) {\n+        //handle key posting list\n+        String key = entry.getKey();\n+\n+        List<Integer> keyPostingList = postingListMap.get(key);\n+        if (keyPostingList == null) {\n+          keyPostingList = new ArrayList<>();\n+          postingListMap.put(key, keyPostingList);\n+        }\n+        keyPostingList.add(numFlatennedDocId);\n+\n+        //handle keyvalue posting list\n+        String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+        List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+        if (keyValuePostingList == null) {\n+          keyValuePostingList = new ArrayList<>();\n+          postingListMap.put(keyValue, keyValuePostingList);\n+        }\n+        keyValuePostingList.add(numFlatennedDocId);\n+      }\n+      //flattenedDocId2RootDocIdMapping\n+      flattenedDocIdList.add(docId);\n+\n+      numFlatennedDocId++;\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  /**\n+   * Multi value\n+   * @param dataArray\n+   * @param length\n+   * @throws IOException\n+   */\n+  public void add(byte[][] dataArray, int length)\n+      throws IOException {\n+\n+    for (int i = 0; i < length; i++) {\n+      byte[] data = dataArray[i];\n+      JsonNode jsonNode = new ObjectMapper().readTree(data);\n+      List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+      for (Map<String, String> map : flattenedMapList) {\n+        //\n+        for (Map.Entry<String, String> entry : map.entrySet()) {\n+          //handle key posting list\n+          String key = entry.getKey();\n+\n+          List<Integer> keyPostingList = postingListMap.get(key);\n+          if (keyPostingList == null) {\n+            keyPostingList = new ArrayList<>();\n+            postingListMap.put(key, keyPostingList);\n+          }\n+          keyPostingList.add(numFlatennedDocId);\n+\n+          //handle keyvalue posting list\n+          String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+          List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+          if (keyValuePostingList == null) {\n+            keyValuePostingList = new ArrayList<>();\n+            postingListMap.put(keyValue, keyValuePostingList);\n+          }\n+          keyValuePostingList.add(numFlatennedDocId);\n+        }\n+        //flattenedDocId2RootDocIdMapping\n+        flattenedDocIdList.add(numFlatennedDocId);\n+\n+        numFlatennedDocId++;\n+      }\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  public void seal()\n+      throws IOException {\n+\n+    flush();\n+\n+    flattenedDocId2RootDocIdWriter.close();\n+    postingListWriter.close();\n+\n+    //key posting list merging\n+    System.out.println(\"InvertedIndex\");\n+    System.out.println(\"=================\");\n+\n+    int maxKeyLength = createInvertedIndex(postingListFile, postingListChunkOffsets, chunkLengths);\n+    System.out.println(\"=================\");\n+\n+    int flattenedDocid = 0;\n+    DataInputStream flattenedDocId2RootDocIdReader =\n+        new DataInputStream(new BufferedInputStream(new FileInputStream(flattenedDocId2RootDocIdMappingFile)));\n+    int[] rootDocIdArray = new int[numFlatennedDocId];\n+    while (flattenedDocid < numFlatennedDocId) {\n+      rootDocIdArray[flattenedDocid++] = flattenedDocId2RootDocIdReader.readInt();\n+    }\n+    System.out.println(\"FlattenedDocId  to RootDocId Mapping = \");\n+    System.out.println(Arrays.toString(rootDocIdArray));\n+\n+    //PUT all contents into one file\n+\n+    //header\n+    // version + maxDictionaryLength + [store the offsets + length for each one (dictionary offset file, dictionaryFile, index offset file, index file, flattened docId to rootDocId file)]\n+    long headerSize = 2 * Integer.BYTES + 6 * 2 * Long.BYTES;\n+\n+    long dataSize =\n+        dictionaryheaderFile.length() + dictionaryOffsetFile.length() + dictionaryFile.length() + invertedIndexFile\n+            .length() + invertedIndexOffsetFile.length() + flattenedDocId2RootDocIdMappingFile.length();\n+\n+    long totalSize = headerSize + dataSize;\n+    PinotDataBuffer pinotDataBuffer =\n+        PinotDataBuffer.mapFile(outputIndexFile, false, 0, totalSize, ByteOrder.BIG_ENDIAN, \"Nested inverted index\");\n+\n+    pinotDataBuffer.putInt(0, VERSION);\n+    pinotDataBuffer.putInt(1 * Integer.BYTES, maxKeyLength);\n+    long writtenBytes = headerSize;\n+\n+    //add dictionary header\n+    int bufferId = 0;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryheaderFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryheaderFile, 0, dictionaryheaderFile.length());\n+    writtenBytes += dictionaryheaderFile.length();\n+\n+    //add dictionary offset\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryOffsetFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryOffsetFile, 0, dictionaryOffsetFile.length());\n+    writtenBytes += dictionaryOffsetFile.length();\n+\n+    //add dictionary\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryFile, 0, dictionaryFile.length());\n+    writtenBytes += dictionaryFile.length();\n+\n+    //add index offset\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, invertedIndexOffsetFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, invertedIndexOffsetFile, 0, invertedIndexOffsetFile.length());\n+    writtenBytes += invertedIndexOffsetFile.length();\n+\n+    //add index data\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, invertedIndexFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, invertedIndexFile, 0, invertedIndexFile.length());\n+    writtenBytes += invertedIndexFile.length();\n+\n+    //add flattened docid to root doc id mapping\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, flattenedDocId2RootDocIdMappingFile.length());\n+    pinotDataBuffer\n+        .readFrom(writtenBytes, flattenedDocId2RootDocIdMappingFile, 0, flattenedDocId2RootDocIdMappingFile.length());\n+    writtenBytes += flattenedDocId2RootDocIdMappingFile.length();\n+  }\n+\n+  private long getBufferStartOffset(int bufferId) {\n+    return 2 * Integer.BYTES + 2 * bufferId * Long.BYTES;\n+  }\n+\n+  private int createInvertedIndex(File postingListFile, List<Integer> postingListChunkOffsets,\n+      List<Integer> chunkLengthList)\n+      throws IOException {\n+\n+    List<Iterator<ImmutablePair<byte[], int[]>>> chunkIterators = new ArrayList<>();\n+\n+    for (int i = 0; i < chunkLengthList.size(); i++) {\n+\n+      final DataInputStream postingListFileReader =\n+          new DataInputStream(new BufferedInputStream(new FileInputStream(postingListFile)));\n+      postingListFileReader.skipBytes(postingListChunkOffsets.get(i));\n+      final int length = chunkLengthList.get(i);\n+      chunkIterators.add(new Iterator<ImmutablePair<byte[], int[]>>() {\n+        int index = 0;\n+\n+        @Override\n+        public boolean hasNext() {\n+          return index < length;\n+        }\n+\n+        @Override\n+        public ImmutablePair<byte[], int[]> next() {\n+          try {\n+            int keyLength = postingListFileReader.readInt();\n+            byte[] keyBytes = new byte[keyLength];\n+            postingListFileReader.read(keyBytes);\n+\n+            int postingListLength = postingListFileReader.readInt();\n+            int[] postingList = new int[postingListLength];\n+            for (int i = 0; i < postingListLength; i++) {\n+              postingList[i] = postingListFileReader.readInt();\n+            }\n+            index++;\n+            return ImmutablePair.of(keyBytes, postingList);\n+          } catch (Exception e) {\n+            throw new RuntimeException(e);\n+          }\n+        }\n+      });\n+    }\n+    final Comparator<byte[]> byteArrayComparator = UnsignedBytes.lexicographicalComparator();\n+\n+    PriorityQueue<ImmutablePair<Integer, ImmutablePair<byte[], int[]>>> queue =\n+        new PriorityQueue<>(chunkLengthList.size(),\n+            (o1, o2) -> byteArrayComparator.compare(o1.getRight().getLeft(), o2.getRight().getLeft()));\n+    for (int i = 0; i < chunkIterators.size(); i++) {\n+      Iterator<ImmutablePair<byte[], int[]>> iterator = chunkIterators.get(i);\n+      if (iterator.hasNext()) {\n+        queue.offer(ImmutablePair.of(i, iterator.next()));\n+      }\n+    }\n+    byte[] prevKey = null;\n+    RoaringBitmap roaringBitmap = new RoaringBitmap();\n+\n+    Writer writer = new Writer(dictionaryheaderFile, dictionaryOffsetFile, dictionaryFile, invertedIndexOffsetFile,\n+        invertedIndexFile);\n+    while (!queue.isEmpty()) {\n+      ImmutablePair<Integer, ImmutablePair<byte[], int[]>> poll = queue.poll();\n+      byte[] currKey = poll.getRight().getLeft();\n+      if (prevKey != null && byteArrayComparator.compare(prevKey, currKey) != 0) {\n+        System.out.println(new String(prevKey) + \":\" + roaringBitmap);\n+        writer.add(prevKey, roaringBitmap);\n+        roaringBitmap.clear();\n+      }\n+\n+      roaringBitmap.add(poll.getRight().getRight());\n+      prevKey = currKey;\n+\n+      //add the next key from the chunk where the currKey was removed from\n+      Iterator<ImmutablePair<byte[], int[]>> iterator = chunkIterators.get(poll.getLeft());\n+      if (iterator.hasNext()) {\n+        queue.offer(ImmutablePair.of(poll.getLeft(), iterator.next()));\n+      }\n+    }\n+\n+    if (prevKey != null) {\n+      writer.add(prevKey, roaringBitmap);\n+    }\n+    writer.finish();\n+    return writer.getMaxDictionaryValueLength();\n+  }\n+\n+  private void flush()\n+      throws IOException {\n+    //write the key (length|actual bytes) - posting list(length, flattenedDocIds)\n+    System.out.println(\"postingListMap = \" + postingListMap);\n+    for (Map.Entry<String, List<Integer>> entry : postingListMap.entrySet()) {\n+      byte[] keyBytes = entry.getKey().getBytes(Charset.forName(\"UTF-8\"));\n+      postingListWriter.writeInt(keyBytes.length);\n+      postingListWriter.write(keyBytes);\n+      List<Integer> flattenedDocIdList = entry.getValue();\n+      postingListWriter.writeInt(flattenedDocIdList.size());\n+      for (int flattenedDocId : flattenedDocIdList) {\n+        postingListWriter.writeInt(flattenedDocId);\n+      }\n+    }\n+\n+    //write flattened doc id to root docId mapping\n+    for (int rootDocId : flattenedDocIdList) {\n+      flattenedDocId2RootDocIdWriter.writeInt(rootDocId);\n+    }\n+    chunkLengths.add(postingListMap.size());\n+    postingListChunkOffsets.add(postingListWriter.size());\n+    postingListMap.clear();\n+    flattenedDocIdList.clear();\n+  }\n+\n+  private static List<Map<String, String>> unnestJson(JsonNode root) {\n+    Iterator<Map.Entry<String, JsonNode>> fields = root.fields();\n+    Map<String, String> flattenedSingleValuesMap = new TreeMap<>();\n+    Map<String, JsonNode> arrNodes = new TreeMap<>();\n+    Map<String, JsonNode> objectNodes = new TreeMap<>();\n+    List<Map<String, String>> resultList = new ArrayList<>();\n+    List<Map<String, String>> tempResultList = new ArrayList<>();\n+    while (fields.hasNext()) {\n+      Map.Entry<String, JsonNode> child = fields.next();\n+      if (child.getValue().isValueNode()) {\n+        //Normal value node\n+        flattenedSingleValuesMap.put(child.getKey(), child.getValue().asText());\n+      } else if (child.getValue().isArray()) {\n+        //Array Node: Process these nodes later\n+        arrNodes.put(child.getKey(), child.getValue());\n+      } else {\n+        //Object Node\n+        objectNodes.put(child.getKey(), child.getValue());\n+      }\n+    }\n+    for (String objectNodeKey : objectNodes.keySet()) {\n+      JsonNode objectNode = objectNodes.get(objectNodeKey);\n+      modifyKeysInMap(flattenedSingleValuesMap, tempResultList, objectNodeKey, objectNode);\n+    }\n+    if (tempResultList.isEmpty()) {\n+      tempResultList.add(flattenedSingleValuesMap);\n+    }\n+    if (!arrNodes.isEmpty()) {\n+      for (Map<String, String> flattenedMapElement : tempResultList) {\n+        for (String arrNodeKey : arrNodes.keySet()) {\n+          JsonNode arrNode = arrNodes.get(arrNodeKey);\n+          for (JsonNode arrNodeElement : arrNode) {\n+            modifyKeysInMap(flattenedMapElement, resultList, arrNodeKey, arrNodeElement);\n+          }\n+        }\n+      }\n+    } else {\n+      resultList.addAll(tempResultList);\n+    }\n+    return resultList;\n+  }\n+\n+  private static void modifyKeysInMap(Map<String, String> flattenedMap, List<Map<String, String>> resultList,\n+      String arrNodeKey, JsonNode arrNode) {\n+    List<Map<String, String>> objectResult = unnestJson(arrNode);\n+    for (Map<String, String> flattenedObject : objectResult) {\n+      Map<String, String> flattenedObjectCopy = new TreeMap<>(flattenedMap);\n+      for (Map.Entry<String, String> entry : flattenedObject.entrySet()) {\n+        flattenedObjectCopy.put(arrNodeKey + \".\" + entry.getKey(), entry.getValue());", "originalCommit": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTI1MzA2MQ==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r541253061", "bodyText": "It does not matter. {\"a.b\":\"c\"} is equivalent to {\"a\":{\"b\":\"c\"}}", "author": "Jackie-Jiang", "createdAt": "2020-12-11T20:26:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1OTQyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1OTc4NA==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515659784", "bodyText": "why do not support number type?", "author": "yupeng9", "createdAt": "2020-11-01T19:12:14Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/index/loader/invertedindex/JSONIndexHandler.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.index.loader.invertedindex;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.core.indexsegment.generator.SegmentVersion;\n+import org.apache.pinot.core.segment.creator.impl.inv.JSONIndexCreator;\n+import org.apache.pinot.core.segment.index.column.PhysicalColumnIndexContainer;\n+import org.apache.pinot.core.segment.index.loader.IndexLoadingConfig;\n+import org.apache.pinot.core.segment.index.loader.LoaderUtils;\n+import org.apache.pinot.core.segment.index.metadata.ColumnMetadata;\n+import org.apache.pinot.core.segment.index.metadata.SegmentMetadataImpl;\n+import org.apache.pinot.core.segment.index.readers.BaseImmutableDictionary;\n+import org.apache.pinot.core.segment.index.readers.Dictionary;\n+import org.apache.pinot.core.segment.index.readers.ForwardIndexReader;\n+import org.apache.pinot.core.segment.index.readers.ForwardIndexReaderContext;\n+import org.apache.pinot.core.segment.index.readers.forward.FixedBitMVForwardIndexReader;\n+import org.apache.pinot.core.segment.index.readers.forward.FixedBitSVForwardIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.core.segment.store.ColumnIndexType;\n+import org.apache.pinot.core.segment.store.SegmentDirectory;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+@SuppressWarnings({\"rawtypes\", \"unchecked\"})\n+public class JSONIndexHandler {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(JSONIndexHandler.class);\n+\n+  private final File _indexDir;\n+  private final SegmentDirectory.Writer _segmentWriter;\n+  private final String _segmentName;\n+  private final SegmentVersion _segmentVersion;\n+  private final Set<ColumnMetadata> _jsonIndexColumns = new HashSet<>();\n+\n+  public JSONIndexHandler(File indexDir, SegmentMetadataImpl segmentMetadata, IndexLoadingConfig indexLoadingConfig,\n+      SegmentDirectory.Writer segmentWriter) {\n+    _indexDir = indexDir;\n+    _segmentWriter = segmentWriter;\n+    _segmentName = segmentMetadata.getName();\n+    _segmentVersion = SegmentVersion.valueOf(segmentMetadata.getVersion());\n+\n+    // Only create json index on dictionary-encoded unsorted columns\n+    for (String column : indexLoadingConfig.getJsonIndexColumns()) {\n+      ColumnMetadata columnMetadata = segmentMetadata.getColumnMetadataFor(column);\n+      if (columnMetadata != null && !columnMetadata.isSorted()) {\n+        _jsonIndexColumns.add(columnMetadata);\n+      }\n+    }\n+  }\n+\n+  public void createJsonIndices()\n+      throws IOException {\n+    for (ColumnMetadata columnMetadata : _jsonIndexColumns) {\n+      createJSONIndexForColumn(columnMetadata);\n+    }\n+  }\n+\n+  private void createJSONIndexForColumn(ColumnMetadata columnMetadata)\n+      throws IOException {\n+    String column = columnMetadata.getColumnName();\n+\n+    File inProgress = new File(_indexDir, column + JSON_INDEX_FILE_EXTENSION + \".inprogress\");\n+    File jsonIndexFile = new File(_indexDir, column + JSON_INDEX_FILE_EXTENSION);\n+\n+    if (!inProgress.exists()) {\n+      // Marker file does not exist, which means last run ended normally.\n+\n+      if (_segmentWriter.hasIndexFor(column, ColumnIndexType.JSON_INDEX)) {\n+        // Skip creating json index if already exists.\n+\n+        LOGGER.info(\"Found json index for segment: {}, column: {}\", _segmentName, column);\n+        return;\n+      }\n+\n+      // Create a marker file.\n+      FileUtils.touch(inProgress);\n+    } else {\n+      // Marker file exists, which means last run gets interrupted.\n+\n+      // Remove json index if exists.\n+      // For v1 and v2, it's the actual json index. For v3, it's the temporary json index.\n+      FileUtils.deleteQuietly(jsonIndexFile);\n+    }\n+\n+    // Create new json index for the column.\n+    LOGGER.info(\"Creating new json index for segment: {}, column: {}\", _segmentName, column);\n+    if (columnMetadata.hasDictionary()) {\n+      handleDictionaryBasedColumn(columnMetadata);\n+    } else {\n+      handleNonDictionaryBasedColumn(columnMetadata);\n+    }\n+\n+    // For v3, write the generated json index file into the single file and remove it.\n+    if (_segmentVersion == SegmentVersion.v3) {\n+      LoaderUtils.writeIndexToV3Format(_segmentWriter, column, jsonIndexFile, ColumnIndexType.JSON_INDEX);\n+    }\n+\n+    // Delete the marker file.\n+    FileUtils.deleteQuietly(inProgress);\n+\n+    LOGGER.info(\"Created json index for segment: {}, column: {}\", _segmentName, column);\n+  }\n+\n+  private void handleDictionaryBasedColumn(ColumnMetadata columnMetadata)\n+      throws IOException {\n+    int numDocs = columnMetadata.getTotalDocs();\n+    try (ForwardIndexReader forwardIndexReader = getForwardIndexReader(columnMetadata, _segmentWriter);\n+        ForwardIndexReaderContext readerContext = forwardIndexReader.createContext();\n+        Dictionary dictionary = getDictionaryReader(columnMetadata, _segmentWriter);\n+        JSONIndexCreator jsonIndexCreator = new JSONIndexCreator(_indexDir, columnMetadata.getFieldSpec())) {\n+      if (columnMetadata.isSingleValue()) {\n+        switch (columnMetadata.getDataType()) {\n+          case STRING:\n+            for (int i = 0; i < numDocs; i++) {\n+              int dictId = forwardIndexReader.getDictId(i, readerContext);\n+              jsonIndexCreator.add(dictionary.getStringValue(dictId).getBytes(\"UTF-8\"));\n+            }\n+            break;\n+          case BYTES:\n+            // Single-value column\n+            for (int i = 0; i < numDocs; i++) {\n+              int dictId = forwardIndexReader.getDictId(i, readerContext);\n+              jsonIndexCreator.add(dictionary.getBytesValue(dictId));\n+            }\n+            break;\n+          default:\n+            throw new IllegalStateException(\"Unsupported data type: \" + columnMetadata.getDataType());\n+        }\n+      } else {\n+        // Multi-value column\n+        throw new IllegalStateException(\"JSON Indexing is not supported on multi-valued columns \");\n+      }\n+      jsonIndexCreator.seal();\n+    }\n+  }\n+\n+  private void handleNonDictionaryBasedColumn(ColumnMetadata columnMetadata)\n+      throws IOException {\n+    FieldSpec.DataType dataType = columnMetadata.getDataType();\n+    if (dataType != FieldSpec.DataType.BYTES || dataType != FieldSpec.DataType.STRING) {\n+      throw new UnsupportedOperationException(\n+          \"JSON indexing is only supported for STRING/BYTES datatype but found: \" + dataType);", "originalCommit": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3Mjk0OA==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515672948", "bodyText": "I think it is the data-type in which JSON content is stored, as opposed to the data type of JSON fields?", "author": "mayankshriv", "createdAt": "2020-11-01T21:15:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1OTc4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTI1NDA2Mg==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r541254062", "bodyText": "Json field must be of STRING type, and the value should be json strings", "author": "Jackie-Jiang", "createdAt": "2020-12-11T20:27:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1OTc4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY1OTk1Ng==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515659956", "bodyText": "can you explain why special handling of one id?", "author": "yupeng9", "createdAt": "2020-11-01T19:14:01Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/index/readers/JSONIndexReader.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.index.readers;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import org.apache.calcite.sql.parser.SqlParseException;\n+import org.apache.pinot.common.utils.StringUtil;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.docidsets.BitmapDocIdSet;\n+import org.apache.pinot.core.operator.filter.BitmapBasedFilterOperator;\n+import org.apache.pinot.core.operator.filter.predicate.PredicateEvaluator;\n+import org.apache.pinot.core.operator.filter.predicate.PredicateEvaluatorProvider;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.buffer.ImmutableRoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+\n+public class JSONIndexReader implements Closeable {\n+\n+  private static int EXPECTED_VERSION = 1;\n+  private static int DICT_HEADER_INDEX = 0;\n+  private static int DICT_OFFSET_INDEX = 1;\n+  private static int DICT_DATA_INDEX = 2;\n+  private static int INV_OFFSET_INDEX = 3;\n+  private static int INV_DATA_INDEX = 4;\n+  private static int FLATTENED_2_ROOT_INDEX = 5;\n+\n+  private final BitmapInvertedIndexReader invertedIndexReader;\n+  private final StringDictionary dictionary;\n+  private final long cardinality;\n+  private final long numFlattenedDocs;\n+  private final PinotDataBuffer flattened2RootDocIdBuffer;\n+\n+  public JSONIndexReader(PinotDataBuffer pinotDataBuffer) {\n+\n+    int version = pinotDataBuffer.getInt(0);\n+    int maxKeyLength = pinotDataBuffer.getInt(1 * Integer.BYTES);\n+\n+    Preconditions.checkState(version == EXPECTED_VERSION, String\n+        .format(\"Index version:{} is not supported by this reader. expected version:{}\", version, EXPECTED_VERSION));\n+\n+    // dictionaryHeaderFile, dictionaryOffsetFile, dictionaryFile, invIndexOffsetFile, invIndexFile, FlattenedDocId2DocIdMappingFile\n+    int numBuffers = 6;\n+    long bufferStartOffsets[] = new long[numBuffers];\n+    long bufferSizeArray[] = new long[numBuffers];\n+    for (int i = 0; i < numBuffers; i++) {\n+      bufferStartOffsets[i] = pinotDataBuffer.getLong(2 * Integer.BYTES + 2 * i * Long.BYTES);\n+      bufferSizeArray[i] = pinotDataBuffer.getLong(2 * Integer.BYTES + 2 * i * Long.BYTES + Long.BYTES);\n+    }\n+    cardinality = bufferSizeArray[DICT_OFFSET_INDEX] / Integer.BYTES - 1;\n+\n+    long dictionaryStartOffset = bufferStartOffsets[DICT_HEADER_INDEX];\n+    long dictionarySize =\n+        bufferSizeArray[DICT_HEADER_INDEX] + bufferSizeArray[DICT_OFFSET_INDEX] + bufferSizeArray[DICT_DATA_INDEX];\n+\n+    //TODO: REMOVE DEBUG START\n+    byte[] dictHeaderBytes = new byte[(int) bufferSizeArray[DICT_HEADER_INDEX]];\n+    pinotDataBuffer.copyTo(bufferStartOffsets[DICT_HEADER_INDEX], dictHeaderBytes);\n+    System.out.println(\"Arrays.toString(dictHeaderBytes) = \" + Arrays.toString(dictHeaderBytes));\n+    //TODO: REMOVE DEBUG  END\n+\n+    PinotDataBuffer dictionaryBuffer =\n+        pinotDataBuffer.view(dictionaryStartOffset, dictionaryStartOffset + dictionarySize);\n+    dictionary = new StringDictionary(dictionaryBuffer, (int) cardinality, maxKeyLength, Byte.valueOf(\"0\"));\n+\n+    long invIndexStartOffset = bufferStartOffsets[INV_OFFSET_INDEX];\n+    long invIndexSize = bufferSizeArray[INV_OFFSET_INDEX] + bufferSizeArray[INV_DATA_INDEX];\n+\n+    PinotDataBuffer invIndexBuffer = pinotDataBuffer.view(invIndexStartOffset, invIndexStartOffset + invIndexSize);\n+    invertedIndexReader = new BitmapInvertedIndexReader(invIndexBuffer, (int) cardinality);\n+\n+    //TODO: REMOVE DEBUG START\n+    for (int dictId = 0; dictId < dictionary.length(); dictId++) {\n+      System.out.println(\"Key = \" + new String(dictionary.getBytes(dictId)));\n+      System.out.println(\"Posting List = \" + invertedIndexReader.getDocIds(dictId));\n+    }\n+    //TODO: REMOVE DEBUG  END\n+\n+    long flattened2RootDocIdStartOffset = bufferStartOffsets[FLATTENED_2_ROOT_INDEX];\n+    long flattened2RootDocIdBufferSize = bufferSizeArray[FLATTENED_2_ROOT_INDEX];\n+    numFlattenedDocs = bufferSizeArray[FLATTENED_2_ROOT_INDEX] / Integer.BYTES;\n+    flattened2RootDocIdBuffer = pinotDataBuffer.view(flattened2RootDocIdStartOffset, flattened2RootDocIdStartOffset + flattened2RootDocIdBufferSize);\n+    //TODO: REMOVE DEBUG START\n+    for (int i = 0; i < numFlattenedDocs; i++) {\n+      System.out.println(\"flattenedDocId: \" + i + \" rootDodId:\" + flattened2RootDocIdBuffer.getInt(i * Integer.BYTES));\n+    }\n+    //TODO: REMOVE DEBUG  END\n+  }\n+\n+  /**\n+   * Returns the matching document ids for the given search query.\n+   */\n+  public MutableRoaringBitmap getMatchingDocIds(Predicate predicate) {\n+\n+    PredicateEvaluator predicateEvaluator =\n+        PredicateEvaluatorProvider.getPredicateEvaluator(predicate, dictionary, FieldSpec.DataType.BYTES);\n+    boolean exclusive = predicateEvaluator.isExclusive();\n+    int[] dictIds = exclusive ? predicateEvaluator.getNonMatchingDictIds() : predicateEvaluator.getMatchingDictIds();\n+    int numDictIds = dictIds.length;\n+\n+    if (numDictIds == 1) {", "originalCommit": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2OTc3MA==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515669770", "bodyText": "Any reason to use a different version of Thrift compiler?", "author": "mayankshriv", "createdAt": "2020-11-01T20:45:02Z", "path": "pinot-common/src/main/java/org/apache/pinot/common/request/AggregationInfo.java", "diffHunk": "@@ -17,7 +17,7 @@\n  * under the License.\n  */\n /**\n- * Autogenerated by Thrift Compiler (0.12.0)\n+ * Autogenerated by Thrift Compiler (0.13.0)", "originalCommit": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTI1NzQxMQ==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r541257411", "bodyText": "Hmm, I think we usually use the latest one if we need to recompile", "author": "Jackie-Jiang", "createdAt": "2020-12-11T20:31:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2OTc3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MDg3Ng==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515670876", "bodyText": "Why do these need to become public?", "author": "mayankshriv", "createdAt": "2020-11-01T20:55:58Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/io/util/VarLengthBytesValueReaderWriter.java", "diffHunk": "@@ -63,19 +66,19 @@\n   /**\n    * Magic bytes used to identify the dictionary files written in variable length bytes format.\n    */\n-  private static final byte[] MAGIC_BYTES = StringUtil.encodeUtf8(\".vl;\");\n+  public static final byte[] MAGIC_BYTES = StringUtil.encodeUtf8(\".vl;\");", "originalCommit": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MDk5MA==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515670990", "bodyText": "Where is this used?", "author": "mayankshriv", "createdAt": "2020-11-01T20:57:06Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/io/util/VarLengthBytesValueReaderWriter.java", "diffHunk": "@@ -144,6 +147,19 @@ public static boolean isVarLengthBytesDictBuffer(PinotDataBuffer buffer) {\n     return false;\n   }\n \n+  public static byte[] getHeaderBytes(int numElements)", "originalCommit": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MTEwMA==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515671100", "bodyText": "+1 for passing InvertedIndexReader instead of DataSource.", "author": "mayankshriv", "createdAt": "2020-11-01T20:58:44Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/operator/filter/BitmapBasedFilterOperator.java", "diffHunk": "@@ -38,9 +38,9 @@\n   private final boolean _exclusive;\n   private final int _numDocs;\n \n-  BitmapBasedFilterOperator(PredicateEvaluator predicateEvaluator, DataSource dataSource, int numDocs) {\n+  public BitmapBasedFilterOperator(PredicateEvaluator predicateEvaluator, InvertedIndexReader invertedIndexReader, int numDocs) {", "originalCommit": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MTIyNQ==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515671225", "bodyText": "Add javadoc?", "author": "mayankshriv", "createdAt": "2020-11-01T20:59:21Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/operator/filter/JSONMatchFilterOperator.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.operator.filter;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.docidsets.BitmapDocIdSet;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.InPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.NotEqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.NotInPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.segment.creator.impl.inv.JSONIndexCreator;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.roaringbitmap.buffer.ImmutableRoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+\n+@SuppressWarnings(\"rawtypes\")\n+public class JSONMatchFilterOperator extends BaseFilterOperator {", "originalCommit": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MTgyMw==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515671823", "bodyText": "private?", "author": "mayankshriv", "createdAt": "2020-11-01T21:05:01Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {\n+\n+  //separator used to join the key and value to create posting list key\n+  public static String POSTING_LIST_KEY_SEPARATOR = \"|\";\n+  static int FLUSH_THRESHOLD = 50_000;", "originalCommit": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MjAyNw==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515672027", "bodyText": "Static finals for _dictionaryOffset.buf, and others?", "author": "mayankshriv", "createdAt": "2020-11-01T21:06:19Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {\n+\n+  //separator used to join the key and value to create posting list key\n+  public static String POSTING_LIST_KEY_SEPARATOR = \"|\";\n+  static int FLUSH_THRESHOLD = 50_000;\n+  static int VERSION = 1;\n+  private final File flattenedDocId2RootDocIdMappingFile;\n+  private final File postingListFile;\n+  private File dictionaryheaderFile;\n+  private File dictionaryOffsetFile;\n+  private File dictionaryFile;\n+  private File invertedIndexOffsetFile;\n+  private File invertedIndexFile;\n+  private File outputIndexFile;\n+\n+  private int docId = 0;\n+  private int numFlatennedDocId = 0;\n+  int chunkId = 0;\n+\n+  private DataOutputStream postingListWriter;\n+  private DataOutputStream flattenedDocId2RootDocIdWriter;\n+\n+  Map<String, List<Integer>> postingListMap = new TreeMap<>();\n+  List<Integer> flattenedDocIdList = new ArrayList<>();\n+  List<Integer> postingListChunkOffsets = new ArrayList<>();\n+  List<Integer> chunkLengths = new ArrayList<>();\n+  private FieldSpec fieldSpec;\n+\n+  public JSONIndexCreator(File indexDir, FieldSpec fieldSpec)\n+      throws IOException {\n+    this.fieldSpec = fieldSpec;\n+    System.out.println(\"indexDir = \" + indexDir);\n+\n+    String name = fieldSpec.getName();\n+    postingListFile = new File(indexDir + name + \"_postingList.buf\");\n+    postingListWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(postingListFile)));\n+    postingListChunkOffsets.add(postingListWriter.size());\n+\n+    dictionaryheaderFile = new File(indexDir, name + \"_dictionaryHeader.buf\");\n+    dictionaryOffsetFile = new File(indexDir, name + \"_dictionaryOffset.buf\");", "originalCommit": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MjE0OQ==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515672149", "bodyText": "Reuse new ObjectMapper() as static final?", "author": "mayankshriv", "createdAt": "2020-11-01T21:07:04Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {\n+\n+  //separator used to join the key and value to create posting list key\n+  public static String POSTING_LIST_KEY_SEPARATOR = \"|\";\n+  static int FLUSH_THRESHOLD = 50_000;\n+  static int VERSION = 1;\n+  private final File flattenedDocId2RootDocIdMappingFile;\n+  private final File postingListFile;\n+  private File dictionaryheaderFile;\n+  private File dictionaryOffsetFile;\n+  private File dictionaryFile;\n+  private File invertedIndexOffsetFile;\n+  private File invertedIndexFile;\n+  private File outputIndexFile;\n+\n+  private int docId = 0;\n+  private int numFlatennedDocId = 0;\n+  int chunkId = 0;\n+\n+  private DataOutputStream postingListWriter;\n+  private DataOutputStream flattenedDocId2RootDocIdWriter;\n+\n+  Map<String, List<Integer>> postingListMap = new TreeMap<>();\n+  List<Integer> flattenedDocIdList = new ArrayList<>();\n+  List<Integer> postingListChunkOffsets = new ArrayList<>();\n+  List<Integer> chunkLengths = new ArrayList<>();\n+  private FieldSpec fieldSpec;\n+\n+  public JSONIndexCreator(File indexDir, FieldSpec fieldSpec)\n+      throws IOException {\n+    this.fieldSpec = fieldSpec;\n+    System.out.println(\"indexDir = \" + indexDir);\n+\n+    String name = fieldSpec.getName();\n+    postingListFile = new File(indexDir + name + \"_postingList.buf\");\n+    postingListWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(postingListFile)));\n+    postingListChunkOffsets.add(postingListWriter.size());\n+\n+    dictionaryheaderFile = new File(indexDir, name + \"_dictionaryHeader.buf\");\n+    dictionaryOffsetFile = new File(indexDir, name + \"_dictionaryOffset.buf\");\n+    dictionaryFile = new File(indexDir, name + \"_dictionary.buf\");\n+    invertedIndexOffsetFile = new File(indexDir, name + \"_invertedIndexOffset.buf\");\n+    invertedIndexFile = new File(indexDir, name + \"_invertedIndex.buf\");\n+    flattenedDocId2RootDocIdMappingFile = new File(indexDir, name + \"_flattenedDocId.buf\");\n+    flattenedDocId2RootDocIdWriter =\n+        new DataOutputStream(new BufferedOutputStream(new FileOutputStream(flattenedDocId2RootDocIdMappingFile)));\n+\n+    //output file\n+    outputIndexFile = new File(indexDir, name + JSON_INDEX_FILE_EXTENSION);\n+  }\n+\n+  public void add(byte[] data)\n+      throws IOException {\n+\n+    JsonNode jsonNode = new ObjectMapper().readTree(data);", "originalCommit": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MjIzMg==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515672232", "bodyText": "Same here, for new ObjectMapper().", "author": "mayankshriv", "createdAt": "2020-11-01T21:08:00Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {\n+\n+  //separator used to join the key and value to create posting list key\n+  public static String POSTING_LIST_KEY_SEPARATOR = \"|\";\n+  static int FLUSH_THRESHOLD = 50_000;\n+  static int VERSION = 1;\n+  private final File flattenedDocId2RootDocIdMappingFile;\n+  private final File postingListFile;\n+  private File dictionaryheaderFile;\n+  private File dictionaryOffsetFile;\n+  private File dictionaryFile;\n+  private File invertedIndexOffsetFile;\n+  private File invertedIndexFile;\n+  private File outputIndexFile;\n+\n+  private int docId = 0;\n+  private int numFlatennedDocId = 0;\n+  int chunkId = 0;\n+\n+  private DataOutputStream postingListWriter;\n+  private DataOutputStream flattenedDocId2RootDocIdWriter;\n+\n+  Map<String, List<Integer>> postingListMap = new TreeMap<>();\n+  List<Integer> flattenedDocIdList = new ArrayList<>();\n+  List<Integer> postingListChunkOffsets = new ArrayList<>();\n+  List<Integer> chunkLengths = new ArrayList<>();\n+  private FieldSpec fieldSpec;\n+\n+  public JSONIndexCreator(File indexDir, FieldSpec fieldSpec)\n+      throws IOException {\n+    this.fieldSpec = fieldSpec;\n+    System.out.println(\"indexDir = \" + indexDir);\n+\n+    String name = fieldSpec.getName();\n+    postingListFile = new File(indexDir + name + \"_postingList.buf\");\n+    postingListWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(postingListFile)));\n+    postingListChunkOffsets.add(postingListWriter.size());\n+\n+    dictionaryheaderFile = new File(indexDir, name + \"_dictionaryHeader.buf\");\n+    dictionaryOffsetFile = new File(indexDir, name + \"_dictionaryOffset.buf\");\n+    dictionaryFile = new File(indexDir, name + \"_dictionary.buf\");\n+    invertedIndexOffsetFile = new File(indexDir, name + \"_invertedIndexOffset.buf\");\n+    invertedIndexFile = new File(indexDir, name + \"_invertedIndex.buf\");\n+    flattenedDocId2RootDocIdMappingFile = new File(indexDir, name + \"_flattenedDocId.buf\");\n+    flattenedDocId2RootDocIdWriter =\n+        new DataOutputStream(new BufferedOutputStream(new FileOutputStream(flattenedDocId2RootDocIdMappingFile)));\n+\n+    //output file\n+    outputIndexFile = new File(indexDir, name + JSON_INDEX_FILE_EXTENSION);\n+  }\n+\n+  public void add(byte[] data)\n+      throws IOException {\n+\n+    JsonNode jsonNode = new ObjectMapper().readTree(data);\n+    List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+    for (Map<String, String> map : flattenedMapList) {\n+      //\n+      for (Map.Entry<String, String> entry : map.entrySet()) {\n+        //handle key posting list\n+        String key = entry.getKey();\n+\n+        List<Integer> keyPostingList = postingListMap.get(key);\n+        if (keyPostingList == null) {\n+          keyPostingList = new ArrayList<>();\n+          postingListMap.put(key, keyPostingList);\n+        }\n+        keyPostingList.add(numFlatennedDocId);\n+\n+        //handle keyvalue posting list\n+        String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+        List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+        if (keyValuePostingList == null) {\n+          keyValuePostingList = new ArrayList<>();\n+          postingListMap.put(keyValue, keyValuePostingList);\n+        }\n+        keyValuePostingList.add(numFlatennedDocId);\n+      }\n+      //flattenedDocId2RootDocIdMapping\n+      flattenedDocIdList.add(docId);\n+\n+      numFlatennedDocId++;\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  /**\n+   * Multi value\n+   * @param dataArray\n+   * @param length\n+   * @throws IOException\n+   */\n+  public void add(byte[][] dataArray, int length)\n+      throws IOException {\n+\n+    for (int i = 0; i < length; i++) {\n+      byte[] data = dataArray[i];\n+      JsonNode jsonNode = new ObjectMapper().readTree(data);", "originalCommit": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MjM5MQ==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515672391", "bodyText": "Are these debugging messages, or expected to be part of code?", "author": "mayankshriv", "createdAt": "2020-11-01T21:09:25Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {\n+\n+  //separator used to join the key and value to create posting list key\n+  public static String POSTING_LIST_KEY_SEPARATOR = \"|\";\n+  static int FLUSH_THRESHOLD = 50_000;\n+  static int VERSION = 1;\n+  private final File flattenedDocId2RootDocIdMappingFile;\n+  private final File postingListFile;\n+  private File dictionaryheaderFile;\n+  private File dictionaryOffsetFile;\n+  private File dictionaryFile;\n+  private File invertedIndexOffsetFile;\n+  private File invertedIndexFile;\n+  private File outputIndexFile;\n+\n+  private int docId = 0;\n+  private int numFlatennedDocId = 0;\n+  int chunkId = 0;\n+\n+  private DataOutputStream postingListWriter;\n+  private DataOutputStream flattenedDocId2RootDocIdWriter;\n+\n+  Map<String, List<Integer>> postingListMap = new TreeMap<>();\n+  List<Integer> flattenedDocIdList = new ArrayList<>();\n+  List<Integer> postingListChunkOffsets = new ArrayList<>();\n+  List<Integer> chunkLengths = new ArrayList<>();\n+  private FieldSpec fieldSpec;\n+\n+  public JSONIndexCreator(File indexDir, FieldSpec fieldSpec)\n+      throws IOException {\n+    this.fieldSpec = fieldSpec;\n+    System.out.println(\"indexDir = \" + indexDir);\n+\n+    String name = fieldSpec.getName();\n+    postingListFile = new File(indexDir + name + \"_postingList.buf\");\n+    postingListWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(postingListFile)));\n+    postingListChunkOffsets.add(postingListWriter.size());\n+\n+    dictionaryheaderFile = new File(indexDir, name + \"_dictionaryHeader.buf\");\n+    dictionaryOffsetFile = new File(indexDir, name + \"_dictionaryOffset.buf\");\n+    dictionaryFile = new File(indexDir, name + \"_dictionary.buf\");\n+    invertedIndexOffsetFile = new File(indexDir, name + \"_invertedIndexOffset.buf\");\n+    invertedIndexFile = new File(indexDir, name + \"_invertedIndex.buf\");\n+    flattenedDocId2RootDocIdMappingFile = new File(indexDir, name + \"_flattenedDocId.buf\");\n+    flattenedDocId2RootDocIdWriter =\n+        new DataOutputStream(new BufferedOutputStream(new FileOutputStream(flattenedDocId2RootDocIdMappingFile)));\n+\n+    //output file\n+    outputIndexFile = new File(indexDir, name + JSON_INDEX_FILE_EXTENSION);\n+  }\n+\n+  public void add(byte[] data)\n+      throws IOException {\n+\n+    JsonNode jsonNode = new ObjectMapper().readTree(data);\n+    List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+    for (Map<String, String> map : flattenedMapList) {\n+      //\n+      for (Map.Entry<String, String> entry : map.entrySet()) {\n+        //handle key posting list\n+        String key = entry.getKey();\n+\n+        List<Integer> keyPostingList = postingListMap.get(key);\n+        if (keyPostingList == null) {\n+          keyPostingList = new ArrayList<>();\n+          postingListMap.put(key, keyPostingList);\n+        }\n+        keyPostingList.add(numFlatennedDocId);\n+\n+        //handle keyvalue posting list\n+        String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+        List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+        if (keyValuePostingList == null) {\n+          keyValuePostingList = new ArrayList<>();\n+          postingListMap.put(keyValue, keyValuePostingList);\n+        }\n+        keyValuePostingList.add(numFlatennedDocId);\n+      }\n+      //flattenedDocId2RootDocIdMapping\n+      flattenedDocIdList.add(docId);\n+\n+      numFlatennedDocId++;\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  /**\n+   * Multi value\n+   * @param dataArray\n+   * @param length\n+   * @throws IOException\n+   */\n+  public void add(byte[][] dataArray, int length)\n+      throws IOException {\n+\n+    for (int i = 0; i < length; i++) {\n+      byte[] data = dataArray[i];\n+      JsonNode jsonNode = new ObjectMapper().readTree(data);\n+      List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+      for (Map<String, String> map : flattenedMapList) {\n+        //\n+        for (Map.Entry<String, String> entry : map.entrySet()) {\n+          //handle key posting list\n+          String key = entry.getKey();\n+\n+          List<Integer> keyPostingList = postingListMap.get(key);\n+          if (keyPostingList == null) {\n+            keyPostingList = new ArrayList<>();\n+            postingListMap.put(key, keyPostingList);\n+          }\n+          keyPostingList.add(numFlatennedDocId);\n+\n+          //handle keyvalue posting list\n+          String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+          List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+          if (keyValuePostingList == null) {\n+            keyValuePostingList = new ArrayList<>();\n+            postingListMap.put(keyValue, keyValuePostingList);\n+          }\n+          keyValuePostingList.add(numFlatennedDocId);\n+        }\n+        //flattenedDocId2RootDocIdMapping\n+        flattenedDocIdList.add(numFlatennedDocId);\n+\n+        numFlatennedDocId++;\n+      }\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  public void seal()\n+      throws IOException {\n+\n+    flush();\n+\n+    flattenedDocId2RootDocIdWriter.close();\n+    postingListWriter.close();\n+\n+    //key posting list merging\n+    System.out.println(\"InvertedIndex\");\n+    System.out.println(\"=================\");\n+\n+    int maxKeyLength = createInvertedIndex(postingListFile, postingListChunkOffsets, chunkLengths);\n+    System.out.println(\"=================\");\n+\n+    int flattenedDocid = 0;\n+    DataInputStream flattenedDocId2RootDocIdReader =\n+        new DataInputStream(new BufferedInputStream(new FileInputStream(flattenedDocId2RootDocIdMappingFile)));\n+    int[] rootDocIdArray = new int[numFlatennedDocId];\n+    while (flattenedDocid < numFlatennedDocId) {\n+      rootDocIdArray[flattenedDocid++] = flattenedDocId2RootDocIdReader.readInt();\n+    }\n+    System.out.println(\"FlattenedDocId  to RootDocId Mapping = \");", "originalCommit": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MjYxMA==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515672610", "bodyText": "Clean up debug messages?", "author": "mayankshriv", "createdAt": "2020-11-01T21:11:38Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {\n+\n+  //separator used to join the key and value to create posting list key\n+  public static String POSTING_LIST_KEY_SEPARATOR = \"|\";\n+  static int FLUSH_THRESHOLD = 50_000;\n+  static int VERSION = 1;\n+  private final File flattenedDocId2RootDocIdMappingFile;\n+  private final File postingListFile;\n+  private File dictionaryheaderFile;\n+  private File dictionaryOffsetFile;\n+  private File dictionaryFile;\n+  private File invertedIndexOffsetFile;\n+  private File invertedIndexFile;\n+  private File outputIndexFile;\n+\n+  private int docId = 0;\n+  private int numFlatennedDocId = 0;\n+  int chunkId = 0;\n+\n+  private DataOutputStream postingListWriter;\n+  private DataOutputStream flattenedDocId2RootDocIdWriter;\n+\n+  Map<String, List<Integer>> postingListMap = new TreeMap<>();\n+  List<Integer> flattenedDocIdList = new ArrayList<>();\n+  List<Integer> postingListChunkOffsets = new ArrayList<>();\n+  List<Integer> chunkLengths = new ArrayList<>();\n+  private FieldSpec fieldSpec;\n+\n+  public JSONIndexCreator(File indexDir, FieldSpec fieldSpec)\n+      throws IOException {\n+    this.fieldSpec = fieldSpec;\n+    System.out.println(\"indexDir = \" + indexDir);\n+\n+    String name = fieldSpec.getName();\n+    postingListFile = new File(indexDir + name + \"_postingList.buf\");\n+    postingListWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(postingListFile)));\n+    postingListChunkOffsets.add(postingListWriter.size());\n+\n+    dictionaryheaderFile = new File(indexDir, name + \"_dictionaryHeader.buf\");\n+    dictionaryOffsetFile = new File(indexDir, name + \"_dictionaryOffset.buf\");\n+    dictionaryFile = new File(indexDir, name + \"_dictionary.buf\");\n+    invertedIndexOffsetFile = new File(indexDir, name + \"_invertedIndexOffset.buf\");\n+    invertedIndexFile = new File(indexDir, name + \"_invertedIndex.buf\");\n+    flattenedDocId2RootDocIdMappingFile = new File(indexDir, name + \"_flattenedDocId.buf\");\n+    flattenedDocId2RootDocIdWriter =\n+        new DataOutputStream(new BufferedOutputStream(new FileOutputStream(flattenedDocId2RootDocIdMappingFile)));\n+\n+    //output file\n+    outputIndexFile = new File(indexDir, name + JSON_INDEX_FILE_EXTENSION);\n+  }\n+\n+  public void add(byte[] data)\n+      throws IOException {\n+\n+    JsonNode jsonNode = new ObjectMapper().readTree(data);\n+    List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+    for (Map<String, String> map : flattenedMapList) {\n+      //\n+      for (Map.Entry<String, String> entry : map.entrySet()) {\n+        //handle key posting list\n+        String key = entry.getKey();\n+\n+        List<Integer> keyPostingList = postingListMap.get(key);\n+        if (keyPostingList == null) {\n+          keyPostingList = new ArrayList<>();\n+          postingListMap.put(key, keyPostingList);\n+        }\n+        keyPostingList.add(numFlatennedDocId);\n+\n+        //handle keyvalue posting list\n+        String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+        List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+        if (keyValuePostingList == null) {\n+          keyValuePostingList = new ArrayList<>();\n+          postingListMap.put(keyValue, keyValuePostingList);\n+        }\n+        keyValuePostingList.add(numFlatennedDocId);\n+      }\n+      //flattenedDocId2RootDocIdMapping\n+      flattenedDocIdList.add(docId);\n+\n+      numFlatennedDocId++;\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  /**\n+   * Multi value\n+   * @param dataArray\n+   * @param length\n+   * @throws IOException\n+   */\n+  public void add(byte[][] dataArray, int length)\n+      throws IOException {\n+\n+    for (int i = 0; i < length; i++) {\n+      byte[] data = dataArray[i];\n+      JsonNode jsonNode = new ObjectMapper().readTree(data);\n+      List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+      for (Map<String, String> map : flattenedMapList) {\n+        //\n+        for (Map.Entry<String, String> entry : map.entrySet()) {\n+          //handle key posting list\n+          String key = entry.getKey();\n+\n+          List<Integer> keyPostingList = postingListMap.get(key);\n+          if (keyPostingList == null) {\n+            keyPostingList = new ArrayList<>();\n+            postingListMap.put(key, keyPostingList);\n+          }\n+          keyPostingList.add(numFlatennedDocId);\n+\n+          //handle keyvalue posting list\n+          String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+          List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+          if (keyValuePostingList == null) {\n+            keyValuePostingList = new ArrayList<>();\n+            postingListMap.put(keyValue, keyValuePostingList);\n+          }\n+          keyValuePostingList.add(numFlatennedDocId);\n+        }\n+        //flattenedDocId2RootDocIdMapping\n+        flattenedDocIdList.add(numFlatennedDocId);\n+\n+        numFlatennedDocId++;\n+      }\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  public void seal()\n+      throws IOException {\n+\n+    flush();\n+\n+    flattenedDocId2RootDocIdWriter.close();\n+    postingListWriter.close();\n+\n+    //key posting list merging\n+    System.out.println(\"InvertedIndex\");\n+    System.out.println(\"=================\");\n+\n+    int maxKeyLength = createInvertedIndex(postingListFile, postingListChunkOffsets, chunkLengths);\n+    System.out.println(\"=================\");\n+\n+    int flattenedDocid = 0;\n+    DataInputStream flattenedDocId2RootDocIdReader =\n+        new DataInputStream(new BufferedInputStream(new FileInputStream(flattenedDocId2RootDocIdMappingFile)));\n+    int[] rootDocIdArray = new int[numFlatennedDocId];\n+    while (flattenedDocid < numFlatennedDocId) {\n+      rootDocIdArray[flattenedDocid++] = flattenedDocId2RootDocIdReader.readInt();\n+    }\n+    System.out.println(\"FlattenedDocId  to RootDocId Mapping = \");\n+    System.out.println(Arrays.toString(rootDocIdArray));\n+\n+    //PUT all contents into one file\n+\n+    //header\n+    // version + maxDictionaryLength + [store the offsets + length for each one (dictionary offset file, dictionaryFile, index offset file, index file, flattened docId to rootDocId file)]\n+    long headerSize = 2 * Integer.BYTES + 6 * 2 * Long.BYTES;\n+\n+    long dataSize =\n+        dictionaryheaderFile.length() + dictionaryOffsetFile.length() + dictionaryFile.length() + invertedIndexFile\n+            .length() + invertedIndexOffsetFile.length() + flattenedDocId2RootDocIdMappingFile.length();\n+\n+    long totalSize = headerSize + dataSize;\n+    PinotDataBuffer pinotDataBuffer =\n+        PinotDataBuffer.mapFile(outputIndexFile, false, 0, totalSize, ByteOrder.BIG_ENDIAN, \"Nested inverted index\");\n+\n+    pinotDataBuffer.putInt(0, VERSION);\n+    pinotDataBuffer.putInt(1 * Integer.BYTES, maxKeyLength);\n+    long writtenBytes = headerSize;\n+\n+    //add dictionary header\n+    int bufferId = 0;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryheaderFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryheaderFile, 0, dictionaryheaderFile.length());\n+    writtenBytes += dictionaryheaderFile.length();\n+\n+    //add dictionary offset\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryOffsetFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryOffsetFile, 0, dictionaryOffsetFile.length());\n+    writtenBytes += dictionaryOffsetFile.length();\n+\n+    //add dictionary\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryFile, 0, dictionaryFile.length());\n+    writtenBytes += dictionaryFile.length();\n+\n+    //add index offset\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, invertedIndexOffsetFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, invertedIndexOffsetFile, 0, invertedIndexOffsetFile.length());\n+    writtenBytes += invertedIndexOffsetFile.length();\n+\n+    //add index data\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, invertedIndexFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, invertedIndexFile, 0, invertedIndexFile.length());\n+    writtenBytes += invertedIndexFile.length();\n+\n+    //add flattened docid to root doc id mapping\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, flattenedDocId2RootDocIdMappingFile.length());\n+    pinotDataBuffer\n+        .readFrom(writtenBytes, flattenedDocId2RootDocIdMappingFile, 0, flattenedDocId2RootDocIdMappingFile.length());\n+    writtenBytes += flattenedDocId2RootDocIdMappingFile.length();\n+  }\n+\n+  private long getBufferStartOffset(int bufferId) {\n+    return 2 * Integer.BYTES + 2 * bufferId * Long.BYTES;\n+  }\n+\n+  private int createInvertedIndex(File postingListFile, List<Integer> postingListChunkOffsets,\n+      List<Integer> chunkLengthList)\n+      throws IOException {\n+\n+    List<Iterator<ImmutablePair<byte[], int[]>>> chunkIterators = new ArrayList<>();\n+\n+    for (int i = 0; i < chunkLengthList.size(); i++) {\n+\n+      final DataInputStream postingListFileReader =\n+          new DataInputStream(new BufferedInputStream(new FileInputStream(postingListFile)));\n+      postingListFileReader.skipBytes(postingListChunkOffsets.get(i));\n+      final int length = chunkLengthList.get(i);\n+      chunkIterators.add(new Iterator<ImmutablePair<byte[], int[]>>() {\n+        int index = 0;\n+\n+        @Override\n+        public boolean hasNext() {\n+          return index < length;\n+        }\n+\n+        @Override\n+        public ImmutablePair<byte[], int[]> next() {\n+          try {\n+            int keyLength = postingListFileReader.readInt();\n+            byte[] keyBytes = new byte[keyLength];\n+            postingListFileReader.read(keyBytes);\n+\n+            int postingListLength = postingListFileReader.readInt();\n+            int[] postingList = new int[postingListLength];\n+            for (int i = 0; i < postingListLength; i++) {\n+              postingList[i] = postingListFileReader.readInt();\n+            }\n+            index++;\n+            return ImmutablePair.of(keyBytes, postingList);\n+          } catch (Exception e) {\n+            throw new RuntimeException(e);\n+          }\n+        }\n+      });\n+    }\n+    final Comparator<byte[]> byteArrayComparator = UnsignedBytes.lexicographicalComparator();\n+\n+    PriorityQueue<ImmutablePair<Integer, ImmutablePair<byte[], int[]>>> queue =\n+        new PriorityQueue<>(chunkLengthList.size(),\n+            (o1, o2) -> byteArrayComparator.compare(o1.getRight().getLeft(), o2.getRight().getLeft()));\n+    for (int i = 0; i < chunkIterators.size(); i++) {\n+      Iterator<ImmutablePair<byte[], int[]>> iterator = chunkIterators.get(i);\n+      if (iterator.hasNext()) {\n+        queue.offer(ImmutablePair.of(i, iterator.next()));\n+      }\n+    }\n+    byte[] prevKey = null;\n+    RoaringBitmap roaringBitmap = new RoaringBitmap();\n+\n+    Writer writer = new Writer(dictionaryheaderFile, dictionaryOffsetFile, dictionaryFile, invertedIndexOffsetFile,\n+        invertedIndexFile);\n+    while (!queue.isEmpty()) {\n+      ImmutablePair<Integer, ImmutablePair<byte[], int[]>> poll = queue.poll();\n+      byte[] currKey = poll.getRight().getLeft();\n+      if (prevKey != null && byteArrayComparator.compare(prevKey, currKey) != 0) {\n+        System.out.println(new String(prevKey) + \":\" + roaringBitmap);\n+        writer.add(prevKey, roaringBitmap);\n+        roaringBitmap.clear();\n+      }\n+\n+      roaringBitmap.add(poll.getRight().getRight());\n+      prevKey = currKey;\n+\n+      //add the next key from the chunk where the currKey was removed from\n+      Iterator<ImmutablePair<byte[], int[]>> iterator = chunkIterators.get(poll.getLeft());\n+      if (iterator.hasNext()) {\n+        queue.offer(ImmutablePair.of(poll.getLeft(), iterator.next()));\n+      }\n+    }\n+\n+    if (prevKey != null) {\n+      writer.add(prevKey, roaringBitmap);\n+    }\n+    writer.finish();\n+    return writer.getMaxDictionaryValueLength();\n+  }\n+\n+  private void flush()\n+      throws IOException {\n+    //write the key (length|actual bytes) - posting list(length, flattenedDocIds)\n+    System.out.println(\"postingListMap = \" + postingListMap);", "originalCommit": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MjcyNQ==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515672725", "bodyText": "Same here, clean up debug messages.", "author": "mayankshriv", "createdAt": "2020-11-01T21:12:59Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {\n+\n+  //separator used to join the key and value to create posting list key\n+  public static String POSTING_LIST_KEY_SEPARATOR = \"|\";\n+  static int FLUSH_THRESHOLD = 50_000;\n+  static int VERSION = 1;\n+  private final File flattenedDocId2RootDocIdMappingFile;\n+  private final File postingListFile;\n+  private File dictionaryheaderFile;\n+  private File dictionaryOffsetFile;\n+  private File dictionaryFile;\n+  private File invertedIndexOffsetFile;\n+  private File invertedIndexFile;\n+  private File outputIndexFile;\n+\n+  private int docId = 0;\n+  private int numFlatennedDocId = 0;\n+  int chunkId = 0;\n+\n+  private DataOutputStream postingListWriter;\n+  private DataOutputStream flattenedDocId2RootDocIdWriter;\n+\n+  Map<String, List<Integer>> postingListMap = new TreeMap<>();\n+  List<Integer> flattenedDocIdList = new ArrayList<>();\n+  List<Integer> postingListChunkOffsets = new ArrayList<>();\n+  List<Integer> chunkLengths = new ArrayList<>();\n+  private FieldSpec fieldSpec;\n+\n+  public JSONIndexCreator(File indexDir, FieldSpec fieldSpec)\n+      throws IOException {\n+    this.fieldSpec = fieldSpec;\n+    System.out.println(\"indexDir = \" + indexDir);\n+\n+    String name = fieldSpec.getName();\n+    postingListFile = new File(indexDir + name + \"_postingList.buf\");\n+    postingListWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(postingListFile)));\n+    postingListChunkOffsets.add(postingListWriter.size());\n+\n+    dictionaryheaderFile = new File(indexDir, name + \"_dictionaryHeader.buf\");\n+    dictionaryOffsetFile = new File(indexDir, name + \"_dictionaryOffset.buf\");\n+    dictionaryFile = new File(indexDir, name + \"_dictionary.buf\");\n+    invertedIndexOffsetFile = new File(indexDir, name + \"_invertedIndexOffset.buf\");\n+    invertedIndexFile = new File(indexDir, name + \"_invertedIndex.buf\");\n+    flattenedDocId2RootDocIdMappingFile = new File(indexDir, name + \"_flattenedDocId.buf\");\n+    flattenedDocId2RootDocIdWriter =\n+        new DataOutputStream(new BufferedOutputStream(new FileOutputStream(flattenedDocId2RootDocIdMappingFile)));\n+\n+    //output file\n+    outputIndexFile = new File(indexDir, name + JSON_INDEX_FILE_EXTENSION);\n+  }\n+\n+  public void add(byte[] data)\n+      throws IOException {\n+\n+    JsonNode jsonNode = new ObjectMapper().readTree(data);\n+    List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+    for (Map<String, String> map : flattenedMapList) {\n+      //\n+      for (Map.Entry<String, String> entry : map.entrySet()) {\n+        //handle key posting list\n+        String key = entry.getKey();\n+\n+        List<Integer> keyPostingList = postingListMap.get(key);\n+        if (keyPostingList == null) {\n+          keyPostingList = new ArrayList<>();\n+          postingListMap.put(key, keyPostingList);\n+        }\n+        keyPostingList.add(numFlatennedDocId);\n+\n+        //handle keyvalue posting list\n+        String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+        List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+        if (keyValuePostingList == null) {\n+          keyValuePostingList = new ArrayList<>();\n+          postingListMap.put(keyValue, keyValuePostingList);\n+        }\n+        keyValuePostingList.add(numFlatennedDocId);\n+      }\n+      //flattenedDocId2RootDocIdMapping\n+      flattenedDocIdList.add(docId);\n+\n+      numFlatennedDocId++;\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  /**\n+   * Multi value\n+   * @param dataArray\n+   * @param length\n+   * @throws IOException\n+   */\n+  public void add(byte[][] dataArray, int length)\n+      throws IOException {\n+\n+    for (int i = 0; i < length; i++) {\n+      byte[] data = dataArray[i];\n+      JsonNode jsonNode = new ObjectMapper().readTree(data);\n+      List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+      for (Map<String, String> map : flattenedMapList) {\n+        //\n+        for (Map.Entry<String, String> entry : map.entrySet()) {\n+          //handle key posting list\n+          String key = entry.getKey();\n+\n+          List<Integer> keyPostingList = postingListMap.get(key);\n+          if (keyPostingList == null) {\n+            keyPostingList = new ArrayList<>();\n+            postingListMap.put(key, keyPostingList);\n+          }\n+          keyPostingList.add(numFlatennedDocId);\n+\n+          //handle keyvalue posting list\n+          String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+          List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+          if (keyValuePostingList == null) {\n+            keyValuePostingList = new ArrayList<>();\n+            postingListMap.put(keyValue, keyValuePostingList);\n+          }\n+          keyValuePostingList.add(numFlatennedDocId);\n+        }\n+        //flattenedDocId2RootDocIdMapping\n+        flattenedDocIdList.add(numFlatennedDocId);\n+\n+        numFlatennedDocId++;\n+      }\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  public void seal()\n+      throws IOException {\n+\n+    flush();\n+\n+    flattenedDocId2RootDocIdWriter.close();\n+    postingListWriter.close();\n+\n+    //key posting list merging\n+    System.out.println(\"InvertedIndex\");\n+    System.out.println(\"=================\");\n+\n+    int maxKeyLength = createInvertedIndex(postingListFile, postingListChunkOffsets, chunkLengths);\n+    System.out.println(\"=================\");\n+\n+    int flattenedDocid = 0;\n+    DataInputStream flattenedDocId2RootDocIdReader =\n+        new DataInputStream(new BufferedInputStream(new FileInputStream(flattenedDocId2RootDocIdMappingFile)));\n+    int[] rootDocIdArray = new int[numFlatennedDocId];\n+    while (flattenedDocid < numFlatennedDocId) {\n+      rootDocIdArray[flattenedDocid++] = flattenedDocId2RootDocIdReader.readInt();\n+    }\n+    System.out.println(\"FlattenedDocId  to RootDocId Mapping = \");\n+    System.out.println(Arrays.toString(rootDocIdArray));\n+\n+    //PUT all contents into one file\n+\n+    //header\n+    // version + maxDictionaryLength + [store the offsets + length for each one (dictionary offset file, dictionaryFile, index offset file, index file, flattened docId to rootDocId file)]\n+    long headerSize = 2 * Integer.BYTES + 6 * 2 * Long.BYTES;\n+\n+    long dataSize =\n+        dictionaryheaderFile.length() + dictionaryOffsetFile.length() + dictionaryFile.length() + invertedIndexFile\n+            .length() + invertedIndexOffsetFile.length() + flattenedDocId2RootDocIdMappingFile.length();\n+\n+    long totalSize = headerSize + dataSize;\n+    PinotDataBuffer pinotDataBuffer =\n+        PinotDataBuffer.mapFile(outputIndexFile, false, 0, totalSize, ByteOrder.BIG_ENDIAN, \"Nested inverted index\");\n+\n+    pinotDataBuffer.putInt(0, VERSION);\n+    pinotDataBuffer.putInt(1 * Integer.BYTES, maxKeyLength);\n+    long writtenBytes = headerSize;\n+\n+    //add dictionary header\n+    int bufferId = 0;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryheaderFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryheaderFile, 0, dictionaryheaderFile.length());\n+    writtenBytes += dictionaryheaderFile.length();\n+\n+    //add dictionary offset\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryOffsetFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryOffsetFile, 0, dictionaryOffsetFile.length());\n+    writtenBytes += dictionaryOffsetFile.length();\n+\n+    //add dictionary\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryFile, 0, dictionaryFile.length());\n+    writtenBytes += dictionaryFile.length();\n+\n+    //add index offset\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, invertedIndexOffsetFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, invertedIndexOffsetFile, 0, invertedIndexOffsetFile.length());\n+    writtenBytes += invertedIndexOffsetFile.length();\n+\n+    //add index data\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, invertedIndexFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, invertedIndexFile, 0, invertedIndexFile.length());\n+    writtenBytes += invertedIndexFile.length();\n+\n+    //add flattened docid to root doc id mapping\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, flattenedDocId2RootDocIdMappingFile.length());\n+    pinotDataBuffer\n+        .readFrom(writtenBytes, flattenedDocId2RootDocIdMappingFile, 0, flattenedDocId2RootDocIdMappingFile.length());\n+    writtenBytes += flattenedDocId2RootDocIdMappingFile.length();\n+  }\n+\n+  private long getBufferStartOffset(int bufferId) {\n+    return 2 * Integer.BYTES + 2 * bufferId * Long.BYTES;\n+  }\n+\n+  private int createInvertedIndex(File postingListFile, List<Integer> postingListChunkOffsets,\n+      List<Integer> chunkLengthList)\n+      throws IOException {\n+\n+    List<Iterator<ImmutablePair<byte[], int[]>>> chunkIterators = new ArrayList<>();\n+\n+    for (int i = 0; i < chunkLengthList.size(); i++) {\n+\n+      final DataInputStream postingListFileReader =\n+          new DataInputStream(new BufferedInputStream(new FileInputStream(postingListFile)));\n+      postingListFileReader.skipBytes(postingListChunkOffsets.get(i));\n+      final int length = chunkLengthList.get(i);\n+      chunkIterators.add(new Iterator<ImmutablePair<byte[], int[]>>() {\n+        int index = 0;\n+\n+        @Override\n+        public boolean hasNext() {\n+          return index < length;\n+        }\n+\n+        @Override\n+        public ImmutablePair<byte[], int[]> next() {\n+          try {\n+            int keyLength = postingListFileReader.readInt();\n+            byte[] keyBytes = new byte[keyLength];\n+            postingListFileReader.read(keyBytes);\n+\n+            int postingListLength = postingListFileReader.readInt();\n+            int[] postingList = new int[postingListLength];\n+            for (int i = 0; i < postingListLength; i++) {\n+              postingList[i] = postingListFileReader.readInt();\n+            }\n+            index++;\n+            return ImmutablePair.of(keyBytes, postingList);\n+          } catch (Exception e) {\n+            throw new RuntimeException(e);\n+          }\n+        }\n+      });\n+    }\n+    final Comparator<byte[]> byteArrayComparator = UnsignedBytes.lexicographicalComparator();\n+\n+    PriorityQueue<ImmutablePair<Integer, ImmutablePair<byte[], int[]>>> queue =\n+        new PriorityQueue<>(chunkLengthList.size(),\n+            (o1, o2) -> byteArrayComparator.compare(o1.getRight().getLeft(), o2.getRight().getLeft()));\n+    for (int i = 0; i < chunkIterators.size(); i++) {\n+      Iterator<ImmutablePair<byte[], int[]>> iterator = chunkIterators.get(i);\n+      if (iterator.hasNext()) {\n+        queue.offer(ImmutablePair.of(i, iterator.next()));\n+      }\n+    }\n+    byte[] prevKey = null;\n+    RoaringBitmap roaringBitmap = new RoaringBitmap();\n+\n+    Writer writer = new Writer(dictionaryheaderFile, dictionaryOffsetFile, dictionaryFile, invertedIndexOffsetFile,\n+        invertedIndexFile);\n+    while (!queue.isEmpty()) {\n+      ImmutablePair<Integer, ImmutablePair<byte[], int[]>> poll = queue.poll();\n+      byte[] currKey = poll.getRight().getLeft();\n+      if (prevKey != null && byteArrayComparator.compare(prevKey, currKey) != 0) {\n+        System.out.println(new String(prevKey) + \":\" + roaringBitmap);\n+        writer.add(prevKey, roaringBitmap);\n+        roaringBitmap.clear();\n+      }\n+\n+      roaringBitmap.add(poll.getRight().getRight());\n+      prevKey = currKey;\n+\n+      //add the next key from the chunk where the currKey was removed from\n+      Iterator<ImmutablePair<byte[], int[]>> iterator = chunkIterators.get(poll.getLeft());\n+      if (iterator.hasNext()) {\n+        queue.offer(ImmutablePair.of(poll.getLeft(), iterator.next()));\n+      }\n+    }\n+\n+    if (prevKey != null) {\n+      writer.add(prevKey, roaringBitmap);\n+    }\n+    writer.finish();\n+    return writer.getMaxDictionaryValueLength();\n+  }\n+\n+  private void flush()\n+      throws IOException {\n+    //write the key (length|actual bytes) - posting list(length, flattenedDocIds)\n+    System.out.println(\"postingListMap = \" + postingListMap);\n+    for (Map.Entry<String, List<Integer>> entry : postingListMap.entrySet()) {\n+      byte[] keyBytes = entry.getKey().getBytes(Charset.forName(\"UTF-8\"));\n+      postingListWriter.writeInt(keyBytes.length);\n+      postingListWriter.write(keyBytes);\n+      List<Integer> flattenedDocIdList = entry.getValue();\n+      postingListWriter.writeInt(flattenedDocIdList.size());\n+      for (int flattenedDocId : flattenedDocIdList) {\n+        postingListWriter.writeInt(flattenedDocId);\n+      }\n+    }\n+\n+    //write flattened doc id to root docId mapping\n+    for (int rootDocId : flattenedDocIdList) {\n+      flattenedDocId2RootDocIdWriter.writeInt(rootDocId);\n+    }\n+    chunkLengths.add(postingListMap.size());\n+    postingListChunkOffsets.add(postingListWriter.size());\n+    postingListMap.clear();\n+    flattenedDocIdList.clear();\n+  }\n+\n+  private static List<Map<String, String>> unnestJson(JsonNode root) {\n+    Iterator<Map.Entry<String, JsonNode>> fields = root.fields();\n+    Map<String, String> flattenedSingleValuesMap = new TreeMap<>();\n+    Map<String, JsonNode> arrNodes = new TreeMap<>();\n+    Map<String, JsonNode> objectNodes = new TreeMap<>();\n+    List<Map<String, String>> resultList = new ArrayList<>();\n+    List<Map<String, String>> tempResultList = new ArrayList<>();\n+    while (fields.hasNext()) {\n+      Map.Entry<String, JsonNode> child = fields.next();\n+      if (child.getValue().isValueNode()) {\n+        //Normal value node\n+        flattenedSingleValuesMap.put(child.getKey(), child.getValue().asText());\n+      } else if (child.getValue().isArray()) {\n+        //Array Node: Process these nodes later\n+        arrNodes.put(child.getKey(), child.getValue());\n+      } else {\n+        //Object Node\n+        objectNodes.put(child.getKey(), child.getValue());\n+      }\n+    }\n+    for (String objectNodeKey : objectNodes.keySet()) {\n+      JsonNode objectNode = objectNodes.get(objectNodeKey);\n+      modifyKeysInMap(flattenedSingleValuesMap, tempResultList, objectNodeKey, objectNode);\n+    }\n+    if (tempResultList.isEmpty()) {\n+      tempResultList.add(flattenedSingleValuesMap);\n+    }\n+    if (!arrNodes.isEmpty()) {\n+      for (Map<String, String> flattenedMapElement : tempResultList) {\n+        for (String arrNodeKey : arrNodes.keySet()) {\n+          JsonNode arrNode = arrNodes.get(arrNodeKey);\n+          for (JsonNode arrNodeElement : arrNode) {\n+            modifyKeysInMap(flattenedMapElement, resultList, arrNodeKey, arrNodeElement);\n+          }\n+        }\n+      }\n+    } else {\n+      resultList.addAll(tempResultList);\n+    }\n+    return resultList;\n+  }\n+\n+  private static void modifyKeysInMap(Map<String, String> flattenedMap, List<Map<String, String>> resultList,\n+      String arrNodeKey, JsonNode arrNode) {\n+    List<Map<String, String>> objectResult = unnestJson(arrNode);\n+    for (Map<String, String> flattenedObject : objectResult) {\n+      Map<String, String> flattenedObjectCopy = new TreeMap<>(flattenedMap);\n+      for (Map.Entry<String, String> entry : flattenedObject.entrySet()) {\n+        flattenedObjectCopy.put(arrNodeKey + \".\" + entry.getKey(), entry.getValue());\n+      }\n+      resultList.add(flattenedObjectCopy);\n+    }\n+  }\n+\n+  @Override\n+  public void close()\n+      throws IOException {\n+\n+  }\n+\n+  private class Writer {\n+    private DataOutputStream _dictionaryHeaderWriter;\n+    private DataOutputStream _dictionaryOffsetWriter;\n+    private File _dictionaryOffsetFile;\n+    private DataOutputStream _dictionaryWriter;\n+    private DataOutputStream _invertedIndexOffsetWriter;\n+    private File _invertedIndexOffsetFile;\n+    private DataOutputStream _invertedIndexWriter;\n+    private int _dictId;\n+    private int _dictOffset;\n+    private int _invertedIndexOffset;\n+    int _maxDictionaryValueLength = Integer.MIN_VALUE;\n+\n+    public Writer(File dictionaryheaderFile, File dictionaryOffsetFile, File dictionaryFile,\n+        File invertedIndexOffsetFile, File invertedIndexFile)\n+        throws IOException {\n+      _dictionaryHeaderWriter =\n+          new DataOutputStream(new BufferedOutputStream(new FileOutputStream(dictionaryheaderFile)));\n+\n+      _dictionaryOffsetWriter =\n+          new DataOutputStream(new BufferedOutputStream(new FileOutputStream(dictionaryOffsetFile)));\n+      _dictionaryOffsetFile = dictionaryOffsetFile;\n+      _dictionaryWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(dictionaryFile)));\n+      _invertedIndexOffsetWriter =\n+          new DataOutputStream(new BufferedOutputStream(new FileOutputStream(invertedIndexOffsetFile)));\n+      _invertedIndexOffsetFile = invertedIndexOffsetFile;\n+      _invertedIndexWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(invertedIndexFile)));\n+      _dictId = 0;\n+      _dictOffset = 0;\n+      _invertedIndexOffset = 0;\n+    }\n+\n+    public void add(byte[] key, RoaringBitmap roaringBitmap)\n+        throws IOException {\n+      if (key.length > _maxDictionaryValueLength) {\n+        _maxDictionaryValueLength = key.length;\n+      }\n+      //write the key to dictionary\n+      _dictionaryOffsetWriter.writeInt(_dictOffset);\n+      _dictionaryWriter.write(key);\n+\n+      //write the roaringBitmap to inverted index\n+      _invertedIndexOffsetWriter.writeInt(_invertedIndexOffset);\n+\n+      int serializedSizeInBytes = roaringBitmap.serializedSizeInBytes();\n+      byte[] serializedRoaringBitmap = new byte[serializedSizeInBytes];\n+      ByteBuffer serializedRoaringBitmapBuffer = ByteBuffer.wrap(serializedRoaringBitmap);\n+      roaringBitmap.serialize(serializedRoaringBitmapBuffer);\n+      _invertedIndexWriter.write(serializedRoaringBitmap);\n+      System.out.println(", "originalCommit": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3Mjc0Ng==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515672746", "bodyText": "Remove?", "author": "mayankshriv", "createdAt": "2020-11-01T21:13:28Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/JSONIndexCreator.java", "diffHunk": "@@ -0,0 +1,658 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.github.wnameless.json.flattener.JsonFlattener;\n+import com.google.common.io.Files;\n+import com.google.common.primitives.UnsignedBytes;\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.pinot.core.common.BlockDocIdIterator;\n+import org.apache.pinot.core.io.util.VarLengthBytesValueReaderWriter;\n+import org.apache.pinot.core.operator.blocks.FilterBlock;\n+import org.apache.pinot.core.operator.filter.JSONMatchFilterOperator;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.predicate.EqPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.QueryContextConverterUtils;\n+import org.apache.pinot.core.segment.index.readers.JSONIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.sql.parsers.CalciteSqlParser;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+public class JSONIndexCreator implements Closeable {\n+\n+  //separator used to join the key and value to create posting list key\n+  public static String POSTING_LIST_KEY_SEPARATOR = \"|\";\n+  static int FLUSH_THRESHOLD = 50_000;\n+  static int VERSION = 1;\n+  private final File flattenedDocId2RootDocIdMappingFile;\n+  private final File postingListFile;\n+  private File dictionaryheaderFile;\n+  private File dictionaryOffsetFile;\n+  private File dictionaryFile;\n+  private File invertedIndexOffsetFile;\n+  private File invertedIndexFile;\n+  private File outputIndexFile;\n+\n+  private int docId = 0;\n+  private int numFlatennedDocId = 0;\n+  int chunkId = 0;\n+\n+  private DataOutputStream postingListWriter;\n+  private DataOutputStream flattenedDocId2RootDocIdWriter;\n+\n+  Map<String, List<Integer>> postingListMap = new TreeMap<>();\n+  List<Integer> flattenedDocIdList = new ArrayList<>();\n+  List<Integer> postingListChunkOffsets = new ArrayList<>();\n+  List<Integer> chunkLengths = new ArrayList<>();\n+  private FieldSpec fieldSpec;\n+\n+  public JSONIndexCreator(File indexDir, FieldSpec fieldSpec)\n+      throws IOException {\n+    this.fieldSpec = fieldSpec;\n+    System.out.println(\"indexDir = \" + indexDir);\n+\n+    String name = fieldSpec.getName();\n+    postingListFile = new File(indexDir + name + \"_postingList.buf\");\n+    postingListWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(postingListFile)));\n+    postingListChunkOffsets.add(postingListWriter.size());\n+\n+    dictionaryheaderFile = new File(indexDir, name + \"_dictionaryHeader.buf\");\n+    dictionaryOffsetFile = new File(indexDir, name + \"_dictionaryOffset.buf\");\n+    dictionaryFile = new File(indexDir, name + \"_dictionary.buf\");\n+    invertedIndexOffsetFile = new File(indexDir, name + \"_invertedIndexOffset.buf\");\n+    invertedIndexFile = new File(indexDir, name + \"_invertedIndex.buf\");\n+    flattenedDocId2RootDocIdMappingFile = new File(indexDir, name + \"_flattenedDocId.buf\");\n+    flattenedDocId2RootDocIdWriter =\n+        new DataOutputStream(new BufferedOutputStream(new FileOutputStream(flattenedDocId2RootDocIdMappingFile)));\n+\n+    //output file\n+    outputIndexFile = new File(indexDir, name + JSON_INDEX_FILE_EXTENSION);\n+  }\n+\n+  public void add(byte[] data)\n+      throws IOException {\n+\n+    JsonNode jsonNode = new ObjectMapper().readTree(data);\n+    List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+    for (Map<String, String> map : flattenedMapList) {\n+      //\n+      for (Map.Entry<String, String> entry : map.entrySet()) {\n+        //handle key posting list\n+        String key = entry.getKey();\n+\n+        List<Integer> keyPostingList = postingListMap.get(key);\n+        if (keyPostingList == null) {\n+          keyPostingList = new ArrayList<>();\n+          postingListMap.put(key, keyPostingList);\n+        }\n+        keyPostingList.add(numFlatennedDocId);\n+\n+        //handle keyvalue posting list\n+        String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+        List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+        if (keyValuePostingList == null) {\n+          keyValuePostingList = new ArrayList<>();\n+          postingListMap.put(keyValue, keyValuePostingList);\n+        }\n+        keyValuePostingList.add(numFlatennedDocId);\n+      }\n+      //flattenedDocId2RootDocIdMapping\n+      flattenedDocIdList.add(docId);\n+\n+      numFlatennedDocId++;\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  /**\n+   * Multi value\n+   * @param dataArray\n+   * @param length\n+   * @throws IOException\n+   */\n+  public void add(byte[][] dataArray, int length)\n+      throws IOException {\n+\n+    for (int i = 0; i < length; i++) {\n+      byte[] data = dataArray[i];\n+      JsonNode jsonNode = new ObjectMapper().readTree(data);\n+      List<Map<String, String>> flattenedMapList = unnestJson(jsonNode);\n+      for (Map<String, String> map : flattenedMapList) {\n+        //\n+        for (Map.Entry<String, String> entry : map.entrySet()) {\n+          //handle key posting list\n+          String key = entry.getKey();\n+\n+          List<Integer> keyPostingList = postingListMap.get(key);\n+          if (keyPostingList == null) {\n+            keyPostingList = new ArrayList<>();\n+            postingListMap.put(key, keyPostingList);\n+          }\n+          keyPostingList.add(numFlatennedDocId);\n+\n+          //handle keyvalue posting list\n+          String keyValue = key + POSTING_LIST_KEY_SEPARATOR + entry.getValue();\n+          List<Integer> keyValuePostingList = postingListMap.get(keyValue);\n+          if (keyValuePostingList == null) {\n+            keyValuePostingList = new ArrayList<>();\n+            postingListMap.put(keyValue, keyValuePostingList);\n+          }\n+          keyValuePostingList.add(numFlatennedDocId);\n+        }\n+        //flattenedDocId2RootDocIdMapping\n+        flattenedDocIdList.add(numFlatennedDocId);\n+\n+        numFlatennedDocId++;\n+      }\n+    }\n+    docId++;\n+\n+    //flush data\n+    if (docId % FLUSH_THRESHOLD == 0) {\n+      flush();\n+    }\n+  }\n+\n+  public void seal()\n+      throws IOException {\n+\n+    flush();\n+\n+    flattenedDocId2RootDocIdWriter.close();\n+    postingListWriter.close();\n+\n+    //key posting list merging\n+    System.out.println(\"InvertedIndex\");\n+    System.out.println(\"=================\");\n+\n+    int maxKeyLength = createInvertedIndex(postingListFile, postingListChunkOffsets, chunkLengths);\n+    System.out.println(\"=================\");\n+\n+    int flattenedDocid = 0;\n+    DataInputStream flattenedDocId2RootDocIdReader =\n+        new DataInputStream(new BufferedInputStream(new FileInputStream(flattenedDocId2RootDocIdMappingFile)));\n+    int[] rootDocIdArray = new int[numFlatennedDocId];\n+    while (flattenedDocid < numFlatennedDocId) {\n+      rootDocIdArray[flattenedDocid++] = flattenedDocId2RootDocIdReader.readInt();\n+    }\n+    System.out.println(\"FlattenedDocId  to RootDocId Mapping = \");\n+    System.out.println(Arrays.toString(rootDocIdArray));\n+\n+    //PUT all contents into one file\n+\n+    //header\n+    // version + maxDictionaryLength + [store the offsets + length for each one (dictionary offset file, dictionaryFile, index offset file, index file, flattened docId to rootDocId file)]\n+    long headerSize = 2 * Integer.BYTES + 6 * 2 * Long.BYTES;\n+\n+    long dataSize =\n+        dictionaryheaderFile.length() + dictionaryOffsetFile.length() + dictionaryFile.length() + invertedIndexFile\n+            .length() + invertedIndexOffsetFile.length() + flattenedDocId2RootDocIdMappingFile.length();\n+\n+    long totalSize = headerSize + dataSize;\n+    PinotDataBuffer pinotDataBuffer =\n+        PinotDataBuffer.mapFile(outputIndexFile, false, 0, totalSize, ByteOrder.BIG_ENDIAN, \"Nested inverted index\");\n+\n+    pinotDataBuffer.putInt(0, VERSION);\n+    pinotDataBuffer.putInt(1 * Integer.BYTES, maxKeyLength);\n+    long writtenBytes = headerSize;\n+\n+    //add dictionary header\n+    int bufferId = 0;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryheaderFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryheaderFile, 0, dictionaryheaderFile.length());\n+    writtenBytes += dictionaryheaderFile.length();\n+\n+    //add dictionary offset\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryOffsetFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryOffsetFile, 0, dictionaryOffsetFile.length());\n+    writtenBytes += dictionaryOffsetFile.length();\n+\n+    //add dictionary\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, dictionaryFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, dictionaryFile, 0, dictionaryFile.length());\n+    writtenBytes += dictionaryFile.length();\n+\n+    //add index offset\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, invertedIndexOffsetFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, invertedIndexOffsetFile, 0, invertedIndexOffsetFile.length());\n+    writtenBytes += invertedIndexOffsetFile.length();\n+\n+    //add index data\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, invertedIndexFile.length());\n+    pinotDataBuffer.readFrom(writtenBytes, invertedIndexFile, 0, invertedIndexFile.length());\n+    writtenBytes += invertedIndexFile.length();\n+\n+    //add flattened docid to root doc id mapping\n+    bufferId = bufferId + 1;\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId), writtenBytes);\n+    pinotDataBuffer.putLong(getBufferStartOffset(bufferId) + Long.BYTES, flattenedDocId2RootDocIdMappingFile.length());\n+    pinotDataBuffer\n+        .readFrom(writtenBytes, flattenedDocId2RootDocIdMappingFile, 0, flattenedDocId2RootDocIdMappingFile.length());\n+    writtenBytes += flattenedDocId2RootDocIdMappingFile.length();\n+  }\n+\n+  private long getBufferStartOffset(int bufferId) {\n+    return 2 * Integer.BYTES + 2 * bufferId * Long.BYTES;\n+  }\n+\n+  private int createInvertedIndex(File postingListFile, List<Integer> postingListChunkOffsets,\n+      List<Integer> chunkLengthList)\n+      throws IOException {\n+\n+    List<Iterator<ImmutablePair<byte[], int[]>>> chunkIterators = new ArrayList<>();\n+\n+    for (int i = 0; i < chunkLengthList.size(); i++) {\n+\n+      final DataInputStream postingListFileReader =\n+          new DataInputStream(new BufferedInputStream(new FileInputStream(postingListFile)));\n+      postingListFileReader.skipBytes(postingListChunkOffsets.get(i));\n+      final int length = chunkLengthList.get(i);\n+      chunkIterators.add(new Iterator<ImmutablePair<byte[], int[]>>() {\n+        int index = 0;\n+\n+        @Override\n+        public boolean hasNext() {\n+          return index < length;\n+        }\n+\n+        @Override\n+        public ImmutablePair<byte[], int[]> next() {\n+          try {\n+            int keyLength = postingListFileReader.readInt();\n+            byte[] keyBytes = new byte[keyLength];\n+            postingListFileReader.read(keyBytes);\n+\n+            int postingListLength = postingListFileReader.readInt();\n+            int[] postingList = new int[postingListLength];\n+            for (int i = 0; i < postingListLength; i++) {\n+              postingList[i] = postingListFileReader.readInt();\n+            }\n+            index++;\n+            return ImmutablePair.of(keyBytes, postingList);\n+          } catch (Exception e) {\n+            throw new RuntimeException(e);\n+          }\n+        }\n+      });\n+    }\n+    final Comparator<byte[]> byteArrayComparator = UnsignedBytes.lexicographicalComparator();\n+\n+    PriorityQueue<ImmutablePair<Integer, ImmutablePair<byte[], int[]>>> queue =\n+        new PriorityQueue<>(chunkLengthList.size(),\n+            (o1, o2) -> byteArrayComparator.compare(o1.getRight().getLeft(), o2.getRight().getLeft()));\n+    for (int i = 0; i < chunkIterators.size(); i++) {\n+      Iterator<ImmutablePair<byte[], int[]>> iterator = chunkIterators.get(i);\n+      if (iterator.hasNext()) {\n+        queue.offer(ImmutablePair.of(i, iterator.next()));\n+      }\n+    }\n+    byte[] prevKey = null;\n+    RoaringBitmap roaringBitmap = new RoaringBitmap();\n+\n+    Writer writer = new Writer(dictionaryheaderFile, dictionaryOffsetFile, dictionaryFile, invertedIndexOffsetFile,\n+        invertedIndexFile);\n+    while (!queue.isEmpty()) {\n+      ImmutablePair<Integer, ImmutablePair<byte[], int[]>> poll = queue.poll();\n+      byte[] currKey = poll.getRight().getLeft();\n+      if (prevKey != null && byteArrayComparator.compare(prevKey, currKey) != 0) {\n+        System.out.println(new String(prevKey) + \":\" + roaringBitmap);\n+        writer.add(prevKey, roaringBitmap);\n+        roaringBitmap.clear();\n+      }\n+\n+      roaringBitmap.add(poll.getRight().getRight());\n+      prevKey = currKey;\n+\n+      //add the next key from the chunk where the currKey was removed from\n+      Iterator<ImmutablePair<byte[], int[]>> iterator = chunkIterators.get(poll.getLeft());\n+      if (iterator.hasNext()) {\n+        queue.offer(ImmutablePair.of(poll.getLeft(), iterator.next()));\n+      }\n+    }\n+\n+    if (prevKey != null) {\n+      writer.add(prevKey, roaringBitmap);\n+    }\n+    writer.finish();\n+    return writer.getMaxDictionaryValueLength();\n+  }\n+\n+  private void flush()\n+      throws IOException {\n+    //write the key (length|actual bytes) - posting list(length, flattenedDocIds)\n+    System.out.println(\"postingListMap = \" + postingListMap);\n+    for (Map.Entry<String, List<Integer>> entry : postingListMap.entrySet()) {\n+      byte[] keyBytes = entry.getKey().getBytes(Charset.forName(\"UTF-8\"));\n+      postingListWriter.writeInt(keyBytes.length);\n+      postingListWriter.write(keyBytes);\n+      List<Integer> flattenedDocIdList = entry.getValue();\n+      postingListWriter.writeInt(flattenedDocIdList.size());\n+      for (int flattenedDocId : flattenedDocIdList) {\n+        postingListWriter.writeInt(flattenedDocId);\n+      }\n+    }\n+\n+    //write flattened doc id to root docId mapping\n+    for (int rootDocId : flattenedDocIdList) {\n+      flattenedDocId2RootDocIdWriter.writeInt(rootDocId);\n+    }\n+    chunkLengths.add(postingListMap.size());\n+    postingListChunkOffsets.add(postingListWriter.size());\n+    postingListMap.clear();\n+    flattenedDocIdList.clear();\n+  }\n+\n+  private static List<Map<String, String>> unnestJson(JsonNode root) {\n+    Iterator<Map.Entry<String, JsonNode>> fields = root.fields();\n+    Map<String, String> flattenedSingleValuesMap = new TreeMap<>();\n+    Map<String, JsonNode> arrNodes = new TreeMap<>();\n+    Map<String, JsonNode> objectNodes = new TreeMap<>();\n+    List<Map<String, String>> resultList = new ArrayList<>();\n+    List<Map<String, String>> tempResultList = new ArrayList<>();\n+    while (fields.hasNext()) {\n+      Map.Entry<String, JsonNode> child = fields.next();\n+      if (child.getValue().isValueNode()) {\n+        //Normal value node\n+        flattenedSingleValuesMap.put(child.getKey(), child.getValue().asText());\n+      } else if (child.getValue().isArray()) {\n+        //Array Node: Process these nodes later\n+        arrNodes.put(child.getKey(), child.getValue());\n+      } else {\n+        //Object Node\n+        objectNodes.put(child.getKey(), child.getValue());\n+      }\n+    }\n+    for (String objectNodeKey : objectNodes.keySet()) {\n+      JsonNode objectNode = objectNodes.get(objectNodeKey);\n+      modifyKeysInMap(flattenedSingleValuesMap, tempResultList, objectNodeKey, objectNode);\n+    }\n+    if (tempResultList.isEmpty()) {\n+      tempResultList.add(flattenedSingleValuesMap);\n+    }\n+    if (!arrNodes.isEmpty()) {\n+      for (Map<String, String> flattenedMapElement : tempResultList) {\n+        for (String arrNodeKey : arrNodes.keySet()) {\n+          JsonNode arrNode = arrNodes.get(arrNodeKey);\n+          for (JsonNode arrNodeElement : arrNode) {\n+            modifyKeysInMap(flattenedMapElement, resultList, arrNodeKey, arrNodeElement);\n+          }\n+        }\n+      }\n+    } else {\n+      resultList.addAll(tempResultList);\n+    }\n+    return resultList;\n+  }\n+\n+  private static void modifyKeysInMap(Map<String, String> flattenedMap, List<Map<String, String>> resultList,\n+      String arrNodeKey, JsonNode arrNode) {\n+    List<Map<String, String>> objectResult = unnestJson(arrNode);\n+    for (Map<String, String> flattenedObject : objectResult) {\n+      Map<String, String> flattenedObjectCopy = new TreeMap<>(flattenedMap);\n+      for (Map.Entry<String, String> entry : flattenedObject.entrySet()) {\n+        flattenedObjectCopy.put(arrNodeKey + \".\" + entry.getKey(), entry.getValue());\n+      }\n+      resultList.add(flattenedObjectCopy);\n+    }\n+  }\n+\n+  @Override\n+  public void close()\n+      throws IOException {\n+\n+  }\n+\n+  private class Writer {\n+    private DataOutputStream _dictionaryHeaderWriter;\n+    private DataOutputStream _dictionaryOffsetWriter;\n+    private File _dictionaryOffsetFile;\n+    private DataOutputStream _dictionaryWriter;\n+    private DataOutputStream _invertedIndexOffsetWriter;\n+    private File _invertedIndexOffsetFile;\n+    private DataOutputStream _invertedIndexWriter;\n+    private int _dictId;\n+    private int _dictOffset;\n+    private int _invertedIndexOffset;\n+    int _maxDictionaryValueLength = Integer.MIN_VALUE;\n+\n+    public Writer(File dictionaryheaderFile, File dictionaryOffsetFile, File dictionaryFile,\n+        File invertedIndexOffsetFile, File invertedIndexFile)\n+        throws IOException {\n+      _dictionaryHeaderWriter =\n+          new DataOutputStream(new BufferedOutputStream(new FileOutputStream(dictionaryheaderFile)));\n+\n+      _dictionaryOffsetWriter =\n+          new DataOutputStream(new BufferedOutputStream(new FileOutputStream(dictionaryOffsetFile)));\n+      _dictionaryOffsetFile = dictionaryOffsetFile;\n+      _dictionaryWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(dictionaryFile)));\n+      _invertedIndexOffsetWriter =\n+          new DataOutputStream(new BufferedOutputStream(new FileOutputStream(invertedIndexOffsetFile)));\n+      _invertedIndexOffsetFile = invertedIndexOffsetFile;\n+      _invertedIndexWriter = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(invertedIndexFile)));\n+      _dictId = 0;\n+      _dictOffset = 0;\n+      _invertedIndexOffset = 0;\n+    }\n+\n+    public void add(byte[] key, RoaringBitmap roaringBitmap)\n+        throws IOException {\n+      if (key.length > _maxDictionaryValueLength) {\n+        _maxDictionaryValueLength = key.length;\n+      }\n+      //write the key to dictionary\n+      _dictionaryOffsetWriter.writeInt(_dictOffset);\n+      _dictionaryWriter.write(key);\n+\n+      //write the roaringBitmap to inverted index\n+      _invertedIndexOffsetWriter.writeInt(_invertedIndexOffset);\n+\n+      int serializedSizeInBytes = roaringBitmap.serializedSizeInBytes();\n+      byte[] serializedRoaringBitmap = new byte[serializedSizeInBytes];\n+      ByteBuffer serializedRoaringBitmapBuffer = ByteBuffer.wrap(serializedRoaringBitmap);\n+      roaringBitmap.serialize(serializedRoaringBitmapBuffer);\n+      _invertedIndexWriter.write(serializedRoaringBitmap);\n+      System.out.println(\n+          \"dictId = \" + _dictId + \", dict offset:\" + _dictOffset + \", valueLength:\" + key.length + \", inv offset:\"\n+              + _invertedIndexOffset + \", serializedSizeInBytes:\" + serializedSizeInBytes);\n+\n+      //increment offsets\n+      _dictOffset = _dictOffset + key.length;\n+      _invertedIndexOffset = _invertedIndexOffset + serializedSizeInBytes;\n+      //increment the dictionary id\n+      _dictId = _dictId + 1;\n+    }\n+\n+    void finish()\n+        throws IOException {\n+      //InvertedIndexReader and VarlengthBytesValueReaderWriter needs one extra entry for offsets since it computes the length for index i using offset[i+1] - offset[i]\n+      _invertedIndexOffsetWriter.writeInt(_invertedIndexOffset);\n+      _dictionaryOffsetWriter.writeInt(_dictOffset);\n+\n+      byte[] headerBytes = VarLengthBytesValueReaderWriter.getHeaderBytes(_dictId);\n+      _dictionaryHeaderWriter.write(headerBytes);\n+      System.out.println(\"headerBytes = \" + Arrays.toString(headerBytes));\n+\n+      _dictionaryHeaderWriter.close();\n+      _dictionaryOffsetWriter.close();\n+      _dictionaryWriter.close();\n+      _invertedIndexOffsetWriter.close();\n+      _invertedIndexWriter.close();\n+\n+      //data offsets started with zero but the actual dictionary and index will contain (header + offsets + data). so all the offsets must be adjusted ( i.e add size(header) + size(offset) to each offset value)\n+      PinotDataBuffer dictionaryOffsetBuffer = PinotDataBuffer\n+          .mapFile(dictionaryOffsetFile, false, 0, _dictionaryOffsetFile.length(), ByteOrder.BIG_ENDIAN,\n+              \"dictionary offset file\");\n+      int dictOffsetBase = _dictionaryHeaderWriter.size() + _dictionaryOffsetWriter.size();\n+      for (int i = 0; i < _dictId + 1; i++) {\n+        int offset = dictionaryOffsetBuffer.getInt(i * Integer.BYTES);\n+        int newOffset = offset + dictOffsetBase;\n+        dictionaryOffsetBuffer.putInt(i * Integer.BYTES, offset + dictOffsetBase);\n+        System.out.println(\"dictId = \" + i + \", offset = \" + offset + \", newOffset = \" + newOffset);\n+      }\n+\n+      PinotDataBuffer invIndexOffsetBuffer = PinotDataBuffer\n+          .mapFile(invertedIndexOffsetFile, false, 0, invertedIndexOffsetFile.length(), ByteOrder.BIG_ENDIAN,\n+              \"invertedIndexOffsetFile\");\n+      int invIndexOffsetBase = _invertedIndexOffsetWriter.size();\n+      for (int i = 0; i < _dictId + 1; i++) {\n+        int offset = invIndexOffsetBuffer.getInt(i * Integer.BYTES);\n+        int newOffset = offset + invIndexOffsetBase;\n+        System.out.println(\"offset = \" + offset + \", newOffset = \" + newOffset);\n+\n+        invIndexOffsetBuffer.putInt(i * Integer.BYTES, newOffset);\n+      }\n+\n+      invIndexOffsetBuffer.close();\n+      dictionaryOffsetBuffer.close();\n+    }\n+\n+    public int getMaxDictionaryValueLength() {\n+      return _maxDictionaryValueLength;\n+    }\n+  }\n+\n+  public static void main(String[] args)", "originalCommit": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3MjgwNQ==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r515672805", "bodyText": "Add javadoc.", "author": "mayankshriv", "createdAt": "2020-11-01T21:14:06Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/index/loader/invertedindex/JSONIndexHandler.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.index.loader.invertedindex;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.pinot.core.indexsegment.generator.SegmentVersion;\n+import org.apache.pinot.core.segment.creator.impl.inv.JSONIndexCreator;\n+import org.apache.pinot.core.segment.index.column.PhysicalColumnIndexContainer;\n+import org.apache.pinot.core.segment.index.loader.IndexLoadingConfig;\n+import org.apache.pinot.core.segment.index.loader.LoaderUtils;\n+import org.apache.pinot.core.segment.index.metadata.ColumnMetadata;\n+import org.apache.pinot.core.segment.index.metadata.SegmentMetadataImpl;\n+import org.apache.pinot.core.segment.index.readers.BaseImmutableDictionary;\n+import org.apache.pinot.core.segment.index.readers.Dictionary;\n+import org.apache.pinot.core.segment.index.readers.ForwardIndexReader;\n+import org.apache.pinot.core.segment.index.readers.ForwardIndexReaderContext;\n+import org.apache.pinot.core.segment.index.readers.forward.FixedBitMVForwardIndexReader;\n+import org.apache.pinot.core.segment.index.readers.forward.FixedBitSVForwardIndexReader;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.core.segment.store.ColumnIndexType;\n+import org.apache.pinot.core.segment.store.SegmentDirectory;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.pinot.core.segment.creator.impl.V1Constants.Indexes.JSON_INDEX_FILE_EXTENSION;\n+\n+\n+@SuppressWarnings({\"rawtypes\", \"unchecked\"})\n+public class JSONIndexHandler {\n+  private static final Logger LOGGER = LoggerFactory.getLogger(JSONIndexHandler.class);\n+\n+  private final File _indexDir;\n+  private final SegmentDirectory.Writer _segmentWriter;\n+  private final String _segmentName;\n+  private final SegmentVersion _segmentVersion;\n+  private final Set<ColumnMetadata> _jsonIndexColumns = new HashSet<>();\n+\n+  public JSONIndexHandler(File indexDir, SegmentMetadataImpl segmentMetadata, IndexLoadingConfig indexLoadingConfig,", "originalCommit": "ee6714c2ac4b6eea174d48e862fb7020494b6c1d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9e6f83785d3e9886572d6d1bc7594ac8366c4031", "url": "https://github.com/apache/pinot/commit/9e6f83785d3e9886572d6d1bc7594ac8366c4031", "message": "adding example data for json", "committedDate": "2020-11-03T04:16:28Z", "type": "forcePushed"}, {"oid": "64dba072325aef552c8b8ccc91b014fc7c46056e", "url": "https://github.com/apache/pinot/commit/64dba072325aef552c8b8ccc91b014fc7c46056e", "message": "adding example data for json", "committedDate": "2020-11-03T19:56:52Z", "type": "forcePushed"}, {"oid": "bd6b9ccf4a8c253d449e313128c5df5b41905b40", "url": "https://github.com/apache/pinot/commit/bd6b9ccf4a8c253d449e313128c5df5b41905b40", "message": "adding support querying based on array index", "committedDate": "2020-11-24T19:48:16Z", "type": "forcePushed"}, {"oid": "36c968568cfb482af474048851c7fae000d07dcc", "url": "https://github.com/apache/pinot/commit/36c968568cfb482af474048851c7fae000d07dcc", "message": "Add json index support", "committedDate": "2020-12-11T03:29:24Z", "type": "forcePushed"}, {"oid": "c0e5f33ec01d80cf76ccc39539872f9e613d0e67", "url": "https://github.com/apache/pinot/commit/c0e5f33ec01d80cf76ccc39539872f9e613d0e67", "message": "Add json index support", "committedDate": "2020-12-11T20:14:58Z", "type": "forcePushed"}, {"oid": "65457539f7e3cf4d4e6f27e58d2cb9f8bb3896ac", "url": "https://github.com/apache/pinot/commit/65457539f7e3cf4d4e6f27e58d2cb9f8bb3896ac", "message": "Add json index support", "committedDate": "2020-12-11T21:52:11Z", "type": "forcePushed"}, {"oid": "7be263b8cd11340c7189fc0880aefb37264dbe0f", "url": "https://github.com/apache/pinot/commit/7be263b8cd11340c7189fc0880aefb37264dbe0f", "message": "Add json index support", "committedDate": "2020-12-14T18:54:40Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjY0MTA1Nw==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r542641057", "bodyText": "Do we not have an existing index creator interface (for example, text index creator could have used the same interface)?", "author": "mayankshriv", "createdAt": "2020-12-14T18:52:42Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/JsonIndexCreator.java", "diffHunk": "@@ -0,0 +1,42 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+\n+\n+/**\n+ * Index creator for json index.\n+ */\n+public interface JsonIndexCreator extends Closeable {", "originalCommit": "65457539f7e3cf4d4e6f27e58d2cb9f8bb3896ac", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjc1OTY4Mw==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r542759683", "bodyText": "Not yet, we only have an interface for the inverted index. We can do the clean up later", "author": "Jackie-Jiang", "createdAt": "2020-12-14T20:41:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjY0MTA1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjY0NDY2MA==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r542644660", "bodyText": "Add javadoc for file-format? Also, consider adding header - version, data-start-offset, etc, to help with maintaining compatability.", "author": "mayankshriv", "createdAt": "2020-12-14T18:55:52Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/BitmapInvertedIndexWriter.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv;\n+\n+import java.io.Closeable;\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.channels.FileChannel;\n+import org.apache.pinot.core.util.CleanerUtil;\n+import org.roaringbitmap.RoaringBitmap;\n+\n+\n+/**\n+ * Writer for bitmap inverted index file.\n+ */\n+public final class BitmapInvertedIndexWriter implements Closeable {", "originalCommit": "7be263b8cd11340c7189fc0880aefb37264dbe0f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjc2Njc3OA==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r542766778", "bodyText": "Added the file format. This writer is using the same format as the existing one (just extracted out the common logic), so we cannot add additional header info because the reader does not support that.", "author": "Jackie-Jiang", "createdAt": "2020-12-14T20:48:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjY0NDY2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjY0NTg2Mg==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r542645862", "bodyText": "This should be a function of total doc size, not just num docs?", "author": "mayankshriv", "createdAt": "2020-12-14T18:56:55Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/json/OffHeapJsonIndexCreator.java", "diffHunk": "@@ -0,0 +1,279 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv.json;\n+\n+import it.unimi.dsi.fastutil.longs.LongArrayList;\n+import it.unimi.dsi.fastutil.longs.LongList;\n+import java.io.BufferedOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteOrder;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.PriorityQueue;\n+import org.apache.pinot.core.io.util.VarLengthValueWriter;\n+import org.apache.pinot.core.segment.creator.impl.inv.BitmapInvertedIndexWriter;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+import org.apache.pinot.spi.utils.StringUtils;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.RoaringBitmapWriter;\n+import org.roaringbitmap.buffer.ImmutableRoaringBitmap;\n+import org.roaringbitmap.buffer.MutableRoaringBitmap;\n+\n+\n+/**\n+ * Implementation of {@link org.apache.pinot.core.segment.creator.JsonIndexCreator} that uses off-heap memory.\n+ * <p>The posting lists (map from value to doc ids) are initially stored in a TreeMap, then flushed into a file for\n+ * every 50000 documents (unflattened records) added. After all the documents are added, we read all the posting lists\n+ * from the file and merge them using a priority queue to calculate the final posting lists. Then we generate the string\n+ * dictionary and inverted index from the final posting lists and create the json index on top of them.\n+ */\n+public class OffHeapJsonIndexCreator extends BaseJsonIndexCreator {\n+  private static final int FLUSH_THRESHOLD = 50_000;", "originalCommit": "7be263b8cd11340c7189fc0880aefb37264dbe0f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjc3MTkyNg==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r542771926", "bodyText": "why do you think so?", "author": "kishoreg", "createdAt": "2020-12-14T20:52:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjY0NTg2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjc3MjgzMw==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r542772833", "bodyText": "We limit the on-heap memory usage by putting a threshold on the docs held in memory. The total doc size doesn't matter because it just increases the number of chunks, and won't increase the on-heap memory usage.", "author": "Jackie-Jiang", "createdAt": "2020-12-14T20:53:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjY0NTg2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjY0NjUxMQ==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r542646511", "bodyText": "Perhaps specify when to use on-heap vs off-heap?", "author": "mayankshriv", "createdAt": "2020-12-14T18:57:31Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/creator/impl/inv/json/OnHeapJsonIndexCreator.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.segment.creator.impl.inv.json;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.pinot.core.io.util.VarLengthValueWriter;\n+import org.apache.pinot.core.segment.creator.impl.inv.BitmapInvertedIndexWriter;\n+import org.apache.pinot.spi.utils.StringUtils;\n+import org.roaringbitmap.RoaringBitmap;\n+import org.roaringbitmap.RoaringBitmapWriter;\n+\n+\n+/**\n+ * Implementation of {@link org.apache.pinot.core.segment.creator.JsonIndexCreator} that uses on-heap memory.", "originalCommit": "7be263b8cd11340c7189fc0880aefb37264dbe0f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjc4MzczNA==", "url": "https://github.com/apache/pinot/pull/6216#discussion_r542783734", "bodyText": "Added", "author": "Jackie-Jiang", "createdAt": "2020-12-14T21:03:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjY0NjUxMQ=="}], "type": "inlineReview"}, {"oid": "b8aab328e686c460b30d8b32fb516dd84d26c712", "url": "https://github.com/apache/pinot/commit/b8aab328e686c460b30d8b32fb516dd84d26c712", "message": "Add json index support", "committedDate": "2020-12-14T21:04:21Z", "type": "forcePushed"}, {"oid": "c217c9e15e6147af07e9c8a70616ab3660696ffa", "url": "https://github.com/apache/pinot/commit/c217c9e15e6147af07e9c8a70616ab3660696ffa", "message": "Add json index support", "committedDate": "2020-12-28T21:47:31Z", "type": "commit"}, {"oid": "c217c9e15e6147af07e9c8a70616ab3660696ffa", "url": "https://github.com/apache/pinot/commit/c217c9e15e6147af07e9c8a70616ab3660696ffa", "message": "Add json index support", "committedDate": "2020-12-28T21:47:31Z", "type": "forcePushed"}]}