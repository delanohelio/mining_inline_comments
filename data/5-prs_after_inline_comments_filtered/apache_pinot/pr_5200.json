{"pr_number": 5200, "pr_title": "[TE] Runner to generate SLA based metric data missing alerts", "pr_createdAt": "2020-03-31T18:06:46Z", "pr_url": "https://github.com/apache/pinot/pull/5200", "timeline": [{"oid": "07b68f0b66ef5cf75fa53a8976d845e2f27149e3", "url": "https://github.com/apache/pinot/commit/07b68f0b66ef5cf75fa53a8976d845e2f27149e3", "message": "[TE] SLA based metric data missing alerts", "committedDate": "2020-03-31T16:52:49Z", "type": "commit"}, {"oid": "c9fbaca08d2ffaadf26c3c25bdf2ff4aa088ef96", "url": "https://github.com/apache/pinot/commit/c9fbaca08d2ffaadf26c3c25bdf2ff4aa088ef96", "message": "remove comment", "committedDate": "2020-03-31T18:23:29Z", "type": "commit"}, {"oid": "722322b51c1f62f0dd9dde7aadec9fb34574fb40", "url": "https://github.com/apache/pinot/commit/722322b51c1f62f0dd9dde7aadec9fb34574fb40", "message": "injected sla settings into the anomaly properties", "committedDate": "2020-03-31T22:08:05Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjcwMTQyOQ==", "url": "https://github.com/apache/pinot/pull/5200#discussion_r402701429", "bodyText": "Why we care about partial data? In our current setting the endTime of detection window is current time, and it is very common that we have partial data.", "author": "xiaohui-sun", "createdAt": "2020-04-03T02:26:33Z", "path": "thirdeye/thirdeye-pinot/src/main/java/org/apache/pinot/thirdeye/detection/datasla/DatasetSlaTaskRunner.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.pinot.thirdeye.detection.datasla;\n+\n+import com.google.common.collect.ArrayListMultimap;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.pinot.thirdeye.anomaly.AnomalyType;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskContext;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskInfo;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskResult;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskRunner;\n+import org.apache.pinot.thirdeye.common.time.TimeGranularity;\n+import org.apache.pinot.thirdeye.dataframe.DataFrame;\n+import org.apache.pinot.thirdeye.dataframe.util.MetricSlice;\n+import org.apache.pinot.thirdeye.datalayer.bao.DatasetConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.DetectionConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.EvaluationManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.EventManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.MergedAnomalyResultManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.MetricConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.dto.DatasetConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.DetectionConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.MergedAnomalyResultDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.MetricConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.pojo.MergedAnomalyResultBean;\n+import org.apache.pinot.thirdeye.datasource.DAORegistry;\n+import org.apache.pinot.thirdeye.datasource.ThirdEyeCacheRegistry;\n+import org.apache.pinot.thirdeye.datasource.loader.AggregationLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.DefaultAggregationLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.DefaultTimeSeriesLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.TimeSeriesLoader;\n+import org.apache.pinot.thirdeye.detection.ConfigUtils;\n+import org.apache.pinot.thirdeye.detection.DataProvider;\n+import org.apache.pinot.thirdeye.detection.DefaultDataProvider;\n+import org.apache.pinot.thirdeye.detection.DetectionPipelineLoader;\n+import org.apache.pinot.thirdeye.detection.DetectionPipelineTaskInfo;\n+import org.apache.pinot.thirdeye.detection.DetectionUtils;\n+import org.apache.pinot.thirdeye.detection.cache.builder.AnomaliesCacheBuilder;\n+import org.apache.pinot.thirdeye.detection.cache.builder.TimeSeriesCacheBuilder;\n+import org.apache.pinot.thirdeye.rootcause.impl.MetricEntity;\n+import org.joda.time.DateTime;\n+import org.joda.time.DateTimeZone;\n+import org.joda.time.Period;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.pinot.thirdeye.anomaly.utils.ThirdeyeMetricsUtil.*;\n+\n+\n+/**\n+ * Runner that generates Data SLA anomalies. DATA_MISSING anomalies are created if\n+ * the data is not available for the sla detection window within the configured SLA.\n+ */\n+public class DatasetSlaTaskRunner implements TaskRunner {\n+  private static final Logger LOG = LoggerFactory.getLogger(DatasetSlaTaskRunner.class);\n+\n+  private final DetectionConfigManager detectionDAO;\n+  private final MergedAnomalyResultManager anomalyDAO;\n+  private final EvaluationManager evaluationDAO;\n+  private DataProvider provider;\n+\n+  private final String DEFAULT_DATA_SLA = \"3_DAYS\";\n+\n+  public DatasetSlaTaskRunner() {\n+    this.detectionDAO = DAORegistry.getInstance().getDetectionConfigManager();\n+    this.anomalyDAO = DAORegistry.getInstance().getMergedAnomalyResultDAO();\n+    this.evaluationDAO = DAORegistry.getInstance().getEvaluationManager();\n+\n+    MetricConfigManager metricDAO = DAORegistry.getInstance().getMetricConfigDAO();\n+    DatasetConfigManager datasetDAO = DAORegistry.getInstance().getDatasetConfigDAO();\n+    EventManager eventDAO = DAORegistry.getInstance().getEventDAO();\n+\n+    TimeSeriesLoader timeseriesLoader =\n+        new DefaultTimeSeriesLoader(metricDAO, datasetDAO,\n+            ThirdEyeCacheRegistry.getInstance().getQueryCache(), ThirdEyeCacheRegistry.getInstance().getTimeSeriesCache());\n+\n+    AggregationLoader aggregationLoader =\n+        new DefaultAggregationLoader(metricDAO, datasetDAO,\n+            ThirdEyeCacheRegistry.getInstance().getQueryCache(),\n+            ThirdEyeCacheRegistry.getInstance().getDatasetMaxDataTimeCache());\n+\n+    this.provider = new DefaultDataProvider(metricDAO, datasetDAO, eventDAO, this.anomalyDAO, this.evaluationDAO,\n+        timeseriesLoader, aggregationLoader, new DetectionPipelineLoader(), TimeSeriesCacheBuilder.getInstance(),\n+        AnomaliesCacheBuilder.getInstance());\n+  }\n+\n+  public DatasetSlaTaskRunner(DetectionConfigManager detectionDAO, MergedAnomalyResultManager anomalyDAO,\n+      EvaluationManager evaluationDAO, DataProvider provider) {\n+    this.detectionDAO = detectionDAO;\n+    this.anomalyDAO = anomalyDAO;\n+    this.evaluationDAO = evaluationDAO;\n+    this.provider = provider;\n+  }\n+\n+  @Override\n+  public List<TaskResult> execute(TaskInfo taskInfo, TaskContext taskContext) throws Exception {\n+    dataAvailabilityTaskCounter.inc();\n+\n+    try {\n+      DetectionPipelineTaskInfo info = (DetectionPipelineTaskInfo) taskInfo;\n+\n+      DetectionConfigDTO config = this.detectionDAO.findById(info.getConfigId());\n+      if (config == null) {\n+        throw new IllegalArgumentException(String.format(\"Could not resolve config id %d\", info.getConfigId()));\n+      }\n+\n+      LOG.info(\"Check data sla for config {} between {} and {}\", config.getId(), info.getStart(), info.getEnd());\n+      Map<String, Object> metricUrnToSlaMap = config.getDataSLAProperties();\n+      if (MapUtils.isNotEmpty(metricUrnToSlaMap)) {\n+        for (Map.Entry<String, Object> metricUrnToSlaMapEntry : metricUrnToSlaMap.entrySet()) {\n+          MetricEntity me = MetricEntity.fromURN(metricUrnToSlaMapEntry.getKey());\n+          MetricConfigDTO\n+              metricConfigDTO = this.provider.fetchMetrics(Collections.singletonList(me.getId())).get(me.getId());\n+          Map<String, DatasetConfigDTO> datasetToConfigMap = provider.fetchDatasets(Collections.singletonList(metricConfigDTO.getDataset()));\n+          for (Map.Entry<String, DatasetConfigDTO> datasetSLA : datasetToConfigMap.entrySet()) {\n+            try {\n+              runSLACheck(me, datasetSLA.getValue(), info, ConfigUtils.getMap(metricUrnToSlaMapEntry.getValue()));\n+            } catch (Exception e) {\n+              LOG.error(\"Failed to run sla check on metric URN %s\", datasetSLA.getKey(), e);\n+            }\n+          }\n+        }\n+      }\n+\n+      //LOG.info(\"End data availability for config {} between {} and {}. Detected {} anomalies.\", config.getId(),\n+      //    info.getStart(), info.getEnd(), anomaliesList.size());\n+      return Collections.emptyList();\n+    } catch(Exception e) {\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * Runs the data sla check for the window (info) on the given metric (me) using the configured sla properties\n+   */\n+  private void runSLACheck(MetricEntity me, DatasetConfigDTO datasetConfig, DetectionPipelineTaskInfo info,\n+      Map<String, Object> slaProps) {\n+    if (me == null || datasetConfig == null || info == null) {\n+      //nothing to check\n+      return;\n+    }\n+\n+    try {\n+      long datasetLastRefreshTime = datasetConfig.getLastRefreshTime();\n+      if (datasetLastRefreshTime <= 0) {\n+        // assume we have processed data till the current detection start\n+        datasetLastRefreshTime = info.getStart() - 1;\n+      }\n+\n+      if (isMissingData(datasetLastRefreshTime, info)) {\n+        // Double check with data source as 2 things are possible.\n+        // 1. This dataset/source may not support availability events\n+        // 2. The data availability event pipeline has some issue.\n+        // We want to measure the overall dataset availability (filters are not taken into account)\n+        MetricSlice metricSlice = MetricSlice.from(me.getId(), info.getStart(), info.getEnd(), ArrayListMultimap.<String, String>create());\n+        DataFrame dataFrame = this.provider.fetchTimeseries(Collections.singleton(metricSlice)).get(metricSlice);\n+        if (dataFrame == null || dataFrame.isEmpty()) {\n+          // no data\n+          if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+            createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);\n+          }\n+        } else {\n+          datasetLastRefreshTime = dataFrame.getDoubles(\"timestamp\").max().longValue();\n+          if (isPartialData(datasetLastRefreshTime, info, datasetConfig)) {\n+            if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+              createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);\n+            }\n+          }\n+        }\n+      } else if (isPartialData(datasetLastRefreshTime, info, datasetConfig)) {\n+        // Optimize for the common case - the common case is that the data availability events are arriving\n+        // correctly and we need not re-fetch the data to double check.\n+        if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+          createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);\n+        }\n+      }\n+    } catch (Exception e) {\n+      LOG.error(String.format(\"Failed to run sla check on metric URN %s\", me.getUrn()), e);\n+    }\n+  }\n+\n+  /**\n+   * We say the data is missing we do not have data in the sla detection window.\n+   * Or more specifically if the dataset watermark is below the sla detection window.\n+   */\n+  private boolean isMissingData(long datasetLastRefreshTime, DetectionPipelineTaskInfo info) {\n+    return datasetLastRefreshTime < info.getStart();\n+  }\n+\n+  /**\n+   * We say the data is partial if we do not have all the data points in the sla detection window.\n+   * Or more specifically if we have at least 1 data-point missing in the sla detection window.\n+   *\n+   * For example:\n+   * Assume that the data is delayed and our current sla detection window is [1st Feb to 3rd Feb). During this scan,\n+   * let's say data for 1st Feb arrives. Now, we have a situation where partial data is present. In other words, in the\n+   * current sla detection window [1st to 3rd Feb) we have data for 1st Feb but data for 2nd Feb is missing. This is the\n+   * partial data scenario.\n+   */\n+  private boolean isPartialData(long datasetLastRefreshTime, DetectionPipelineTaskInfo info, DatasetConfigDTO datasetConfig) {", "originalCommit": "722322b51c1f62f0dd9dde7aadec9fb34574fb40", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODQ4NTgxNw==", "url": "https://github.com/apache/pinot/pull/5200#discussion_r408485817", "bodyText": "discussed offline! others - pls refer to the comment in the code.", "author": "akshayrai", "createdAt": "2020-04-14T23:03:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjcwMTQyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjcwMTk4OQ==", "url": "https://github.com/apache/pinot/pull/5200#discussion_r402701989", "bodyText": "We need to consider the expectedDelay in datasetconfig.\nThere are some datasets that have several days delay by default.", "author": "xiaohui-sun", "createdAt": "2020-04-03T02:28:49Z", "path": "thirdeye/thirdeye-pinot/src/main/java/org/apache/pinot/thirdeye/detection/datasla/DatasetSlaTaskRunner.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.pinot.thirdeye.detection.datasla;\n+\n+import com.google.common.collect.ArrayListMultimap;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.pinot.thirdeye.anomaly.AnomalyType;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskContext;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskInfo;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskResult;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskRunner;\n+import org.apache.pinot.thirdeye.common.time.TimeGranularity;\n+import org.apache.pinot.thirdeye.dataframe.DataFrame;\n+import org.apache.pinot.thirdeye.dataframe.util.MetricSlice;\n+import org.apache.pinot.thirdeye.datalayer.bao.DatasetConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.DetectionConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.EvaluationManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.EventManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.MergedAnomalyResultManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.MetricConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.dto.DatasetConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.DetectionConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.MergedAnomalyResultDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.MetricConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.pojo.MergedAnomalyResultBean;\n+import org.apache.pinot.thirdeye.datasource.DAORegistry;\n+import org.apache.pinot.thirdeye.datasource.ThirdEyeCacheRegistry;\n+import org.apache.pinot.thirdeye.datasource.loader.AggregationLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.DefaultAggregationLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.DefaultTimeSeriesLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.TimeSeriesLoader;\n+import org.apache.pinot.thirdeye.detection.ConfigUtils;\n+import org.apache.pinot.thirdeye.detection.DataProvider;\n+import org.apache.pinot.thirdeye.detection.DefaultDataProvider;\n+import org.apache.pinot.thirdeye.detection.DetectionPipelineLoader;\n+import org.apache.pinot.thirdeye.detection.DetectionPipelineTaskInfo;\n+import org.apache.pinot.thirdeye.detection.DetectionUtils;\n+import org.apache.pinot.thirdeye.detection.cache.builder.AnomaliesCacheBuilder;\n+import org.apache.pinot.thirdeye.detection.cache.builder.TimeSeriesCacheBuilder;\n+import org.apache.pinot.thirdeye.rootcause.impl.MetricEntity;\n+import org.joda.time.DateTime;\n+import org.joda.time.DateTimeZone;\n+import org.joda.time.Period;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.pinot.thirdeye.anomaly.utils.ThirdeyeMetricsUtil.*;\n+\n+\n+/**\n+ * Runner that generates Data SLA anomalies. DATA_MISSING anomalies are created if\n+ * the data is not available for the sla detection window within the configured SLA.\n+ */\n+public class DatasetSlaTaskRunner implements TaskRunner {\n+  private static final Logger LOG = LoggerFactory.getLogger(DatasetSlaTaskRunner.class);\n+\n+  private final DetectionConfigManager detectionDAO;\n+  private final MergedAnomalyResultManager anomalyDAO;\n+  private final EvaluationManager evaluationDAO;\n+  private DataProvider provider;\n+\n+  private final String DEFAULT_DATA_SLA = \"3_DAYS\";\n+\n+  public DatasetSlaTaskRunner() {\n+    this.detectionDAO = DAORegistry.getInstance().getDetectionConfigManager();\n+    this.anomalyDAO = DAORegistry.getInstance().getMergedAnomalyResultDAO();\n+    this.evaluationDAO = DAORegistry.getInstance().getEvaluationManager();\n+\n+    MetricConfigManager metricDAO = DAORegistry.getInstance().getMetricConfigDAO();\n+    DatasetConfigManager datasetDAO = DAORegistry.getInstance().getDatasetConfigDAO();\n+    EventManager eventDAO = DAORegistry.getInstance().getEventDAO();\n+\n+    TimeSeriesLoader timeseriesLoader =\n+        new DefaultTimeSeriesLoader(metricDAO, datasetDAO,\n+            ThirdEyeCacheRegistry.getInstance().getQueryCache(), ThirdEyeCacheRegistry.getInstance().getTimeSeriesCache());\n+\n+    AggregationLoader aggregationLoader =\n+        new DefaultAggregationLoader(metricDAO, datasetDAO,\n+            ThirdEyeCacheRegistry.getInstance().getQueryCache(),\n+            ThirdEyeCacheRegistry.getInstance().getDatasetMaxDataTimeCache());\n+\n+    this.provider = new DefaultDataProvider(metricDAO, datasetDAO, eventDAO, this.anomalyDAO, this.evaluationDAO,\n+        timeseriesLoader, aggregationLoader, new DetectionPipelineLoader(), TimeSeriesCacheBuilder.getInstance(),\n+        AnomaliesCacheBuilder.getInstance());\n+  }\n+\n+  public DatasetSlaTaskRunner(DetectionConfigManager detectionDAO, MergedAnomalyResultManager anomalyDAO,\n+      EvaluationManager evaluationDAO, DataProvider provider) {\n+    this.detectionDAO = detectionDAO;\n+    this.anomalyDAO = anomalyDAO;\n+    this.evaluationDAO = evaluationDAO;\n+    this.provider = provider;\n+  }\n+\n+  @Override\n+  public List<TaskResult> execute(TaskInfo taskInfo, TaskContext taskContext) throws Exception {\n+    dataAvailabilityTaskCounter.inc();\n+\n+    try {\n+      DetectionPipelineTaskInfo info = (DetectionPipelineTaskInfo) taskInfo;\n+\n+      DetectionConfigDTO config = this.detectionDAO.findById(info.getConfigId());\n+      if (config == null) {\n+        throw new IllegalArgumentException(String.format(\"Could not resolve config id %d\", info.getConfigId()));\n+      }\n+\n+      LOG.info(\"Check data sla for config {} between {} and {}\", config.getId(), info.getStart(), info.getEnd());\n+      Map<String, Object> metricUrnToSlaMap = config.getDataSLAProperties();\n+      if (MapUtils.isNotEmpty(metricUrnToSlaMap)) {\n+        for (Map.Entry<String, Object> metricUrnToSlaMapEntry : metricUrnToSlaMap.entrySet()) {\n+          MetricEntity me = MetricEntity.fromURN(metricUrnToSlaMapEntry.getKey());\n+          MetricConfigDTO\n+              metricConfigDTO = this.provider.fetchMetrics(Collections.singletonList(me.getId())).get(me.getId());\n+          Map<String, DatasetConfigDTO> datasetToConfigMap = provider.fetchDatasets(Collections.singletonList(metricConfigDTO.getDataset()));\n+          for (Map.Entry<String, DatasetConfigDTO> datasetSLA : datasetToConfigMap.entrySet()) {\n+            try {\n+              runSLACheck(me, datasetSLA.getValue(), info, ConfigUtils.getMap(metricUrnToSlaMapEntry.getValue()));\n+            } catch (Exception e) {\n+              LOG.error(\"Failed to run sla check on metric URN %s\", datasetSLA.getKey(), e);\n+            }\n+          }\n+        }\n+      }\n+\n+      //LOG.info(\"End data availability for config {} between {} and {}. Detected {} anomalies.\", config.getId(),\n+      //    info.getStart(), info.getEnd(), anomaliesList.size());\n+      return Collections.emptyList();\n+    } catch(Exception e) {\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * Runs the data sla check for the window (info) on the given metric (me) using the configured sla properties\n+   */\n+  private void runSLACheck(MetricEntity me, DatasetConfigDTO datasetConfig, DetectionPipelineTaskInfo info,\n+      Map<String, Object> slaProps) {\n+    if (me == null || datasetConfig == null || info == null) {\n+      //nothing to check\n+      return;\n+    }\n+\n+    try {\n+      long datasetLastRefreshTime = datasetConfig.getLastRefreshTime();\n+      if (datasetLastRefreshTime <= 0) {\n+        // assume we have processed data till the current detection start\n+        datasetLastRefreshTime = info.getStart() - 1;\n+      }\n+\n+      if (isMissingData(datasetLastRefreshTime, info)) {\n+        // Double check with data source as 2 things are possible.\n+        // 1. This dataset/source may not support availability events\n+        // 2. The data availability event pipeline has some issue.\n+        // We want to measure the overall dataset availability (filters are not taken into account)\n+        MetricSlice metricSlice = MetricSlice.from(me.getId(), info.getStart(), info.getEnd(), ArrayListMultimap.<String, String>create());\n+        DataFrame dataFrame = this.provider.fetchTimeseries(Collections.singleton(metricSlice)).get(metricSlice);\n+        if (dataFrame == null || dataFrame.isEmpty()) {\n+          // no data\n+          if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+            createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);\n+          }\n+        } else {\n+          datasetLastRefreshTime = dataFrame.getDoubles(\"timestamp\").max().longValue();\n+          if (isPartialData(datasetLastRefreshTime, info, datasetConfig)) {\n+            if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+              createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);\n+            }\n+          }\n+        }\n+      } else if (isPartialData(datasetLastRefreshTime, info, datasetConfig)) {\n+        // Optimize for the common case - the common case is that the data availability events are arriving\n+        // correctly and we need not re-fetch the data to double check.\n+        if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+          createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);\n+        }\n+      }\n+    } catch (Exception e) {\n+      LOG.error(String.format(\"Failed to run sla check on metric URN %s\", me.getUrn()), e);\n+    }\n+  }\n+\n+  /**\n+   * We say the data is missing we do not have data in the sla detection window.\n+   * Or more specifically if the dataset watermark is below the sla detection window.\n+   */\n+  private boolean isMissingData(long datasetLastRefreshTime, DetectionPipelineTaskInfo info) {\n+    return datasetLastRefreshTime < info.getStart();\n+  }\n+\n+  /**\n+   * We say the data is partial if we do not have all the data points in the sla detection window.\n+   * Or more specifically if we have at least 1 data-point missing in the sla detection window.\n+   *\n+   * For example:\n+   * Assume that the data is delayed and our current sla detection window is [1st Feb to 3rd Feb). During this scan,\n+   * let's say data for 1st Feb arrives. Now, we have a situation where partial data is present. In other words, in the\n+   * current sla detection window [1st to 3rd Feb) we have data for 1st Feb but data for 2nd Feb is missing. This is the\n+   * partial data scenario.\n+   */\n+  private boolean isPartialData(long datasetLastRefreshTime, DetectionPipelineTaskInfo info, DatasetConfigDTO datasetConfig) {\n+    long granularity = datasetConfig.bucketTimeGranularity().toMillis();\n+    return (info.getEnd() - datasetLastRefreshTime) * 1.0 / granularity > 1;\n+  }\n+\n+  /**\n+   * Validates if the data is delayed or not based on the user specified SLA configuration\n+   */\n+  private boolean hasMissedSLA(long datasetLastRefreshTime, Map<String, Object> slaProps, long slaDetectionEndTime) {\n+    // fetch the user configured SLA, otherwise default 3_DAYS.\n+    long delay = TimeGranularity.fromString(MapUtils.getString(slaProps,  \"sla\", DEFAULT_DATA_SLA))\n+        .toPeriod().toStandardDuration().getMillis();\n+\n+    return (slaDetectionEndTime - datasetLastRefreshTime) >= delay;", "originalCommit": "722322b51c1f62f0dd9dde7aadec9fb34574fb40", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODQ4NjMyNQ==", "url": "https://github.com/apache/pinot/pull/5200#discussion_r408486325", "bodyText": "The default dataset delay logic will apply when we automatically onboard all the alerts for SLA detection. The yaml translation logic needs to take care of it.\nHere, we expect that the sla is defined by the user and we will strictly adhere to it.", "author": "akshayrai", "createdAt": "2020-04-14T23:05:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjcwMTk4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjcwNjg3Mg==", "url": "https://github.com/apache/pinot/pull/5200#discussion_r402706872", "bodyText": "What we have so complicated logic here? Let's find some time to discuss offline.", "author": "xiaohui-sun", "createdAt": "2020-04-03T02:48:23Z", "path": "thirdeye/thirdeye-pinot/src/main/java/org/apache/pinot/thirdeye/detection/datasla/DatasetSlaTaskRunner.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.pinot.thirdeye.detection.datasla;\n+\n+import com.google.common.collect.ArrayListMultimap;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.pinot.thirdeye.anomaly.AnomalyType;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskContext;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskInfo;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskResult;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskRunner;\n+import org.apache.pinot.thirdeye.common.time.TimeGranularity;\n+import org.apache.pinot.thirdeye.dataframe.DataFrame;\n+import org.apache.pinot.thirdeye.dataframe.util.MetricSlice;\n+import org.apache.pinot.thirdeye.datalayer.bao.DatasetConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.DetectionConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.EvaluationManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.EventManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.MergedAnomalyResultManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.MetricConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.dto.DatasetConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.DetectionConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.MergedAnomalyResultDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.MetricConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.pojo.MergedAnomalyResultBean;\n+import org.apache.pinot.thirdeye.datasource.DAORegistry;\n+import org.apache.pinot.thirdeye.datasource.ThirdEyeCacheRegistry;\n+import org.apache.pinot.thirdeye.datasource.loader.AggregationLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.DefaultAggregationLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.DefaultTimeSeriesLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.TimeSeriesLoader;\n+import org.apache.pinot.thirdeye.detection.ConfigUtils;\n+import org.apache.pinot.thirdeye.detection.DataProvider;\n+import org.apache.pinot.thirdeye.detection.DefaultDataProvider;\n+import org.apache.pinot.thirdeye.detection.DetectionPipelineLoader;\n+import org.apache.pinot.thirdeye.detection.DetectionPipelineTaskInfo;\n+import org.apache.pinot.thirdeye.detection.DetectionUtils;\n+import org.apache.pinot.thirdeye.detection.cache.builder.AnomaliesCacheBuilder;\n+import org.apache.pinot.thirdeye.detection.cache.builder.TimeSeriesCacheBuilder;\n+import org.apache.pinot.thirdeye.rootcause.impl.MetricEntity;\n+import org.joda.time.DateTime;\n+import org.joda.time.DateTimeZone;\n+import org.joda.time.Period;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.pinot.thirdeye.anomaly.utils.ThirdeyeMetricsUtil.*;\n+\n+\n+/**\n+ * Runner that generates Data SLA anomalies. DATA_MISSING anomalies are created if\n+ * the data is not available for the sla detection window within the configured SLA.\n+ */\n+public class DatasetSlaTaskRunner implements TaskRunner {\n+  private static final Logger LOG = LoggerFactory.getLogger(DatasetSlaTaskRunner.class);\n+\n+  private final DetectionConfigManager detectionDAO;\n+  private final MergedAnomalyResultManager anomalyDAO;\n+  private final EvaluationManager evaluationDAO;\n+  private DataProvider provider;\n+\n+  private final String DEFAULT_DATA_SLA = \"3_DAYS\";\n+\n+  public DatasetSlaTaskRunner() {\n+    this.detectionDAO = DAORegistry.getInstance().getDetectionConfigManager();\n+    this.anomalyDAO = DAORegistry.getInstance().getMergedAnomalyResultDAO();\n+    this.evaluationDAO = DAORegistry.getInstance().getEvaluationManager();\n+\n+    MetricConfigManager metricDAO = DAORegistry.getInstance().getMetricConfigDAO();\n+    DatasetConfigManager datasetDAO = DAORegistry.getInstance().getDatasetConfigDAO();\n+    EventManager eventDAO = DAORegistry.getInstance().getEventDAO();\n+\n+    TimeSeriesLoader timeseriesLoader =\n+        new DefaultTimeSeriesLoader(metricDAO, datasetDAO,\n+            ThirdEyeCacheRegistry.getInstance().getQueryCache(), ThirdEyeCacheRegistry.getInstance().getTimeSeriesCache());\n+\n+    AggregationLoader aggregationLoader =\n+        new DefaultAggregationLoader(metricDAO, datasetDAO,\n+            ThirdEyeCacheRegistry.getInstance().getQueryCache(),\n+            ThirdEyeCacheRegistry.getInstance().getDatasetMaxDataTimeCache());\n+\n+    this.provider = new DefaultDataProvider(metricDAO, datasetDAO, eventDAO, this.anomalyDAO, this.evaluationDAO,\n+        timeseriesLoader, aggregationLoader, new DetectionPipelineLoader(), TimeSeriesCacheBuilder.getInstance(),\n+        AnomaliesCacheBuilder.getInstance());\n+  }\n+\n+  public DatasetSlaTaskRunner(DetectionConfigManager detectionDAO, MergedAnomalyResultManager anomalyDAO,\n+      EvaluationManager evaluationDAO, DataProvider provider) {\n+    this.detectionDAO = detectionDAO;\n+    this.anomalyDAO = anomalyDAO;\n+    this.evaluationDAO = evaluationDAO;\n+    this.provider = provider;\n+  }\n+\n+  @Override\n+  public List<TaskResult> execute(TaskInfo taskInfo, TaskContext taskContext) throws Exception {\n+    dataAvailabilityTaskCounter.inc();\n+\n+    try {\n+      DetectionPipelineTaskInfo info = (DetectionPipelineTaskInfo) taskInfo;\n+\n+      DetectionConfigDTO config = this.detectionDAO.findById(info.getConfigId());\n+      if (config == null) {\n+        throw new IllegalArgumentException(String.format(\"Could not resolve config id %d\", info.getConfigId()));\n+      }\n+\n+      LOG.info(\"Check data sla for config {} between {} and {}\", config.getId(), info.getStart(), info.getEnd());\n+      Map<String, Object> metricUrnToSlaMap = config.getDataSLAProperties();\n+      if (MapUtils.isNotEmpty(metricUrnToSlaMap)) {\n+        for (Map.Entry<String, Object> metricUrnToSlaMapEntry : metricUrnToSlaMap.entrySet()) {\n+          MetricEntity me = MetricEntity.fromURN(metricUrnToSlaMapEntry.getKey());\n+          MetricConfigDTO\n+              metricConfigDTO = this.provider.fetchMetrics(Collections.singletonList(me.getId())).get(me.getId());\n+          Map<String, DatasetConfigDTO> datasetToConfigMap = provider.fetchDatasets(Collections.singletonList(metricConfigDTO.getDataset()));\n+          for (Map.Entry<String, DatasetConfigDTO> datasetSLA : datasetToConfigMap.entrySet()) {\n+            try {\n+              runSLACheck(me, datasetSLA.getValue(), info, ConfigUtils.getMap(metricUrnToSlaMapEntry.getValue()));\n+            } catch (Exception e) {\n+              LOG.error(\"Failed to run sla check on metric URN %s\", datasetSLA.getKey(), e);\n+            }\n+          }\n+        }\n+      }\n+\n+      //LOG.info(\"End data availability for config {} between {} and {}. Detected {} anomalies.\", config.getId(),\n+      //    info.getStart(), info.getEnd(), anomaliesList.size());\n+      return Collections.emptyList();\n+    } catch(Exception e) {\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * Runs the data sla check for the window (info) on the given metric (me) using the configured sla properties\n+   */\n+  private void runSLACheck(MetricEntity me, DatasetConfigDTO datasetConfig, DetectionPipelineTaskInfo info,\n+      Map<String, Object> slaProps) {\n+    if (me == null || datasetConfig == null || info == null) {\n+      //nothing to check\n+      return;\n+    }\n+\n+    try {\n+      long datasetLastRefreshTime = datasetConfig.getLastRefreshTime();\n+      if (datasetLastRefreshTime <= 0) {\n+        // assume we have processed data till the current detection start\n+        datasetLastRefreshTime = info.getStart() - 1;\n+      }\n+\n+      if (isMissingData(datasetLastRefreshTime, info)) {\n+        // Double check with data source as 2 things are possible.\n+        // 1. This dataset/source may not support availability events\n+        // 2. The data availability event pipeline has some issue.\n+        // We want to measure the overall dataset availability (filters are not taken into account)\n+        MetricSlice metricSlice = MetricSlice.from(me.getId(), info.getStart(), info.getEnd(), ArrayListMultimap.<String, String>create());\n+        DataFrame dataFrame = this.provider.fetchTimeseries(Collections.singleton(metricSlice)).get(metricSlice);\n+        if (dataFrame == null || dataFrame.isEmpty()) {\n+          // no data\n+          if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+            createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);\n+          }\n+        } else {\n+          datasetLastRefreshTime = dataFrame.getDoubles(\"timestamp\").max().longValue();\n+          if (isPartialData(datasetLastRefreshTime, info, datasetConfig)) {\n+            if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+              createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);\n+            }\n+          }\n+        }\n+      } else if (isPartialData(datasetLastRefreshTime, info, datasetConfig)) {\n+        // Optimize for the common case - the common case is that the data availability events are arriving\n+        // correctly and we need not re-fetch the data to double check.\n+        if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+          createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);", "originalCommit": "722322b51c1f62f0dd9dde7aadec9fb34574fb40", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODQ4NTg0Ng==", "url": "https://github.com/apache/pinot/pull/5200#discussion_r408485846", "bodyText": "discussed offline", "author": "akshayrai", "createdAt": "2020-04-14T23:03:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjcwNjg3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjcwNzkzMA==", "url": "https://github.com/apache/pinot/pull/5200#discussion_r402707930", "bodyText": "Why this is called \"merge\"? It is only dedup?", "author": "xiaohui-sun", "createdAt": "2020-04-03T02:52:23Z", "path": "thirdeye/thirdeye-pinot/src/main/java/org/apache/pinot/thirdeye/detection/datasla/DatasetSlaTaskRunner.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.pinot.thirdeye.detection.datasla;\n+\n+import com.google.common.collect.ArrayListMultimap;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.commons.collections4.MapUtils;\n+import org.apache.pinot.thirdeye.anomaly.AnomalyType;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskContext;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskInfo;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskResult;\n+import org.apache.pinot.thirdeye.anomaly.task.TaskRunner;\n+import org.apache.pinot.thirdeye.common.time.TimeGranularity;\n+import org.apache.pinot.thirdeye.dataframe.DataFrame;\n+import org.apache.pinot.thirdeye.dataframe.util.MetricSlice;\n+import org.apache.pinot.thirdeye.datalayer.bao.DatasetConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.DetectionConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.EvaluationManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.EventManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.MergedAnomalyResultManager;\n+import org.apache.pinot.thirdeye.datalayer.bao.MetricConfigManager;\n+import org.apache.pinot.thirdeye.datalayer.dto.DatasetConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.DetectionConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.MergedAnomalyResultDTO;\n+import org.apache.pinot.thirdeye.datalayer.dto.MetricConfigDTO;\n+import org.apache.pinot.thirdeye.datalayer.pojo.MergedAnomalyResultBean;\n+import org.apache.pinot.thirdeye.datasource.DAORegistry;\n+import org.apache.pinot.thirdeye.datasource.ThirdEyeCacheRegistry;\n+import org.apache.pinot.thirdeye.datasource.loader.AggregationLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.DefaultAggregationLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.DefaultTimeSeriesLoader;\n+import org.apache.pinot.thirdeye.datasource.loader.TimeSeriesLoader;\n+import org.apache.pinot.thirdeye.detection.ConfigUtils;\n+import org.apache.pinot.thirdeye.detection.DataProvider;\n+import org.apache.pinot.thirdeye.detection.DefaultDataProvider;\n+import org.apache.pinot.thirdeye.detection.DetectionPipelineLoader;\n+import org.apache.pinot.thirdeye.detection.DetectionPipelineTaskInfo;\n+import org.apache.pinot.thirdeye.detection.DetectionUtils;\n+import org.apache.pinot.thirdeye.detection.cache.builder.AnomaliesCacheBuilder;\n+import org.apache.pinot.thirdeye.detection.cache.builder.TimeSeriesCacheBuilder;\n+import org.apache.pinot.thirdeye.rootcause.impl.MetricEntity;\n+import org.joda.time.DateTime;\n+import org.joda.time.DateTimeZone;\n+import org.joda.time.Period;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.pinot.thirdeye.anomaly.utils.ThirdeyeMetricsUtil.*;\n+\n+\n+/**\n+ * Runner that generates Data SLA anomalies. DATA_MISSING anomalies are created if\n+ * the data is not available for the sla detection window within the configured SLA.\n+ */\n+public class DatasetSlaTaskRunner implements TaskRunner {\n+  private static final Logger LOG = LoggerFactory.getLogger(DatasetSlaTaskRunner.class);\n+\n+  private final DetectionConfigManager detectionDAO;\n+  private final MergedAnomalyResultManager anomalyDAO;\n+  private final EvaluationManager evaluationDAO;\n+  private DataProvider provider;\n+\n+  private final String DEFAULT_DATA_SLA = \"3_DAYS\";\n+\n+  public DatasetSlaTaskRunner() {\n+    this.detectionDAO = DAORegistry.getInstance().getDetectionConfigManager();\n+    this.anomalyDAO = DAORegistry.getInstance().getMergedAnomalyResultDAO();\n+    this.evaluationDAO = DAORegistry.getInstance().getEvaluationManager();\n+\n+    MetricConfigManager metricDAO = DAORegistry.getInstance().getMetricConfigDAO();\n+    DatasetConfigManager datasetDAO = DAORegistry.getInstance().getDatasetConfigDAO();\n+    EventManager eventDAO = DAORegistry.getInstance().getEventDAO();\n+\n+    TimeSeriesLoader timeseriesLoader =\n+        new DefaultTimeSeriesLoader(metricDAO, datasetDAO,\n+            ThirdEyeCacheRegistry.getInstance().getQueryCache(), ThirdEyeCacheRegistry.getInstance().getTimeSeriesCache());\n+\n+    AggregationLoader aggregationLoader =\n+        new DefaultAggregationLoader(metricDAO, datasetDAO,\n+            ThirdEyeCacheRegistry.getInstance().getQueryCache(),\n+            ThirdEyeCacheRegistry.getInstance().getDatasetMaxDataTimeCache());\n+\n+    this.provider = new DefaultDataProvider(metricDAO, datasetDAO, eventDAO, this.anomalyDAO, this.evaluationDAO,\n+        timeseriesLoader, aggregationLoader, new DetectionPipelineLoader(), TimeSeriesCacheBuilder.getInstance(),\n+        AnomaliesCacheBuilder.getInstance());\n+  }\n+\n+  public DatasetSlaTaskRunner(DetectionConfigManager detectionDAO, MergedAnomalyResultManager anomalyDAO,\n+      EvaluationManager evaluationDAO, DataProvider provider) {\n+    this.detectionDAO = detectionDAO;\n+    this.anomalyDAO = anomalyDAO;\n+    this.evaluationDAO = evaluationDAO;\n+    this.provider = provider;\n+  }\n+\n+  @Override\n+  public List<TaskResult> execute(TaskInfo taskInfo, TaskContext taskContext) throws Exception {\n+    dataAvailabilityTaskCounter.inc();\n+\n+    try {\n+      DetectionPipelineTaskInfo info = (DetectionPipelineTaskInfo) taskInfo;\n+\n+      DetectionConfigDTO config = this.detectionDAO.findById(info.getConfigId());\n+      if (config == null) {\n+        throw new IllegalArgumentException(String.format(\"Could not resolve config id %d\", info.getConfigId()));\n+      }\n+\n+      LOG.info(\"Check data sla for config {} between {} and {}\", config.getId(), info.getStart(), info.getEnd());\n+      Map<String, Object> metricUrnToSlaMap = config.getDataSLAProperties();\n+      if (MapUtils.isNotEmpty(metricUrnToSlaMap)) {\n+        for (Map.Entry<String, Object> metricUrnToSlaMapEntry : metricUrnToSlaMap.entrySet()) {\n+          MetricEntity me = MetricEntity.fromURN(metricUrnToSlaMapEntry.getKey());\n+          MetricConfigDTO\n+              metricConfigDTO = this.provider.fetchMetrics(Collections.singletonList(me.getId())).get(me.getId());\n+          Map<String, DatasetConfigDTO> datasetToConfigMap = provider.fetchDatasets(Collections.singletonList(metricConfigDTO.getDataset()));\n+          for (Map.Entry<String, DatasetConfigDTO> datasetSLA : datasetToConfigMap.entrySet()) {\n+            try {\n+              runSLACheck(me, datasetSLA.getValue(), info, ConfigUtils.getMap(metricUrnToSlaMapEntry.getValue()));\n+            } catch (Exception e) {\n+              LOG.error(\"Failed to run sla check on metric URN %s\", datasetSLA.getKey(), e);\n+            }\n+          }\n+        }\n+      }\n+\n+      //LOG.info(\"End data availability for config {} between {} and {}. Detected {} anomalies.\", config.getId(),\n+      //    info.getStart(), info.getEnd(), anomaliesList.size());\n+      return Collections.emptyList();\n+    } catch(Exception e) {\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * Runs the data sla check for the window (info) on the given metric (me) using the configured sla properties\n+   */\n+  private void runSLACheck(MetricEntity me, DatasetConfigDTO datasetConfig, DetectionPipelineTaskInfo info,\n+      Map<String, Object> slaProps) {\n+    if (me == null || datasetConfig == null || info == null) {\n+      //nothing to check\n+      return;\n+    }\n+\n+    try {\n+      long datasetLastRefreshTime = datasetConfig.getLastRefreshTime();\n+      if (datasetLastRefreshTime <= 0) {\n+        // assume we have processed data till the current detection start\n+        datasetLastRefreshTime = info.getStart() - 1;\n+      }\n+\n+      if (isMissingData(datasetLastRefreshTime, info)) {\n+        // Double check with data source as 2 things are possible.\n+        // 1. This dataset/source may not support availability events\n+        // 2. The data availability event pipeline has some issue.\n+        // We want to measure the overall dataset availability (filters are not taken into account)\n+        MetricSlice metricSlice = MetricSlice.from(me.getId(), info.getStart(), info.getEnd(), ArrayListMultimap.<String, String>create());\n+        DataFrame dataFrame = this.provider.fetchTimeseries(Collections.singleton(metricSlice)).get(metricSlice);\n+        if (dataFrame == null || dataFrame.isEmpty()) {\n+          // no data\n+          if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+            createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);\n+          }\n+        } else {\n+          datasetLastRefreshTime = dataFrame.getDoubles(\"timestamp\").max().longValue();\n+          if (isPartialData(datasetLastRefreshTime, info, datasetConfig)) {\n+            if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+              createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);\n+            }\n+          }\n+        }\n+      } else if (isPartialData(datasetLastRefreshTime, info, datasetConfig)) {\n+        // Optimize for the common case - the common case is that the data availability events are arriving\n+        // correctly and we need not re-fetch the data to double check.\n+        if (hasMissedSLA(datasetLastRefreshTime, slaProps, info.getEnd())) {\n+          createDataSLAAnomaly(datasetLastRefreshTime + 1, info.getEnd(), info.getConfigId(), datasetConfig, slaProps);\n+        }\n+      }\n+    } catch (Exception e) {\n+      LOG.error(String.format(\"Failed to run sla check on metric URN %s\", me.getUrn()), e);\n+    }\n+  }\n+\n+  /**\n+   * We say the data is missing we do not have data in the sla detection window.\n+   * Or more specifically if the dataset watermark is below the sla detection window.\n+   */\n+  private boolean isMissingData(long datasetLastRefreshTime, DetectionPipelineTaskInfo info) {\n+    return datasetLastRefreshTime < info.getStart();\n+  }\n+\n+  /**\n+   * We say the data is partial if we do not have all the data points in the sla detection window.\n+   * Or more specifically if we have at least 1 data-point missing in the sla detection window.\n+   *\n+   * For example:\n+   * Assume that the data is delayed and our current sla detection window is [1st Feb to 3rd Feb). During this scan,\n+   * let's say data for 1st Feb arrives. Now, we have a situation where partial data is present. In other words, in the\n+   * current sla detection window [1st to 3rd Feb) we have data for 1st Feb but data for 2nd Feb is missing. This is the\n+   * partial data scenario.\n+   */\n+  private boolean isPartialData(long datasetLastRefreshTime, DetectionPipelineTaskInfo info, DatasetConfigDTO datasetConfig) {\n+    long granularity = datasetConfig.bucketTimeGranularity().toMillis();\n+    return (info.getEnd() - datasetLastRefreshTime) * 1.0 / granularity > 1;\n+  }\n+\n+  /**\n+   * Validates if the data is delayed or not based on the user specified SLA configuration\n+   */\n+  private boolean hasMissedSLA(long datasetLastRefreshTime, Map<String, Object> slaProps, long slaDetectionEndTime) {\n+    // fetch the user configured SLA, otherwise default 3_DAYS.\n+    long delay = TimeGranularity.fromString(MapUtils.getString(slaProps,  \"sla\", DEFAULT_DATA_SLA))\n+        .toPeriod().toStandardDuration().getMillis();\n+\n+    return (slaDetectionEndTime - datasetLastRefreshTime) >= delay;\n+  }\n+\n+  /**\n+   * Align and round off start time to the upper boundary of the granularity\n+   */\n+  private static long alignToUpperBoundary(long start, DatasetConfigDTO datasetConfig) {\n+    Period granularityPeriod = datasetConfig.bucketTimeGranularity().toPeriod();\n+    DateTimeZone timezone = DateTimeZone.forID(datasetConfig.getTimezone());\n+    DateTime startTime = new DateTime(start - 1, timezone).plus(granularityPeriod);\n+    return startTime.getMillis() / granularityPeriod.toStandardDuration().getMillis() * granularityPeriod.toStandardDuration().getMillis();\n+  }\n+\n+  /**\n+   * Merges one DATA_MISSING anomaly with remaining existing anomalies.\n+   */\n+  private void mergeSLAAnomalies(MergedAnomalyResultDTO anomaly, List<MergedAnomalyResultDTO> existingAnomalies) {\n+    // Extract the parent SLA anomaly. We can have only 1 parent DATA_MISSING anomaly in a window.\n+    existingAnomalies.removeIf(MergedAnomalyResultBean::isChild);\n+    MergedAnomalyResultDTO existingParentSLAAnomaly = existingAnomalies.get(0);\n+\n+    if (isDuplicateSLAAnomaly(existingParentSLAAnomaly, anomaly)) {\n+      // Ensure anomalies are not duplicated. Ignore and return.\n+      // Example: daily data with hourly cron should generate only 1 sla alert if data is missing\n+      return;\n+    }\n+    existingParentSLAAnomaly.setChild(true);\n+    anomaly.setChild(false);", "originalCommit": "722322b51c1f62f0dd9dde7aadec9fb34574fb40", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODQ4NTg5Nw==", "url": "https://github.com/apache/pinot/pull/5200#discussion_r408485897", "bodyText": "Discussed offline. When 2 sla anomalies overlap (not duplicate) we merge them into a parent child relationship.", "author": "akshayrai", "createdAt": "2020-04-14T23:03:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjcwNzkzMA=="}], "type": "inlineReview"}]}