{"pr_number": 4987, "pr_title": "Pinot batch ingestion hadoop", "pr_createdAt": "2020-01-15T12:30:18Z", "pr_url": "https://github.com/apache/pinot/pull/4987", "timeline": [{"oid": "632e21efb3745dcd5ab0e6907311c9d69dafd752", "url": "https://github.com/apache/pinot/commit/632e21efb3745dcd5ab0e6907311c9d69dafd752", "message": "Adding pinot-batch-ingestion-hadoop module", "committedDate": "2020-01-16T02:41:18Z", "type": "forcePushed"}, {"oid": "c77c5d7c517a5b032246d01dbc38f994da69503f", "url": "https://github.com/apache/pinot/commit/c77c5d7c517a5b032246d01dbc38f994da69503f", "message": "Adding pinot-batch-ingestion-hadoop module", "committedDate": "2020-01-16T05:15:20Z", "type": "forcePushed"}, {"oid": "70e42a0ef4acc2146eb8dd5d08a7aac1c5a688c0", "url": "https://github.com/apache/pinot/commit/70e42a0ef4acc2146eb8dd5d08a7aac1c5a688c0", "message": "Adding pinot-batch-ingestion-hadoop module", "committedDate": "2020-01-16T07:13:41Z", "type": "forcePushed"}, {"oid": "f3ea3ca93371461c81fb98394d8eb24519a0820d", "url": "https://github.com/apache/pinot/commit/f3ea3ca93371461c81fb98394d8eb24519a0820d", "message": "Adding pinot-batch-ingestion-hadoop module", "committedDate": "2020-01-16T08:02:34Z", "type": "forcePushed"}, {"oid": "e14bd751f736c048a32270fc0f53eb266ad10c1c", "url": "https://github.com/apache/pinot/commit/e14bd751f736c048a32270fc0f53eb266ad10c1c", "message": "Adding pinot-batch-ingestion-hadoop module", "committedDate": "2020-01-16T08:05:36Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY5NTUxOA==", "url": "https://github.com/apache/pinot/pull/4987#discussion_r367695518", "bodyText": "Add the logic here and some explanation on why we are compressing the plugins directory and shipping it to mapper via distributed cache.", "author": "kishoreg", "createdAt": "2020-01-16T23:06:23Z", "path": "pinot-plugins/pinot-batch-ingestion/pinot-batch-ingestion-hadoop/src/main/java/org/apache/pinot/plugin/ingestion/batch/hadoop/HadoopSegmentGenerationJobRunner.java", "diffHunk": "@@ -0,0 +1,313 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.plugin.ingestion.batch.hadoop;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.nio.file.FileSystems;\n+import java.nio.file.PathMatcher;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import org.apache.commons.configuration.Configuration;\n+import org.apache.commons.configuration.MapConfiguration;\n+import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.Mapper;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n+import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;\n+import org.apache.pinot.common.utils.StringUtil;\n+import org.apache.pinot.common.utils.TarGzCompressionUtils;\n+import org.apache.pinot.plugin.ingestion.batch.common.SegmentGenerationUtils;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.apache.pinot.spi.filesystem.PinotFSFactory;\n+import org.apache.pinot.spi.ingestion.batch.runner.IngestionJobRunner;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.plugin.PluginManager;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.yaml.snakeyaml.Yaml;\n+\n+import static org.apache.pinot.plugin.ingestion.batch.common.SegmentGenerationUtils.PINOT_PLUGINS_TAR_GZ;\n+import static org.apache.pinot.spi.plugin.PluginManager.PLUGINS_INCLUDE_PROPERTY_NAME;\n+\n+\n+public class HadoopSegmentGenerationJobRunner extends Configured implements IngestionJobRunner, Serializable {", "originalCommit": "e14bd751f736c048a32270fc0f53eb266ad10c1c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY5NTY4Mg==", "url": "https://github.com/apache/pinot/pull/4987#discussion_r367695682", "bodyText": "check for the existence of this directory.", "author": "kishoreg", "createdAt": "2020-01-16T23:06:58Z", "path": "pinot-plugins/pinot-batch-ingestion/pinot-batch-ingestion-hadoop/src/main/java/org/apache/pinot/plugin/ingestion/batch/hadoop/HadoopSegmentGenerationJobRunner.java", "diffHunk": "@@ -0,0 +1,313 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.plugin.ingestion.batch.hadoop;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.nio.file.FileSystems;\n+import java.nio.file.PathMatcher;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import org.apache.commons.configuration.Configuration;\n+import org.apache.commons.configuration.MapConfiguration;\n+import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.Mapper;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n+import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;\n+import org.apache.pinot.common.utils.StringUtil;\n+import org.apache.pinot.common.utils.TarGzCompressionUtils;\n+import org.apache.pinot.plugin.ingestion.batch.common.SegmentGenerationUtils;\n+import org.apache.pinot.spi.filesystem.PinotFS;\n+import org.apache.pinot.spi.filesystem.PinotFSFactory;\n+import org.apache.pinot.spi.ingestion.batch.runner.IngestionJobRunner;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotClusterSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.PinotFSSpec;\n+import org.apache.pinot.spi.ingestion.batch.spec.SegmentGenerationJobSpec;\n+import org.apache.pinot.spi.plugin.PluginManager;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.yaml.snakeyaml.Yaml;\n+\n+import static org.apache.pinot.plugin.ingestion.batch.common.SegmentGenerationUtils.PINOT_PLUGINS_TAR_GZ;\n+import static org.apache.pinot.spi.plugin.PluginManager.PLUGINS_INCLUDE_PROPERTY_NAME;\n+\n+\n+public class HadoopSegmentGenerationJobRunner extends Configured implements IngestionJobRunner, Serializable {\n+\n+  public static final String SEGMENT_GENERATION_JOB_SPEC = \"segmentGenerationJobSpec\";\n+  private static final Logger LOGGER = LoggerFactory.getLogger(HadoopSegmentGenerationJobRunner.class);\n+  private static final String DEPS_JAR_DIR = \"dependencyJarDir\";\n+  private static final String STAGING_DIR = \"stagingDir\";\n+  private static final String SEGMENT_TAR_DIR = \"segmentTar\";\n+\n+  private SegmentGenerationJobSpec _spec;\n+\n+  public HadoopSegmentGenerationJobRunner() {\n+    setConf(new org.apache.hadoop.conf.Configuration());\n+    getConf().set(\"mapreduce.job.user.classpath.first\", \"true\");\n+  }\n+\n+  public HadoopSegmentGenerationJobRunner(SegmentGenerationJobSpec spec) {\n+    this();\n+    init(spec);\n+  }\n+\n+  @Override\n+  public void init(SegmentGenerationJobSpec spec) {\n+    _spec = spec;\n+    if (_spec.getInputDirURI() == null) {\n+      throw new RuntimeException(\"Missing property 'inputDirURI' in 'jobSpec' file\");\n+    }\n+    if (_spec.getOutputDirURI() == null) {\n+      throw new RuntimeException(\"Missing property 'outputDirURI' in 'jobSpec' file\");\n+    }\n+    if (_spec.getRecordReaderSpec() == null) {\n+      throw new RuntimeException(\"Missing property 'recordReaderSpec' in 'jobSpec' file\");\n+    }\n+    if (_spec.getTableSpec() == null) {\n+      throw new RuntimeException(\"Missing property 'tableSpec' in 'jobSpec' file\");\n+    }\n+    if (_spec.getTableSpec().getTableName() == null) {\n+      throw new RuntimeException(\"Missing property 'tableName' in 'tableSpec'\");\n+    }\n+    if (_spec.getTableSpec().getSchemaURI() == null) {\n+      if (_spec.getPinotClusterSpecs() == null || _spec.getPinotClusterSpecs().length == 0) {\n+        throw new RuntimeException(\"Missing property 'schemaURI' in 'tableSpec'\");\n+      }\n+      PinotClusterSpec pinotClusterSpec = _spec.getPinotClusterSpecs()[0];\n+      String schemaURI = SegmentGenerationUtils\n+          .generateSchemaURI(pinotClusterSpec.getControllerURI(), _spec.getTableSpec().getTableName());\n+      _spec.getTableSpec().setSchemaURI(schemaURI);\n+    }\n+    if (_spec.getTableSpec().getTableConfigURI() == null) {\n+      if (_spec.getPinotClusterSpecs() == null || _spec.getPinotClusterSpecs().length == 0) {\n+        throw new RuntimeException(\"Missing property 'tableConfigURI' in 'tableSpec'\");\n+      }\n+      PinotClusterSpec pinotClusterSpec = _spec.getPinotClusterSpecs()[0];\n+      String tableConfigURI = SegmentGenerationUtils\n+          .generateTableConfigURI(pinotClusterSpec.getControllerURI(), _spec.getTableSpec().getTableName());\n+      _spec.getTableSpec().setTableConfigURI(tableConfigURI);\n+    }\n+    if (_spec.getExecutionFrameworkSpec().getExtraConfigs() == null) {\n+      _spec.getExecutionFrameworkSpec().setExtraConfigs(new HashMap<>());\n+    }\n+  }\n+\n+  @Override\n+  public void run()\n+      throws Exception {\n+    //init all file systems\n+    List<PinotFSSpec> pinotFSSpecs = _spec.getPinotFSSpecs();\n+    for (PinotFSSpec pinotFSSpec : pinotFSSpecs) {\n+      Configuration config = new MapConfiguration(pinotFSSpec.getConfigs());\n+      PinotFSFactory.register(pinotFSSpec.getScheme(), pinotFSSpec.getClassName(), config);\n+    }\n+\n+    //Get pinotFS for input\n+    URI inputDirURI = new URI(_spec.getInputDirURI());\n+    if (inputDirURI.getScheme() == null) {\n+      inputDirURI = new File(_spec.getInputDirURI()).toURI();\n+    }\n+    PinotFS inputDirFS = PinotFSFactory.create(inputDirURI.getScheme());\n+\n+    //Get outputFS for writing output pinot segments\n+    URI outputDirURI = new URI(_spec.getOutputDirURI());\n+    if (outputDirURI.getScheme() == null) {\n+      outputDirURI = new File(_spec.getOutputDirURI()).toURI();\n+    }\n+    PinotFS outputDirFS = PinotFSFactory.create(outputDirURI.getScheme());\n+    outputDirFS.mkdir(outputDirURI);\n+\n+    //Get staging directory for temporary output pinot segments\n+    String stagingDir = _spec.getExecutionFrameworkSpec().getExtraConfigs().get(STAGING_DIR);\n+    Preconditions.checkNotNull(stagingDir, \"Please set config: stagingDir under 'executionFrameworkSpec.extraConfigs'\");\n+    URI stagingDirURI = URI.create(stagingDir);\n+    if (stagingDirURI.getScheme() == null) {\n+      stagingDirURI = new File(stagingDir).toURI();\n+    }\n+    if (!outputDirURI.getScheme().equals(stagingDirURI.getScheme())) {\n+      throw new RuntimeException(String\n+          .format(\"The scheme of staging directory URI [%s] and output directory URI [%s] has to be same.\",\n+              stagingDirURI, outputDirURI));\n+    }\n+    outputDirFS.mkdir(stagingDirURI);\n+    Path stagingInputDir = new Path(stagingDirURI.toString(), \"input\");\n+    outputDirFS.mkdir(stagingInputDir.toUri());\n+    Path stagingSegmentTarUri = new Path(stagingDirURI.toString(), SEGMENT_TAR_DIR);\n+    outputDirFS.mkdir(stagingSegmentTarUri.toUri());\n+\n+    //Get list of files to process\n+    String[] files = inputDirFS.listFiles(inputDirURI, true);\n+\n+    //TODO: sort input files based on creation time\n+    List<String> filteredFiles = new ArrayList<>();\n+    PathMatcher includeFilePathMatcher = null;\n+    if (_spec.getIncludeFileNamePattern() != null) {\n+      includeFilePathMatcher = FileSystems.getDefault().getPathMatcher(_spec.getIncludeFileNamePattern());\n+    }\n+    PathMatcher excludeFilePathMatcher = null;\n+    if (_spec.getExcludeFileNamePattern() != null) {\n+      excludeFilePathMatcher = FileSystems.getDefault().getPathMatcher(_spec.getExcludeFileNamePattern());\n+    }\n+\n+    for (String file : files) {\n+      if (includeFilePathMatcher != null) {\n+        if (!includeFilePathMatcher.matches(Paths.get(file))) {\n+          continue;\n+        }\n+      }\n+      if (excludeFilePathMatcher != null) {\n+        if (excludeFilePathMatcher.matches(Paths.get(file))) {\n+          continue;\n+        }\n+      }\n+      if (!inputDirFS.isDirectory(new URI(file))) {\n+        filteredFiles.add(file);\n+      }\n+    }\n+\n+    int numDataFiles = filteredFiles.size();\n+    if (numDataFiles == 0) {\n+      String errorMessage = String\n+          .format(\"No data file founded in [%s], with include file pattern: [%s] and exclude file  pattern [%s]\",\n+              _spec.getInputDirURI(), _spec.getIncludeFileNamePattern(), _spec.getExcludeFileNamePattern());\n+      LOGGER.error(errorMessage);\n+      throw new RuntimeException(errorMessage);\n+    } else {\n+      LOGGER.info(\"Creating segments with data files: {}\", filteredFiles);\n+      for (int i = 0; i < numDataFiles; i++) {\n+        String dataFilePath = filteredFiles.get(i);\n+\n+        File localFile = new File(\"tmp\");\n+        try (DataOutputStream dataOutputStream = new DataOutputStream(new FileOutputStream(localFile))) {\n+          dataOutputStream.write(StringUtil.encodeUtf8(dataFilePath + \" \" + i));\n+          dataOutputStream.flush();\n+          outputDirFS.copyFromLocalFile(localFile, new Path(stagingInputDir, Integer.toString(i)).toUri());\n+        }\n+      }\n+    }\n+\n+    try {\n+      // Set up the job\n+      Job job = Job.getInstance(getConf());\n+      job.setJarByClass(getClass());\n+      job.setJobName(getClass().getName());\n+\n+      org.apache.hadoop.conf.Configuration jobConf = job.getConfiguration();\n+      String hadoopTokenFileLocation = System.getenv(\"HADOOP_TOKEN_FILE_LOCATION\");\n+      if (hadoopTokenFileLocation != null) {\n+        jobConf.set(\"mapreduce.job.credentials.binary\", hadoopTokenFileLocation);\n+      }\n+      jobConf.setInt(JobContext.NUM_MAPS, numDataFiles);\n+\n+      packPluginsToDistributedCache(job);\n+      // Add dependency jars\n+      if (_spec.getExecutionFrameworkSpec().getExtraConfigs().containsKey(DEPS_JAR_DIR)) {\n+        addDepsJarToDistributedCache(job, _spec.getExecutionFrameworkSpec().getExtraConfigs().get(DEPS_JAR_DIR));\n+      }\n+\n+      _spec.setOutputDirURI(stagingSegmentTarUri.toUri().toString());\n+      jobConf.set(SEGMENT_GENERATION_JOB_SPEC, new Yaml().dump(_spec));\n+      _spec.setOutputDirURI(outputDirURI.toString());\n+\n+      job.setMapperClass(getMapperClass());\n+      job.setNumReduceTasks(0);\n+\n+      job.setInputFormatClass(TextInputFormat.class);\n+      job.setOutputFormatClass(TextOutputFormat.class);\n+\n+      job.setMapOutputKeyClass(LongWritable.class);\n+      job.setMapOutputValueClass(Text.class);\n+\n+      FileInputFormat.addInputPath(job, stagingInputDir);\n+      FileOutputFormat.setOutputPath(job, new Path(stagingDir, \"output\"));\n+\n+      // Submit the job\n+      job.waitForCompletion(true);\n+      if (!job.isSuccessful()) {\n+        throw new RuntimeException(\"Job failed: \" + job);\n+      }\n+\n+      LOGGER.info(\"Trying to copy segment tars from staging directory: [{}] to output directory [{}]\", stagingDirURI,\n+          outputDirURI);\n+      outputDirFS.copy(new Path(stagingDir, SEGMENT_TAR_DIR).toUri(), outputDirURI);\n+    } finally {\n+      LOGGER.info(\"Trying to clean up staging directory: [{}]\", stagingDirURI);\n+      outputDirFS.delete(stagingDirURI, true);\n+    }\n+  }\n+\n+  /**\n+   * Can be overridden to plug in custom mapper.\n+   */\n+  protected Class<? extends Mapper<LongWritable, Text, LongWritable, Text>> getMapperClass() {\n+    return HadoopSegmentCreationMapper.class;\n+  }\n+\n+  protected void packPluginsToDistributedCache(Job job) {\n+    String pluginsRootDir = PluginManager.get().getPluginsRootDir();\n+    File pluginsTarGzFile = new File(PINOT_PLUGINS_TAR_GZ);", "originalCommit": "e14bd751f736c048a32270fc0f53eb266ad10c1c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "4d98456410a44ea2f1bb24f940edf28ab95e4736", "url": "https://github.com/apache/pinot/commit/4d98456410a44ea2f1bb24f940edf28ab95e4736", "message": "refactor common utils", "committedDate": "2020-01-17T01:03:35Z", "type": "commit"}, {"oid": "2751cf528cc028a0acc18f051dd2e9acc080bd6c", "url": "https://github.com/apache/pinot/commit/2751cf528cc028a0acc18f051dd2e9acc080bd6c", "message": "Adding pinot-batch-ingestion-hadoop module", "committedDate": "2020-01-17T01:03:35Z", "type": "commit"}, {"oid": "c48d41ea81811dc704a22ab62d86f7f848159cf2", "url": "https://github.com/apache/pinot/commit/c48d41ea81811dc704a22ab62d86f7f848159cf2", "message": "Address comments", "committedDate": "2020-01-17T01:03:35Z", "type": "commit"}, {"oid": "c48d41ea81811dc704a22ab62d86f7f848159cf2", "url": "https://github.com/apache/pinot/commit/c48d41ea81811dc704a22ab62d86f7f848159cf2", "message": "Address comments", "committedDate": "2020-01-17T01:03:35Z", "type": "forcePushed"}]}