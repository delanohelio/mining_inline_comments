{"pr_number": 5816, "pr_title": "Enhance VarByteChunkSVForwardIndexReader to directly read from data buffer for uncompressed data", "pr_createdAt": "2020-08-05T23:21:20Z", "pr_url": "https://github.com/apache/pinot/pull/5816", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4MTU2NQ==", "url": "https://github.com/apache/pinot/pull/5816#discussion_r466081565", "bodyText": "typo; trunk -> chunk", "author": "siddharthteotia", "createdAt": "2020-08-06T00:47:17Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/index/readers/forward/VarByteChunkSVForwardIndexReader.java", "diffHunk": "@@ -19,92 +19,172 @@\n package org.apache.pinot.core.segment.index.readers.forward;\n \n import java.nio.ByteBuffer;\n+import javax.annotation.Nullable;\n import org.apache.pinot.common.utils.StringUtil;\n import org.apache.pinot.core.io.writer.impl.VarByteChunkSVForwardIndexWriter;\n import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n import org.apache.pinot.spi.data.FieldSpec.DataType;\n \n \n /**\n- * Chunk-based single-value raw (non-dictionary-encoded) forward index reader for values of  of variable length data\n- * type (STRING, BYTES).\n+ * Chunk-based single-value raw (non-dictionary-encoded) forward index reader for values of variable length data type\n+ * (STRING, BYTES).\n  * <p>For data layout, please refer to the documentation for {@link VarByteChunkSVForwardIndexWriter}\n  */\n public final class VarByteChunkSVForwardIndexReader extends BaseChunkSVForwardIndexReader {\n+  private static final int ROW_OFFSET_SIZE = VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE;\n+\n   private final int _maxChunkSize;\n \n   // Thread local (reusable) byte[] to read bytes from data file.\n   private final ThreadLocal<byte[]> _reusableBytes = ThreadLocal.withInitial(() -> new byte[_lengthOfLongestEntry]);\n \n   public VarByteChunkSVForwardIndexReader(PinotDataBuffer dataBuffer, DataType valueType) {\n     super(dataBuffer, valueType);\n-    _maxChunkSize = _numDocsPerChunk * (VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE\n-        + _lengthOfLongestEntry);\n+    _maxChunkSize = _numDocsPerChunk * (ROW_OFFSET_SIZE + _lengthOfLongestEntry);\n   }\n \n+  @Nullable\n   @Override\n   public ChunkReaderContext createContext() {\n-    return new ChunkReaderContext(_maxChunkSize);\n+    if (_isCompressed) {\n+      return new ChunkReaderContext(_maxChunkSize);\n+    } else {\n+      return null;\n+    }\n   }\n \n   @Override\n   public String getString(int docId, ChunkReaderContext context) {\n+    if (_isCompressed) {\n+      return getStringCompressed(docId, context);\n+    } else {\n+      return getStringUncompressed(docId);\n+    }\n+  }\n+\n+  /**\n+   * Helper method to read STRING value from the compressed index.\n+   */\n+  private String getStringCompressed(int docId, ChunkReaderContext context) {\n     int chunkRowId = docId % _numDocsPerChunk;\n     ByteBuffer chunkBuffer = getChunkBuffer(docId, context);\n \n-    int rowOffset =\n-        chunkBuffer.getInt(chunkRowId * VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE);\n-    int nextRowOffset = getNextRowOffset(chunkRowId, chunkBuffer);\n+    // These offsets are offset in the chunk buffer\n+    int valueStartOffset = chunkBuffer.getInt(chunkRowId * ROW_OFFSET_SIZE);\n+    int valueEndOffset = getValueEndOffset(chunkRowId, chunkBuffer);\n \n-    int length = nextRowOffset - rowOffset;\n+    int length = valueEndOffset - valueStartOffset;\n     byte[] bytes = _reusableBytes.get();\n-\n-    chunkBuffer.position(rowOffset);\n+    chunkBuffer.position(valueStartOffset);\n     chunkBuffer.get(bytes, 0, length);\n+    return StringUtil.decodeUtf8(bytes, 0, length);\n+  }\n+\n+  /**\n+   * Helper method to read STRING value from the uncompressed index.\n+   */\n+  private String getStringUncompressed(int docId) {\n+    int chunkId = docId / _numDocsPerChunk;\n+    int chunkRowId = docId % _numDocsPerChunk;\n+\n+    // These offsets are offset in the data buffer\n+    long chunkStartOffset = getChunkPosition(chunkId);\n+    long valueStartOffset = chunkStartOffset + _dataBuffer.getInt(chunkStartOffset + chunkRowId * ROW_OFFSET_SIZE);\n+    long valueEndOffset = getValueEndOffset(chunkId, chunkRowId, chunkStartOffset);\n \n+    int length = (int) (valueEndOffset - valueStartOffset);\n+    byte[] bytes = _reusableBytes.get();\n+    _dataBuffer.copyTo(valueStartOffset, bytes, 0, length);\n     return StringUtil.decodeUtf8(bytes, 0, length);\n   }\n \n   @Override\n   public byte[] getBytes(int docId, ChunkReaderContext context) {\n+    if (_isCompressed) {\n+      return getBytesCompressed(docId, context);\n+    } else {\n+      return getBytesUncompressed(docId);\n+    }\n+  }\n+\n+  /**\n+   * Helper method to read BYTES value from the compressed index.\n+   */\n+  private byte[] getBytesCompressed(int docId, ChunkReaderContext context) {\n     int chunkRowId = docId % _numDocsPerChunk;\n     ByteBuffer chunkBuffer = getChunkBuffer(docId, context);\n \n-    int rowOffset =\n-        chunkBuffer.getInt(chunkRowId * VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE);\n-    int nextRowOffset = getNextRowOffset(chunkRowId, chunkBuffer);\n+    // These offsets are offset in the chunk buffer\n+    int valueStartOffset = chunkBuffer.getInt(chunkRowId * ROW_OFFSET_SIZE);\n+    int valueEndOffset = getValueEndOffset(chunkRowId, chunkBuffer);\n \n-    int length = nextRowOffset - rowOffset;\n-    byte[] bytes = new byte[length];\n+    byte[] bytes = new byte[valueEndOffset - valueStartOffset];\n+    chunkBuffer.position(valueStartOffset);\n+    chunkBuffer.get(bytes);\n+    return bytes;\n+  }\n \n-    chunkBuffer.position(rowOffset);\n-    chunkBuffer.get(bytes, 0, length);\n+  /**\n+   * Helper method to read BYTES value from the uncompressed index.\n+   */\n+  private byte[] getBytesUncompressed(int docId) {\n+    int chunkId = docId / _numDocsPerChunk;\n+    int chunkRowId = docId % _numDocsPerChunk;\n+\n+    // These offsets are offset in the data buffer\n+    long chunkStartOffset = getChunkPosition(chunkId);\n+    long valueStartOffset = chunkStartOffset + _dataBuffer.getInt(chunkStartOffset + chunkRowId * ROW_OFFSET_SIZE);\n+    long valueEndOffset = getValueEndOffset(chunkId, chunkRowId, chunkStartOffset);\n+\n+    byte[] bytes = new byte[(int) (valueEndOffset - valueStartOffset)];\n+    _dataBuffer.copyTo(valueStartOffset, bytes);\n     return bytes;\n   }\n \n   /**\n-   * Helper method to compute the offset of next row in the chunk buffer.\n-   *\n-   * @param currentRowId Current row id within the chunk buffer.\n-   * @param chunkBuffer Chunk buffer containing the rows.\n-   *\n-   * @return Offset of next row within the chunk buffer. If current row is the last one,\n-   * chunkBuffer.limit() is returned.\n+   * Helper method to compute the end offset of the value in the chunk buffer.\n    */\n-  private int getNextRowOffset(int currentRowId, ByteBuffer chunkBuffer) {\n-    int nextRowOffset;\n+  private int getValueEndOffset(int rowId, ByteBuffer chunkBuffer) {\n+    if (rowId == _numDocsPerChunk - 1) {\n+      // Last row in the trunk", "originalCommit": "d6deb16d447bdef5356709e7583fa5e6f0f7d9d0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4Mzg1Ng==", "url": "https://github.com/apache/pinot/pull/5816#discussion_r466083856", "bodyText": "Good catch", "author": "Jackie-Jiang", "createdAt": "2020-08-06T00:55:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4MTU2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4MzUzNQ==", "url": "https://github.com/apache/pinot/pull/5816#discussion_r466083535", "bodyText": "Why the algorithm for getting endoffset or startOffset for next row is different for uncompressed?", "author": "siddharthteotia", "createdAt": "2020-08-06T00:54:50Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/segment/index/readers/forward/VarByteChunkSVForwardIndexReader.java", "diffHunk": "@@ -19,92 +19,172 @@\n package org.apache.pinot.core.segment.index.readers.forward;\n \n import java.nio.ByteBuffer;\n+import javax.annotation.Nullable;\n import org.apache.pinot.common.utils.StringUtil;\n import org.apache.pinot.core.io.writer.impl.VarByteChunkSVForwardIndexWriter;\n import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n import org.apache.pinot.spi.data.FieldSpec.DataType;\n \n \n /**\n- * Chunk-based single-value raw (non-dictionary-encoded) forward index reader for values of  of variable length data\n- * type (STRING, BYTES).\n+ * Chunk-based single-value raw (non-dictionary-encoded) forward index reader for values of variable length data type\n+ * (STRING, BYTES).\n  * <p>For data layout, please refer to the documentation for {@link VarByteChunkSVForwardIndexWriter}\n  */\n public final class VarByteChunkSVForwardIndexReader extends BaseChunkSVForwardIndexReader {\n+  private static final int ROW_OFFSET_SIZE = VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE;\n+\n   private final int _maxChunkSize;\n \n   // Thread local (reusable) byte[] to read bytes from data file.\n   private final ThreadLocal<byte[]> _reusableBytes = ThreadLocal.withInitial(() -> new byte[_lengthOfLongestEntry]);\n \n   public VarByteChunkSVForwardIndexReader(PinotDataBuffer dataBuffer, DataType valueType) {\n     super(dataBuffer, valueType);\n-    _maxChunkSize = _numDocsPerChunk * (VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE\n-        + _lengthOfLongestEntry);\n+    _maxChunkSize = _numDocsPerChunk * (ROW_OFFSET_SIZE + _lengthOfLongestEntry);\n   }\n \n+  @Nullable\n   @Override\n   public ChunkReaderContext createContext() {\n-    return new ChunkReaderContext(_maxChunkSize);\n+    if (_isCompressed) {\n+      return new ChunkReaderContext(_maxChunkSize);\n+    } else {\n+      return null;\n+    }\n   }\n \n   @Override\n   public String getString(int docId, ChunkReaderContext context) {\n+    if (_isCompressed) {\n+      return getStringCompressed(docId, context);\n+    } else {\n+      return getStringUncompressed(docId);\n+    }\n+  }\n+\n+  /**\n+   * Helper method to read STRING value from the compressed index.\n+   */\n+  private String getStringCompressed(int docId, ChunkReaderContext context) {\n     int chunkRowId = docId % _numDocsPerChunk;\n     ByteBuffer chunkBuffer = getChunkBuffer(docId, context);\n \n-    int rowOffset =\n-        chunkBuffer.getInt(chunkRowId * VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE);\n-    int nextRowOffset = getNextRowOffset(chunkRowId, chunkBuffer);\n+    // These offsets are offset in the chunk buffer\n+    int valueStartOffset = chunkBuffer.getInt(chunkRowId * ROW_OFFSET_SIZE);\n+    int valueEndOffset = getValueEndOffset(chunkRowId, chunkBuffer);\n \n-    int length = nextRowOffset - rowOffset;\n+    int length = valueEndOffset - valueStartOffset;\n     byte[] bytes = _reusableBytes.get();\n-\n-    chunkBuffer.position(rowOffset);\n+    chunkBuffer.position(valueStartOffset);\n     chunkBuffer.get(bytes, 0, length);\n+    return StringUtil.decodeUtf8(bytes, 0, length);\n+  }\n+\n+  /**\n+   * Helper method to read STRING value from the uncompressed index.\n+   */\n+  private String getStringUncompressed(int docId) {\n+    int chunkId = docId / _numDocsPerChunk;\n+    int chunkRowId = docId % _numDocsPerChunk;\n+\n+    // These offsets are offset in the data buffer\n+    long chunkStartOffset = getChunkPosition(chunkId);\n+    long valueStartOffset = chunkStartOffset + _dataBuffer.getInt(chunkStartOffset + chunkRowId * ROW_OFFSET_SIZE);\n+    long valueEndOffset = getValueEndOffset(chunkId, chunkRowId, chunkStartOffset);\n \n+    int length = (int) (valueEndOffset - valueStartOffset);\n+    byte[] bytes = _reusableBytes.get();\n+    _dataBuffer.copyTo(valueStartOffset, bytes, 0, length);\n     return StringUtil.decodeUtf8(bytes, 0, length);\n   }\n \n   @Override\n   public byte[] getBytes(int docId, ChunkReaderContext context) {\n+    if (_isCompressed) {\n+      return getBytesCompressed(docId, context);\n+    } else {\n+      return getBytesUncompressed(docId);\n+    }\n+  }\n+\n+  /**\n+   * Helper method to read BYTES value from the compressed index.\n+   */\n+  private byte[] getBytesCompressed(int docId, ChunkReaderContext context) {\n     int chunkRowId = docId % _numDocsPerChunk;\n     ByteBuffer chunkBuffer = getChunkBuffer(docId, context);\n \n-    int rowOffset =\n-        chunkBuffer.getInt(chunkRowId * VarByteChunkSVForwardIndexWriter.CHUNK_HEADER_ENTRY_ROW_OFFSET_SIZE);\n-    int nextRowOffset = getNextRowOffset(chunkRowId, chunkBuffer);\n+    // These offsets are offset in the chunk buffer\n+    int valueStartOffset = chunkBuffer.getInt(chunkRowId * ROW_OFFSET_SIZE);\n+    int valueEndOffset = getValueEndOffset(chunkRowId, chunkBuffer);\n \n-    int length = nextRowOffset - rowOffset;\n-    byte[] bytes = new byte[length];\n+    byte[] bytes = new byte[valueEndOffset - valueStartOffset];\n+    chunkBuffer.position(valueStartOffset);\n+    chunkBuffer.get(bytes);\n+    return bytes;\n+  }\n \n-    chunkBuffer.position(rowOffset);\n-    chunkBuffer.get(bytes, 0, length);\n+  /**\n+   * Helper method to read BYTES value from the uncompressed index.\n+   */\n+  private byte[] getBytesUncompressed(int docId) {\n+    int chunkId = docId / _numDocsPerChunk;\n+    int chunkRowId = docId % _numDocsPerChunk;\n+\n+    // These offsets are offset in the data buffer\n+    long chunkStartOffset = getChunkPosition(chunkId);\n+    long valueStartOffset = chunkStartOffset + _dataBuffer.getInt(chunkStartOffset + chunkRowId * ROW_OFFSET_SIZE);\n+    long valueEndOffset = getValueEndOffset(chunkId, chunkRowId, chunkStartOffset);\n+\n+    byte[] bytes = new byte[(int) (valueEndOffset - valueStartOffset)];\n+    _dataBuffer.copyTo(valueStartOffset, bytes);\n     return bytes;\n   }\n \n   /**\n-   * Helper method to compute the offset of next row in the chunk buffer.\n-   *\n-   * @param currentRowId Current row id within the chunk buffer.\n-   * @param chunkBuffer Chunk buffer containing the rows.\n-   *\n-   * @return Offset of next row within the chunk buffer. If current row is the last one,\n-   * chunkBuffer.limit() is returned.\n+   * Helper method to compute the end offset of the value in the chunk buffer.\n    */\n-  private int getNextRowOffset(int currentRowId, ByteBuffer chunkBuffer) {\n-    int nextRowOffset;\n+  private int getValueEndOffset(int rowId, ByteBuffer chunkBuffer) {\n+    if (rowId == _numDocsPerChunk - 1) {\n+      // Last row in the trunk\n+      return chunkBuffer.limit();\n+    } else {\n+      int valueEndOffset = chunkBuffer.getInt((rowId + 1) * ROW_OFFSET_SIZE);\n+      if (valueEndOffset == 0) {\n+        // Last row in the last chunk (chunk is incomplete, which stores 0 as the offset for the absent rows)\n+        return chunkBuffer.limit();\n+      } else {\n+        return valueEndOffset;\n+      }\n+    }\n+  }\n \n-    if (currentRowId == _numDocsPerChunk - 1) {\n-      // Last row in this trunk.\n-      nextRowOffset = chunkBuffer.limit();\n+  /**\n+   * Helper method to compute the end offset of the value in the data buffer.\n+   */\n+  private long getValueEndOffset(int chunkId, int chunkRowId, long chunkStartOffset) {", "originalCommit": "d6deb16d447bdef5356709e7583fa5e6f0f7d9d0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4NTEyMg==", "url": "https://github.com/apache/pinot/pull/5816#discussion_r466085122", "bodyText": "Algorithm is slightly different because with the chunkBuffer we can directly get the chunkEndOffset via chunkBuffer.limit(), which is not the case for the uncompressed one. That is why we have a branch on the last chunk.", "author": "Jackie-Jiang", "createdAt": "2020-08-06T01:00:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA4MzUzNQ=="}], "type": "inlineReview"}, {"oid": "2d12ff65d4dd547fdd85e92530811b07f69f13ba", "url": "https://github.com/apache/pinot/commit/2d12ff65d4dd547fdd85e92530811b07f69f13ba", "message": "Enhance VarByteChunkSVForwardIndexReader to directly read from data buffer for uncompressed data", "committedDate": "2020-08-06T00:58:56Z", "type": "commit"}, {"oid": "2d12ff65d4dd547fdd85e92530811b07f69f13ba", "url": "https://github.com/apache/pinot/commit/2d12ff65d4dd547fdd85e92530811b07f69f13ba", "message": "Enhance VarByteChunkSVForwardIndexReader to directly read from data buffer for uncompressed data", "committedDate": "2020-08-06T00:58:56Z", "type": "forcePushed"}]}