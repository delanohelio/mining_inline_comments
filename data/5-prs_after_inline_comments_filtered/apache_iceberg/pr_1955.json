{"pr_number": 1955, "pr_title": "Spark: Sort retained rows in DELETE FROM by file and position", "pr_createdAt": "2020-12-18T01:16:52Z", "pr_url": "https://github.com/apache/iceberg/pull/1955", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUxMDIzNA==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r545510234", "bodyText": "Needed to allow projecting _file even though it isn't in the data file.", "author": "rdblue", "createdAt": "2020-12-18T01:17:35Z", "path": "core/src/main/java/org/apache/iceberg/avro/BuildAvroProjection.java", "diffHunk": "@@ -96,7 +96,7 @@ public Schema record(Schema record, List<String> names, Iterable<Schema.Field> s\n \n       } else {\n         Preconditions.checkArgument(\n-            field.isOptional() || field.fieldId() == MetadataColumns.ROW_POSITION.fieldId(),\n+            field.isOptional() || MetadataColumns.metadataFieldIds().contains(field.fieldId()),", "originalCommit": "934a375adbf9efca155936d63f4003f94adb070b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAzMzI4NA==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546033284", "bodyText": "This reminds me we need to fix that projection bug / selection bug", "author": "RussellSpitzer", "createdAt": "2020-12-18T19:05:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUxMDIzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUxMDM3Ng==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r545510376", "bodyText": "This adds _file to the constants map so it is set in records like a partition value.", "author": "rdblue", "createdAt": "2020-12-18T01:18:02Z", "path": "core/src/main/java/org/apache/iceberg/util/PartitionUtil.java", "diffHunk": "@@ -39,13 +40,17 @@ private PartitionUtil() {\n   }\n \n   public static Map<Integer, ?> constantsMap(FileScanTask task, BiFunction<Type, Object, Object> convertConstant) {\n-    return constantsMap(task.spec(), task.file().partition(), convertConstant);\n-  }\n+    PartitionSpec spec = task.spec();\n+    StructLike partitionData = task.file().partition();\n \n-  private static Map<Integer, ?> constantsMap(PartitionSpec spec, StructLike partitionData,\n-                                              BiFunction<Type, Object, Object> convertConstant) {\n     // use java.util.HashMap because partition data may contain null values\n     Map<Integer, Object> idToConstant = new HashMap<>();\n+\n+    // add _file\n+    idToConstant.put(\n+        MetadataColumns.FILE_PATH.fieldId(),\n+        convertConstant.apply(Types.StringType.get(), task.file().path()));", "originalCommit": "934a375adbf9efca155936d63f4003f94adb070b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTY5NDU5OQ==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r545694599", "bodyText": "nit: should it be isNonMetadataColumn to indicate it is a boolean flag?", "author": "aokolnychyi", "createdAt": "2020-12-18T09:15:57Z", "path": "core/src/main/java/org/apache/iceberg/MetadataColumns.java", "diffHunk": "@@ -55,4 +55,16 @@ private MetadataColumns() {\n   public static Set<Integer> metadataFieldIds() {\n     return META_IDS;\n   }\n+\n+  public static NestedField get(String name) {\n+    return META_COLUMNS.get(name);\n+  }\n+\n+  public static boolean isMetadataColumn(String name) {\n+    return META_COLUMNS.containsKey(name);\n+  }\n+\n+  public static boolean nonMetadataColumn(String name) {", "originalCommit": "934a375adbf9efca155936d63f4003f94adb070b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk4NTQ4Ng==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r545985486", "bodyText": "Yeah, we could do that. I was following the slightly shorter Scala convention, where there are methods like nonEmpty. I prefer this way, but if others agree I'm happy to change it.", "author": "rdblue", "createdAt": "2020-12-18T17:38:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTY5NDU5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjYxMTY0OA==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546611648", "bodyText": "Let's leave it, I don't mind as long as it wasn't an oversight and there is an idea/convention being followed.", "author": "aokolnychyi", "createdAt": "2020-12-21T09:56:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTY5NDU5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTcwOTI3OQ==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r545709279", "bodyText": "Are there any cases when pruneColumns is going to be called multiple times? Should we worry about it at all?", "author": "aokolnychyi", "createdAt": "2020-12-18T09:31:06Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java", "diffHunk": "@@ -131,28 +138,52 @@ public SparkScanBuilder caseSensitive(boolean isCaseSensitive) {\n \n   @Override\n   public void pruneColumns(StructType requestedSchema) {\n-    this.requestedProjection = requestedSchema;\n+    this.requestedProjection = new StructType(Stream.of(requestedSchema.fields())\n+        .filter(field -> MetadataColumns.nonMetadataColumn(field.name()))\n+        .toArray(StructField[]::new));\n+\n+    Stream.of(requestedSchema.fields())", "originalCommit": "934a375adbf9efca155936d63f4003f94adb070b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTcwOTY5NQ==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r545709695", "bodyText": "I see we call distinct in schemaWithMetadataColumns, never mind.", "author": "aokolnychyi", "createdAt": "2020-12-18T09:31:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTcwOTI3OQ=="}], "type": "inlineReview"}, {"oid": "7a87a438309f57eb83ee21178c01f8bae336b89f", "url": "https://github.com/apache/iceberg/commit/7a87a438309f57eb83ee21178c01f8bae336b89f", "message": "Fix vectorized Parquet _file and _pos.", "committedDate": "2020-12-19T00:13:02Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjE1NzU5OQ==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546157599", "bodyText": "This is needed for cases where Arrow checks the validity buffer.", "author": "rdblue", "createdAt": "2020-12-19T00:14:55Z", "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -381,9 +382,13 @@ public VectorHolder read(VectorHolder reuse, int numValsToRead) {\n         for (int i = 0; i < numValsToRead; i += 1) {\n           vec.getDataBuffer().setLong(i * Long.BYTES, rowStart + i);\n         }\n+        for (int i = 0; i < numValsToRead; i += 1) {\n+          BitVectorHelper.setValidityBitToOne(vec.getValidityBuffer(), i);\n+        }", "originalCommit": "7a87a438309f57eb83ee21178c01f8bae336b89f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjE1NzY2NA==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546157664", "bodyText": "Looks like this was an oversight in the original PR. FYI @chenjunjiedada.", "author": "rdblue", "createdAt": "2020-12-19T00:15:14Z", "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -381,9 +382,13 @@ public VectorHolder read(VectorHolder reuse, int numValsToRead) {\n         for (int i = 0; i < numValsToRead; i += 1) {\n           vec.getDataBuffer().setLong(i * Long.BYTES, rowStart + i);\n         }\n+        for (int i = 0; i < numValsToRead; i += 1) {\n+          BitVectorHelper.setValidityBitToOne(vec.getValidityBuffer(), i);\n+        }\n         nulls = new NullabilityHolder(numValsToRead);\n       }\n \n+      rowStart += numValsToRead;", "originalCommit": "7a87a438309f57eb83ee21178c01f8bae336b89f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjE4NDk5OQ==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546184999", "bodyText": "Great catch! Let me update the unit test as well.", "author": "chenjunjiedada", "createdAt": "2020-12-19T03:39:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjE1NzY2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjE1ODAyMw==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546158023", "bodyText": "It isn't necessary to check whether there are projected ID columns. The code is shorter if the values are available by default, even if they aren't used. This fixes the problem where there are constants to add (like _file) but no identity partition values are projected.", "author": "rdblue", "createdAt": "2020-12-19T00:16:50Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java", "diffHunk": "@@ -68,18 +68,7 @@\n     // update the current file for Spark's filename() function\n     InputFileBlockHolder.set(file.path().toString(), task.start(), task.length());\n \n-    // schema or rows returned by readers\n-    PartitionSpec spec = task.spec();\n-    Set<Integer> idColumns = spec.identitySourceIds();\n-    Schema partitionSchema = TypeUtil.select(expectedSchema, idColumns);\n-    boolean projectsIdentityPartitionColumns = !partitionSchema.columns().isEmpty();\n-\n-    Map<Integer, ?> idToConstant;\n-    if (projectsIdentityPartitionColumns) {\n-      idToConstant = PartitionUtil.constantsMap(task, BatchDataReader::convertConstant);\n-    } else {\n-      idToConstant = ImmutableMap.of();\n-    }\n+    Map<Integer, ?> idToConstant = PartitionUtil.constantsMap(task, BatchDataReader::convertConstant);", "originalCommit": "7a87a438309f57eb83ee21178c01f8bae336b89f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjYzNTQ0Ng==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546635446", "bodyText": "Used for local testing?", "author": "aokolnychyi", "createdAt": "2020-12-21T10:45:18Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkRowLevelOperationsTestBase.java", "diffHunk": "@@ -48,32 +48,32 @@ public SparkRowLevelOperationsTestBase(String catalogName, String implementation\n   @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}, format = {3}, vectorized = {4}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-        { \"testhive\", SparkCatalog.class.getName(),\n-            ImmutableMap.of(\n-                \"type\", \"hive\",\n-                \"default-namespace\", \"default\"\n-            ),\n-            \"orc\",\n-            true\n-        },\n+//        { \"testhive\", SparkCatalog.class.getName(),\n+//            ImmutableMap.of(\n+//                \"type\", \"hive\",\n+//                \"default-namespace\", \"default\"", "originalCommit": "d9213621701f7061dde785e821d588f0ab9020c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjgzODgyMw==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546838823", "bodyText": "Yes, will remove. Sorry about that, I usually look through the PR to catch these before review!", "author": "rdblue", "createdAt": "2020-12-21T17:37:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjYzNTQ0Ng=="}], "type": "inlineReview"}, {"oid": "6ae2a917c7cd905420db715be5fc43e4eb1f057e", "url": "https://github.com/apache/iceberg/commit/6ae2a917c7cd905420db715be5fc43e4eb1f057e", "message": "Spark: Sort retained rows in DELETE FROM by file and position.", "committedDate": "2020-12-21T18:10:26Z", "type": "commit"}, {"oid": "2bb4199772e19967f58c7b86b2d95f34c67c05c1", "url": "https://github.com/apache/iceberg/commit/2bb4199772e19967f58c7b86b2d95f34c67c05c1", "message": "Fix vectorized Parquet _file and _pos.", "committedDate": "2020-12-21T18:10:27Z", "type": "commit"}, {"oid": "545f5db1d3c7d4a0fbcb58ec5ba5f804a79dbab1", "url": "https://github.com/apache/iceberg/commit/545f5db1d3c7d4a0fbcb58ec5ba5f804a79dbab1", "message": "Fix checkstyle.", "committedDate": "2020-12-21T18:10:27Z", "type": "commit"}, {"oid": "eb90c18be19583c5fbfb219c9b54cc28a7074a17", "url": "https://github.com/apache/iceberg/commit/eb90c18be19583c5fbfb219c9b54cc28a7074a17", "message": "Fix test parameters from debugging.", "committedDate": "2020-12-21T18:10:27Z", "type": "commit"}, {"oid": "5b3b8d83141bd637a48b2499519a2db1ad819ef0", "url": "https://github.com/apache/iceberg/commit/5b3b8d83141bd637a48b2499519a2db1ad819ef0", "message": "Simplify statement in buildScanPlan.", "committedDate": "2020-12-21T18:10:27Z", "type": "commit"}, {"oid": "5b3b8d83141bd637a48b2499519a2db1ad819ef0", "url": "https://github.com/apache/iceberg/commit/5b3b8d83141bd637a48b2499519a2db1ad819ef0", "message": "Simplify statement in buildScanPlan.", "committedDate": "2020-12-21T18:10:27Z", "type": "forcePushed"}, {"oid": "ab4505c3aee81d005d2f48e5ef7b689ba80a834a", "url": "https://github.com/apache/iceberg/commit/ab4505c3aee81d005d2f48e5ef7b689ba80a834a", "message": "Remove ExtendedScanRelation node.", "committedDate": "2020-12-21T19:28:50Z", "type": "commit"}]}