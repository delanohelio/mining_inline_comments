{"pr_number": 1612, "pr_title": "Hive: Using Hive schema to create tables and partition specification", "pr_createdAt": "2020-10-14T11:16:50Z", "pr_url": "https://github.com/apache/iceberg/pull/1612", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEyNjg5Mw==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r511126893", "bodyText": "What about disallowing a schema in Hive DDL when the table already exists? Then we could always fill in the schema from the Iceberg table.", "author": "rdblue", "createdAt": "2020-10-23T19:55:43Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -79,6 +83,7 @@ public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable)\n             \"Iceberg table already created - can not use provided schema\");\n         Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n             \"Iceberg table already created - can not use provided partition specification\");\n+        // TODO: Check type compatibility between this.icebergTable and hmsTable.getSd().getCols()", "originalCommit": "047357045604654c0dab6bf50f55d01b77e4f88c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjAyMDcwNw==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r516020707", "bodyText": "Iceberg schema definition is sometimes richer, so a user might want to specify both the type in Hive and in Iceberg by hand and would not want to rely on the automatic conversion.\nIt is a non-trivial task to find out here whether the columns are generated from the SQL statement or from the Iceberg table schema so I left it as it is.\nShall we sink effort here to temporarily forbid table creation?", "author": "pvary", "createdAt": "2020-11-02T14:46:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEyNjg5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjY5NjczMA==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r516696730", "bodyText": "Ok. Implemented the first version of the check", "author": "pvary", "createdAt": "2020-11-03T14:15:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEyNjg5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEyODQ3NA==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r511128474", "bodyText": "I think it would be safer to detect the Hadoop catalog, right? There could be other catalogs that are not Hive but also don't require a location.", "author": "rdblue", "createdAt": "2020-10-23T19:59:38Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -89,22 +94,39 @@ public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable)\n     }\n \n     // If the table does not exist collect data for table creation\n-    String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n-    Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n-    // Just check if it is parsable, and later use for partition specification parsing\n-    Schema schema = SchemaParser.fromJson(schemaString);\n-\n-    String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n-    if (specString != null) {\n-      // Just check if it is parsable\n-      PartitionSpecParser.fromJson(schema, specString);\n+    // - InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC takes precedence so the user can override the\n+    // Iceberg schema and specification generated by the code\n+    // - Partitioned Hive tables are converted to non-partitioned Hive tables and the Iceberg partition specification\n+    // is generated automatically based on the provided columns. If the MetaStore table contains partitioning\n+    // information then:\n+    //     - Merging the normal and partitioned columns for the table we are creating\n+    //     - Removing partition columns for the table we are creating\n+    //     - Creating Iceberg partitioning specification using the partition columns\n+\n+    Schema schema = schema(catalogProperties, hmsTable);\n+    PartitionSpec spec = spec(schema, catalogProperties, hmsTable);\n+\n+    catalogProperties.put(InputFormatConfig.TABLE_SCHEMA, SchemaParser.toJson(schema));\n+    catalogProperties.put(InputFormatConfig.PARTITION_SPEC, PartitionSpecParser.toJson(spec));\n+\n+    // Merging partition columns to the normal columns, since Hive table reads are working only on non-partitioned\n+    // tables\n+    if (hmsTable.getPartitionKeys() != null && !hmsTable.getPartitionKeys().isEmpty()) {\n+      hmsTable.getSd().getCols().addAll(hmsTable.getPartitionKeys());\n+      hmsTable.setPartitionKeysIsSet(false);\n     }\n \n     // Allow purging table data if the table is created now and not set otherwise\n     if (hmsTable.getParameters().get(InputFormatConfig.EXTERNAL_TABLE_PURGE) == null) {\n       hmsTable.getParameters().put(InputFormatConfig.EXTERNAL_TABLE_PURGE, \"TRUE\");\n     }\n \n+    // If the table is not managed by Hive catalog then the location should be set\n+    if (!Catalogs.hiveCatalog(conf)) {\n+      Preconditions.checkArgument(hmsTable.getSd() != null && hmsTable.getSd().getLocation() != null,\n+          \"Table location not set\");", "originalCommit": "047357045604654c0dab6bf50f55d01b77e4f88c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjAyNDEwNg==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r516024106", "bodyText": "The location is needed for HadoopCatalog / HadoopTable, and in the tests for CustomCatalog as well.\nWe very specifically do not need the location for Hive tables since we reuse the Hive default location to store the metadata as well. Every other Catalog should provide a location for the metadata or should be aware of the Hive default location and use that.\nI felt that it is better to cut this link for every other Catalog.", "author": "pvary", "createdAt": "2020-11-02T14:51:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTEyODQ3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE0MjkzMg==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r511142932", "bodyText": "I think CHAR should be mapped to String. Why map it to fixed? While the length is fixed, the behavior should be identical to string because the padding characters are ignored. CHAR is completely up to the engine to enforce, and otherwise behaves as a String.", "author": "rdblue", "createdAt": "2020-10-23T20:34:50Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveSchemaUtil.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Types;\n+\n+public class HiveSchemaUtil {\n+  private HiveSchemaUtil() {\n+  }\n+\n+  /**\n+   * Converts the list of Hive FieldSchemas to an Iceberg schema.\n+   * <p>\n+   * The list should contain the columns and the partition columns as well.\n+   * @param fieldSchemas The list of the columns\n+   * @return An equivalent Iceberg Schema\n+   */\n+  public static Schema schema(List<FieldSchema> fieldSchemas) {\n+    List<String> names = new ArrayList<>(fieldSchemas.size());\n+    List<TypeInfo> typeInfos = new ArrayList<>(fieldSchemas.size());\n+\n+    for (FieldSchema col : fieldSchemas) {\n+      names.add(col.getName());\n+      typeInfos.add(TypeInfoUtils.getTypeInfoFromTypeString(col.getType()));\n+    }\n+\n+    return convert(names, typeInfos);\n+  }\n+\n+  /**\n+   * Converts the Hive properties defining the columns to an Iceberg schema.\n+   * @param columnNames The property containing the column names\n+   * @param columnTypes The property containing the column types\n+   * @param columnNameDelimiter The name delimiter\n+   * @return The Iceberg schema\n+   */\n+  public static Schema schema(String columnNames, String columnTypes, String columnNameDelimiter) {\n+    // Parse the configuration parameters\n+    List<String> names = new ArrayList<>();\n+    Collections.addAll(names, columnNames.split(columnNameDelimiter));\n+\n+    return HiveSchemaUtil.convert(names, TypeInfoUtils.getTypeInfosFromTypeString(columnTypes));\n+  }\n+\n+  /**\n+   * Converts the Hive partition columns to Iceberg identity partition specification.\n+   * @param schema The Iceberg schema\n+   * @param fieldSchemas The partition column specification\n+   * @return The Iceberg partition specification\n+   */\n+  public static PartitionSpec spec(Schema schema, List<FieldSchema> fieldSchemas) {\n+    PartitionSpec.Builder builder = PartitionSpec.builderFor(schema);\n+    fieldSchemas.forEach(fieldSchema -> builder.identity(fieldSchema.getName()));\n+    return builder.build();\n+  }\n+\n+  private static Schema convert(List<String> names, List<TypeInfo> typeInfos) {\n+    HiveSchemaVisitor visitor = new HiveSchemaVisitor();\n+    return new Schema(visitor.visit(names, typeInfos));\n+  }\n+\n+  private static class HiveSchemaVisitor {\n+    private int id;\n+\n+    private HiveSchemaVisitor() {\n+      id = 0;\n+    }\n+\n+    private List<Types.NestedField> visit(List<String> names, List<TypeInfo> typeInfos) {\n+      List<Types.NestedField> result = new ArrayList<>(names.size());\n+      for (int i = 0; i < names.size(); ++i) {\n+        result.add(visit(names.get(i), typeInfos.get(i)));\n+      }\n+\n+      return result;\n+    }\n+\n+    private Types.NestedField visit(String name, TypeInfo typeInfo) {\n+      switch (typeInfo.getCategory()) {\n+        case PRIMITIVE:\n+          switch (((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory()) {\n+            case FLOAT:\n+              return Types.NestedField.optional(id++, name, Types.FloatType.get());\n+            case DOUBLE:\n+              return Types.NestedField.optional(id++, name, Types.DoubleType.get());\n+            case BOOLEAN:\n+              return Types.NestedField.optional(id++, name, Types.BooleanType.get());\n+            case BYTE:\n+            case SHORT:\n+            case INT:\n+              return Types.NestedField.optional(id++, name, Types.IntegerType.get());\n+            case LONG:\n+              return Types.NestedField.optional(id++, name, Types.LongType.get());\n+            case BINARY:\n+              return Types.NestedField.optional(id++, name, Types.BinaryType.get());\n+            case STRING:\n+            case VARCHAR:\n+              return Types.NestedField.optional(id++, name, Types.StringType.get());\n+            case CHAR:\n+              Types.FixedType fixedType = Types.FixedType.ofLength(((CharTypeInfo) typeInfo).getLength());\n+              return Types.NestedField.optional(id++, name, fixedType);", "originalCommit": "047357045604654c0dab6bf50f55d01b77e4f88c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjAyNDY5MA==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r516024690", "bodyText": "Done", "author": "pvary", "createdAt": "2020-11-02T14:51:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE0MjkzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE0MzMyOQ==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r511143329", "bodyText": "Does Hive support both TIMESTAMP WITH ZONE and WITHOUT ZONE? I thought that the default was WITHOUT ZONE, so I'm surprised to see the default here to with zone.", "author": "rdblue", "createdAt": "2020-10-23T20:35:47Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveSchemaUtil.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Types;\n+\n+public class HiveSchemaUtil {\n+  private HiveSchemaUtil() {\n+  }\n+\n+  /**\n+   * Converts the list of Hive FieldSchemas to an Iceberg schema.\n+   * <p>\n+   * The list should contain the columns and the partition columns as well.\n+   * @param fieldSchemas The list of the columns\n+   * @return An equivalent Iceberg Schema\n+   */\n+  public static Schema schema(List<FieldSchema> fieldSchemas) {\n+    List<String> names = new ArrayList<>(fieldSchemas.size());\n+    List<TypeInfo> typeInfos = new ArrayList<>(fieldSchemas.size());\n+\n+    for (FieldSchema col : fieldSchemas) {\n+      names.add(col.getName());\n+      typeInfos.add(TypeInfoUtils.getTypeInfoFromTypeString(col.getType()));\n+    }\n+\n+    return convert(names, typeInfos);\n+  }\n+\n+  /**\n+   * Converts the Hive properties defining the columns to an Iceberg schema.\n+   * @param columnNames The property containing the column names\n+   * @param columnTypes The property containing the column types\n+   * @param columnNameDelimiter The name delimiter\n+   * @return The Iceberg schema\n+   */\n+  public static Schema schema(String columnNames, String columnTypes, String columnNameDelimiter) {\n+    // Parse the configuration parameters\n+    List<String> names = new ArrayList<>();\n+    Collections.addAll(names, columnNames.split(columnNameDelimiter));\n+\n+    return HiveSchemaUtil.convert(names, TypeInfoUtils.getTypeInfosFromTypeString(columnTypes));\n+  }\n+\n+  /**\n+   * Converts the Hive partition columns to Iceberg identity partition specification.\n+   * @param schema The Iceberg schema\n+   * @param fieldSchemas The partition column specification\n+   * @return The Iceberg partition specification\n+   */\n+  public static PartitionSpec spec(Schema schema, List<FieldSchema> fieldSchemas) {\n+    PartitionSpec.Builder builder = PartitionSpec.builderFor(schema);\n+    fieldSchemas.forEach(fieldSchema -> builder.identity(fieldSchema.getName()));\n+    return builder.build();\n+  }\n+\n+  private static Schema convert(List<String> names, List<TypeInfo> typeInfos) {\n+    HiveSchemaVisitor visitor = new HiveSchemaVisitor();\n+    return new Schema(visitor.visit(names, typeInfos));\n+  }\n+\n+  private static class HiveSchemaVisitor {\n+    private int id;\n+\n+    private HiveSchemaVisitor() {\n+      id = 0;\n+    }\n+\n+    private List<Types.NestedField> visit(List<String> names, List<TypeInfo> typeInfos) {\n+      List<Types.NestedField> result = new ArrayList<>(names.size());\n+      for (int i = 0; i < names.size(); ++i) {\n+        result.add(visit(names.get(i), typeInfos.get(i)));\n+      }\n+\n+      return result;\n+    }\n+\n+    private Types.NestedField visit(String name, TypeInfo typeInfo) {\n+      switch (typeInfo.getCategory()) {\n+        case PRIMITIVE:\n+          switch (((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory()) {\n+            case FLOAT:\n+              return Types.NestedField.optional(id++, name, Types.FloatType.get());\n+            case DOUBLE:\n+              return Types.NestedField.optional(id++, name, Types.DoubleType.get());\n+            case BOOLEAN:\n+              return Types.NestedField.optional(id++, name, Types.BooleanType.get());\n+            case BYTE:\n+            case SHORT:\n+            case INT:\n+              return Types.NestedField.optional(id++, name, Types.IntegerType.get());\n+            case LONG:\n+              return Types.NestedField.optional(id++, name, Types.LongType.get());\n+            case BINARY:\n+              return Types.NestedField.optional(id++, name, Types.BinaryType.get());\n+            case STRING:\n+            case VARCHAR:\n+              return Types.NestedField.optional(id++, name, Types.StringType.get());\n+            case CHAR:\n+              Types.FixedType fixedType = Types.FixedType.ofLength(((CharTypeInfo) typeInfo).getLength());\n+              return Types.NestedField.optional(id++, name, fixedType);\n+            case TIMESTAMP:\n+              return Types.NestedField.optional(id++, name, Types.TimestampType.withZone());", "originalCommit": "047357045604654c0dab6bf50f55d01b77e4f88c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjAzMzgwMQ==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r516033801", "bodyText": "Hive3 has Timestamp and Timestamp with TZ.\nhttps://issues.apache.org/jira/browse/HIVE-14412\nhttps://cwiki.apache.org/confluence/display/Hive/Different+TIMESTAMP+types\nMy reasoning were that we are better off if we start storing data with TZ information present, so with further upgrades we do not lose information.", "author": "pvary", "createdAt": "2020-11-02T15:04:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE0MzMyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjY5Nzc0MQ==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r516697741", "bodyText": "Ok. Implemented a different conversion for Hive3/Hive2. So Timestamp will be Timestamp.withoutZone() and TimestampTZ will be Timestamp.withZone() if Hive3 is used.", "author": "pvary", "createdAt": "2020-11-03T14:16:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE0MzMyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE0NDMxNw==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r511144317", "bodyText": "Would it make sense to have a method for TypeInfo so you don't have to create an artificial name and then discard the field information that isn't data type?", "author": "rdblue", "createdAt": "2020-10-23T20:38:18Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveSchemaUtil.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.serde2.typeinfo.CharTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Types;\n+\n+public class HiveSchemaUtil {\n+  private HiveSchemaUtil() {\n+  }\n+\n+  /**\n+   * Converts the list of Hive FieldSchemas to an Iceberg schema.\n+   * <p>\n+   * The list should contain the columns and the partition columns as well.\n+   * @param fieldSchemas The list of the columns\n+   * @return An equivalent Iceberg Schema\n+   */\n+  public static Schema schema(List<FieldSchema> fieldSchemas) {\n+    List<String> names = new ArrayList<>(fieldSchemas.size());\n+    List<TypeInfo> typeInfos = new ArrayList<>(fieldSchemas.size());\n+\n+    for (FieldSchema col : fieldSchemas) {\n+      names.add(col.getName());\n+      typeInfos.add(TypeInfoUtils.getTypeInfoFromTypeString(col.getType()));\n+    }\n+\n+    return convert(names, typeInfos);\n+  }\n+\n+  /**\n+   * Converts the Hive properties defining the columns to an Iceberg schema.\n+   * @param columnNames The property containing the column names\n+   * @param columnTypes The property containing the column types\n+   * @param columnNameDelimiter The name delimiter\n+   * @return The Iceberg schema\n+   */\n+  public static Schema schema(String columnNames, String columnTypes, String columnNameDelimiter) {\n+    // Parse the configuration parameters\n+    List<String> names = new ArrayList<>();\n+    Collections.addAll(names, columnNames.split(columnNameDelimiter));\n+\n+    return HiveSchemaUtil.convert(names, TypeInfoUtils.getTypeInfosFromTypeString(columnTypes));\n+  }\n+\n+  /**\n+   * Converts the Hive partition columns to Iceberg identity partition specification.\n+   * @param schema The Iceberg schema\n+   * @param fieldSchemas The partition column specification\n+   * @return The Iceberg partition specification\n+   */\n+  public static PartitionSpec spec(Schema schema, List<FieldSchema> fieldSchemas) {\n+    PartitionSpec.Builder builder = PartitionSpec.builderFor(schema);\n+    fieldSchemas.forEach(fieldSchema -> builder.identity(fieldSchema.getName()));\n+    return builder.build();\n+  }\n+\n+  private static Schema convert(List<String> names, List<TypeInfo> typeInfos) {\n+    HiveSchemaVisitor visitor = new HiveSchemaVisitor();\n+    return new Schema(visitor.visit(names, typeInfos));\n+  }\n+\n+  private static class HiveSchemaVisitor {\n+    private int id;\n+\n+    private HiveSchemaVisitor() {\n+      id = 0;\n+    }\n+\n+    private List<Types.NestedField> visit(List<String> names, List<TypeInfo> typeInfos) {\n+      List<Types.NestedField> result = new ArrayList<>(names.size());\n+      for (int i = 0; i < names.size(); ++i) {\n+        result.add(visit(names.get(i), typeInfos.get(i)));\n+      }\n+\n+      return result;\n+    }\n+\n+    private Types.NestedField visit(String name, TypeInfo typeInfo) {\n+      switch (typeInfo.getCategory()) {\n+        case PRIMITIVE:\n+          switch (((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory()) {\n+            case FLOAT:\n+              return Types.NestedField.optional(id++, name, Types.FloatType.get());\n+            case DOUBLE:\n+              return Types.NestedField.optional(id++, name, Types.DoubleType.get());\n+            case BOOLEAN:\n+              return Types.NestedField.optional(id++, name, Types.BooleanType.get());\n+            case BYTE:\n+            case SHORT:\n+            case INT:\n+              return Types.NestedField.optional(id++, name, Types.IntegerType.get());\n+            case LONG:\n+              return Types.NestedField.optional(id++, name, Types.LongType.get());\n+            case BINARY:\n+              return Types.NestedField.optional(id++, name, Types.BinaryType.get());\n+            case STRING:\n+            case VARCHAR:\n+              return Types.NestedField.optional(id++, name, Types.StringType.get());\n+            case CHAR:\n+              Types.FixedType fixedType = Types.FixedType.ofLength(((CharTypeInfo) typeInfo).getLength());\n+              return Types.NestedField.optional(id++, name, fixedType);\n+            case TIMESTAMP:\n+              return Types.NestedField.optional(id++, name, Types.TimestampType.withZone());\n+            case DATE:\n+              return Types.NestedField.optional(id++, name, Types.DateType.get());\n+            case DECIMAL:\n+              DecimalTypeInfo decimalTypeInfo = (DecimalTypeInfo) typeInfo;\n+              Types.DecimalType decimalType =\n+                  Types.DecimalType.of(decimalTypeInfo.precision(), decimalTypeInfo.scale());\n+              return Types.NestedField.optional(id++, name, decimalType);\n+            // TODO: In Hive3 we have TIMESTAMPLOCALTZ\n+            default:\n+              throw new IllegalArgumentException(\"Unknown primitive type \" +\n+                  ((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory());\n+          }\n+        case STRUCT:\n+          StructTypeInfo structTypeInfo = (StructTypeInfo) typeInfo;\n+          List<Types.NestedField> fields =\n+              visit(structTypeInfo.getAllStructFieldNames(), structTypeInfo.getAllStructFieldTypeInfos());\n+          Types.StructType structType = Types.StructType.of(fields);\n+          return Types.NestedField.optional(id++, name, structType);\n+        case MAP:\n+          MapTypeInfo mapTypeInfo = (MapTypeInfo) typeInfo;\n+          Types.NestedField keyField = visit(name + \"_key\", mapTypeInfo.getMapKeyTypeInfo());\n+          Types.NestedField valueField = visit(name + \"_value\", mapTypeInfo.getMapValueTypeInfo());", "originalCommit": "047357045604654c0dab6bf50f55d01b77e4f88c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjEzMTU0MA==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r516131540", "bodyText": "Done", "author": "pvary", "createdAt": "2020-11-02T17:19:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE0NDMxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE0NDY4Nw==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r511144687", "bodyText": "Empty string defaults are very suspicious to me. Why not use null here?", "author": "rdblue", "createdAt": "2020-10-23T20:39:15Z", "path": "mr/src/test/java/org/apache/iceberg/mr/hive/TestTables.java", "diffHunk": "@@ -172,5 +221,18 @@ public String identifier(String tableIdentifier) {\n     public Map<String, String> properties() {\n       return ImmutableMap.of(InputFormatConfig.CATALOG, \"hive\");\n     }\n+\n+    @Override\n+    public String locationForCreateTableSQL(TableIdentifier identifier) {\n+      return \"\";", "originalCommit": "047357045604654c0dab6bf50f55d01b77e4f88c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjAzNDkxNw==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r516034917", "bodyText": "I used this to add location string to the Hive test queries.\nI wanted to avoid writing every time:\n(location==null ? \"\" : location)", "author": "pvary", "createdAt": "2020-11-02T15:06:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE0NDY4Nw=="}], "type": "inlineReview"}, {"oid": "d1940bb6ff27e961f5e18039772519676d86776d", "url": "https://github.com/apache/iceberg/commit/d1940bb6ff27e961f5e18039772519676d86776d", "message": "Some review comment changes:\n- CHAR\n- TypeInfo generation instead of NestedField in conversion", "committedDate": "2020-11-02T17:05:33Z", "type": "forcePushed"}, {"oid": "ba014580c536aa8d8aec85b0a90e0f48f7d96c55", "url": "https://github.com/apache/iceberg/commit/ba014580c536aa8d8aec85b0a90e0f48f7d96c55", "message": "Different Hive2/Hive3 timestamp handling", "committedDate": "2020-11-03T14:09:18Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODU4NzAzOQ==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r518587039", "bodyText": "Returning Types.TimestampType.withoutZone() will use IcebergTimestampObjectInspector.INSTANCE_WITHOUT_ZONE instance to get local time and then throw ClassCastException", "author": "qphien", "createdAt": "2020-11-06T08:16:16Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveSchemaVisitor.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+public class HiveSchemaVisitor {\n+  private int id;\n+\n+  public HiveSchemaVisitor() {\n+    id = 0;\n+  }\n+\n+  List<Types.NestedField> visit(List<String> names, List<TypeInfo> typeInfos) {\n+    List<Types.NestedField> result = new ArrayList<>(names.size());\n+    for (int i = 0; i < names.size(); ++i) {\n+      result.add(Types.NestedField.optional(id++, names.get(i), visit(typeInfos.get(i))));\n+    }\n+\n+    return result;\n+  }\n+\n+  Type visit(TypeInfo typeInfo) {\n+    switch (typeInfo.getCategory()) {\n+      case PRIMITIVE:\n+        switch (((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory()) {\n+          case FLOAT:\n+            return Types.FloatType.get();\n+          case DOUBLE:\n+            return Types.DoubleType.get();\n+          case BOOLEAN:\n+            return Types.BooleanType.get();\n+          case BYTE:\n+          case SHORT:\n+          case INT:\n+            return Types.IntegerType.get();\n+          case LONG:\n+            return Types.LongType.get();\n+          case BINARY:\n+            return Types.BinaryType.get();\n+          case STRING:\n+          case CHAR:\n+          case VARCHAR:\n+            return Types.StringType.get();\n+          case TIMESTAMP:\n+            return Types.TimestampType.withoutZone();", "originalCommit": "ba014580c536aa8d8aec85b0a90e0f48f7d96c55", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTgzMjE2Mg==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r519832162", "bodyText": "@qphien: Thanks for reporting this. Could you please help with the concrete repro steps? Based on my understanding the way we store the timestamps in Iceberg should not affect Hive queries.\nThanks,\nPeter", "author": "pvary", "createdAt": "2020-11-09T13:57:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODU4NzAzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY3NjI1MA==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r520676250", "bodyText": "I think the reported issue is most probably independent from the create table command.\nI have tried several different ways to repro anyway, but was not able \ud83d\ude22", "author": "pvary", "createdAt": "2020-11-10T15:59:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODU4NzAzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTM0MDgwMw==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r521340803", "bodyText": "Sorry for my late reply. I have a iceberg table schema like below:\nhive (default)> desc complex_iceberg_table_2;\nOK\ncol_name\tdata_type\tcomment\nid                  \tbigint              \tfrom deserializer\nuser                \tstring              \tfrom deserializer\nasset               \tdecimal(7,3)        \tfrom deserializer\nevent_time          \ttimestamp           \tfrom deserializer\nday                 \tdate                \tfrom deserializer\nlogs                \tarray<string>       \tfrom deserializer\nlocation            \tmap<string,string>  \tfrom deserializer\nverbose             \tstruct<idcomment:string,count:int,elements:array<int>>\tfrom deserializer\n\nwhen i select event_time, hive throws ClassCastException\nhive (default)> select event_time from complex_iceberg_table_2;\nOK\nevent_time\nFailed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: java.time.OffsetDateTime cannot be cast to java.time.LocalDateTime", "author": "qphien", "createdAt": "2020-11-11T12:59:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODU4NzAzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTQyNjk3OA==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r521426978", "bodyText": "@qphien: Hive2, or Hive3? - Seems like an issue with the timestamp handling itself.\nThanks, Peter", "author": "pvary", "createdAt": "2020-11-11T15:13:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODU4NzAzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTg1OTkzNA==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r521859934", "bodyText": "@pvary Sorry, it's my mistake. This table iceberg data was produced with this repo commit 3a319f3 before. However, i tested hive with the latest commit and got the class cast exception. I reproduce the iceberg data with the latest commit and everything works fine.\nThanks for your help.", "author": "qphien", "createdAt": "2020-11-12T06:07:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODU4NzAzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTg2NzAwNw==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r521867007", "bodyText": "One more question, HiveSchemaVisitor converts hive timestamp to iceberg timestamp, but with https://github.com/pvary/iceberg/blob/ba014580c536aa8d8aec85b0a90e0f48f7d96c55/spark/src/main/java/org/apache/iceberg/spark/SparkTypeToType.java#L148-L149, spark converts java timestamp to  timestamptz. In such case: the iceberg table is created by hive and spark is used to generate iceberg data, will this inconsistency be a problem?", "author": "qphien", "createdAt": "2020-11-12T06:30:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODU4NzAzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjg1NTY5OA==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r522855698", "bodyText": "Discussed with Ryan and we will move Timestamp related stuff out from this PR to simplify the scope.\nThere is no good way to interop between Spark and Hive2. AFAIK Spark only supports timestamptz. Hive 2 has a very mixed up timestamp handling dependent on the file format used. Hive 3 has fixed this, and it has 2 different types for timestamp and timestamptz which cleans up this mess and will help us to have a clean mapping.", "author": "pvary", "createdAt": "2020-11-13T10:17:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODU4NzAzOQ=="}], "type": "inlineReview"}, {"oid": "4102aba5abe38f18c0b92cf9d15c5ddf0d36a147", "url": "https://github.com/apache/iceberg/commit/4102aba5abe38f18c0b92cf9d15c5ddf0d36a147", "message": "Handling Timestamp / Timestamp with local timezone and friends.\nAlso 2 small fixes", "committedDate": "2020-11-12T12:48:04Z", "type": "forcePushed"}, {"oid": "2c1630f2492ce25481989f869e788ef2bd3ae8f0", "url": "https://github.com/apache/iceberg/commit/2c1630f2492ce25481989f869e788ef2bd3ae8f0", "message": "Handling Timestamp / Timestamp with local timezone and friends.\nAlso 2 small fixes", "committedDate": "2020-11-12T12:56:49Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU1NjY4MA==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r522556680", "bodyText": "It looks like this visitor is a converter. We may want to fix the name.", "author": "rdblue", "createdAt": "2020-11-13T01:46:09Z", "path": "hive3/src/main/java/org/apache/iceberg/mr/hive/Hive3SchemaVisitor.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+public class Hive3SchemaVisitor extends HiveSchemaVisitor {", "originalCommit": "2c1630f2492ce25481989f869e788ef2bd3ae8f0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjg1Njc4MQ==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r522856781", "bodyText": "Yeah, renamed...\nThanks for catching", "author": "pvary", "createdAt": "2020-11-13T10:19:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU1NjY4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU1NzAxNw==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r522557017", "bodyText": "Looks like a lot of this is handling that is specific to Hive 3. Is it possible to exclude those tests from Hive 3 for now and review just the base changes? Then we can add Hive 3 later and enable the shared tests.", "author": "rdblue", "createdAt": "2020-11-13T01:47:09Z", "path": "hive3/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergTimestampLocalTZObjectInspectorHive3.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive.serde.objectinspector;\n+\n+import java.time.Instant;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.ZonedDateTime;\n+import org.apache.hadoop.hive.common.type.TimestampTZ;\n+import org.apache.hadoop.hive.serde2.io.TimestampLocalTZWritable;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.TimestampLocalTZObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+public class IcebergTimestampLocalTZObjectInspectorHive3 extends AbstractPrimitiveJavaObjectInspector", "originalCommit": "2c1630f2492ce25481989f869e788ef2bd3ae8f0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjg1NzA1OA==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r522857058", "bodyText": "Removed the Timestamp handling cleanup to another patch", "author": "pvary", "createdAt": "2020-11-13T10:19:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU1NzAxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU1ODQ1NA==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r522558454", "bodyText": "To make sure tests don't break across zones, we create date/time values using Instant or millis. Looks like you're trying to add micros to this test, which I think is a good idea. But we should avoid using these factory methods because the underlying values are not reliable.", "author": "rdblue", "createdAt": "2020-11-13T01:52:09Z", "path": "hive3/src/test/java/org/apache/iceberg/mr/hive/serde/objectinspector/TestIcebergTimestampObjectInspectorHive3.java", "diffHunk": "@@ -52,9 +54,8 @@ public void testIcebergTimestampObjectInspector() {\n     Assert.assertNull(oi.getPrimitiveJavaObject(null));\n     Assert.assertNull(oi.getPrimitiveWritableObject(null));\n \n-    long epochMilli = 1601471970000L;\n-    LocalDateTime local = LocalDateTime.ofInstant(Instant.ofEpochMilli(epochMilli), ZoneId.of(\"UTC\"));\n-    Timestamp ts = Timestamp.ofEpochMilli(epochMilli);\n+    LocalDateTime local = LocalDateTime.of(2020, 11, 22, 8, 11, 16, 123456789);\n+    Timestamp ts = Timestamp.valueOf(\"2020-11-22 8:11:16.123456789\");", "originalCommit": "2c1630f2492ce25481989f869e788ef2bd3ae8f0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjg4NjUyNg==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r522886526", "bodyText": "The purpose of the check here is that we really want to check that the conversion is correct from the LocaleDateTime to org.apache.hadoop.hive.common.type.Timestamp.\nFor the Hive Timestamp should behave like LocalDateTime in a way that is should be independent of the TZ. Sadly there is no constructor for the Hive Timestamp where I can provide the (year/month/day/etc), and this was the only constructor where I do not have to convert the data to some TZ aware construct to create the Timestamp.\nImportant to note that this Timestamp is org.apache.hadoop.hive.common.type.Timestamp which by definition should not break across TZ-s, so if we find such an issue then we should fix it.\nWe can continue this discussion in the Timestamp PR by the way \ud83d\ude04", "author": "pvary", "createdAt": "2020-11-13T11:16:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU1ODQ1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU1OTU1MQ==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r522559551", "bodyText": "What about rejecting the use of a schema if the table already exists in a different catalog? I think that would make more sense than allowing it but checking compatibility.", "author": "rdblue", "createdAt": "2020-11-13T01:55:54Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -80,6 +81,10 @@ public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable)\n         Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n             \"Iceberg table already created - can not use provided partition specification\");\n \n+        Schema hmsSchema = HiveSchemaUtil.schema(hmsTable.getSd().getCols());\n+        Preconditions.checkArgument(HiveSchemaUtil.compatible(hmsSchema, icebergTable.schema()),\n+            \"Iceberg table already created - with different specification\");", "originalCommit": "2c1630f2492ce25481989f869e788ef2bd3ae8f0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjg2NzQ0MQ==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r522867441", "bodyText": "The Hive API messes up with us. Here we always receive the Hive column definition which was generated based on the SerDe.getObjectInspector. The objectInspector generation either uses the columns provided in the query or columns generated based on the Iceberg table schema. We can try to find a way to massage through the API the information whether the source of the schema is an Iceberg table or provided in the query, but in the medium-long term I think we would like to decouple the Iceberg schema and the Hive schema anyway.\nJust a few examples where decoupling could be useful:\n\nIceberg UUID column - we might want to map to STRING / VARCHAR / CHAR or even BINARY\nIceberg Time - we might want to find a way to read these columns to some of the Hive types\nWe might want to create an Hive table which only contains a few column from the underlying Iceberg table\n\nSo instead of providing a throw-away temporary solution I decided to create the first implementation which checks the compatibility of the 2 schemas. The current implementation only allows exact matches, but it could be easily changed to allow differences if the ObjectInspectors are improved to handle the conversions.\nDoes this makes sense?\nThanks,\nPeter", "author": "pvary", "createdAt": "2020-11-13T10:39:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU1OTU1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU3MDI4MA==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r526570280", "bodyText": "Not sure I understand the usecases mentioned here. Why would the user need to specify a Hive schema when creating the Hive table even for the UUID type lets say? Shouldn't the deserializer be responsible for returning the Hive compatible schema?\nE.g. for Avro tables, users do not need to specify a \"string\" column for enum types, the AvroSerDe maps it to string correctly.", "author": "shardulm94", "createdAt": "2020-11-19T03:26:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU1OTU1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjY4MzA2Nw==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r526683067", "bodyText": "After discussing offline with @marton-bod we agreed that we should have stricter mapping. See: #1612 (comment)\nThere is still one remaining important use-case:\n\nThe table is created before trying to access from Hive and the Catalog is HadoopTable/HadoopCatalog/CustomCatalog. When the user wants to create the Hive table it wants to use only a few columns from the table. Either because it has too many columns and wants to have a simplified table in Hive, or the Iceberg table contains columns which are not supported by Hive, like time or fixed.", "author": "pvary", "createdAt": "2020-11-19T08:41:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU1OTU1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODI0ODA1OQ==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r528248059", "bodyText": "We have combinations of two sources of schemas when creating tables\n\nInputFormatConfig.TABLE_SCHEMA (either provided by the user when creating table, or coming from an existing table)\nHive schema (provided by the user, or based on our previous comment seems like Hive will generate it based on ObjectInspector anyways)\n\nThere are a few problems I think of when it comes to Hive schemas when compared to Iceberg\n\nField names are always lowercase\nField are always nullable\nDifference in supported types\n\nSo, I don't see a good reason why we should even look at the Hive schema when InputFormatConfig.TABLE_SCHEMA is present. Can we just disregard the Hive schema in such cases? I would prefer looking at Hive schema when thats the only schema information we have available. We won't need compatibility checks as there would be nothing to compare against.\nFor the use case to read a subset of columns from a non-Hive catalog table, can we just make the user specify the the column names they want to read? The user having to provide type information would be error-prone and especially tedious for nested column definitions. The user provided type information can also go stale overtime in reference to the source table.", "author": "shardulm94", "createdAt": "2020-11-21T22:04:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU1OTU1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc0ODk0Nw==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r528748947", "bodyText": "Good summary @shardulm94! Thanks!\n\nThere are a few problems I think of when it comes to Hive schemas when compared to Iceberg\n\nField names are always lowercase\nField are always nullable\nDifference in supported types\n\n\nAnd we should add different partitioning philosophy, but that will be something for later \ud83d\ude04\nGood point on the differences. Definitely we should create a PR handling all those differences.\n\nSo, I don't see a good reason why we should even look at the Hive schema when InputFormatConfig.TABLE_SCHEMA is present. Can we just disregard the Hive schema in such cases? I would prefer looking at Hive schema when thats the only schema information we have available. We won't need compatibility checks as there would be nothing to compare against.\nFor the use case to read a subset of columns from a non-Hive catalog table, can we just make the user specify the the column names they want to read? The user having to provide type information would be error-prone and especially tedious for nested column definitions. The user provided type information can also go stale overtime in reference to the source table.\n\nFor the longer term:\n\nI think we should stick to the SQL standard as much as possible. I think it would be an overkill to introduce new SQL language elements only for the case where the Iceberg table is already created and we are creating a new Hive table with fever columns.\nIf the above point stands, then we have a schema from the SQL and the schema from the table which we have to compare anyway.\n\nFor short term:\n\nWe can decide to stick to the Hive schema generated from the Iceberg schema if we accept that Hive recreates the SerDe quite often and we have to load the table every case. This is \"only\" a performance question.\nThe bigger problem is that there is no technically correct way to differentiate cases when the column specification is provided by the user or when it is generated based on the Iceberg schema. We can rely on the column comments (\"from deserializer\") or we can use undocumented feature to add a new serDeProperties, but I would no like to depend on these for features. This means that we either have to compare the Hive schema and the Iceberg schema, or we have to throw away the schema provided by Hive. This later solution could be a usability problem since we will hide the case when the user provided a schema and it is not the same as the schema generated from the Iceberg schema.\n\nSo my thought process was the following:\n\nIn the long term we need the schema comparison\nIn the short term it would be good to have a schema comparison to provide good usability\nThis is why I decided to implement the comparison.\n\nThe options I see:\n\nMove forward with the change as it is \ud83d\ude04\nRemove comparison and if there is an Iceberg schema available for us, use it even if the user provided columns. We can remove the unsupported columns / struct fields / maps, so the Hive table could be created. This could raise some eyebrows when the table specification is provided in the SQL and the created table does not conform to that specification, but we can document this \"feature\" \ud83d\ude04\nI am open to other suggestions\n\nThanks,\nPeter", "author": "pvary", "createdAt": "2020-11-23T14:38:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU1OTU1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU1OTg4Mg==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r522559882", "bodyText": "In this case, I think we also need to check the hmsTable schema and validate that it is either null or compatible with the one set in the table property. I'd prefer requiring it to be null.", "author": "rdblue", "createdAt": "2020-11-13T01:57:05Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -185,4 +198,20 @@ private Properties getCatalogProperties(org.apache.hadoop.hive.metastore.api.Tab\n \n     return properties;\n   }\n+\n+  private Schema schema(Properties properties, org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    if (properties.getProperty(InputFormatConfig.TABLE_SCHEMA) != null) {\n+      return SchemaParser.fromJson(properties.getProperty(InputFormatConfig.TABLE_SCHEMA));", "originalCommit": "2c1630f2492ce25481989f869e788ef2bd3ae8f0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjg3NTUxNw==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r522875517", "bodyText": "Good point. Added the compatibility check here as well.", "author": "pvary", "createdAt": "2020-11-13T10:54:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU1OTg4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU2MDE0Ng==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r522560146", "bodyText": "I think this check should be in spec, like validation that the table schema is not present.", "author": "rdblue", "createdAt": "2020-11-13T01:58:04Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -89,22 +94,30 @@ public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable)\n     }\n \n     // If the table does not exist collect data for table creation\n-    String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n-    Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n-    // Just check if it is parsable, and later use for partition specification parsing\n-    Schema schema = SchemaParser.fromJson(schemaString);\n-\n-    String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n-    if (specString != null) {\n-      // Just check if it is parsable\n-      PartitionSpecParser.fromJson(schema, specString);\n-    }\n+    // - InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC takes precedence so the user can override the\n+    // Iceberg schema and specification generated by the code\n+    // - Partitioned Hive tables are currently not allowed\n+\n+    Schema schema = schema(catalogProperties, hmsTable);\n+    PartitionSpec spec = spec(schema, catalogProperties);\n+\n+    catalogProperties.put(InputFormatConfig.TABLE_SCHEMA, SchemaParser.toJson(schema));\n+    catalogProperties.put(InputFormatConfig.PARTITION_SPEC, PartitionSpecParser.toJson(spec));\n+\n+    Preconditions.checkArgument(hmsTable.getPartitionKeys() == null || hmsTable.getPartitionKeys().isEmpty(),\n+        \"Partitioned Hive tables are currently not supported\");", "originalCommit": "2c1630f2492ce25481989f869e788ef2bd3ae8f0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzM4Mzc2NA==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r523383764", "bodyText": "@rdblue: I am afraid I don't get what you mean here by \"this check should be in spec\". You simply mean that we should document it, or you have something else in your mind?\nCould you please check the other comments too? This one is blocking writes too long...\nThanks, Peter", "author": "pvary", "createdAt": "2020-11-14T06:23:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU2MDE0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU2NDE4Mg==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r526564182", "bodyText": "I think what @rdblue means here is that it should be in the spec() method below", "author": "shardulm94", "createdAt": "2020-11-19T03:10:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU2MDE0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjY4NDgwMw==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r526684803", "bodyText": "Ohh.. I get it. Thanks!\nDone", "author": "pvary", "createdAt": "2020-11-19T08:44:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU2MDE0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk5NjYwMg==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r525996602", "bodyText": "It might make sense to throw an exception if the user wants to use these Hive types (byte, short, char, varchar, etc.) which do not exist in Iceberg. E.g. \"unsupported Hive type (byte) for Iceberg tables. Consider using type (int) instead.\" - or something along the lines. Silently using a different Iceberg type than the one specified in the Hive DDL (e.g. Hive short -> Iceberg int) under the hood can violate the expectations of the user, for example regarding storage footprint, so it might be better to force a one-to-one type mapping.", "author": "marton-bod", "createdAt": "2020-11-18T10:59:38Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveSchemaConverter.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+class HiveSchemaConverter {\n+  private int id;\n+\n+  HiveSchemaConverter() {\n+    id = 0;\n+  }\n+\n+  List<Types.NestedField> convert(List<String> names, List<TypeInfo> typeInfos) {\n+    List<Types.NestedField> result = new ArrayList<>(names.size());\n+    for (int i = 0; i < names.size(); ++i) {\n+      result.add(Types.NestedField.optional(id++, names.get(i), convert(typeInfos.get(i))));\n+    }\n+\n+    return result;\n+  }\n+\n+  Type convert(TypeInfo typeInfo) {\n+    switch (typeInfo.getCategory()) {\n+      case PRIMITIVE:\n+        switch (((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory()) {\n+          case FLOAT:\n+            return Types.FloatType.get();\n+          case DOUBLE:\n+            return Types.DoubleType.get();\n+          case BOOLEAN:\n+            return Types.BooleanType.get();\n+          case BYTE:\n+          case SHORT:", "originalCommit": "99f8bad169c4c0859909981b068072141df05b6d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjA2NTU0NA==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r526065544", "bodyText": "Updated a patch to enforce one-to-one mapping from Hive side.\nHere is the current status:\n\n\n\nIceberg type\nHive2 type\nHive3 type\nStatus\n\n\n\n\nboolean\nBOOLEAN\nBOOLEAN\nOK\n\n\nint\nINTEGER\nINTEGER\nOK\n\n\nlong\nBIGINT\nBIGINT\nOK\n\n\nfloat\nFLOAT\nFLOAT\nOK\n\n\ndouble\nDOUBLE\nDOUBLE\nOK\n\n\ndecimal(P,S)\nDECIMAL(P,S)\nDECIMAL(P,S)\nOK\n\n\nbinary\nBINARY\nBINARY\nOK\n\n\ndate\nDATE\nDATE\nOK\n\n\ntimestamp\nTIMESTAMP\nTIMESTAMP\nOK\n\n\ntimestamptz\nTIMESTAMP\nTIMESTAMP WITH LOCAL TIMEZONE\nTODO\n\n\nstring\nSTRING\nSTRING\nOK\n\n\nuuid\nSTRING or BINARY\nSTRING or BINARY\nTODO\n\n\ntime\n-\n-\n-\n\n\nfixed(L)\n-\n-\n-\n\n\n-\nTINYINT\nTINYINT\n-\n\n\n-\nSMALLINT\nSMALLINT\n-\n\n\n-\nINTERVAL\nINTERVAL\n-\n\n\n-\nVARCHAR\nVARCHAR\n-\n\n\n-\nCHAR\nCHAR\n-", "author": "pvary", "createdAt": "2020-11-18T12:57:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk5NjYwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUyMzc0Ng==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r526523746", "bodyText": "Should we make this a static method? Seems like .convert() only makes sense to be called once per HiveSchemaConverter object, else the id counter will be already incremented at the beginning.", "author": "shardulm94", "createdAt": "2020-11-19T01:02:15Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveSchemaConverter.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+class HiveSchemaConverter {\n+  private int id;\n+\n+  HiveSchemaConverter() {\n+    id = 0;\n+  }\n+\n+  List<Types.NestedField> convert(List<String> names, List<TypeInfo> typeInfos) {", "originalCommit": "35324a864e676e89ca3e4a999e0bf0c48e4c51d0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjY5MzMwOA==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r526693308", "bodyText": "Makes sense. Moved the HiveUtil.convert method to the HiveSchemaConverter.\nThe code looks better now.\nThanks!", "author": "pvary", "createdAt": "2020-11-19T08:57:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUyMzc0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU3MjE2MA==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r526572160", "bodyText": "Are these properties set on table as properties, or are they generated dynamically during reads for column pruning?\nIf they are set as table properties, what happens if they go stale with respect to the iceberg table? e.g. column renames, type promotions, new top level fields added, etc.", "author": "shardulm94", "createdAt": "2020-11-19T03:32:57Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSerDe.java", "diffHunk": "@@ -56,10 +62,22 @@ public void initialize(@Nullable Configuration configuration, Properties serDePr\n     } else if (serDeProperties.get(InputFormatConfig.TABLE_SCHEMA) != null) {\n       tableSchema = SchemaParser.fromJson((String) serDeProperties.get(InputFormatConfig.TABLE_SCHEMA));\n     } else {\n-      try {\n-        tableSchema = Catalogs.loadTable(configuration, serDeProperties).schema();\n-      } catch (NoSuchTableException nte) {\n-        throw new SerDeException(\"Please provide an existing table or a valid schema\", nte);\n+      // Read the configuration parameters\n+      String columnNames = serDeProperties.getProperty(serdeConstants.LIST_COLUMNS);\n+      String columnTypes = serDeProperties.getProperty(serdeConstants.LIST_COLUMN_TYPES);", "originalCommit": "35324a864e676e89ca3e4a999e0bf0c48e4c51d0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjY5NzkyOQ==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r526697929", "bodyText": "Hive does a lot of magic here:\n\nFirst creates a SerDe to get the columns - in this case these properties are empty\nUses the objectInspectors from the SerDe to update column specification of the table.\nThen when it does the query planning it again calls this part, but the properties are already set based on the result of the previous call\n\nWith that sad this theoretically solves the schema evolution problem, but I am not convinced that we caught all of the nuances. So further down the road we plan to test this out as well and file the new PRs as required.", "author": "pvary", "createdAt": "2020-11-19T09:05:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU3MjE2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzAzMDczNg==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r527030736", "bodyText": "So If I understand correctly, this saves us from having to load the table from the catalog multiple times during query compile/planning.", "author": "shardulm94", "createdAt": "2020-11-19T16:38:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU3MjE2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzA1MTMwNA==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r527051304", "bodyText": "Yes, and also this is the way how we get the information when the columns are provided in the create table query.\nCREATE EXTERNAL TABLE (customer_id INT, first_name STRING) customers \nSTORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler';\n\nWill result in:\ncolumnNames = \"customer_id,first_name\";\ncolumnTypes = \"int:string\";\n\nDon't ask me how Hive arrived to this solution, but it is how it is...", "author": "pvary", "createdAt": "2020-11-19T17:04:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU3MjE2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU3MzU0NA==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r526573544", "bodyText": "Does this preserve field name casing or will the return schema have all lowercase field names?", "author": "shardulm94", "createdAt": "2020-11-19T03:37:56Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSerDe.java", "diffHunk": "@@ -56,10 +62,22 @@ public void initialize(@Nullable Configuration configuration, Properties serDePr\n     } else if (serDeProperties.get(InputFormatConfig.TABLE_SCHEMA) != null) {\n       tableSchema = SchemaParser.fromJson((String) serDeProperties.get(InputFormatConfig.TABLE_SCHEMA));\n     } else {\n-      try {\n-        tableSchema = Catalogs.loadTable(configuration, serDeProperties).schema();\n-      } catch (NoSuchTableException nte) {\n-        throw new SerDeException(\"Please provide an existing table or a valid schema\", nte);\n+      // Read the configuration parameters\n+      String columnNames = serDeProperties.getProperty(serdeConstants.LIST_COLUMNS);\n+      String columnTypes = serDeProperties.getProperty(serdeConstants.LIST_COLUMN_TYPES);\n+      String columnNameDelimiter = serDeProperties.containsKey(serdeConstants.COLUMN_NAME_DELIMITER) ?\n+          serDeProperties.getProperty(serdeConstants.COLUMN_NAME_DELIMITER) : String.valueOf(SerDeUtils.COMMA);\n+      if (columnNames != null && columnTypes != null && columnNameDelimiter != null &&\n+          !columnNames.isEmpty() && !columnTypes.isEmpty() && !columnNameDelimiter.isEmpty()) {\n+        tableSchema = HiveSchemaUtil.schema(columnNames, columnTypes, columnNameDelimiter);", "originalCommit": "35324a864e676e89ca3e4a999e0bf0c48e4c51d0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjcwNzYzMQ==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r526707631", "bodyText": "Hive always converts the column/table names to lower case.\nUpdated the test case to contain some capital letters to test/confirm this.\nGood to have a test case for this as well.\nThanks!", "author": "pvary", "createdAt": "2020-11-19T09:20:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU3MzU0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU3Mzk0MA==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r526573940", "bodyText": "Nit: Typo shake -> sake", "author": "shardulm94", "createdAt": "2020-11-19T03:39:31Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveSchemaUtil.java", "diffHunk": "@@ -0,0 +1,167 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.TimestampObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergObjectInspector;\n+import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergRecordObjectInspector;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveSchemaUtil {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveSchemaUtil.class);\n+\n+  private HiveSchemaUtil() {\n+  }\n+\n+  /**\n+   * Converts the list of Hive FieldSchemas to an Iceberg schema.\n+   * <p>\n+   * The list should contain the columns and the partition columns as well.\n+   * @param fieldSchemas The list of the columns\n+   * @return An equivalent Iceberg Schema\n+   */\n+  public static Schema schema(List<FieldSchema> fieldSchemas) {\n+    List<String> names = new ArrayList<>(fieldSchemas.size());\n+    List<TypeInfo> typeInfos = new ArrayList<>(fieldSchemas.size());\n+\n+    for (FieldSchema col : fieldSchemas) {\n+      names.add(col.getName());\n+      typeInfos.add(TypeInfoUtils.getTypeInfoFromTypeString(col.getType()));\n+    }\n+\n+    return convert(names, typeInfos);\n+  }\n+\n+  /**\n+   * Converts the Hive properties defining the columns to an Iceberg schema.\n+   * @param columnNames The property containing the column names\n+   * @param columnTypes The property containing the column types\n+   * @param columnNameDelimiter The name delimiter\n+   * @return The Iceberg schema\n+   */\n+  public static Schema schema(String columnNames, String columnTypes, String columnNameDelimiter) {\n+    // Parse the configuration parameters\n+    List<String> names = new ArrayList<>();\n+    Collections.addAll(names, columnNames.split(columnNameDelimiter));\n+\n+    return HiveSchemaUtil.convert(names, TypeInfoUtils.getTypeInfosFromTypeString(columnTypes));\n+  }\n+\n+  /**\n+   * Checks if the ObjectInspectors generated by the two schema definitions are compatible.\n+   * <p>\n+   * Currently only allows the same column names and column types. Later we might want allow compatible column types as\n+   * well.\n+   * TODO: We might want to allow compatible conversions\n+   * @param schema First schema\n+   * @param other Second schema\n+   * @return True if the two schema is compatible\n+   */\n+  public static boolean compatible(Schema schema, Schema other) {\n+    ObjectInspector inspector = IcebergObjectInspector.create(schema);\n+    ObjectInspector otherInspector = IcebergObjectInspector.create(other);\n+\n+    if (!(inspector instanceof IcebergRecordObjectInspector) ||\n+        !(otherInspector instanceof IcebergRecordObjectInspector)) {\n+      return false;\n+    }\n+\n+    return compatible(inspector, otherInspector);\n+  }\n+\n+  private static boolean compatible(ObjectInspector inspector, ObjectInspector other) {\n+    if (inspector == null && other == null) {\n+      // We do not expect this type of calls, but for completeness shake", "originalCommit": "35324a864e676e89ca3e4a999e0bf0c48e4c51d0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjY5ODM2Nw==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r526698367", "bodyText": "Fixed!\nThanks for pointing out!", "author": "pvary", "createdAt": "2020-11-19T09:05:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU3Mzk0MA=="}], "type": "inlineReview"}, {"oid": "1cb69d297c4c3f44f181d2477b0253902fb59143", "url": "https://github.com/apache/iceberg/commit/1cb69d297c4c3f44f181d2477b0253902fb59143", "message": "Removing schema comparison", "committedDate": "2020-11-24T10:52:24Z", "type": "forcePushed"}, {"oid": "356f723adcb022d701b3c7ab0d9c03a69934e9a9", "url": "https://github.com/apache/iceberg/commit/356f723adcb022d701b3c7ab0d9c03a69934e9a9", "message": "Using Hive schema to create tables and partition specification", "committedDate": "2020-11-24T12:13:23Z", "type": "commit"}, {"oid": "4ff16a8c309456d71dec8f75263f465316a3a7f6", "url": "https://github.com/apache/iceberg/commit/4ff16a8c309456d71dec8f75263f465316a3a7f6", "message": "Fixing Custom catalog tests", "committedDate": "2020-11-24T12:13:28Z", "type": "commit"}, {"oid": "a40b6c27b14c34e72a292e6d3f35a18b618fb500", "url": "https://github.com/apache/iceberg/commit/a40b6c27b14c34e72a292e6d3f35a18b618fb500", "message": "Moved table specific location generation to TestTables instead of the specific catalog test classes", "committedDate": "2020-11-24T12:14:15Z", "type": "commit"}, {"oid": "e3b3905ae8fa59ae00a8daeb0eade6f4bead460c", "url": "https://github.com/apache/iceberg/commit/e3b3905ae8fa59ae00a8daeb0eade6f4bead460c", "message": "Some review comment changes:\n- CHAR\n- TypeInfo generation instead of NestedField in conversion", "committedDate": "2020-11-24T12:14:18Z", "type": "commit"}, {"oid": "1d71fee3599bf47ebe77802f0a9d1a4f86bf7759", "url": "https://github.com/apache/iceberg/commit/1d71fee3599bf47ebe77802f0a9d1a4f86bf7759", "message": "Throw an exception when PARTITONED BY is used in the CREATE TABLE command", "committedDate": "2020-11-24T12:14:19Z", "type": "commit"}, {"oid": "133f1882f0ca6188ed46ed6d22971990a4068942", "url": "https://github.com/apache/iceberg/commit/133f1882f0ca6188ed46ed6d22971990a4068942", "message": "Checkstyle", "committedDate": "2020-11-24T12:14:19Z", "type": "commit"}, {"oid": "87202bf7a3dc823d1d30f02dea28631fefeacb77", "url": "https://github.com/apache/iceberg/commit/87202bf7a3dc823d1d30f02dea28631fefeacb77", "message": "First version of compatibility checks", "committedDate": "2020-11-24T12:14:19Z", "type": "commit"}, {"oid": "0fecd948af6f974fa3cdea3161b531d635561f60", "url": "https://github.com/apache/iceberg/commit/0fecd948af6f974fa3cdea3161b531d635561f60", "message": "Different Hive2/Hive3 timestamp handling", "committedDate": "2020-11-24T12:14:19Z", "type": "commit"}, {"oid": "aefbca0aa6b134aa4d7f8a37fb9ab2b9d10501e5", "url": "https://github.com/apache/iceberg/commit/aefbca0aa6b134aa4d7f8a37fb9ab2b9d10501e5", "message": "Handling Timestamp / Timestamp with local timezone and friends.\nAlso 2 small fixes", "committedDate": "2020-11-24T12:14:19Z", "type": "commit"}, {"oid": "3c3f1bed72b1200621f95e3b3293448b80713309", "url": "https://github.com/apache/iceberg/commit/3c3f1bed72b1200621f95e3b3293448b80713309", "message": "Reverted Timestamp related changes\nAddressed review comments", "committedDate": "2020-11-24T12:14:19Z", "type": "commit"}, {"oid": "ae751a6ef46283f3c0f1b973a3daae690951cc0d", "url": "https://github.com/apache/iceberg/commit/ae751a6ef46283f3c0f1b973a3daae690951cc0d", "message": "Renamed wrong method name", "committedDate": "2020-11-24T12:14:19Z", "type": "commit"}, {"oid": "b4b985e0d31ca46f37db8516c02f73bc6c9232b5", "url": "https://github.com/apache/iceberg/commit/b4b985e0d31ca46f37db8516c02f73bc6c9232b5", "message": "Prevent creating columns with Hive types which does not have an exact Iceberg interpretation.", "committedDate": "2020-11-24T12:14:19Z", "type": "commit"}, {"oid": "1a3b6101f72c13c60db8b80f00189c3d9e4364a8", "url": "https://github.com/apache/iceberg/commit/1a3b6101f72c13c60db8b80f00189c3d9e4364a8", "message": "Addressed review comments", "committedDate": "2020-11-24T12:14:19Z", "type": "commit"}, {"oid": "a6622db349cda45960338428fe3a440ba1a8c279", "url": "https://github.com/apache/iceberg/commit/a6622db349cda45960338428fe3a440ba1a8c279", "message": "Removing schema comparison", "committedDate": "2020-11-24T12:14:19Z", "type": "forcePushed"}, {"oid": "a6792e2d815fb8a5674746f6db34d6e548b4ab78", "url": "https://github.com/apache/iceberg/commit/a6792e2d815fb8a5674746f6db34d6e548b4ab78", "message": "Removing schema comparison", "committedDate": "2020-11-24T14:33:28Z", "type": "commit"}, {"oid": "a6792e2d815fb8a5674746f6db34d6e548b4ab78", "url": "https://github.com/apache/iceberg/commit/a6792e2d815fb8a5674746f6db34d6e548b4ab78", "message": "Removing schema comparison", "committedDate": "2020-11-24T14:33:28Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDA5MzQyNQ==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r530093425", "bodyText": "Can we extract this out in a common function to reuse above?", "author": "shardulm94", "createdAt": "2020-11-25T04:00:32Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSerDe.java", "diffHunk": "@@ -56,10 +61,28 @@ public void initialize(@Nullable Configuration configuration, Properties serDePr\n     } else if (serDeProperties.get(InputFormatConfig.TABLE_SCHEMA) != null) {\n       tableSchema = SchemaParser.fromJson((String) serDeProperties.get(InputFormatConfig.TABLE_SCHEMA));\n     } else {\n-      try {\n-        tableSchema = Catalogs.loadTable(configuration, serDeProperties).schema();\n-      } catch (NoSuchTableException nte) {\n-        throw new SerDeException(\"Please provide an existing table or a valid schema\", nte);\n+      if (Catalogs.hiveCatalog(configuration)) {\n+        tableSchema = hiveSchema(serDeProperties);\n+        if (tableSchema == null) {\n+          throw new SerDeException(\"Please provide a valid schema\");\n+        } else {\n+          LOG.info(\"Using hive schema {}\", SchemaParser.toJson(tableSchema));\n+        }\n+      } else {\n+        try {\n+          // always prefer the original table schema if there is one\n+          tableSchema = Catalogs.loadTable(configuration, serDeProperties).schema();\n+          LOG.info(\"Using schema from existing table {}\", SchemaParser.toJson(tableSchema));\n+        } catch (Exception e) {\n+          // If we can not load the table try the provided hive schema\n+          tableSchema = hiveSchema(serDeProperties);\n+          if (tableSchema == null) {\n+            throw new SerDeException(\"Please provide an existing table or a valid schema\", e);\n+          } else {\n+            LOG.info(\"Using schema from column specification {} since table load is failed\",\n+                SchemaParser.toJson(tableSchema), e);", "originalCommit": "a6792e2d815fb8a5674746f6db34d6e548b4ab78", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDIwMDQ5MA==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r530200490", "bodyText": "Done", "author": "pvary", "createdAt": "2020-11-25T08:51:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDA5MzQyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDA5MzYxNg==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r530093616", "bodyText": "What is the reason behind handling HiveCatalogs separately?", "author": "shardulm94", "createdAt": "2020-11-25T04:01:10Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSerDe.java", "diffHunk": "@@ -56,10 +61,28 @@ public void initialize(@Nullable Configuration configuration, Properties serDePr\n     } else if (serDeProperties.get(InputFormatConfig.TABLE_SCHEMA) != null) {\n       tableSchema = SchemaParser.fromJson((String) serDeProperties.get(InputFormatConfig.TABLE_SCHEMA));\n     } else {\n-      try {\n-        tableSchema = Catalogs.loadTable(configuration, serDeProperties).schema();\n-      } catch (NoSuchTableException nte) {\n-        throw new SerDeException(\"Please provide an existing table or a valid schema\", nte);\n+      if (Catalogs.hiveCatalog(configuration)) {", "originalCommit": "a6792e2d815fb8a5674746f6db34d6e548b4ab78", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDE5MDU4Mw==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r530190583", "bodyText": "HiveTableOperations converts the table schema to Hive columns / StorageDescriptor when any change is committed to the table. This means that the Iceberg schema and the Hive schema is always synchronized.\nSince the above synchronization, I think it is better to use the \"cached\" schema instead of loading the table again and again. This might change when we clean up Timestamps / UUIDs since the mapping is not 1-on-1 there, but I would leave something for that new PR too \ud83d\ude04", "author": "pvary", "createdAt": "2020-11-25T08:35:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDA5MzYxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDI1MjE0MQ==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r530252141", "bodyText": "Thought better of it.\nKeep it simple and general. And branch only if we can have a clean benefit in the final solution.", "author": "pvary", "createdAt": "2020-11-25T10:07:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDA5MzYxNg=="}], "type": "inlineReview"}, {"oid": "49569703b2cd1ab26a63674796dacfe287b7944d", "url": "https://github.com/apache/iceberg/commit/49569703b2cd1ab26a63674796dacfe287b7944d", "message": "Updated hiveSchema parsing", "committedDate": "2020-11-25T08:48:22Z", "type": "commit"}, {"oid": "6f881bb80609b0ca13de963d3c919bf07c505f42", "url": "https://github.com/apache/iceberg/commit/6f881bb80609b0ca13de963d3c919bf07c505f42", "message": "Handle HiveCatalog the same way as the other catalogs. This is suboptimal, but later we need that anyway", "committedDate": "2020-11-25T09:35:33Z", "type": "commit"}, {"oid": "ad53a0ae5d4a74ca79a3547695e4d35af480d09e", "url": "https://github.com/apache/iceberg/commit/ad53a0ae5d4a74ca79a3547695e4d35af480d09e", "message": "Timestamp changens are not needed if we stick to the Iceberg schema\nAlso reverting some formatting only changes", "committedDate": "2020-11-25T11:56:31Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDcwNzQ5OA==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r530707498", "bodyText": "I would be fine mapping these to string.", "author": "rdblue", "createdAt": "2020-11-26T00:30:46Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveSchemaConverter.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+class HiveSchemaConverter {\n+  private int id;\n+\n+  private HiveSchemaConverter() {\n+    id = 0;\n+  }\n+\n+  static Schema convert(List<String> names, List<TypeInfo> typeInfos) {\n+    HiveSchemaConverter converter = new HiveSchemaConverter();\n+    return new Schema(converter.convertInternal(names, typeInfos));\n+  }\n+\n+  List<Types.NestedField> convertInternal(List<String> names, List<TypeInfo> typeInfos) {\n+    List<Types.NestedField> result = new ArrayList<>(names.size());\n+    for (int i = 0; i < names.size(); ++i) {\n+      result.add(Types.NestedField.optional(id++, names.get(i), convert(typeInfos.get(i))));\n+    }\n+\n+    return result;\n+  }\n+\n+  Type convert(TypeInfo typeInfo) {\n+    switch (typeInfo.getCategory()) {\n+      case PRIMITIVE:\n+        switch (((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory()) {\n+          case FLOAT:\n+            return Types.FloatType.get();\n+          case DOUBLE:\n+            return Types.DoubleType.get();\n+          case BOOLEAN:\n+            return Types.BooleanType.get();\n+          case BYTE:\n+          case SHORT:\n+            throw new IllegalArgumentException(\"Unsupported Hive type (\" +\n+                ((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory() +\n+                \") for Iceberg tables. Consider using INT/INTEGER type instead.\");\n+          case INT:\n+            return Types.IntegerType.get();\n+          case LONG:\n+            return Types.LongType.get();\n+          case BINARY:\n+            return Types.BinaryType.get();\n+          case CHAR:\n+          case VARCHAR:\n+            throw new IllegalArgumentException(\"Unsupported Hive type (\" +\n+                ((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory() +\n+                \") for Iceberg tables. Consider using STRING type instead.\");", "originalCommit": "ad53a0ae5d4a74ca79a3547695e4d35af480d09e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDgzNzczOQ==", "url": "https://github.com/apache/iceberg/pull/1612#discussion_r530837739", "bodyText": "Let's discuss this on he dev list where we are talking about the schema mapping.\nIf we decide that the Iceberg schema is the master and we always convert from there to Hive schema, then we can relax the 1-on-1 mapping restriction, and convert multiple Iceberg types to a single Hive type.", "author": "pvary", "createdAt": "2020-11-26T08:07:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDcwNzQ5OA=="}], "type": "inlineReview"}]}