{"pr_number": 1875, "pr_title": "Allow Spark2 DataFrame to use a custom catalog", "pr_createdAt": "2020-12-04T13:36:05Z", "pr_url": "https://github.com/apache/iceberg/pull/1875", "timeline": [{"oid": "b864ffb8a4bba4438bc2ae87b35b748509f1cb00", "url": "https://github.com/apache/iceberg/commit/b864ffb8a4bba4438bc2ae87b35b748509f1cb00", "message": "Allow Spark2 DataFrame to use a custom catalog", "committedDate": "2020-12-04T14:34:20Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTI3Mzc0Ng==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r541273746", "bodyText": "New paragraphs in Javadoc require <p>.", "author": "rdblue", "createdAt": "2020-12-11T20:48:55Z", "path": "spark2/src/main/java/org/apache/iceberg/spark/source/CustomCatalogs.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.github.benmanes.caffeine.cache.Cache;\n+import com.github.benmanes.caffeine.cache.Caffeine;\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.iceberg.CatalogProperties;\n+import org.apache.iceberg.CatalogUtil;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.sql.SparkSession;\n+\n+public final class CustomCatalogs {\n+  private static final Cache<String, Catalog> CATALOG_CACHE = Caffeine.newBuilder().softValues().build();\n+\n+  public static final String ICEBERG_CATALOG_PREFIX = \"spark.sql.catalog.iceberg.\";\n+  public static final String ICEBERG_CATALOG_TYPE = \"type\";\n+  public static final String ICEBERG_CATALOG_TYPE_HADOOP = \"hadoop\";\n+  public static final String ICEBERG_CATALOG_TYPE_HIVE = \"hive\";\n+\n+  private CustomCatalogs() {\n+  }\n+\n+  /**\n+   * Build an Iceberg {@link Catalog} to be used by this Spark source adapter.\n+   *", "originalCommit": "6009447f49c5cbb3a389529f68f30db587bee75b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjQ3NzQ5NQ==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r542477495", "bodyText": "\ud83d\udc4d", "author": "rymurr", "createdAt": "2020-12-14T15:32:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTI3Mzc0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0MTU5Mw==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547441593", "bodyText": "The usual style is to use only <p> on the newline between paragraphs.", "author": "rdblue", "createdAt": "2020-12-22T18:39:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTI3Mzc0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTI3NjAxMQ==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r541276011", "bodyText": "It looks like the options here are the read options. I don't think that it is necessary to pass any of the read options to create a catalog. In Spark 3, the catalogs exist and are configured using Spark config and the read options are independent.", "author": "rdblue", "createdAt": "2020-12-11T20:51:20Z", "path": "spark2/src/main/java/org/apache/iceberg/spark/source/CustomCatalogs.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.github.benmanes.caffeine.cache.Cache;\n+import com.github.benmanes.caffeine.cache.Caffeine;\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.iceberg.CatalogProperties;\n+import org.apache.iceberg.CatalogUtil;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.sql.SparkSession;\n+\n+public final class CustomCatalogs {\n+  private static final Cache<String, Catalog> CATALOG_CACHE = Caffeine.newBuilder().softValues().build();\n+\n+  public static final String ICEBERG_CATALOG_PREFIX = \"spark.sql.catalog.iceberg.\";\n+  public static final String ICEBERG_CATALOG_TYPE = \"type\";\n+  public static final String ICEBERG_CATALOG_TYPE_HADOOP = \"hadoop\";\n+  public static final String ICEBERG_CATALOG_TYPE_HIVE = \"hive\";\n+\n+  private CustomCatalogs() {\n+  }\n+\n+  /**\n+   * Build an Iceberg {@link Catalog} to be used by this Spark source adapter.\n+   *\n+   * The cache is to facilitate reuse of catalogs, especially if wrapped in CachingCatalog. For non-Hive catalogs all\n+   * custom parameters passed to the catalog are considered in the cache key. Hive catalogs only cache based on\n+   * the Metastore URIs as per previous behaviour.\n+   *\n+   * @param options options from Spark\n+   * @return an Iceberg catalog\n+   */\n+  public static Catalog buildIcebergCatalog(Map<String, String> options) {", "originalCommit": "6009447f49c5cbb3a389529f68f30db587bee75b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjQ3Njg1NQ==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r542476855", "bodyText": "ok, I had originally aimed to support setting catalog options via SparkConf or via read options. But I guess there isn't much of a reason to support the 2nd use case. Have removed this feature.", "author": "rymurr", "createdAt": "2020-12-14T15:32:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTI3NjAxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTI3ODM2Mw==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r541278363", "bodyText": "I would expect the cache to delegate to buildIcebergCatalog, no the other way around. What about using a simple getter:\npublic static Catalog loadCatalog(SparkSession spark, String name) {\n  return CATALOG_CACHE.get(Pair.of(spark, name));\n}\nThen this just needs to build the named catalog for a particular Spark session. And that could be done by moving the existing SparkCatalog.buildIcebergCatalog into CatalogUtil and calling it with the right name and config from the session.", "author": "rdblue", "createdAt": "2020-12-11T20:53:54Z", "path": "spark2/src/main/java/org/apache/iceberg/spark/source/CustomCatalogs.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.github.benmanes.caffeine.cache.Cache;\n+import com.github.benmanes.caffeine.cache.Caffeine;\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.iceberg.CatalogProperties;\n+import org.apache.iceberg.CatalogUtil;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.sql.SparkSession;\n+\n+public final class CustomCatalogs {\n+  private static final Cache<String, Catalog> CATALOG_CACHE = Caffeine.newBuilder().softValues().build();\n+\n+  public static final String ICEBERG_CATALOG_PREFIX = \"spark.sql.catalog.iceberg.\";\n+  public static final String ICEBERG_CATALOG_TYPE = \"type\";\n+  public static final String ICEBERG_CATALOG_TYPE_HADOOP = \"hadoop\";\n+  public static final String ICEBERG_CATALOG_TYPE_HIVE = \"hive\";\n+\n+  private CustomCatalogs() {\n+  }\n+\n+  /**\n+   * Build an Iceberg {@link Catalog} to be used by this Spark source adapter.\n+   *\n+   * The cache is to facilitate reuse of catalogs, especially if wrapped in CachingCatalog. For non-Hive catalogs all\n+   * custom parameters passed to the catalog are considered in the cache key. Hive catalogs only cache based on\n+   * the Metastore URIs as per previous behaviour.\n+   *\n+   * @param options options from Spark\n+   * @return an Iceberg catalog\n+   */\n+  public static Catalog buildIcebergCatalog(Map<String, String> options) {\n+    String name = \"spark_source\";\n+    SparkConf sparkConf = SparkSession.active().sparkContext().getConf();\n+    Map<String, String> sparkMap = Arrays.stream(sparkConf.getAllWithPrefix(ICEBERG_CATALOG_PREFIX))\n+        .collect(Collectors.toMap(x -> x._1, x -> x._2));\n+    sparkMap.putAll(options);\n+    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n+\n+    String catalogImpl = sparkMap.get(CatalogProperties.CATALOG_IMPL);\n+    if (catalogImpl != null) {\n+      String cacheKey = options.entrySet()\n+          .stream().map(x -> String.format(\"%s:%s\", x.getKey(), x.getValue())).collect(Collectors.joining(\";\"));\n+      return CATALOG_CACHE.get(cacheKey, x -> CatalogUtil.loadCatalog(catalogImpl, name, sparkMap, conf));", "originalCommit": "6009447f49c5cbb3a389529f68f30db587bee75b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjQ3NzQyNg==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r542477426", "bodyText": "I was thinking on similar lines at first also. The main reason i didn't is the HiveCatalog isn't in iceberg-core and we would get a circular dep between core and hive-metastore. We can change HiveCatalog to instantiate it similar to the custom catalogs and create it via reflection? or move the buildIcebergCatalog method higher up (eg spark module)", "author": "rymurr", "createdAt": "2020-12-14T15:32:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTI3ODM2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUxODk5OQ==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r543518999", "bodyText": "+1 for decoupling and using reflection for HiveCatalog. I think that's the right solution.", "author": "rdblue", "createdAt": "2020-12-15T16:58:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTI3ODM2Mw=="}], "type": "inlineReview"}, {"oid": "8a267310cb01236ce0335c4c82d8d22add1f3335", "url": "https://github.com/apache/iceberg/commit/8a267310cb01236ce0335c4c82d8d22add1f3335", "message": "clarify catalog util and fix tests", "committedDate": "2020-12-15T16:27:17Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUyMDA4MA==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r543520080", "bodyText": "This works, but I think it would be better to make it so that the Hive catalog can be loaded using the normal no-arg constructor followed by initialize and setConf.", "author": "rdblue", "createdAt": "2020-12-15T16:59:53Z", "path": "core/src/main/java/org/apache/iceberg/CatalogUtil.java", "diffHunk": "@@ -169,6 +174,40 @@ public static Catalog loadCatalog(\n     return catalog;\n   }\n \n+  public static Catalog buildIcebergCatalog(String name, Map<String, String> options, Configuration conf) {\n+\n+    String catalogImpl = options.get(CatalogProperties.CATALOG_IMPL);\n+    if (catalogImpl != null) {\n+      return CatalogUtil.loadCatalog(catalogImpl, name, options, conf);\n+    }\n+\n+    String catalogType = options.getOrDefault(ICEBERG_CATALOG_TYPE, ICEBERG_CATALOG_TYPE_HIVE);\n+    switch (catalogType.toLowerCase(Locale.ENGLISH)) {\n+      case ICEBERG_CATALOG_TYPE_HIVE:\n+        String clientPoolSize = options.getOrDefault(CatalogProperties.HIVE_CLIENT_POOL_SIZE,\n+            Integer.toString(CatalogProperties.HIVE_CLIENT_POOL_SIZE_DEFAULT));\n+        String uri = options.get(CatalogProperties.HIVE_URI);\n+        return buildHiveCatalog(name, uri, Integer.parseInt(clientPoolSize), conf);\n+      case ICEBERG_CATALOG_TYPE_HADOOP:\n+        String warehouseLocation = options.get(CatalogProperties.WAREHOUSE_LOCATION);\n+        return new HadoopCatalog(name, conf, warehouseLocation, options);\n+\n+      default:\n+        throw new UnsupportedOperationException(\"Unknown catalog type: \" + catalogType);\n+    }\n+  }\n+\n+  private static Catalog buildHiveCatalog(String name, String uri, int clientPoolSize, Configuration conf) {", "originalCommit": "8a267310cb01236ce0335c4c82d8d22add1f3335", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzY3MDE2OQ==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r543670169", "bodyText": "cool. I have switched both Hive and Hadoop catalogs to use this pattern. I left the old constructors and marked Deprecated.", "author": "rymurr", "createdAt": "2020-12-15T20:39:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUyMDA4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUyMDgzNg==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r543520836", "bodyText": "Nit: indentation.", "author": "rdblue", "createdAt": "2020-12-15T17:00:48Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java", "diffHunk": "@@ -61,4 +65,40 @@ public static void validatePartitionTransforms(PartitionSpec spec) {\n           String.format(\"Cannot write using unsupported transforms: %s\", unsupported));\n     }\n   }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static <C, T> Pair<C,T> catalogAndIdentifier(List<String> nameParts,\n+                                                       Function<String, C> catalog,\n+                                                       IdentiferFunction<T> identifer,\n+                                                       String[] currentNamespace) {", "originalCommit": "8a267310cb01236ce0335c4c82d8d22add1f3335", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzY3MDI1Mg==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r543670252", "bodyText": "fixed", "author": "rymurr", "createdAt": "2020-12-15T20:39:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUyMDgzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUyNjk4Nw==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r543526987", "bodyText": "This seems like a concern for CatalogUtil.buildIcebergCatalog, not here. This should create the map and pass it on to allow that method to reject the options. That way, the user gets a more specific error, like \"Missing catalog implementation class or type\". That can also create a default catalog if we choose to later -- though I'm skeptical we would -- without needing to change all of the callers.", "author": "rdblue", "createdAt": "2020-12-15T17:08:44Z", "path": "spark2/src/main/java/org/apache/iceberg/spark/source/CustomCatalogs.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.github.benmanes.caffeine.cache.Cache;\n+import com.github.benmanes.caffeine.cache.Caffeine;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.CatalogUtil;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.iceberg.relocated.com.google.common.base.Splitter;\n+import org.apache.iceberg.spark.SparkUtil;\n+import org.apache.iceberg.util.Pair;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.sql.SparkSession;\n+\n+public final class CustomCatalogs {\n+  private static final Cache<Pair<SparkSession, String>, Catalog> CATALOG_CACHE = Caffeine.newBuilder().softValues().build();\n+\n+  public static final String ICEBERG_DEFAULT_CATALOG = \"default_catalog\";\n+  public static final String ICEBERG_CATALOG_PREFIX = \"spark.sql.catalog\";\n+\n+  private CustomCatalogs() {\n+  }\n+\n+  /**\n+   * Build an Iceberg {@link Catalog} to be used by this Spark source adapter.\n+   *\n+   * <p>\n+   *   The cache is to facilitate reuse of catalogs, especially if wrapped in CachingCatalog. For non-Hive catalogs all\n+   *   custom parameters passed to the catalog are considered in the cache key. Hive catalogs only cache based on\n+   *   the Metastore URIs as per previous behaviour.\n+   * </p>\n+   *\n+   * @param spark Spark Session\n+   * @param name Catalog Name\n+   * @return an Iceberg catalog\n+   */\n+  public static Catalog buildIcebergCatalog(SparkSession spark, String name) {\n+    return CATALOG_CACHE.get(Pair.of(spark, name), CustomCatalogs::build);\n+  }\n+\n+  private static Catalog build(Pair<SparkSession, String> sparkAndName) {\n+    SparkSession spark = sparkAndName.first();\n+    String name =  sparkAndName.second() == null ? ICEBERG_DEFAULT_CATALOG : sparkAndName.second();\n+    SparkConf sparkConf = spark.sparkContext().getConf();\n+    Configuration conf = spark.sessionState().newHadoopConf();\n+\n+    String catalogPrefix = String.format(\"%s.%s.\", ICEBERG_CATALOG_PREFIX, name);\n+    Map<String, String> options = Arrays.stream(sparkConf.getAllWithPrefix(catalogPrefix))\n+        .collect(Collectors.toMap(x -> x._1, x -> x._2));\n+\n+    if (options.isEmpty() && !name.equals(ICEBERG_DEFAULT_CATALOG)) {\n+      throw new IllegalArgumentException(String.format(\"Cannot instantiate catalog %s. Incorrect Parameters\", name));", "originalCommit": "8a267310cb01236ce0335c4c82d8d22add1f3335", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzY3NTM0MA==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r543675340", "bodyText": "the problem arises when a eg default.table is passed into this method. It will end up constructing a Hive catalog w/ existing options called default. The Hive catalog tries to pull config from Configuration if not passed via the constructor. So we eat the namespace and using it in the catalog name rather than constructing a default catalog and a table identifier default.table.\nWe could:\n\nnot allow catalog names to be specified in Spark2 sources\nStop using Configuration in hive tables\nkeep this the way it is\n\nI prefer option 2 but i am worried that may have unintended consequences elsewhere.", "author": "rymurr", "createdAt": "2020-12-15T20:48:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUyNjk4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0NDMyNA==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547444324", "bodyText": "I don't think I quite understand you here.\nIn Spark, a catalog exists if spark.sql.catalog.name is present and is a class that can be loaded. I would expect this method to return null if that property isn't the case. For Spark 2.4, I think it would be fine to assume that the class will be SparkCatalog.class.getName(). If it isn't, then return null. If it is, then the catalog exists and this should load the IcebergCatalog that would be used by SparkCatalog in Spark 3.x. Does that make sense?", "author": "rdblue", "createdAt": "2020-12-22T18:45:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUyNjk4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUxNjE5OQ==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547516199", "bodyText": "To put it another way. If spark is set up to with spark.sql.catalog.name set then behave as if you are Spark3 and return a custom catalog. Otherwise return Hive.", "author": "rymurr", "createdAt": "2020-12-22T21:30:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUyNjk4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0MzAwNg==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547543006", "bodyText": "Yep! I would say that this should just respect Spark 3 settings.", "author": "rdblue", "createdAt": "2020-12-22T22:45:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUyNjk4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0NTA1Mw==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547545053", "bodyText": "cool. I have attempted to do that here. The complication is the same method fetches default catalog and custom catalog so there is an extra check there. Can separate out if you prefer.", "author": "rymurr", "createdAt": "2020-12-22T22:51:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUyNjk4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUyODAwMg==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r543528002", "bodyText": "This isn't really a \"build\" method any more since it wraps the cache, it is more of a \"load\" I think.", "author": "rdblue", "createdAt": "2020-12-15T17:10:07Z", "path": "spark2/src/main/java/org/apache/iceberg/spark/source/CustomCatalogs.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.github.benmanes.caffeine.cache.Cache;\n+import com.github.benmanes.caffeine.cache.Caffeine;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.CatalogUtil;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.iceberg.relocated.com.google.common.base.Splitter;\n+import org.apache.iceberg.spark.SparkUtil;\n+import org.apache.iceberg.util.Pair;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.sql.SparkSession;\n+\n+public final class CustomCatalogs {\n+  private static final Cache<Pair<SparkSession, String>, Catalog> CATALOG_CACHE = Caffeine.newBuilder().softValues().build();\n+\n+  public static final String ICEBERG_DEFAULT_CATALOG = \"default_catalog\";\n+  public static final String ICEBERG_CATALOG_PREFIX = \"spark.sql.catalog\";\n+\n+  private CustomCatalogs() {\n+  }\n+\n+  /**\n+   * Build an Iceberg {@link Catalog} to be used by this Spark source adapter.\n+   *\n+   * <p>\n+   *   The cache is to facilitate reuse of catalogs, especially if wrapped in CachingCatalog. For non-Hive catalogs all\n+   *   custom parameters passed to the catalog are considered in the cache key. Hive catalogs only cache based on\n+   *   the Metastore URIs as per previous behaviour.\n+   * </p>\n+   *\n+   * @param spark Spark Session\n+   * @param name Catalog Name\n+   * @return an Iceberg catalog\n+   */\n+  public static Catalog buildIcebergCatalog(SparkSession spark, String name) {", "originalCommit": "8a267310cb01236ce0335c4c82d8d22add1f3335", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzY3MDM4Nw==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r543670387", "bodyText": "fixed", "author": "rymurr", "createdAt": "2020-12-15T20:39:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUyODAwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0ODM0Nw==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547548347", "bodyText": "Sorry, I think this method should be renamed to loadCatalog. The private method was fine as buildCatalog.", "author": "rdblue", "createdAt": "2020-12-22T23:03:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUyODAwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Nzk5NzIxMA==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547997210", "bodyText": "ahh, yup. Agreed.", "author": "rymurr", "createdAt": "2020-12-23T14:55:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUyODAwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUyODkyNA==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r543528924", "bodyText": "Looks like some imports might be stale.", "author": "rdblue", "createdAt": "2020-12-15T17:11:19Z", "path": "spark2/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -24,16 +24,16 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n import org.apache.iceberg.catalog.TableIdentifier;\n import org.apache.iceberg.encryption.EncryptionManager;\n import org.apache.iceberg.hadoop.HadoopTables;\n-import org.apache.iceberg.hive.HiveCatalog;\n-import org.apache.iceberg.hive.HiveCatalogs;\n import org.apache.iceberg.io.FileIO;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.spark.SparkSchemaUtil;\n import org.apache.iceberg.spark.SparkUtil;\n import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.Pair;", "originalCommit": "8a267310cb01236ce0335c4c82d8d22add1f3335", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzY3MDQzMA==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r543670430", "bodyText": "fixed", "author": "rymurr", "createdAt": "2020-12-15T20:39:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzUyODkyNA=="}], "type": "inlineReview"}, {"oid": "dc8baca0a6e3919360a5eefc517080967f13e04b", "url": "https://github.com/apache/iceberg/commit/dc8baca0a6e3919360a5eefc517080967f13e04b", "message": "refactor and clean-up", "committedDate": "2020-12-15T20:52:21Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQzODYwOQ==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547438609", "bodyText": "This adds properties to the map twice?\nWe usually prefer Maps.newHashMap(), too.", "author": "rdblue", "createdAt": "2020-12-22T18:32:34Z", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java", "diffHunk": "@@ -68,45 +70,62 @@\n  *\n  * Note: The HadoopCatalog requires that the underlying file system supports atomic rename.\n  */\n-public class HadoopCatalog extends BaseMetastoreCatalog implements Closeable, SupportsNamespaces {\n+public class HadoopCatalog extends BaseMetastoreCatalog implements Closeable, SupportsNamespaces, Configurable {\n \n   private static final String ICEBERG_HADOOP_WAREHOUSE_BASE = \"iceberg/warehouse\";\n   private static final String TABLE_METADATA_FILE_EXTENSION = \".metadata.json\";\n   private static final Joiner SLASH = Joiner.on(\"/\");\n   private static final PathFilter TABLE_FILTER = path -> path.getName().endsWith(TABLE_METADATA_FILE_EXTENSION);\n \n-  private final String catalogName;\n-  private final Configuration conf;\n-  private final String warehouseLocation;\n-  private final FileSystem fs;\n-  private final FileIO fileIO;\n+  private String catalogName;\n+  private Configuration conf;\n+  private String warehouseLocation;\n+  private FileSystem fs;\n+  private FileIO fileIO;\n+\n+  public HadoopCatalog(){\n+  }\n \n   /**\n    * The constructor of the HadoopCatalog. It uses the passed location as its warehouse directory.\n    *\n+   * @deprecated please use the no-arg constructor, setConf and initialize to construct the catalog\n    * @param name The catalog name\n    * @param conf The Hadoop configuration\n    * @param warehouseLocation The location used as warehouse directory\n    */\n+  @Deprecated\n   public HadoopCatalog(String name, Configuration conf, String warehouseLocation) {\n     this(name, conf, warehouseLocation, Maps.newHashMap());\n   }\n \n   /**\n    * The all-arg constructor of the HadoopCatalog.\n    *\n+   * @deprecated please use the no-arg constructor, setConf and initialize to construct the catalog\n    * @param name The catalog name\n    * @param conf The Hadoop configuration\n    * @param warehouseLocation The location used as warehouse directory\n    * @param properties catalog properties\n    */\n+  @Deprecated\n   public HadoopCatalog(String name, Configuration conf, String warehouseLocation, Map<String, String> properties) {\n     Preconditions.checkArgument(warehouseLocation != null && !warehouseLocation.equals(\"\"),\n         \"no location provided for warehouse\");\n+    setConf(conf);\n+    Map<String, String> props = new HashMap<>(properties);\n+    props.putAll(properties);", "originalCommit": "25b2f918c96611af21de42c5a47cf8fa3b824ae2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUyNDk0MQ==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547524941", "bodyText": "sigh...dumb error. Fixed", "author": "rymurr", "createdAt": "2020-12-22T21:54:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQzODYwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQzOTY0OQ==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547439649", "bodyText": "Nit: Error messages should use sentence case, with the first word capitalized. I'd also make it more clear that warehouse is the configuration key to set, like \"Cannot create Hadoop catalog without 'warehouse' location\".", "author": "rdblue", "createdAt": "2020-12-22T18:34:53Z", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java", "diffHunk": "@@ -68,45 +70,62 @@\n  *\n  * Note: The HadoopCatalog requires that the underlying file system supports atomic rename.\n  */\n-public class HadoopCatalog extends BaseMetastoreCatalog implements Closeable, SupportsNamespaces {\n+public class HadoopCatalog extends BaseMetastoreCatalog implements Closeable, SupportsNamespaces, Configurable {\n \n   private static final String ICEBERG_HADOOP_WAREHOUSE_BASE = \"iceberg/warehouse\";\n   private static final String TABLE_METADATA_FILE_EXTENSION = \".metadata.json\";\n   private static final Joiner SLASH = Joiner.on(\"/\");\n   private static final PathFilter TABLE_FILTER = path -> path.getName().endsWith(TABLE_METADATA_FILE_EXTENSION);\n \n-  private final String catalogName;\n-  private final Configuration conf;\n-  private final String warehouseLocation;\n-  private final FileSystem fs;\n-  private final FileIO fileIO;\n+  private String catalogName;\n+  private Configuration conf;\n+  private String warehouseLocation;\n+  private FileSystem fs;\n+  private FileIO fileIO;\n+\n+  public HadoopCatalog(){\n+  }\n \n   /**\n    * The constructor of the HadoopCatalog. It uses the passed location as its warehouse directory.\n    *\n+   * @deprecated please use the no-arg constructor, setConf and initialize to construct the catalog\n    * @param name The catalog name\n    * @param conf The Hadoop configuration\n    * @param warehouseLocation The location used as warehouse directory\n    */\n+  @Deprecated\n   public HadoopCatalog(String name, Configuration conf, String warehouseLocation) {\n     this(name, conf, warehouseLocation, Maps.newHashMap());\n   }\n \n   /**\n    * The all-arg constructor of the HadoopCatalog.\n    *\n+   * @deprecated please use the no-arg constructor, setConf and initialize to construct the catalog\n    * @param name The catalog name\n    * @param conf The Hadoop configuration\n    * @param warehouseLocation The location used as warehouse directory\n    * @param properties catalog properties\n    */\n+  @Deprecated\n   public HadoopCatalog(String name, Configuration conf, String warehouseLocation, Map<String, String> properties) {\n     Preconditions.checkArgument(warehouseLocation != null && !warehouseLocation.equals(\"\"),\n         \"no location provided for warehouse\");\n+    setConf(conf);\n+    Map<String, String> props = new HashMap<>(properties);\n+    props.putAll(properties);\n+    props.put(CatalogProperties.WAREHOUSE_LOCATION, warehouseLocation);\n+    initialize(name, props);\n+  }\n \n+  @Override\n+  public void initialize(String name, Map<String, String> properties) {\n+    String inputWarehouseLocation = properties.get(CatalogProperties.WAREHOUSE_LOCATION);\n+    Preconditions.checkArgument(inputWarehouseLocation != null && !inputWarehouseLocation.equals(\"\"),\n+        \"no location provided for warehouse\");", "originalCommit": "25b2f918c96611af21de42c5a47cf8fa3b824ae2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUyNDk3MQ==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547524971", "bodyText": "sure, copy/paste...I have now fixed both instances", "author": "rymurr", "createdAt": "2020-12-22T21:54:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQzOTY0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0MDExNA==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547440114", "bodyText": "Can you call this last? I think it is typically set after calling initialize. The important thing is to try to use the same order.", "author": "rdblue", "createdAt": "2020-12-22T18:35:58Z", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java", "diffHunk": "@@ -68,45 +70,62 @@\n  *\n  * Note: The HadoopCatalog requires that the underlying file system supports atomic rename.\n  */\n-public class HadoopCatalog extends BaseMetastoreCatalog implements Closeable, SupportsNamespaces {\n+public class HadoopCatalog extends BaseMetastoreCatalog implements Closeable, SupportsNamespaces, Configurable {\n \n   private static final String ICEBERG_HADOOP_WAREHOUSE_BASE = \"iceberg/warehouse\";\n   private static final String TABLE_METADATA_FILE_EXTENSION = \".metadata.json\";\n   private static final Joiner SLASH = Joiner.on(\"/\");\n   private static final PathFilter TABLE_FILTER = path -> path.getName().endsWith(TABLE_METADATA_FILE_EXTENSION);\n \n-  private final String catalogName;\n-  private final Configuration conf;\n-  private final String warehouseLocation;\n-  private final FileSystem fs;\n-  private final FileIO fileIO;\n+  private String catalogName;\n+  private Configuration conf;\n+  private String warehouseLocation;\n+  private FileSystem fs;\n+  private FileIO fileIO;\n+\n+  public HadoopCatalog(){\n+  }\n \n   /**\n    * The constructor of the HadoopCatalog. It uses the passed location as its warehouse directory.\n    *\n+   * @deprecated please use the no-arg constructor, setConf and initialize to construct the catalog\n    * @param name The catalog name\n    * @param conf The Hadoop configuration\n    * @param warehouseLocation The location used as warehouse directory\n    */\n+  @Deprecated\n   public HadoopCatalog(String name, Configuration conf, String warehouseLocation) {\n     this(name, conf, warehouseLocation, Maps.newHashMap());\n   }\n \n   /**\n    * The all-arg constructor of the HadoopCatalog.\n    *\n+   * @deprecated please use the no-arg constructor, setConf and initialize to construct the catalog\n    * @param name The catalog name\n    * @param conf The Hadoop configuration\n    * @param warehouseLocation The location used as warehouse directory\n    * @param properties catalog properties\n    */\n+  @Deprecated\n   public HadoopCatalog(String name, Configuration conf, String warehouseLocation, Map<String, String> properties) {\n     Preconditions.checkArgument(warehouseLocation != null && !warehouseLocation.equals(\"\"),\n         \"no location provided for warehouse\");\n+    setConf(conf);", "originalCommit": "25b2f918c96611af21de42c5a47cf8fa3b824ae2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUyNDk5MA==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547524990", "bodyText": "In catalog util setConf is called first. I think everywhere initialize is called setConf is called before", "author": "rymurr", "createdAt": "2020-12-22T21:54:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0MDExNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0NDc1MA==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547544750", "bodyText": "Okay, I was wrong then. The important thing is being consistent so you've already done that.", "author": "rdblue", "createdAt": "2020-12-22T22:50:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0MDExNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0MDM0OQ==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547440349", "bodyText": "Can you also add when this will be removed? Like \"will be removed in 0.12.0\".", "author": "rdblue", "createdAt": "2020-12-22T18:36:29Z", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java", "diffHunk": "@@ -68,45 +70,62 @@\n  *\n  * Note: The HadoopCatalog requires that the underlying file system supports atomic rename.\n  */\n-public class HadoopCatalog extends BaseMetastoreCatalog implements Closeable, SupportsNamespaces {\n+public class HadoopCatalog extends BaseMetastoreCatalog implements Closeable, SupportsNamespaces, Configurable {\n \n   private static final String ICEBERG_HADOOP_WAREHOUSE_BASE = \"iceberg/warehouse\";\n   private static final String TABLE_METADATA_FILE_EXTENSION = \".metadata.json\";\n   private static final Joiner SLASH = Joiner.on(\"/\");\n   private static final PathFilter TABLE_FILTER = path -> path.getName().endsWith(TABLE_METADATA_FILE_EXTENSION);\n \n-  private final String catalogName;\n-  private final Configuration conf;\n-  private final String warehouseLocation;\n-  private final FileSystem fs;\n-  private final FileIO fileIO;\n+  private String catalogName;\n+  private Configuration conf;\n+  private String warehouseLocation;\n+  private FileSystem fs;\n+  private FileIO fileIO;\n+\n+  public HadoopCatalog(){\n+  }\n \n   /**\n    * The constructor of the HadoopCatalog. It uses the passed location as its warehouse directory.\n    *\n+   * @deprecated please use the no-arg constructor, setConf and initialize to construct the catalog\n    * @param name The catalog name\n    * @param conf The Hadoop configuration\n    * @param warehouseLocation The location used as warehouse directory\n    */\n+  @Deprecated\n   public HadoopCatalog(String name, Configuration conf, String warehouseLocation) {\n     this(name, conf, warehouseLocation, Maps.newHashMap());\n   }\n \n   /**\n    * The all-arg constructor of the HadoopCatalog.\n    *\n+   * @deprecated please use the no-arg constructor, setConf and initialize to construct the catalog", "originalCommit": "25b2f918c96611af21de42c5a47cf8fa3b824ae2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUyNTAwNA==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547525004", "bodyText": "done", "author": "rymurr", "createdAt": "2020-12-22T21:54:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0MDM0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0NTA4NQ==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547445085", "bodyText": "Looks like this newline should be above the return to separate it from the control flow.", "author": "rdblue", "createdAt": "2020-12-22T18:47:10Z", "path": "core/src/main/java/org/apache/iceberg/CatalogUtil.java", "diffHunk": "@@ -169,6 +175,26 @@ public static Catalog loadCatalog(\n     return catalog;\n   }\n \n+  public static Catalog buildIcebergCatalog(String name, Map<String, String> options, Configuration conf) {\n+\n+    String catalogImpl = options.get(CatalogProperties.CATALOG_IMPL);\n+    if (catalogImpl == null) {\n+      String catalogType = options.getOrDefault(ICEBERG_CATALOG_TYPE, ICEBERG_CATALOG_TYPE_HIVE);\n+      switch (catalogType.toLowerCase(Locale.ENGLISH)) {\n+        case ICEBERG_CATALOG_TYPE_HIVE:\n+          catalogImpl = ICEBERG_CATALOG_HIVE;\n+          break;\n+        case ICEBERG_CATALOG_TYPE_HADOOP:\n+          catalogImpl = ICEBERG_CATALOG_HADOOP;\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\"Unknown catalog type: \" + catalogType);\n+      }\n+    }\n+    return CatalogUtil.loadCatalog(catalogImpl, name, options, conf);\n+", "originalCommit": "25b2f918c96611af21de42c5a47cf8fa3b824ae2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUyNTAyNQ==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547525025", "bodyText": "done", "author": "rymurr", "createdAt": "2020-12-22T21:54:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0NTA4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0NTEyOQ==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547445129", "bodyText": "No need to start methods with a newline.", "author": "rdblue", "createdAt": "2020-12-22T18:47:18Z", "path": "core/src/main/java/org/apache/iceberg/CatalogUtil.java", "diffHunk": "@@ -169,6 +175,26 @@ public static Catalog loadCatalog(\n     return catalog;\n   }\n \n+  public static Catalog buildIcebergCatalog(String name, Map<String, String> options, Configuration conf) {\n+", "originalCommit": "25b2f918c96611af21de42c5a47cf8fa3b824ae2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUyNTA0MA==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547525040", "bodyText": "done", "author": "rymurr", "createdAt": "2020-12-22T21:54:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0NTEyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0NjEyOA==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547446128", "bodyText": "Rather than try/catch, I think this should check whether catalog.apply returns null. If the result is null, then the catalog does not exist and it should not be set in the pair (set null). Then the caller can fill in the default catalog.", "author": "rdblue", "createdAt": "2020-12-22T18:49:28Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java", "diffHunk": "@@ -61,4 +65,40 @@ public static void validatePartitionTransforms(PartitionSpec spec) {\n           String.format(\"Cannot write using unsupported transforms: %s\", unsupported));\n     }\n   }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static <C, T> Pair<C, T> catalogAndIdentifier(List<String> nameParts,\n+                                                       Function<String, C> catalog,\n+                                                       IdentiferFunction<T> identifer,\n+                                                       String[] currentNamespace) {\n+    Preconditions.checkArgument(!nameParts.isEmpty(),\n+        \"Cannot determine catalog and Identifier from empty name parts\");\n+\n+    int lastElementIndex = nameParts.size() - 1;\n+    String name = nameParts.get(lastElementIndex);\n+\n+    if (nameParts.size() == 1) {\n+      // Only a single element, use current catalog and namespace\n+      return Pair.of(catalog.apply(null), identifer.of(currentNamespace, name));\n+    } else {\n+      try {", "originalCommit": "25b2f918c96611af21de42c5a47cf8fa3b824ae2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUyNTA1NA==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547525054", "bodyText": "done", "author": "rymurr", "createdAt": "2020-12-22T21:54:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0NjEyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0Nzc3NA==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547447774", "bodyText": "I think that the logic here should be identical to the Spark 3 case, but with the catalog load function and identifier construction replaced. That doesn't appear to be what is done because catalog.apply is used when the catalog is not set (1 part name).", "author": "rdblue", "createdAt": "2020-12-22T18:53:07Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java", "diffHunk": "@@ -61,4 +65,40 @@ public static void validatePartitionTransforms(PartitionSpec spec) {\n           String.format(\"Cannot write using unsupported transforms: %s\", unsupported));\n     }\n   }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static <C, T> Pair<C, T> catalogAndIdentifier(List<String> nameParts,", "originalCommit": "25b2f918c96611af21de42c5a47cf8fa3b824ae2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUyNTA4OQ==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547525089", "bodyText": "This was originally meant to return the default catalog if the catalog function got a null. Have now adjusted to add a default catalog parameter and have modified Spark3Util to use this function also. Ensuring they have the same behaviour.", "author": "rymurr", "createdAt": "2020-12-22T21:54:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0Nzc3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0ODg4MQ==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547448881", "bodyText": "Can you use the variant of this that checks the exception message?", "author": "rdblue", "createdAt": "2020-12-22T18:55:40Z", "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestCustomCatalog.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.File;\n+import java.util.List;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.CatalogProperties;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public class TestCustomCatalog {\n+  private static final String CATALOG_IMPL = String.format(\"%s.%s.%s\", CustomCatalogs.ICEBERG_CATALOG_PREFIX,\n+      CustomCatalogs.ICEBERG_DEFAULT_CATALOG, CatalogProperties.CATALOG_IMPL);\n+  private static final String WAREHOUSE = String.format(\"%s.%s.%s\", CustomCatalogs.ICEBERG_CATALOG_PREFIX,\n+      CustomCatalogs.ICEBERG_DEFAULT_CATALOG, CatalogProperties.WAREHOUSE_LOCATION);\n+  private static final String URI_KEY = String.format(\"%s.%s.%s\", CustomCatalogs.ICEBERG_CATALOG_PREFIX,\n+      CustomCatalogs.ICEBERG_DEFAULT_CATALOG, CatalogProperties.HIVE_URI);\n+  private static final String URI_VAL = \"thrift://localhost:12345\"; // dummy uri\n+  private static final String CATALOG_VAL = \"org.apache.iceberg.spark.source.TestCatalog\";\n+  private static final TableIdentifier TABLE = TableIdentifier.of(\"default\", \"table\");\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"id\", Types.IntegerType.get()),\n+      optional(2, \"data\", Types.StringType.get())\n+  );\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  File tableDir = null;\n+  String tableLocation = null;\n+  HadoopTables tables;\n+\n+  protected static SparkSession spark = null;\n+\n+  @BeforeClass\n+  public static void startMetastoreAndSpark() {\n+    spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n+  }\n+\n+  @AfterClass\n+  public static void stopMetastoreAndSpark() {\n+    spark.stop();\n+    spark = null;\n+  }\n+\n+  @Before\n+  public void setupTable() throws Exception {\n+    this.tables = new HadoopTables(spark.sessionState().newHadoopConf());\n+    this.tableDir = temp.newFolder();\n+    tableDir.delete(); // created by table create\n+    this.tableLocation = tableDir.toURI().toString();\n+    tables.create(SCHEMA, PartitionSpec.unpartitioned(), String.format(\"%s/%s\", tableLocation, TABLE.name()));\n+  }\n+\n+  @After\n+  public void removeTable() {\n+    SparkConf sparkConf = spark.sparkContext().conf();\n+    sparkConf.remove(CATALOG_IMPL);\n+    sparkConf.remove(WAREHOUSE);\n+    sparkConf.remove(URI_KEY);\n+    tables.dropTable(String.format(\"%s/%s\", tableLocation, TABLE.name()));\n+    tableDir.delete();\n+    CustomCatalogs.clearCache();\n+  }\n+\n+  @Test\n+  public void withSparkOptions() {\n+\n+    SparkConf sparkConf = spark.sparkContext().conf();\n+    sparkConf.set(CATALOG_IMPL, CATALOG_VAL);\n+    sparkConf.set(URI_KEY, URI_VAL);\n+\n+    List<SimpleRecord> expected = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\"),\n+        new SimpleRecord(2, \"b\"),\n+        new SimpleRecord(3, \"c\")\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, SimpleRecord.class);\n+    AssertHelpers.assertThrows(\"We have not set all properties\", IllegalArgumentException.class, () ->", "originalCommit": "25b2f918c96611af21de42c5a47cf8fa3b824ae2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0MDc0Nw==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547540747", "bodyText": "done", "author": "rymurr", "createdAt": "2020-12-22T22:37:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0ODg4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0OTcyMA==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547449720", "bodyText": "This makes config properties case sensitive. Should we convert to a Java case insensitive map?", "author": "rdblue", "createdAt": "2020-12-22T18:57:21Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/SparkCatalog.java", "diffHunk": "@@ -101,27 +94,7 @@\n    */\n   protected Catalog buildIcebergCatalog(String name, CaseInsensitiveStringMap options) {\n     Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-\n-    String catalogImpl = options.get(CatalogProperties.CATALOG_IMPL);\n-    if (catalogImpl != null) {\n-      return CatalogUtil.loadCatalog(catalogImpl, name, options, conf);\n-    }\n-\n-    String catalogType = options.getOrDefault(ICEBERG_CATALOG_TYPE, ICEBERG_CATALOG_TYPE_HIVE);\n-    switch (catalogType.toLowerCase(Locale.ENGLISH)) {\n-      case ICEBERG_CATALOG_TYPE_HIVE:\n-        int clientPoolSize = options.getInt(CatalogProperties.HIVE_CLIENT_POOL_SIZE,\n-            CatalogProperties.HIVE_CLIENT_POOL_SIZE_DEFAULT);\n-        String uri = options.get(CatalogProperties.HIVE_URI);\n-        return new HiveCatalog(name, uri, clientPoolSize, conf);\n-\n-      case ICEBERG_CATALOG_TYPE_HADOOP:\n-        String warehouseLocation = options.get(CatalogProperties.WAREHOUSE_LOCATION);\n-        return new HadoopCatalog(name, conf, warehouseLocation, options.asCaseSensitiveMap());\n-\n-      default:\n-        throw new UnsupportedOperationException(\"Unknown catalog type: \" + catalogType);\n-    }\n+    return CatalogUtil.buildIcebergCatalog(name, options.asCaseSensitiveMap(), conf);", "originalCommit": "25b2f918c96611af21de42c5a47cf8fa3b824ae2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUyNTE0Mw==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547525143", "bodyText": "good catch, fixed", "author": "rymurr", "createdAt": "2020-12-22T21:55:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0OTcyMA=="}], "type": "inlineReview"}, {"oid": "a92b06cf32a5583cea9e8228d2a1a56fe35d6352", "url": "https://github.com/apache/iceberg/commit/a92b06cf32a5583cea9e8228d2a1a56fe35d6352", "message": "address code review", "committedDate": "2020-12-22T22:42:16Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0NTE1NA==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547545154", "bodyText": "Current catalog? I think it would be current catalog and current namespace.", "author": "rdblue", "createdAt": "2020-12-22T22:52:14Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java", "diffHunk": "@@ -61,4 +65,42 @@ public static void validatePartitionTransforms(PartitionSpec spec) {\n           String.format(\"Cannot write using unsupported transforms: %s\", unsupported));\n     }\n   }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static <C, T> Pair<C, T> catalogAndIdentifier(List<String> nameParts,\n+                                                       Function<String, C> catalogProvider,\n+                                                       IdentiferFunction<T> identiferProvider,\n+                                                       C defaultCatalog,", "originalCommit": "a92b06cf32a5583cea9e8228d2a1a56fe35d6352", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Nzk5MjUzMg==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547992532", "bodyText": "It is the default catalog in Spark3Util on master. Is that wrong? current catalog seems more appropriate to me...", "author": "rymurr", "createdAt": "2020-12-23T14:45:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0NTE1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUxMTU3MQ==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r549511571", "bodyText": "I think it's okay. The logic is correct.", "author": "rdblue", "createdAt": "2020-12-28T22:56:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0NTE1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0NTQ4NQ==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547545485", "bodyText": "\"name parts\" is an internal thing. What about dropping \"parts\" and just referring to it as \"name\"?\nAlso, no need to capitalize identifier.", "author": "rdblue", "createdAt": "2020-12-22T22:53:23Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java", "diffHunk": "@@ -61,4 +65,42 @@ public static void validatePartitionTransforms(PartitionSpec spec) {\n           String.format(\"Cannot write using unsupported transforms: %s\", unsupported));\n     }\n   }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static <C, T> Pair<C, T> catalogAndIdentifier(List<String> nameParts,\n+                                                       Function<String, C> catalogProvider,\n+                                                       IdentiferFunction<T> identiferProvider,\n+                                                       C defaultCatalog,\n+                                                       String[] currentNamespace) {\n+    Preconditions.checkArgument(!nameParts.isEmpty(),\n+        \"Cannot determine catalog and Identifier from empty name parts\");", "originalCommit": "a92b06cf32a5583cea9e8228d2a1a56fe35d6352", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Nzk5MjU2Nw==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547992567", "bodyText": "agreed, done.", "author": "rymurr", "createdAt": "2020-12-23T14:45:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0NTQ4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0NTk0Nw==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547545947", "bodyText": "Does this need subList if it is returning the entire list?", "author": "rdblue", "createdAt": "2020-12-22T22:54:47Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java", "diffHunk": "@@ -61,4 +65,42 @@ public static void validatePartitionTransforms(PartitionSpec spec) {\n           String.format(\"Cannot write using unsupported transforms: %s\", unsupported));\n     }\n   }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static <C, T> Pair<C, T> catalogAndIdentifier(List<String> nameParts,\n+                                                       Function<String, C> catalogProvider,\n+                                                       IdentiferFunction<T> identiferProvider,\n+                                                       C defaultCatalog,\n+                                                       String[] currentNamespace) {\n+    Preconditions.checkArgument(!nameParts.isEmpty(),\n+        \"Cannot determine catalog and Identifier from empty name parts\");\n+\n+    int lastElementIndex = nameParts.size() - 1;\n+    String name = nameParts.get(lastElementIndex);\n+\n+    if (nameParts.size() == 1) {\n+      // Only a single element, use current catalog and namespace\n+      return Pair.of(defaultCatalog, identiferProvider.of(currentNamespace, name));\n+    } else {\n+      C catalog = catalogProvider.apply(nameParts.get(0));\n+      if (catalog == null) {\n+        // The first element was not a valid catalog, treat it like part of the namespace\n+        String[] namespace =  nameParts.subList(0, lastElementIndex).toArray(new String[0]);", "originalCommit": "a92b06cf32a5583cea9e8228d2a1a56fe35d6352", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Nzk5MzUyMA==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547993520", "bodyText": "Good point, just copied from Spark3Util but it doesn't make much sense.", "author": "rymurr", "createdAt": "2020-12-23T14:47:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0NTk0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODA1MTk2MQ==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r548051961", "bodyText": "actually, scratch that. It does make sense. It takes [a,b,c] as the namespace and d as the name for [a,b,c,d]. So I have left it in.\nThe other one returns a as the catalog [b,c] as the namespace and d as the table.", "author": "rymurr", "createdAt": "2020-12-23T16:54:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0NTk0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUwOTM1OA==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r549509358", "bodyText": "Right, toIndex is exclusive, so this removes the last element that is passed as the name. Thanks!", "author": "rdblue", "createdAt": "2020-12-28T22:45:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0NTk0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0NjI3Ng==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547546276", "bodyText": "This could be BiFunction<String[], String, T>. Then you wouldn't need a separate interface for it.", "author": "rdblue", "createdAt": "2020-12-22T22:55:47Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkUtil.java", "diffHunk": "@@ -61,4 +65,42 @@ public static void validatePartitionTransforms(PartitionSpec spec) {\n           String.format(\"Cannot write using unsupported transforms: %s\", unsupported));\n     }\n   }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static <C, T> Pair<C, T> catalogAndIdentifier(List<String> nameParts,\n+                                                       Function<String, C> catalogProvider,\n+                                                       IdentiferFunction<T> identiferProvider,\n+                                                       C defaultCatalog,\n+                                                       String[] currentNamespace) {\n+    Preconditions.checkArgument(!nameParts.isEmpty(),\n+        \"Cannot determine catalog and Identifier from empty name parts\");\n+\n+    int lastElementIndex = nameParts.size() - 1;\n+    String name = nameParts.get(lastElementIndex);\n+\n+    if (nameParts.size() == 1) {\n+      // Only a single element, use current catalog and namespace\n+      return Pair.of(defaultCatalog, identiferProvider.of(currentNamespace, name));\n+    } else {\n+      C catalog = catalogProvider.apply(nameParts.get(0));\n+      if (catalog == null) {\n+        // The first element was not a valid catalog, treat it like part of the namespace\n+        String[] namespace =  nameParts.subList(0, lastElementIndex).toArray(new String[0]);\n+        return Pair.of(defaultCatalog, identiferProvider.of(namespace, name));\n+      } else {\n+        // Assume the first element is a valid catalog\n+        String[] namespace = nameParts.subList(1, lastElementIndex).toArray(new String[0]);\n+        return Pair.of(catalog, identiferProvider.of(namespace, name));\n+      }\n+    }\n+  }\n+\n+  public interface IdentiferFunction<T> {", "originalCommit": "a92b06cf32a5583cea9e8228d2a1a56fe35d6352", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Nzk5NDQ0Mg==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547994442", "bodyText": "agreed, don't know why I couldn't find that before", "author": "rymurr", "createdAt": "2020-12-23T14:49:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0NjI3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0NzE2NQ==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547547165", "bodyText": "When I suggested using SparkCatalog earlier, I didn't think about how it isn't defined for 2.4. Instead of this check, let's just check whether the catalog property is defined at all. As long as catalogName is non-null, it is a catalog for the purposes of this config. If the catalog doesn't have a valid type or implementation class then loading it will fail.\nAlso, it would be catalogImpl not name because name is the catalog name.", "author": "rdblue", "createdAt": "2020-12-22T22:58:51Z", "path": "spark2/src/main/java/org/apache/iceberg/spark/source/CustomCatalogs.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.github.benmanes.caffeine.cache.Cache;\n+import com.github.benmanes.caffeine.cache.Caffeine;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.CatalogUtil;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.iceberg.relocated.com.google.common.base.Splitter;\n+import org.apache.iceberg.spark.SparkUtil;\n+import org.apache.iceberg.util.Pair;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.sql.SparkSession;\n+\n+public final class CustomCatalogs {\n+  private static final Cache<Pair<SparkSession, String>, Catalog> CATALOG_CACHE = Caffeine.newBuilder()\n+      .softValues().build();\n+\n+  public static final String ICEBERG_DEFAULT_CATALOG = \"default_catalog\";\n+  public static final String ICEBERG_CATALOG_PREFIX = \"spark.sql.catalog\";\n+\n+  private CustomCatalogs() {\n+  }\n+\n+  /**\n+   * Build an Iceberg {@link Catalog} to be used by this Spark source adapter.\n+   *\n+   * <p>\n+   * The cache is to facilitate reuse of catalogs, especially if wrapped in CachingCatalog. For non-Hive catalogs all\n+   * custom parameters passed to the catalog are considered in the cache key. Hive catalogs only cache based on\n+   * the Metastore URIs as per previous behaviour.\n+   *\n+   *\n+   * @param spark Spark Session\n+   * @param name Catalog Name\n+   * @return an Iceberg catalog\n+   */\n+  public static Catalog buildIcebergCatalog(SparkSession spark, String name) {\n+    return CATALOG_CACHE.get(Pair.of(spark, name), CustomCatalogs::load);\n+  }\n+\n+  private static Catalog load(Pair<SparkSession, String> sparkAndName) {\n+    SparkSession spark = sparkAndName.first();\n+    String name = sparkAndName.second();\n+    SparkConf sparkConf = spark.sparkContext().getConf();\n+    Configuration conf = spark.sessionState().newHadoopConf();\n+\n+    String catalogPrefix = String.format(\"%s.%s\", ICEBERG_CATALOG_PREFIX, name);\n+    String catalogName = sparkConf.get(catalogPrefix, null);\n+    if (!name.equals(ICEBERG_DEFAULT_CATALOG) &&\n+        !org.apache.spark.sql.catalog.Catalog.class.getName().equals(catalogName)) {", "originalCommit": "a92b06cf32a5583cea9e8228d2a1a56fe35d6352", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Nzk5NTg3NQ==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547995875", "bodyText": "cool, changed to an existence check.\n\nAlso, it would be catalogImpl not name because name is the catalog name.\n\nNot sure what you mean wrt the catalogImpl comment?", "author": "rymurr", "createdAt": "2020-12-23T14:52:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0NzE2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUwNzc0Nw==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r549507747", "bodyText": "The variable name catalogName seems incorrect because this is the catalog implementation class in Spark. A better variable name would be catalogImpl.", "author": "rdblue", "createdAt": "2020-12-28T22:37:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0NzE2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUxMzI4Nw==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r549513287", "bodyText": "This is minor, so I'm merging without it. We can clean it up next time we touch this code.", "author": "rdblue", "createdAt": "2020-12-28T23:06:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0NzE2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0Nzk2Nw==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547547967", "bodyText": "Can you also add a test for a catalog.db.table name?", "author": "rdblue", "createdAt": "2020-12-22T23:02:04Z", "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestCustomCatalog.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.File;\n+import java.util.List;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.CatalogProperties;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalog.Catalog;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public class TestCustomCatalog {\n+  private static final String CATALOG_IMPL = String.format(\"%s.%s.%s\", CustomCatalogs.ICEBERG_CATALOG_PREFIX,\n+      CustomCatalogs.ICEBERG_DEFAULT_CATALOG, CatalogProperties.CATALOG_IMPL);\n+  private static final String WAREHOUSE = String.format(\"%s.%s.%s\", CustomCatalogs.ICEBERG_CATALOG_PREFIX,\n+      CustomCatalogs.ICEBERG_DEFAULT_CATALOG, CatalogProperties.WAREHOUSE_LOCATION);\n+  private static final String URI_KEY = String.format(\"%s.%s.%s\", CustomCatalogs.ICEBERG_CATALOG_PREFIX,\n+      CustomCatalogs.ICEBERG_DEFAULT_CATALOG, CatalogProperties.HIVE_URI);\n+  private static final String URI_VAL = \"thrift://localhost:12345\"; // dummy uri\n+  private static final String CATALOG_VAL = \"org.apache.iceberg.spark.source.TestCatalog\";\n+  private static final TableIdentifier TABLE = TableIdentifier.of(\"default\", \"table\");\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"id\", Types.IntegerType.get()),\n+      optional(2, \"data\", Types.StringType.get())\n+  );\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  File tableDir = null;\n+  String tableLocation = null;\n+  HadoopTables tables;\n+\n+  protected static SparkSession spark = null;\n+\n+  @BeforeClass\n+  public static void startMetastoreAndSpark() {\n+    spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n+  }\n+\n+  @AfterClass\n+  public static void stopMetastoreAndSpark() {\n+    spark.stop();\n+    spark = null;\n+  }\n+\n+  @Before\n+  public void setupTable() throws Exception {\n+    SparkConf sparkConf = spark.sparkContext().conf();\n+    sparkConf.set(\n+        String.format(\"%s.%s\", CustomCatalogs.ICEBERG_CATALOG_PREFIX, CustomCatalogs.ICEBERG_DEFAULT_CATALOG),\n+        Catalog.class.getName());\n+    this.tables = new HadoopTables(spark.sessionState().newHadoopConf());\n+    this.tableDir = temp.newFolder();\n+    tableDir.delete(); // created by table create\n+    this.tableLocation = tableDir.toURI().toString();\n+    tables.create(SCHEMA, PartitionSpec.unpartitioned(), String.format(\"%s/%s\", tableLocation, TABLE.name()));\n+  }\n+\n+  @After\n+  public void removeTable() {\n+    SparkConf sparkConf = spark.sparkContext().conf();\n+    sparkConf.remove(CATALOG_IMPL);\n+    sparkConf.remove(WAREHOUSE);\n+    sparkConf.remove(URI_KEY);\n+    tables.dropTable(String.format(\"%s/%s\", tableLocation, TABLE.name()));\n+    tableDir.delete();\n+    CustomCatalogs.clearCache();\n+  }\n+\n+  @Test\n+  public void withSparkOptions() {", "originalCommit": "a92b06cf32a5583cea9e8228d2a1a56fe35d6352", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Nzk5NjM3MA==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r547996370", "bodyText": "sorry, could you explain a bit more?", "author": "rymurr", "createdAt": "2020-12-23T14:53:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0Nzk2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTQyOTkxNw==", "url": "https://github.com/apache/iceberg/pull/1875#discussion_r549429917", "bodyText": "misunderstood your comment. Added a test for that.", "author": "rymurr", "createdAt": "2020-12-28T17:46:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU0Nzk2Nw=="}], "type": "inlineReview"}, {"oid": "7a557b064050b844d9aeab033ace3a72c2dd9904", "url": "https://github.com/apache/iceberg/commit/7a557b064050b844d9aeab033ace3a72c2dd9904", "message": "Allow Spark2 DataFrame to use a custom catalog", "committedDate": "2020-12-28T17:48:33Z", "type": "commit"}, {"oid": "8ae16f205223966256c79db2f0efa05dd1a3ccf7", "url": "https://github.com/apache/iceberg/commit/8ae16f205223966256c79db2f0efa05dd1a3ccf7", "message": "skip cache for Hive", "committedDate": "2020-12-28T17:48:34Z", "type": "commit"}, {"oid": "f907ccd1d5e2683ef35ae42c68459571981deb3d", "url": "https://github.com/apache/iceberg/commit/f907ccd1d5e2683ef35ae42c68459571981deb3d", "message": "update cache behaviour and javadoc", "committedDate": "2020-12-28T17:48:35Z", "type": "commit"}, {"oid": "915dda23583a4e07767acee2d5b51e1a8eb9a636", "url": "https://github.com/apache/iceberg/commit/915dda23583a4e07767acee2d5b51e1a8eb9a636", "message": "address code review comments and more code reuse", "committedDate": "2020-12-28T17:48:36Z", "type": "commit"}, {"oid": "8ef997be848a107c0a3cb1b45d9ed0ac8d309294", "url": "https://github.com/apache/iceberg/commit/8ef997be848a107c0a3cb1b45d9ed0ac8d309294", "message": "clarify catalog util and fix tests", "committedDate": "2020-12-28T17:48:36Z", "type": "commit"}, {"oid": "9d1548a58f6a8395b5ca9b0558dae01c9ca9a446", "url": "https://github.com/apache/iceberg/commit/9d1548a58f6a8395b5ca9b0558dae01c9ca9a446", "message": "style fixes", "committedDate": "2020-12-28T17:48:37Z", "type": "commit"}, {"oid": "50a2b7dfa9d8992dceed67f0572130f7b42cece6", "url": "https://github.com/apache/iceberg/commit/50a2b7dfa9d8992dceed67f0572130f7b42cece6", "message": "refactor and clean-up", "committedDate": "2020-12-28T17:48:38Z", "type": "commit"}, {"oid": "83586650320af4a5551681362ad09bdbe4f32e41", "url": "https://github.com/apache/iceberg/commit/83586650320af4a5551681362ad09bdbe4f32e41", "message": "fix checkstyle", "committedDate": "2020-12-28T17:48:39Z", "type": "commit"}, {"oid": "49b591097f49fcb9854decba8d84086e34b64310", "url": "https://github.com/apache/iceberg/commit/49b591097f49fcb9854decba8d84086e34b64310", "message": "address code review", "committedDate": "2020-12-28T17:48:40Z", "type": "commit"}, {"oid": "56e115ba8e721297d717d90ffb690c244e49fdcf", "url": "https://github.com/apache/iceberg/commit/56e115ba8e721297d717d90ffb690c244e49fdcf", "message": "clean up and address code review comments", "committedDate": "2020-12-28T17:48:41Z", "type": "commit"}, {"oid": "d97f48c6d14046f00aac02f3e8cff7fd50dc027d", "url": "https://github.com/apache/iceberg/commit/d97f48c6d14046f00aac02f3e8cff7fd50dc027d", "message": "add test for different catalog and catalog in the identifier", "committedDate": "2020-12-28T17:48:42Z", "type": "commit"}, {"oid": "d97f48c6d14046f00aac02f3e8cff7fd50dc027d", "url": "https://github.com/apache/iceberg/commit/d97f48c6d14046f00aac02f3e8cff7fd50dc027d", "message": "add test for different catalog and catalog in the identifier", "committedDate": "2020-12-28T17:48:42Z", "type": "forcePushed"}]}