{"pr_number": 1776, "pr_title": "Spark: Refactor Spark writes into separate classes (V2)", "pr_createdAt": "2020-11-16T22:39:21Z", "pr_url": "https://github.com/apache/iceberg/pull/1776", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDY5NjQyNA==", "url": "https://github.com/apache/iceberg/pull/1776#discussion_r524696424", "bodyText": "SparkWrite contains only common things now.", "author": "aokolnychyi", "createdAt": "2020-11-16T22:43:20Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "diffHunk": "@@ -72,52 +75,67 @@\n import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n \n-class SparkBatchWrite implements BatchWrite {\n-  private static final Logger LOG = LoggerFactory.getLogger(SparkBatchWrite.class);\n+class SparkWrite {\n+  private static final Logger LOG = LoggerFactory.getLogger(SparkWrite.class);\n \n   private final Table table;\n+  private final String queryId;\n   private final FileFormat format;\n   private final Broadcast<FileIO> io;\n   private final Broadcast<EncryptionManager> encryptionManager;\n-  private final boolean overwriteDynamic;", "originalCommit": "2afa35f5c098f9588f22b7559d5d3f2dc7986eb6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDY5NzEwMg==", "url": "https://github.com/apache/iceberg/pull/1776#discussion_r524697102", "bodyText": "This logic moved to inner classes.", "author": "aokolnychyi", "createdAt": "2020-11-16T22:44:02Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "diffHunk": "@@ -170,45 +177,7 @@ protected void commitOperation(SnapshotUpdate<?> operation, int numFiles, String\n     LOG.info(\"Committed in {} ms\", duration);\n   }\n \n-  private void append(WriterCommitMessage[] messages) {", "originalCommit": "2afa35f5c098f9588f22b7559d5d3f2dc7986eb6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDY5NzY2Mg==", "url": "https://github.com/apache/iceberg/pull/1776#discussion_r524697662", "bodyText": "This logic moved to base batch and streaming write classes.", "author": "aokolnychyi", "createdAt": "2020-11-16T22:44:30Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "diffHunk": "@@ -236,9 +201,171 @@ protected Table table() {\n     return ImmutableList.of();\n   }\n \n-  @Override\n-  public String toString() {", "originalCommit": "2afa35f5c098f9588f22b7559d5d3f2dc7986eb6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDcwMTk3OA==", "url": "https://github.com/apache/iceberg/pull/1776#discussion_r524701978", "bodyText": "In the future, Spark may have a write abstraction so we will make this class extend Write.", "author": "aokolnychyi", "createdAt": "2020-11-16T22:48:47Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "diffHunk": "@@ -72,52 +75,67 @@\n import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n \n-class SparkBatchWrite implements BatchWrite {\n-  private static final Logger LOG = LoggerFactory.getLogger(SparkBatchWrite.class);\n+class SparkWrite {", "originalCommit": "2afa35f5c098f9588f22b7559d5d3f2dc7986eb6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc5MTk5Mg==", "url": "https://github.com/apache/iceberg/pull/1776#discussion_r524791992", "bodyText": "Since this searches through old snapshots, what about find instead of get here? That signals that the operation is doing more than just fetching a pre-computed value.", "author": "rdblue", "createdAt": "2020-11-17T00:12:18Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "diffHunk": "@@ -236,9 +201,171 @@ protected Table table() {\n     return ImmutableList.of();\n   }\n \n-  @Override\n-  public String toString() {\n-    return String.format(\"IcebergWrite(table=%s, format=%s)\", table, format);\n+  private abstract class BaseBatchWrite implements BatchWrite {\n+    @Override\n+    public DataWriterFactory createBatchWriterFactory(PhysicalWriteInfo info) {\n+      return createWriterFactory();\n+    }\n+\n+    @Override\n+    public void abort(WriterCommitMessage[] messages) {\n+      SparkWrite.this.abort(messages);\n+    }\n+\n+    @Override\n+    public String toString() {\n+      return String.format(\"IcebergBatchWrite(table=%s, format=%s)\", table, format);\n+    }\n+  }\n+\n+  private class BatchAppend extends BaseBatchWrite {\n+    @Override\n+    public void commit(WriterCommitMessage[] messages) {\n+      AppendFiles append = table.newAppend();\n+\n+      int numFiles = 0;\n+      for (DataFile file : files(messages)) {\n+        numFiles += 1;\n+        append.appendFile(file);\n+      }\n+\n+      commitOperation(append, String.format(\"append with %d new data files\", numFiles));\n+    }\n+  }\n+\n+  private class DynamicOverwrite extends BaseBatchWrite {\n+    @Override\n+    public void commit(WriterCommitMessage[] messages) {\n+      ReplacePartitions dynamicOverwrite = table.newReplacePartitions();\n+\n+      int numFiles = 0;\n+      for (DataFile file : files(messages)) {\n+        numFiles += 1;\n+        dynamicOverwrite.addFile(file);\n+      }\n+\n+      commitOperation(dynamicOverwrite, String.format(\"dynamic partition overwrite with %d new data files\", numFiles));\n+    }\n+  }\n+\n+  private class OverwriteByFilter extends BaseBatchWrite {\n+    private final Expression overwriteExpr;\n+\n+    private OverwriteByFilter(Expression overwriteExpr) {\n+      this.overwriteExpr = overwriteExpr;\n+    }\n+\n+    @Override\n+    public void commit(WriterCommitMessage[] messages) {\n+      OverwriteFiles overwriteFiles = table.newOverwrite();\n+      overwriteFiles.overwriteByRowFilter(overwriteExpr);\n+\n+      int numFiles = 0;\n+      for (DataFile file : files(messages)) {\n+        numFiles += 1;\n+        overwriteFiles.addFile(file);\n+      }\n+\n+      String commitMsg = String.format(\"overwrite by filter %s with %d new data files\", overwriteExpr, numFiles);\n+      commitOperation(overwriteFiles, commitMsg);\n+    }\n+  }\n+\n+  private abstract class BaseStreamingWrite implements StreamingWrite {\n+    private static final String QUERY_ID_PROPERTY = \"spark.sql.streaming.queryId\";\n+    private static final String EPOCH_ID_PROPERTY = \"spark.sql.streaming.epochId\";\n+\n+    protected abstract String mode();\n+\n+    @Override\n+    public StreamingDataWriterFactory createStreamingWriterFactory(PhysicalWriteInfo info) {\n+      return createWriterFactory();\n+    }\n+\n+    @Override\n+    public final void commit(long epochId, WriterCommitMessage[] messages) {\n+      LOG.info(\"Committing epoch {} for query {} in {} mode\", epochId, queryId, mode());\n+\n+      table.refresh();\n+      Long lastCommittedEpochId = getLastCommittedEpochId();\n+      if (lastCommittedEpochId != null && epochId <= lastCommittedEpochId) {\n+        LOG.info(\"Skipping epoch {} for query {} as it was already committed\", epochId, queryId);\n+        return;\n+      }\n+\n+      doCommit(epochId, messages);\n+    }\n+\n+    protected abstract void doCommit(long epochId, WriterCommitMessage[] messages);\n+\n+    protected <T> void commit(SnapshotUpdate<T> snapshotUpdate, long epochId, String description) {\n+      snapshotUpdate.set(QUERY_ID_PROPERTY, queryId);\n+      snapshotUpdate.set(EPOCH_ID_PROPERTY, Long.toString(epochId));\n+      commitOperation(snapshotUpdate, description);\n+    }\n+\n+    private Long getLastCommittedEpochId() {", "originalCommit": "2afa35f5c098f9588f22b7559d5d3f2dc7986eb6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDgxNzU0Mg==", "url": "https://github.com/apache/iceberg/pull/1776#discussion_r524817542", "bodyText": "I kept the old name but I agree find is better. Let me update that.", "author": "aokolnychyi", "createdAt": "2020-11-17T01:04:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc5MTk5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDk0ODUyMg==", "url": "https://github.com/apache/iceberg/pull/1776#discussion_r524948522", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-11-17T08:01:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc5MTk5Mg=="}], "type": "inlineReview"}, {"oid": "e55227374fcab7a3286d053d507849fb92e0b7e7", "url": "https://github.com/apache/iceberg/commit/e55227374fcab7a3286d053d507849fb92e0b7e7", "message": "Spark: Refactor Spark writes into separate classes", "committedDate": "2020-11-17T05:17:11Z", "type": "forcePushed"}, {"oid": "ebfad1be45f4df24a539fa9b4a743fe39e0e4318", "url": "https://github.com/apache/iceberg/commit/ebfad1be45f4df24a539fa9b4a743fe39e0e4318", "message": "Spark: Refactor Spark writes into separate classes", "committedDate": "2020-11-17T05:21:20Z", "type": "commit"}, {"oid": "ebfad1be45f4df24a539fa9b4a743fe39e0e4318", "url": "https://github.com/apache/iceberg/commit/ebfad1be45f4df24a539fa9b4a743fe39e0e4318", "message": "Spark: Refactor Spark writes into separate classes", "committedDate": "2020-11-17T05:21:20Z", "type": "forcePushed"}]}