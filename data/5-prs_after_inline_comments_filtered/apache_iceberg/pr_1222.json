{"pr_number": 1222, "pr_title": "Add Avro row position reader", "pr_createdAt": "2020-07-20T21:42:03Z", "pr_url": "https://github.com/apache/iceberg/pull/1222", "timeline": [{"oid": "18cbef987f64026345067f1816326de3ce567654", "url": "https://github.com/apache/iceberg/commit/18cbef987f64026345067f1816326de3ce567654", "message": "Add Avro row position reader.", "committedDate": "2020-07-20T21:27:15Z", "type": "commit"}, {"oid": "2b15b522e039429da9e7fed5cd67da3e9125a463", "url": "https://github.com/apache/iceberg/commit/2b15b522e039429da9e7fed5cd67da3e9125a463", "message": "Suppress new checkstyle problem in BuildAvroProjection.", "committedDate": "2020-07-21T00:41:46Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzc3MjE5Ng==", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r457772196", "bodyText": "Should remove this before committing.", "author": "rdblue", "createdAt": "2020-07-21T00:52:25Z", "path": "core/src/main/java/org/apache/iceberg/data/avro/DataReader.java", "diffHunk": "@@ -117,6 +126,7 @@ private ReadBuilder(Map<Integer, ?> idToConstant) {\n     @Override\n     public ValueReader<?> record(Types.StructType struct, Schema record,\n                                  List<String> names, List<ValueReader<?>> fields) {\n+", "originalCommit": "2b15b522e039429da9e7fed5cd67da3e9125a463", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODk2MTM0MQ==", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r458961341", "bodyText": "Removing the empty line? I'm OK", "author": "rdsr", "createdAt": "2020-07-22T17:28:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzc3MjE5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIzOTI5Mg==", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459239292", "bodyText": "I don't understand this comment line\n\nonly if the position reader is set\n\nreader is set where?\n\nand this is a top-level field\n\nI don't see where the top-level field restriction comes from", "author": "shardulm94", "createdAt": "2020-07-23T06:32:12Z", "path": "core/src/main/java/org/apache/iceberg/avro/ValueReaders.java", "diffHunk": "@@ -582,13 +585,37 @@ protected StructReader(List<ValueReader<?>> readers, Types.StructType struct, Ma\n         if (idToConstant.containsKey(field.fieldId())) {\n           positionList.add(pos);\n           constantList.add(idToConstant.get(field.fieldId()));\n+        } else if (field.fieldId() == MetadataColumns.ROW_POSITION.fieldId()) {\n+          // replace the _pos field reader with a position reader\n+          // only if the position reader is set and this is a top-level field", "originalCommit": "d107f778da2ea0997d9eda087982c20ed1e1f95c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTYyNzI0Mw==", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459627243", "bodyText": "This is out of date. I had to move the check because the position reader is set after this constructor. I'll update the comment.", "author": "rdblue", "createdAt": "2020-07-23T17:55:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIzOTI5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI0NzcxMA==", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459247710", "bodyText": "Not sure if this valid start state, but I think there can be an edge case here\nrow-count|compressed-size-in-bytes|block-bytes|sync|EOF\n                                                 ^\n                                                 |\n                                               start\n\nIn this case it will seek and read the sync and then continue to read next block row count even after EOF\nSo should probably be while (nextSyncPos + 16 < start)?", "author": "shardulm94", "createdAt": "2020-07-23T06:55:00Z", "path": "core/src/main/java/org/apache/iceberg/avro/AvroIO.java", "diffHunk": "@@ -131,4 +144,55 @@ public boolean markSupported() {\n       return stream.markSupported();\n     }\n   }\n+\n+  static long findStartingRowPos(Supplier<SeekableInputStream> open, long start) {\n+    try (SeekableInputStream in = open.get()) {\n+      // use a direct decoder that will not buffer so the position of the input stream is accurate\n+      BinaryDecoder decoder = DecoderFactory.get().directBinaryDecoder(in, null);\n+\n+      // an Avro file's layout looks like this:\n+      //   header|block|block|...\n+      // the header contains:\n+      //   magic|string-map|sync\n+      // each block consists of:\n+      //   row-count|compressed-size-in-bytes|block-bytes|sync\n+\n+      // it is necessary to read the header here because this is the only way to get the expected file sync bytes\n+      byte[] magic = MAGIC_READER.read(decoder, null);\n+      if (!Arrays.equals(AVRO_MAGIC, magic)) {\n+        throw new InvalidAvroMagicException(\"Not an Avro file\");\n+      }\n+\n+      META_READER.read(decoder, null); // ignore the file metadata, it isn't needed\n+      byte[] fileSync = SYNC_READER.read(decoder, null);\n+\n+      // the while loop reads row counts and seeks past the block bytes until the next sync pos is >= start, which\n+      // indicates that the next sync is the start of the split.\n+      byte[] blockSync = new byte[16];\n+      long totalRows = 0;\n+      long nextSyncPos = in.getPos();\n+\n+      while (nextSyncPos < start) {", "originalCommit": "d107f778da2ea0997d9eda087982c20ed1e1f95c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTYzMjUyMQ==", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459632521", "bodyText": "The loop condition is correct with respect to start because a block starts at the beginning of a sync. You're right about the EOF behavior, though. I'll take a look at that.", "author": "rdblue", "createdAt": "2020-07-23T18:04:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI0NzcxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY2NzE1Mw==", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459667153", "bodyText": "Thanks for the insightful comment! (before it went away \ud83d\ude03 ) I was not familiar Avro's split reading behavior.", "author": "shardulm94", "createdAt": "2020-07-23T19:06:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI0NzcxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY4MTUzNQ==", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459681535", "bodyText": "Sorry, I should have edited, not deleted. I missed the part about it being just before the EOF.", "author": "rdblue", "createdAt": "2020-07-23T19:34:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI0NzcxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcxMjM3Ng==", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459712376", "bodyText": "The fix is to catch EOFException and return the number of rows.", "author": "rdblue", "createdAt": "2020-07-23T20:34:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI0NzcxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTYyMTY0MA==", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459621640", "bodyText": "nit: This is deprecated in favor of UncheckedIOException", "author": "rdsr", "createdAt": "2020-07-23T17:45:24Z", "path": "core/src/main/java/org/apache/iceberg/avro/AvroIO.java", "diffHunk": "@@ -131,4 +144,55 @@ public boolean markSupported() {\n       return stream.markSupported();\n     }\n   }\n+\n+  static long findStartingRowPos(Supplier<SeekableInputStream> open, long start) {\n+    try (SeekableInputStream in = open.get()) {\n+      // use a direct decoder that will not buffer so the position of the input stream is accurate\n+      BinaryDecoder decoder = DecoderFactory.get().directBinaryDecoder(in, null);\n+\n+      // an Avro file's layout looks like this:\n+      //   header|block|block|...\n+      // the header contains:\n+      //   magic|string-map|sync\n+      // each block consists of:\n+      //   row-count|compressed-size-in-bytes|block-bytes|sync\n+\n+      // it is necessary to read the header here because this is the only way to get the expected file sync bytes\n+      byte[] magic = MAGIC_READER.read(decoder, null);\n+      if (!Arrays.equals(AVRO_MAGIC, magic)) {\n+        throw new InvalidAvroMagicException(\"Not an Avro file\");\n+      }\n+\n+      META_READER.read(decoder, null); // ignore the file metadata, it isn't needed\n+      byte[] fileSync = SYNC_READER.read(decoder, null);\n+\n+      // the while loop reads row counts and seeks past the block bytes until the next sync pos is >= start, which\n+      // indicates that the next sync is the start of the split.\n+      byte[] blockSync = new byte[16];\n+      long totalRows = 0;\n+      long nextSyncPos = in.getPos();\n+\n+      while (nextSyncPos < start) {\n+        if (nextSyncPos != in.getPos()) {\n+          in.seek(nextSyncPos);\n+          SYNC_READER.read(decoder, blockSync);\n+\n+          if (!Arrays.equals(fileSync, blockSync)) {\n+            throw new RuntimeIOException(\"Invalid sync at %s\", nextSyncPos);", "originalCommit": "d107f778da2ea0997d9eda087982c20ed1e1f95c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTYyODE0Mg==", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459628142", "bodyText": "Yes, but readers still expect it from existing code paths. Since this code is called in paths that may still depend on RuntimeIOException, we should still throw it here. I think we should continue to throw it until the 0.11 release, when we can move all of the instances to UncheckedIOException.", "author": "rdblue", "createdAt": "2020-07-23T17:56:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTYyMTY0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1OTQ2Ng==", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459659466", "bodyText": "Makes sense", "author": "rdsr", "createdAt": "2020-07-23T18:52:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTYyMTY0MA=="}], "type": "inlineReview"}, {"oid": "6f1fc7139c4e03973550e1da371ec8da85056b8b", "url": "https://github.com/apache/iceberg/commit/6f1fc7139c4e03973550e1da371ec8da85056b8b", "message": "Handle EOF.", "committedDate": "2020-07-23T20:32:14Z", "type": "commit"}, {"oid": "1adf3cfdd27c91456089be7d0d20740c6af4444b", "url": "https://github.com/apache/iceberg/commit/1adf3cfdd27c91456089be7d0d20740c6af4444b", "message": "Update comment.", "committedDate": "2020-07-23T20:34:18Z", "type": "commit"}, {"oid": "df1cae9a872567e9efaa012aaa3d13234fda73b5", "url": "https://github.com/apache/iceberg/commit/df1cae9a872567e9efaa012aaa3d13234fda73b5", "message": "Remove unnecessary whitespace change in DataReader.", "committedDate": "2020-07-23T20:35:37Z", "type": "commit"}, {"oid": "df1cae9a872567e9efaa012aaa3d13234fda73b5", "url": "https://github.com/apache/iceberg/commit/df1cae9a872567e9efaa012aaa3d13234fda73b5", "message": "Remove unnecessary whitespace change in DataReader.", "committedDate": "2020-07-23T20:35:37Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcxMzQwOA==", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459713408", "bodyText": "@shardulm94, here's a test for the EOF case.", "author": "rdblue", "createdAt": "2020-07-23T20:36:53Z", "path": "core/src/test/java/org/apache/iceberg/avro/TestAvroFileSplit.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.avro;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.UUID;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.avro.DataWriter;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.types.Types.NestedField;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestAvroFileSplit {\n+  private static final Schema SCHEMA = new Schema(\n+      NestedField.required(1, \"id\", Types.LongType.get()),\n+      NestedField.required(2, \"data\", Types.StringType.get()));\n+\n+  private static final int NUM_RECORDS = 100_000;\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  public List<Record> expected = null;\n+  public InputFile file = null;\n+\n+  @Before\n+  public void writeDataFile() throws IOException {\n+    this.expected = Lists.newArrayList();\n+\n+    OutputFile out = Files.localOutput(temp.newFile());\n+\n+    try (FileAppender<Object> writer = Avro.write(out)\n+        .set(TableProperties.AVRO_COMPRESSION, \"uncompressed\")\n+        .createWriterFunc(DataWriter::create)\n+        .schema(SCHEMA)\n+        .overwrite()\n+        .build()) {\n+\n+      Record record = GenericRecord.create(SCHEMA);\n+      for (long i = 0; i < NUM_RECORDS; i += 1) {\n+        Record next = record.copy(ImmutableMap.of(\n+            \"id\", i,\n+            \"data\", UUID.randomUUID().toString()));\n+        expected.add(next);\n+        writer.add(next);\n+      }\n+    }\n+\n+    this.file = out.toInputFile();\n+  }\n+\n+  @Test\n+  public void testSplitDataSkipping() throws IOException {\n+    long end = file.getLength();\n+    long splitLocation = end / 2;\n+\n+    List<Record> firstHalf = readAvro(file, SCHEMA, 0, splitLocation);\n+    Assert.assertNotEquals(\"First split should not be empty\", 0, firstHalf.size());\n+\n+    List<Record> secondHalf = readAvro(file, SCHEMA, splitLocation + 1, end - splitLocation - 1);\n+    Assert.assertNotEquals(\"Second split should not be empty\", 0, secondHalf.size());\n+\n+    Assert.assertEquals(\"Total records should match expected\",\n+        expected.size(), firstHalf.size() + secondHalf.size());\n+\n+    for (int i = 0; i < firstHalf.size(); i += 1) {\n+      Assert.assertEquals(expected.get(i), firstHalf.get(i));\n+    }\n+\n+    for (int i = 0; i < secondHalf.size(); i += 1) {\n+      Assert.assertEquals(expected.get(firstHalf.size() + i), secondHalf.get(i));\n+    }\n+  }\n+\n+  @Test\n+  public void testPosField() throws IOException {\n+    Schema projection = new Schema(\n+        SCHEMA.columns().get(0),\n+        MetadataColumns.ROW_POSITION,\n+        SCHEMA.columns().get(1));\n+\n+    List<Record> records = readAvro(file, projection, 0, file.getLength());\n+\n+    for (int i = 0; i < expected.size(); i += 1) {\n+      Assert.assertEquals(\"Field _pos should match\",\n+          (long) i, records.get(i).getField(MetadataColumns.ROW_POSITION.name()));\n+      Assert.assertEquals(\"Field id should match\",\n+          expected.get(i).getField(\"id\"), records.get(i).getField(\"id\"));\n+      Assert.assertEquals(\"Field data should match\",\n+          expected.get(i).getField(\"data\"), records.get(i).getField(\"data\"));\n+    }\n+  }\n+\n+  @Test\n+  public void testPosFieldWithSplits() throws IOException {\n+    Schema projection = new Schema(\n+        SCHEMA.columns().get(0),\n+        MetadataColumns.ROW_POSITION,\n+        SCHEMA.columns().get(1));\n+\n+    long end = file.getLength();\n+    long splitLocation = end / 2;\n+\n+    List<Record> secondHalf = readAvro(file, projection, splitLocation + 1, end - splitLocation - 1);\n+    Assert.assertNotEquals(\"Second split should not be empty\", 0, secondHalf.size());\n+\n+    List<Record> firstHalf = readAvro(file, projection, 0, splitLocation);\n+    Assert.assertNotEquals(\"First split should not be empty\", 0, firstHalf.size());\n+\n+    Assert.assertEquals(\"Total records should match expected\",\n+        expected.size(), firstHalf.size() + secondHalf.size());\n+\n+    for (int i = 0; i < firstHalf.size(); i += 1) {\n+      Assert.assertEquals(\"Field _pos should match\",\n+          (long) i, firstHalf.get(i).getField(MetadataColumns.ROW_POSITION.name()));\n+      Assert.assertEquals(\"Field id should match\",\n+          expected.get(i).getField(\"id\"), firstHalf.get(i).getField(\"id\"));\n+      Assert.assertEquals(\"Field data should match\",\n+          expected.get(i).getField(\"data\"), firstHalf.get(i).getField(\"data\"));\n+    }\n+\n+    for (int i = 0; i < secondHalf.size(); i += 1) {\n+      Assert.assertEquals(\"Field _pos should match\",\n+          (long) (firstHalf.size() + i), secondHalf.get(i).getField(MetadataColumns.ROW_POSITION.name()));\n+      Assert.assertEquals(\"Field id should match\",\n+          expected.get(firstHalf.size() + i).getField(\"id\"), secondHalf.get(i).getField(\"id\"));\n+      Assert.assertEquals(\"Field data should match\",\n+          expected.get(firstHalf.size() + i).getField(\"data\"), secondHalf.get(i).getField(\"data\"));\n+    }\n+  }\n+\n+  @Test\n+  public void testPosWithEOFSplit() throws IOException {", "originalCommit": "df1cae9a872567e9efaa012aaa3d13234fda73b5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}