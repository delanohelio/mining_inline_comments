{"pr_number": 857, "pr_title": "[ISSUE #855] Add the Test Util And ORC Writer for Spark", "pr_createdAt": "2020-03-20T18:53:43Z", "pr_url": "https://github.com/apache/iceberg/pull/857", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU2MDA0Mg==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397560042", "bodyText": "Why comment out this?", "author": "chenjunjiedada", "createdAt": "2020-03-25T01:33:24Z", "path": "orc/src/main/java/org/apache/iceberg/orc/OrcFileAppender.java", "diffHunk": "@@ -96,8 +96,8 @@ public Metrics metrics() {\n \n   @Override\n   public long length() {\n-    Preconditions.checkState(isClosed,\n-        \"Cannot return length while appending to an open file.\");\n+//    Preconditions.checkState(isClosed,", "originalCommit": "1765d3c2249f9e44dec8f9cd518184701a842b39", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU2MDM3OQ==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397560379", "bodyText": "Please add apache license", "author": "chenjunjiedada", "createdAt": "2020-03-25T01:34:24Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/NestedRecord.java", "diffHunk": "@@ -0,0 +1,77 @@\n+package org.apache.iceberg.spark.source;", "originalCommit": "1765d3c2249f9e44dec8f9cd518184701a842b39", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU2MDk3Nw==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397560977", "bodyText": "Please remove these unrelated changes.", "author": "chenjunjiedada", "createdAt": "2020-03-25T01:36:33Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/SimpleRecord.java", "diffHunk": "@@ -20,6 +20,9 @@\n package org.apache.iceberg.spark.source;\n \n import com.google.common.base.Objects;\n+import com.oracle.tools.packager.mac.MacAppBundler;", "originalCommit": "1765d3c2249f9e44dec8f9cd518184701a842b39", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU2MTgzNA==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397561834", "bodyText": "The Iceberg code style doesn't allow using the wildcard import. You might want to change your IDE setting to disable the import optimization.", "author": "chenjunjiedada", "createdAt": "2020-03-25T01:39:47Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestORCWrite.java", "diffHunk": "@@ -0,0 +1,99 @@\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.*;", "originalCommit": "1765d3c2249f9e44dec8f9cd518184701a842b39", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU2MTg4MQ==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397561881", "bodyText": "ditto.", "author": "chenjunjiedada", "createdAt": "2020-03-25T01:39:56Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestORCWrite.java", "diffHunk": "@@ -0,0 +1,99 @@\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.*;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.*;", "originalCommit": "1765d3c2249f9e44dec8f9cd518184701a842b39", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU2MjE1MQ==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397562151", "bodyText": "Please sort the import alphabetically.", "author": "chenjunjiedada", "createdAt": "2020-03-25T01:41:11Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestORCWrite.java", "diffHunk": "@@ -0,0 +1,99 @@\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.*;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.*;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.File;", "originalCommit": "1765d3c2249f9e44dec8f9cd518184701a842b39", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU2MjQ5Nw==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397562497", "bodyText": "unnecessary blank line.", "author": "chenjunjiedada", "createdAt": "2020-03-25T01:42:38Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestORCWrite.java", "diffHunk": "@@ -0,0 +1,99 @@\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.*;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.*;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public class TestORCWrite {\n+  private static final Configuration CONF = new Configuration();\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"id\", Types.IntegerType.get()),\n+      optional(2, \"data\", Types.StringType.get()),\n+      optional(3, \"info\", Types.MapType.ofOptional(4,5,Types.StringType.get(),Types.StringType.get()))\n+  );\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private static SparkSession spark = null;\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestORCWrite.spark = SparkSession.builder().master(\"local[1]\").getOrCreate();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestORCWrite.spark;\n+    TestORCWrite.spark = null;\n+    currentSpark.stop();\n+  }\n+\n+  @Test\n+  public void testBasicWrite() throws IOException {\n+    File parent = temp.newFolder(\"orc\");\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n+    Table table = tables.create(SCHEMA, spec, location.toString());\n+    Map<String, String> info = ImmutableMap.of(\"a\", \"A\", \"b\", \"B\");\n+\n+    List<NestedRecord> expected = Lists.newArrayList(\n+        new NestedRecord(1, \"a\", info),\n+        new NestedRecord(2, \"a\", info),\n+        new NestedRecord(3, \"b\", info),\n+        new NestedRecord(4, \"b\", info),\n+        new NestedRecord(5, \"c\", info),\n+        new NestedRecord(6, \"c\", info),\n+        new NestedRecord(7, \"d\", info),\n+        new NestedRecord(8, \"d\", info),\n+        new NestedRecord(9, \"e\", info),\n+        new NestedRecord(10, \"e\", info),\n+        new NestedRecord(11, \"f\", info),\n+        new NestedRecord(12, \"f\", info)\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, NestedRecord.class);\n+\n+    // TODO: incoming columns must be ordered according to the table's schema\n+    df.select(\"id\", \"data\", \"info\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", \"orc\")\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    table.refresh();\n+\n+    Dataset<Row> result = spark.read()\n+        .format(\"iceberg\")\n+        .load(location.toString());\n+", "originalCommit": "1765d3c2249f9e44dec8f9cd518184701a842b39", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU2MjY4NA==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397562684", "bodyText": "Please add a blank line before this.", "author": "chenjunjiedada", "createdAt": "2020-03-25T01:43:24Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Writer.java", "diffHunk": "@@ -308,6 +310,13 @@ public String toString() {\n                   .schema(dsSchema)\n                   .overwrite()\n                   .build();\n+            case ORC:", "originalCommit": "1765d3c2249f9e44dec8f9cd518184701a842b39", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU5NjE2Nw==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397596167", "bodyText": "remove commented code", "author": "rdsr", "createdAt": "2020-03-25T03:55:06Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/NestedRecord.java", "diffHunk": "@@ -0,0 +1,77 @@\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Objects;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class NestedRecord {\n+  private Integer id;\n+  private String data;\n+  private Map info;// = new HashMap<String, String>();", "originalCommit": "1765d3c2249f9e44dec8f9cd518184701a842b39", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU5Njc5Mg==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397596792", "bodyText": "this shouldn't be here?", "author": "rdsr", "createdAt": "2020-03-25T03:57:51Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/NestedRecord.java", "diffHunk": "@@ -0,0 +1,77 @@\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Objects;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class NestedRecord {\n+  private Integer id;\n+  private String data;\n+  private Map info;// = new HashMap<String, String>();\n+\n+  public NestedRecord() {\n+  }\n+\n+  NestedRecord(Integer id, String data, Map<String, String> info) {\n+    this.id = id;\n+    this.data = data;\n+    this.info = info;\n+  }\n+\n+  public Integer getId() {\n+    return id;\n+  }\n+\n+  public void setId(Integer id) {\n+    this.id = id;\n+  }\n+\n+  public String getData() {\n+    return data;\n+  }\n+\n+  public void setData(String data) {\n+    this.data = data;\n+  }\n+\n+  public Map<String, String> getInfo() {\n+    return info;\n+  }\n+\n+  public void setInfo(Map<String, String> info) {\n+    this.info = info;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+\n+    NestedRecord record = (NestedRecord) o;\n+    return Objects.equal(id, record.id) && Objects.equal(data, record.data) && Objects.equal(info, record.info);\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    return Objects.hashCode(id, data, info);\n+  }\n+\n+  @Override\n+  public String toString() {\n+    StringBuilder buffer = new StringBuilder();\n+    buffer.append(\"{\\\"id\\\"=\");\n+    buffer.append(id);\n+    buffer.append(\",\\\"data\\\"=\\\"\");\n+    buffer.append(data);\n+    buffer.append(\"\\\"}\");", "originalCommit": "1765d3c2249f9e44dec8f9cd518184701a842b39", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU5NzI4MQ==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397597281", "bodyText": "Q. Why did we create a new class, why not SimpleRecord?", "author": "rdsr", "createdAt": "2020-03-25T03:59:57Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/NestedRecord.java", "diffHunk": "@@ -0,0 +1,77 @@\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Objects;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class NestedRecord {\n+  private Integer id;\n+  private String data;\n+  private Map info;// = new HashMap<String, String>();\n+\n+  public NestedRecord() {", "originalCommit": "1765d3c2249f9e44dec8f9cd518184701a842b39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzYxNjU3OQ==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397616579", "bodyText": "I think, ORC file not support Nested type, so I want to confirmed that could work correctly", "author": "XiaokunDing", "createdAt": "2020-03-25T05:27:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU5NzI4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU5ODI4Mw==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397598283", "bodyText": "@shawnding  what are your thoughts on making the class org.apache.iceberg.spark.source.TestParquetWrite parameterized and making it work for both Parquet and ORC. Seems like that's less work and it also tests with a lot more test cases. Thoughts?\nIf you want to see some examples of parameterized tests, please see org.apache.iceberg.data.TestLocalScan", "author": "rdsr", "createdAt": "2020-03-25T04:04:53Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestORCWrite.java", "diffHunk": "@@ -0,0 +1,99 @@\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;", "originalCommit": "1765d3c2249f9e44dec8f9cd518184701a842b39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzYxNzM3Nw==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397617377", "bodyText": "Ok thanks Rdsr, that I will rewrite it.", "author": "XiaokunDing", "createdAt": "2020-03-25T05:31:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU5ODI4Mw=="}], "type": "inlineReview"}, {"oid": "223ce0489276024c8dd320507bf30f703174013b", "url": "https://github.com/apache/iceberg/commit/223ce0489276024c8dd320507bf30f703174013b", "message": "[ISSUE #855] Add the Test Util And ORC file writer", "committedDate": "2020-03-26T10:14:32Z", "type": "forcePushed"}, {"oid": "8e0aaea9ddb3d457bd4b77b0d8bb6a24539cdcba", "url": "https://github.com/apache/iceberg/commit/8e0aaea9ddb3d457bd4b77b0d8bb6a24539cdcba", "message": "[ISSUE #855] Add the Test Util And ORC file writer", "committedDate": "2020-03-26T10:20:20Z", "type": "forcePushed"}, {"oid": "8197ea3baf248637b47816882f2fcd85d2e3d097", "url": "https://github.com/apache/iceberg/commit/8197ea3baf248637b47816882f2fcd85d2e3d097", "message": "[ISSUE #855] Add the Test Util And ORC file writer", "committedDate": "2020-03-26T10:28:15Z", "type": "forcePushed"}, {"oid": "fc5e9a6640fff195f71dbb9b2fb4f1aa1ee86abe", "url": "https://github.com/apache/iceberg/commit/fc5e9a6640fff195f71dbb9b2fb4f1aa1ee86abe", "message": "[ISSUE #855] Add the Test Util And ORC file writer", "committedDate": "2020-03-26T11:58:58Z", "type": "commit"}, {"oid": "fc5e9a6640fff195f71dbb9b2fb4f1aa1ee86abe", "url": "https://github.com/apache/iceberg/commit/fc5e9a6640fff195f71dbb9b2fb4f1aa1ee86abe", "message": "[ISSUE #855] Add the Test Util And ORC file writer", "committedDate": "2020-03-26T11:58:58Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyNjU3Ng==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r399826576", "bodyText": "Why was this change required?", "author": "rdsr", "createdAt": "2020-03-29T17:17:51Z", "path": "orc/src/main/java/org/apache/iceberg/orc/ORCSchemaUtil.java", "diffHunk": "@@ -269,7 +269,7 @@ private static TypeDescription buildOrcProjection(Integer fieldId, Type type, bo\n         break;\n       case MAP:\n         Types.MapType map = (Types.MapType) type;\n-        TypeDescription keyType = buildOrcProjection(map.keyId(), map.keyType(), true, mapping);", "originalCommit": "fc5e9a6640fff195f71dbb9b2fb4f1aa1ee86abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg5MTQ2Mg==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r399891462", "bodyText": "While a MapType  column is optional if insert an  empty map like null,  the key and value  both are null", "author": "XiaokunDing", "createdAt": "2020-03-30T01:58:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyNjU3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTI0MTI3MA==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r401241270", "bodyText": "Map keys are always required. Does this make the map key optional?", "author": "rdblue", "createdAt": "2020-03-31T22:00:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyNjU3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyNjg2Nw==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r399826867", "bodyText": "What exception you see if when running with ORC? I would have thought that it should work fine.\nAlso I think it is  disabled for Avro also now.", "author": "rdsr", "createdAt": "2020-03-29T17:20:12Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Writer.java", "diffHunk": "@@ -389,7 +399,9 @@ public EncryptedOutputFile newOutputFile(PartitionKey key) {\n     public abstract void write(InternalRow row) throws IOException;\n \n     public void writeInternal(InternalRow row)  throws IOException {\n-      if (currentRows % ROWS_DIVISOR == 0 && currentAppender.length() >= targetFileSize) {\n+      //TODO: ORC file now not support target file size", "originalCommit": "fc5e9a6640fff195f71dbb9b2fb4f1aa1ee86abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg5NDcyMw==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r399894723", "bodyText": "Yes in Avro file is work,  cannot get the file length before closed the  ORC file, I have fix it.", "author": "XiaokunDing", "createdAt": "2020-03-30T02:16:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyNjg2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyNjk1OQ==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r399826959", "bodyText": "nit: would it be easier to address this change as a separate PR. Seems un-related to current work", "author": "rdsr", "createdAt": "2020-03-29T17:21:00Z", "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestSparkOrcReader.java", "diffHunk": "@@ -49,7 +49,7 @@ protected void writeAndValidate(Schema schema) throws IOException {\n     }\n \n     try (CloseableIterable<InternalRow> reader = ORC.read(Files.localInput(testFile))\n-        .schema(schema)\n+        .project(schema)", "originalCommit": "fc5e9a6640fff195f71dbb9b2fb4f1aa1ee86abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg5NTU1OQ==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r399895559", "bodyText": "OK I split it to another PR", "author": "XiaokunDing", "createdAt": "2020-03-30T02:20:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyNjk1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyNzE2MA==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r399827160", "bodyText": "Does this not work for Avro as well?", "author": "rdsr", "createdAt": "2020-03-29T17:22:40Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java", "diffHunk": "@@ -0,0 +1,457 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestReader;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+@RunWith(Parameterized.class)\n+public class TestSparkDataWrite {\n+  private static final Configuration CONF = new Configuration();\n+  private final FileFormat format;\n+  private static SparkSession spark = null;\n+  private static final Map<String, String> info = ImmutableMap.of(\"a\", \"A\", \"b\", \"B\");\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"id\", Types.IntegerType.get()),\n+      optional(2, \"data\", Types.StringType.get()),\n+      optional(3, \"info\", Types.MapType.ofOptional(\n+          4, 5, Types.StringType.get(), Types.StringType.get()))\n+  );\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @Parameterized.Parameters\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] { \"parquet\" },\n+        new Object[] { \"avro\" },\n+        new Object[] { \"orc\" }\n+    };\n+  }\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestSparkDataWrite.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestSparkDataWrite.spark;\n+    TestSparkDataWrite.spark = null;\n+    currentSpark.stop();\n+  }\n+\n+  public TestSparkDataWrite(String format) {\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+  }\n+\n+  @Test\n+  public void testBasicWrite() throws IOException {\n+    File parent = temp.newFolder(format.toString());\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n+    Table table = tables.create(SCHEMA, spec, location.toString());\n+\n+    List<NestedRecord> expected = Lists.newArrayList(\n+        new NestedRecord(1, \"a\", info),\n+        new NestedRecord(2, \"b\", info),\n+        new NestedRecord(3, \"c\", info)\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, NestedRecord.class);\n+    // TODO: incoming columns must be ordered according to the table's schema\n+    df.select(\"id\", \"data\", \"info\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    table.refresh();\n+\n+    Dataset<Row> result = spark.read()\n+        .format(\"iceberg\")\n+        .load(location.toString());\n+\n+    List<NestedRecord> actual = result.orderBy(\"id\").as(Encoders.bean(NestedRecord.class)).collectAsList();\n+    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n+    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+    for (ManifestFile manifest : table.currentSnapshot().manifests()) {\n+      for (DataFile file : ManifestReader.read(manifest, table.io())) {\n+        // TODO: avro not support split\n+        if (!format.equals(FileFormat.AVRO)) {\n+          Assert.assertNotNull(\"Split offsets not present\", file.splitOffsets());\n+        }\n+        Assert.assertEquals(\"Should have reported record count as 1\", 1, file.recordCount());\n+        //TODO: append more metric info\n+        if (format.equals(FileFormat.PARQUET)) {\n+          Assert.assertNotNull(\"Column sizes metric not present\", file.columnSizes());\n+          Assert.assertNotNull(\"Counts metric not present\", file.valueCounts());\n+          Assert.assertNotNull(\"Null value counts metric not present\", file.nullValueCounts());\n+          Assert.assertNotNull(\"Lower bounds metric not present\", file.lowerBounds());\n+          Assert.assertNotNull(\"Upper bounds metric not present\", file.upperBounds());\n+        }\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void testAppend() throws IOException {\n+    File parent = temp.newFolder(format.toString());\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n+    Table table = tables.create(SCHEMA, spec, location.toString());\n+\n+    List<NestedRecord> records = Lists.newArrayList(\n+        new NestedRecord(1, \"a\", info),\n+        new NestedRecord(2, \"b\", info),\n+        new NestedRecord(3, \"c\", info)\n+    );\n+\n+    List<NestedRecord> expected = Lists.newArrayList(\n+        new NestedRecord(1, \"a\", info),\n+        new NestedRecord(2, \"b\", info),\n+        new NestedRecord(3, \"c\", info),\n+        new NestedRecord(4, \"a\", info),\n+        new NestedRecord(5, \"b\", info),\n+        new NestedRecord(6, \"c\", info)\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(records, NestedRecord.class);\n+\n+    df.select(\"id\", \"data\", \"info\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    df.withColumn(\"id\", df.col(\"id\").plus(3)).select(\"id\", \"data\", \"info\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    table.refresh();\n+\n+    Dataset<Row> result = spark.read()\n+        .format(\"iceberg\")\n+        .load(location.toString());\n+\n+    List<NestedRecord> actual = result.orderBy(\"id\").as(Encoders.bean(NestedRecord.class)).collectAsList();\n+    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n+    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+  }\n+\n+  @Test\n+  public void testOverwrite() throws IOException {\n+    File parent = temp.newFolder(format.toString());\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"id\").build();\n+    Table table = tables.create(SCHEMA, spec, location.toString());\n+\n+    List<NestedRecord> records = Lists.newArrayList(\n+        new NestedRecord(1, \"a\", info),\n+        new NestedRecord(2, \"b\", info),\n+        new NestedRecord(3, \"c\", info)\n+    );\n+\n+    List<NestedRecord> expected = Lists.newArrayList(\n+        new NestedRecord(1, \"a\", info),\n+        new NestedRecord(2, \"a\", info),\n+        new NestedRecord(3, \"c\", info),\n+        new NestedRecord(4, \"b\", info),\n+        new NestedRecord(6, \"c\", info)\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(records, NestedRecord.class);\n+\n+    df.select(\"id\", \"data\", \"info\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    // overwrite with 2*id to replace record 2, append 4 and 6\n+    df.withColumn(\"id\", df.col(\"id\").multiply(2)).select(\"id\", \"data\", \"info\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"overwrite\")\n+        .save(location.toString());\n+\n+    table.refresh();\n+\n+    Dataset<Row> result = spark.read()\n+        .format(\"iceberg\")\n+        .load(location.toString());\n+\n+    List<NestedRecord> actual = result.orderBy(\"id\").as(Encoders.bean(NestedRecord.class)).collectAsList();\n+    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n+    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+  }\n+\n+  @Test\n+  public void testUnpartitionedOverwrite() throws IOException {\n+    File parent = temp.newFolder(format.toString());\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    Table table = tables.create(SCHEMA, spec, location.toString());\n+\n+    List<NestedRecord> expected = Lists.newArrayList(\n+        new NestedRecord(1, \"a\", info),\n+        new NestedRecord(2, \"b\", info),\n+        new NestedRecord(3, \"c\", info)\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, NestedRecord.class);\n+\n+    df.select(\"id\", \"data\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    // overwrite with the same data; should not produce two copies\n+    df.select(\"id\", \"data\", \"info\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"overwrite\")\n+        .save(location.toString());\n+\n+    table.refresh();\n+\n+    Dataset<Row> result = spark.read()\n+        .format(\"iceberg\")\n+        .load(location.toString());\n+\n+    List<NestedRecord> actual = result.orderBy(\"id\").as(Encoders.bean(NestedRecord.class)).collectAsList();\n+    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n+    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+  }\n+\n+  @Test\n+  public void testUnpartitionedCreateWithTargetFileSizeViaTableProperties() throws IOException {\n+    File parent = temp.newFolder(format.toString());\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    Table table = tables.create(SCHEMA, spec, location.toString());\n+\n+    table.updateProperties()\n+        .set(TableProperties.WRITE_TARGET_FILE_SIZE_BYTES, \"4\") // ~4 bytes; low enough to trigger\n+        .commit();\n+\n+    List<NestedRecord> expected = Lists.newArrayListWithCapacity(4000);\n+    for (int i = 0; i < 4000; i++) {\n+      expected.add(new NestedRecord(i, \"a\", info));\n+    }\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, NestedRecord.class);\n+\n+    df.select(\"id\", \"data\", \"info\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    table.refresh();\n+\n+    Dataset<Row> result = spark.read()\n+        .format(\"iceberg\")\n+        .load(location.toString());\n+\n+    List<NestedRecord> actual = result.orderBy(\"id\").as(Encoders.bean(NestedRecord.class)).collectAsList();\n+    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n+    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+\n+    List<DataFile> files = Lists.newArrayList();\n+    for (ManifestFile manifest : table.currentSnapshot().manifests()) {\n+      for (DataFile file : ManifestReader.read(manifest, table.io())) {\n+        files.add(file);\n+      }\n+    }\n+    // TODO: ORC file now not support target file size\n+    if (format.equals(FileFormat.PARQUET)) {\n+      Assert.assertEquals(\"Should have 4 DataFiles\", 4, files.size());\n+      Assert.assertTrue(\"All DataFiles contain 1000 rows\", files.stream().allMatch(d -> d.recordCount() == 1000));\n+    }\n+  }\n+\n+  @Test\n+  public void testPartitionedCreateWithTargetFileSizeViaOption() throws IOException {\n+    File parent = temp.newFolder(format.toString());\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n+    Table table = tables.create(SCHEMA, spec, location.toString());\n+\n+    List<NestedRecord> expected = Lists.newArrayListWithCapacity(8000);\n+    for (int i = 0; i < 2000; i++) {\n+      expected.add(new NestedRecord(i, \"a\", info));\n+      expected.add(new NestedRecord(i, \"b\", info));\n+      expected.add(new NestedRecord(i, \"c\", info));\n+      expected.add(new NestedRecord(i, \"d\", info));\n+    }\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, NestedRecord.class);\n+\n+    df.select(\"id\", \"data\", \"info\").sort(\"data\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .option(\"target-file-size-bytes\", 4) // ~4 bytes; low enough to trigger\n+        .save(location.toString());\n+\n+    table.refresh();\n+\n+    Dataset<Row> result = spark.read()\n+        .format(\"iceberg\")\n+        .load(location.toString());\n+\n+    List<NestedRecord> actual = result.orderBy(\"id\").as(Encoders.bean(NestedRecord.class)).collectAsList();\n+    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n+    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+\n+    List<DataFile> files = Lists.newArrayList();\n+    for (ManifestFile manifest : table.currentSnapshot().manifests()) {\n+      for (DataFile file : ManifestReader.read(manifest, table.io())) {\n+        files.add(file);\n+      }\n+    }\n+    // TODO: ORC file now not support target file size\n+    if (format.equals(FileFormat.PARQUET)) {", "originalCommit": "fc5e9a6640fff195f71dbb9b2fb4f1aa1ee86abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg4ODg1MQ==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r399888851", "bodyText": "Yes now target-file-size-byte only  not work on Orc file. I fix it", "author": "XiaokunDing", "createdAt": "2020-03-30T01:43:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyNzE2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyNzQzNg==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r399827436", "bodyText": "Does this file replace org.apache.iceberg.spark.source.TestParquetWrite or do u think both are required?", "author": "rdsr", "createdAt": "2020-03-29T17:24:46Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java", "diffHunk": "@@ -0,0 +1,457 @@\n+/*", "originalCommit": "fc5e9a6640fff195f71dbb9b2fb4f1aa1ee86abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg4NzU0NQ==", "url": "https://github.com/apache/iceberg/pull/857#discussion_r399887545", "bodyText": "Yes , I Think this file replace org.apache.iceberg.spark.source.TestParquetWrite I deleted the  org.apache.iceberg.spark.source.TestParquetWrite", "author": "XiaokunDing", "createdAt": "2020-03-30T01:35:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyNzQzNg=="}], "type": "inlineReview"}, {"oid": "63bab5c8ade171af401e3c0f5124ac26797d8daa", "url": "https://github.com/apache/iceberg/commit/63bab5c8ade171af401e3c0f5124ac26797d8daa", "message": "[ISSUE #855] Add the Test Util And ORC file writer", "committedDate": "2020-03-30T03:20:21Z", "type": "commit"}, {"oid": "63bab5c8ade171af401e3c0f5124ac26797d8daa", "url": "https://github.com/apache/iceberg/commit/63bab5c8ade171af401e3c0f5124ac26797d8daa", "message": "[ISSUE #855] Add the Test Util And ORC file writer", "committedDate": "2020-03-30T03:20:21Z", "type": "forcePushed"}, {"oid": "5031d355b947dc89770b7a7040109debec46eef3", "url": "https://github.com/apache/iceberg/commit/5031d355b947dc89770b7a7040109debec46eef3", "message": "[ISSUE #855] Delete Nested test", "committedDate": "2020-04-02T04:02:50Z", "type": "commit"}]}