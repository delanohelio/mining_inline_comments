{"pr_number": 1404, "pr_title": "Flink: Add flink job id to state backend for handling flink job redeployment", "pr_createdAt": "2020-08-31T13:45:38Z", "pr_url": "https://github.com/apache/iceberg/pull/1404", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDE1MDYzMw==", "url": "https://github.com/apache/iceberg/pull/1404#discussion_r480150633", "bodyText": "Assume the case:   the flink job bootstrap first and write few records,  after the first snapshotState(1) finished, its job crashed, then it started to restore from the first snapshot.  Because we don't commit any iceberg transaction, so the maxCommittedCheckpointId will be -1,  finally the job won't recover successfully.\nSo in theory, this Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID) should be incorrect here.   For the snapshot expiration case,  we may need to find other ways to deal with.", "author": "openinx", "createdAt": "2020-08-31T14:00:27Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -108,18 +115,26 @@ public void initializeState(StateInitializationContext context) throws Exception\n     this.maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n \n     this.checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    this.jobIdState = context.getOperatorStateStore().getListState(JOB_ID_DESCRIPTOR);\n     if (context.isRestored()) {\n-      this.maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);\n-      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n-      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n-      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n-      // the iceberg table.\n-      Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID,", "originalCommit": "1cde142db5bdf9f24b1f577c77878756e9787041", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDIxOTM3Ng==", "url": "https://github.com/apache/iceberg/pull/1404#discussion_r480219376", "bodyText": "nit: move this inside the if section to make the if-else more symmetric.", "author": "stevenzwu", "createdAt": "2020-08-31T15:44:40Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -108,18 +115,26 @@ public void initializeState(StateInitializationContext context) throws Exception\n     this.maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n \n     this.checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    this.jobIdState = context.getOperatorStateStore().getListState(JOB_ID_DESCRIPTOR);\n     if (context.isRestored()) {\n-      this.maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);\n-      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n-      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n-      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n-      // the iceberg table.\n-      Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID,\n-          \"There should be an existing iceberg snapshot for current flink job: %s\", flinkJobId);\n-\n-      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n-      // Only keep the uncommitted data files in the cache.\n-      this.dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));\n+      String oldFlinkJobId = jobIdState.get().iterator().next();\n+      Preconditions.checkState(oldFlinkJobId != null && oldFlinkJobId.length() > 0,\n+          \"Flink job id parsed from checkpoint snapshot shouldn't be null or empty\");\n+      long oldMaxCommittedCheckpointId = getMaxCommittedCheckpointId(table, oldFlinkJobId);", "originalCommit": "1cde142db5bdf9f24b1f577c77878756e9787041", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY0NjEyNA==", "url": "https://github.com/apache/iceberg/pull/1404#discussion_r480646124", "bodyText": "The tailMap will use the oldMaxCommittedCheckpointId to filter all committed data files, so even if we move the line inside if-else,  it still need to access the oldMaxCommittedCheckpointId outside it.", "author": "openinx", "createdAt": "2020-09-01T02:48:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDIxOTM3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDIzMDI2NQ==", "url": "https://github.com/apache/iceberg/pull/1404#discussion_r480230265", "bodyText": "I am wondering if it is better to consolidate all committer checkpoint states into a single structure, e.g. Pojo class or Avro record which are Flink supported state types for schema evolution.", "author": "stevenzwu", "createdAt": "2020-08-31T16:02:53Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -88,6 +88,13 @@\n   private transient Table table;\n   private transient long maxCommittedCheckpointId;\n \n+  // There're two cases that we restore from flink checkpoints: the first case is restoring from snapshot created by the\n+  // same flink job; another case is restoring from snapshot created by another different job. For the second case, we\n+  // need to maintain the old flink job's id in flink state backend to find the max-committed-checkpoint-id when\n+  // traversing iceberg table's snapshots.\n+  private static final ListStateDescriptor<String> JOB_ID_DESCRIPTOR = new ListStateDescriptor<>(", "originalCommit": "1cde142db5bdf9f24b1f577c77878756e9787041", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY4NjYwMA==", "url": "https://github.com/apache/iceberg/pull/1404#discussion_r480686600", "bodyText": "We've considered to use avro or pojo to consolidate the job-id and data files into a single structure. It could be considered as two separate issue:\n\nuse avro or pojo to serialize/deserialize.  If use avro, then we need the detailed schema for the whole structure,  but the DataFile  hides its schema inside a non-public implementation GenericDataFile, that was designed intentionally because we don't want to expose the detail schema to upper users for iceberg-core.  POJO need all fields provide getter/setter, while the DataFile don't support setter now.\nMaking them into a single structure.  I think we could do but I'm not sure what's the benefit.", "author": "openinx", "createdAt": "2020-09-01T03:18:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDIzMDI2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTI1NDUzNg==", "url": "https://github.com/apache/iceberg/pull/1404#discussion_r481254536", "bodyText": "fair enough. agree that we shouldn't expose the GenericDataFile.\nOn the other hand, this can be simpler if we decided to just checkpoint the manifest file. Then we only need to track the manifest file path along with other metadata. Maybe we can follow up on the issue: #1403\nHere is our Avro schema file.\n{\n    \"type\": \"record\",\n    \"name\": \"ManifestFileState\",\n    \"namespace\": \"com.netflix.spaas.nfflink.connector.iceberg.model\",\n    \"fields\": [\n        {\"name\":\"path\", \"type\":\"string\"},\n        {\"name\":\"length\", \"type\":\"long\"},\n        {\"name\":\"specId\", \"type\":\"int\"},\n        { \"name\":\"checkpointId\", \"type\": \"long\"},\n        { \"name\":\"checkpointTimestamp\", \"type\": \"long\"},\n        { \"name\":\"dataFileCount\", \"type\": \"long\"},\n        { \"name\":\"recordCount\", \"type\": \"long\"},\n        { \"name\":\"byteCount\", \"type\": \"long\"},\n        { \"name\":\"lowWatermark\", \"type\": [\"null\", \"long\"], \"default\": null},\n        { \"name\":\"highWatermark\", \"type\": [\"null\", \"long\"], \"default\": null}\n    ]\n}", "author": "stevenzwu", "createdAt": "2020-09-01T16:00:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDIzMDI2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDI0ODM1Nw==", "url": "https://github.com/apache/iceberg/pull/1404#discussion_r480248357", "bodyText": "what is the reason making this a static method now?", "author": "stevenzwu", "createdAt": "2020-08-31T16:34:19Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -151,13 +169,16 @@ public void notifyCheckpointComplete(long checkpointId) throws Exception {\n     // For step#4, we don't need to commit iceberg table again because in step#3 we've committed all the files,\n     // Besides, we need to maintain the max-committed-checkpoint-id to be increasing.\n     if (checkpointId > maxCommittedCheckpointId) {\n-      commitUpToCheckpoint(checkpointId);\n+      commitUpToCheckpoint(table, dataFilesPerCheckpoint, flinkJobId, checkpointId);\n       this.maxCommittedCheckpointId = checkpointId;\n     }\n   }\n \n-  private void commitUpToCheckpoint(long checkpointId) {\n-    NavigableMap<Long, List<DataFile>> pendingFileMap = dataFilesPerCheckpoint.headMap(checkpointId, true);\n+  private static void commitUpToCheckpoint(Table table,", "originalCommit": "1cde142db5bdf9f24b1f577c77878756e9787041", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY1Njg1MQ==", "url": "https://github.com/apache/iceberg/pull/1404#discussion_r480656851", "bodyText": "Because I want the user to explicitly write the parameters on the method,  to avoid unexpected bugs caused by the usage of the internal variables of the class.  I encountered the bug before because  I used the current flinkJobId here unintentionally when commitUpToCheckpoint in the restoring path,", "author": "openinx", "createdAt": "2020-09-01T02:56:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDI0ODM1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA1ODIwMg==", "url": "https://github.com/apache/iceberg/pull/1404#discussion_r486058202", "bodyText": "nit: since we are using guava, we can use Strings.isNullOrEmpty.", "author": "stevenzwu", "createdAt": "2020-09-10T04:38:02Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -108,18 +115,25 @@ public void initializeState(StateInitializationContext context) throws Exception\n     this.maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n \n     this.checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    this.jobIdState = context.getOperatorStateStore().getListState(JOB_ID_DESCRIPTOR);\n     if (context.isRestored()) {\n       this.maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);\n-      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n-      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n-      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n-      // the iceberg table.\n-      Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID,\n-          \"There should be an existing iceberg snapshot for current flink job: %s\", flinkJobId);\n-\n-      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n-      // Only keep the uncommitted data files in the cache.\n-      this.dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));\n+\n+      String oldFlinkJobId = jobIdState.get().iterator().next();\n+      Preconditions.checkState(oldFlinkJobId != null && oldFlinkJobId.length() > 0,", "originalCommit": "4582f1585aa5ac82477bc11f9728ecd2b6e89a8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjEwNjMxNw==", "url": "https://github.com/apache/iceberg/pull/1404#discussion_r486106317", "bodyText": "I think you mean Strings.isNotEmpty ?", "author": "openinx", "createdAt": "2020-09-10T06:54:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA1ODIwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjExNjA0NA==", "url": "https://github.com/apache/iceberg/pull/1404#discussion_r486116044", "bodyText": "OK, you are right.  the Strings.isNotEmpty is from another dependency,   should be OK to use !Strings.isNullOrEmpty in guava.  Thanks.", "author": "openinx", "createdAt": "2020-09-10T07:14:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA1ODIwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA1ODM4NA==", "url": "https://github.com/apache/iceberg/pull/1404#discussion_r486058384", "bodyText": "nit: oldFlinkJobId -> restoredFlinkJobId, since it may be the same job id for failure recovery case.", "author": "stevenzwu", "createdAt": "2020-09-10T04:38:41Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -108,18 +115,25 @@ public void initializeState(StateInitializationContext context) throws Exception\n     this.maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n \n     this.checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    this.jobIdState = context.getOperatorStateStore().getListState(JOB_ID_DESCRIPTOR);\n     if (context.isRestored()) {\n       this.maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);\n-      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n-      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n-      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n-      // the iceberg table.\n-      Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID,\n-          \"There should be an existing iceberg snapshot for current flink job: %s\", flinkJobId);\n-\n-      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n-      // Only keep the uncommitted data files in the cache.\n-      this.dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));\n+\n+      String oldFlinkJobId = jobIdState.get().iterator().next();", "originalCommit": "4582f1585aa5ac82477bc11f9728ecd2b6e89a8d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA1OTgxMw==", "url": "https://github.com/apache/iceberg/pull/1404#discussion_r486059813", "bodyText": "nit: maybe a simpler version? we can also remove the line 120 above\nlong maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, restoredFlinkJobId);", "author": "stevenzwu", "createdAt": "2020-09-10T04:44:15Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -108,18 +115,25 @@ public void initializeState(StateInitializationContext context) throws Exception\n     this.maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n \n     this.checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    this.jobIdState = context.getOperatorStateStore().getListState(JOB_ID_DESCRIPTOR);\n     if (context.isRestored()) {\n       this.maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);\n-      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n-      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n-      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n-      // the iceberg table.\n-      Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID,\n-          \"There should be an existing iceberg snapshot for current flink job: %s\", flinkJobId);\n-\n-      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n-      // Only keep the uncommitted data files in the cache.\n-      this.dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));\n+\n+      String oldFlinkJobId = jobIdState.get().iterator().next();\n+      Preconditions.checkState(oldFlinkJobId != null && oldFlinkJobId.length() > 0,\n+          \"Flink job id parsed from checkpoint snapshot shouldn't be null or empty\");\n+\n+      long oldMaxCommittedCheckpointId = flinkJobId.equals(oldFlinkJobId) ?", "originalCommit": "4582f1585aa5ac82477bc11f9728ecd2b6e89a8d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjExNzEwNw==", "url": "https://github.com/apache/iceberg/pull/1404#discussion_r486117107", "bodyText": "Since flink's checkpoint id will start from the max-committed-checkpoint-id + 1 in the new flink job even if it's restored from a snapshot created by another different flink job, so it's safe to assign the max committed\ncheckpoint id from restored flink job to the current flink job.\nYeah, we could make it more simpler now.", "author": "openinx", "createdAt": "2020-09-10T07:16:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA1OTgxMw=="}], "type": "inlineReview"}, {"oid": "9d4897ee56570767336ca1ebcf43f6b3d9c71bc9", "url": "https://github.com/apache/iceberg/commit/9d4897ee56570767336ca1ebcf43f6b3d9c71bc9", "message": "Flink: Add flink job id to state backend for handling flink job redeployment.", "committedDate": "2020-09-10T07:34:21Z", "type": "commit"}, {"oid": "c82d04a43e3fbc6b95e364ce10e68be2c582b58c", "url": "https://github.com/apache/iceberg/commit/c82d04a43e3fbc6b95e364ce10e68be2c582b58c", "message": "Minior fixes", "committedDate": "2020-09-10T07:34:21Z", "type": "commit"}, {"oid": "1d6d8f00e18e2006a71b2c7c175761964314b8b7", "url": "https://github.com/apache/iceberg/commit/1d6d8f00e18e2006a71b2c7c175761964314b8b7", "message": "Make the max-committed-checkpoint-id parsing more clear", "committedDate": "2020-09-10T07:34:21Z", "type": "commit"}, {"oid": "69a1089b918a50193bccf7d231ce4c443b8670cb", "url": "https://github.com/apache/iceberg/commit/69a1089b918a50193bccf7d231ce4c443b8670cb", "message": "Address comment from stevenzwu", "committedDate": "2020-09-10T07:34:21Z", "type": "commit"}, {"oid": "b0614f63ecf8ba964e0a61429638af6dbff3c0cf", "url": "https://github.com/apache/iceberg/commit/b0614f63ecf8ba964e0a61429638af6dbff3c0cf", "message": "Rebase to master and fix the compile issues", "committedDate": "2020-09-10T07:39:50Z", "type": "commit"}, {"oid": "b0614f63ecf8ba964e0a61429638af6dbff3c0cf", "url": "https://github.com/apache/iceberg/commit/b0614f63ecf8ba964e0a61429638af6dbff3c0cf", "message": "Rebase to master and fix the compile issues", "committedDate": "2020-09-10T07:39:50Z", "type": "forcePushed"}]}