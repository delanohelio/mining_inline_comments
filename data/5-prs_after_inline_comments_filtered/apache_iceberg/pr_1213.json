{"pr_number": 1213, "pr_title": "Abstract the generic task writers for sharing the common codes between spark and flink", "pr_createdAt": "2020-07-16T13:31:04Z", "pr_url": "https://github.com/apache/iceberg/pull/1213", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODI2NTM4NQ==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r458265385", "bodyText": "This will need Javadoc.", "author": "rdblue", "createdAt": "2020-07-21T17:24:55Z", "path": "core/src/main/java/org/apache/iceberg/taskio/FileAppenderFactory.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.taskio;\n+\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.OutputFile;\n+\n+public interface FileAppenderFactory<T> {", "originalCommit": "a539fb89f4c9e0728d3f14b55b45e95bf2954788", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE1MTM0Mw==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r459151343", "bodyText": "If this is a refactor, I'd prefer to keep the existing structure the way it was, with openCurent, closeCurrent, and writeInternal. That way we aren't introducing additional changes in this PR. If we want to refactor how writers work, we can do that separately.", "author": "rdblue", "createdAt": "2020-07-23T00:08:50Z", "path": "core/src/main/java/org/apache/iceberg/taskio/BaseTaskWriter.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.taskio;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Tasks;\n+\n+abstract class BaseTaskWriter<T> implements TaskWriter<T> {\n+  protected static final int ROWS_DIVISOR = 1000;\n+\n+  private final List<DataFile> completedFiles = Lists.newArrayList();\n+  private final PartitionSpec spec;\n+  private final FileFormat format;\n+  private final FileAppenderFactory<T> appenderFactory;\n+  private final OutputFileFactory fileFactory;\n+  private final FileIO io;\n+  private final long targetFileSize;\n+\n+  protected BaseTaskWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                           OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n+    this.spec = spec;\n+    this.format = format;\n+    this.appenderFactory = appenderFactory;\n+    this.fileFactory = fileFactory;\n+    this.io = io;\n+    this.targetFileSize = targetFileSize;\n+  }\n+\n+  @Override\n+  public void abort() throws IOException {\n+    close();\n+\n+    // clean up files created by this writer\n+    Tasks.foreach(completedFiles)\n+        .throwFailureWhenFinished()\n+        .noRetry()\n+        .run(file -> io.deleteFile(file.path().toString()));\n+  }\n+\n+  @Override\n+  public List<DataFile> pollCompleteFiles() {\n+    if (completedFiles.size() > 0) {\n+      List<DataFile> dataFiles = ImmutableList.copyOf(completedFiles);\n+      completedFiles.clear();\n+      return dataFiles;\n+    } else {\n+      return Collections.emptyList();\n+    }\n+  }\n+\n+  protected OutputFileFactory outputFileFactory() {\n+    return this.fileFactory;\n+  }\n+\n+  WrappedFileAppender createWrappedFileAppender(PartitionKey partitionKey,", "originalCommit": "37e378577d53567afae212a531be6df9c998ad33", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE1Mzk5OA==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r459153998", "bodyText": "If there is concern about openCurrent() having code specific to sub-classes, then we can change it to accept an EncryptedOutputFile when it is called. Then BaseWriter wouldn't need to have any logic other than for releasing files that get too large. That would probably require a factory method for EncryptedOutputFile since it is called in writeInternal though. Probably easier just to leave this as it is for now to make this easier to review and get in.", "author": "rdblue", "createdAt": "2020-07-23T00:18:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE1MTM0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2MDAwOA==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r459160008", "bodyText": "Okay, looks like this is intended to support the fan-out writer because the current BaseWriter only supports one open file at a time. I see why there are more changes.", "author": "rdblue", "createdAt": "2020-07-23T00:42:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE1MTM0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE1MjgzMA==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r459152830", "bodyText": "I don't think it's a good idea to have a poll method like this one because it leaks critical state (completedFiles) and creates an opportunity for threading issues between write and pollCompleteFiles.\nInstead, I think the base implementation should use a push model, where each file is released as it is closed.\n  /**\n   * Called when a data file is completed and no longer needed by the writer.\n   */\n  protected abstract void completedFile(DataFile file);\nThen closeCurrent would call completedFile(dataFile) and the implementation of completedFile would handle it from there.", "author": "rdblue", "createdAt": "2020-07-23T00:14:19Z", "path": "core/src/main/java/org/apache/iceberg/taskio/BaseTaskWriter.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.taskio;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Tasks;\n+\n+abstract class BaseTaskWriter<T> implements TaskWriter<T> {\n+  protected static final int ROWS_DIVISOR = 1000;\n+\n+  private final List<DataFile> completedFiles = Lists.newArrayList();\n+  private final PartitionSpec spec;\n+  private final FileFormat format;\n+  private final FileAppenderFactory<T> appenderFactory;\n+  private final OutputFileFactory fileFactory;\n+  private final FileIO io;\n+  private final long targetFileSize;\n+\n+  protected BaseTaskWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                           OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n+    this.spec = spec;\n+    this.format = format;\n+    this.appenderFactory = appenderFactory;\n+    this.fileFactory = fileFactory;\n+    this.io = io;\n+    this.targetFileSize = targetFileSize;\n+  }\n+\n+  @Override\n+  public void abort() throws IOException {\n+    close();\n+\n+    // clean up files created by this writer\n+    Tasks.foreach(completedFiles)\n+        .throwFailureWhenFinished()\n+        .noRetry()\n+        .run(file -> io.deleteFile(file.path().toString()));\n+  }\n+\n+  @Override\n+  public List<DataFile> pollCompleteFiles() {", "originalCommit": "37e378577d53567afae212a531be6df9c998ad33", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5NDY2Nw==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r459194667", "bodyText": "I read the BaseWriter code again and got the difference here.  For spark streaming writer,  once we did a commit ,  then we will create another new  streaming writer to write the future records,  so we don't need a method like pollCompleteFiles()  to poll the newly added DataFile continusely.  In the current iceberg flink writer implementation,  I will use the same TaskWriter to write record even if  a checkpoint happen,  so I designed the pollCompleteFiles  to fetch all completed data files incrementally.    I think it's design difference,  the state leaks issues and threading issues as you said,  it's not a problem in current version but I agree that it's easy to get into those issues if others did not handle it carefully.   I can align with the current spark design.", "author": "openinx", "createdAt": "2020-07-23T03:19:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE1MjgzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE1NDYxOQ==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r459154619", "bodyText": "Minor: I think the Javadoc for arguments should describe the argument's purpose, like an OutputFile used to create an output stream. If the purpose is clear from the expected type, then keeping it simple is fine, like an OutputFile.", "author": "rdblue", "createdAt": "2020-07-23T00:21:19Z", "path": "core/src/main/java/org/apache/iceberg/taskio/FileAppenderFactory.java", "diffHunk": "@@ -0,0 +1,41 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.taskio;\n+\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.OutputFile;\n+\n+/**\n+ * Factory to create a new {@link FileAppender} to write records.\n+ *\n+ * @param <T> data type of the rows to append.\n+ */\n+public interface FileAppenderFactory<T> {\n+\n+  /**\n+   * Create a new {@link FileAppender}.\n+   *\n+   * @param outputFile indicate the file location to write.", "originalCommit": "37e378577d53567afae212a531be6df9c998ad33", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE1NDk4MQ==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r459154981", "bodyText": "Why not just use the existing io package? That, or maybe a tasks package.", "author": "rdblue", "createdAt": "2020-07-23T00:22:38Z", "path": "core/src/main/java/org/apache/iceberg/taskio/OutputFileFactory.java", "diffHunk": "@@ -17,7 +17,7 @@\n  * under the License.\n  */\n \n-package org.apache.iceberg.spark.source;\n+package org.apache.iceberg.taskio;", "originalCommit": "37e378577d53567afae212a531be6df9c998ad33", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2MjI4OQ==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r459162289", "bodyText": "I don't see much value in this class. Its primary use is to keep track of whether a file is large enough to release, but it doesn't actually have any of the logic to do that. As a consequence, the code is now split across multiple places.\nThis also has the logic for closing an appender and converting it to a DataFile, but that could just as easily be done in a DataFile closeAppender(FileAppender appender) method.\nIt would make sense to keep this class if it completely encapsulated the logic of rolling new files. That would require some refactoring so that it could create new files using the file and appender factories. It would also require passing a Consumer<DataFile> so that it can release closed files. Otherwise, I think we should remove this class.", "author": "rdblue", "createdAt": "2020-07-23T00:51:53Z", "path": "core/src/main/java/org/apache/iceberg/taskio/BaseTaskWriter.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.taskio;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Tasks;\n+\n+abstract class BaseTaskWriter<T> implements TaskWriter<T> {\n+  protected static final int ROWS_DIVISOR = 1000;\n+\n+  private final List<DataFile> completedFiles = Lists.newArrayList();\n+  private final PartitionSpec spec;\n+  private final FileFormat format;\n+  private final FileAppenderFactory<T> appenderFactory;\n+  private final OutputFileFactory fileFactory;\n+  private final FileIO io;\n+  private final long targetFileSize;\n+\n+  protected BaseTaskWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                           OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n+    this.spec = spec;\n+    this.format = format;\n+    this.appenderFactory = appenderFactory;\n+    this.fileFactory = fileFactory;\n+    this.io = io;\n+    this.targetFileSize = targetFileSize;\n+  }\n+\n+  @Override\n+  public void abort() throws IOException {\n+    close();\n+\n+    // clean up files created by this writer\n+    Tasks.foreach(completedFiles)\n+        .throwFailureWhenFinished()\n+        .noRetry()\n+        .run(file -> io.deleteFile(file.path().toString()));\n+  }\n+\n+  @Override\n+  public List<DataFile> pollCompleteFiles() {\n+    if (completedFiles.size() > 0) {\n+      List<DataFile> dataFiles = ImmutableList.copyOf(completedFiles);\n+      completedFiles.clear();\n+      return dataFiles;\n+    } else {\n+      return Collections.emptyList();\n+    }\n+  }\n+\n+  protected OutputFileFactory outputFileFactory() {\n+    return this.fileFactory;\n+  }\n+\n+  WrappedFileAppender createWrappedFileAppender(PartitionKey partitionKey,\n+                                                Supplier<EncryptedOutputFile> outputFileSupplier) {\n+    EncryptedOutputFile outputFile = outputFileSupplier.get();\n+    FileAppender<T> appender = appenderFactory.newAppender(outputFile.encryptingOutputFile(), format);\n+    return new WrappedFileAppender(partitionKey, outputFile, appender);\n+  }\n+\n+  class WrappedFileAppender {", "originalCommit": "37e378577d53567afae212a531be6df9c998ad33", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5Njc5NQ==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r459196795", "bodyText": "I created this class because in fanout writer  we will have several opening writers  and when building the DataFile,  we will need all the informations for the given FileAppender,  such as partitionKey,  EncryptedOutputFile etc. The previous spark implementations won't need the class because all of the context information are maintained inside the  PartitionedWriter (currentXXX ), that's not work for fanout writer. It will be better to have such a class to hold those infos to build DataFile.\n\nIt would make sense to keep this class if it completely encapsulated the logic of rolling new files\n\nGood point.   Make the WrappedFileAppender to accomplish all the rolling things,  let me refactor this.", "author": "openinx", "createdAt": "2020-07-23T03:29:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2MjI4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2MjcxNQ==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r459162715", "bodyText": "We should consider changing the ORC appender to simply return 0 if the file isn't finished. That way this check is still valid, but the file will never be rolled.", "author": "rdblue", "createdAt": "2020-07-23T00:53:52Z", "path": "core/src/main/java/org/apache/iceberg/taskio/BaseTaskWriter.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.taskio;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Tasks;\n+\n+abstract class BaseTaskWriter<T> implements TaskWriter<T> {\n+  protected static final int ROWS_DIVISOR = 1000;\n+\n+  private final List<DataFile> completedFiles = Lists.newArrayList();\n+  private final PartitionSpec spec;\n+  private final FileFormat format;\n+  private final FileAppenderFactory<T> appenderFactory;\n+  private final OutputFileFactory fileFactory;\n+  private final FileIO io;\n+  private final long targetFileSize;\n+\n+  protected BaseTaskWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                           OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n+    this.spec = spec;\n+    this.format = format;\n+    this.appenderFactory = appenderFactory;\n+    this.fileFactory = fileFactory;\n+    this.io = io;\n+    this.targetFileSize = targetFileSize;\n+  }\n+\n+  @Override\n+  public void abort() throws IOException {\n+    close();\n+\n+    // clean up files created by this writer\n+    Tasks.foreach(completedFiles)\n+        .throwFailureWhenFinished()\n+        .noRetry()\n+        .run(file -> io.deleteFile(file.path().toString()));\n+  }\n+\n+  @Override\n+  public List<DataFile> pollCompleteFiles() {\n+    if (completedFiles.size() > 0) {\n+      List<DataFile> dataFiles = ImmutableList.copyOf(completedFiles);\n+      completedFiles.clear();\n+      return dataFiles;\n+    } else {\n+      return Collections.emptyList();\n+    }\n+  }\n+\n+  protected OutputFileFactory outputFileFactory() {\n+    return this.fileFactory;\n+  }\n+\n+  WrappedFileAppender createWrappedFileAppender(PartitionKey partitionKey,\n+                                                Supplier<EncryptedOutputFile> outputFileSupplier) {\n+    EncryptedOutputFile outputFile = outputFileSupplier.get();\n+    FileAppender<T> appender = appenderFactory.newAppender(outputFile.encryptingOutputFile(), format);\n+    return new WrappedFileAppender(partitionKey, outputFile, appender);\n+  }\n+\n+  class WrappedFileAppender {\n+    private final PartitionKey partitionKey;\n+    private final EncryptedOutputFile encryptedOutputFile;\n+    private final FileAppender<T> appender;\n+\n+    private boolean closed = false;\n+    private long currentRows = 0;\n+\n+    WrappedFileAppender(PartitionKey partitionKey, EncryptedOutputFile encryptedOutputFile, FileAppender<T> appender) {\n+      this.partitionKey = partitionKey;\n+      this.encryptedOutputFile = encryptedOutputFile;\n+      this.appender = appender;\n+    }\n+\n+    void add(T record) {\n+      this.appender.add(record);\n+      this.currentRows++;\n+    }\n+\n+    boolean shouldRollToNewFile() {\n+      //TODO: ORC file now not support target file size before closed", "originalCommit": "37e378577d53567afae212a531be6df9c998ad33", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5NzI0Mw==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r459197243", "bodyText": "It could be a separate issue to address this ORC issue you described ?   I think we could focus on the writer refactor.", "author": "openinx", "createdAt": "2020-07-23T03:31:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2MjcxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NTA1NA==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r459165054", "bodyText": "Instead of passing a function, I think this should be an abstract method:\n  /**\n   * Create a PartitionKey from the values in row.\n   * <p>\n   * Any PartitionKey returned by this method can be reused by the implementation.\n   *\n   * @param row a data row\n   */\n  protected abstract PartitionKey partition(T row);\nPassing a function is good if we need to inject behavior that might need to be customized, but here the only customization that would be required is to partition the objects that this class is already parameterized by. So it will be easier just to add a method for subclasses to implement. And that puts the responsibility on the implementation instead of on the code that constructs the writer.", "author": "rdblue", "createdAt": "2020-07-23T01:03:28Z", "path": "core/src/main/java/org/apache/iceberg/taskio/PartitionedFanoutWriter.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.taskio;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+\n+public class PartitionedFanoutWriter<T> extends BaseTaskWriter<T> {\n+  private final Function<T, PartitionKey> keyGetter;", "originalCommit": "37e378577d53567afae212a531be6df9c998ad33", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NTM0MQ==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r459165341", "bodyText": "Like the other partitioned writer, I think this should use an abstract method to be implemented by subclasses.", "author": "rdblue", "createdAt": "2020-07-23T01:04:40Z", "path": "core/src/main/java/org/apache/iceberg/taskio/PartitionedWriter.java", "diffHunk": "@@ -17,41 +17,40 @@\n  * under the License.\n  */\n \n-package org.apache.iceberg.spark.source;\n+package org.apache.iceberg.taskio;\n \n import java.io.IOException;\n import java.util.Set;\n+import java.util.function.Function;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.PartitionKey;\n import org.apache.iceberg.PartitionSpec;\n-import org.apache.iceberg.Schema;\n import org.apache.iceberg.io.FileIO;\n import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n-import org.apache.iceberg.spark.SparkSchemaUtil;\n-import org.apache.spark.sql.catalyst.InternalRow;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-class PartitionedWriter extends BaseWriter {\n+public class PartitionedWriter<T> extends BaseTaskWriter<T> {\n   private static final Logger LOG = LoggerFactory.getLogger(PartitionedWriter.class);\n \n-  private final PartitionKey key;\n-  private final InternalRowWrapper wrapper;\n+  private final Function<T, PartitionKey> keyGetter;\n   private final Set<PartitionKey> completedPartitions = Sets.newHashSet();\n \n-  PartitionedWriter(PartitionSpec spec, FileFormat format, SparkAppenderFactory appenderFactory,\n-                    OutputFileFactory fileFactory, FileIO io, long targetFileSize, Schema writeSchema) {\n+  private PartitionKey currentKey = null;\n+  private WrappedFileAppender currentAppender = null;\n+\n+  public PartitionedWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                           OutputFileFactory fileFactory, FileIO io, long targetFileSize,\n+                           Function<T, PartitionKey> keyGetter) {\n     super(spec, format, appenderFactory, fileFactory, io, targetFileSize);\n-    this.key = new PartitionKey(spec, writeSchema);\n-    this.wrapper = new InternalRowWrapper(SparkSchemaUtil.convert(writeSchema));\n+    this.keyGetter = keyGetter;", "originalCommit": "37e378577d53567afae212a531be6df9c998ad33", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU3NzI4Nw==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460577287", "bodyText": "This code here is handling the case where we've not seen this partition key yet. This is especially likely to happen when users did not keyBy or otherwise pre-shuffle the data according to the partition key.\nIs pre-shuffling something that the users should be doing before writing to the table (either keyBy or ORDER BY in Flink SQL)? I understand that this is specifically a PartitionedFanoutWriter, and so it makes sense that keys might not always come together (and even in the case where users did keyBy the partition key, if the number of TaskManager slots that are writing does not equal the cardinality of the partition key you'll still wind up with multiple RollingFileAppenders in a single Flink writing task and thus fanout). However, for long running streaming queries, it's possible that this TaskManager doesn't see this partition key again for days or even weeks (especially at a high enough volume to emit a complete file of the given target file size).\nI guess my concern is that users wind up with a very high cardinality of keys on a single TaskManager. Either because they didn't pre-shuffle their data or perhaps they have an imbalance between the cardinality on the partition key and the parallelism at the write stage such that records might not naturally group together enough to emit an entire file. Or,  as another edge case, one partition key value is simply not common enough to emit an entire file from this PartitionedFanoutWriter.\nIIUC, if the PartitionedFanoutWriter does not see this partition key enough times in this TaskManager again to emit a full file for quite some time, a file containing this data won't be written until close is called. For very long running streaming jobs, this could be days or even weeks in my experience. This could also lead to small files upon close. Is this a concern that Iceberg should take into consideration or is this left to the users in their Flink query to determine when tuning their queries?\nI imagine with S3, data locality of a file written much later than its timestamp of when the data was received is not a major concern, as the manifest file will tell whatever query engine reads this table which keys in their S3 bucket to grab and the locality issue is relatively abstracted away from the user, but what about if the user is using HDFS? Could this lead to performance issues (or even correctness issues) on read if records with relatively similar timestamps at their RollingFileAppender are scattered across a potentially large number of files?\nI suppose this amounts to three concerns (and forgive me if these are non-issues as I am still new to the project, but not new to Flink so partially this is for helping me understand, as well as reviewing my concerns when reading this code):\n\nShould we be concerned that a writer won't emit a file until a streaming query is closed due to the previously mentioned case? Possibly tracking the time that each writer has existed and then emitting a file if it has been far too long (however that could be determined).\nIf a record comes in at some time, and then the file containing that record isn't written for a much greater period of time (on the order of days or weeks), could this lead to correctness problems or very large performance problems when any query engine reads this table?\nWould it be beneficial to at least emit a warning or info level log to the user that it might be beneficial to pre-partition their data according to the partition key spec if perhaps the number of unique RollingFileAppender writers gets too high for one given Flink writer slot / TaskManager? Admittedly, it might be difficult to determine a heuristic of when this might be a problem vs just the natural difference in the parallelism of writing task slots vs the cardinality of the partition key.", "author": "kbendick", "createdAt": "2020-07-26T21:46:39Z", "path": "core/src/main/java/org/apache/iceberg/io/PartitionedFanoutWriter.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+\n+public abstract class PartitionedFanoutWriter<T> extends BaseTaskWriter<T> {\n+  private final Map<PartitionKey, RollingFileAppender> writers = Maps.newHashMap();\n+\n+  public PartitionedFanoutWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                                 OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n+    super(spec, format, appenderFactory, fileFactory, io, targetFileSize);\n+  }\n+\n+  /**\n+   * Create a PartitionKey from the values in row.\n+   * <p>\n+   * Any PartitionKey returned by this method can be reused by the implementation.\n+   *\n+   * @param row a data row\n+   */\n+  protected abstract PartitionKey partition(T row);\n+\n+  @Override\n+  public void write(T row) throws IOException {\n+    PartitionKey partitionKey = partition(row);\n+\n+    RollingFileAppender writer = writers.get(partitionKey);\n+    if (writer == null) {\n+      // NOTICE: we need to copy a new partition key here, in case of messing up the keys in writers.\n+      PartitionKey copiedKey = partitionKey.copy();\n+      writer = new RollingFileAppender(copiedKey);\n+      writers.put(copiedKey, writer);", "originalCommit": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4NTgwMQ==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460585801", "bodyText": "Should we be concerned that a writer won't emit a file until a streaming query is closed due to the previously mentioned case?\n\nI think that the intent is to close and emit all of the file files each checkpoint, rather than keeping them open. That is required to achieve exactly-once writes because the data needs to be committed to the table.\nI think that also takes care of your second question because data is constantly added to the table.\n\nWould it be beneficial to at least emit a warning or info level log to the user that it might be beneficial to pre-partition their data according to the partition key spec . . .\n\nI think a reasonable thing to do is to limit the number of writers that are kept open, to limit the resources that are held. Then you can either fail if you go over that limit, or can close and release files with a LRU policy. Failing brings the problem to the user's attention immediately and is similar to what we do on the Spark side, which doesn't allow writing new data to a partition after it is finished. That ensures that data is either clustered for the write, or the job fails.\nThe long-term plan for Spark is to be able to influence the logical plan that is writing to a table. That would be the equivalent of adding an automatic keyBy or rough orderBy for Flink. I think we would eventually want to do this for Flink as well, but I'm not sure what data clustering and sorting operations are supported currently.", "author": "rdblue", "createdAt": "2020-07-26T23:19:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU3NzI4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4OTQ4Mw==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460589483", "bodyText": "Ah ok. I hadn't realized that was the plan.\nI wrote a parquet writer for flink way back when flink did not support it and outputting files on checkpoint was the only real solution that I could come up with.\nIt also involved forking the base parquet-library, so we wound up abandoning it as we don't really have the engineering head count to be constantly updating and maintaining something like that. Despite the fact that Flink can now support writing parquet files etc, this is why I'm interested in this project. That and then the numerous additions to the data lake that the project supports.\nThanks for the info @rdblue!", "author": "kbendick", "createdAt": "2020-07-26T23:53:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU3NzI4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDYzMzAzMA==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460633030", "bodyText": "During scan planning, IIUC, an inclusive projection could possibly match a very large number of rows that might fall outside of the predicate range if the RollingFileAppender for this rarely observed predicate at this Task Manager buffers its data for a very long time before writing (say days or even weeks in a longer running streaming query).\n\nYou mean the flink streaming reader won't see the buffered data which is still not committed to iceberg table ?  Actually,  that's exactly the expected behavior.  Say we have a data pipeline:\n(flink-streaming-sink-job-A) -> (iceberg table) -> (flink-streaming-reader-job-B). \n\nThe upstream flink-streaming-sink-job-A  will append the records to iceberg table continuously, and commit to the iceberg table if checkpoint happen. we need to guarantee the transaction semantic, so the downstream flink streaming reader could only see the committed iceberg data, the delta data between two contiguous snapshots is the incremental data that the flink streaming reader should consume.", "author": "openinx", "createdAt": "2020-07-27T03:52:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU3NzI4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4MjUzMg==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460582532", "bodyText": "Minor: This doesn't implement FileAppender, so maybe RollingFileWriter would make more sense?", "author": "rdblue", "createdAt": "2020-07-26T22:42:03Z", "path": "core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Tasks;\n+\n+abstract class BaseTaskWriter<T> implements TaskWriter<T> {\n+  private final List<DataFile> completedFiles = Lists.newArrayList();\n+  private final PartitionSpec spec;\n+  private final FileFormat format;\n+  private final FileAppenderFactory<T> appenderFactory;\n+  private final OutputFileFactory fileFactory;\n+  private final FileIO io;\n+  private final long targetFileSize;\n+\n+  protected BaseTaskWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                           OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n+    this.spec = spec;\n+    this.format = format;\n+    this.appenderFactory = appenderFactory;\n+    this.fileFactory = fileFactory;\n+    this.io = io;\n+    this.targetFileSize = targetFileSize;\n+  }\n+\n+  @Override\n+  public void abort() throws IOException {\n+    close();\n+\n+    // clean up files created by this writer\n+    Tasks.foreach(completedFiles)\n+        .throwFailureWhenFinished()\n+        .noRetry()\n+        .run(file -> io.deleteFile(file.path().toString()));\n+  }\n+\n+  @Override\n+  public List<DataFile> complete() throws IOException {\n+    close();\n+\n+    if (completedFiles.size() > 0) {\n+      return ImmutableList.copyOf(completedFiles);\n+    } else {\n+      return Collections.emptyList();\n+    }\n+  }\n+\n+  class RollingFileAppender implements Closeable {", "originalCommit": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDYxMTMzMw==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460611333", "bodyText": "Well, sounds great.", "author": "openinx", "createdAt": "2020-07-27T02:07:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4MjUzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4MzA0Mw==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460583043", "bodyText": "It would be nice to not change the logic for opening an appender. Before, this was part of the flow of changing partitions and I don't see any value in moving it.", "author": "rdblue", "createdAt": "2020-07-26T22:48:20Z", "path": "core/src/main/java/org/apache/iceberg/io/PartitionedWriter.java", "diffHunk": "@@ -63,10 +66,29 @@ public void write(InternalRow row) throws IOException {\n         throw new IllegalStateException(\"Already closed files for partition: \" + key.toPath());\n       }\n \n-      setCurrentKey(key.copy());\n-      openCurrent();\n+      currentKey = key.copy();\n+    }\n+\n+    if (currentAppender == null) {\n+      currentAppender = new RollingFileAppender(currentKey);", "originalCommit": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDY4NDY1Mg==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460684652", "bodyText": "Now we've changed to maintain the partitionKey inside the RollingFileWriter (we've discussed this before , this is because for fanout writer we may have multiple writers append records),  so the  RollingFileAppender creation is actually doing the partition key setting.  I did not open the appender here because we only need to open an appender when there's a real record to write (in case of opening an appender without writing a record) ,  all those logic have been hidden inside the RollingFileAppender.", "author": "openinx", "createdAt": "2020-07-27T07:01:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4MzA0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4MzIzNw==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460583237", "bodyText": "Now that the current key is null, we will need a check before adding it to completedPartitions in the write method:\nif (!key.equals(currentKey)) {\n  closeCurrent();\n  if (currentKey != null) {\n    // if the key is null, there was no previous current key\n    completedPartitions.add(currentKey);\n  }\n  ...\n}", "author": "rdblue", "createdAt": "2020-07-26T22:50:38Z", "path": "core/src/main/java/org/apache/iceberg/io/PartitionedWriter.java", "diffHunk": "@@ -17,41 +17,44 @@\n  * under the License.\n  */\n \n-package org.apache.iceberg.spark.source;\n+package org.apache.iceberg.io;\n \n import java.io.IOException;\n import java.util.Set;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.PartitionKey;\n import org.apache.iceberg.PartitionSpec;\n-import org.apache.iceberg.Schema;\n-import org.apache.iceberg.io.FileIO;\n import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n-import org.apache.iceberg.spark.SparkSchemaUtil;\n-import org.apache.spark.sql.catalyst.InternalRow;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-class PartitionedWriter extends BaseWriter {\n+public abstract class PartitionedWriter<T> extends BaseTaskWriter<T> {\n   private static final Logger LOG = LoggerFactory.getLogger(PartitionedWriter.class);\n \n-  private final PartitionKey key;\n-  private final InternalRowWrapper wrapper;\n   private final Set<PartitionKey> completedPartitions = Sets.newHashSet();\n \n-  PartitionedWriter(PartitionSpec spec, FileFormat format, SparkAppenderFactory appenderFactory,\n-                    OutputFileFactory fileFactory, FileIO io, long targetFileSize, Schema writeSchema) {\n+  private PartitionKey currentKey = null;\n+  private RollingFileAppender currentAppender = null;", "originalCommit": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDYzNTI1NQ==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460635255", "bodyText": "Nice finding.", "author": "openinx", "createdAt": "2020-07-27T04:02:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4MzIzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4MzM4MQ==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460583381", "bodyText": "Why not initialize currentAppender in the constructor? Then we don't need an additional null check in write, which is called in a tight loop.", "author": "rdblue", "createdAt": "2020-07-26T22:52:38Z", "path": "core/src/main/java/org/apache/iceberg/io/UnpartitionedWriter.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.IOException;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+\n+public class UnpartitionedWriter<T> extends BaseTaskWriter<T> {\n+\n+  private RollingFileAppender currentAppender = null;\n+\n+  public UnpartitionedWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                             OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n+    super(spec, format, appenderFactory, fileFactory, io, targetFileSize);\n+  }\n+\n+  @Override\n+  public void write(T record) throws IOException {\n+    if (currentAppender == null) {\n+      currentAppender = new RollingFileAppender(null);", "originalCommit": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDYzNzMxMw==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460637313", "bodyText": "I refactor this part because we don't need to initialize any real writer if there's no record come in.  Before this patch ,   it will open a real file writer even if there's no record to write, and in the end we will need to close this useless writer and clean its file.", "author": "openinx", "createdAt": "2020-07-27T04:12:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4MzM4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4MzUyOQ==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460583529", "bodyText": "Is this change needed? It looks non-functional.", "author": "rdblue", "createdAt": "2020-07-26T22:54:23Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "diffHunk": "@@ -92,16 +96,17 @@ private TaskResult rewriteDataForTask(CombinedScanTask task) throws Exception {\n     RowDataReader dataReader = new RowDataReader(\n         task, schema, schema, nameMapping, io.value(), encryptionManager.value(), caseSensitive);\n \n-    SparkAppenderFactory appenderFactory = new SparkAppenderFactory(\n-        properties, schema, SparkSchemaUtil.convert(schema));\n+    StructType structType = SparkSchemaUtil.convert(schema);\n+    SparkAppenderFactory appenderFactory = new SparkAppenderFactory(properties, schema, structType);", "originalCommit": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDYzNzM4Nw==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460637387", "bodyText": "I changed this because the SparkPartitionedWriter  will need the spark's StructType to decide how to partition, so I move the StructType structType = SparkSchemaUtil.convert(schema);  into a separate line.", "author": "openinx", "createdAt": "2020-07-27T04:12:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4MzUyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0NDIyNg==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r463344226", "bodyText": "Got it. I see it's used later.", "author": "rdblue", "createdAt": "2020-07-31T00:34:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4MzUyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4MzgxMw==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460583813", "bodyText": "If complete doesn't produce TaskResult, then I'm not sure that we need it at all anymore. Could we just construct TaskCommit directly?", "author": "rdblue", "createdAt": "2020-07-26T22:57:05Z", "path": "spark2/src/main/java/org/apache/iceberg/spark/source/Writer.java", "diffHunk": "@@ -250,33 +253,42 @@ public String toString() {\n       if (spec.fields().isEmpty()) {\n         return new Unpartitioned24Writer(spec, format, appenderFactory, fileFactory, io.value(), targetFileSize);\n       } else {\n-        return new Partitioned24Writer(\n-            spec, format, appenderFactory, fileFactory, io.value(), targetFileSize, writeSchema);\n+        return new Partitioned24Writer(spec, format, appenderFactory, fileFactory, io.value(),\n+            targetFileSize, writeSchema, dsSchema);\n       }\n     }\n   }\n \n-  private static class Unpartitioned24Writer extends UnpartitionedWriter implements DataWriter<InternalRow> {\n+  private static class Unpartitioned24Writer extends UnpartitionedWriter<InternalRow>\n+      implements DataWriter<InternalRow> {\n     Unpartitioned24Writer(PartitionSpec spec, FileFormat format, SparkAppenderFactory appenderFactory,\n                           OutputFileFactory fileFactory, FileIO fileIo, long targetFileSize) {\n       super(spec, format, appenderFactory, fileFactory, fileIo, targetFileSize);\n     }\n \n     @Override\n     public WriterCommitMessage commit() throws IOException {\n-      return new TaskCommit(complete());\n+      this.close();\n+\n+      List<DataFile> dataFiles = complete();\n+      return new TaskCommit(new TaskResult(dataFiles));", "originalCommit": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDYzNzQxMQ==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460637411", "bodyText": "OK", "author": "openinx", "createdAt": "2020-07-27T04:12:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4MzgxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4NDE5OQ==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460584199", "bodyText": "#1232 and #1237 rebuild the Avro and Parquet writers to use RowData instead of Row. To deconflict, do you think it makes sense to get the base classes and Spark refactor in this PR and separate out the Flink side? I'm fine either way, whatever you think is going to be easier.", "author": "rdblue", "createdAt": "2020-07-26T23:01:23Z", "path": "flink/src/main/java/org/apache/iceberg/flink/TaskWriterFactory.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Map;\n+import org.apache.flink.types.Row;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.flink.data.FlinkAvroWriter;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileAppenderFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.io.OutputFileFactory;\n+import org.apache.iceberg.io.PartitionedFanoutWriter;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.io.UnpartitionedWriter;\n+import org.apache.iceberg.parquet.Parquet;\n+\n+class TaskWriterFactory {\n+  private TaskWriterFactory() {\n+  }\n+\n+  static TaskWriter<Row> createTaskWriter(Schema schema,\n+                                          PartitionSpec spec,\n+                                          FileFormat format,\n+                                          FileAppenderFactory<Row> appenderFactory,\n+                                          OutputFileFactory fileFactory,\n+                                          FileIO io,\n+                                          long targetFileSizeBytes) {\n+    if (spec.fields().isEmpty()) {\n+      return new UnpartitionedWriter<>(spec, format, appenderFactory, fileFactory, io, targetFileSizeBytes);\n+    } else {\n+      return new RowPartitionedFanoutWriter(spec, format, appenderFactory, fileFactory,\n+          io, targetFileSizeBytes, schema);\n+    }\n+  }\n+\n+  private static class RowPartitionedFanoutWriter extends PartitionedFanoutWriter<Row> {\n+\n+    private final PartitionKey partitionKey;\n+    private final RowWrapper rowWrapper;\n+\n+    RowPartitionedFanoutWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<Row> appenderFactory,\n+                               OutputFileFactory fileFactory, FileIO io, long targetFileSize, Schema schema) {\n+      super(spec, format, appenderFactory, fileFactory, io, targetFileSize);\n+      this.partitionKey = new PartitionKey(spec, schema);\n+      this.rowWrapper = new RowWrapper(schema.asStruct());\n+    }\n+\n+    @Override\n+    protected PartitionKey partition(Row row) {\n+      partitionKey.partition(rowWrapper.wrap(row));\n+      return partitionKey;\n+    }\n+  }\n+\n+  static class FlinkFileAppenderFactory implements FileAppenderFactory<Row> {\n+    private final Schema schema;\n+    private final Map<String, String> props;\n+\n+    FlinkFileAppenderFactory(Schema schema, Map<String, String> props) {\n+      this.schema = schema;\n+      this.props = props;\n+    }\n+\n+    @Override\n+    public FileAppender<Row> newAppender(OutputFile outputFile, FileFormat format) {", "originalCommit": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDYzODAzMw==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460638033", "bodyText": "I see those RP(s),  I'd prefer to keep the current version so that we could introduce the flink unit test to address this big change.  Changing it to RowData should be easy in future I think.", "author": "openinx", "createdAt": "2020-07-27T04:16:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4NDE5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4NTAzMw==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460585033", "bodyText": "Many iterator classes don't implement remove. What about iterating over the key set separately instead?\n  if (!writers.isEmpty()) {\n    for (PartitionKey key : writers.keySet()) {\n      RollingFileAppender writer = writers.remove(key);\n      writer.close();\n    }\n  }", "author": "rdblue", "createdAt": "2020-07-26T23:11:18Z", "path": "core/src/main/java/org/apache/iceberg/io/PartitionedFanoutWriter.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+\n+public abstract class PartitionedFanoutWriter<T> extends BaseTaskWriter<T> {\n+  private final Map<PartitionKey, RollingFileAppender> writers = Maps.newHashMap();\n+\n+  public PartitionedFanoutWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                                 OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n+    super(spec, format, appenderFactory, fileFactory, io, targetFileSize);\n+  }\n+\n+  /**\n+   * Create a PartitionKey from the values in row.\n+   * <p>\n+   * Any PartitionKey returned by this method can be reused by the implementation.\n+   *\n+   * @param row a data row\n+   */\n+  protected abstract PartitionKey partition(T row);\n+\n+  @Override\n+  public void write(T row) throws IOException {\n+    PartitionKey partitionKey = partition(row);\n+\n+    RollingFileAppender writer = writers.get(partitionKey);\n+    if (writer == null) {\n+      // NOTICE: we need to copy a new partition key here, in case of messing up the keys in writers.\n+      PartitionKey copiedKey = partitionKey.copy();\n+      writer = new RollingFileAppender(copiedKey);\n+      writers.put(copiedKey, writer);\n+    }\n+\n+    writer.add(row);\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (!writers.isEmpty()) {\n+      Iterator<RollingFileAppender> iterator = writers.values().iterator();\n+      while (iterator.hasNext()) {\n+        iterator.next().close();\n+        // Remove from the writers after closed.\n+        iterator.remove();", "originalCommit": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDYzNDc5OA==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460634798", "bodyText": "OK, sounds good.", "author": "openinx", "createdAt": "2020-07-27T04:00:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4NTAzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDU4NjM1NQ==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r460586355", "bodyText": "I think this is fine, but you might want to move this into Flink and combine it with the Flink-specific writer. There are a lot of concerns that might need to change for this class, like using a LRU cache for writers, incrementally releasing files, etc. Since this is only used by Flink, we might just want to iterate on it there instead of trying to maintain this as an independent class. We can always bring it back out when we have an additional use case.", "author": "rdblue", "createdAt": "2020-07-26T23:24:19Z", "path": "core/src/main/java/org/apache/iceberg/io/PartitionedFanoutWriter.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+\n+public abstract class PartitionedFanoutWriter<T> extends BaseTaskWriter<T> {\n+  private final Map<PartitionKey, RollingFileAppender> writers = Maps.newHashMap();\n+\n+  public PartitionedFanoutWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                                 OutputFileFactory fileFactory, FileIO io, long targetFileSize) {", "originalCommit": "9732fcd651878e3ccf84ce8bde1fa8cdeff8120c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0NTkzNw==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r463345937", "bodyText": "Unpartitioned writers pass a null partition key. Would it make more sense to use that instead of using spec?", "author": "rdblue", "createdAt": "2020-07-31T00:41:11Z", "path": "core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Tasks;\n+\n+public abstract class BaseTaskWriter<T> implements TaskWriter<T> {\n+  private final List<DataFile> completedFiles = Lists.newArrayList();\n+  private final PartitionSpec spec;\n+  private final FileFormat format;\n+  private final FileAppenderFactory<T> appenderFactory;\n+  private final OutputFileFactory fileFactory;\n+  private final FileIO io;\n+  private final long targetFileSize;\n+\n+  protected BaseTaskWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                           OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n+    this.spec = spec;\n+    this.format = format;\n+    this.appenderFactory = appenderFactory;\n+    this.fileFactory = fileFactory;\n+    this.io = io;\n+    this.targetFileSize = targetFileSize;\n+  }\n+\n+  @Override\n+  public void abort() throws IOException {\n+    close();\n+\n+    // clean up files created by this writer\n+    Tasks.foreach(completedFiles)\n+        .throwFailureWhenFinished()\n+        .noRetry()\n+        .run(file -> io.deleteFile(file.path().toString()));\n+  }\n+\n+  @Override\n+  public List<DataFile> complete() throws IOException {\n+    close();\n+\n+    if (completedFiles.size() > 0) {\n+      return ImmutableList.copyOf(completedFiles);\n+    } else {\n+      return Collections.emptyList();\n+    }\n+  }\n+\n+  protected class RollingFileWriter implements Closeable {\n+    private static final int ROWS_DIVISOR = 1000;\n+    private final PartitionKey partitionKey;\n+\n+    private EncryptedOutputFile currentFile = null;\n+    private FileAppender<T> currentAppender = null;\n+    private long currentRows = 0;\n+\n+    public RollingFileWriter(PartitionKey partitionKey) {\n+      this.partitionKey = partitionKey;\n+    }\n+\n+    public void add(T record) throws IOException {\n+      if (currentAppender == null) {\n+        openCurrent();\n+      }\n+\n+      this.currentAppender.add(record);\n+      this.currentRows++;\n+\n+      if (shouldRollToNewFile()) {\n+        closeCurrent();\n+      }\n+    }\n+\n+    private void openCurrent() {\n+      if (spec.fields().size() == 0) {", "originalCommit": "7135c10f2caff4dc5b0de95c9928f68cdfa94f75", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQzNDk5Nw==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r463434997", "bodyText": "Yeah, it make sense. Thanks.", "author": "openinx", "createdAt": "2020-07-31T06:48:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0NTkzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0NDEwNw==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r463444107", "bodyText": "Yeah,  it makes sense.", "author": "openinx", "createdAt": "2020-07-31T07:14:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0NTkzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0Njc0NA==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r463346744", "bodyText": "This should be an Array, like it was when this class was based on TaskResult. Arrays are easier to handle when working with Serializable classes because we don't have to worry about bugs caused by List implementations (like the recent Kryo bug with unmodifiable lists).", "author": "rdblue", "createdAt": "2020-07-31T00:44:30Z", "path": "spark2/src/main/java/org/apache/iceberg/spark/source/Writer.java", "diffHunk": "@@ -209,9 +212,15 @@ public String toString() {\n     return String.format(\"IcebergWrite(table=%s, format=%s)\", table, format);\n   }\n \n-  private static class TaskCommit extends TaskResult implements WriterCommitMessage {\n-    TaskCommit(TaskResult toCopy) {\n-      super(toCopy.files());\n+  private static class TaskCommit implements WriterCommitMessage {\n+    private final List<DataFile> taskFiles;", "originalCommit": "7135c10f2caff4dc5b0de95c9928f68cdfa94f75", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQzNjI2Ng==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r463436266", "bodyText": "Fine,  sounds good.", "author": "openinx", "createdAt": "2020-07-31T06:52:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0Njc0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0NDYzOA==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r463444638", "bodyText": "Nice finding,  make sense.", "author": "openinx", "createdAt": "2020-07-31T07:15:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0Njc0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0Njg2Ng==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r463346866", "bodyText": "No need to use the prefix this for close calls, is there?", "author": "rdblue", "createdAt": "2020-07-31T00:44:59Z", "path": "spark2/src/main/java/org/apache/iceberg/spark/source/Writer.java", "diffHunk": "@@ -250,32 +259,39 @@ public String toString() {\n       if (spec.fields().isEmpty()) {\n         return new Unpartitioned24Writer(spec, format, appenderFactory, fileFactory, io.value(), targetFileSize);\n       } else {\n-        return new Partitioned24Writer(\n-            spec, format, appenderFactory, fileFactory, io.value(), targetFileSize, writeSchema);\n+        return new Partitioned24Writer(spec, format, appenderFactory, fileFactory, io.value(),\n+            targetFileSize, writeSchema, dsSchema);\n       }\n     }\n   }\n \n-  private static class Unpartitioned24Writer extends UnpartitionedWriter implements DataWriter<InternalRow> {\n+  private static class Unpartitioned24Writer extends UnpartitionedWriter<InternalRow>\n+      implements DataWriter<InternalRow> {\n     Unpartitioned24Writer(PartitionSpec spec, FileFormat format, SparkAppenderFactory appenderFactory,\n                           OutputFileFactory fileFactory, FileIO fileIo, long targetFileSize) {\n       super(spec, format, appenderFactory, fileFactory, fileIo, targetFileSize);\n     }\n \n     @Override\n     public WriterCommitMessage commit() throws IOException {\n+      this.close();\n+\n       return new TaskCommit(complete());\n     }\n   }\n \n-  private static class Partitioned24Writer extends PartitionedWriter implements DataWriter<InternalRow> {\n+  private static class Partitioned24Writer extends SparkPartitionedWriter implements DataWriter<InternalRow> {\n+\n     Partitioned24Writer(PartitionSpec spec, FileFormat format, SparkAppenderFactory appenderFactory,\n-                               OutputFileFactory fileFactory, FileIO fileIo, long targetFileSize, Schema writeSchema) {\n-      super(spec, format, appenderFactory, fileFactory, fileIo, targetFileSize, writeSchema);\n+                        OutputFileFactory fileFactory, FileIO fileIo, long targetFileSize,\n+                        Schema schema, StructType sparkSchema) {\n+      super(spec, format, appenderFactory, fileFactory, fileIo, targetFileSize, schema, sparkSchema);\n     }\n \n     @Override\n     public WriterCommitMessage commit() throws IOException {\n+      this.close();", "originalCommit": "7135c10f2caff4dc5b0de95c9928f68cdfa94f75", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0Njk0Nw==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r463346947", "bodyText": "Same here, this class should use an Array of data files.", "author": "rdblue", "createdAt": "2020-07-31T00:45:16Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkBatchWrite.java", "diffHunk": "@@ -226,9 +229,15 @@ public String toString() {\n     return String.format(\"IcebergWrite(table=%s, format=%s)\", table, format);\n   }\n \n-  public static class TaskCommit extends TaskResult implements WriterCommitMessage {\n-    TaskCommit(TaskResult result) {\n-      super(result.files());\n+  public static class TaskCommit implements WriterCommitMessage {\n+    private final List<DataFile> taskFiles;", "originalCommit": "7135c10f2caff4dc5b0de95c9928f68cdfa94f75", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0NzkxMQ==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r463347911", "bodyText": "Is this method needed? Why not merge it with close?", "author": "rdblue", "createdAt": "2020-07-31T00:48:22Z", "path": "core/src/main/java/org/apache/iceberg/io/UnpartitionedWriter.java", "diffHunk": "@@ -17,24 +17,42 @@\n  * under the License.\n  */\n \n-package org.apache.iceberg.spark.source;\n+package org.apache.iceberg.io;\n \n import java.io.IOException;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.PartitionSpec;\n-import org.apache.iceberg.io.FileIO;\n-import org.apache.spark.sql.catalyst.InternalRow;\n \n-class UnpartitionedWriter extends BaseWriter {\n-  UnpartitionedWriter(PartitionSpec spec, FileFormat format, SparkAppenderFactory appenderFactory,\n-                      OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n+public class UnpartitionedWriter<T> extends BaseTaskWriter<T> {\n+\n+  private RollingFileWriter currentWriter = null;\n+\n+  public UnpartitionedWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                             OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n     super(spec, format, appenderFactory, fileFactory, io, targetFileSize);\n+  }\n \n-    openCurrent();\n+  @Override\n+  public void write(T record) throws IOException {\n+    if (currentWriter == null) {\n+      currentWriter = new RollingFileWriter(null);\n+    }\n+    currentWriter.add(record);\n   }\n \n   @Override\n-  public void write(InternalRow row) throws IOException {\n-    writeInternal(row);\n+  public void close() throws IOException {\n+    closeCurrent();\n+  }\n+\n+  private void closeCurrent() throws IOException {", "originalCommit": "7135c10f2caff4dc5b0de95c9928f68cdfa94f75", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ0MzY4OA==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r463443688", "bodyText": "Yeah,  its logics could be just moved to close().", "author": "openinx", "createdAt": "2020-07-31T07:12:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0NzkxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MTk4OA==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r463351988", "bodyText": "I think this PR should not change writers to be lazily created.\nFirst, it changes the assumptions in the writers, which doesn't make sense to include in what is primarily a refactor.\nSecond, I think those assumptions were a better structure for these classes. Opening the file in the constructor and relying on it always being there avoids a null check in write, which is called in a tight loop. The main benefit of this is to avoid a delete in close when no records were written, but that check is still present in RollingFileWriter. And I think that check should be there because it is another helpful invariant: if a 0-record file is produced by any writer wrapped by RollingFileWriter, then it should be discarded. That helps avoid the problem in future implementations, which may not consider the case.\nThis is fairly minor, but since there are other changes needed (in particular, the array fix for task commit messages), I'd like to change at least the Spark writers back to eagerly creating output files instead of lazily checking for null in write.", "author": "rdblue", "createdAt": "2020-07-31T01:03:12Z", "path": "core/src/main/java/org/apache/iceberg/io/UnpartitionedWriter.java", "diffHunk": "@@ -17,24 +17,42 @@\n  * under the License.\n  */\n \n-package org.apache.iceberg.spark.source;\n+package org.apache.iceberg.io;\n \n import java.io.IOException;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.PartitionSpec;\n-import org.apache.iceberg.io.FileIO;\n-import org.apache.spark.sql.catalyst.InternalRow;\n \n-class UnpartitionedWriter extends BaseWriter {\n-  UnpartitionedWriter(PartitionSpec spec, FileFormat format, SparkAppenderFactory appenderFactory,\n-                      OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n+public class UnpartitionedWriter<T> extends BaseTaskWriter<T> {\n+\n+  private RollingFileWriter currentWriter = null;\n+\n+  public UnpartitionedWriter(PartitionSpec spec, FileFormat format, FileAppenderFactory<T> appenderFactory,\n+                             OutputFileFactory fileFactory, FileIO io, long targetFileSize) {\n     super(spec, format, appenderFactory, fileFactory, io, targetFileSize);\n+  }\n \n-    openCurrent();\n+  @Override\n+  public void write(T record) throws IOException {\n+    if (currentWriter == null) {\n+      currentWriter = new RollingFileWriter(null);\n+    }", "originalCommit": "7135c10f2caff4dc5b0de95c9928f68cdfa94f75", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ2NjY1NQ==", "url": "https://github.com/apache/iceberg/pull/1213#discussion_r463466655", "bodyText": "Fine, that sounds reasonable.", "author": "openinx", "createdAt": "2020-07-31T08:07:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM1MTk4OA=="}], "type": "inlineReview"}, {"oid": "e7bc4e6dd913cdcfdd43bcb445b0875509bc23a2", "url": "https://github.com/apache/iceberg/commit/e7bc4e6dd913cdcfdd43bcb445b0875509bc23a2", "message": "Abstract the generic task writers for sharing the common codes between spark and flink.", "committedDate": "2020-07-31T06:43:35Z", "type": "commit"}, {"oid": "5f1b29ee019e7379e95671099c41209aab14dd2a", "url": "https://github.com/apache/iceberg/commit/5f1b29ee019e7379e95671099c41209aab14dd2a", "message": "Minor fixes", "committedDate": "2020-07-31T06:43:35Z", "type": "commit"}, {"oid": "14e4dc5ef381213d417c2a68273584e68f51a931", "url": "https://github.com/apache/iceberg/commit/14e4dc5ef381213d417c2a68273584e68f51a931", "message": "Add flink task writers and unit tests.", "committedDate": "2020-07-31T06:43:35Z", "type": "commit"}, {"oid": "f1ccdfd691eb3062ba66da6ea21470eb37f65c16", "url": "https://github.com/apache/iceberg/commit/f1ccdfd691eb3062ba66da6ea21470eb37f65c16", "message": "Adjust the unit tests.", "committedDate": "2020-07-31T06:43:35Z", "type": "commit"}, {"oid": "8fe27c2104d32886978b004d112927c69936eefe", "url": "https://github.com/apache/iceberg/commit/8fe27c2104d32886978b004d112927c69936eefe", "message": "Addressing the failure unit tests.", "committedDate": "2020-07-31T06:43:35Z", "type": "commit"}, {"oid": "a586c5112f348fae3de210dcc2b6e6a8ce47e6e3", "url": "https://github.com/apache/iceberg/commit/a586c5112f348fae3de210dcc2b6e6a8ce47e6e3", "message": "Add unit test and more javadoc", "committedDate": "2020-07-31T06:43:35Z", "type": "commit"}, {"oid": "93a9318e9f53849fa8095c331434c3cbfabde583", "url": "https://github.com/apache/iceberg/commit/93a9318e9f53849fa8095c331434c3cbfabde583", "message": "Fix the broken TestRewriteDataFilesAction", "committedDate": "2020-07-31T06:43:35Z", "type": "commit"}, {"oid": "bb69950dfd0d4456ca74b78426f5aa05d4cf2dc3", "url": "https://github.com/apache/iceberg/commit/bb69950dfd0d4456ca74b78426f5aa05d4cf2dc3", "message": "Add javadoc for FileAppenderFactory", "committedDate": "2020-07-31T06:43:35Z", "type": "commit"}, {"oid": "abb0b5d6a7608cc9cd5ed8e9e1ad289c5cdacd06", "url": "https://github.com/apache/iceberg/commit/abb0b5d6a7608cc9cd5ed8e9e1ad289c5cdacd06", "message": "More unit tests", "committedDate": "2020-07-31T06:43:35Z", "type": "commit"}, {"oid": "9f1fa70d55eb563199395ebc0274a290d5713c39", "url": "https://github.com/apache/iceberg/commit/9f1fa70d55eb563199395ebc0274a290d5713c39", "message": "Addressing the comment.", "committedDate": "2020-07-31T06:44:20Z", "type": "commit"}, {"oid": "499a48f30667b9f3b00fea9de01421bbba78a4a0", "url": "https://github.com/apache/iceberg/commit/499a48f30667b9f3b00fea9de01421bbba78a4a0", "message": "Remove the keyGetter and use the abstract partitioned(..) method.", "committedDate": "2020-07-31T06:45:05Z", "type": "commit"}, {"oid": "21be53a686aab6c6be54334a7598377d148b4856", "url": "https://github.com/apache/iceberg/commit/21be53a686aab6c6be54334a7598377d148b4856", "message": "Make the few public classes/methods to be package-access", "committedDate": "2020-07-31T06:45:05Z", "type": "commit"}, {"oid": "61dde9b99619e1de92de96358ae39239c5416179", "url": "https://github.com/apache/iceberg/commit/61dde9b99619e1de92de96358ae39239c5416179", "message": "Addressing the comments", "committedDate": "2020-07-31T06:45:05Z", "type": "commit"}, {"oid": "8a0920796a1958c9d92e8f564b60d171c5007786", "url": "https://github.com/apache/iceberg/commit/8a0920796a1958c9d92e8f564b60d171c5007786", "message": "Remove the public modifiers from PartitionedFanoutWriter", "committedDate": "2020-07-31T06:46:07Z", "type": "commit"}, {"oid": "0c7423aed5800862f2830d290b204e099926b1f8", "url": "https://github.com/apache/iceberg/commit/0c7423aed5800862f2830d290b204e099926b1f8", "message": "Addressing the lastest comment from Ryan Blue.", "committedDate": "2020-07-31T08:04:02Z", "type": "commit"}, {"oid": "0c7423aed5800862f2830d290b204e099926b1f8", "url": "https://github.com/apache/iceberg/commit/0c7423aed5800862f2830d290b204e099926b1f8", "message": "Addressing the lastest comment from Ryan Blue.", "committedDate": "2020-07-31T08:04:02Z", "type": "forcePushed"}, {"oid": "e6152d19453d144bf8a7bade90ede997537c5eda", "url": "https://github.com/apache/iceberg/commit/e6152d19453d144bf8a7bade90ede997537c5eda", "message": "Fix NullPointerException in PartitionedWriter", "committedDate": "2020-07-31T18:38:37Z", "type": "commit"}]}