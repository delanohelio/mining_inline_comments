{"pr_number": 1618, "pr_title": "dynamically load custom FileIO implementation", "pr_createdAt": "2020-10-15T23:39:56Z", "pr_url": "https://github.com/apache/iceberg/pull/1618", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTk2NzgyNg==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r505967826", "bodyText": "I don't think this is actually cleaner than just using the reflection utilities here. I understand wanting to reuse code, but I don't think that this operation is a good candidate for it.\nThis mixes property checking with the reflect operation, and as a result the default implementation is now loaded using reflection as well. That means that we lose the compile time check that we're instantiating HadoopFileIO correctly and lose the ability to easily refactor.\nThis also doesn't support more than one constructor. For LocationProvider, the constructor could accept either location and properties or no args, but this only checks for a single constructor and is harder to read as well.\nThe first thing I would change is to move the property check out of the utility method and directly instantiate HadoopFileIO in an else block. After making that change, there's little value in wrapping the reflection helpers. The main complexity is catching NoSuchMethodException to return a better error message (which is optional), but making that generic ends up being quite complex:\n      List<List<String>> allowedArgTypesString = allowedArgTypes.stream()\n          .map(argTypes -> Arrays.stream(argTypes).map(Class::getName).collect(Collectors.toList()))\n          .collect(Collectors.toList());\n      throw new IllegalArgumentException(String.format(\n          \"Unable to find a constructor for implementation %s of %s. \" +\n              \"Make sure the implementation is in classpath, and that it either \" +\n              \"has a public no-arg constructor or one of the following constructors: %s \",\n          impl, classInterface, allowedArgTypesString), e);", "author": "rdblue", "createdAt": "2020-10-16T01:33:43Z", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java", "diffHunk": "@@ -174,7 +176,13 @@ public void commit(TableMetadata base, TableMetadata metadata) {\n   @Override\n   public FileIO io() {\n     if (defaultFileIo == null) {\n-      defaultFileIo = new HadoopFileIO(conf);\n+      defaultFileIo = ClassLoaderUtil.fromProperty(\n+          current() == null ? new HashMap<>() : current().properties(),\n+          TableProperties.WRITE_FILE_IO_IMPL,\n+          HadoopFileIO.class.getName(),\n+          HadoopFileIO.class,\n+          new Class<?>[] { Configuration.class },\n+          new Object[]{ conf });", "originalCommit": "96e58f38de39c2291e5f68005534acd4b840fbae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTk4MDY5NQ==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r505980695", "bodyText": "The method supports no arg constructor, and also allows using multiple constructor options. The main reason for refactoring is because it has 2 lengthy try catch blocks for NoSuchMethodException when looking for a constructor, and also a ClassCastException when trying to cast the class.\nBut I agree that it loses the ability to check default implementation at compile time. What if I leave the default class construction in the else blocks, and keep the util to only initialize the dynamic class?", "author": "jackye1995", "createdAt": "2020-10-16T02:00:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTk2NzgyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzkwMzYwMQ==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r507903601", "bodyText": "If you left the property handling and default classes to else blocks, then the only value that is left is having one method with blocks to throw a little more friendly exception messages. But the cost of that is that both the call and the implementation are quite a bit more complicated and harder to read. I don't really think it is worth the refactor.", "author": "rdblue", "createdAt": "2020-10-19T16:51:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTk2NzgyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTk2ODE4NQ==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r505968185", "bodyText": "The interface should be FileIO instead of HadoopFileIO.", "author": "rdblue", "createdAt": "2020-10-16T01:34:22Z", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java", "diffHunk": "@@ -174,7 +176,13 @@ public void commit(TableMetadata base, TableMetadata metadata) {\n   @Override\n   public FileIO io() {\n     if (defaultFileIo == null) {\n-      defaultFileIo = new HadoopFileIO(conf);\n+      defaultFileIo = ClassLoaderUtil.fromProperty(\n+          current() == null ? new HashMap<>() : current().properties(),\n+          TableProperties.WRITE_FILE_IO_IMPL,\n+          HadoopFileIO.class.getName(),\n+          HadoopFileIO.class,", "originalCommit": "96e58f38de39c2291e5f68005534acd4b840fbae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTk4MTQ2NA==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r505981464", "bodyText": "It's HadoopFileIO in the original class. Is it intended or a bug?\nhttps://github.com/apache/iceberg/blob/master/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java#L65", "author": "jackye1995", "createdAt": "2020-10-16T02:02:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTk2ODE4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzkwNDc5OA==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r507904798", "bodyText": "Since the same concrete class was always instantiated, I think we used the class in case we needed anything specific to HadoopFileIO. But it can be FileIO since nothing specific is used.", "author": "rdblue", "createdAt": "2020-10-19T16:52:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTk2ODE4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTk2ODcwOA==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r505968708", "bodyText": "We should move instantiation to a separate method that accepts TableMetadata so that we can use the correct metadata in transactions.", "author": "rdblue", "createdAt": "2020-10-16T01:35:13Z", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java", "diffHunk": "@@ -174,7 +176,13 @@ public void commit(TableMetadata base, TableMetadata metadata) {\n   @Override\n   public FileIO io() {\n     if (defaultFileIo == null) {\n-      defaultFileIo = new HadoopFileIO(conf);\n+      defaultFileIo = ClassLoaderUtil.fromProperty(\n+          current() == null ? new HashMap<>() : current().properties(),", "originalCommit": "96e58f38de39c2291e5f68005534acd4b840fbae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTk2OTQ5MQ==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r505969491", "bodyText": "There's no need to make this public, and I'd like to avoid exposing new classes through the public API. If we need to use reflection, we can always use hiddenImpl to set the accessible flag. But I think it would be better to instantiate these classes directly like before so that we have compile checks.", "author": "rdblue", "createdAt": "2020-10-16T01:36:41Z", "path": "core/src/main/java/org/apache/iceberg/LocationProviders.java", "diffHunk": "@@ -21,57 +21,43 @@\n \n import java.util.Map;\n import org.apache.hadoop.fs.Path;\n-import org.apache.iceberg.common.DynConstructors;\n import org.apache.iceberg.io.LocationProvider;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.transforms.Transform;\n import org.apache.iceberg.transforms.Transforms;\n import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ClassLoaderUtil;\n import org.apache.iceberg.util.PropertyUtil;\n \n import static org.apache.iceberg.TableProperties.OBJECT_STORE_PATH;\n \n public class LocationProviders {\n \n+  private static final Class<?>[] CONSTRUCTOR_ARG_TYPES = { String.class, Map.class };\n+\n   private LocationProviders() {\n   }\n \n   public static LocationProvider locationsFor(String location, Map<String, String> properties) {\n-    if (properties.containsKey(TableProperties.WRITE_LOCATION_PROVIDER_IMPL)) {\n-      String impl = properties.get(TableProperties.WRITE_LOCATION_PROVIDER_IMPL);\n-      DynConstructors.Ctor<LocationProvider> ctor;\n-      try {\n-        ctor = DynConstructors.builder(LocationProvider.class)\n-            .impl(impl, String.class, Map.class)\n-            .impl(impl).buildChecked(); // fall back to no-arg constructor\n-      } catch (NoSuchMethodException e) {\n-        throw new IllegalArgumentException(String.format(\n-            \"Unable to find a constructor for implementation %s of %s. \" +\n-                \"Make sure the implementation is in classpath, and that it either \" +\n-                \"has a public no-arg constructor or a two-arg constructor \" +\n-                \"taking in the string base table location and its property string map.\",\n-            impl, LocationProvider.class), e);\n-      }\n-      try {\n-        return ctor.newInstance(location, properties);\n-      } catch (ClassCastException e) {\n-        throw new IllegalArgumentException(\n-            String.format(\"Provided implementation for dynamic instantiation should implement %s.\",\n-                LocationProvider.class), e);\n-      }\n-    } else if (PropertyUtil.propertyAsBoolean(properties,\n+    // for backwards compatibility of write.object-storage.enabled property\n+    Class<?> defaultProviderClass = PropertyUtil.propertyAsBoolean(properties,\n         TableProperties.OBJECT_STORE_ENABLED,\n-        TableProperties.OBJECT_STORE_ENABLED_DEFAULT)) {\n-      return new ObjectStoreLocationProvider(location, properties);\n-    } else {\n-      return new DefaultLocationProvider(location, properties);\n-    }\n+        TableProperties.OBJECT_STORE_ENABLED_DEFAULT) ?\n+        ObjectStoreLocationProvider.class : DefaultLocationProvider.class;\n+\n+    return ClassLoaderUtil.fromProperty(\n+        properties,\n+        TableProperties.WRITE_LOCATION_PROVIDER_IMPL,\n+        defaultProviderClass.getName(),\n+        LocationProvider.class,\n+        CONSTRUCTOR_ARG_TYPES,\n+        new Object[]{location, properties});\n   }\n \n-  static class DefaultLocationProvider implements LocationProvider {\n+  public static class DefaultLocationProvider implements LocationProvider {", "originalCommit": "96e58f38de39c2291e5f68005534acd4b840fbae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ3MDA4NA==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r509470084", "bodyText": "Nit: this is a non-functional change that could cause unnecessary commit conflicts.", "author": "rdblue", "createdAt": "2020-10-21T17:22:43Z", "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -108,9 +108,9 @@ protected String tableName() {\n   @Override\n   public FileIO io() {\n     if (fileIO == null) {\n-      fileIO = new HadoopFileIO(conf);\n+      // avoid refresh metadata because refresh() calls io() again\n+      fileIO = CatalogUtil.loadFileIO(getCurrentMetadataNoRefresh(), conf);\n     }\n-", "originalCommit": "d8c159d4f42953a66a18522137e67cf146a99f00", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTUxNjE5Mw==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r509516193", "bodyText": "Looks like we only need properties here. I think that we should probably pass a map of properties to this method instead. That way changes to table metadata won't need to affect this class.", "author": "rdblue", "createdAt": "2020-10-21T17:55:32Z", "path": "core/src/main/java/org/apache/iceberg/CatalogUtil.java", "diffHunk": "@@ -117,4 +120,42 @@ private static void deleteFiles(FileIO io, Set<ManifestFile> allManifests) {\n           }\n         });\n   }\n+\n+  /**\n+   * Load FileIO, default to HadoopFileIO\n+   * @param tableMetadata table metadata\n+   * @param conf Hadoop configuration\n+   * @return FileIO class\n+   * @throws IllegalArgumentException if class path not found or\n+   *  right constructor not found or\n+   *  the loaded class cannot be casted to the given interface type\n+   */\n+  public static FileIO loadFileIO(TableMetadata tableMetadata, Configuration conf) {\n+    if (tableMetadata != null) {\n+      Map<String, String> properties = tableMetadata.properties();", "originalCommit": "fbf9fd40c7d219f4f24bb11bac56eebcfc0c8ac0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTUxOTIyNg==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r509519226", "bodyText": "I think it would make sense to add an alternative constructor that accepts Map to pass the table properties. We can do this in a follow-up when it's needed, or here if you agree.", "author": "rdblue", "createdAt": "2020-10-21T17:56:54Z", "path": "core/src/main/java/org/apache/iceberg/CatalogUtil.java", "diffHunk": "@@ -117,4 +120,42 @@ private static void deleteFiles(FileIO io, Set<ManifestFile> allManifests) {\n           }\n         });\n   }\n+\n+  /**\n+   * Load FileIO, default to HadoopFileIO\n+   * @param tableMetadata table metadata\n+   * @param conf Hadoop configuration\n+   * @return FileIO class\n+   * @throws IllegalArgumentException if class path not found or\n+   *  right constructor not found or\n+   *  the loaded class cannot be casted to the given interface type\n+   */\n+  public static FileIO loadFileIO(TableMetadata tableMetadata, Configuration conf) {\n+    if (tableMetadata != null) {\n+      Map<String, String> properties = tableMetadata.properties();\n+      if (properties.containsKey(TableProperties.WRITE_FILE_IO_IMPL)) {\n+        String impl = properties.get(TableProperties.WRITE_FILE_IO_IMPL);\n+        LOG.info(\"Loading custom FileIO implementation: {}\", impl);\n+        DynConstructors.Ctor<FileIO> ctor;\n+        try {\n+          ctor = DynConstructors.builder(FileIO.class)\n+              .impl(impl, Configuration.class)", "originalCommit": "fbf9fd40c7d219f4f24bb11bac56eebcfc0c8ac0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTUyMTAwOQ==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r509521009", "bodyText": "Style: does this need to be on a separate line?", "author": "rdblue", "createdAt": "2020-10-21T17:57:41Z", "path": "core/src/main/java/org/apache/iceberg/CatalogUtil.java", "diffHunk": "@@ -117,4 +120,42 @@ private static void deleteFiles(FileIO io, Set<ManifestFile> allManifests) {\n           }\n         });\n   }\n+\n+  /**\n+   * Load FileIO, default to HadoopFileIO\n+   * @param tableMetadata table metadata\n+   * @param conf Hadoop configuration\n+   * @return FileIO class\n+   * @throws IllegalArgumentException if class path not found or\n+   *  right constructor not found or\n+   *  the loaded class cannot be casted to the given interface type\n+   */\n+  public static FileIO loadFileIO(TableMetadata tableMetadata, Configuration conf) {\n+    if (tableMetadata != null) {\n+      Map<String, String> properties = tableMetadata.properties();\n+      if (properties.containsKey(TableProperties.WRITE_FILE_IO_IMPL)) {\n+        String impl = properties.get(TableProperties.WRITE_FILE_IO_IMPL);\n+        LOG.info(\"Loading custom FileIO implementation: {}\", impl);\n+        DynConstructors.Ctor<FileIO> ctor;\n+        try {\n+          ctor = DynConstructors.builder(FileIO.class)\n+              .impl(impl, Configuration.class)\n+              .impl(impl) // fall back to no-arg constructor\n+              .buildChecked();\n+        } catch (NoSuchMethodException e) {\n+          throw new IllegalArgumentException(String.format(\n+              \"Cannot initialize FiloIO, please make sure %s has a constructor taking %s, or a no-arg constructor\",\n+              impl, Configuration.class), e);\n+        }\n+        try {\n+          return ctor.newInstance(conf);\n+        } catch (ClassCastException e) {\n+          throw new IllegalArgumentException(\n+              String.format(\"Cannot initialize FileIO, fail to cast %s to class %s.\",\n+                  impl, FileIO.class), e);", "originalCommit": "fbf9fd40c7d219f4f24bb11bac56eebcfc0c8ac0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTUyODM3OA==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r509528378", "bodyText": "This isn't just a write option. It controls how files are read as well.\nHow about using an io namespace instead? Maybe io.file-io.impl or io.impl?", "author": "rdblue", "createdAt": "2020-10-21T18:01:58Z", "path": "core/src/main/java/org/apache/iceberg/TableProperties.java", "diffHunk": "@@ -91,6 +91,8 @@ private TableProperties() {\n \n   public static final String WRITE_LOCATION_PROVIDER_IMPL = \"write.location-provider.impl\";\n \n+  public static final String WRITE_FILE_IO_IMPL = \"write.file-io.impl\";", "originalCommit": "fbf9fd40c7d219f4f24bb11bac56eebcfc0c8ac0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU2Njc3Mg==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r509566772", "bodyText": "Yeah I used write because there is no namespace for both read and write. I can change to io.file-io.impl, io.impl seems too generic in case we want to have something else in the same namespace in the future.", "author": "jackye1995", "createdAt": "2020-10-21T18:40:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTUyODM3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU5ODk2Ng==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r509598966", "bodyText": "What do you think about moving this to the base class? Then we wouldn't need to add the \"no refresh\" method to get metadata and could access current directly. This PR also makes it far less likely that implementations will override io, since a new one can be plugged in easily through table properties.", "author": "rdblue", "createdAt": "2020-10-21T19:10:53Z", "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -108,7 +108,8 @@ protected String tableName() {\n   @Override\n   public FileIO io() {\n     if (fileIO == null) {\n-      fileIO = new HadoopFileIO(conf);\n+      // avoid refresh metadata because refresh() calls io() again\n+      fileIO = CatalogUtil.loadFileIO(getCurrentMetadataNoRefresh(), conf);", "originalCommit": "fbf9fd40c7d219f4f24bb11bac56eebcfc0c8ac0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTY1OTcyNA==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r509659724", "bodyText": "I thought about doing that, but (1) if it is put to base class, the logic still needs to be duplicated, because HadoopTableOperations does not extend that base class; (2) the method needs to take the Hadoop configuration object, and it seems like the base class tries to not assume an implementation must use Hadoop configuration.", "author": "jackye1995", "createdAt": "2020-10-21T20:19:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU5ODk2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTc5Nzk3Nw==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r509797977", "bodyText": "Good point about Configuration. We don't want to rely on it in the base class. Let me take a closer look at this problem, it's concerning that there's a loop.", "author": "rdblue", "createdAt": "2020-10-21T23:46:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU5ODk2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTgwMDIxNg==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r509800216", "bodyText": "Yeah I am also just thinking about the loop issue. If someone tries to create a new table, at that point of time currentMetadata is null, so it will choose use the default FileIO to write the initial metadata, and the new FileIO might not be able to read it, which would be a problem.\nThat means when creating the table it needs to check if the io.file-io.impl is set in the properties and update the FileIO implementation accordingly in the doCommit method. But it feels like an ugly hack...", "author": "jackye1995", "createdAt": "2020-10-21T23:53:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU5ODk2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTgwNzEzMw==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r509807133", "bodyText": "Yeah, and we have to instantiate FileIO before we load the metadata for a table, so we don't really know if we're going to get it right. Seems like we are trying to configure this in the wrong place. Let's talk about where this should go in the sync today.", "author": "rdblue", "createdAt": "2020-10-22T00:16:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU5ODk2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDU0MzMyOA==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r510543328", "bodyText": "Because Iceberg catalog does not enforce the implementation detail of how table properties are store, an implementation can potentially write the table properties using the io(). Therefore if we try to determine the io() to use based on table property, there is always a chance for cyclic dependency in logic. HadoopCatalog has exactly this problem.\nHere are 3 potential ways to solve this problem:\n\n\nCreate a new constructor for each TableOperation that accepts FileIO as an argument. If the constructor is used, then FileIO is set at construction time instead of the first time io() is called. A default implementation can be loaded based on namespace properties. Engines like Spark and Flink can load a configuration key from Hadoop config to load the FileIO outside the core logic. This requires code change at multiple places, including (1) add new constructors in existing table operations, and (2) add support in each engine separately.\n\n\nDetermine FileIO based on the warehouse path scheme. For example, s3:// always use S3FileIO, hdfs:// always use HadoopFileIO. However, it is easy to create a cyclic dependency issue, for example: iceberg-aws module depends on iceberg-core, so HadoopCatalog in iceberg-core cannot import S3FileIO in iceberg-aws. This means we need a registry that allows people to register custom IO mapping at runtime. This approach has a similar amount of work as approach 1, because we need code change in each existing catalog, and each engine needs to find a way to register FIleIO implementation to scheme mapping at JVM start time.\n\n\nLoad FileIO using Hadoop configuration. Because HadoopFileIO is always the default implementation, Hadoop configuration object is always passed in. So user can always just define custom implementation at core-site.xml. This creates a simple solution with no dependency concern. However, this is not an elegant option because ideally we would like to load all Iceberg classes using Iceberg-based configs such as table properties.", "author": "jackye1995", "createdAt": "2020-10-23T01:18:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU5ODk2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTA5NDg1Nw==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r511094857", "bodyText": "One quick thing to note: we want to avoid increasing dependence on a Hadoop configuration. It's fine to pass one where it is required, but we should always make sure there is an alternative and should generally avoid using config from it.\nThanks for writing up the options. Sounds like we have options for configuring a FileIO:\n\nPass FileIO in from the catalog and use catalog config to initialize it\nInstantiate FileIO based on table location or metadata path just before using it\nUse config from the environment, like a Hadoop FileSystem\n\nI think the best choice is #1, catalog-level configuration.\nWe can't use table-level config because that creates a situation where it isn't available to load a table. Working around this requires deciding which FileIO to load based on different information at different times (location for create, metadata location for load), and would also make supplying a custom FileIO implementation in configuration difficult.\nEnvironment-level config doesn't fit with the current model for customizing behavior, where everything is injected through Catalog and Table. Plus, Iceberg has no environment config like Hadoop Configuration and I don't think it is a good idea to add it. I think the most sensible thing is to maintain the chain of configuration: application passes config to catalogs, catalogs pass config to tables.\nI don't think it would be too difficult to change over to FileIO passed in from the catalog, but this would mean not basing the implementation on table path. We wouldn't know to choose between S3FileIO or HadoopFileIO for a table and would have to use one for all tables from a catalog, or build a FileIO that knows when to choose between the two. I was already thinking that we would need a delegating implementation, since S3FileIO can't handle non-S3 paths. That should be okay.", "author": "rdblue", "createdAt": "2020-10-23T19:14:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU5ODk2Ng=="}], "type": "inlineReview"}, {"oid": "1e574efd653ae3e73d78e039e1ea3fc8bf5dd3d4", "url": "https://github.com/apache/iceberg/commit/1e574efd653ae3e73d78e039e1ea3fc8bf5dd3d4", "message": "add fileIO config as a part of constructor", "committedDate": "2020-10-30T01:28:39Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2NTI4MA==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r515665280", "bodyText": "Why do we need a Hadoop config property?", "author": "rdblue", "createdAt": "2020-11-01T20:02:47Z", "path": "core/src/main/java/org/apache/iceberg/hadoop/ConfigProperties.java", "diffHunk": "@@ -25,4 +25,6 @@ private ConfigProperties() {\n   }\n \n   public static final String ENGINE_HIVE_ENABLED = \"iceberg.engine.hive.enabled\";\n+\n+  public static final String CUSTOM_FILE_IO_IMPL = \"iceberg.fileio.impl\";", "originalCommit": "f17b993b8ed75a05cca78b88d3a21b0a22c3b94f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2NjgyOA==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r515666828", "bodyText": "I see this is used to get the implementation class. Iceberg should never use a Hadoop Configuration for config, except when integrating with an engine that uses it for config. It's okay to store configuration for Hive, but not for Iceberg core.\nFor FileIO implementation, I think the config should come from the catalog property map.", "author": "rdblue", "createdAt": "2020-11-01T20:17:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2NTI4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY3OTI2Mw==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r515679263", "bodyText": "Thank you for the comments! I will fix the other parts accordingly. I think this is the place I do not fully understand. Is there a catalog property map? I don't see such a thing in the catalog interface, and that is why I have to use the Hadoop config.", "author": "jackye1995", "createdAt": "2020-11-01T22:14:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2NTI4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE0ODM0Mg==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r516148342", "bodyText": "There are config properties from both Flink and Spark that we use when constructing the Hive and Hadoop catalogs. And for #1640, we are talking about using an initialization method to set the config using a string map (and a catalog name). While these classes don't pull their own config out of a map, I think of the config as coming from a map of config properties.\nSorry for the confusion. I think we should configure this however the catalog is configured, which will very likely be a string map passed to an init method in the future.", "author": "rdblue", "createdAt": "2020-11-02T17:42:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2NTI4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2NTUxMQ==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r515665511", "bodyText": "I think that the IO implementation should be passed in instead of the class name. HadoopFileIO only needs to be instantiated once per catalog and can be shared. Passing the instance from a Catalog into TableOperations allows the catalog implementation to choose whether to instantiate FileIO per table or not.", "author": "rdblue", "createdAt": "2020-11-01T20:04:54Z", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java", "diffHunk": "@@ -163,7 +163,10 @@ protected boolean isValidIdentifier(TableIdentifier identifier) {\n \n   @Override\n   protected TableOperations newTableOps(TableIdentifier identifier) {\n-    return new HadoopTableOperations(new Path(defaultWarehouseLocation(identifier)), conf);\n+    return new HadoopTableOperations(\n+        new Path(defaultWarehouseLocation(identifier)),\n+        conf.get(ConfigProperties.CUSTOM_FILE_IO_IMPL),", "originalCommit": "f17b993b8ed75a05cca78b88d3a21b0a22c3b94f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2NTY2Mw==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r515665663", "bodyText": "Style: We like to have empty lines after control flow blocks like for, while, if, else, try, and catch. It helps keep the code at a readable density.", "author": "rdblue", "createdAt": "2020-11-01T20:06:10Z", "path": "core/src/main/java/org/apache/iceberg/CatalogUtil.java", "diffHunk": "@@ -117,4 +120,39 @@ private static void deleteFiles(FileIO io, Set<ManifestFile> allManifests) {\n           }\n         });\n   }\n+\n+  /**\n+   * Load a custom {@link FileIO} implementation.\n+   * The implementation must have a no-arg constructor.\n+   * If the class implements {@link org.apache.hadoop.conf.Configurable}, we will also set the Hadoop configuration.\n+   * @param impl full class name of a custom FileIO implementation\n+   * @param hadoopConf hadoop configuration\n+   * @return FileIO class\n+   * @throws IllegalArgumentException if class path not found or\n+   *  right constructor not found or\n+   *  the loaded class cannot be casted to the given interface type\n+   */\n+  public static FileIO loadCustomFileIO(String impl, Configuration hadoopConf) {\n+    LOG.info(\"Loading custom FileIO implementation: {}\", impl);\n+    DynConstructors.Ctor<FileIO> ctor;\n+    try {\n+      ctor = DynConstructors.builder(FileIO.class)\n+          .impl(impl)\n+          .buildChecked();\n+    } catch (NoSuchMethodException e) {\n+      throw new IllegalArgumentException(String.format(\n+          \"Cannot initialize FiloIO, please make sure %s has a no-arg constructor\", impl), e);\n+    }\n+    try {", "originalCommit": "f17b993b8ed75a05cca78b88d3a21b0a22c3b94f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2NTk2Mg==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r515665962", "bodyText": "Javadoc paragraphs require <p> between them or else the docs are all one paragraph. Also, it is more clear and direct not to refer to the library maintainers with \"we\", like this: If the class implements {@link Configurable}, a Hadoop config will be passed using {@link Configurable#setConf(Configuration)}.", "author": "rdblue", "createdAt": "2020-11-01T20:09:21Z", "path": "core/src/main/java/org/apache/iceberg/CatalogUtil.java", "diffHunk": "@@ -117,4 +120,39 @@ private static void deleteFiles(FileIO io, Set<ManifestFile> allManifests) {\n           }\n         });\n   }\n+\n+  /**\n+   * Load a custom {@link FileIO} implementation.\n+   * The implementation must have a no-arg constructor.\n+   * If the class implements {@link org.apache.hadoop.conf.Configurable}, we will also set the Hadoop configuration.", "originalCommit": "f17b993b8ed75a05cca78b88d3a21b0a22c3b94f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2NjI4OQ==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r515666289", "bodyText": "I think it would be more clear to say %s does not implement FileIO. Passing the class calls toString, not getName, and that output includes \"interface\" or \"class\". This would actually log \"fail to cast com.example.X to class interface ...\". The problem is that com.example.X needs to implement FileIO, so saying it directly is a bit better.", "author": "rdblue", "createdAt": "2020-11-01T20:12:18Z", "path": "core/src/main/java/org/apache/iceberg/CatalogUtil.java", "diffHunk": "@@ -117,4 +120,39 @@ private static void deleteFiles(FileIO io, Set<ManifestFile> allManifests) {\n           }\n         });\n   }\n+\n+  /**\n+   * Load a custom {@link FileIO} implementation.\n+   * The implementation must have a no-arg constructor.\n+   * If the class implements {@link org.apache.hadoop.conf.Configurable}, we will also set the Hadoop configuration.\n+   * @param impl full class name of a custom FileIO implementation\n+   * @param hadoopConf hadoop configuration\n+   * @return FileIO class\n+   * @throws IllegalArgumentException if class path not found or\n+   *  right constructor not found or\n+   *  the loaded class cannot be casted to the given interface type\n+   */\n+  public static FileIO loadCustomFileIO(String impl, Configuration hadoopConf) {\n+    LOG.info(\"Loading custom FileIO implementation: {}\", impl);\n+    DynConstructors.Ctor<FileIO> ctor;\n+    try {\n+      ctor = DynConstructors.builder(FileIO.class)\n+          .impl(impl)\n+          .buildChecked();\n+    } catch (NoSuchMethodException e) {\n+      throw new IllegalArgumentException(String.format(\n+          \"Cannot initialize FiloIO, please make sure %s has a no-arg constructor\", impl), e);\n+    }\n+    try {\n+      FileIO fileIO = ctor.newInstance();\n+      if (fileIO instanceof Configurable) {\n+        ((Configurable) fileIO).setConf(hadoopConf);\n+      }\n+      return fileIO;\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot initialize FileIO, fail to cast %s to class %s.\",\n+              impl, FileIO.class), e);", "originalCommit": "f17b993b8ed75a05cca78b88d3a21b0a22c3b94f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2NjM0NQ==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r515666345", "bodyText": "I think this should not be in the try/catch block.", "author": "rdblue", "createdAt": "2020-11-01T20:12:57Z", "path": "core/src/main/java/org/apache/iceberg/CatalogUtil.java", "diffHunk": "@@ -117,4 +120,39 @@ private static void deleteFiles(FileIO io, Set<ManifestFile> allManifests) {\n           }\n         });\n   }\n+\n+  /**\n+   * Load a custom {@link FileIO} implementation.\n+   * The implementation must have a no-arg constructor.\n+   * If the class implements {@link org.apache.hadoop.conf.Configurable}, we will also set the Hadoop configuration.\n+   * @param impl full class name of a custom FileIO implementation\n+   * @param hadoopConf hadoop configuration\n+   * @return FileIO class\n+   * @throws IllegalArgumentException if class path not found or\n+   *  right constructor not found or\n+   *  the loaded class cannot be casted to the given interface type\n+   */\n+  public static FileIO loadCustomFileIO(String impl, Configuration hadoopConf) {\n+    LOG.info(\"Loading custom FileIO implementation: {}\", impl);\n+    DynConstructors.Ctor<FileIO> ctor;\n+    try {\n+      ctor = DynConstructors.builder(FileIO.class)\n+          .impl(impl)\n+          .buildChecked();\n+    } catch (NoSuchMethodException e) {\n+      throw new IllegalArgumentException(String.format(\n+          \"Cannot initialize FiloIO, please make sure %s has a no-arg constructor\", impl), e);\n+    }\n+    try {\n+      FileIO fileIO = ctor.newInstance();\n+      if (fileIO instanceof Configurable) {\n+        ((Configurable) fileIO).setConf(hadoopConf);\n+      }", "originalCommit": "f17b993b8ed75a05cca78b88d3a21b0a22c3b94f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY2NjU2Nw==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r515666567", "bodyText": "We try to be direct and omit phrases like \"please make sure\". A better error message is Cannot load FileIO, missing no-arg constructor: %s.\nAlso, typo: FiloIO should be FileIO.", "author": "rdblue", "createdAt": "2020-11-01T20:14:58Z", "path": "core/src/main/java/org/apache/iceberg/CatalogUtil.java", "diffHunk": "@@ -117,4 +120,39 @@ private static void deleteFiles(FileIO io, Set<ManifestFile> allManifests) {\n           }\n         });\n   }\n+\n+  /**\n+   * Load a custom {@link FileIO} implementation.\n+   * The implementation must have a no-arg constructor.\n+   * If the class implements {@link org.apache.hadoop.conf.Configurable}, we will also set the Hadoop configuration.\n+   * @param impl full class name of a custom FileIO implementation\n+   * @param hadoopConf hadoop configuration\n+   * @return FileIO class\n+   * @throws IllegalArgumentException if class path not found or\n+   *  right constructor not found or\n+   *  the loaded class cannot be casted to the given interface type\n+   */\n+  public static FileIO loadCustomFileIO(String impl, Configuration hadoopConf) {\n+    LOG.info(\"Loading custom FileIO implementation: {}\", impl);\n+    DynConstructors.Ctor<FileIO> ctor;\n+    try {\n+      ctor = DynConstructors.builder(FileIO.class)\n+          .impl(impl)\n+          .buildChecked();\n+    } catch (NoSuchMethodException e) {\n+      throw new IllegalArgumentException(String.format(\n+          \"Cannot initialize FiloIO, please make sure %s has a no-arg constructor\", impl), e);", "originalCommit": "f17b993b8ed75a05cca78b88d3a21b0a22c3b94f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg1MTYzMA==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r516851630", "bodyText": "Instantiating HadoopFileIO before initialize is called is because it isn't always called right now?", "author": "rdblue", "createdAt": "2020-11-03T17:51:26Z", "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java", "diffHunk": "@@ -69,6 +74,7 @@ public HiveCatalog(Configuration conf) {\n     this.conf = conf;\n     this.createStack = Thread.currentThread().getStackTrace();\n     this.closed = false;\n+    this.defaultFileIO = new HadoopFileIO(conf);", "originalCommit": "672d62f1d76974fa0f0f39ef5cce819c5defab3c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "04ce8e3f7e264e5bae7ed9bf95ba3cf4d2fc9cd4", "url": "https://github.com/apache/iceberg/commit/04ce8e3f7e264e5bae7ed9bf95ba3cf4d2fc9cd4", "message": "dynamically load custom FileIO implementation", "committedDate": "2020-11-05T05:30:18Z", "type": "forcePushed"}, {"oid": "0ac42b947f2c99922aa556ddb14163bb1d13b3de", "url": "https://github.com/apache/iceberg/commit/0ac42b947f2c99922aa556ddb14163bb1d13b3de", "message": "dynamically load custom FileIO implementation", "committedDate": "2020-11-05T07:07:06Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODE4NTY3MQ==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r518185671", "bodyText": "For Flink, I think we should pass the warehouse location and other config through properties. Right now, we pull it out in FlinkCatalogFactory, but it doesn't make much sense to pull out only some properties.\nHow about leaving the current CatalogLoader.hadoop and CatalogLoader.hive as they are and adding variants like CatalogLoader.hadoop(String name, Map<String, String> properties, Configuration conf)? Then we can pull the properties out using standard config properties that we put in CatalogProperties in the loader classes.\nFYI @JingsongLi and @openinx: we're improving how we configure catalogs and allowing some parts to be loaded dynamically. The main change here is passing properties to the catalog as a map.", "author": "rdblue", "createdAt": "2020-11-05T16:28:30Z", "path": "flink/src/main/java/org/apache/iceberg/flink/CatalogLoader.java", "diffHunk": "@@ -45,12 +45,22 @@\n    */\n   Catalog loadCatalog();\n \n-  static CatalogLoader hadoop(String name, Configuration hadoopConf, String warehouseLocation) {\n-    return new HadoopCatalogLoader(name, hadoopConf, warehouseLocation);\n+  static CatalogLoader hadoop(\n+      String name,\n+      Configuration hadoopConf,\n+      String warehouseLocation,\n+      Map<String, String> properties) {\n+    return new HadoopCatalogLoader(name, hadoopConf, warehouseLocation, properties);", "originalCommit": "8c57598d6957c55945abcff8a70e923c866d07cc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODIxMDU2OQ==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r518210569", "bodyText": "I only see the CatalogLoader.hadoop and CatalogLoader.hive used in a single place FlinkCatalogFactory.createCatalogLoader, that is why I directly changed the class signature. I don't know if there is any benefit in keeping the old ones.\nChanging the signature to CatalogLoader.hadoop(String name, Map<String, String> properties, Configuration conf) looks like a good idea that simplifies the code, let me do that first.\n(speaking of this, I should also add null check for the properties map and also give a fixed serialization id for those classes because Flink serializes the catalog loader)", "author": "jackye1995", "createdAt": "2020-11-05T17:02:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODE4NTY3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODMxNDU5MQ==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r518314591", "bodyText": "You're right. Since these are package-private, I don't think we need to maintain them. I was thinking that people would call these from the public API to configure the source and sink builders, but these aren't public.\nWe may still want to keep them to avoid changing lots of files in this PR, and I still think it is a good idea to pull out the config here, rather than in FlinkCatalogFactory because we want to move toward catalogs interpreting their own property maps.", "author": "rdblue", "createdAt": "2020-11-05T19:34:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODE4NTY3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODMyNDkyMw==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r518324923", "bodyText": "also give a fixed serialization id for those classes because Flink serializes the catalog loader\n\nWe know that serialization across Iceberg versions may be a problem, but I'm not sure that we want to introduce a serialization id. In general, we avoid this because we want serialization to fail if there are multiple library versions. Java serialization isn't something that we want to use for compatibility across versions. The cases where we have multiple library versions are rare, and we want to design something for a rolling update.", "author": "rdblue", "createdAt": "2020-11-05T19:51:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODE4NTY3MQ=="}], "type": "inlineReview"}, {"oid": "cea285ec7ac76edd7118055a0cb49235e3b3c52b", "url": "https://github.com/apache/iceberg/commit/cea285ec7ac76edd7118055a0cb49235e3b3c52b", "message": "dynamically load custom FileIO implementation", "committedDate": "2020-11-05T20:28:09Z", "type": "commit"}, {"oid": "cea285ec7ac76edd7118055a0cb49235e3b3c52b", "url": "https://github.com/apache/iceberg/commit/cea285ec7ac76edd7118055a0cb49235e3b3c52b", "message": "dynamically load custom FileIO implementation", "committedDate": "2020-11-05T20:28:09Z", "type": "forcePushed"}, {"oid": "03442e8320f8547614bdc80c969cbbc4f7cbafc2", "url": "https://github.com/apache/iceberg/commit/03442e8320f8547614bdc80c969cbbc4f7cbafc2", "message": "add backwards compatible constructors", "committedDate": "2020-11-05T20:37:17Z", "type": "commit"}, {"oid": "76f53bccebf55461928f7b3c857b0190052310c1", "url": "https://github.com/apache/iceberg/commit/76f53bccebf55461928f7b3c857b0190052310c1", "message": "fix constructor order in commit", "committedDate": "2020-11-05T20:46:51Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQxNzUzOA==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r518417538", "bodyText": "Nit: we prefer args on as few lines as possible, rather than this style.", "author": "rdblue", "createdAt": "2020-11-05T22:49:30Z", "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java", "diffHunk": "@@ -69,13 +73,24 @@ public HiveCatalog(Configuration conf) {\n     this.conf = conf;\n     this.createStack = Thread.currentThread().getStackTrace();\n     this.closed = false;\n+    this.fileIO = new HadoopFileIO(conf);\n   }\n \n   public HiveCatalog(String name, String uri, int clientPoolSize, Configuration conf) {\n     this(name, uri, null, clientPoolSize, conf);\n   }\n \n   public HiveCatalog(String name, String uri, String warehouse, int clientPoolSize, Configuration conf) {\n+    this(name, uri, warehouse, clientPoolSize, conf, Maps.newHashMap());\n+  }\n+\n+  public HiveCatalog(\n+      String name,\n+      String uri,\n+      String warehouse,\n+      int clientPoolSize,\n+      Configuration conf,\n+      Map<String, String> properties) {", "originalCommit": "76f53bccebf55461928f7b3c857b0190052310c1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODU3NTk0NA==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r518575944", "bodyText": "I see, sorry I did not see the comments and get a chance to change it before the merge. I guess this kind of issue will be less frequent as I get more familiar with the code style here, but feel free to ping me at any time and I can fix all the style issues before merge \ud83d\ude03", "author": "jackye1995", "createdAt": "2020-11-06T07:51:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQxNzUzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkwNDE2MA==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r518904160", "bodyText": "No problem! I made this comment just before I merged it. Commits don't need to be perfect. I just wanted to let you know for next time.", "author": "rdblue", "createdAt": "2020-11-06T17:40:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQxNzUzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQxODIzNw==", "url": "https://github.com/apache/iceberg/pull/1618#discussion_r518418237", "bodyText": "Nit: leaving these in place would have reduce the number of files that this needed to touch, and avoided a possible problem removing public fields. I don't think it's worth blocking for this change, but we like to keep patches as small as possible by not breaking references like these.", "author": "rdblue", "createdAt": "2020-11-05T22:51:17Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalogFactory.java", "diffHunk": "@@ -63,11 +63,7 @@\n   public static final String ICEBERG_CATALOG_TYPE_HADOOP = \"hadoop\";\n   public static final String ICEBERG_CATALOG_TYPE_HIVE = \"hive\";\n \n-  public static final String HIVE_URI = \"uri\";\n-  public static final String HIVE_CLIENT_POOL_SIZE = \"clients\";\n   public static final String HIVE_CONF_DIR = \"hive-conf-dir\";\n-  public static final String WAREHOUSE_LOCATION = \"warehouse\";", "originalCommit": "76f53bccebf55461928f7b3c857b0190052310c1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}