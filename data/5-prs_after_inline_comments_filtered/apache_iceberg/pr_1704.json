{"pr_number": 1704, "pr_title": "Flink : Fix Repeated Rewrite for RewriteDataFilesAction", "pr_createdAt": "2020-11-02T08:28:09Z", "pr_url": "https://github.com/apache/iceberg/pull/1704", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzYxMDU4MA==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r517610580", "bodyText": "Typo in variable name", "author": "RussellSpitzer", "createdAt": "2020-11-04T20:25:17Z", "path": "core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java", "diffHunk": "@@ -228,9 +228,18 @@ public RewriteDataFilesActionResult execute() {\n         .flatMap(Streams::stream)\n         .collect(Collectors.toList());\n \n+    // add a filter  to the CombinedScanTask list to avoid repeated rewrite datafile\n+    List<CombinedScanTask> fileterCombinedScanTasks =", "originalCommit": "31f66b872b5a2ee00ec0385376861eb4047f4747", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzYxMTY1MQ==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r517611651", "bodyText": "Maybe more clear as !(task.files.nonempty)", "author": "RussellSpitzer", "createdAt": "2020-11-04T20:27:22Z", "path": "core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java", "diffHunk": "@@ -228,9 +228,18 @@ public RewriteDataFilesActionResult execute() {\n         .flatMap(Streams::stream)\n         .collect(Collectors.toList());\n \n+    // add a filter  to the CombinedScanTask list to avoid repeated rewrite datafile\n+    List<CombinedScanTask> fileterCombinedScanTasks =\n+        combinedScanTasks.stream().filter(task -> task.files().size() > 1).collect(Collectors.toList());", "originalCommit": "31f66b872b5a2ee00ec0385376861eb4047f4747", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzYxMTkxMQ==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r517611911", "bodyText": "Could also push this into the build of Combined scan tasks on line 228, rather than doing a new variable", "author": "RussellSpitzer", "createdAt": "2020-11-04T20:27:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzYxMTY1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzcyNDA2OA==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r517724068", "bodyText": "Maybe more clear as !(task.files.nonempty)\n\nwe need to remove the combinedScanTasks which size is 1 \uff0cnot nonempty", "author": "zhangjun0x01", "createdAt": "2020-11-05T01:13:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzYxMTY1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzcyNTA3MQ==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r517725071", "bodyText": "Could also push this into the build of Combined scan tasks on line 228, rather than doing a new variable\n\nI refer to the variable filteredGroupedTasks above\uff0c I think it is maybe more clear", "author": "zhangjun0x01", "createdAt": "2020-11-05T01:16:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzYxMTY1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTc4MjM1OA==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r521782358", "bodyText": "I agree with @RussellSpitzer 's comment about \"push this into the build of Combined scan tasks on line 228\", because in that way we don't have to construct the List<CombinedScanTask> twice,  that is unnecessary.\nBTW,  would you pls add few unit tests to address this case ?  I think it's great to improve this but better to fix it with a UT.\nThanks for the work.", "author": "openinx", "createdAt": "2020-11-12T02:43:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzYxMTY1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzYxMzMxMA==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r517613310", "bodyText": "I think the flatmap here is unecessary since we can flatMap within the map operation instead,\n    filteredCombinedScanTasks.stream()\n        .flatMap(task -> task.files().stream().map(FileScanTask::file))\n        .collect(Collectors.toList())\n\nThis should skip at least one List allocation :)", "author": "RussellSpitzer", "createdAt": "2020-11-04T20:30:13Z", "path": "core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java", "diffHunk": "@@ -228,9 +228,18 @@ public RewriteDataFilesActionResult execute() {\n         .flatMap(Streams::stream)\n         .collect(Collectors.toList());\n \n+    // add a filter  to the CombinedScanTask list to avoid repeated rewrite datafile\n+    List<CombinedScanTask> fileterCombinedScanTasks =\n+        combinedScanTasks.stream().filter(task -> task.files().size() > 1).collect(Collectors.toList());\n+\n+    if (fileterCombinedScanTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n     List<DataFile> addedDataFiles = rewriteDataForTasks(combinedScanTasks);\n-    List<DataFile> currentDataFiles = filteredGroupedTasks.values().stream()\n-        .flatMap(tasks -> tasks.stream().map(FileScanTask::file))\n+    List<DataFile> currentDataFiles = fileterCombinedScanTasks.stream()", "originalCommit": "31f66b872b5a2ee00ec0385376861eb4047f4747", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzcyNTM2Nw==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r517725367", "bodyText": "yes thanks for your suggestion\uff0cI update it", "author": "zhangjun0x01", "createdAt": "2020-11-05T01:17:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzYxMzMxMA=="}], "type": "inlineReview"}, {"oid": "d925f9eacd41bd54708f3d451b045d0a3abe6490", "url": "https://github.com/apache/iceberg/commit/d925f9eacd41bd54708f3d451b045d0a3abe6490", "message": "FixRepeatedRewrite", "committedDate": "2020-11-05T00:58:17Z", "type": "forcePushed"}, {"oid": "edc890acd5bb90bbfeda3cce07270dbc3d75d899", "url": "https://github.com/apache/iceberg/commit/edc890acd5bb90bbfeda3cce07270dbc3d75d899", "message": "FixRepeatedRewrite", "committedDate": "2020-11-10T05:29:05Z", "type": "forcePushed"}, {"oid": "8927787217fde3ce34403ed4e51fb6f85d1e4f1a", "url": "https://github.com/apache/iceberg/commit/8927787217fde3ce34403ed4e51fb6f85d1e4f1a", "message": "add UT", "committedDate": "2020-11-12T06:15:23Z", "type": "forcePushed"}, {"oid": "9653e59c5f9e83bf1f811082b12c7278a1074e0a", "url": "https://github.com/apache/iceberg/commit/9653e59c5f9e83bf1f811082b12c7278a1074e0a", "message": "add UT", "committedDate": "2020-11-12T06:23:38Z", "type": "forcePushed"}, {"oid": "6a7c9a2ed3895d60b712d809662fb76c4997de53", "url": "https://github.com/apache/iceberg/commit/6a7c9a2ed3895d60b712d809662fb76c4997de53", "message": "FixRepeatedRewrite", "committedDate": "2020-11-12T06:15:23Z", "type": "forcePushed"}, {"oid": "e6af44271a45fcd93868edc0e103d4b539f22093", "url": "https://github.com/apache/iceberg/commit/e6af44271a45fcd93868edc0e103d4b539f22093", "message": "add UT testRewriteAvoidRepeateCompress", "committedDate": "2020-11-13T01:28:25Z", "type": "forcePushed"}, {"oid": "50347a4e1e2588385236f322e568e081549d5dc1", "url": "https://github.com/apache/iceberg/commit/50347a4e1e2588385236f322e568e081549d5dc1", "message": "add UT testRewriteAvoidRepeateCompress", "committedDate": "2020-11-13T01:37:59Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU5MTIxMQ==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r522591211", "bodyText": "Is this change correct  ?   If we have two file1 (200MB),  file2 (1MB), and the target file size is 128MB, then it will be planned into three combined tasks:  128MB, 72MB, 1MB.  Finally the file1 will occur twice in the currentDataFiles  ?", "author": "openinx", "createdAt": "2020-11-13T03:29:33Z", "path": "core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java", "diffHunk": "@@ -226,11 +226,16 @@ public RewriteDataFilesActionResult execute() {\n           return TableScanUtil.planTasks(splitTasks, targetSizeInBytes, splitLookback, splitOpenFileCost);\n         })\n         .flatMap(Streams::stream)\n+        .filter(task -> task.files().size() > 1)\n         .collect(Collectors.toList());\n \n+    if (combinedScanTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n     List<DataFile> addedDataFiles = rewriteDataForTasks(combinedScanTasks);\n-    List<DataFile> currentDataFiles = filteredGroupedTasks.values().stream()\n-        .flatMap(tasks -> tasks.stream().map(FileScanTask::file))\n+    List<DataFile> currentDataFiles = combinedScanTasks.stream()", "originalCommit": "50347a4e1e2588385236f322e568e081549d5dc1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU5MjA4NA==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r522592084", "bodyText": "if there is a datafile which size > targetSizeInBytes ,I think it should not be rewrited , I open a new PR to deal this .\n#1762", "author": "zhangjun0x01", "createdAt": "2020-11-13T03:33:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU5MTIxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjgxMTU3MQ==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r522811571", "bodyText": "It's right that we may don't have to rewrite those files whose size greater than target file size.  But in this patch, I don't understand why we need to change it , could you revert this pls ?", "author": "openinx", "createdAt": "2020-11-13T08:56:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU5MTIxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjg1NzMzMA==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r522857330", "bodyText": "filteredGroupedTasks contains all the datafiles that meet the conditions of the query. combinedScanTasks removes some datafiles that are smaller than targetSizeInBytes but do not need to be compressed. If when constructing currentDataFiles,we use filteredGroupedTasks  instead of combinedScanTasks, those large files that do not need to be compressed will be deleted, resulting in results Incorrect.\nWhen I doing this PR, I forgot to consider the datefile  whose size greater than the target file size. I think it is another issue, so I opened a new PR #1762 to fix it when  I found this issue. I think we should modify #1762 first, then modify this PR, or merge #1762 into  this PR ? what do you think?", "author": "zhangjun0x01", "createdAt": "2020-11-13T10:20:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU5MTIxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzI2Mzc4OA==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r533263788", "bodyText": "I think we should not rely on specific file-selection strategies here, because different selection strategies  may result in duplicated files for currentDataFiles.  The correct way is:  remove the duplicated files so that the currentDataFiles has correct files.\nI saw the here has used the set to remove duplicated files.  So it's OK now.", "author": "openinx", "createdAt": "2020-12-01T10:04:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU5MTIxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU5MjgzNA==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r522592834", "bodyText": "nit:  do we need to concat such a complex string for the data  ?  It doesn't look elegant.", "author": "openinx", "createdAt": "2020-11-13T03:36:30Z", "path": "flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -280,4 +280,91 @@ public void testRewriteLargeTableHasResiduals() throws IOException {\n     // Assert the table records as expected.\n     SimpleDataUtil.assertTableRecords(icebergTableUnPartitioned, expected);\n   }\n+\n+  /**\n+   * a test case to test avoid repeate compress\n+   * <p>\n+   * If datafile cannot be combined to CombinedScanTask with other DataFiles, the size of the CombinedScanTask list size\n+   * is 1, so we remove these CombinedScanTasks to avoid compressed repeatedly.\n+   * <p>\n+   * In this test case,we generated 3 data files and set targetSizeInBytes greater than the largest file size so that\n+   * it cannot be  combined a CombinedScanTask with other datafiles. The datafile with the largest file size will not be\n+   * compressed.\n+   * <p>\n+   * For the same data, the file sizes of different formats are different. The file sizes of different formats generated\n+   * by the data in the current test case are as follows:\n+   * <p>\n+   *   avro :\n+   *  size  file\n+   *  408 00000-0-5a218337-1742-4ed1-83d8-55e301da49b8-00001.avro\n+   * 2390 00000-0-8f431924-ec8d-4957-a238-b8fe2b136210-00001.avro\n+   *  408 00000-0-9c75bcc4-49f0-4722-9528-c1d5faa50fa7-00001.avro\n+   *\n+   * orc :\n+   * size  file\n+   * 1626 00000-0-260d42d1-f00f-4c5f-9628-5f41f6395093-00001.orc\n+   *  331 00000-0-942fd38b-d7af-4ad2-a985-0e6ccdb4d8d3-00001.orc\n+   *  333 00000-0-ad8f2c34-6cf7-43fe-990f-f8f6389d198e-00001.orc\n+   *\n+   * parquet :\n+   * size  file\n+   *  611 00000-0-84e1fd63-a840-4a23-983f-5247e9218cbe-00001.parquet\n+   *  611 00000-0-91b070f0-7d17-487c-97ec-de0f0b09aa31-00001.parquet\n+   * 2691 00000-0-e09c969d-d6ee-4a41-9e42-9dcbf42bc4e1-00001.parquet\n+   *\n+   * @throws IOException IOException\n+   */\n+  @Test\n+  public void testRewriteAvoidRepeateCompress() throws IOException {\n+    List<String> records = Lists.newArrayList();\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < 500; i++) {\n+      String data = String.valueOf(i) + \"hello iceberg,hello flink\";\n+      records.add(\"(\" + i + \",'\" + data + \"')\");", "originalCommit": "50347a4e1e2588385236f322e568e081549d5dc1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU5NDYyMg==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r522594622", "bodyText": "This is to construct a file with a larger file size to distinguish the other two small files, so that we can compress the other two small files without compressing the large file. If the three files are similar in size, we will not be able to do the rewrite", "author": "zhangjun0x01", "createdAt": "2020-11-13T03:43:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU5MjgzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjgxODU5Mg==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r522818592", "bodyText": "Yeah, i can understand that it's constructing a large string so that we could have a large file,  then we could pack the larger file into a separate CombinedScanTask which should not execute the rewrite action. I mean we could just generate a string like that Strings.repeat(String.valueOf(I), 10), don't have to concat and concat again.  It's a minor issue.", "author": "openinx", "createdAt": "2020-11-13T09:10:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU5MjgzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjgyMjk4OA==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r522822988", "bodyText": "About the test case, I think it may be better to generate three data files by using GenericAppenderFactory :\n\ncreate two small files, each have on record ;\ncreate a larger file by using FileAppender ( write few records until the file length exceed the given target file size).\n\nThen in theory,  we don't have to  set different threshold for different file format here, The current code is hard to maintain because it seems tricky.", "author": "openinx", "createdAt": "2020-11-13T09:18:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU5MjgzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjg2MTM5MA==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r522861390", "bodyText": "Yes, we should generate a file that meets the target size for different formats. I did not find a suitable method to generate a file and meet my expectations at  that time. I will take a look GenericAppenderFactory", "author": "zhangjun0x01", "createdAt": "2020-11-13T10:27:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU5MjgzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzg5MDMxNw==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r523890317", "bodyText": "create a larger file by using FileAppender ( write few records until the file length exceed the given target file size).\n\n\nI took a look and thought it might not be easy to implement. We need to get the file length through the length method until the file reaches the target file size, and then close the appender.\nBut for orc, we cannot get length from an open appending file, we can only get the file length when the file is closed, which is just the opposite of our needs.\norg.apache.iceberg.orc.OrcFileAppender#length method\n  @Override\n  public long length() {\n    Preconditions.checkState(isClosed,\n        \"Cannot return length while appending to an open file.\");\n    return file.toInputFile().getLength();\n  }", "author": "zhangjun0x01", "createdAt": "2020-11-16T04:11:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU5MjgzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDgzODAxNg==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r524838016", "bodyText": "I update the pr,use GenericAppenderFactory to generate the datafile except ORC format", "author": "zhangjun0x01", "createdAt": "2020-11-17T02:06:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjU5MjgzNA=="}], "type": "inlineReview"}, {"oid": "3eeb5879b8a48f0306e2177082e6e9bf599b1933", "url": "https://github.com/apache/iceberg/commit/3eeb5879b8a48f0306e2177082e6e9bf599b1933", "message": "generate file by FileAppender", "committedDate": "2020-11-17T02:00:25Z", "type": "forcePushed"}, {"oid": "a09f0adb6a0cd3327f75eb479ba3507b7629b283", "url": "https://github.com/apache/iceberg/commit/a09f0adb6a0cd3327f75eb479ba3507b7629b283", "message": "generate file by FileAppender", "committedDate": "2020-11-27T03:51:56Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzI2ODQ2Mw==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r533268463", "bodyText": "Use Assume.assumeFalse(\"ORC does not support getting length when file is opening\", format.equals(FileFormat.ORC));", "author": "openinx", "createdAt": "2020-12-01T10:08:50Z", "path": "flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -280,4 +290,79 @@ public void testRewriteLargeTableHasResiduals() throws IOException {\n     // Assert the table records as expected.\n     SimpleDataUtil.assertTableRecords(icebergTableUnPartitioned, expected);\n   }\n+\n+  /**\n+   * a test case to test avoid repeate compress\n+   * <p>\n+   * If datafile cannot be combined to CombinedScanTask with other DataFiles, the size of the CombinedScanTask list size\n+   * is 1, so we remove these CombinedScanTasks to avoid compressed repeatedly.\n+   * <p>\n+   * In this test case,we generated 3 data files and set targetSizeInBytes greater than the largest file size so that it\n+   * cannot be  combined a CombinedScanTask with other datafiles. The datafile with the largest file size will not be\n+   * compressed.\n+   *\n+   * @throws IOException IOException\n+   */\n+  @Test\n+  public void testRewriteAvoidRepeateCompress() throws IOException {\n+    if (!format.equals(FileFormat.ORC)) {", "originalCommit": "a09f0adb6a0cd3327f75eb479ba3507b7629b283", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzI3NzQyNA==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r533277424", "bodyText": "Use the try(...){} to close the fileAppender like this:\n      File file = temp.newFile();\n\n      int fileSize = 2000;\n      try (FileAppender<Record> fileAppender = genericAppenderFactory.newAppender(Files.localOutput(file), format)) {\n        for (int idx = 0; fileAppender.length() < fileSize; idx++) {\n          Record record = RECORD.copy();\n          record.setField(\"id\", idx);\n          record.setField(\"data\", \"iceberg\");\n          fileAppender.add(record);\n          expected.add(record);\n        }\n      }\n\n      DataFile dataFile = DataFiles.builder(icebergTableUnPartitioned.spec())\n          .withPath(file.getAbsolutePath())\n          .withFileSizeInBytes(file.length())\n          .withFormat(format)\n          .withRecordCount(expected.size())\n          .build();", "author": "openinx", "createdAt": "2020-12-01T10:16:33Z", "path": "flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -280,4 +290,79 @@ public void testRewriteLargeTableHasResiduals() throws IOException {\n     // Assert the table records as expected.\n     SimpleDataUtil.assertTableRecords(icebergTableUnPartitioned, expected);\n   }\n+\n+  /**\n+   * a test case to test avoid repeate compress\n+   * <p>\n+   * If datafile cannot be combined to CombinedScanTask with other DataFiles, the size of the CombinedScanTask list size\n+   * is 1, so we remove these CombinedScanTasks to avoid compressed repeatedly.\n+   * <p>\n+   * In this test case,we generated 3 data files and set targetSizeInBytes greater than the largest file size so that it\n+   * cannot be  combined a CombinedScanTask with other datafiles. The datafile with the largest file size will not be\n+   * compressed.\n+   *\n+   * @throws IOException IOException\n+   */\n+  @Test\n+  public void testRewriteAvoidRepeateCompress() throws IOException {\n+    if (!format.equals(FileFormat.ORC)) {\n+      List<Record> expected = Lists.newArrayList();\n+      Schema schema = icebergTableUnPartitioned.schema();\n+      GenericAppenderFactory genericAppenderFactory = new GenericAppenderFactory(schema);\n+      File file = temp.newFile();\n+      FileAppender<Record> fileAppender = genericAppenderFactory.newAppender(Files.localOutput(file), format);\n+      long filesize = 20000;\n+      int count = 0;\n+      for (; fileAppender.length() < filesize; count++) {\n+        Record record = RECORD.copy();\n+        record.setField(\"id\", count);\n+        record.setField(\"data\", \"iceberg\");\n+        fileAppender.add(record);\n+        expected.add(record);\n+      }\n+      fileAppender.close();\n+\n+      DataFile dataFile = DataFiles.builder(icebergTableUnPartitioned.spec())\n+          .withPath(file.getAbsolutePath())\n+          .withFileSizeInBytes(file.length())\n+          .withFormat(format)\n+          .withRecordCount(count)\n+          .build();", "originalCommit": "a09f0adb6a0cd3327f75eb479ba3507b7629b283", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzI3OTk4Ng==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r533279986", "bodyText": "I think we'd better to check that the compaction did not compaction the biggest file with one of the two small files ?     That means the dataFiles1  should have included the dataFile ?", "author": "openinx", "createdAt": "2020-12-01T10:18:37Z", "path": "flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -280,4 +290,79 @@ public void testRewriteLargeTableHasResiduals() throws IOException {\n     // Assert the table records as expected.\n     SimpleDataUtil.assertTableRecords(icebergTableUnPartitioned, expected);\n   }\n+\n+  /**\n+   * a test case to test avoid repeate compress\n+   * <p>\n+   * If datafile cannot be combined to CombinedScanTask with other DataFiles, the size of the CombinedScanTask list size\n+   * is 1, so we remove these CombinedScanTasks to avoid compressed repeatedly.\n+   * <p>\n+   * In this test case,we generated 3 data files and set targetSizeInBytes greater than the largest file size so that it\n+   * cannot be  combined a CombinedScanTask with other datafiles. The datafile with the largest file size will not be\n+   * compressed.\n+   *\n+   * @throws IOException IOException\n+   */\n+  @Test\n+  public void testRewriteAvoidRepeateCompress() throws IOException {\n+    if (!format.equals(FileFormat.ORC)) {\n+      List<Record> expected = Lists.newArrayList();\n+      Schema schema = icebergTableUnPartitioned.schema();\n+      GenericAppenderFactory genericAppenderFactory = new GenericAppenderFactory(schema);\n+      File file = temp.newFile();\n+      FileAppender<Record> fileAppender = genericAppenderFactory.newAppender(Files.localOutput(file), format);\n+      long filesize = 20000;\n+      int count = 0;\n+      for (; fileAppender.length() < filesize; count++) {\n+        Record record = RECORD.copy();\n+        record.setField(\"id\", count);\n+        record.setField(\"data\", \"iceberg\");\n+        fileAppender.add(record);\n+        expected.add(record);\n+      }\n+      fileAppender.close();\n+\n+      DataFile dataFile = DataFiles.builder(icebergTableUnPartitioned.spec())\n+          .withPath(file.getAbsolutePath())\n+          .withFileSizeInBytes(file.length())\n+          .withFormat(format)\n+          .withRecordCount(count)\n+          .build();\n+\n+      icebergTableUnPartitioned.newAppend()\n+          .appendFile(dataFile)\n+          .commit();\n+\n+      sql(\"INSERT INTO %s SELECT 1,'a' \", TABLE_NAME_UNPARTITIONED);\n+      sql(\"INSERT INTO %s SELECT 2,'b' \", TABLE_NAME_UNPARTITIONED);\n+\n+      icebergTableUnPartitioned.refresh();\n+\n+      CloseableIterable<FileScanTask> tasks = icebergTableUnPartitioned.newScan().planFiles();\n+      List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+      Assert.assertEquals(\"Should have 3 data files before rewrite\", 3, dataFiles.size());\n+\n+      Actions actions = Actions.forTable(icebergTableUnPartitioned);\n+\n+      long targetSizeInBytes = file.length() + 10;\n+      RewriteDataFilesActionResult result = actions\n+          .rewriteDataFiles()\n+          .targetSizeInBytes(targetSizeInBytes)\n+          .splitOpenFileCost(1)\n+          .execute();\n+      Assert.assertEquals(\"Action should rewrite 2 data files\", 2, result.deletedDataFiles().size());\n+      Assert.assertEquals(\"Action should add 1 data file\", 1, result.addedDataFiles().size());\n+\n+      icebergTableUnPartitioned.refresh();\n+\n+      CloseableIterable<FileScanTask> tasks1 = icebergTableUnPartitioned.newScan().planFiles();\n+      List<DataFile> dataFiles1 = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));\n+      Assert.assertEquals(\"Should have 2 data files after rewrite\", 2, dataFiles1.size());", "originalCommit": "a09f0adb6a0cd3327f75eb479ba3507b7629b283", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "df3918aea6408c36cb119399ab2cf36f457acc0c", "url": "https://github.com/apache/iceberg/commit/df3918aea6408c36cb119399ab2cf36f457acc0c", "message": "fix test case\n\nfix test case", "committedDate": "2020-12-04T09:19:43Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTYzNjA4NA==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r549636084", "bodyText": "nit: we could just use the the:\nRecord record = SimpleDataUtil.createRecord(count, \"iceberg\");", "author": "openinx", "createdAt": "2020-12-29T09:36:36Z", "path": "flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -280,4 +292,82 @@ public void testRewriteLargeTableHasResiduals() throws IOException {\n     // Assert the table records as expected.\n     SimpleDataUtil.assertTableRecords(icebergTableUnPartitioned, expected);\n   }\n+\n+  /**\n+   * a test case to test avoid repeate compress\n+   * <p>\n+   * If datafile cannot be combined to CombinedScanTask with other DataFiles, the size of the CombinedScanTask list size\n+   * is 1, so we remove these CombinedScanTasks to avoid compressed repeatedly.\n+   * <p>\n+   * In this test case,we generated 3 data files and set targetSizeInBytes greater than the largest file size so that it\n+   * cannot be  combined a CombinedScanTask with other datafiles. The datafile with the largest file size will not be\n+   * compressed.\n+   *\n+   * @throws IOException IOException\n+   */\n+  @Test\n+  public void testRewriteAvoidRepeateCompress() throws IOException {\n+    Assume.assumeFalse(\"ORC does not support getting length when file is opening\", format.equals(FileFormat.ORC));\n+    List<Record> expected = Lists.newArrayList();\n+    Schema schema = icebergTableUnPartitioned.schema();\n+    GenericAppenderFactory genericAppenderFactory = new GenericAppenderFactory(schema);\n+    File file = temp.newFile();\n+    int count = 0;\n+    try (FileAppender<Record> fileAppender = genericAppenderFactory.newAppender(Files.localOutput(file), format)) {\n+      long filesize = 20000;\n+      for (; fileAppender.length() < filesize; count++) {\n+        Record record = RECORD.copy();\n+        record.setField(\"id\", count);\n+        record.setField(\"data\", \"iceberg\");", "originalCommit": "df3918aea6408c36cb119399ab2cf36f457acc0c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTYzNzk1Mg==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r549637952", "bodyText": "nit:  Rewrited -> Rewrote,  or we could just name it newDataFiles.", "author": "openinx", "createdAt": "2020-12-29T09:42:50Z", "path": "flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -280,4 +292,82 @@ public void testRewriteLargeTableHasResiduals() throws IOException {\n     // Assert the table records as expected.\n     SimpleDataUtil.assertTableRecords(icebergTableUnPartitioned, expected);\n   }\n+\n+  /**\n+   * a test case to test avoid repeate compress\n+   * <p>\n+   * If datafile cannot be combined to CombinedScanTask with other DataFiles, the size of the CombinedScanTask list size\n+   * is 1, so we remove these CombinedScanTasks to avoid compressed repeatedly.\n+   * <p>\n+   * In this test case,we generated 3 data files and set targetSizeInBytes greater than the largest file size so that it\n+   * cannot be  combined a CombinedScanTask with other datafiles. The datafile with the largest file size will not be\n+   * compressed.\n+   *\n+   * @throws IOException IOException\n+   */\n+  @Test\n+  public void testRewriteAvoidRepeateCompress() throws IOException {\n+    Assume.assumeFalse(\"ORC does not support getting length when file is opening\", format.equals(FileFormat.ORC));\n+    List<Record> expected = Lists.newArrayList();\n+    Schema schema = icebergTableUnPartitioned.schema();\n+    GenericAppenderFactory genericAppenderFactory = new GenericAppenderFactory(schema);\n+    File file = temp.newFile();\n+    int count = 0;\n+    try (FileAppender<Record> fileAppender = genericAppenderFactory.newAppender(Files.localOutput(file), format)) {\n+      long filesize = 20000;\n+      for (; fileAppender.length() < filesize; count++) {\n+        Record record = RECORD.copy();\n+        record.setField(\"id\", count);\n+        record.setField(\"data\", \"iceberg\");\n+        fileAppender.add(record);\n+        expected.add(record);\n+      }\n+    }\n+\n+    DataFile dataFile = DataFiles.builder(icebergTableUnPartitioned.spec())\n+        .withPath(file.getAbsolutePath())\n+        .withFileSizeInBytes(file.length())\n+        .withFormat(format)\n+        .withRecordCount(count)\n+        .build();\n+\n+    icebergTableUnPartitioned.newAppend()\n+        .appendFile(dataFile)\n+        .commit();\n+\n+    sql(\"INSERT INTO %s SELECT 1,'a' \", TABLE_NAME_UNPARTITIONED);\n+    sql(\"INSERT INTO %s SELECT 2,'b' \", TABLE_NAME_UNPARTITIONED);\n+\n+    icebergTableUnPartitioned.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = icebergTableUnPartitioned.newScan().planFiles();\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 3 data files before rewrite\", 3, dataFiles.size());\n+\n+    Actions actions = Actions.forTable(icebergTableUnPartitioned);\n+\n+    long targetSizeInBytes = file.length() + 10;\n+    RewriteDataFilesActionResult result = actions\n+        .rewriteDataFiles()\n+        .targetSizeInBytes(targetSizeInBytes)\n+        .splitOpenFileCost(1)\n+        .execute();\n+    Assert.assertEquals(\"Action should rewrite 2 data files\", 2, result.deletedDataFiles().size());\n+    Assert.assertEquals(\"Action should add 1 data file\", 1, result.addedDataFiles().size());\n+\n+    icebergTableUnPartitioned.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks1 = icebergTableUnPartitioned.newScan().planFiles();\n+    List<DataFile> dataFilesRewrited = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));", "originalCommit": "df3918aea6408c36cb119399ab2cf36f457acc0c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTYzODk0MQ==", "url": "https://github.com/apache/iceberg/pull/1704#discussion_r549638941", "bodyText": "nit:  could use the method references here.\n        // The biggest file do not be rewrote.\n    List<CharSequence> newPaths = newDataFiles.stream().map(ContentFile::path).collect(Collectors.toList());\n    Assert.assertTrue(newPaths.contains(file.getAbsolutePath()));", "author": "openinx", "createdAt": "2020-12-29T09:45:51Z", "path": "flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -280,4 +292,82 @@ public void testRewriteLargeTableHasResiduals() throws IOException {\n     // Assert the table records as expected.\n     SimpleDataUtil.assertTableRecords(icebergTableUnPartitioned, expected);\n   }\n+\n+  /**\n+   * a test case to test avoid repeate compress\n+   * <p>\n+   * If datafile cannot be combined to CombinedScanTask with other DataFiles, the size of the CombinedScanTask list size\n+   * is 1, so we remove these CombinedScanTasks to avoid compressed repeatedly.\n+   * <p>\n+   * In this test case,we generated 3 data files and set targetSizeInBytes greater than the largest file size so that it\n+   * cannot be  combined a CombinedScanTask with other datafiles. The datafile with the largest file size will not be\n+   * compressed.\n+   *\n+   * @throws IOException IOException\n+   */\n+  @Test\n+  public void testRewriteAvoidRepeateCompress() throws IOException {\n+    Assume.assumeFalse(\"ORC does not support getting length when file is opening\", format.equals(FileFormat.ORC));\n+    List<Record> expected = Lists.newArrayList();\n+    Schema schema = icebergTableUnPartitioned.schema();\n+    GenericAppenderFactory genericAppenderFactory = new GenericAppenderFactory(schema);\n+    File file = temp.newFile();\n+    int count = 0;\n+    try (FileAppender<Record> fileAppender = genericAppenderFactory.newAppender(Files.localOutput(file), format)) {\n+      long filesize = 20000;\n+      for (; fileAppender.length() < filesize; count++) {\n+        Record record = RECORD.copy();\n+        record.setField(\"id\", count);\n+        record.setField(\"data\", \"iceberg\");\n+        fileAppender.add(record);\n+        expected.add(record);\n+      }\n+    }\n+\n+    DataFile dataFile = DataFiles.builder(icebergTableUnPartitioned.spec())\n+        .withPath(file.getAbsolutePath())\n+        .withFileSizeInBytes(file.length())\n+        .withFormat(format)\n+        .withRecordCount(count)\n+        .build();\n+\n+    icebergTableUnPartitioned.newAppend()\n+        .appendFile(dataFile)\n+        .commit();\n+\n+    sql(\"INSERT INTO %s SELECT 1,'a' \", TABLE_NAME_UNPARTITIONED);\n+    sql(\"INSERT INTO %s SELECT 2,'b' \", TABLE_NAME_UNPARTITIONED);\n+\n+    icebergTableUnPartitioned.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = icebergTableUnPartitioned.newScan().planFiles();\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 3 data files before rewrite\", 3, dataFiles.size());\n+\n+    Actions actions = Actions.forTable(icebergTableUnPartitioned);\n+\n+    long targetSizeInBytes = file.length() + 10;\n+    RewriteDataFilesActionResult result = actions\n+        .rewriteDataFiles()\n+        .targetSizeInBytes(targetSizeInBytes)\n+        .splitOpenFileCost(1)\n+        .execute();\n+    Assert.assertEquals(\"Action should rewrite 2 data files\", 2, result.deletedDataFiles().size());\n+    Assert.assertEquals(\"Action should add 1 data file\", 1, result.addedDataFiles().size());\n+\n+    icebergTableUnPartitioned.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks1 = icebergTableUnPartitioned.newScan().planFiles();\n+    List<DataFile> dataFilesRewrited = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 2 data files after rewrite\", 2, dataFilesRewrited.size());\n+\n+    // the biggest file do not be rewrited\n+    List rewritedDataFileNames = dataFilesRewrited.stream().map(df -> df.path()).collect(Collectors.toList());", "originalCommit": "df3918aea6408c36cb119399ab2cf36f457acc0c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "88f5b331e7e0d2c9fb2df6b4914beeca4026aaf3", "url": "https://github.com/apache/iceberg/commit/88f5b331e7e0d2c9fb2df6b4914beeca4026aaf3", "message": "fix typo", "committedDate": "2020-12-30T02:00:47Z", "type": "forcePushed"}, {"oid": "fa106b398b69dc080a3e0d84128cd6d6235c4556", "url": "https://github.com/apache/iceberg/commit/fa106b398b69dc080a3e0d84128cd6d6235c4556", "message": "FixRepeatedRewrite", "committedDate": "2020-12-31T06:07:14Z", "type": "commit"}, {"oid": "9e7bdbb0a29fe19519b569a1ffd7a232932cac26", "url": "https://github.com/apache/iceberg/commit/9e7bdbb0a29fe19519b569a1ffd7a232932cac26", "message": "add UT testRewriteAvoidRepeateCompress", "committedDate": "2020-12-31T06:07:15Z", "type": "commit"}, {"oid": "faa9e0d6c5dea3b4dc010b1a2f11715e852586c6", "url": "https://github.com/apache/iceberg/commit/faa9e0d6c5dea3b4dc010b1a2f11715e852586c6", "message": "generate file by FileAppender", "committedDate": "2020-12-31T06:07:15Z", "type": "commit"}, {"oid": "cd1fdbd08a2fae6be19b3f2e87797a31f0018aa8", "url": "https://github.com/apache/iceberg/commit/cd1fdbd08a2fae6be19b3f2e87797a31f0018aa8", "message": "fix test case\n\nfix test case", "committedDate": "2020-12-31T06:07:15Z", "type": "commit"}, {"oid": "03a2236f91234cd57ce7641d91b252a4be7edcf4", "url": "https://github.com/apache/iceberg/commit/03a2236f91234cd57ce7641d91b252a4be7edcf4", "message": "fix typo", "committedDate": "2020-12-31T06:07:15Z", "type": "commit"}, {"oid": "03a2236f91234cd57ce7641d91b252a4be7edcf4", "url": "https://github.com/apache/iceberg/commit/03a2236f91234cd57ce7641d91b252a4be7edcf4", "message": "fix typo", "committedDate": "2020-12-31T06:07:15Z", "type": "forcePushed"}]}