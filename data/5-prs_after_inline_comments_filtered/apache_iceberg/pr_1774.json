{"pr_number": 1774, "pr_title": "[iceberg-1746] Implement spark fanout writer", "pr_createdAt": "2020-11-15T05:33:43Z", "pr_url": "https://github.com/apache/iceberg/pull/1774", "timeline": [{"oid": "1e3e886a451e8fbcf0b2e65daa9ab4f902e905fb", "url": "https://github.com/apache/iceberg/commit/1e3e886a451e8fbcf0b2e65daa9ab4f902e905fb", "message": "[iceberg-1746] Implement spark fanout writer", "committedDate": "2020-11-19T06:37:37Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDIxNDIwMA==", "url": "https://github.com/apache/iceberg/pull/1774#discussion_r530214200", "bodyText": "nit:  don't need to break into a new line here", "author": "openinx", "createdAt": "2020-11-25T09:13:08Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "diffHunk": "@@ -106,8 +107,17 @@ public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n     if (spec.fields().isEmpty()) {\n       writer = new UnpartitionedWriter<>(spec, format, appenderFactory, fileFactory, io.value(), Long.MAX_VALUE);\n     } else {\n-      writer = new SparkPartitionedWriter(spec, format, appenderFactory, fileFactory, io.value(), Long.MAX_VALUE,\n-          schema, structType);\n+      if (PropertyUtil.propertyAsBoolean(properties,\n+          TableProperties.WRITE_PARTITIONED_FANOUT_ENABLED,\n+          TableProperties.WRITE_PARTITIONED_FANOUT_ENABLED_DEFAULT)) {\n+        writer = new SparkPartitionedFanoutWriter(spec, format, appenderFactory, fileFactory, io.value(),\n+            Long.MAX_VALUE,", "originalCommit": "1e3e886a451e8fbcf0b2e65daa9ab4f902e905fb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDIxNDMwMg==", "url": "https://github.com/apache/iceberg/pull/1774#discussion_r530214302", "bodyText": "ditto.", "author": "openinx", "createdAt": "2020-11-25T09:13:17Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "diffHunk": "@@ -106,8 +107,17 @@ public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n     if (spec.fields().isEmpty()) {\n       writer = new UnpartitionedWriter<>(spec, format, appenderFactory, fileFactory, io.value(), Long.MAX_VALUE);\n     } else {\n-      writer = new SparkPartitionedWriter(spec, format, appenderFactory, fileFactory, io.value(), Long.MAX_VALUE,\n-          schema, structType);\n+      if (PropertyUtil.propertyAsBoolean(properties,\n+          TableProperties.WRITE_PARTITIONED_FANOUT_ENABLED,\n+          TableProperties.WRITE_PARTITIONED_FANOUT_ENABLED_DEFAULT)) {\n+        writer = new SparkPartitionedFanoutWriter(spec, format, appenderFactory, fileFactory, io.value(),\n+            Long.MAX_VALUE,\n+            schema, structType);\n+      } else {\n+        writer = new SparkPartitionedWriter(spec, format, appenderFactory, fileFactory, io.value(),\n+            Long.MAX_VALUE,", "originalCommit": "1e3e886a451e8fbcf0b2e65daa9ab4f902e905fb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDIxNjIxMw==", "url": "https://github.com/apache/iceberg/pull/1774#discussion_r530216213", "bodyText": "Q:  will we set this for a given table ?  In my option,  it's per job ?", "author": "openinx", "createdAt": "2020-11-25T09:16:08Z", "path": "spark2/src/main/java/org/apache/iceberg/spark/source/Writer.java", "diffHunk": "@@ -113,6 +116,10 @@\n     long tableTargetFileSize = PropertyUtil.propertyAsLong(\n         table.properties(), WRITE_TARGET_FILE_SIZE_BYTES, WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n     this.targetFileSize = options.getLong(\"target-file-size-bytes\", tableTargetFileSize);\n+\n+    boolean tablePartitionedFanoutEnabled = PropertyUtil.propertyAsBoolean(", "originalCommit": "1e3e886a451e8fbcf0b2e65daa9ab4f902e905fb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDI5Njg5OA==", "url": "https://github.com/apache/iceberg/pull/1774#discussion_r530296898", "bodyText": "we need set this option for a given table, Because some tables require fanout.", "author": "XuQianJin-Stars", "createdAt": "2020-11-25T11:18:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDIxNjIxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDIxNzEyMw==", "url": "https://github.com/apache/iceberg/pull/1774#discussion_r530217123", "bodyText": "ditto.", "author": "openinx", "createdAt": "2020-11-25T09:17:26Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "diffHunk": "@@ -113,6 +116,11 @@\n     long tableTargetFileSize = PropertyUtil.propertyAsLong(\n         table.properties(), WRITE_TARGET_FILE_SIZE_BYTES, WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n     this.targetFileSize = writeInfo.options().getLong(\"target-file-size-bytes\", tableTargetFileSize);\n+\n+    boolean tablePartitionedFanoutEnabled = PropertyUtil.propertyAsBoolean(", "originalCommit": "1e3e886a451e8fbcf0b2e65daa9ab4f902e905fb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDIxOTkxNw==", "url": "https://github.com/apache/iceberg/pull/1774#discussion_r530219917", "bodyText": "For partitioned fanout case, we don't have to sort based on data column ?  Otherwise, what's the difference compared to PartitionedWriter ?", "author": "openinx", "createdAt": "2020-11-25T09:21:19Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java", "diffHunk": "@@ -374,6 +375,58 @@ public void testPartitionedCreateWithTargetFileSizeViaOption() throws IOExceptio\n     }\n   }\n \n+  @Test\n+  public void testPartitionedFanoutCreateWithTargetFileSizeViaOption() throws IOException {\n+    File parent = temp.newFolder(format.toString());\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n+    Table table = tables.create(SCHEMA, spec, location.toString());\n+    table.updateProperties()\n+        .set(WRITE_PARTITIONED_FANOUT_ENABLED, \"true\")\n+        .commit();\n+\n+    List<SimpleRecord> expected = Lists.newArrayListWithCapacity(8000);\n+    for (int i = 0; i < 2000; i++) {\n+      expected.add(new SimpleRecord(i, \"a\"));\n+      expected.add(new SimpleRecord(i, \"b\"));\n+      expected.add(new SimpleRecord(i, \"c\"));\n+      expected.add(new SimpleRecord(i, \"d\"));\n+    }\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, SimpleRecord.class);\n+\n+    df.select(\"id\", \"data\").sort(\"data\").write()", "originalCommit": "1e3e886a451e8fbcf0b2e65daa9ab4f902e905fb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDIyMDQ4NA==", "url": "https://github.com/apache/iceberg/pull/1774#discussion_r530220484", "bodyText": "In general,  I think we could abstract a common method between this method and testPartitionedCreateWithTargetFileSizeViaOption,  they are almost the same.", "author": "openinx", "createdAt": "2020-11-25T09:22:11Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java", "diffHunk": "@@ -374,6 +375,58 @@ public void testPartitionedCreateWithTargetFileSizeViaOption() throws IOExceptio\n     }\n   }\n \n+  @Test\n+  public void testPartitionedFanoutCreateWithTargetFileSizeViaOption() throws IOException {", "originalCommit": "1e3e886a451e8fbcf0b2e65daa9ab4f902e905fb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDIyNjI2Ng==", "url": "https://github.com/apache/iceberg/pull/1774#discussion_r530226266", "bodyText": "Well, let me change it.", "author": "XuQianJin-Stars", "createdAt": "2020-11-25T09:30:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDIyMDQ4NA=="}], "type": "inlineReview"}, {"oid": "6f102777203c537089e430216710be7b08d7418f", "url": "https://github.com/apache/iceberg/commit/6f102777203c537089e430216710be7b08d7418f", "message": "[iceberg-1746] Implement spark fanout writer", "committedDate": "2020-11-25T12:01:56Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDM0ODg4MQ==", "url": "https://github.com/apache/iceberg/pull/1774#discussion_r530348881", "bodyText": "nit:   seems we could format the code to align with the previous line ?", "author": "openinx", "createdAt": "2020-11-25T12:48:35Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/SparkPartitionedFanoutWriter.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.io.FileAppenderFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFileFactory;\n+import org.apache.iceberg.io.PartitionedFanoutWriter;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class SparkPartitionedFanoutWriter extends PartitionedFanoutWriter<InternalRow> {\n+  private final PartitionKey partitionKey;\n+  private final InternalRowWrapper internalRowWrapper;\n+\n+  public SparkPartitionedFanoutWriter(PartitionSpec spec, FileFormat format,\n+                                FileAppenderFactory<InternalRow> appenderFactory,", "originalCommit": "6f102777203c537089e430216710be7b08d7418f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDM0OTkyOA==", "url": "https://github.com/apache/iceberg/pull/1774#discussion_r530349928", "bodyText": "well", "author": "XuQianJin-Stars", "createdAt": "2020-11-25T12:50:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDM0ODg4MQ=="}], "type": "inlineReview"}, {"oid": "600d2307bd0705dd290d068eebec268f5846039f", "url": "https://github.com/apache/iceberg/commit/600d2307bd0705dd290d068eebec268f5846039f", "message": "[iceberg-1746] Implement spark fanout writer", "committedDate": "2020-11-25T12:58:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDczMDkxMw==", "url": "https://github.com/apache/iceberg/pull/1774#discussion_r530730913", "bodyText": "nit:  I think it's more clear to just use :\nif(spec.fields().isEmpty()){\n   return UnpartitionedWriter\n} else if(xxx){\n    return SparkPartitionedFanoutWriter ; \n} else {\n    return SparkPartitionedWriter\n}", "author": "openinx", "createdAt": "2020-11-26T01:59:24Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "diffHunk": "@@ -106,8 +107,15 @@ public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n     if (spec.fields().isEmpty()) {\n       writer = new UnpartitionedWriter<>(spec, format, appenderFactory, fileFactory, io.value(), Long.MAX_VALUE);\n     } else {\n-      writer = new SparkPartitionedWriter(spec, format, appenderFactory, fileFactory, io.value(), Long.MAX_VALUE,\n-          schema, structType);\n+      if (PropertyUtil.propertyAsBoolean(properties,", "originalCommit": "600d2307bd0705dd290d068eebec268f5846039f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDczMTIzMA==", "url": "https://github.com/apache/iceberg/pull/1774#discussion_r530731230", "bodyText": "well", "author": "XuQianJin-Stars", "createdAt": "2020-11-26T02:00:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDczMDkxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDczMTk3NQ==", "url": "https://github.com/apache/iceberg/pull/1774#discussion_r530731975", "bodyText": "Do we need an unit test to cover the spark's partitioned.fanout.enabled option ?   I saw there's an unit test which use the table's write.partitioned.fanout.enabled property to define the fanout behavior.", "author": "openinx", "createdAt": "2020-11-26T02:03:43Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java", "diffHunk": "@@ -327,51 +328,12 @@ public void testUnpartitionedCreateWithTargetFileSizeViaTableProperties() throws\n \n   @Test\n   public void testPartitionedCreateWithTargetFileSizeViaOption() throws IOException {\n-    File parent = temp.newFolder(format.toString());\n-    File location = new File(parent, \"test\");\n-\n-    HadoopTables tables = new HadoopTables(CONF);\n-    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n-    Table table = tables.create(SCHEMA, spec, location.toString());\n-\n-    List<SimpleRecord> expected = Lists.newArrayListWithCapacity(8000);\n-    for (int i = 0; i < 2000; i++) {\n-      expected.add(new SimpleRecord(i, \"a\"));\n-      expected.add(new SimpleRecord(i, \"b\"));\n-      expected.add(new SimpleRecord(i, \"c\"));\n-      expected.add(new SimpleRecord(i, \"d\"));\n-    }\n-\n-    Dataset<Row> df = spark.createDataFrame(expected, SimpleRecord.class);\n-\n-    df.select(\"id\", \"data\").sort(\"data\").write()\n-        .format(\"iceberg\")\n-        .option(\"write-format\", format.toString())\n-        .mode(\"append\")\n-        .option(\"target-file-size-bytes\", 4) // ~4 bytes; low enough to trigger\n-        .save(location.toString());\n-\n-    table.refresh();\n-\n-    Dataset<Row> result = spark.read()\n-        .format(\"iceberg\")\n-        .load(location.toString());\n-\n-    List<SimpleRecord> actual = result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n-    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+    partitionedCreateWithTargetFileSizeViaOption(false);\n+  }\n \n-    List<DataFile> files = Lists.newArrayList();\n-    for (ManifestFile manifest : table.currentSnapshot().allManifests()) {\n-      for (DataFile file : ManifestFiles.read(manifest, table.io())) {\n-        files.add(file);\n-      }\n-    }\n-    // TODO: ORC file now not support target file size\n-    if (!format.equals(FileFormat.ORC)) {\n-      Assert.assertEquals(\"Should have 8 DataFiles\", 8, files.size());\n-      Assert.assertTrue(\"All DataFiles contain 1000 rows\", files.stream().allMatch(d -> d.recordCount() == 1000));\n-    }\n+  @Test\n+  public void testPartitionedFanoutCreateWithTargetFileSizeViaOption() throws IOException {\n+    partitionedCreateWithTargetFileSizeViaOption(true);", "originalCommit": "600d2307bd0705dd290d068eebec268f5846039f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDczMjc1MQ==", "url": "https://github.com/apache/iceberg/pull/1774#discussion_r530732751", "bodyText": "nit:  could simplify it as:\nif(spec.fields().isEmpty()){\n\n} else if(partitionedFanoutEnabled){\n\n} else {\n\n}", "author": "openinx", "createdAt": "2020-11-26T02:06:30Z", "path": "spark2/src/main/java/org/apache/iceberg/spark/source/Writer.java", "diffHunk": "@@ -271,8 +280,13 @@ public String toString() {\n       if (spec.fields().isEmpty()) {\n         return new Unpartitioned24Writer(spec, format, appenderFactory, fileFactory, io.value(), targetFileSize);\n       } else {\n-        return new Partitioned24Writer(spec, format, appenderFactory, fileFactory, io.value(),\n-            targetFileSize, writeSchema, dsSchema);\n+        if (partitionedFanoutEnabled) {", "originalCommit": "600d2307bd0705dd290d068eebec268f5846039f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDczMzEyNw==", "url": "https://github.com/apache/iceberg/pull/1774#discussion_r530733127", "bodyText": "nit: format those lines as:\n    PartitionedFanout24Writer(PartitionSpec spec, FileFormat format,\n                              SparkAppenderFactory appenderFactory,\n                              OutputFileFactory fileFactory, FileIO fileIo, long targetFileSize,\n                              Schema schema, StructType sparkSchema) {\n      super(spec, format, appenderFactory, fileFactory, fileIo, targetFileSize, schema, sparkSchema);\n    }", "author": "openinx", "createdAt": "2020-11-26T02:08:05Z", "path": "spark2/src/main/java/org/apache/iceberg/spark/source/Writer.java", "diffHunk": "@@ -307,4 +321,23 @@ public WriterCommitMessage commit() throws IOException {\n       return new TaskCommit(complete());\n     }\n   }\n+\n+  private static class PartitionedFanout24Writer extends SparkPartitionedFanoutWriter\n+      implements DataWriter<InternalRow> {\n+\n+    PartitionedFanout24Writer(PartitionSpec spec, FileFormat format,\n+        SparkAppenderFactory appenderFactory,\n+        OutputFileFactory fileFactory, FileIO fileIo, long targetFileSize,\n+        Schema schema, StructType sparkSchema) {", "originalCommit": "600d2307bd0705dd290d068eebec268f5846039f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDczMzIzNw==", "url": "https://github.com/apache/iceberg/pull/1774#discussion_r530733237", "bodyText": "ditto", "author": "openinx", "createdAt": "2020-11-26T02:08:38Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "diffHunk": "@@ -420,8 +430,15 @@ protected WriterFactory(PartitionSpec spec, FileFormat format, LocationProvider\n       if (spec.fields().isEmpty()) {\n         return new Unpartitioned3Writer(spec, format, appenderFactory, fileFactory, io.value(), targetFileSize);\n       } else {\n-        return new Partitioned3Writer(\n-            spec, format, appenderFactory, fileFactory, io.value(), targetFileSize, writeSchema, dsSchema);\n+        if (partitionedFanoutEnabled) {", "originalCommit": "600d2307bd0705dd290d068eebec268f5846039f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDczMzI5NQ==", "url": "https://github.com/apache/iceberg/pull/1774#discussion_r530733295", "bodyText": "nit: format", "author": "openinx", "createdAt": "2020-11-26T02:08:45Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "diffHunk": "@@ -455,4 +472,20 @@ public WriterCommitMessage commit() throws IOException {\n       return new TaskCommit(complete());\n     }\n   }\n+\n+  private static class PartitionedFanout3Writer extends SparkPartitionedFanoutWriter\n+      implements DataWriter<InternalRow> {\n+    PartitionedFanout3Writer(PartitionSpec spec, FileFormat format, SparkAppenderFactory appenderFactory,", "originalCommit": "600d2307bd0705dd290d068eebec268f5846039f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "0917c7cbbab0c0d72f5e1130507a14dd030219db", "url": "https://github.com/apache/iceberg/commit/0917c7cbbab0c0d72f5e1130507a14dd030219db", "message": "[iceberg-1746] Implement spark fanout writer", "committedDate": "2020-11-26T07:22:01Z", "type": "commit"}, {"oid": "03c1c77219141621f84bce6a7339b6dbafb4be97", "url": "https://github.com/apache/iceberg/commit/03c1c77219141621f84bce6a7339b6dbafb4be97", "message": "[iceberg-1746] Implement spark fanout writer", "committedDate": "2020-11-26T08:48:49Z", "type": "commit"}]}