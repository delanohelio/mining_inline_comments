{"pr_number": 1640, "pr_title": "Allow loading custom Catalog implementation in Spark and Flink", "pr_createdAt": "2020-10-22T00:23:38Z", "pr_url": "https://github.com/apache/iceberg/pull/1640", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ1MzIxMw==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r510453213", "bodyText": "NIT: Avoid hard code it", "author": "giovannifumarola", "createdAt": "2020-10-22T20:58:53Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalogFactory.java", "diffHunk": "@@ -79,6 +80,10 @@ protected CatalogLoader createCatalogLoader(String name, Map<String, String> pro\n         String warehouseLocation = properties.get(HADOOP_WAREHOUSE_LOCATION);\n         return CatalogLoader.hadoop(name, hadoopConf, warehouseLocation);\n \n+      case \"custom\":", "originalCommit": "17f1c5c17b7789778599b2d9f499fc47837a7806", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ5ODk1MQ==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r510498951", "bodyText": "This is trying to be consistent with the other cases which also use hard-coded strings, both in Spark and Flink.", "author": "jackye1995", "createdAt": "2020-10-22T22:43:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ1MzIxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTA1MTQyOA==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r511051428", "bodyText": "Maybe we should make them an Enum, which in general are preferred for using in switch statements. Also, I'm not sure if we make sure properties are lower case for this switch.", "author": "edgarRd", "createdAt": "2020-10-23T18:02:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ1MzIxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ1NDUwNw==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r510454507", "bodyText": "Maybe this method can be static and be called from other places.", "author": "giovannifumarola", "createdAt": "2020-10-22T21:01:15Z", "path": "flink/src/main/java/org/apache/iceberg/flink/CatalogLoader.java", "diffHunk": "@@ -94,4 +100,47 @@ public String toString() {\n           .toString();\n     }\n   }\n+\n+  class CustomCatalogLoader implements CatalogLoader {\n+\n+    private final SerializableConfiguration hadoopConf;\n+    private final String name;\n+    private final String impl;\n+\n+    private CustomCatalogLoader(String name, Configuration conf, String impl) {\n+      this.hadoopConf = new SerializableConfiguration(conf);\n+      this.name = name;\n+      this.impl = Preconditions.checkNotNull(impl,\n+          \"Cannot initialize custom Catalog because impl property is not set\");\n+    }\n+\n+    @Override\n+    public Catalog loadCatalog() {", "originalCommit": "17f1c5c17b7789778599b2d9f499fc47837a7806", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ5ODcwMg==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r510498702", "bodyText": "It implements the CatalogLoader interface and cannot be changed to static.", "author": "jackye1995", "createdAt": "2020-10-22T22:43:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ1NDUwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzExNjY4MA==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r513116680", "bodyText": "What about using a static method in some other class in core? Maybe CatalogUtil.loadCatalog?", "author": "rdblue", "createdAt": "2020-10-28T00:54:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ1NDUwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzgzNTk3OA==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r513835978", "bodyText": "Yeah sounds like a good idea to move the logic to a common util, let me do that", "author": "jackye1995", "createdAt": "2020-10-29T00:15:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ1NDUwNw=="}], "type": "inlineReview"}, {"oid": "6942c9da6dc9e0065ed6494b6689220708370b2f", "url": "https://github.com/apache/iceberg/commit/6942c9da6dc9e0065ed6494b6689220708370b2f", "message": "initial impl using custom type", "committedDate": "2020-10-27T23:13:04Z", "type": "forcePushed"}, {"oid": "40c914d0ea7a659a48dbc445240ac8a30ab8bf97", "url": "https://github.com/apache/iceberg/commit/40c914d0ea7a659a48dbc445240ac8a30ab8bf97", "message": "Hive: Add TestHiveShell for parameterized StorageHandler tests (#1631)\n\nCo-authored-by: Marton Bod <mbod@cloudera.com>", "committedDate": "2020-10-27T23:13:04Z", "type": "forcePushed"}, {"oid": "f1776f64ba1d298a063bb3da6dc41e52b803b038", "url": "https://github.com/apache/iceberg/commit/f1776f64ba1d298a063bb3da6dc41e52b803b038", "message": "Spark: Bump Spark 3 version to 3.0.1 (#1656)", "committedDate": "2020-10-27T23:13:04Z", "type": "forcePushed"}, {"oid": "f4708aa2f414d4c9027b797ebdb87232563bc3cb", "url": "https://github.com/apache/iceberg/commit/f4708aa2f414d4c9027b797ebdb87232563bc3cb", "message": "Docs: Fix docs typo in ParquetVectorizedReader (#1658)", "committedDate": "2020-10-27T23:13:04Z", "type": "forcePushed"}, {"oid": "fe56b3b346f5ccea6d22f530f31cb0677f7d16a1", "url": "https://github.com/apache/iceberg/commit/fe56b3b346f5ccea6d22f530f31cb0677f7d16a1", "message": "Hive: Update test code for Hive4 (#1645)\n\nCo-authored-by: Marton Bod <mbod@cloudera.com>", "committedDate": "2020-10-27T23:13:04Z", "type": "forcePushed"}, {"oid": "e72109d246c2aa2b1f1409e189c597742b342f86", "url": "https://github.com/apache/iceberg/commit/e72109d246c2aa2b1f1409e189c597742b342f86", "message": "Parquet: Add test for Arrow buffer reallocation (#1480)", "committedDate": "2020-10-27T23:13:04Z", "type": "forcePushed"}, {"oid": "0ff356b03cfbf9a0a1ef535d5f9754641f7c9edf", "url": "https://github.com/apache/iceberg/commit/0ff356b03cfbf9a0a1ef535d5f9754641f7c9edf", "message": "Flink: Load hive-site.xml for HiveCatalog (#1586)", "committedDate": "2020-10-27T23:13:04Z", "type": "forcePushed"}, {"oid": "9d0083be4c8d2d03d95af7c1dde59931673133ff", "url": "https://github.com/apache/iceberg/commit/9d0083be4c8d2d03d95af7c1dde59931673133ff", "message": "Docs: Document property behavior for Spark REPLACE TABLE (#1644)", "committedDate": "2020-10-27T23:11:59Z", "type": "forcePushed"}, {"oid": "4f8efcbc39387e2b13b961bd3a777cefbf846427", "url": "https://github.com/apache/iceberg/commit/4f8efcbc39387e2b13b961bd3a777cefbf846427", "message": "Spark: Bump Spark 2 module to 2.4.7 (#1646)", "committedDate": "2020-10-27T23:11:59Z", "type": "forcePushed"}, {"oid": "9535ebe2244a94277901095a70e9e827a810f4b0", "url": "https://github.com/apache/iceberg/commit/9535ebe2244a94277901095a70e9e827a810f4b0", "message": "Lint: Fix small issues (#1650)", "committedDate": "2020-10-27T23:11:59Z", "type": "forcePushed"}, {"oid": "47bb57a0f2787dd1b2aaf8857717321ca4b39efd", "url": "https://github.com/apache/iceberg/commit/47bb57a0f2787dd1b2aaf8857717321ca4b39efd", "message": "Spark: Split Actions for Spark 2 and 3 using reflection (#1616)", "committedDate": "2020-10-27T23:11:59Z", "type": "forcePushed"}, {"oid": "d3201f241dfec6c9a8091b39ce6f16cd3bf9bb96", "url": "https://github.com/apache/iceberg/commit/d3201f241dfec6c9a8091b39ce6f16cd3bf9bb96", "message": "Parquet: Fix row group filtering with old CDH stats (#1638)", "committedDate": "2020-10-27T23:11:59Z", "type": "forcePushed"}, {"oid": "17f1c5c17b7789778599b2d9f499fc47837a7806", "url": "https://github.com/apache/iceberg/commit/17f1c5c17b7789778599b2d9f499fc47837a7806", "message": "initial impl using custom type", "committedDate": "2020-10-22T00:10:55Z", "type": "forcePushed"}, {"oid": "d3ec64385b2ca6687167fa09a7e768e6da7c61df", "url": "https://github.com/apache/iceberg/commit/d3ec64385b2ca6687167fa09a7e768e6da7c61df", "message": "Flink: Support configurable parallelism for write tasks (#1619)", "committedDate": "2020-10-21T01:09:35Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzExNzY0Mw==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r513117643", "bodyText": "We definitely need an option to create a catalog without passing Configuration but still passing config properties. I originally thought that it would make sense to use another constructor, but then I thought about how name is passed... and I think the number of possible constructors may get out of hand.\nInstead of adding a lot of constructors, I think we should do this:\n\nUse a no-arg constructor for all catalogs\nAdd an initialize method to catalogs that is called to pass the catalog name and a string map of config (this matches what Spark does)\nIf the catalog implements Hadoop's Configurable interface, also call setConf to set the Hadoop config.\n\nThat way, we avoid having Configuration in any of our APIs and minimize the number of constructors that we need to support. What do you think, @jackye1995?", "author": "rdblue", "createdAt": "2020-10-28T00:57:22Z", "path": "flink/src/main/java/org/apache/iceberg/flink/CatalogLoader.java", "diffHunk": "@@ -105,4 +113,54 @@ public String toString() {\n           .toString();\n     }\n   }\n+\n+  class CustomCatalogLoader implements CatalogLoader {\n+\n+    private final SerializableConfiguration hadoopConf;\n+    private final Map<String, String> properties;\n+    private final String name;\n+    private final String impl;\n+\n+    private CustomCatalogLoader(\n+        String name,\n+        Map<String, String> properties,\n+        Configuration conf,\n+        String impl) {\n+      this.hadoopConf = new SerializableConfiguration(conf);\n+      this.properties = new HashMap<>(properties); // use hashmap for serialization\n+      this.name = name;\n+      this.impl = Preconditions.checkNotNull(impl,\n+          \"Cannot initialize custom Catalog because impl property is not set\");\n+    }\n+\n+    @Override\n+    public Catalog loadCatalog() {\n+      DynConstructors.Ctor<Catalog> ctor;\n+      try {\n+        ctor = DynConstructors.builder(Catalog.class)\n+            .impl(impl, Map.class, Configuration.class) // take in flink properties and hadoop configs\n+            .impl(impl) // fall back to no-arg constructor", "originalCommit": "1b3997450a142d951915aae70474cef63d6ba4f9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzEyMTk0OQ==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r513121949", "bodyText": "Yes I am also thinking about this issue. Another way I am considering is to use a constructor that only takes a string map. Becasue Hadoop configuration implements the Iterable<Map.Entry<String, String>> interface, we can merge Spark or Flink properties with Hadoop configurations together and pass into the constructor in a single map. We can use a wrapper class for the merged map to ensure the names of those properties and config keys do not conflict.", "author": "jackye1995", "createdAt": "2020-10-28T01:13:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzExNzY0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY1NzY0MQ==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r513657641", "bodyText": "I don't think it is a good idea to pass a Configuration as a flattened map. Then it isn't possible to reconstruct the original Configuration without everything being an override. I think we should keep Configuration separate and discourage its use where it isn't needed for Hadoop classes used by Iceberg.", "author": "rdblue", "createdAt": "2020-10-28T18:06:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzExNzY0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY1Nzc4NQ==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r513657785", "bodyText": "Actually, ignore the last comment, I overlooked the fact that the HadoopIO still needs the Hadoop configuration object. So the iceberg-hive-metastore already provides a way to solve this, that is to dynamically construct a CatalogLoader object, and call loader.load(conf). (link)\nThis provides maximum flexibility, and allow each engine to have a different loader interface that takes different arguments in constructor. Flink already has one, and I can add another one for Spark.", "author": "jackye1995", "createdAt": "2020-10-28T18:06:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzExNzY0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY4MDYwMg==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r513680602", "bodyText": "I don't think it is a good idea to pass a Configuration as a flattened map. Then it isn't possible to reconstruct the original Configuration without everything being an override. I think we should keep Configuration separate and discourage its use where it isn't needed for Hadoop classes used by Iceberg.\n\nyeah I replied at the same time lol. Please take a look at the latest reply", "author": "jackye1995", "createdAt": "2020-10-28T18:43:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzExNzY0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY5MTAwNw==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r513691007", "bodyText": "I don't think that we want to use CatalogLoader. The use case for that is a bit different: it is for Hive, where Configuration is the correct way to pass options. That loader should call whatever dynamic loader function we introduce in this PR. I also think that it shouldn't be located in the iceberg-hive-metastore module. That should be in iceberg-mr.", "author": "rdblue", "createdAt": "2020-10-28T19:01:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzExNzY0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzcxNDE0MQ==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r513714141", "bodyText": "Oh yes my typo, it's in iceberg-mr. So if my understanding is correct, we will have something like:\npublic class MyCatalog implements Catalog, Configurable {\n  \n  private Configuration conf;\n\n  public MyCatalog() {\n  }\n  \n  // need to add a default in Catalog interface\n  @Override\n  public void initialize(Map<String, String> options) {\n      // initialize logic\n  }\n  \n  @Override\n  public void setConf(Configuration conf) {\n    this.conf = conf;\n  }\n}\n\n// in SparkCatalog\nString impl = options.get(\"impl\");\nDynConstructors.Ctor<Catalog> ctor = DynConstructors.builder(Catalog.class)\n    .impl(impl) // fall back to no-arg constructor\n    .buildChecked();\nCatalog customCatalog = ctor.newInstance();\nif (customCatalog instanceof Configurable) {\n  ((Configurable) customCatalog).setConf(conf);\n}\ncustomCatalog.initialize(options);\nDoes this look good to you?", "author": "jackye1995", "createdAt": "2020-10-28T19:42:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzExNzY0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzczNTY3Mg==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r513735672", "bodyText": "Mainly, yes. I think initialize should be initialize(String name, Map<String, String> options) so that we can set the name the catalog is configured to use.", "author": "rdblue", "createdAt": "2020-10-28T20:21:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzExNzY0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM3Njk3Nw==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r516376977", "bodyText": "Nit: I would expect impl to come first to keep the configuration (name, options) together.", "author": "rdblue", "createdAt": "2020-11-03T01:04:38Z", "path": "core/src/main/java/org/apache/iceberg/CatalogUtil.java", "diffHunk": "@@ -117,4 +122,45 @@ private static void deleteFiles(FileIO io, Set<ManifestFile> allManifests) {\n           }\n         });\n   }\n+\n+  /**\n+   * Load a custom catalog implementation.\n+   * The catalog must have a no-arg constructor.\n+   * If the catalog implements {@link org.apache.hadoop.conf.Configurable},\n+   * {@code Configurable.setConf(org.apache.hadoop.conf.Configuration conf)} is called to set Hadoop configuration.\n+   * {@code Catalog.initialize(String name, Map<String, String> options)} is called to complete the initialization.\n+   * @param catalogName catalog name\n+   * @param impl catalog implementation full class name\n+   * @param engineOptions configuration options from a compute engine like Spark or Flink to initialize the catalog\n+   * @param hadoopConf hadoop configuration if needed\n+   * @return initialized catalog object\n+   * @throws IllegalArgumentException if no-arg constructor not found or error during initialization\n+   */\n+  public static Catalog loadCustomCatalog(\n+      String catalogName,\n+      String impl,", "originalCommit": "c427e9875a27c4e52dcafa8e53ea0561234786db", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM3NzE0Ng==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r516377146", "bodyText": "This may all fit on one line now.", "author": "rdblue", "createdAt": "2020-11-03T01:04:57Z", "path": "core/src/main/java/org/apache/iceberg/CatalogUtil.java", "diffHunk": "@@ -117,4 +122,45 @@ private static void deleteFiles(FileIO io, Set<ManifestFile> allManifests) {\n           }\n         });\n   }\n+\n+  /**\n+   * Load a custom catalog implementation.\n+   * The catalog must have a no-arg constructor.\n+   * If the catalog implements {@link org.apache.hadoop.conf.Configurable},\n+   * {@code Configurable.setConf(org.apache.hadoop.conf.Configuration conf)} is called to set Hadoop configuration.\n+   * {@code Catalog.initialize(String name, Map<String, String> options)} is called to complete the initialization.\n+   * @param catalogName catalog name\n+   * @param impl catalog implementation full class name\n+   * @param engineOptions configuration options from a compute engine like Spark or Flink to initialize the catalog\n+   * @param hadoopConf hadoop configuration if needed\n+   * @return initialized catalog object\n+   * @throws IllegalArgumentException if no-arg constructor not found or error during initialization\n+   */\n+  public static Catalog loadCustomCatalog(\n+      String catalogName,\n+      String impl,\n+      Map<String, String> engineOptions,\n+      Configuration hadoopConf) {\n+    Preconditions.checkNotNull(impl, \"Cannot initialize custom Catalog because impl property is not set\");\n+    DynConstructors.Ctor<Catalog> ctor;\n+    try {\n+      ctor = DynConstructors.builder(Catalog.class)\n+          .impl(impl)\n+          .buildChecked();", "originalCommit": "c427e9875a27c4e52dcafa8e53ea0561234786db", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM3NzQ5MQ==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r516377491", "bodyText": "Can we move configuration out of the try/catch block? It doesn't need to be there.", "author": "rdblue", "createdAt": "2020-11-03T01:05:44Z", "path": "core/src/main/java/org/apache/iceberg/CatalogUtil.java", "diffHunk": "@@ -117,4 +122,45 @@ private static void deleteFiles(FileIO io, Set<ManifestFile> allManifests) {\n           }\n         });\n   }\n+\n+  /**\n+   * Load a custom catalog implementation.\n+   * The catalog must have a no-arg constructor.\n+   * If the catalog implements {@link org.apache.hadoop.conf.Configurable},\n+   * {@code Configurable.setConf(org.apache.hadoop.conf.Configuration conf)} is called to set Hadoop configuration.\n+   * {@code Catalog.initialize(String name, Map<String, String> options)} is called to complete the initialization.\n+   * @param catalogName catalog name\n+   * @param impl catalog implementation full class name\n+   * @param engineOptions configuration options from a compute engine like Spark or Flink to initialize the catalog\n+   * @param hadoopConf hadoop configuration if needed\n+   * @return initialized catalog object\n+   * @throws IllegalArgumentException if no-arg constructor not found or error during initialization\n+   */\n+  public static Catalog loadCustomCatalog(\n+      String catalogName,\n+      String impl,\n+      Map<String, String> engineOptions,\n+      Configuration hadoopConf) {\n+    Preconditions.checkNotNull(impl, \"Cannot initialize custom Catalog because impl property is not set\");\n+    DynConstructors.Ctor<Catalog> ctor;\n+    try {\n+      ctor = DynConstructors.builder(Catalog.class)\n+          .impl(impl)\n+          .buildChecked();\n+    } catch (NoSuchMethodException e) {\n+      throw new IllegalArgumentException(String.format(\n+          \"Cannot initialize Catalog, please make sure %s has a no-arg constructor\", impl), e);\n+    }\n+    try {\n+      Catalog catalog = ctor.newInstance();\n+      if (catalog instanceof Configurable) {\n+        ((Configurable) catalog).setConf(hadoopConf);\n+      }\n+      catalog.initialize(catalogName, engineOptions);", "originalCommit": "c427e9875a27c4e52dcafa8e53ea0561234786db", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM4ODAyOQ==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r516388029", "bodyText": "The name engineOptions seems too specific because it assumes that the caller is an engine. But it could be a user of the API that isn't an engine. I think config or properties would be a better name.", "author": "rdblue", "createdAt": "2020-11-03T01:48:35Z", "path": "core/src/main/java/org/apache/iceberg/CatalogUtil.java", "diffHunk": "@@ -117,4 +122,45 @@ private static void deleteFiles(FileIO io, Set<ManifestFile> allManifests) {\n           }\n         });\n   }\n+\n+  /**\n+   * Load a custom catalog implementation.\n+   * The catalog must have a no-arg constructor.\n+   * If the catalog implements {@link org.apache.hadoop.conf.Configurable},\n+   * {@code Configurable.setConf(org.apache.hadoop.conf.Configuration conf)} is called to set Hadoop configuration.\n+   * {@code Catalog.initialize(String name, Map<String, String> options)} is called to complete the initialization.\n+   * @param catalogName catalog name\n+   * @param impl catalog implementation full class name\n+   * @param engineOptions configuration options from a compute engine like Spark or Flink to initialize the catalog\n+   * @param hadoopConf hadoop configuration if needed\n+   * @return initialized catalog object\n+   * @throws IllegalArgumentException if no-arg constructor not found or error during initialization\n+   */\n+  public static Catalog loadCustomCatalog(\n+      String catalogName,\n+      String impl,\n+      Map<String, String> engineOptions,", "originalCommit": "c427e9875a27c4e52dcafa8e53ea0561234786db", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM4ODYwMA==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r516388600", "bodyText": "Minor: I tend to opt for removing words that aren't needed, so I would remove \"custom\" from here. I think that's pretty much implied by loading an implementation class.", "author": "rdblue", "createdAt": "2020-11-03T01:50:25Z", "path": "core/src/main/java/org/apache/iceberg/CatalogUtil.java", "diffHunk": "@@ -117,4 +122,45 @@ private static void deleteFiles(FileIO io, Set<ManifestFile> allManifests) {\n           }\n         });\n   }\n+\n+  /**\n+   * Load a custom catalog implementation.\n+   * The catalog must have a no-arg constructor.\n+   * If the catalog implements {@link org.apache.hadoop.conf.Configurable},\n+   * {@code Configurable.setConf(org.apache.hadoop.conf.Configuration conf)} is called to set Hadoop configuration.\n+   * {@code Catalog.initialize(String name, Map<String, String> options)} is called to complete the initialization.\n+   * @param catalogName catalog name\n+   * @param impl catalog implementation full class name\n+   * @param engineOptions configuration options from a compute engine like Spark or Flink to initialize the catalog\n+   * @param hadoopConf hadoop configuration if needed\n+   * @return initialized catalog object\n+   * @throws IllegalArgumentException if no-arg constructor not found or error during initialization\n+   */\n+  public static Catalog loadCustomCatalog(", "originalCommit": "c427e9875a27c4e52dcafa8e53ea0561234786db", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM4ODkzNw==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r516388937", "bodyText": "Instead of using type=custom and impl=com.example.Catalog, why not just combine them into type=com.example.Catalog. We can try to load the type as an implementation class if it isn't a well-known name like \"hive\".", "author": "rdblue", "createdAt": "2020-11-03T01:51:50Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalogFactory.java", "diffHunk": "@@ -58,13 +59,19 @@\n \n   // Can not just use \"type\", it conflicts with CATALOG_TYPE.\n   public static final String ICEBERG_CATALOG_TYPE = \"catalog-type\";\n+  public static final String ICEBERG_CATALOG_TYPE_HADOOP = \"hadoop\";\n+  public static final String ICEBERG_CATALOG_TYPE_HIVE = \"hive\";\n+  public static final String ICEBERG_CATALOG_TYPE_CUSTOM = \"custom\";", "originalCommit": "c427e9875a27c4e52dcafa8e53ea0561234786db", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM4OTQ5Ng==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r516389496", "bodyText": "@aokolnychyi and @RussellSpitzer, do you have an opinion here?", "author": "rdblue", "createdAt": "2020-11-03T01:54:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM4ODkzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQxNDg3Nw==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r516414877", "bodyText": "Yeah this sounds like a cleaner way to go, the only disadvantage is that we are overloading the term type. I have updated the code.", "author": "jackye1995", "createdAt": "2020-11-03T03:50:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM4ODkzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM4OTMwMA==", "url": "https://github.com/apache/iceberg/pull/1640#discussion_r516389300", "bodyText": "I think we shouldn't change the behavior of IcebergSource in this PR. We want to change how this source works and route queries through a catalog, but I'm not sure that using impl is the right way to do it. Let's stick with HiveCatalogs for now and revisit this in a follow up.", "author": "rdblue", "createdAt": "2020-11-03T01:53:27Z", "path": "spark2/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -137,9 +138,11 @@ protected Table findTable(DataSourceOptions options, Configuration conf) {\n       HadoopTables tables = new HadoopTables(conf);\n       return tables.load(path.get());\n     } else {\n-      HiveCatalog hiveCatalog = HiveCatalogs.loadCatalog(conf);\n+      Catalog catalog = options.get(\"impl\")\n+          .map(impl -> CatalogUtil.loadCustomCatalog(\"custom\", impl, options.asMap(), conf))\n+          .orElseGet(() -> HiveCatalogs.loadCatalog(conf));\n       TableIdentifier tableIdentifier = TableIdentifier.parse(path.get());\n-      return hiveCatalog.loadTable(tableIdentifier);\n+      return catalog.loadTable(tableIdentifier);", "originalCommit": "c427e9875a27c4e52dcafa8e53ea0561234786db", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "c7ce41ab642d0320142f50da78c2b0c95b622a27", "url": "https://github.com/apache/iceberg/commit/c7ce41ab642d0320142f50da78c2b0c95b622a27", "message": "Allow loading custom Catalog implementation in Spark and Flink", "committedDate": "2020-11-03T18:46:36Z", "type": "forcePushed"}, {"oid": "485dea0552b40faa329c2c956c2c462412dc8a8e", "url": "https://github.com/apache/iceberg/commit/485dea0552b40faa329c2c956c2c462412dc8a8e", "message": "Allow loading custom Catalog implementation in Spark and Flink", "committedDate": "2020-11-03T21:37:08Z", "type": "commit"}, {"oid": "485dea0552b40faa329c2c956c2c462412dc8a8e", "url": "https://github.com/apache/iceberg/commit/485dea0552b40faa329c2c956c2c462412dc8a8e", "message": "Allow loading custom Catalog implementation in Spark and Flink", "committedDate": "2020-11-03T21:37:08Z", "type": "forcePushed"}]}