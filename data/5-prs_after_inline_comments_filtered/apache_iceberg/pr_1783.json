{"pr_number": 1783, "pr_title": "Custom catalogs from `IcebergSource`", "pr_createdAt": "2020-11-18T18:48:22Z", "pr_url": "https://github.com/apache/iceberg/pull/1783", "timeline": [{"oid": "2e5ab046dd34c19a07aba7c737ddd5db2b4a6291", "url": "https://github.com/apache/iceberg/commit/2e5ab046dd34c19a07aba7c737ddd5db2b4a6291", "message": "source fixes", "committedDate": "2020-11-30T17:30:11Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyMjQ0Nw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533022447", "bodyText": "I don't think catalogName is accurate because catalogs have names (this one is spark_catalog). It should be catalogClass to be more clear.", "author": "rdblue", "createdAt": "2020-12-01T01:57:49Z", "path": "spark3/src/test/java/org/apache/iceberg/spark/source/SetupSourceCatalog.java", "diffHunk": "@@ -0,0 +1,41 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+\n+public final class SetupSourceCatalog {\n+\n+  private SetupSourceCatalog() {\n+\n+  }\n+\n+  public static void setupSparkCatalog(SparkSession spark) {\n+    setupSparkCatalog(spark, SparkSessionCatalog.class.getName());\n+  }\n+\n+  public static void setupSparkCatalog(SparkSession spark, String catalogName) {", "originalCommit": "2e5ab046dd34c19a07aba7c737ddd5db2b4a6291", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM3MzcxMA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533373710", "bodyText": "\ud83d\udc4d", "author": "rymurr", "createdAt": "2020-12-01T12:32:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyMjQ0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyMjg0Mg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533022842", "bodyText": "Why not add this to the iceberg-spark module? The only thing that isn't compatible is SparkSessionCatalog.class.getName(). The 2.4 code should ignore these settings, and this change would be a lot smaller.", "author": "rdblue", "createdAt": "2020-12-01T01:59:15Z", "path": "spark3/src/test/java/org/apache/iceberg/spark/source/SetupSourceCatalog.java", "diffHunk": "@@ -0,0 +1,41 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+\n+public final class SetupSourceCatalog {", "originalCommit": "2e5ab046dd34c19a07aba7c737ddd5db2b4a6291", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzM3NDMzMg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533374332", "bodyText": "cool, I was thinking similar but wanted to get your opinion first", "author": "rymurr", "createdAt": "2020-12-01T12:33:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyMjg0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyMzUwNw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533023507", "bodyText": "@RussellSpitzer, I seem to remember looking at very similar logic recently. Did we commit that anywhere that we can reuse?", "author": "rdblue", "createdAt": "2020-12-01T02:01:00Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +61,62 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Pair<String, TableIdentifier> tableIdentifier(CaseInsensitiveStringMap options) {\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    Namespace defaultNamespace = Namespace.of(catalogManager.currentNamespace());", "originalCommit": "2e5ab046dd34c19a07aba7c737ddd5db2b4a6291", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzkyNTYzMA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r537925630", "bodyText": "The logic was added to Spark3Util: https://github.com/apache/iceberg/blob/master/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java#L607\nCan you update this to use those utils?", "author": "rdblue", "createdAt": "2020-12-07T23:56:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyMzUwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM1MTQ1OA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r538351458", "bodyText": "lovely, thats fixed", "author": "rymurr", "createdAt": "2020-12-08T13:11:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyMzUwNw=="}], "type": "inlineReview"}, {"oid": "ac33ea3fd109d0b808f3c7f90cc38dc704b6923c", "url": "https://github.com/apache/iceberg/commit/ac33ea3fd109d0b808f3c7f90cc38dc704b6923c", "message": "updates based on code review", "committedDate": "2020-12-01T13:00:27Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzQxMjEyNg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533412126", "bodyText": "A potential complication: SupportsCatalogOptions doesn't allow for specifying the schema. I don't know how much people rely on this feature but it is a breaking change for the IcebergSource", "author": "rymurr", "createdAt": "2020-12-01T13:37:45Z", "path": "spark3/src/test/java/org/apache/iceberg/spark/source/TestSparkSchema3.java", "diffHunk": "@@ -1,23 +0,0 @@\n-/*", "originalCommit": "ac33ea3fd109d0b808f3c7f90cc38dc704b6923c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxNDM4MA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533614380", "bodyText": "Nit: unnecessary whitespace change.", "author": "rdblue", "createdAt": "2020-12-01T18:02:16Z", "path": "spark/src/test/java/org/apache/iceberg/TestScanTaskSerialization.java", "diffHunk": "@@ -60,11 +62,17 @@\n       optional(3, \"c3\", Types.StringType.get())\n   );\n \n+", "originalCommit": "ac33ea3fd109d0b808f3c7f90cc38dc704b6923c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNzEyNw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r534117127", "bodyText": "fixed", "author": "rymurr", "createdAt": "2020-12-02T12:07:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxNDM4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxNDYwNg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533614606", "bodyText": "Nit: can you remove this newline?", "author": "rdblue", "createdAt": "2020-12-01T18:02:38Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/SetupSourceCatalog.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.SparkSession;\n+\n+public final class SetupSourceCatalog {\n+\n+  private SetupSourceCatalog() {\n+", "originalCommit": "ac33ea3fd109d0b808f3c7f90cc38dc704b6923c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNzI5Ng==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r534117296", "bodyText": "fixed", "author": "rymurr", "createdAt": "2020-12-02T12:07:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxNDYwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxNjA4MQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533616081", "bodyText": "I think the correct exception is the Spark exception since this is going to be called from Spark code.", "author": "rdblue", "createdAt": "2020-12-01T18:05:03Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +61,62 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);", "originalCommit": "ac33ea3fd109d0b808f3c7f90cc38dc704b6923c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEyMDQxMA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r534120410", "bodyText": "The Spark NoSuchTableException is typed which would change the interface. It appears there is no well defined way to return from this function w/o a table being found. It NPEs if you return null so an untyped exception seems to be the expected way to denote no table found. I thought the Iceberg NoSuchTableException was the best compromise here.", "author": "rymurr", "createdAt": "2020-12-02T12:13:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxNjA4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ3MjE3Mw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r534472173", "bodyText": "Got it, I agree in that case. Thanks for explaining! You may want to add a comment here to explain in the code as well.", "author": "rdblue", "createdAt": "2020-12-02T20:50:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxNjA4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxODE0NQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533618145", "bodyText": "Why is this second attempt done?\nWrapping the identifier in ` should escape the entire string as a single identifier, so the parser would return the original path as one component.\nI think this would be equivalent to ident = path.", "author": "rdblue", "createdAt": "2020-12-01T18:08:28Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +61,62 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Pair<String, TableIdentifier> tableIdentifier(CaseInsensitiveStringMap options) {\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    Namespace defaultNamespace = Namespace.of(catalogManager.currentNamespace());\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n     String path = options.get(\"path\");\n-\n-    if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n+    List<String> ident;\n+    try {\n+      ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(path));\n+    } catch (ParseException e) {\n+      try {\n+        ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(String.format(\"`%s`\", path)));", "originalCommit": "ac33ea3fd109d0b808f3c7f90cc38dc704b6923c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEyMjg2MA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r534122860", "bodyText": "you are correct. Fixed", "author": "rymurr", "createdAt": "2020-12-02T12:17:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxODE0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxOTY2MQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533619661", "bodyText": "The default implementation always returns spark_catalog, not the current catalog. Since we want to use the current catalog when it isn't defined in the identifier, tableIdentifier(options) should fill it in. That would simplify this logic because it should always be non-null.", "author": "rdblue", "createdAt": "2020-12-01T18:10:55Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +61,62 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Pair<String, TableIdentifier> tableIdentifier(CaseInsensitiveStringMap options) {\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    Namespace defaultNamespace = Namespace.of(catalogManager.currentNamespace());\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n     String path = options.get(\"path\");\n-\n-    if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n+    List<String> ident;\n+    try {\n+      ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(path));\n+    } catch (ParseException e) {\n+      try {\n+        ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(String.format(\"`%s`\", path)));\n+      } catch (ParseException ignored) {\n+        throw new RuntimeException(e);\n+      }\n+    }\n+    if (ident.size() == 1) {\n+      return Pair.of(null, TableIdentifier.of(defaultNamespace, ident.get(0)));\n+    } else if (ident.size() == 2) {\n+      if (catalogManager.isCatalogRegistered(ident.get(0))) {\n+        return Pair.of(ident.get(0), TableIdentifier.of(defaultNamespace, ident.get(1))); //todo what if path?\n+      } else {\n+        return Pair.of(null, TableIdentifier.of(ident.toArray(new String[0])));\n+      }\n     } else {\n-      HiveCatalog hiveCatalog = HiveCatalogs.loadCatalog(conf);\n-      TableIdentifier tableIdentifier = TableIdentifier.parse(path);\n-      return hiveCatalog.loadTable(tableIdentifier);\n+      if (catalogManager.isCatalogRegistered(ident.get(0))) {\n+        return Pair.of(ident.get(0), TableIdentifier.of(ident.subList(1, ident.size()).toArray(new String[0])));\n+      } else {\n+        return Pair.of(null, TableIdentifier.of(ident.toArray(new String[0])));\n+      }\n     }\n   }\n \n-  private Table getTableAndResolveHadoopConfiguration(Map<String, String> options, Configuration conf) {\n-    // Overwrite configurations from the Spark Context with configurations from the options.\n-    mergeIcebergHadoopConfs(conf, options);\n-\n-    Table table = findTable(options, conf);\n-\n-    // Set confs from table properties\n-    mergeIcebergHadoopConfs(conf, table.properties());\n-\n-    // Re-overwrite values set in options and table properties but were not in the environment.\n-    mergeIcebergHadoopConfs(conf, options);\n-\n-    return table;\n+  @Override\n+  public Identifier extractIdentifier(CaseInsensitiveStringMap options) {\n+    TableIdentifier tableIdentifier = tableIdentifier(options).second();\n+    return Identifier.of(tableIdentifier.namespace().levels(), tableIdentifier.name());\n   }\n \n-  private static void mergeIcebergHadoopConfs(Configuration baseConf, Map<String, String> options) {\n-    options.keySet().stream()\n-        .filter(key -> key.startsWith(\"hadoop.\"))\n-        .forEach(key -> baseConf.set(key.replaceFirst(\"hadoop.\", \"\"), options.get(key)));\n+  @Override\n+  public String extractCatalog(CaseInsensitiveStringMap options) {\n+    String catalogName = tableIdentifier(options).first();\n+    return (catalogName == null) ? SupportsCatalogOptions.super.extractCatalog(options) : catalogName;", "originalCommit": "ac33ea3fd109d0b808f3c7f90cc38dc704b6923c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEyNDIwNA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r534124204", "bodyText": "Ahh, yes. I just re-read the comments. Make sense, fixed.", "author": "rymurr", "createdAt": "2020-12-02T12:19:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxOTY2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYyMTM5NQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533621395", "bodyText": "This shouldn't fill in the default namespace. If the identifier was two parts, like prod.items, then it is not correct to modify that to be prod.default.items.\nI think that this should use the same logic as the else.", "author": "rdblue", "createdAt": "2020-12-01T18:13:57Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +61,62 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Pair<String, TableIdentifier> tableIdentifier(CaseInsensitiveStringMap options) {\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    Namespace defaultNamespace = Namespace.of(catalogManager.currentNamespace());\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n     String path = options.get(\"path\");\n-\n-    if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n+    List<String> ident;\n+    try {\n+      ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(path));\n+    } catch (ParseException e) {\n+      try {\n+        ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(String.format(\"`%s`\", path)));\n+      } catch (ParseException ignored) {\n+        throw new RuntimeException(e);\n+      }\n+    }\n+    if (ident.size() == 1) {\n+      return Pair.of(null, TableIdentifier.of(defaultNamespace, ident.get(0)));\n+    } else if (ident.size() == 2) {\n+      if (catalogManager.isCatalogRegistered(ident.get(0))) {\n+        return Pair.of(ident.get(0), TableIdentifier.of(defaultNamespace, ident.get(1))); //todo what if path?", "originalCommit": "ac33ea3fd109d0b808f3c7f90cc38dc704b6923c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEyNjAxMw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r534126013", "bodyText": "ok, makes sense. That also clarifies the todo i left there. Fixed to not add a default namespace. Does the same logic apply for the case above. If I pass items it will get re-written to default_catalog.default.items, should it only be default_catalog.items?", "author": "rymurr", "createdAt": "2020-12-02T12:23:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYyMTM5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYyMTUyMA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r533621520", "bodyText": "Nit: could you separate the control flow statements with a newline?", "author": "rdblue", "createdAt": "2020-12-01T18:14:09Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +61,62 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Pair<String, TableIdentifier> tableIdentifier(CaseInsensitiveStringMap options) {\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    Namespace defaultNamespace = Namespace.of(catalogManager.currentNamespace());\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n     String path = options.get(\"path\");\n-\n-    if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n+    List<String> ident;\n+    try {\n+      ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(path));\n+    } catch (ParseException e) {\n+      try {\n+        ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(String.format(\"`%s`\", path)));\n+      } catch (ParseException ignored) {\n+        throw new RuntimeException(e);\n+      }\n+    }\n+    if (ident.size() == 1) {", "originalCommit": "ac33ea3fd109d0b808f3c7f90cc38dc704b6923c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDEyNjE1Mg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r534126152", "bodyText": "\ud83d\udc4d", "author": "rymurr", "createdAt": "2020-12-02T12:23:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYyMTUyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ3OTk0Mg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r535479942", "bodyText": "nit: should not change license", "author": "jackye1995", "createdAt": "2020-12-03T18:28:07Z", "path": "spark3/src/test/java/org/apache/iceberg/spark/source/TestIcebergSource.java", "diffHunk": "@@ -1,27 +1,23 @@\n /*\n- * Licensed to the Apache Software Foundation (ASF) under one", "originalCommit": "033a36adbdb4ab57f50d70e5dfb369541ffd1c67", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjEwOTcwMQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r536109701", "bodyText": "fixed, dunno how that happened...", "author": "rymurr", "createdAt": "2020-12-04T13:45:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ3OTk0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTUwMzk0Nw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r535503947", "bodyText": "Because we are now assigning an empty namespace for size==2 case,  I think it can be merged together with the else case. We only get ident.subList(1, ident.size()).toArray(new String[0]) simplified to ident.get(1), but the rest are all duplicates.", "author": "jackye1995", "createdAt": "2020-12-03T19:04:56Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +62,62 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Pair<String, TableIdentifier> tableIdentifier(CaseInsensitiveStringMap options) {\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    String currentCatalogName = catalogManager.currentCatalog().name();\n+    Namespace defaultNamespace = Namespace.of(catalogManager.currentNamespace());\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n     String path = options.get(\"path\");\n+    List<String> ident;\n+    try {\n+      ident = scala.collection.JavaConverters.seqAsJavaList(SparkSession.active().sessionState().sqlParser().parseMultipartIdentifier(path));\n+    } catch (ParseException e) {\n+      ident = new ArrayList<>();\n+      ident.add(path);\n+    }\n \n-    if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n+    if (ident.size() == 1) {\n+      return Pair.of(currentCatalogName, TableIdentifier.of(defaultNamespace, ident.get(0)));\n+    } else if (ident.size() == 2) {", "originalCommit": "033a36adbdb4ab57f50d70e5dfb369541ffd1c67", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjExMDIwMQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r536110201", "bodyText": "done", "author": "rymurr", "createdAt": "2020-12-04T13:46:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTUwMzk0Nw=="}], "type": "inlineReview"}, {"oid": "b96e39268dfe840446f260fa8de6eba2b39ac7b3", "url": "https://github.com/apache/iceberg/commit/b96e39268dfe840446f260fa8de6eba2b39ac7b3", "message": "use util function to get catalog and identifier", "committedDate": "2020-12-08T13:14:13Z", "type": "forcePushed"}, {"oid": "d4bb4b7eb12626bf224ce6aa95aef07bd37f1b93", "url": "https://github.com/apache/iceberg/commit/d4bb4b7eb12626bf224ce6aa95aef07bd37f1b93", "message": "rebase to pick up #1843 and fix build", "committedDate": "2020-12-09T13:40:16Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ4OTI3MA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539489270", "bodyText": "What does \"using settings from 'catalog'\" mean in practice? Hadoop tables are loaded using a generic HadoopTables, so the catalog doesn't really affect it at all, I think. In that case, this is promising something that it doesn't need to and I'd prefer to avoid making claims about behavior like that.", "author": "rdblue", "createdAt": "2020-12-09T17:10:59Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -19,22 +19,49 @@\n \n package org.apache.iceberg.spark.source;\n \n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n import java.util.Map;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.iceberg.Table;\n-import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.hadoop.HadoopTables;\n-import org.apache.iceberg.hive.HiveCatalog;\n-import org.apache.iceberg.hive.HiveCatalogs;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.PathIdentifier;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n import org.apache.spark.sql.SparkSession;\n-import org.apache.spark.sql.connector.catalog.TableProvider;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.connector.catalog.CatalogManager;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.SupportsCatalogOptions;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n import org.apache.spark.sql.connector.expressions.Transform;\n import org.apache.spark.sql.sources.DataSourceRegister;\n import org.apache.spark.sql.types.StructType;\n import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n \n-public class IcebergSource implements DataSourceRegister, TableProvider {\n+/**\n+ * The IcebergSource loads/writes tables with format \"iceberg\". It can load paths and tables.\n+ *\n+ * How paths/tables are loaded when using spark.read().format(\"iceberg\").path(table)\n+ *\n+ *  table = \"file:/path/to/table\" -> loads a HadoopTable at given path\n+ *  table = \"catalog.`file:/path/to/table`\" -> loads a HadoopTable at given path using settings from 'catalog'", "originalCommit": "512c268d7c84470e314be4f2b6d3352da62ec6c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTUyNDgzMQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539524831", "bodyText": "Yes you are correct. I was thinking of a catalog having different hadoop Configuration but I don't think that makes any sense. Fixed now.", "author": "rymurr", "createdAt": "2020-12-09T17:58:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ4OTI3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5MDE5NQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539490195", "bodyText": "In these examples, I think it is easier to understand if you separate the cases and use catalog and namespace, rather than \"if xxx is a catalog . . . otherwise . . .\".", "author": "rdblue", "createdAt": "2020-12-09T17:12:11Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -19,22 +19,49 @@\n \n package org.apache.iceberg.spark.source;\n \n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n import java.util.Map;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.iceberg.Table;\n-import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.hadoop.HadoopTables;\n-import org.apache.iceberg.hive.HiveCatalog;\n-import org.apache.iceberg.hive.HiveCatalogs;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.PathIdentifier;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n import org.apache.spark.sql.SparkSession;\n-import org.apache.spark.sql.connector.catalog.TableProvider;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.connector.catalog.CatalogManager;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.SupportsCatalogOptions;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n import org.apache.spark.sql.connector.expressions.Transform;\n import org.apache.spark.sql.sources.DataSourceRegister;\n import org.apache.spark.sql.types.StructType;\n import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n \n-public class IcebergSource implements DataSourceRegister, TableProvider {\n+/**\n+ * The IcebergSource loads/writes tables with format \"iceberg\". It can load paths and tables.\n+ *\n+ * How paths/tables are loaded when using spark.read().format(\"iceberg\").path(table)\n+ *\n+ *  table = \"file:/path/to/table\" -> loads a HadoopTable at given path\n+ *  table = \"catalog.`file:/path/to/table`\" -> loads a HadoopTable at given path using settings from 'catalog'\n+ *  table = \"catalog.namespace.`file:/path/to/table`\" -> fails. Namespace doesn't exist for paths\n+ *  table = \"tablename\" -> loads currentCatalog.currentNamespace.tablename\n+ *  table = \"xxx.tablename\" -> if xxx is a catalog load \"tablename\" from the specified catalog. Otherwise\n+ *          load \"xxx.tablename\" from current catalog\n+ *  table = \"xxx.yyy.tablename\" -> if xxx is a catalog load \"yyy.tablename\" from the specified catalog. Otherwise\n+ *          load \"xxx.yyy.tablename\" from current catalog", "originalCommit": "512c268d7c84470e314be4f2b6d3352da62ec6c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTUyNDg4OA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539524888", "bodyText": "agreed, fixed.", "author": "rymurr", "createdAt": "2020-12-09T17:58:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5MDE5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5MjQzMw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539492433", "bodyText": "When implementing SupportsCatalogOptions, I don't think this will ever be called. Should we remove it and throw UnsupportedOperationException instead?", "author": "rdblue", "createdAt": "2020-12-09T17:15:13Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +83,85 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {", "originalCommit": "512c268d7c84470e314be4f2b6d3352da62ec6c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTUyNzc4MQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539527781", "bodyText": "inferPartitioning method uses it :-( Can we return Transform[0] and have Spark do the work?", "author": "rymurr", "createdAt": "2020-12-09T18:02:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5MjQzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5NTE2Mw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539495163", "bodyText": "Is this try/catch to avoid the / check? I think that's a pretty reasonable check. I'm not sure that this is a good idea because the cases you listed above would pass this.\nFor example, this uses a valid identifier, but is clearly a path reference:\nspark.format(\"iceberg\").load(\"catalog.`file:/path/to/table`\");\nAs I mentioned above, I don't think that we should support the mixed path and catalog identifiers, but I still think it is probably a more predictable check to just look for /.", "author": "rdblue", "createdAt": "2020-12-09T17:18:49Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +83,85 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Spark3Util.CatalogAndIdentifier catalogAndIdentifier(CaseInsensitiveStringMap options) {\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n     String path = options.get(\"path\");\n-\n-    if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n+    SparkSession spark = SparkSession.active();\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n+    try {\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, path);", "originalCommit": "512c268d7c84470e314be4f2b6d3352da62ec6c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTUzMTExMQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539531111", "bodyText": "the try component takes care of catalog.table, catalog.namespace.table and catalog.`file://path/to/table`  but doens't take care of file://path/to/table which throws a parse exception. The catch part deals w/ file://path/to/table.\nI suppose we can check\n\nif there is a ` character and a / and reject (don't mix catalogs and paths)\nIf just / then treat as path\nassume its tables and parse it", "author": "rymurr", "createdAt": "2020-12-09T18:07:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5NTE2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTUzNzgyMw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539537823", "bodyText": "Yeah, I think that the problem is that catalog.`file://path/to/table`  is ambiguous and should not be supported. But parsing it successfully means that it would be translated to a path-based table with a catalog.\nI don't think a more complicated check is needed, to see if the / is probably escaped. If there is a /, then it's a path. That's a really simple rule to follow.", "author": "rdblue", "createdAt": "2020-12-09T18:17:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5NTE2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU0NDkwMw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539544903", "bodyText": "After thnking more I have thrown out catalog.`file:/path/to/table` and simplified the method", "author": "rymurr", "createdAt": "2020-12-09T18:27:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5NTE2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5NTgwNw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539495807", "bodyText": "Nit: Using ImmutableList makes this more concise: Spark3Util.catalogAndIdentifier(spark, ImmutableList.of(path))", "author": "rdblue", "createdAt": "2020-12-09T17:19:33Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +83,85 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Spark3Util.CatalogAndIdentifier catalogAndIdentifier(CaseInsensitiveStringMap options) {\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n     String path = options.get(\"path\");\n-\n-    if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n+    SparkSession spark = SparkSession.active();\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n+    try {\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, path);\n+    } catch (ParseException e) {\n+      List<String> ident = new ArrayList<>();\n+      ident.add(path);\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, ident);", "originalCommit": "512c268d7c84470e314be4f2b6d3352da62ec6c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTUzMTE4OQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539531189", "bodyText": "fixed", "author": "rymurr", "createdAt": "2020-12-09T18:07:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5NTgwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5ODk4Mg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539498982", "bodyText": "I think it would be simpler just to start this method with a check for the path:\nif (path.contains(\"/\")) {\n  Identifier ident = new PathIdentifer(path);\n  // do catalog resolution\n  return catalog, ident;\n}\n\nIt's easier to document and understand if the rules are simple.", "author": "rdblue", "createdAt": "2020-12-09T17:23:57Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +83,85 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Spark3Util.CatalogAndIdentifier catalogAndIdentifier(CaseInsensitiveStringMap options) {\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n     String path = options.get(\"path\");\n-\n-    if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n+    SparkSession spark = SparkSession.active();\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n+    try {\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, path);\n+    } catch (ParseException e) {\n+      List<String> ident = new ArrayList<>();\n+      ident.add(path);\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, ident);\n+    }\n+    CatalogManager catalogManager = spark.sessionState().catalogManager();\n+    String[] currentNamespace = catalogManager.currentNamespace();\n+    // we have to check for paths but want to re-use the exiting utils to extract catalog/identifier\n+    if (checkPathIdentifier(catalogAndIdentifier.identifier(), currentNamespace)) {", "originalCommit": "512c268d7c84470e314be4f2b6d3352da62ec6c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU0NTE0OQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539545149", "bodyText": "agreed, cleaned up", "author": "rymurr", "createdAt": "2020-12-09T18:28:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ5ODk4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc0ODAxOA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539748018", "bodyText": "Same here, no need for the catalog/path lines.", "author": "rdblue", "createdAt": "2020-12-10T00:20:50Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -20,21 +20,45 @@\n package org.apache.iceberg.spark.source;\n \n import java.util.Map;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.iceberg.Table;\n-import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.hadoop.HadoopTables;\n-import org.apache.iceberg.hive.HiveCatalog;\n-import org.apache.iceberg.hive.HiveCatalogs;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.base.Splitter;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.PathIdentifier;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n import org.apache.spark.sql.SparkSession;\n-import org.apache.spark.sql.connector.catalog.TableProvider;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.CatalogManager;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.SupportsCatalogOptions;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n import org.apache.spark.sql.connector.expressions.Transform;\n import org.apache.spark.sql.sources.DataSourceRegister;\n import org.apache.spark.sql.types.StructType;\n import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n \n-public class IcebergSource implements DataSourceRegister, TableProvider {\n+/**\n+ * The IcebergSource loads/writes tables with format \"iceberg\". It can load paths and tables.\n+ *\n+ * How paths/tables are loaded when using spark.read().format(\"iceberg\").path(table)\n+ *\n+ *  table = \"file:/path/to/table\" -> loads a HadoopTable at given path\n+ *  table = \"catalog.`file:/path/to/table`\" -> fails. Don't set a catalog for paths\n+ *  table = \"catalog.namespace.`file:/path/to/table`\" -> fails. Namespace doesn't exist for paths", "originalCommit": "e89c1ba706b123ba1de1a5a19fd6ea92eb90bc37", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDEzMzA2Mw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r540133063", "bodyText": "done", "author": "rymurr", "createdAt": "2020-12-10T12:34:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc0ODAxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc0ODA4Ng==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539748086", "bodyText": "Nit: starts with \"otherwise\" still.", "author": "rdblue", "createdAt": "2020-12-10T00:21:02Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -20,21 +20,45 @@\n package org.apache.iceberg.spark.source;\n \n import java.util.Map;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.iceberg.Table;\n-import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.hadoop.HadoopTables;\n-import org.apache.iceberg.hive.HiveCatalog;\n-import org.apache.iceberg.hive.HiveCatalogs;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.base.Splitter;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.PathIdentifier;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n import org.apache.spark.sql.SparkSession;\n-import org.apache.spark.sql.connector.catalog.TableProvider;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.CatalogManager;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.SupportsCatalogOptions;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n import org.apache.spark.sql.connector.expressions.Transform;\n import org.apache.spark.sql.sources.DataSourceRegister;\n import org.apache.spark.sql.types.StructType;\n import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n \n-public class IcebergSource implements DataSourceRegister, TableProvider {\n+/**\n+ * The IcebergSource loads/writes tables with format \"iceberg\". It can load paths and tables.\n+ *\n+ * How paths/tables are loaded when using spark.read().format(\"iceberg\").path(table)\n+ *\n+ *  table = \"file:/path/to/table\" -> loads a HadoopTable at given path\n+ *  table = \"catalog.`file:/path/to/table`\" -> fails. Don't set a catalog for paths\n+ *  table = \"catalog.namespace.`file:/path/to/table`\" -> fails. Namespace doesn't exist for paths\n+ *  table = \"tablename\" -> loads currentCatalog.currentNamespace.tablename\n+ *  table = \"catalog.tablename\" -> load \"tablename\" from the specified catalog.\n+ *  table = \"namespace.tablename\" -> Otherwise load \"namespace.tablename\" from current catalog", "originalCommit": "e89c1ba706b123ba1de1a5a19fd6ea92eb90bc37", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDEzMzA5Mg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r540133092", "bodyText": "done", "author": "rymurr", "createdAt": "2020-12-10T12:34:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc0ODA4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc0ODIxMg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539748212", "bodyText": "Nit: looks like newlines between control flow are missing in a lot of these changes.", "author": "rdblue", "createdAt": "2020-12-10T00:21:23Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +80,70 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface", "originalCommit": "e89c1ba706b123ba1de1a5a19fd6ea92eb90bc37", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDEzMzEyMg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r540133122", "bodyText": "done.", "author": "rymurr", "createdAt": "2020-12-10T12:34:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc0ODIxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc0OTIxMw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539749213", "bodyText": "This is doing a lot of extra work by not calling CatalogAndIdentifier directly. There are two maps created, two identical catalog/table resolutions, and then this needs to get the active session and look up the catalog that was already loaded. Is it possible to refactor so that the \"extract\" functions use a common method that can be used here?", "author": "rdblue", "createdAt": "2020-12-10T00:23:47Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +80,70 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));", "originalCommit": "e89c1ba706b123ba1de1a5a19fd6ea92eb90bc37", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDEzMzE4Nw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r540133187", "bodyText": "yup, this wasn't refactored when I removed some code from the extract methods. Now that they don't do anything I have cleaned up the getTable method.", "author": "rymurr", "createdAt": "2020-12-10T12:34:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc0OTIxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc0OTU4NA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r539749584", "bodyText": "I think this should parse the identifier using Spark rather than DOT.splitToList. It should be spark.sessionState().sqlParser().parseMultipartIdentifier(path).", "author": "rdblue", "createdAt": "2020-12-10T00:24:39Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +80,70 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    String catalogName = extractCatalog(new CaseInsensitiveStringMap(options));\n+    Identifier ident = extractIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogManager catalogManager = SparkSession.active().sessionState().catalogManager();\n+    CatalogPlugin catalog = catalogManager.catalog(catalogName);\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Spark3Util.CatalogAndIdentifier catalogAndIdentifier(CaseInsensitiveStringMap options) {\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n+    setupDefaultSparkCatalog();\n     String path = options.get(\"path\");\n+    SparkSession spark = SparkSession.active();\n+    CatalogManager catalogManager = spark.sessionState().catalogManager();\n \n     if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n+      // contains a path. Return iceberg default catalog and a PathIdentifier\n+      return new Spark3Util.CatalogAndIdentifier(catalogManager.catalog(DEFAULT_CATALOG_NAME),\n+          new PathIdentifier(path));\n+    }\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark,\n+        DOT.splitToList(path));", "originalCommit": "e89c1ba706b123ba1de1a5a19fd6ea92eb90bc37", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDEzMzMxNg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r540133316", "bodyText": "done. I have thrown an IllegalArgumentException in place of the Spark ParseException. This will eventually be caught and turned back into an AnalysisException.", "author": "rymurr", "createdAt": "2020-12-10T12:34:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc0OTU4NA=="}], "type": "inlineReview"}, {"oid": "3ed51bd68b10fe2fcced2b231f28499b69f7cbf5", "url": "https://github.com/apache/iceberg/commit/3ed51bd68b10fe2fcced2b231f28499b69f7cbf5", "message": "clean up based on code review", "committedDate": "2020-12-10T12:52:51Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDYwMDI3MA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r540600270", "bodyText": "Sorry for not catching this before, but I think this should be passed in. This should avoid extra calls to SparkSession.active().", "author": "rdblue", "createdAt": "2020-12-11T00:27:02Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +77,77 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    Spark3Util.CatalogAndIdentifier catalogIdentifier = catalogAndIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogPlugin catalog = catalogIdentifier.catalog();\n+    Identifier ident = catalogIdentifier.identifier();\n+\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Spark3Util.CatalogAndIdentifier catalogAndIdentifier(CaseInsensitiveStringMap options) {\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n+    setupDefaultSparkCatalog();\n     String path = options.get(\"path\");\n+    SparkSession spark = SparkSession.active();\n+    CatalogManager catalogManager = spark.sessionState().catalogManager();\n \n     if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n-    } else {\n-      HiveCatalog hiveCatalog = HiveCatalogs.loadCatalog(conf);\n-      TableIdentifier tableIdentifier = TableIdentifier.parse(path);\n-      return hiveCatalog.loadTable(tableIdentifier);\n+      // contains a path. Return iceberg default catalog and a PathIdentifier\n+      return new Spark3Util.CatalogAndIdentifier(catalogManager.catalog(DEFAULT_CATALOG_NAME),\n+          new PathIdentifier(path));\n     }\n-  }\n \n-  private Table getTableAndResolveHadoopConfiguration(Map<String, String> options, Configuration conf) {\n-    // Overwrite configurations from the Spark Context with configurations from the options.\n-    mergeIcebergHadoopConfs(conf, options);\n-\n-    Table table = findTable(options, conf);\n+    final Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n+    try {\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, path);\n+    } catch (ParseException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot parse path %s. It is not a valid SQL table\", path), e);\n+    }\n \n-    // Set confs from table properties\n-    mergeIcebergHadoopConfs(conf, table.properties());\n+    if (catalogAndIdentifier.catalog().name().equals(\"spark_catalog\") &&\n+        !(catalogAndIdentifier.catalog() instanceof SparkSessionCatalog)) {\n+      // catalog is a session catalog but does not support Iceberg. Use Iceberg instead.\n+      return new Spark3Util.CatalogAndIdentifier(catalogManager.catalog(DEFAULT_CATALOG_NAME),\n+          catalogAndIdentifier.identifier());\n+    } else {\n+      return catalogAndIdentifier;\n+    }\n+  }\n \n-    // Re-overwrite values set in options and table properties but were not in the environment.\n-    mergeIcebergHadoopConfs(conf, options);\n+  @Override\n+  public Identifier extractIdentifier(CaseInsensitiveStringMap options) {\n+    return catalogAndIdentifier(options).identifier();\n+  }\n \n-    return table;\n+  @Override\n+  public String extractCatalog(CaseInsensitiveStringMap options) {\n+    return catalogAndIdentifier(options).catalog().name();\n   }\n \n-  private static void mergeIcebergHadoopConfs(Configuration baseConf, Map<String, String> options) {\n-    options.keySet().stream()\n-        .filter(key -> key.startsWith(\"hadoop.\"))\n-        .forEach(key -> baseConf.set(key.replaceFirst(\"hadoop.\", \"\"), options.get(key)));\n+  private static void setupDefaultSparkCatalog() {\n+    SparkSession spark = SparkSession.active();", "originalCommit": "3ed51bd68b10fe2fcced2b231f28499b69f7cbf5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDgzMDg4Mg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r540830882", "bodyText": "I should have too. Fixed :-)", "author": "rymurr", "createdAt": "2020-12-11T10:03:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDYwMDI3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDYwMjIwMw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r540602203", "bodyText": "There's no need to catch ParseException any more. A new version of catalogAndIdentifer was added with context to form the error message. This could be:\nSpark3Util.CatalogAndIdentifier catalogAndIdentifier = Spark3Util.catalogAndIdentifier(\"identifier\", spark, path);", "author": "rdblue", "createdAt": "2020-12-11T00:32:20Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +77,77 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    Spark3Util.CatalogAndIdentifier catalogIdentifier = catalogAndIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogPlugin catalog = catalogIdentifier.catalog();\n+    Identifier ident = catalogIdentifier.identifier();\n+\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Spark3Util.CatalogAndIdentifier catalogAndIdentifier(CaseInsensitiveStringMap options) {\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n+    setupDefaultSparkCatalog();\n     String path = options.get(\"path\");\n+    SparkSession spark = SparkSession.active();\n+    CatalogManager catalogManager = spark.sessionState().catalogManager();\n \n     if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n-    } else {\n-      HiveCatalog hiveCatalog = HiveCatalogs.loadCatalog(conf);\n-      TableIdentifier tableIdentifier = TableIdentifier.parse(path);\n-      return hiveCatalog.loadTable(tableIdentifier);\n+      // contains a path. Return iceberg default catalog and a PathIdentifier\n+      return new Spark3Util.CatalogAndIdentifier(catalogManager.catalog(DEFAULT_CATALOG_NAME),\n+          new PathIdentifier(path));\n     }\n-  }\n \n-  private Table getTableAndResolveHadoopConfiguration(Map<String, String> options, Configuration conf) {\n-    // Overwrite configurations from the Spark Context with configurations from the options.\n-    mergeIcebergHadoopConfs(conf, options);\n-\n-    Table table = findTable(options, conf);\n+    final Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n+    try {\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, path);\n+    } catch (ParseException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot parse path %s. It is not a valid SQL table\", path), e);", "originalCommit": "3ed51bd68b10fe2fcced2b231f28499b69f7cbf5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDgzMzU4NA==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r540833584", "bodyText": "nice, thanks to @RussellSpitzer! I added another method there to extract the default catalog w/o passing it in.", "author": "rymurr", "createdAt": "2020-12-11T10:07:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDYwMjIwMw=="}], "type": "inlineReview"}, {"oid": "ed421acd7e54a6a88ebc429fc8b3d068c5e1a196", "url": "https://github.com/apache/iceberg/commit/ed421acd7e54a6a88ebc429fc8b3d068c5e1a196", "message": "minor fixes and simplification", "committedDate": "2020-12-11T10:10:46Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTExNTkwMw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r541115903", "bodyText": "The \"Cannot parse\" part is filled in automatically, so you just need to pass the name of the arg. Probably \"path or identifier\".", "author": "rdblue", "createdAt": "2020-12-11T17:40:31Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -56,48 +76,72 @@ public boolean supportsExternalMetadata() {\n   }\n \n   @Override\n-  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n-    // Get Iceberg table from options\n-    Configuration conf = SparkSession.active().sessionState().newHadoopConf();\n-    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n-\n-    // Build Spark table based on Iceberg table, and return it\n-    // Eagerly refresh the table before reading to ensure views containing this table show up-to-date data\n-    return new SparkTable(icebergTable, schema, true);\n+  public Table getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    Spark3Util.CatalogAndIdentifier catalogIdentifier = catalogAndIdentifier(new CaseInsensitiveStringMap(options));\n+    CatalogPlugin catalog = catalogIdentifier.catalog();\n+    Identifier ident = catalogIdentifier.identifier();\n+\n+    try {\n+      if (catalog instanceof TableCatalog) {\n+        return ((TableCatalog) catalog).loadTable(ident);\n+      }\n+    } catch (NoSuchTableException e) {\n+      // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(e, \"Cannot find table for %s.\", ident);\n+    }\n+\n+    // throwing an iceberg NoSuchTableException because the Spark one is typed and cant be thrown from this interface\n+    throw new org.apache.iceberg.exceptions.NoSuchTableException(\"Cannot find table for %s.\", ident);\n   }\n \n-  protected Table findTable(Map<String, String> options, Configuration conf) {\n+  private Spark3Util.CatalogAndIdentifier catalogAndIdentifier(CaseInsensitiveStringMap options) {\n     Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n+    SparkSession spark = SparkSession.active();\n+    setupDefaultSparkCatalog(spark);\n     String path = options.get(\"path\");\n+    CatalogManager catalogManager = spark.sessionState().catalogManager();\n \n     if (path.contains(\"/\")) {\n-      HadoopTables tables = new HadoopTables(conf);\n-      return tables.load(path);\n-    } else {\n-      HiveCatalog hiveCatalog = HiveCatalogs.loadCatalog(conf);\n-      TableIdentifier tableIdentifier = TableIdentifier.parse(path);\n-      return hiveCatalog.loadTable(tableIdentifier);\n+      // contains a path. Return iceberg default catalog and a PathIdentifier\n+      return new Spark3Util.CatalogAndIdentifier(catalogManager.catalog(DEFAULT_CATALOG_NAME),\n+          new PathIdentifier(path));\n     }\n-  }\n \n-  private Table getTableAndResolveHadoopConfiguration(Map<String, String> options, Configuration conf) {\n-    // Overwrite configurations from the Spark Context with configurations from the options.\n-    mergeIcebergHadoopConfs(conf, options);\n+    final Spark3Util.CatalogAndIdentifier catalogAndIdentifier = Spark3Util.catalogAndIdentifier(\n+        \"Cannot parse path %s. It is not a valid SQL table\", spark, path);", "originalCommit": "ed421acd7e54a6a88ebc429fc8b3d068c5e1a196", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTExODI5OQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r541118299", "bodyText": "\ud83e\udd26 fixed", "author": "rymurr", "createdAt": "2020-12-11T17:44:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTExNTkwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTEyMTI5Nw==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r541121297", "bodyText": "That was fast!", "author": "rdblue", "createdAt": "2020-12-11T17:49:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTExNTkwMw=="}], "type": "inlineReview"}, {"oid": "d8aada53d5b15833d49fe452021215b7e35ec0e3", "url": "https://github.com/apache/iceberg/commit/d8aada53d5b15833d49fe452021215b7e35ec0e3", "message": "log line fix", "committedDate": "2020-12-11T17:45:21Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTI5MjYzMQ==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r541292631", "bodyText": "Maybe important to note that these are used with this priority as well, ie\nif \"catalog.namespace.table\" is valid it will be read and \"namespace.namespace.table\" will be ignored ?", "author": "RussellSpitzer", "createdAt": "2020-12-11T21:07:59Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -20,21 +20,41 @@\n package org.apache.iceberg.spark.source;\n \n import java.util.Map;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.iceberg.Table;\n-import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.hadoop.HadoopTables;\n-import org.apache.iceberg.hive.HiveCatalog;\n-import org.apache.iceberg.hive.HiveCatalogs;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.PathIdentifier;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n import org.apache.spark.sql.SparkSession;\n-import org.apache.spark.sql.connector.catalog.TableProvider;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.CatalogManager;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.SupportsCatalogOptions;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n import org.apache.spark.sql.connector.expressions.Transform;\n import org.apache.spark.sql.sources.DataSourceRegister;\n import org.apache.spark.sql.types.StructType;\n import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n \n-public class IcebergSource implements DataSourceRegister, TableProvider {\n+/**\n+ * The IcebergSource loads/writes tables with format \"iceberg\". It can load paths and tables.\n+ *\n+ * How paths/tables are loaded when using spark.read().format(\"iceberg\").path(table)\n+ *\n+ *  table = \"file:/path/to/table\" -> loads a HadoopTable at given path", "originalCommit": "d8aada53d5b15833d49fe452021215b7e35ec0e3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjQyODc0Mg==", "url": "https://github.com/apache/iceberg/pull/1783#discussion_r542428742", "bodyText": "very good point @RussellSpitzer, I have included a note to that effect", "author": "rymurr", "createdAt": "2020-12-14T14:31:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTI5MjYzMQ=="}], "type": "inlineReview"}, {"oid": "409b1ad069f7723fd083d98c1bd6babe2f607395", "url": "https://github.com/apache/iceberg/commit/409b1ad069f7723fd083d98c1bd6babe2f607395", "message": "straw man custom catalog from `IcebergSource`", "committedDate": "2020-12-22T16:45:38Z", "type": "commit"}, {"oid": "d45ee4cc6f0e2c198e192d33e2bf85c87c584de1", "url": "https://github.com/apache/iceberg/commit/d45ee4cc6f0e2c198e192d33e2bf85c87c584de1", "message": "IcebergSource now uses SupportsCatalogOptions\n\nIt compiles and mostly works but SessionCatalog isn't set anywhere.\n\nHave to change a lot of the Spark3 tests to register a catalog now", "committedDate": "2020-12-22T16:45:39Z", "type": "commit"}, {"oid": "239c2e72f7424fed4c77c66e82ac8ee86aaff3d3", "url": "https://github.com/apache/iceberg/commit/239c2e72f7424fed4c77c66e82ac8ee86aaff3d3", "message": "start fixing tests", "committedDate": "2020-12-22T16:45:40Z", "type": "commit"}, {"oid": "581638cded5a197447024eecf2970f2418b60eb2", "url": "https://github.com/apache/iceberg/commit/581638cded5a197447024eecf2970f2418b60eb2", "message": "tests all fixed", "committedDate": "2020-12-22T16:45:41Z", "type": "commit"}, {"oid": "ef262111680150957ce4d24f67690045df53ddf4", "url": "https://github.com/apache/iceberg/commit/ef262111680150957ce4d24f67690045df53ddf4", "message": "source fixes", "committedDate": "2020-12-22T16:45:41Z", "type": "commit"}, {"oid": "2a8bac3dfcfa01b0eeed7cb260bfc21653caaf2f", "url": "https://github.com/apache/iceberg/commit/2a8bac3dfcfa01b0eeed7cb260bfc21653caaf2f", "message": "updates based on code review", "committedDate": "2020-12-22T16:45:42Z", "type": "commit"}, {"oid": "c39f108ed03c06f8381c2569ec9e05f20e1643df", "url": "https://github.com/apache/iceberg/commit/c39f108ed03c06f8381c2569ec9e05f20e1643df", "message": "clean up based on code review", "committedDate": "2020-12-22T16:45:43Z", "type": "commit"}, {"oid": "50808b7e5f3bdd4e16a2a630e68647c4beee9394", "url": "https://github.com/apache/iceberg/commit/50808b7e5f3bdd4e16a2a630e68647c4beee9394", "message": "comment explaining exception type", "committedDate": "2020-12-22T16:45:44Z", "type": "commit"}, {"oid": "8ebe3c9db9c9efdb188a2b8fbb1644e9629564c9", "url": "https://github.com/apache/iceberg/commit/8ebe3c9db9c9efdb188a2b8fbb1644e9629564c9", "message": "clean up and docs", "committedDate": "2020-12-22T16:45:45Z", "type": "commit"}, {"oid": "20e9d218fb43411fa5766a26281bffdad8435ea3", "url": "https://github.com/apache/iceberg/commit/20e9d218fb43411fa5766a26281bffdad8435ea3", "message": "use util function to get catalog and identifier", "committedDate": "2020-12-22T16:45:46Z", "type": "commit"}, {"oid": "806e92ac97ef7ecbb002647d176b0434609cc8be", "url": "https://github.com/apache/iceberg/commit/806e92ac97ef7ecbb002647d176b0434609cc8be", "message": "rebase to pick up #1843 and fix build", "committedDate": "2020-12-22T16:45:47Z", "type": "commit"}, {"oid": "4821282b25229bf82445a8b01c7bca92ebe1acaf", "url": "https://github.com/apache/iceberg/commit/4821282b25229bf82445a8b01c7bca92ebe1acaf", "message": "start a custom catalog if no iceberg catalogs are available", "committedDate": "2020-12-22T16:45:48Z", "type": "commit"}, {"oid": "04ce0069aeb9ce0e88f4e9e9fa5fc9560ad6cc12", "url": "https://github.com/apache/iceberg/commit/04ce0069aeb9ce0e88f4e9e9fa5fc9560ad6cc12", "message": "clean up", "committedDate": "2020-12-22T16:45:49Z", "type": "commit"}, {"oid": "ca249bc2770c997df29f322cc544085cf1e221a6", "url": "https://github.com/apache/iceberg/commit/ca249bc2770c997df29f322cc544085cf1e221a6", "message": "address code review and simplify default catalog logic", "committedDate": "2020-12-22T16:45:50Z", "type": "commit"}, {"oid": "4a38352a144ee806cb3118e27fb4a353b3f76ca0", "url": "https://github.com/apache/iceberg/commit/4a38352a144ee806cb3118e27fb4a353b3f76ca0", "message": "clean up based on code review", "committedDate": "2020-12-22T16:45:51Z", "type": "commit"}, {"oid": "c9b6483d078188a7993fbcb0f7d3be9d5dbe20f7", "url": "https://github.com/apache/iceberg/commit/c9b6483d078188a7993fbcb0f7d3be9d5dbe20f7", "message": "minor fixes and simplification", "committedDate": "2020-12-22T16:45:52Z", "type": "commit"}, {"oid": "b8c2320225fec647416249fed3dcc6bef788bc1e", "url": "https://github.com/apache/iceberg/commit/b8c2320225fec647416249fed3dcc6bef788bc1e", "message": "log line fix", "committedDate": "2020-12-22T16:45:53Z", "type": "commit"}, {"oid": "a4c08d1c538e5ecef17fa1e551cc7d5bf2dfd051", "url": "https://github.com/apache/iceberg/commit/a4c08d1c538e5ecef17fa1e551cc7d5bf2dfd051", "message": "log line fix", "committedDate": "2020-12-22T16:45:53Z", "type": "commit"}, {"oid": "a4c08d1c538e5ecef17fa1e551cc7d5bf2dfd051", "url": "https://github.com/apache/iceberg/commit/a4c08d1c538e5ecef17fa1e551cc7d5bf2dfd051", "message": "log line fix", "committedDate": "2020-12-22T16:45:53Z", "type": "forcePushed"}]}