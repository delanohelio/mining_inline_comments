{"pr_number": 750, "pr_title": "Add time-travel methods (asOfTime,useSnapshot) in IcebergGenerics", "pr_createdAt": "2020-01-26T10:28:56Z", "pr_url": "https://github.com/apache/iceberg/pull/750", "timeline": [{"oid": "6abbe497bf35aadf32f03f6e023469c20dd344a1", "url": "https://github.com/apache/iceberg/commit/6abbe497bf35aadf32f03f6e023469c20dd344a1", "message": "+ time travel methods (asOfTime,useSnapshot) in IcebergGenerics", "committedDate": "2020-01-24T21:22:58Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM3MjY2OQ==", "url": "https://github.com/apache/iceberg/pull/750#discussion_r371372669", "bodyText": "Instead of using ExpectedException, we use AssertHelpers.assertThrows. That also supports matching the contents of the thrown exception's message.", "author": "rdblue", "createdAt": "2020-01-27T17:18:11Z", "path": "data/src/test/java/org/apache/iceberg/data/TestLocalScan.java", "diffHunk": "@@ -215,28 +275,90 @@ public void testFullScan() {\n     Iterable<Record> results = IcebergGenerics.read(sharedTable).build();\n \n     Set<Record> expected = Sets.newHashSet();\n-    expected.addAll(file1Records);\n-    expected.addAll(file2Records);\n-    expected.addAll(file3Records);\n+    expected.addAll(file1SecondSnapshotRecords);\n+    expected.addAll(file2SecondSnapshotRecords);\n+    expected.addAll(file3SecondSnapshotRecords);\n+\n+    Set<Record> records = Sets.newHashSet(results);\n+    Assert.assertEquals(\"Should produce correct number of records\",\n+        expected.size(), records.size());\n+    Assert.assertEquals(\"Random record set should match\",\n+        Sets.newHashSet(expected), records);\n+    Assert.assertNotNull(records.stream().findFirst().get().getField(\"id\"));\n+    Assert.assertNotNull(records.stream().findFirst().get().getField(\"data\"));\n+  }\n+\n+  @Test\n+  public void testUnknownSnapshotId() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(startsWith(\"Cannot find snapshot with ID \"));", "originalCommit": "6abbe497bf35aadf32f03f6e023469c20dd344a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM3Mzg4Ng==", "url": "https://github.com/apache/iceberg/pull/750#discussion_r371373886", "bodyText": "The TableScan API is a refinement API, like Spark's data frames. When you filter, you get a new scan that has the new filter applied with the existing scan's filters. So there's no need to use Expressions.and. In fact, using and with true and another filter will just return the filter.", "author": "rdblue", "createdAt": "2020-01-27T17:20:32Z", "path": "data/src/main/java/org/apache/iceberg/data/IcebergGenerics.java", "diffHunk": "@@ -56,27 +61,33 @@ public ScanBuilder reuseContainers() {\n     }\n \n     public ScanBuilder where(Expression rowFilter) {\n-      this.where = Expressions.and(where, rowFilter);\n+      this.tableScan = this.tableScan.filter(Expressions.and(defaultWhere, rowFilter));", "originalCommit": "6abbe497bf35aadf32f03f6e023469c20dd344a1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTQyNDAzNA==", "url": "https://github.com/apache/iceberg/pull/750#discussion_r371424034", "bodyText": "Seems like we can either keeping refining the tableScan obj in each of the builder methods in which case we might not need to have instance fields like where, caseSensitive etc and we should delete them where possible. Or we create the tablescan once in the build() method in which case we will need to keep our own instance fields, but then we should remove the tablescan refine from each fo the build methods. Either approach is fine by me.", "author": "rdsr", "createdAt": "2020-01-27T19:03:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM3Mzg4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTQ5MTI4MQ==", "url": "https://github.com/apache/iceberg/pull/750#discussion_r371491281", "bodyText": "Agreed. I just don't think we should have two sets of scan defaults.", "author": "rdblue", "createdAt": "2020-01-27T21:26:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM3Mzg4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM3Mzk1Mw==", "url": "https://github.com/apache/iceberg/pull/750#discussion_r371373953", "bodyText": "Instead of having defaults here as well as in the table scan, this should just create a new scan and rely on its defaults.", "author": "rdblue", "createdAt": "2020-01-27T17:20:38Z", "path": "data/src/main/java/org/apache/iceberg/data/IcebergGenerics.java", "diffHunk": "@@ -41,13 +42,17 @@ public static ScanBuilder read(Table table) {\n \n   public static class ScanBuilder {\n     private final Table table;\n-    private Expression where = Expressions.alwaysTrue();\n-    private List<String> columns = ImmutableList.of(\"*\");\n+    private TableScan tableScan;\n+    private final Expression defaultWhere = Expressions.alwaysTrue();\n+    private final List<String> defaultColumns = ImmutableList.of(\"*\");\n     private boolean reuseContainers = false;\n-    private boolean caseSensitive = true;\n+    private final boolean defaultCaseSensitive = true;\n \n     public ScanBuilder(Table table) {\n       this.table = table;\n+      this.tableScan = table.newScan()\n+          .select(this.defaultColumns)\n+          .caseSensitive(this.defaultCaseSensitive);", "originalCommit": "6abbe497bf35aadf32f03f6e023469c20dd344a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM3NDY3OQ==", "url": "https://github.com/apache/iceberg/pull/750#discussion_r371374679", "bodyText": "We only use this. when assigning to an instance field, so that it is clear from that line that the assignment is not to a local variable. We don't use this. when accessing the variable. Can you remove those?", "author": "rdblue", "createdAt": "2020-01-27T17:21:51Z", "path": "data/src/main/java/org/apache/iceberg/data/IcebergGenerics.java", "diffHunk": "@@ -56,27 +61,33 @@ public ScanBuilder reuseContainers() {\n     }\n \n     public ScanBuilder where(Expression rowFilter) {\n-      this.where = Expressions.and(where, rowFilter);\n+      this.tableScan = this.tableScan.filter(Expressions.and(defaultWhere, rowFilter));\n       return this;\n     }\n \n     public ScanBuilder caseInsensitive() {\n-      this.caseSensitive = false;\n+      this.tableScan = this.tableScan.caseSensitive(false);\n       return this;\n     }\n \n     public ScanBuilder select(String... selectedColumns) {\n-      this.columns = ImmutableList.copyOf(selectedColumns);\n+      this.tableScan = this.tableScan.select(ImmutableList.copyOf(selectedColumns));\n+      return this;\n+    }\n+\n+    public ScanBuilder useSnapshot(long scanSnapshotId) {\n+      this.tableScan = this.tableScan.useSnapshot(scanSnapshotId);\n+      return this;\n+    }\n+\n+    public ScanBuilder asOfTime(long scanTimestampMillis) {\n+      this.tableScan = this.tableScan.asOfTime(scanTimestampMillis);\n       return this;\n     }\n \n     public Iterable<Record> build() {\n       return new TableScanIterable(\n-        table\n-          .newScan()\n-          .filter(where)\n-          .caseSensitive(caseSensitive)\n-          .select(columns),\n+        this.tableScan,", "originalCommit": "6abbe497bf35aadf32f03f6e023469c20dd344a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM3NDkzOQ==", "url": "https://github.com/apache/iceberg/pull/750#discussion_r371374939", "bodyText": "Minor: using Iterables rather than streams is less verbose: Iterables.get(set, 0).getField(\"id\")", "author": "rdblue", "createdAt": "2020-01-27T17:22:20Z", "path": "data/src/test/java/org/apache/iceberg/data/TestLocalScan.java", "diffHunk": "@@ -215,28 +275,90 @@ public void testFullScan() {\n     Iterable<Record> results = IcebergGenerics.read(sharedTable).build();\n \n     Set<Record> expected = Sets.newHashSet();\n-    expected.addAll(file1Records);\n-    expected.addAll(file2Records);\n-    expected.addAll(file3Records);\n+    expected.addAll(file1SecondSnapshotRecords);\n+    expected.addAll(file2SecondSnapshotRecords);\n+    expected.addAll(file3SecondSnapshotRecords);\n+\n+    Set<Record> records = Sets.newHashSet(results);\n+    Assert.assertEquals(\"Should produce correct number of records\",\n+        expected.size(), records.size());\n+    Assert.assertEquals(\"Random record set should match\",\n+        Sets.newHashSet(expected), records);\n+    Assert.assertNotNull(records.stream().findFirst().get().getField(\"id\"));\n+    Assert.assertNotNull(records.stream().findFirst().get().getField(\"data\"));\n+  }\n+\n+  @Test\n+  public void testUnknownSnapshotId() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(startsWith(\"Cannot find snapshot with ID \"));\n+\n+    Long minSnapshotId = sharedTable.history().stream().map(h -> h.snapshotId()).min(Long::compareTo).get();\n+\n+    IcebergGenerics.read(sharedTable)\n+        .useSnapshot(/* unknown snapshot id */ minSnapshotId - 1);\n+  }\n+\n+  @Test\n+  public void testAsOfTimeOlderThanFirstSnapshot() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(startsWith(\"Cannot find a snapshot older than \"));\n+\n+    IcebergGenerics.read(sharedTable)\n+        .asOfTime(/* older than first snapshot */ sharedTable.history().get(0).timestampMillis() - 1);\n+  }\n+\n+  @Test\n+  public void testUseSnapshot() {\n+    Iterable<Record> results = IcebergGenerics.read(sharedTable)\n+        .useSnapshot(/* first snapshot */ sharedTable.history().get(0).snapshotId())\n+        .build();\n+\n+    Set<Record> expected = Sets.newHashSet();\n+    expected.addAll(file1FirstSnapshotRecords);\n+    expected.addAll(file2FirstSnapshotRecords);\n+    expected.addAll(file3FirstSnapshotRecords);\n+\n+    Set<Record> records = Sets.newHashSet(results);\n+    Assert.assertEquals(\"Should produce correct number of records\",\n+        expected.size(), records.size());\n+    Assert.assertEquals(\"Random record set should match\",\n+        Sets.newHashSet(expected), records);\n+    Assert.assertNotNull(records.stream().findFirst().get().getField(\"id\"));\n+    Assert.assertNotNull(records.stream().findFirst().get().getField(\"data\"));\n+  }\n+\n+  @Test\n+  public void testAsOfTime() {\n+    Iterable<Record> results = IcebergGenerics.read(sharedTable)\n+        .asOfTime(/* timestamp first snapshot */ sharedTable.history().get(0).timestampMillis())\n+        .build();\n+\n+    Set<Record> expected = Sets.newHashSet();\n+    expected.addAll(file1FirstSnapshotRecords);\n+    expected.addAll(file2FirstSnapshotRecords);\n+    expected.addAll(file3FirstSnapshotRecords);\n \n     Set<Record> records = Sets.newHashSet(results);\n     Assert.assertEquals(\"Should produce correct number of records\",\n         expected.size(), records.size());\n     Assert.assertEquals(\"Random record set should match\",\n         Sets.newHashSet(expected), records);\n+    Assert.assertNotNull(records.stream().findFirst().get().getField(\"id\"));", "originalCommit": "6abbe497bf35aadf32f03f6e023469c20dd344a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM3NTgwNQ==", "url": "https://github.com/apache/iceberg/pull/750#discussion_r371375805", "bodyText": "Continuation indents should be 2 indents = 4 spaces, not 8 spaces.", "author": "rdblue", "createdAt": "2020-01-27T17:23:55Z", "path": "data/src/test/java/org/apache/iceberg/data/TestLocalScan.java", "diffHunk": "@@ -114,53 +122,105 @@ public void createTables() throws IOException {\n \n     Record record = GenericRecord.create(SCHEMA);\n \n-    this.file1Records = Lists.newArrayList(\n+    this.file1FirstSnapshotRecords = Lists.newArrayList(\n+            record.copy(ImmutableMap.of(\"id\", 4L, \"data\", \"obscure\")),", "originalCommit": "6abbe497bf35aadf32f03f6e023469c20dd344a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM3Nzg0OQ==", "url": "https://github.com/apache/iceberg/pull/750#discussion_r371377849", "bodyText": "Instead of hard-coding the metrics for each data file, I think that this PR should update writeFile to return a DataFile instance. That method can call appender.metrics() to get the metrics when creating the DataFile, which would ensure they are always correct. It would also make this setup method much shorter.", "author": "rdblue", "createdAt": "2020-01-27T17:27:59Z", "path": "data/src/test/java/org/apache/iceberg/data/TestLocalScan.java", "diffHunk": "@@ -114,53 +122,105 @@ public void createTables() throws IOException {\n \n     Record record = GenericRecord.create(SCHEMA);\n \n-    this.file1Records = Lists.newArrayList(\n+    this.file1FirstSnapshotRecords = Lists.newArrayList(\n+            record.copy(ImmutableMap.of(\"id\", 4L, \"data\", \"obscure\")),\n+            record.copy(ImmutableMap.of(\"id\", 5L, \"data\", \"secure\")),\n+            record.copy(ImmutableMap.of(\"id\", 6L, \"data\", \"fetta\"))\n+    );\n+    InputFile file11 = writeFile(sharedTableLocation, format.addExtension(\"file-11\"), file1FirstSnapshotRecords);\n+\n+    this.file2FirstSnapshotRecords = Lists.newArrayList(\n+            record.copy(ImmutableMap.of(\"id\", 14L, \"data\", \"radical\")),\n+            record.copy(ImmutableMap.of(\"id\", 15L, \"data\", \"collocation\")),\n+            record.copy(ImmutableMap.of(\"id\", 16L, \"data\", \"book\"))\n+    );\n+    InputFile file21 = writeFile(sharedTableLocation, format.addExtension(\"file-21\"), file2FirstSnapshotRecords);\n+\n+    this.file3FirstSnapshotRecords = Lists.newArrayList(\n+            record.copy(ImmutableMap.of(\"id\", 24L, \"data\", \"cloud\")),\n+            record.copy(ImmutableMap.of(\"id\", 25L, \"data\", \"zen\")),\n+            record.copy(ImmutableMap.of(\"id\", 26L, \"data\", \"sky\"))\n+    );\n+    InputFile file31 = writeFile(sharedTableLocation, format.addExtension(\"file-31\"), file3FirstSnapshotRecords);\n+\n+    this.file1SecondSnapshotRecords = Lists.newArrayList(\n         record.copy(ImmutableMap.of(\"id\", 0L, \"data\", \"clarification\")),\n         record.copy(ImmutableMap.of(\"id\", 1L, \"data\", \"risky\")),\n         record.copy(ImmutableMap.of(\"id\", 2L, \"data\", \"falafel\"))\n     );\n-    InputFile file1 = writeFile(sharedTableLocation, format.addExtension(\"file-1\"), file1Records);\n+    InputFile file12 = writeFile(sharedTableLocation, format.addExtension(\"file-12\"), file1SecondSnapshotRecords);\n \n     Record nullData = record.copy();\n     nullData.setField(\"id\", 11L);\n     nullData.setField(\"data\", null);\n \n-    this.file2Records = Lists.newArrayList(\n+    this.file2SecondSnapshotRecords = Lists.newArrayList(\n         record.copy(ImmutableMap.of(\"id\", 10L, \"data\", \"clammy\")),\n         record.copy(ImmutableMap.of(\"id\", 11L, \"data\", \"evacuate\")),\n         record.copy(ImmutableMap.of(\"id\", 12L, \"data\", \"tissue\"))\n     );\n-    InputFile file2 = writeFile(sharedTableLocation, format.addExtension(\"file-2\"), file2Records);\n+    InputFile file22 = writeFile(sharedTableLocation, format.addExtension(\"file-22\"), file2SecondSnapshotRecords);\n \n-    this.file3Records = Lists.newArrayList(\n+    this.file3SecondSnapshotRecords = Lists.newArrayList(\n         record.copy(ImmutableMap.of(\"id\", 20L, \"data\", \"ocean\")),\n         record.copy(ImmutableMap.of(\"id\", 21L, \"data\", \"holistic\")),\n         record.copy(ImmutableMap.of(\"id\", 22L, \"data\", \"preventative\"))\n     );\n-    InputFile file3 = writeFile(sharedTableLocation, format.addExtension(\"file-3\"), file3Records);\n+    InputFile file32 = writeFile(sharedTableLocation, format.addExtension(\"file-32\"), file3SecondSnapshotRecords);\n \n     // commit the test data\n     sharedTable.newAppend()\n         .appendFile(DataFiles.builder(PartitionSpec.unpartitioned())\n-            .withInputFile(file1)\n+            .withInputFile(file11)\n+            .withMetrics(new Metrics(3L,", "originalCommit": "6abbe497bf35aadf32f03f6e023469c20dd344a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM3OTE0MA==", "url": "https://github.com/apache/iceberg/pull/750#discussion_r371379140", "bodyText": "Looks like a copy/paste error. This isn't a random record set. Can you update this to \"Record set should match\"?", "author": "rdblue", "createdAt": "2020-01-27T17:30:26Z", "path": "data/src/test/java/org/apache/iceberg/data/TestLocalScan.java", "diffHunk": "@@ -215,28 +275,90 @@ public void testFullScan() {\n     Iterable<Record> results = IcebergGenerics.read(sharedTable).build();\n \n     Set<Record> expected = Sets.newHashSet();\n-    expected.addAll(file1Records);\n-    expected.addAll(file2Records);\n-    expected.addAll(file3Records);\n+    expected.addAll(file1SecondSnapshotRecords);\n+    expected.addAll(file2SecondSnapshotRecords);\n+    expected.addAll(file3SecondSnapshotRecords);\n+\n+    Set<Record> records = Sets.newHashSet(results);\n+    Assert.assertEquals(\"Should produce correct number of records\",\n+        expected.size(), records.size());\n+    Assert.assertEquals(\"Random record set should match\",\n+        Sets.newHashSet(expected), records);\n+    Assert.assertNotNull(records.stream().findFirst().get().getField(\"id\"));\n+    Assert.assertNotNull(records.stream().findFirst().get().getField(\"data\"));\n+  }\n+\n+  @Test\n+  public void testUnknownSnapshotId() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(startsWith(\"Cannot find snapshot with ID \"));\n+\n+    Long minSnapshotId = sharedTable.history().stream().map(h -> h.snapshotId()).min(Long::compareTo).get();\n+\n+    IcebergGenerics.read(sharedTable)\n+        .useSnapshot(/* unknown snapshot id */ minSnapshotId - 1);\n+  }\n+\n+  @Test\n+  public void testAsOfTimeOlderThanFirstSnapshot() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(startsWith(\"Cannot find a snapshot older than \"));\n+\n+    IcebergGenerics.read(sharedTable)\n+        .asOfTime(/* older than first snapshot */ sharedTable.history().get(0).timestampMillis() - 1);\n+  }\n+\n+  @Test\n+  public void testUseSnapshot() {\n+    Iterable<Record> results = IcebergGenerics.read(sharedTable)\n+        .useSnapshot(/* first snapshot */ sharedTable.history().get(0).snapshotId())\n+        .build();\n+\n+    Set<Record> expected = Sets.newHashSet();\n+    expected.addAll(file1FirstSnapshotRecords);\n+    expected.addAll(file2FirstSnapshotRecords);\n+    expected.addAll(file3FirstSnapshotRecords);\n+\n+    Set<Record> records = Sets.newHashSet(results);\n+    Assert.assertEquals(\"Should produce correct number of records\",\n+        expected.size(), records.size());\n+    Assert.assertEquals(\"Random record set should match\",", "originalCommit": "6abbe497bf35aadf32f03f6e023469c20dd344a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM4MTAxNg==", "url": "https://github.com/apache/iceberg/pull/750#discussion_r371381016", "bodyText": "The table setup is done in a @Before method, so the table is recreated for each test case. Since the first snapshot is only used by this test case and testAsOfTime, you might consider moving the overwrite into a helper method. That would require fewer changes to existing test methods.\nIf you did that, then this would start with overwriteExistingData() and then you could test that you can read the old data or the overwrite snapshot.", "author": "rdblue", "createdAt": "2020-01-27T17:33:59Z", "path": "data/src/test/java/org/apache/iceberg/data/TestLocalScan.java", "diffHunk": "@@ -215,28 +275,90 @@ public void testFullScan() {\n     Iterable<Record> results = IcebergGenerics.read(sharedTable).build();\n \n     Set<Record> expected = Sets.newHashSet();\n-    expected.addAll(file1Records);\n-    expected.addAll(file2Records);\n-    expected.addAll(file3Records);\n+    expected.addAll(file1SecondSnapshotRecords);\n+    expected.addAll(file2SecondSnapshotRecords);\n+    expected.addAll(file3SecondSnapshotRecords);\n+\n+    Set<Record> records = Sets.newHashSet(results);\n+    Assert.assertEquals(\"Should produce correct number of records\",\n+        expected.size(), records.size());\n+    Assert.assertEquals(\"Random record set should match\",\n+        Sets.newHashSet(expected), records);\n+    Assert.assertNotNull(records.stream().findFirst().get().getField(\"id\"));\n+    Assert.assertNotNull(records.stream().findFirst().get().getField(\"data\"));\n+  }\n+\n+  @Test\n+  public void testUnknownSnapshotId() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(startsWith(\"Cannot find snapshot with ID \"));\n+\n+    Long minSnapshotId = sharedTable.history().stream().map(h -> h.snapshotId()).min(Long::compareTo).get();\n+\n+    IcebergGenerics.read(sharedTable)\n+        .useSnapshot(/* unknown snapshot id */ minSnapshotId - 1);\n+  }\n+\n+  @Test\n+  public void testAsOfTimeOlderThanFirstSnapshot() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(startsWith(\"Cannot find a snapshot older than \"));\n+\n+    IcebergGenerics.read(sharedTable)\n+        .asOfTime(/* older than first snapshot */ sharedTable.history().get(0).timestampMillis() - 1);\n+  }\n+\n+  @Test\n+  public void testUseSnapshot() {", "originalCommit": "6abbe497bf35aadf32f03f6e023469c20dd344a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "7be0b1665c9196052a6d6581f80b190a5741c5df", "url": "https://github.com/apache/iceberg/commit/7be0b1665c9196052a6d6581f80b190a5741c5df", "message": "PR review:\n- Use AssertHelpers.assertThrows instead of ExpectedException.\n- Rely on table.newScan defaults.\n- Don't use this. when accessing variables.\n- Get metrics from data (instead of hardcoding).\n- Add helper method overwriteExistingData instead of modifying createTables().", "committedDate": "2020-01-28T18:18:17Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjUxMDcxNw==", "url": "https://github.com/apache/iceberg/pull/750#discussion_r372510717", "bodyText": "Typo: should be parquetAppender.", "author": "rdblue", "createdAt": "2020-01-29T17:02:48Z", "path": "data/src/test/java/org/apache/iceberg/data/TestLocalScan.java", "diffHunk": "@@ -279,31 +323,107 @@ public void testProjectWithMissingFilterColumn() {\n         Sets.newHashSet(transform(results, record -> record.getField(\"data\").toString())));\n   }\n \n-  private InputFile writeFile(String location, String filename, List<Record> records) throws IOException {\n+  @Test\n+  public void testUseSnapshot() throws IOException {\n+    overwriteExistingData();\n+    Iterable<Record> results = IcebergGenerics.read(sharedTable)\n+        .useSnapshot(/* first snapshot */ sharedTable.history().get(1).snapshotId())\n+        .build();\n+\n+    Set<Record> expected = Sets.newHashSet();\n+    expected.addAll(file1FirstSnapshotRecords);\n+    expected.addAll(file2FirstSnapshotRecords);\n+    expected.addAll(file3FirstSnapshotRecords);\n+\n+    Set<Record> records = Sets.newHashSet(results);\n+    Assert.assertEquals(\"Should produce correct number of records\",\n+        expected.size(), records.size());\n+    Assert.assertEquals(\"Record set should match\",\n+        Sets.newHashSet(expected), records);\n+    Assert.assertNotNull(Iterables.get(records, 0).getField(\"id\"));\n+    Assert.assertNotNull(Iterables.get(records, 0).getField(\"data\"));\n+  }\n+\n+  @Test\n+  public void testAsOfTime() throws IOException {\n+    overwriteExistingData();\n+    Iterable<Record> results = IcebergGenerics.read(sharedTable)\n+        .asOfTime(/* timestamp first snapshot */ sharedTable.history().get(2).timestampMillis())\n+        .build();\n+\n+    Set<Record> expected = Sets.newHashSet();\n+    expected.addAll(file1SecondSnapshotRecords);\n+    expected.addAll(file2SecondSnapshotRecords);\n+    expected.addAll(file3SecondSnapshotRecords);\n+\n+    Set<Record> records = Sets.newHashSet(results);\n+    Assert.assertEquals(\"Should produce correct number of records\",\n+        expected.size(), records.size());\n+    Assert.assertEquals(\"Record set should match\",\n+        Sets.newHashSet(expected), records);\n+    Assert.assertNotNull(Iterables.get(records, 0).getField(\"id\"));\n+    Assert.assertNotNull(Iterables.get(records, 0).getField(\"data\"));\n+  }\n+\n+  @Test\n+  public void testUnknownSnapshotId() {\n+    Long minSnapshotId = sharedTable.history().stream().map(h -> h.snapshotId()).min(Long::compareTo).get();\n+\n+    IcebergGenerics.ScanBuilder scanBuilder = IcebergGenerics.read(sharedTable);\n+\n+    AssertHelpers.assertThrows(\"Should fail on unknown snapshot id\",\n+        IllegalArgumentException.class,\n+        \"Cannot find snapshot with ID \",\n+        () -> scanBuilder.useSnapshot(/* unknown snapshot id */ minSnapshotId - 1));\n+  }\n+\n+  @Test\n+  public void testAsOfTimeOlderThanFirstSnapshot() {\n+    IcebergGenerics.ScanBuilder scanBuilder = IcebergGenerics.read(sharedTable);\n+\n+    AssertHelpers.assertThrows(\"Should fail on timestamp sooner than first write\",\n+        IllegalArgumentException.class,\n+        \"Cannot find a snapshot older than \",\n+        () -> scanBuilder.asOfTime(/* older than first snapshot */ sharedTable.history().get(0).timestampMillis() - 1));\n+  }\n+\n+  private DataFile writeFile(String location, String filename, List<Record> records) throws IOException {\n     Path path = new Path(location, filename);\n     FileFormat fileFormat = FileFormat.fromFileName(filename);\n     Preconditions.checkNotNull(fileFormat, \"Cannot determine format for file: %s\", filename);\n     switch (fileFormat) {\n       case AVRO:\n-        try (FileAppender<Record> appender = Avro.write(fromPath(path, CONF))\n+        FileAppender avroAppender = Avro.write(fromPath(path, CONF))\n             .schema(SCHEMA)\n             .createWriterFunc(DataWriter::create)\n             .named(fileFormat.name())\n-            .build()) {\n-          appender.addAll(records);\n+            .build();\n+        try {\n+          avroAppender.addAll(records);\n+        } finally {\n+          avroAppender.close();\n         }\n \n-        return HadoopInputFile.fromPath(path, CONF);\n+        return DataFiles.builder(PartitionSpec.unpartitioned())\n+            .withInputFile(HadoopInputFile.fromPath(path, CONF))\n+            .withMetrics(avroAppender.metrics())\n+            .build();\n \n       case PARQUET:\n-        try (FileAppender<Record> appender = Parquet.write(fromPath(path, CONF))\n+        FileAppender<Record> orcAppender = Parquet.write(fromPath(path, CONF))", "originalCommit": "7be0b1665c9196052a6d6581f80b190a5741c5df", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}