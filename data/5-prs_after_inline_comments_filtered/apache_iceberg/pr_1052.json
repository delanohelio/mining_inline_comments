{"pr_number": 1052, "pr_title": "Fix RemoveOrphanFilesAction when file_path is not a qualified path", "pr_createdAt": "2020-05-21T09:39:53Z", "pr_url": "https://github.com/apache/iceberg/pull/1052", "timeline": [{"oid": "f6fd92f4c02c55da9245376ff4a941ce79c9fe74", "url": "https://github.com/apache/iceberg/commit/f6fd92f4c02c55da9245376ff4a941ce79c9fe74", "message": "Fix RemoveOrphanFilesAction when file_path is not a qualified path", "committedDate": "2020-05-21T08:34:36Z", "type": "commit"}, {"oid": "eaca67080c3db7f5473b7d1d9970115fa25ebf11", "url": "https://github.com/apache/iceberg/commit/eaca67080c3db7f5473b7d1d9970115fa25ebf11", "message": "add UT", "committedDate": "2020-05-21T09:24:08Z", "type": "commit"}, {"oid": "7f19edcf8a8d8e56aa20c0584461b802496bcc0a", "url": "https://github.com/apache/iceberg/commit/7f19edcf8a8d8e56aa20c0584461b802496bcc0a", "message": "Simplify UT", "committedDate": "2020-05-21T09:39:05Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODgzOTUzNg==", "url": "https://github.com/apache/iceberg/pull/1052#discussion_r428839536", "bodyText": "Isn't this going to cause Spark to use a nested loop join (full join) because there is no way to partition the data for this expression?\nTo fix it, what about using just the file name as well? File names should be unique because we embed the write UUID, partition, and task ID. And if we add both checks, filename could be used to distribute the data without many collisions and contains could be used for final correctness.\nColumn nameEqual = filename(actualFileDF.col(\"file_path\")).equals(filename(validFileDF.col(\"file_path\")));\nColumn actualContains = actualFileDF.col(\"file_path\").contains(validFileDF.col(\"file_path\"));\nColumn joinCond = nameEqual.and(actualContains);\nFYI @aokolnychyi.", "author": "rdblue", "createdAt": "2020-05-21T18:37:26Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RemoveOrphanFilesAction.java", "diffHunk": "@@ -141,7 +141,7 @@ public RemoveOrphanFilesAction deleteWith(Consumer<String> newDeleteFunc) {\n     Dataset<Row> validFileDF = validDataFileDF.union(validMetadataFileDF);\n     Dataset<Row> actualFileDF = buildActualFileDF();\n \n-    Column joinCond = validFileDF.col(\"file_path\").equalTo(actualFileDF.col(\"file_path\"));\n+    Column joinCond = actualFileDF.col(\"file_path\").contains(validFileDF.col(\"file_path\"));", "originalCommit": "7f19edcf8a8d8e56aa20c0584461b802496bcc0a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTAwNTQ1OQ==", "url": "https://github.com/apache/iceberg/pull/1052#discussion_r429005459", "bodyText": "Ok, I see. If the file name is unique, then I think it would be fine to change to this way. Let me update the code.", "author": "jerryshao", "createdAt": "2020-05-22T02:02:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODgzOTUzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTMyMTE5Mw==", "url": "https://github.com/apache/iceberg/pull/1052#discussion_r429321193", "bodyText": "Even if it isn't unique, we don't expect many duplicates because writers will be operating in parallel. And the worst case is all files have the same name and all get joined in a task -- which is pretty much the same as using a nested loop join.", "author": "rdblue", "createdAt": "2020-05-22T15:42:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODgzOTUzNg=="}], "type": "inlineReview"}, {"oid": "cecf38821b6fee7fd01c1ffb2e0d276553cef049", "url": "https://github.com/apache/iceberg/commit/cecf38821b6fee7fd01c1ffb2e0d276553cef049", "message": "Address the comments", "committedDate": "2020-05-22T05:17:14Z", "type": "commit"}]}