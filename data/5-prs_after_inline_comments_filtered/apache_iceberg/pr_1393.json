{"pr_number": 1393, "pr_title": "Flink: Support creating table and altering table in Flink SQL", "pr_createdAt": "2020-08-27T05:40:19Z", "pr_url": "https://github.com/apache/iceberg/pull/1393", "timeline": [{"oid": "b003b29513c8ef9cb95840db0291f5bd45aeb1de", "url": "https://github.com/apache/iceberg/commit/b003b29513c8ef9cb95840db0291f5bd45aeb1de", "message": "Flink: Support creating table and altering table in Flink SQL", "committedDate": "2020-08-27T05:39:05Z", "type": "commit"}, {"oid": "87f8c79f679aef109f0fce0fe55745e880948dca", "url": "https://github.com/apache/iceberg/commit/87f8c79f679aef109f0fce0fe55745e880948dca", "message": "Fix case", "committedDate": "2020-08-27T05:57:52Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI2ODc4NA==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r478268784", "bodyText": "nit: better to use ImmutableMap ?", "author": "openinx", "createdAt": "2020-08-27T09:04:58Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +338,158 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+    Map<String, String> options = Maps.newHashMap(table.getOptions());", "originalCommit": "87f8c79f679aef109f0fce0fe55745e880948dca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI3MTgxNA==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r478271814", "bodyText": "What's the reason that we could not support adding /removing/renaming column ?", "author": "openinx", "createdAt": "2020-08-27T09:10:10Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +338,158 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+    Map<String, String> options = Maps.newHashMap(table.getOptions());\n+\n+    try {\n+      icebergCatalog.createTable(\n+          toIdentifier(tablePath),\n+          icebergSchema,\n+          spec,\n+          options.get(\"location\"),\n+          options);\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n   }\n \n   @Override\n   public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support alterTable now.\");\n+      throws CatalogException, TableNotExistException {\n+    validateFlinkTable(newTable);\n+    Table icebergTable = getIcebergTable(tablePath);\n+    CatalogTable table = toCatalogTable(icebergTable);\n+\n+    // Currently, Flink SQL only support altering table properties.", "originalCommit": "87f8c79f679aef109f0fce0fe55745e880948dca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI3MzkxMQ==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r478273911", "bodyText": "No Flink DLL to add/removing/renaming column...", "author": "JingsongLi", "createdAt": "2020-08-27T09:13:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI3MTgxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NTI1Ng==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479565256", "bodyText": "I should also note that support for adding/removing/renaming columns cannot be done by comparing CatalogTable instances, unless the Flink schema contains Iceberg column IDs.\nThe problem is clear when you consider a simple example:\n\nIceberg table schema: id bigint, a float, b float\nFlink table schema: id bigint, x float, y float\n\nThere are two ways to get the Flink schema: rename a -> x and b -> y, or drop a, drop b, add x, add y. Guessing which one was intended by the user is not okay because it would corrupt data. If the values from a are read when projecting x after a was actually dropped, then this is a serious correctness bug.\nAlso note that there are some transformations that can't be detected. For example, drop a then add a. The result should be that all values of column a are discarded. This happens when the wrong data was written to a column but the column is still needed for newer data.", "author": "rdblue", "createdAt": "2020-08-28T22:50:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI3MTgxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTg2MDgxMA==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479860810", "bodyText": "Good point.\nIf there is only one operation in a single call, it seems feasible. And actually, in the SQL DDLs, only a single type is in a single SQL.\nBut yes, this API is unclear, if we look at it from the API level alone, there are too many possibilities...\nI'll add comments in the code.", "author": "JingsongLi", "createdAt": "2020-08-31T02:44:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI3MTgxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI4NDk5MA==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r478284990", "bodyText": "Q:  Does this align with the flink sql semantics ?\nI saw the document said: \"Set one or more properties in the specified table. If a particular property is already set in the table, override the old value with the new one.\"\nALTER TABLE [catalog_name.][db_name.]table_name SET (key1=val1, key2=val2, ...)\nFor the existing key-values  (in old table ) which don't appear in the new table,  should we remove them from old table ?  ( The document did not describe this case clearly,  just for confirmation).", "author": "openinx", "createdAt": "2020-08-27T09:32:18Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +338,158 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+    Map<String, String> options = Maps.newHashMap(table.getOptions());\n+\n+    try {\n+      icebergCatalog.createTable(\n+          toIdentifier(tablePath),\n+          icebergSchema,\n+          spec,\n+          options.get(\"location\"),\n+          options);\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n   }\n \n   @Override\n   public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support alterTable now.\");\n+      throws CatalogException, TableNotExistException {\n+    validateFlinkTable(newTable);\n+    Table icebergTable = getIcebergTable(tablePath);\n+    CatalogTable table = toCatalogTable(icebergTable);\n+\n+    // Currently, Flink SQL only support altering table properties.\n+\n+    if (!table.getSchema().equals(newTable.getSchema())) {\n+      throw new UnsupportedOperationException(\"Altering schema is not supported yet.\");\n+    }\n+\n+    if (!table.getPartitionKeys().equals(((CatalogTable) newTable).getPartitionKeys())) {\n+      throw new UnsupportedOperationException(\"Altering partition keys is not supported yet.\");\n+    }\n+\n+    Map<String, String> oldOptions = table.getOptions();\n+    Map<String, String> setProperties = Maps.newHashMap();\n+\n+    String setLocation = null;\n+    String setSnapshotId = null;\n+    String pickSnapshotId = null;\n+\n+    for (Map.Entry<String, String> entry : newTable.getOptions().entrySet()) {\n+      String key = entry.getKey();\n+      String value = entry.getValue();\n+\n+      if (Objects.equals(value, oldOptions.get(key))) {\n+        continue;\n+      }\n+\n+      if (\"location\".equalsIgnoreCase(key)) {\n+        setLocation = value;\n+      } else if (\"current-snapshot-id\".equalsIgnoreCase(key)) {\n+        setSnapshotId = value;\n+      } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(key)) {\n+        pickSnapshotId = value;\n+      } else {\n+        setProperties.put(key, value);\n+      }\n+    }\n+\n+    oldOptions.keySet().forEach(k -> {\n+      if (!newTable.getOptions().containsKey(k)) {\n+        setProperties.put(k, null);", "originalCommit": "87f8c79f679aef109f0fce0fe55745e880948dca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI4ODUyMA==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r478288520", "bodyText": "You can take a look to tests.\nThe existing key-values will be keep. The newTable.getOptions() is not just from alter DDL. It is already merged with old options.\nActually, there is not DDL to delete key-value too...", "author": "JingsongLi", "createdAt": "2020-08-27T09:38:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI4NDk5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI5NjE4MQ==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r478296181", "bodyText": "It is already merged with old options.\n\nIf the new options are already merged with old options,  so for the key in old options, shouldn't it be always in new options ?   Seems there's no reason to add this sentence here ?", "author": "openinx", "createdAt": "2020-08-27T09:51:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI4NDk5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODMyMzEyOA==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r478323128", "bodyText": "What do you mean?\nThe new options is new all options.\nFor example:\nold: {'j' = 'am', 'p' = 'an'}\nalter: ALTER TABLE t UNSET TBLPROPERTIES ('j')\nnewTable.getOptions() will be: {'p' = 'an'}", "author": "JingsongLi", "createdAt": "2020-08-27T10:40:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI4NDk5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc5MzY1Mg==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r478793652", "bodyText": "I'll add a test for unsetting PROPERTIES.", "author": "JingsongLi", "createdAt": "2020-08-28T02:19:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI4NDk5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NjI5OQ==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479566299", "bodyText": "I think it is okay to diff the property sets like this, but it seems like it would be easier not to. Right now, Flink has to apply the changes, then this code diffs the property sets, then Iceberg will re-apply the changes. In addition, this model doesn't work for schema updates, as I noted above.", "author": "rdblue", "createdAt": "2020-08-28T22:55:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI4NDk5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTg2MjU4Ng==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479862586", "bodyText": "But properties updates does not have column IDs. As long as the last is the same.\nSorry, I don't get your point.", "author": "JingsongLi", "createdAt": "2020-08-31T02:53:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI4NDk5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTk5NTQzMw==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479995433", "bodyText": "setProperties.put(k, null) ?  The javadoc from Map said :\n * @throws NullPointerException if the specified key or value is null\n     *         and this map does not permit null keys or values\n     * @throws IllegalArgumentException if some property of the specified key\n     *         or value prevents it from being stored in this map\n     */\n    V put(K key, V value);", "author": "openinx", "createdAt": "2020-08-31T09:04:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI4NDk5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjUyNDQxNw==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r482524417", "bodyText": "My point is that there isn't a correctness problem so this is okay. But, this causes Flink to do much more work because it has to apply changes from SQL, then recover those changes by comparing property maps, and pass the changes to Iceberg so that Iceberg can apply the changes. It is easier to pass the changes directly to Iceberg if the Flink API can be updated to support it.", "author": "rdblue", "createdAt": "2020-09-02T22:10:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI4NDk5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjY2Nzk2Mg==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r482667962", "bodyText": "Got it, I think we can have a try in Flink.", "author": "JingsongLi", "createdAt": "2020-09-03T02:37:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI4NDk5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODMwMTE0NA==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r478301144", "bodyText": "nit:  we may need to change this comment ? Actually, I did not found any class named IcebergCatalogTable,  or I misunderstood something ?", "author": "openinx", "createdAt": "2020-08-27T09:59:42Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -277,20 +286,29 @@ public void alterDatabase(String name, CatalogDatabase newDatabase, boolean igno\n   }\n \n   @Override\n-  public CatalogBaseTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {\n-    try {\n-      Table table = icebergCatalog.loadTable(toIdentifier(tablePath));\n-      TableSchema tableSchema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(table.schema()));\n+  public CatalogTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {\n+    Table table = getIcebergTable(tablePath);\n+    return toCatalogTable(table);\n+  }\n \n-      // NOTE: We can not create a IcebergCatalogTable, because Flink optimizer may use CatalogTableImpl to copy a new\n-      // catalog table.\n-      // Let's re-loading table from Iceberg catalog when creating source/sink operators.\n-      return new CatalogTableImpl(tableSchema, table.properties(), null);\n+  private Table getIcebergTable(ObjectPath tablePath) throws TableNotExistException {\n+    try {\n+      return icebergCatalog.loadTable(toIdentifier(tablePath));\n     } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n       throw new TableNotExistException(getName(), tablePath, e);\n     }\n   }\n \n+  private CatalogTable toCatalogTable(Table table) {\n+    TableSchema schema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(table.schema()));\n+    List<String> partitionKeys = toPartitionKeys(table.spec(), table.schema());\n+\n+    // NOTE: We can not create a IcebergCatalogTable, because Flink optimizer may use CatalogTableImpl to copy a new", "originalCommit": "87f8c79f679aef109f0fce0fe55745e880948dca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODMxODQwMw==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r478318403", "bodyText": "I mean, we cannot create a IcebergCatalogTable class extends CatalogTable to carry iceberg Table Object.", "author": "JingsongLi", "createdAt": "2020-08-27T10:31:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODMwMTE0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTk4NjQ2MA==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479986460", "bodyText": "Got it,  maybe could write this comment more clear.", "author": "openinx", "createdAt": "2020-08-31T08:47:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODMwMTE0NA=="}], "type": "inlineReview"}, {"oid": "5adac5c64d27e3a538c9741f9e9b37da60940ff5", "url": "https://github.com/apache/iceberg/commit/5adac5c64d27e3a538c9741f9e9b37da60940ff5", "message": "Address comments", "committedDate": "2020-08-28T08:23:34Z", "type": "commit"}, {"oid": "2d1aed64d2681f41d27c7d945e009a72e51e05d2", "url": "https://github.com/apache/iceberg/commit/2d1aed64d2681f41d27c7d945e009a72e51e05d2", "message": "checkstyle", "committedDate": "2020-08-28T09:15:55Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2MjgxMQ==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479562811", "bodyText": "What is null? Could you add a comment?", "author": "rdblue", "createdAt": "2020-08-28T22:40:04Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -277,20 +287,29 @@ public void alterDatabase(String name, CatalogDatabase newDatabase, boolean igno\n   }\n \n   @Override\n-  public CatalogBaseTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {\n-    try {\n-      Table table = icebergCatalog.loadTable(toIdentifier(tablePath));\n-      TableSchema tableSchema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(table.schema()));\n+  public CatalogTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {\n+    Table table = getIcebergTable(tablePath);\n+    return toCatalogTable(table);\n+  }\n \n-      // NOTE: We can not create a IcebergCatalogTable, because Flink optimizer may use CatalogTableImpl to copy a new\n-      // catalog table.\n-      // Let's re-loading table from Iceberg catalog when creating source/sink operators.\n-      return new CatalogTableImpl(tableSchema, table.properties(), null);\n+  private Table getIcebergTable(ObjectPath tablePath) throws TableNotExistException {\n+    try {\n+      return icebergCatalog.loadTable(toIdentifier(tablePath));\n     } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n       throw new TableNotExistException(getName(), tablePath, e);\n     }\n   }\n \n+  private CatalogTable toCatalogTable(Table table) {\n+    TableSchema schema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(table.schema()));\n+    List<String> partitionKeys = toPartitionKeys(table.spec(), table.schema());\n+\n+    // NOTE: We can not create a IcebergCatalogTable, because Flink optimizer may use CatalogTableImpl to copy a new\n+    // catalog table.\n+    // Let's re-loading table from Iceberg catalog when creating source/sink operators.\n+    return new CatalogTableImpl(schema, partitionKeys, table.properties(), null);", "originalCommit": "2d1aed64d2681f41d27c7d945e009a72e51e05d2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2Mzk4Ng==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479563986", "bodyText": "get is not a very specific verb. I usually prefer load for cases like this because it more accurately describes what is happening.", "author": "rdblue", "createdAt": "2020-08-28T22:44:59Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -277,20 +287,29 @@ public void alterDatabase(String name, CatalogDatabase newDatabase, boolean igno\n   }\n \n   @Override\n-  public CatalogBaseTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {\n-    try {\n-      Table table = icebergCatalog.loadTable(toIdentifier(tablePath));\n-      TableSchema tableSchema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(table.schema()));\n+  public CatalogTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {\n+    Table table = getIcebergTable(tablePath);\n+    return toCatalogTable(table);\n+  }\n \n-      // NOTE: We can not create a IcebergCatalogTable, because Flink optimizer may use CatalogTableImpl to copy a new\n-      // catalog table.\n-      // Let's re-loading table from Iceberg catalog when creating source/sink operators.\n-      return new CatalogTableImpl(tableSchema, table.properties(), null);\n+  private Table getIcebergTable(ObjectPath tablePath) throws TableNotExistException {", "originalCommit": "2d1aed64d2681f41d27c7d945e009a72e51e05d2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcxMDE1NA==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479710154", "bodyText": "I agree on the get, but it does align with the interface's getTable method which this is used in. However, it also calls out to load so the argument that the iceberg specific stuff might stick with load vs get is still valid.", "author": "kbendick", "createdAt": "2020-08-30T01:58:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2Mzk4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTg2NDU5Mg==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479864592", "bodyText": "OK, it is an iceberg table, change it to load.", "author": "JingsongLi", "createdAt": "2020-08-31T03:02:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2Mzk4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NjQ0OQ==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479566449", "bodyText": "Is there a case where CatalogBaseTable doesn't implement CatalogTable?", "author": "rdblue", "createdAt": "2020-08-28T22:55:32Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +339,167 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+\n+    ImmutableMap.Builder<String, String> properties = ImmutableMap.builder();\n+    String location = null;\n+    for (Map.Entry<String, String> entry : table.getOptions().entrySet()) {\n+      if (\"location\".equalsIgnoreCase(entry.getKey())) {\n+        location = entry.getValue();\n+      } else {\n+        properties.put(entry.getKey(), entry.getValue());\n+      }\n+    }\n+\n+    try {\n+      icebergCatalog.createTable(\n+          toIdentifier(tablePath),\n+          icebergSchema,\n+          spec,\n+          location,\n+          properties.build());\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n   }\n \n   @Override\n   public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support alterTable now.\");\n+      throws CatalogException, TableNotExistException {\n+    validateFlinkTable(newTable);\n+    Table icebergTable = getIcebergTable(tablePath);\n+    CatalogTable table = toCatalogTable(icebergTable);\n+\n+    // Currently, Flink SQL only support altering table properties.\n+\n+    if (!table.getSchema().equals(newTable.getSchema())) {\n+      throw new UnsupportedOperationException(\"Altering schema is not supported yet.\");\n+    }\n+\n+    if (!table.getPartitionKeys().equals(((CatalogTable) newTable).getPartitionKeys())) {\n+      throw new UnsupportedOperationException(\"Altering partition keys is not supported yet.\");\n+    }\n+\n+    Map<String, String> oldOptions = table.getOptions();\n+    Map<String, String> setProperties = Maps.newHashMap();\n+\n+    String setLocation = null;\n+    String setSnapshotId = null;\n+    String pickSnapshotId = null;\n+\n+    for (Map.Entry<String, String> entry : newTable.getOptions().entrySet()) {\n+      String key = entry.getKey();\n+      String value = entry.getValue();\n+\n+      if (Objects.equals(value, oldOptions.get(key))) {\n+        continue;\n+      }\n+\n+      if (\"location\".equalsIgnoreCase(key)) {\n+        setLocation = value;\n+      } else if (\"current-snapshot-id\".equalsIgnoreCase(key)) {\n+        setSnapshotId = value;\n+      } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(key)) {\n+        pickSnapshotId = value;\n+      } else {\n+        setProperties.put(key, value);\n+      }\n+    }\n+\n+    oldOptions.keySet().forEach(k -> {\n+      if (!newTable.getOptions().containsKey(k)) {\n+        setProperties.put(k, null);\n+      }\n+    });\n+\n+    commitChanges(icebergTable, setLocation, setSnapshotId, pickSnapshotId, setProperties);\n+  }\n+\n+  private static void validateFlinkTable(CatalogBaseTable table) {\n+    Preconditions.checkArgument(table instanceof CatalogTable, \"The Table should be a CatalogTable.\");", "originalCommit": "2d1aed64d2681f41d27c7d945e009a72e51e05d2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcxMTgwNw==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479711807", "bodyText": "CatalogTable is a subinterface that inherits from CatalogBaseTable. So definitely, yes.\nSee the java docs on the current CatalogBaseTable in Flink:\nhttps://ci.apache.org/projects/flink/flink-docs-release-1.11/api/java/org/apache/flink/table/catalog/CatalogBaseTable.html", "author": "kbendick", "createdAt": "2020-08-30T02:23:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NjQ0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTg2NTEzMA==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479865130", "bodyText": "It is CatalogView, Iceberg catalog does not support views, so if there is a view, should be a bug...", "author": "JingsongLi", "createdAt": "2020-08-31T03:05:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NjQ0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NjYyOA==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479566628", "bodyText": "Is this something we should add to Iceberg for Flink use cases? What does Flink use the primary key for?", "author": "rdblue", "createdAt": "2020-08-28T22:56:23Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +339,167 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+\n+    ImmutableMap.Builder<String, String> properties = ImmutableMap.builder();\n+    String location = null;\n+    for (Map.Entry<String, String> entry : table.getOptions().entrySet()) {\n+      if (\"location\".equalsIgnoreCase(entry.getKey())) {\n+        location = entry.getValue();\n+      } else {\n+        properties.put(entry.getKey(), entry.getValue());\n+      }\n+    }\n+\n+    try {\n+      icebergCatalog.createTable(\n+          toIdentifier(tablePath),\n+          icebergSchema,\n+          spec,\n+          location,\n+          properties.build());\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n   }\n \n   @Override\n   public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support alterTable now.\");\n+      throws CatalogException, TableNotExistException {\n+    validateFlinkTable(newTable);\n+    Table icebergTable = getIcebergTable(tablePath);\n+    CatalogTable table = toCatalogTable(icebergTable);\n+\n+    // Currently, Flink SQL only support altering table properties.\n+\n+    if (!table.getSchema().equals(newTable.getSchema())) {\n+      throw new UnsupportedOperationException(\"Altering schema is not supported yet.\");\n+    }\n+\n+    if (!table.getPartitionKeys().equals(((CatalogTable) newTable).getPartitionKeys())) {\n+      throw new UnsupportedOperationException(\"Altering partition keys is not supported yet.\");\n+    }\n+\n+    Map<String, String> oldOptions = table.getOptions();\n+    Map<String, String> setProperties = Maps.newHashMap();\n+\n+    String setLocation = null;\n+    String setSnapshotId = null;\n+    String pickSnapshotId = null;\n+\n+    for (Map.Entry<String, String> entry : newTable.getOptions().entrySet()) {\n+      String key = entry.getKey();\n+      String value = entry.getValue();\n+\n+      if (Objects.equals(value, oldOptions.get(key))) {\n+        continue;\n+      }\n+\n+      if (\"location\".equalsIgnoreCase(key)) {\n+        setLocation = value;\n+      } else if (\"current-snapshot-id\".equalsIgnoreCase(key)) {\n+        setSnapshotId = value;\n+      } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(key)) {\n+        pickSnapshotId = value;\n+      } else {\n+        setProperties.put(key, value);\n+      }\n+    }\n+\n+    oldOptions.keySet().forEach(k -> {\n+      if (!newTable.getOptions().containsKey(k)) {\n+        setProperties.put(k, null);\n+      }\n+    });\n+\n+    commitChanges(icebergTable, setLocation, setSnapshotId, pickSnapshotId, setProperties);\n+  }\n+\n+  private static void validateFlinkTable(CatalogBaseTable table) {\n+    Preconditions.checkArgument(table instanceof CatalogTable, \"The Table should be a CatalogTable.\");\n+\n+    TableSchema schema = table.getSchema();\n+    schema.getTableColumns().forEach(column -> {\n+      if (column.isGenerated()) {\n+        throw new UnsupportedOperationException(\"Creating table with computed columns is not supported yet.\");\n+      }\n+    });\n+\n+    if (!schema.getWatermarkSpecs().isEmpty()) {\n+      throw new UnsupportedOperationException(\"Creating table with watermark specs is not supported yet.\");\n+    }\n+\n+    if (schema.getPrimaryKey().isPresent()) {", "originalCommit": "2d1aed64d2681f41d27c7d945e009a72e51e05d2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcxMTA3MA==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479711070", "bodyText": "I found this which might answer your question: https://cwiki.apache.org/confluence/display/FLINK/FLIP+87%3A+Primary+key+constraints+in+Table+API\nIn particular, here are the proposed changes:\nProposed Changes\nWe suggest to introduce the concept of primary key constraint as a hint for FLINK to leverage for optimizations.\n\nPrimary key constraints tell that a column or a set of columns of a table or a view are unique and they do not contain null.\nNeither of columns in a primary can be nullable.\nPrimary key therefore uniquely identify a row in a table.\n\nSo it sounds just like an RDBMS primary key.\nNote however, that even in the FLIP (which is just the proposal and not necessarily the finished product), it does state that there's no planned enforcement on the PK. It's up to the user to ensure that the PK is non-null and unique.\nPrimary key validity checks\nSQL standard specifies that a constraint can either be ENFORCED or NOT ENFORCED.\nThis controls if the constraint checks are performed on the incoming/outgoing data.\nFlink does not own the data therefore the only mode we want to support is the NOT ENFORCED mode.\nIts up to the user to ensure that the query enforces key integrity.\n\nSo I agree here that throwing might be the most useful option and that there's likely nothing on the iceberg side to be added to enforce this as Flink doesn't enforce it either. In an entirely streaming setting, ensuring unique keys would be rather difficult and so to me it somewhat sounds like the PK is just more metadata that could very well be in TBLPROPERTIES.\nBut a more experienced Flink SQL user than myself might have more to say on the matter. I've never attempted to enforce a PK when using Flink SQL. Sounds like the work to do so would involve custom operators etc.\nTLDR: The Primary Key is just a constraint, which is currently part of Flink's Table spec but goes unenforced and is up to the user. It does not appear as though the PK info is supported in any UpsertSinks etc, though that may be discussed / planned in the future. Support in the DDL for Primary Key constraints is relatively new (Flink 1.11 / current, with support in the API coming in at Flink 1.10).", "author": "kbendick", "createdAt": "2020-08-30T02:12:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NjYyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTg2NjcwMw==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479866703", "bodyText": "Thanks @kbendick .\nAt present, in Flink, PK is mainly used to process CDC stream.\n\nFor example, if user access a Kafka source of a CDC stream, the user can define a primary key. In this way, the downstream can perform efficient dynamic table / static table conversion (restore to the original static table) according to a certain primary key.\nFor example, when the stream data (CDC) is written into a JDBC database, the user can define primary key. In this way, Flink can insert the data into the database by using the upsert writing way.\n\nI think, If iceberg supports CDC native processing in the future, we may be able to use it.", "author": "JingsongLi", "createdAt": "2020-08-31T03:13:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NjYyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NzAyMA==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479567020", "bodyText": "All tables with any partition transform other than identity appear to be unpartitioned? Why not return all of the identity fields at least?", "author": "rdblue", "createdAt": "2020-08-28T22:58:06Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +339,167 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+\n+    ImmutableMap.Builder<String, String> properties = ImmutableMap.builder();\n+    String location = null;\n+    for (Map.Entry<String, String> entry : table.getOptions().entrySet()) {\n+      if (\"location\".equalsIgnoreCase(entry.getKey())) {\n+        location = entry.getValue();\n+      } else {\n+        properties.put(entry.getKey(), entry.getValue());\n+      }\n+    }\n+\n+    try {\n+      icebergCatalog.createTable(\n+          toIdentifier(tablePath),\n+          icebergSchema,\n+          spec,\n+          location,\n+          properties.build());\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n   }\n \n   @Override\n   public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support alterTable now.\");\n+      throws CatalogException, TableNotExistException {\n+    validateFlinkTable(newTable);\n+    Table icebergTable = getIcebergTable(tablePath);\n+    CatalogTable table = toCatalogTable(icebergTable);\n+\n+    // Currently, Flink SQL only support altering table properties.\n+\n+    if (!table.getSchema().equals(newTable.getSchema())) {\n+      throw new UnsupportedOperationException(\"Altering schema is not supported yet.\");\n+    }\n+\n+    if (!table.getPartitionKeys().equals(((CatalogTable) newTable).getPartitionKeys())) {\n+      throw new UnsupportedOperationException(\"Altering partition keys is not supported yet.\");\n+    }\n+\n+    Map<String, String> oldOptions = table.getOptions();\n+    Map<String, String> setProperties = Maps.newHashMap();\n+\n+    String setLocation = null;\n+    String setSnapshotId = null;\n+    String pickSnapshotId = null;\n+\n+    for (Map.Entry<String, String> entry : newTable.getOptions().entrySet()) {\n+      String key = entry.getKey();\n+      String value = entry.getValue();\n+\n+      if (Objects.equals(value, oldOptions.get(key))) {\n+        continue;\n+      }\n+\n+      if (\"location\".equalsIgnoreCase(key)) {\n+        setLocation = value;\n+      } else if (\"current-snapshot-id\".equalsIgnoreCase(key)) {\n+        setSnapshotId = value;\n+      } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(key)) {\n+        pickSnapshotId = value;\n+      } else {\n+        setProperties.put(key, value);\n+      }\n+    }\n+\n+    oldOptions.keySet().forEach(k -> {\n+      if (!newTable.getOptions().containsKey(k)) {\n+        setProperties.put(k, null);\n+      }\n+    });\n+\n+    commitChanges(icebergTable, setLocation, setSnapshotId, pickSnapshotId, setProperties);\n+  }\n+\n+  private static void validateFlinkTable(CatalogBaseTable table) {\n+    Preconditions.checkArgument(table instanceof CatalogTable, \"The Table should be a CatalogTable.\");\n+\n+    TableSchema schema = table.getSchema();\n+    schema.getTableColumns().forEach(column -> {\n+      if (column.isGenerated()) {\n+        throw new UnsupportedOperationException(\"Creating table with computed columns is not supported yet.\");\n+      }\n+    });\n+\n+    if (!schema.getWatermarkSpecs().isEmpty()) {\n+      throw new UnsupportedOperationException(\"Creating table with watermark specs is not supported yet.\");\n+    }\n+\n+    if (schema.getPrimaryKey().isPresent()) {\n+      throw new UnsupportedOperationException(\"Creating table with primary key is not supported yet.\");\n+    }\n+  }\n+\n+  private static PartitionSpec toPartitionSpec(List<String> partitionKeys, Schema icebergSchema) {\n+    PartitionSpec.Builder builder = PartitionSpec.builderFor(icebergSchema);\n+    partitionKeys.forEach(builder::identity);\n+    return builder.build();\n+  }\n+\n+  private static List<String> toPartitionKeys(PartitionSpec spec, Schema icebergSchema) {\n+    List<String> partitionKeys = Lists.newArrayList();\n+    for (PartitionField field : spec.fields()) {\n+      if (field.transform().isIdentity()) {\n+        partitionKeys.add(icebergSchema.findColumnName(field.sourceId()));\n+      } else {\n+        // Not created by Flink SQL.\n+        // For compatibility with iceberg tables, return empty.\n+        // TODO modify this after Flink support partition transform.\n+        return Collections.emptyList();", "originalCommit": "2d1aed64d2681f41d27c7d945e009a72e51e05d2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcxMTk3Nw==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479711977", "bodyText": "To me, it seems like adding all of the identity fields (but not the transformed fields) would likely be incorrect. Although returning an empty list when the table is partitioned seems like a possible correctness bug to me too.\nShould we consider throwing an exception in this case instead until such a time that Flink supports partition transforms?", "author": "kbendick", "createdAt": "2020-08-30T02:26:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NzAyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTg2NzU3NA==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479867574", "bodyText": "+1 to likely be incorrect.\nBut I don't want to throw an exception because it can read the existing iceberg table.\nActually, the returned partition keys are useless, except that Flink can show users the meta information of the table.\nAll partition operations are directly delegated to specific source / sink, so Flink does not need to see partition information.\nI tend to support it so that we can read existing Iceberg tables.", "author": "JingsongLi", "createdAt": "2020-08-31T03:18:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NzAyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjUyOTAyOQ==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r482529029", "bodyText": "I was thinking that since operations are delegated to Iceberg, correctness is not an issue. It would be nice to show users which columns are partition columns so they can see which ones are good candidates for query predicates. I don't think this is a blocker, though.", "author": "rdblue", "createdAt": "2020-09-02T22:16:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NzAyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjY2OTQ0Nw==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r482669447", "bodyText": "I see, maybe we can expose these information in properties.", "author": "JingsongLi", "createdAt": "2020-09-03T02:42:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NzAyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NzM0NQ==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479567345", "bodyText": "Should we do the operations above this point in the transaction as well? That seems reasonable to me. I'm not sure why we don't in other places.", "author": "rdblue", "createdAt": "2020-08-28T22:59:33Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +339,167 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+\n+    ImmutableMap.Builder<String, String> properties = ImmutableMap.builder();\n+    String location = null;\n+    for (Map.Entry<String, String> entry : table.getOptions().entrySet()) {\n+      if (\"location\".equalsIgnoreCase(entry.getKey())) {\n+        location = entry.getValue();\n+      } else {\n+        properties.put(entry.getKey(), entry.getValue());\n+      }\n+    }\n+\n+    try {\n+      icebergCatalog.createTable(\n+          toIdentifier(tablePath),\n+          icebergSchema,\n+          spec,\n+          location,\n+          properties.build());\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n   }\n \n   @Override\n   public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support alterTable now.\");\n+      throws CatalogException, TableNotExistException {\n+    validateFlinkTable(newTable);\n+    Table icebergTable = getIcebergTable(tablePath);\n+    CatalogTable table = toCatalogTable(icebergTable);\n+\n+    // Currently, Flink SQL only support altering table properties.\n+\n+    if (!table.getSchema().equals(newTable.getSchema())) {\n+      throw new UnsupportedOperationException(\"Altering schema is not supported yet.\");\n+    }\n+\n+    if (!table.getPartitionKeys().equals(((CatalogTable) newTable).getPartitionKeys())) {\n+      throw new UnsupportedOperationException(\"Altering partition keys is not supported yet.\");\n+    }\n+\n+    Map<String, String> oldOptions = table.getOptions();\n+    Map<String, String> setProperties = Maps.newHashMap();\n+\n+    String setLocation = null;\n+    String setSnapshotId = null;\n+    String pickSnapshotId = null;\n+\n+    for (Map.Entry<String, String> entry : newTable.getOptions().entrySet()) {\n+      String key = entry.getKey();\n+      String value = entry.getValue();\n+\n+      if (Objects.equals(value, oldOptions.get(key))) {\n+        continue;\n+      }\n+\n+      if (\"location\".equalsIgnoreCase(key)) {\n+        setLocation = value;\n+      } else if (\"current-snapshot-id\".equalsIgnoreCase(key)) {\n+        setSnapshotId = value;\n+      } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(key)) {\n+        pickSnapshotId = value;\n+      } else {\n+        setProperties.put(key, value);\n+      }\n+    }\n+\n+    oldOptions.keySet().forEach(k -> {\n+      if (!newTable.getOptions().containsKey(k)) {\n+        setProperties.put(k, null);\n+      }\n+    });\n+\n+    commitChanges(icebergTable, setLocation, setSnapshotId, pickSnapshotId, setProperties);\n+  }\n+\n+  private static void validateFlinkTable(CatalogBaseTable table) {\n+    Preconditions.checkArgument(table instanceof CatalogTable, \"The Table should be a CatalogTable.\");\n+\n+    TableSchema schema = table.getSchema();\n+    schema.getTableColumns().forEach(column -> {\n+      if (column.isGenerated()) {\n+        throw new UnsupportedOperationException(\"Creating table with computed columns is not supported yet.\");\n+      }\n+    });\n+\n+    if (!schema.getWatermarkSpecs().isEmpty()) {\n+      throw new UnsupportedOperationException(\"Creating table with watermark specs is not supported yet.\");\n+    }\n+\n+    if (schema.getPrimaryKey().isPresent()) {\n+      throw new UnsupportedOperationException(\"Creating table with primary key is not supported yet.\");\n+    }\n+  }\n+\n+  private static PartitionSpec toPartitionSpec(List<String> partitionKeys, Schema icebergSchema) {\n+    PartitionSpec.Builder builder = PartitionSpec.builderFor(icebergSchema);\n+    partitionKeys.forEach(builder::identity);\n+    return builder.build();\n+  }\n+\n+  private static List<String> toPartitionKeys(PartitionSpec spec, Schema icebergSchema) {\n+    List<String> partitionKeys = Lists.newArrayList();\n+    for (PartitionField field : spec.fields()) {\n+      if (field.transform().isIdentity()) {\n+        partitionKeys.add(icebergSchema.findColumnName(field.sourceId()));\n+      } else {\n+        // Not created by Flink SQL.\n+        // For compatibility with iceberg tables, return empty.\n+        // TODO modify this after Flink support partition transform.\n+        return Collections.emptyList();\n+      }\n+    }\n+    return partitionKeys;\n+  }\n+\n+  private static void commitChanges(Table table, String setLocation, String setSnapshotId,\n+                                    String pickSnapshotId, Map<String, String> setProperties) {\n+    // don't allow setting the snapshot and picking a commit at the same time because order is ambiguous and choosing\n+    // one order leads to different results\n+    Preconditions.checkArgument(setSnapshotId == null || pickSnapshotId == null,\n+        \"Cannot set the current the current snapshot ID and cherry-pick snapshot changes\");\n+\n+    if (setSnapshotId != null) {\n+      long newSnapshotId = Long.parseLong(setSnapshotId);\n+      table.manageSnapshots().setCurrentSnapshot(newSnapshotId).commit();\n+    }\n+\n+    // if updating the table snapshot, perform that update first in case it fails\n+    if (pickSnapshotId != null) {\n+      long newSnapshotId = Long.parseLong(pickSnapshotId);\n+      table.manageSnapshots().cherrypick(newSnapshotId).commit();\n+    }\n+\n+    Transaction transaction = table.newTransaction();", "originalCommit": "2d1aed64d2681f41d27c7d945e009a72e51e05d2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTg2ODcyNQ==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479868725", "bodyText": "Looks like the manageSnapshots is unsupported in TransactionTable.", "author": "JingsongLi", "createdAt": "2020-08-31T03:24:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NzM0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjUyNjIwOQ==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r482526209", "bodyText": "That explains it. Thanks!", "author": "rdblue", "createdAt": "2020-09-02T22:13:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NzM0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NzYzOQ==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479567639", "bodyText": "Does Flink support the LOCATION clause, or just the table property?", "author": "rdblue", "createdAt": "2020-08-28T23:00:44Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java", "diffHunk": "@@ -88,4 +105,172 @@ public void testRenameTable() {\n         Collections.singletonList(TableColumn.of(\"id\", DataTypes.BIGINT())),\n         tEnv.from(\"tl2\").getSchema().getTableColumns());\n   }\n+\n+  @Test\n+  public void testCreateTable() throws TableNotExistException {\n+    tEnv.executeSql(\"CREATE TABLE tl(id BIGINT)\");\n+\n+    Table table = table(\"tl\");\n+    Assert.assertEquals(\n+        new Schema(Types.NestedField.optional(1, \"id\", Types.LongType.get())).asStruct(),\n+        table.schema().asStruct());\n+    Assert.assertEquals(Maps.newHashMap(), table.properties());\n+\n+    CatalogTable catalogTable = catalogTable(\"tl\");\n+    Assert.assertEquals(TableSchema.builder().field(\"id\", DataTypes.BIGINT()).build(), catalogTable.getSchema());\n+    Assert.assertEquals(Maps.newHashMap(), catalogTable.getOptions());\n+  }\n+\n+  @Test\n+  public void testCreateTableLocation() {\n+    Assume.assumeFalse(\"HadoopCatalog does not support creating table with location\", isHadoopCatalog);\n+\n+    tEnv.executeSql(\"CREATE TABLE tl(id BIGINT) WITH ('location'='/tmp/location')\");\n+\n+    Table table = table(\"tl\");\n+    Assert.assertEquals(\n+        new Schema(Types.NestedField.optional(1, \"id\", Types.LongType.get())).asStruct(),\n+        table.schema().asStruct());\n+    Assert.assertEquals(\"/tmp/location\", table.location());\n+    Assert.assertEquals(Maps.newHashMap(), table.properties());\n+  }\n+\n+  @Test\n+  public void testCreatePartitionTable() throws TableNotExistException {\n+    tEnv.executeSql(\"CREATE TABLE tl(id BIGINT, dt STRING) PARTITIONED BY(dt)\");\n+\n+    Table table = table(\"tl\");\n+    Assert.assertEquals(\n+        new Schema(\n+            Types.NestedField.optional(1, \"id\", Types.LongType.get()),\n+            Types.NestedField.optional(2, \"dt\", Types.StringType.get())).asStruct(),\n+        table.schema().asStruct());\n+    Assert.assertEquals(PartitionSpec.builderFor(table.schema()).identity(\"dt\").build(), table.spec());\n+    Assert.assertEquals(Maps.newHashMap(), table.properties());\n+\n+    CatalogTable catalogTable = catalogTable(\"tl\");\n+    Assert.assertEquals(\n+        TableSchema.builder().field(\"id\", DataTypes.BIGINT()).field(\"dt\", DataTypes.STRING()).build(),\n+        catalogTable.getSchema());\n+    Assert.assertEquals(Maps.newHashMap(), catalogTable.getOptions());\n+    Assert.assertEquals(Collections.singletonList(\"dt\"), catalogTable.getPartitionKeys());\n+  }\n+\n+  @Test\n+  public void testLoadTransformPartitionTable() throws TableNotExistException {\n+    Schema schema = new Schema(Types.NestedField.optional(0, \"id\", Types.LongType.get()));\n+    validationCatalog.createTable(\n+        TableIdentifier.of(icebergNamespace, \"tl\"), schema,\n+        PartitionSpec.builderFor(schema).bucket(\"id\", 100).build());\n+\n+    CatalogTable catalogTable = catalogTable(\"tl\");\n+    Assert.assertEquals(\n+        TableSchema.builder().field(\"id\", DataTypes.BIGINT()).build(),\n+        catalogTable.getSchema());\n+    Assert.assertEquals(Maps.newHashMap(), catalogTable.getOptions());\n+    Assert.assertEquals(Collections.emptyList(), catalogTable.getPartitionKeys());\n+  }\n+\n+  @Test\n+  public void testAlterTable() throws TableNotExistException {\n+    tEnv.executeSql(\"CREATE TABLE tl(id BIGINT) WITH ('oldK'='oldV')\");\n+    Map<String, String> properties = Maps.newHashMap();\n+    properties.put(\"oldK\", \"oldV\");\n+\n+    // new\n+    tEnv.executeSql(\"ALTER TABLE tl SET('newK'='newV')\");\n+    properties.put(\"newK\", \"newV\");\n+    Assert.assertEquals(properties, table(\"tl\").properties());\n+\n+    // update old\n+    tEnv.executeSql(\"ALTER TABLE tl SET('oldK'='oldV2')\");\n+    properties.put(\"oldK\", \"oldV2\");\n+    Assert.assertEquals(properties, table(\"tl\").properties());\n+\n+    // remove property\n+    CatalogTable catalogTable = catalogTable(\"tl\");\n+    properties.remove(\"oldK\");\n+    tEnv.getCatalog(tEnv.getCurrentCatalog()).get().alterTable(\n+        new ObjectPath(DATABASE, \"tl\"), catalogTable.copy(properties), false);\n+    Assert.assertEquals(properties, table(\"tl\").properties());\n+  }\n+\n+  @Test\n+  public void testRelocateTable() {\n+    Assume.assumeFalse(\"HadoopCatalog does not support relocate table\", isHadoopCatalog);\n+\n+    tEnv.executeSql(\"CREATE TABLE tl(id BIGINT)\");\n+    tEnv.executeSql(\"ALTER TABLE tl SET('location'='/tmp/location')\");", "originalCommit": "2d1aed64d2681f41d27c7d945e009a72e51e05d2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcxMjM3NA==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479712374", "bodyText": "EDIT: In the CREATE TABLE documentation, I cannot find a reference to LOCATION outside of table properties: https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/create.html#create-table", "author": "kbendick", "createdAt": "2020-08-30T02:32:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NzYzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcxMjY5Mg==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479712692", "bodyText": "However, looking forward at the Flink 1.12 prerelease docs, it appears that there's added support for a Hive Dialect which supports LOCATION clause.\nhttps://ci.apache.org/projects/flink/flink-docs-master/dev/table/hive/hive_dialect.html#create-1\nCREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name\n  [(col_name data_type [column_constraint] [COMMENT col_comment], ... [table_constraint])]\n  [COMMENT table_comment]\n  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]\n  [\n    [ROW FORMAT row_format]\n    [STORED AS file_format]\n  ]\n  [LOCATION fs_path]\n  [TBLPROPERTIES (property_name=property_value, ...)]", "author": "kbendick", "createdAt": "2020-08-30T02:37:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NzYzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTg2OTI5OA==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479869298", "bodyText": "Yes, just the table property...\nhttps://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/filesystem.html\nFlink Filesystem connector also support location using path property.", "author": "JingsongLi", "createdAt": "2020-08-31T03:27:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NzYzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTg2OTQ1OA==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479869458", "bodyText": "The Hive dialect should only works for Hive tables. There are lots of tricky things..", "author": "JingsongLi", "createdAt": "2020-08-31T03:28:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU2NzYzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcxMDMwOQ==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479710309", "bodyText": "Should location still be placed in the table properties or will that cause some kind of conflict / error?", "author": "kbendick", "createdAt": "2020-08-30T02:00:39Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +339,167 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+\n+    ImmutableMap.Builder<String, String> properties = ImmutableMap.builder();\n+    String location = null;\n+    for (Map.Entry<String, String> entry : table.getOptions().entrySet()) {\n+      if (\"location\".equalsIgnoreCase(entry.getKey())) {\n+        location = entry.getValue();\n+      } else {\n+        properties.put(entry.getKey(), entry.getValue());", "originalCommit": "2d1aed64d2681f41d27c7d945e009a72e51e05d2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTg2OTY4Nw==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479869687", "bodyText": "I think there is no conflict/error, but I think it is good to reduce duplicate storage, cause iceberg has saved this information.", "author": "JingsongLi", "createdAt": "2020-08-31T03:29:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcxMDMwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjUyNzQzNA==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r482527434", "bodyText": "I'd prefer not to duplicate it in table properties. Then we would have to worry about keeping the two in sync.", "author": "rdblue", "createdAt": "2020-09-02T22:14:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcxMDMwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcxMjA1Mw==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479712053", "bodyText": "Nit: Duplication of the words the current in the Preconditions string. It currently reads Cannot set the current the current.", "author": "kbendick", "createdAt": "2020-08-30T02:27:32Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +339,167 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+\n+    ImmutableMap.Builder<String, String> properties = ImmutableMap.builder();\n+    String location = null;\n+    for (Map.Entry<String, String> entry : table.getOptions().entrySet()) {\n+      if (\"location\".equalsIgnoreCase(entry.getKey())) {\n+        location = entry.getValue();\n+      } else {\n+        properties.put(entry.getKey(), entry.getValue());\n+      }\n+    }\n+\n+    try {\n+      icebergCatalog.createTable(\n+          toIdentifier(tablePath),\n+          icebergSchema,\n+          spec,\n+          location,\n+          properties.build());\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n   }\n \n   @Override\n   public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support alterTable now.\");\n+      throws CatalogException, TableNotExistException {\n+    validateFlinkTable(newTable);\n+    Table icebergTable = getIcebergTable(tablePath);\n+    CatalogTable table = toCatalogTable(icebergTable);\n+\n+    // Currently, Flink SQL only support altering table properties.\n+\n+    if (!table.getSchema().equals(newTable.getSchema())) {\n+      throw new UnsupportedOperationException(\"Altering schema is not supported yet.\");\n+    }\n+\n+    if (!table.getPartitionKeys().equals(((CatalogTable) newTable).getPartitionKeys())) {\n+      throw new UnsupportedOperationException(\"Altering partition keys is not supported yet.\");\n+    }\n+\n+    Map<String, String> oldOptions = table.getOptions();\n+    Map<String, String> setProperties = Maps.newHashMap();\n+\n+    String setLocation = null;\n+    String setSnapshotId = null;\n+    String pickSnapshotId = null;\n+\n+    for (Map.Entry<String, String> entry : newTable.getOptions().entrySet()) {\n+      String key = entry.getKey();\n+      String value = entry.getValue();\n+\n+      if (Objects.equals(value, oldOptions.get(key))) {\n+        continue;\n+      }\n+\n+      if (\"location\".equalsIgnoreCase(key)) {\n+        setLocation = value;\n+      } else if (\"current-snapshot-id\".equalsIgnoreCase(key)) {\n+        setSnapshotId = value;\n+      } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(key)) {\n+        pickSnapshotId = value;\n+      } else {\n+        setProperties.put(key, value);\n+      }\n+    }\n+\n+    oldOptions.keySet().forEach(k -> {\n+      if (!newTable.getOptions().containsKey(k)) {\n+        setProperties.put(k, null);\n+      }\n+    });\n+\n+    commitChanges(icebergTable, setLocation, setSnapshotId, pickSnapshotId, setProperties);\n+  }\n+\n+  private static void validateFlinkTable(CatalogBaseTable table) {\n+    Preconditions.checkArgument(table instanceof CatalogTable, \"The Table should be a CatalogTable.\");\n+\n+    TableSchema schema = table.getSchema();\n+    schema.getTableColumns().forEach(column -> {\n+      if (column.isGenerated()) {\n+        throw new UnsupportedOperationException(\"Creating table with computed columns is not supported yet.\");\n+      }\n+    });\n+\n+    if (!schema.getWatermarkSpecs().isEmpty()) {\n+      throw new UnsupportedOperationException(\"Creating table with watermark specs is not supported yet.\");\n+    }\n+\n+    if (schema.getPrimaryKey().isPresent()) {\n+      throw new UnsupportedOperationException(\"Creating table with primary key is not supported yet.\");\n+    }\n+  }\n+\n+  private static PartitionSpec toPartitionSpec(List<String> partitionKeys, Schema icebergSchema) {\n+    PartitionSpec.Builder builder = PartitionSpec.builderFor(icebergSchema);\n+    partitionKeys.forEach(builder::identity);\n+    return builder.build();\n+  }\n+\n+  private static List<String> toPartitionKeys(PartitionSpec spec, Schema icebergSchema) {\n+    List<String> partitionKeys = Lists.newArrayList();\n+    for (PartitionField field : spec.fields()) {\n+      if (field.transform().isIdentity()) {\n+        partitionKeys.add(icebergSchema.findColumnName(field.sourceId()));\n+      } else {\n+        // Not created by Flink SQL.\n+        // For compatibility with iceberg tables, return empty.\n+        // TODO modify this after Flink support partition transform.\n+        return Collections.emptyList();\n+      }\n+    }\n+    return partitionKeys;\n+  }\n+\n+  private static void commitChanges(Table table, String setLocation, String setSnapshotId,\n+                                    String pickSnapshotId, Map<String, String> setProperties) {\n+    // don't allow setting the snapshot and picking a commit at the same time because order is ambiguous and choosing\n+    // one order leads to different results\n+    Preconditions.checkArgument(setSnapshotId == null || pickSnapshotId == null,\n+        \"Cannot set the current the current snapshot ID and cherry-pick snapshot changes\");", "originalCommit": "2d1aed64d2681f41d27c7d945e009a72e51e05d2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "6eb6a5cf0cb4ded46f54aa457d499a835fdf9208", "url": "https://github.com/apache/iceberg/commit/6eb6a5cf0cb4ded46f54aa457d499a835fdf9208", "message": "Address comments", "committedDate": "2020-08-31T03:31:15Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTk4NTI5MA==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479985290", "bodyText": "nit: how about making the iceberg schema -> flink TableSchema conversion to be a static method inside FlinkSchemaUtil ?  The table sink pr #1348 will also depend on this static method (https://github.com/apache/iceberg/pull/1348/files#diff-0ad7dfff9cfa32fbb760796d976fd650R50).", "author": "openinx", "createdAt": "2020-08-31T08:45:02Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -277,20 +287,30 @@ public void alterDatabase(String name, CatalogDatabase newDatabase, boolean igno\n   }\n \n   @Override\n-  public CatalogBaseTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {\n-    try {\n-      Table table = icebergCatalog.loadTable(toIdentifier(tablePath));\n-      TableSchema tableSchema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(table.schema()));\n+  public CatalogTable getTable(ObjectPath tablePath) throws TableNotExistException, CatalogException {\n+    Table table = loadIcebergTable(tablePath);\n+    return toCatalogTable(table);\n+  }\n \n-      // NOTE: We can not create a IcebergCatalogTable, because Flink optimizer may use CatalogTableImpl to copy a new\n-      // catalog table.\n-      // Let's re-loading table from Iceberg catalog when creating source/sink operators.\n-      return new CatalogTableImpl(tableSchema, table.properties(), null);\n+  private Table loadIcebergTable(ObjectPath tablePath) throws TableNotExistException {\n+    try {\n+      return icebergCatalog.loadTable(toIdentifier(tablePath));\n     } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n       throw new TableNotExistException(getName(), tablePath, e);\n     }\n   }\n \n+  private CatalogTable toCatalogTable(Table table) {\n+    TableSchema schema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(table.schema()));", "originalCommit": "6eb6a5cf0cb4ded46f54aa457d499a835fdf9208", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDAwNDExNQ==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r480004115", "bodyText": "static looks good to me, but I think we can keep it in this class.", "author": "JingsongLi", "createdAt": "2020-08-31T09:21:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTk4NTI5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTk4OTYzNg==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479989636", "bodyText": "Do we need to add a TODO indicating that we flink only support identity partition now but will support hidden column future  ?", "author": "openinx", "createdAt": "2020-08-31T08:53:36Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +340,169 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+\n+    ImmutableMap.Builder<String, String> properties = ImmutableMap.builder();\n+    String location = null;\n+    for (Map.Entry<String, String> entry : table.getOptions().entrySet()) {\n+      if (\"location\".equalsIgnoreCase(entry.getKey())) {\n+        location = entry.getValue();\n+      } else {\n+        properties.put(entry.getKey(), entry.getValue());\n+      }\n+    }\n+\n+    try {\n+      icebergCatalog.createTable(\n+          toIdentifier(tablePath),\n+          icebergSchema,\n+          spec,\n+          location,\n+          properties.build());\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n   }\n \n   @Override\n   public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support alterTable now.\");\n+      throws CatalogException, TableNotExistException {\n+    validateFlinkTable(newTable);\n+    Table icebergTable = loadIcebergTable(tablePath);\n+    CatalogTable table = toCatalogTable(icebergTable);\n+\n+    // Currently, Flink SQL only support altering table properties.\n+\n+    // For current Flink Catalog API, support for adding/removing/renaming columns cannot be done by comparing\n+    // CatalogTable instances, unless the Flink schema contains Iceberg column IDs.\n+    if (!table.getSchema().equals(newTable.getSchema())) {\n+      throw new UnsupportedOperationException(\"Altering schema is not supported yet.\");\n+    }\n+\n+    if (!table.getPartitionKeys().equals(((CatalogTable) newTable).getPartitionKeys())) {\n+      throw new UnsupportedOperationException(\"Altering partition keys is not supported yet.\");\n+    }\n+\n+    Map<String, String> oldOptions = table.getOptions();\n+    Map<String, String> setProperties = Maps.newHashMap();\n+\n+    String setLocation = null;\n+    String setSnapshotId = null;\n+    String pickSnapshotId = null;\n+\n+    for (Map.Entry<String, String> entry : newTable.getOptions().entrySet()) {\n+      String key = entry.getKey();\n+      String value = entry.getValue();\n+\n+      if (Objects.equals(value, oldOptions.get(key))) {\n+        continue;\n+      }\n+\n+      if (\"location\".equalsIgnoreCase(key)) {\n+        setLocation = value;\n+      } else if (\"current-snapshot-id\".equalsIgnoreCase(key)) {\n+        setSnapshotId = value;\n+      } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(key)) {\n+        pickSnapshotId = value;\n+      } else {\n+        setProperties.put(key, value);\n+      }\n+    }\n+\n+    oldOptions.keySet().forEach(k -> {\n+      if (!newTable.getOptions().containsKey(k)) {\n+        setProperties.put(k, null);\n+      }\n+    });\n+\n+    commitChanges(icebergTable, setLocation, setSnapshotId, pickSnapshotId, setProperties);\n+  }\n+\n+  private static void validateFlinkTable(CatalogBaseTable table) {\n+    Preconditions.checkArgument(table instanceof CatalogTable, \"The Table should be a CatalogTable.\");\n+\n+    TableSchema schema = table.getSchema();\n+    schema.getTableColumns().forEach(column -> {\n+      if (column.isGenerated()) {\n+        throw new UnsupportedOperationException(\"Creating table with computed columns is not supported yet.\");\n+      }\n+    });\n+\n+    if (!schema.getWatermarkSpecs().isEmpty()) {\n+      throw new UnsupportedOperationException(\"Creating table with watermark specs is not supported yet.\");\n+    }\n+\n+    if (schema.getPrimaryKey().isPresent()) {\n+      throw new UnsupportedOperationException(\"Creating table with primary key is not supported yet.\");\n+    }\n+  }\n+", "originalCommit": "6eb6a5cf0cb4ded46f54aa457d499a835fdf9208", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDAwNTQzNg==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r480005436", "bodyText": "Already have todo in toPartitionKeys.", "author": "JingsongLi", "createdAt": "2020-08-31T09:24:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTk4OTYzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTk5NjI3Mg==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r479996272", "bodyText": "The v should never be null in HashMap ?", "author": "openinx", "createdAt": "2020-08-31T09:05:58Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -320,19 +340,169 @@ public void renameTable(ObjectPath tablePath, String newTableName, boolean ignor\n     }\n   }\n \n-  /**\n-   * TODO Add partitioning to the Flink DDL parser.\n-   */\n   @Override\n   public void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support createTable now.\");\n+      throws CatalogException, TableAlreadyExistException {\n+    validateFlinkTable(table);\n+\n+    Schema icebergSchema = FlinkSchemaUtil.convert(table.getSchema());\n+    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+\n+    ImmutableMap.Builder<String, String> properties = ImmutableMap.builder();\n+    String location = null;\n+    for (Map.Entry<String, String> entry : table.getOptions().entrySet()) {\n+      if (\"location\".equalsIgnoreCase(entry.getKey())) {\n+        location = entry.getValue();\n+      } else {\n+        properties.put(entry.getKey(), entry.getValue());\n+      }\n+    }\n+\n+    try {\n+      icebergCatalog.createTable(\n+          toIdentifier(tablePath),\n+          icebergSchema,\n+          spec,\n+          location,\n+          properties.build());\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistException(getName(), tablePath, e);\n+    }\n   }\n \n   @Override\n   public void alterTable(ObjectPath tablePath, CatalogBaseTable newTable, boolean ignoreIfNotExists)\n-      throws CatalogException {\n-    throw new UnsupportedOperationException(\"Not support alterTable now.\");\n+      throws CatalogException, TableNotExistException {\n+    validateFlinkTable(newTable);\n+    Table icebergTable = loadIcebergTable(tablePath);\n+    CatalogTable table = toCatalogTable(icebergTable);\n+\n+    // Currently, Flink SQL only support altering table properties.\n+\n+    // For current Flink Catalog API, support for adding/removing/renaming columns cannot be done by comparing\n+    // CatalogTable instances, unless the Flink schema contains Iceberg column IDs.\n+    if (!table.getSchema().equals(newTable.getSchema())) {\n+      throw new UnsupportedOperationException(\"Altering schema is not supported yet.\");\n+    }\n+\n+    if (!table.getPartitionKeys().equals(((CatalogTable) newTable).getPartitionKeys())) {\n+      throw new UnsupportedOperationException(\"Altering partition keys is not supported yet.\");\n+    }\n+\n+    Map<String, String> oldOptions = table.getOptions();\n+    Map<String, String> setProperties = Maps.newHashMap();\n+\n+    String setLocation = null;\n+    String setSnapshotId = null;\n+    String pickSnapshotId = null;\n+\n+    for (Map.Entry<String, String> entry : newTable.getOptions().entrySet()) {\n+      String key = entry.getKey();\n+      String value = entry.getValue();\n+\n+      if (Objects.equals(value, oldOptions.get(key))) {\n+        continue;\n+      }\n+\n+      if (\"location\".equalsIgnoreCase(key)) {\n+        setLocation = value;\n+      } else if (\"current-snapshot-id\".equalsIgnoreCase(key)) {\n+        setSnapshotId = value;\n+      } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(key)) {\n+        pickSnapshotId = value;\n+      } else {\n+        setProperties.put(key, value);\n+      }\n+    }\n+\n+    oldOptions.keySet().forEach(k -> {\n+      if (!newTable.getOptions().containsKey(k)) {\n+        setProperties.put(k, null);\n+      }\n+    });\n+\n+    commitChanges(icebergTable, setLocation, setSnapshotId, pickSnapshotId, setProperties);\n+  }\n+\n+  private static void validateFlinkTable(CatalogBaseTable table) {\n+    Preconditions.checkArgument(table instanceof CatalogTable, \"The Table should be a CatalogTable.\");\n+\n+    TableSchema schema = table.getSchema();\n+    schema.getTableColumns().forEach(column -> {\n+      if (column.isGenerated()) {\n+        throw new UnsupportedOperationException(\"Creating table with computed columns is not supported yet.\");\n+      }\n+    });\n+\n+    if (!schema.getWatermarkSpecs().isEmpty()) {\n+      throw new UnsupportedOperationException(\"Creating table with watermark specs is not supported yet.\");\n+    }\n+\n+    if (schema.getPrimaryKey().isPresent()) {\n+      throw new UnsupportedOperationException(\"Creating table with primary key is not supported yet.\");\n+    }\n+  }\n+\n+  private static PartitionSpec toPartitionSpec(List<String> partitionKeys, Schema icebergSchema) {\n+    PartitionSpec.Builder builder = PartitionSpec.builderFor(icebergSchema);\n+    partitionKeys.forEach(builder::identity);\n+    return builder.build();\n+  }\n+\n+  private static List<String> toPartitionKeys(PartitionSpec spec, Schema icebergSchema) {\n+    List<String> partitionKeys = Lists.newArrayList();\n+    for (PartitionField field : spec.fields()) {\n+      if (field.transform().isIdentity()) {\n+        partitionKeys.add(icebergSchema.findColumnName(field.sourceId()));\n+      } else {\n+        // Not created by Flink SQL.\n+        // For compatibility with iceberg tables, return empty.\n+        // TODO modify this after Flink support partition transform.\n+        return Collections.emptyList();\n+      }\n+    }\n+    return partitionKeys;\n+  }\n+\n+  private static void commitChanges(Table table, String setLocation, String setSnapshotId,\n+                                    String pickSnapshotId, Map<String, String> setProperties) {\n+    // don't allow setting the snapshot and picking a commit at the same time because order is ambiguous and choosing\n+    // one order leads to different results\n+    Preconditions.checkArgument(setSnapshotId == null || pickSnapshotId == null,\n+        \"Cannot set the current snapshot ID and cherry-pick snapshot changes\");\n+\n+    if (setSnapshotId != null) {\n+      long newSnapshotId = Long.parseLong(setSnapshotId);\n+      table.manageSnapshots().setCurrentSnapshot(newSnapshotId).commit();\n+    }\n+\n+    // if updating the table snapshot, perform that update first in case it fails\n+    if (pickSnapshotId != null) {\n+      long newSnapshotId = Long.parseLong(pickSnapshotId);\n+      table.manageSnapshots().cherrypick(newSnapshotId).commit();\n+    }\n+\n+    Transaction transaction = table.newTransaction();\n+\n+    if (setLocation != null) {\n+      transaction.updateLocation()\n+          .setLocation(setLocation)\n+          .commit();\n+    }\n+\n+    if (!setProperties.isEmpty()) {\n+      UpdateProperties updateProperties = transaction.updateProperties();\n+      setProperties.forEach((k, v) -> {\n+        if (v == null) {", "originalCommit": "6eb6a5cf0cb4ded46f54aa457d499a835fdf9208", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDAwNjU3MA==", "url": "https://github.com/apache/iceberg/pull/1393#discussion_r480006570", "bodyText": "HashMap allows nulls:\n * Hash table based implementation of the <tt>Map</tt> interface.  This\n * implementation provides all of the optional map operations, and permits\n * <tt>null</tt> values and the <tt>null</tt> key.  (The <tt>HashMap</tt>\n * class is roughly equivalent to <tt>Hashtable</tt>, except that it is\n * unsynchronized and permits nulls.)", "author": "JingsongLi", "createdAt": "2020-08-31T09:26:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTk5NjI3Mg=="}], "type": "inlineReview"}, {"oid": "e1dcb7f35d773e69749180ed337f7711c817c713", "url": "https://github.com/apache/iceberg/commit/e1dcb7f35d773e69749180ed337f7711c817c713", "message": "Address comment", "committedDate": "2020-08-31T09:27:08Z", "type": "commit"}, {"oid": "ac06e524f3191fc303340ec23bc2149c8f93dcf2", "url": "https://github.com/apache/iceberg/commit/ac06e524f3191fc303340ec23bc2149c8f93dcf2", "message": "checkstyles", "committedDate": "2020-09-01T08:13:19Z", "type": "commit"}]}