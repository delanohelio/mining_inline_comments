{"pr_number": 1126, "pr_title": "Convert SparkTableUtil.scala to SparkTableUtil.java", "pr_createdAt": "2020-06-18T23:22:29Z", "pr_url": "https://github.com/apache/iceberg/pull/1126", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjkzMjgxOQ==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r442932819", "bodyText": "We avoid throwing checked exceptions. Although this was the old behavior, I'm reluctant to add them to the signature of the SparkTableUtil methods because they weren't there previously. I think it would be better to wrap these using equivalent Iceberg exception classes that are unchecked, like NoSuchNamespaceException, NoSuchTableException, and ValidationException.", "author": "rdblue", "createdAt": "2020-06-19T16:21:38Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -131,17 +133,17 @@ public void after() throws IOException {\n   }\n \n   @Test\n-  public void testPartitionScan() {\n-    Seq<SparkPartition> partitions = SparkTableUtil.getPartitions(spark, qualifiedTableName);\n+  public void testPartitionScan() throws NoSuchDatabaseException, NoSuchTableException, ParseException {", "originalCommit": "67ba0019b7f043db5049f30d1ee4ffce865dad35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjkzMjg5Mw==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r442932893", "bodyText": "@aokolnychyi, thoughts?", "author": "rdblue", "createdAt": "2020-06-19T16:21:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjkzMjgxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAzNzgxMA==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443037810", "bodyText": "+1 to keeping it consistent with Iceberg conventions regarding use of UncheckedExceptions.", "author": "rdsr", "createdAt": "2020-06-19T20:47:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjkzMjgxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA4MDQzNQ==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443080435", "bodyText": "I am also +1. We have multiple examples in the codebase. For example, HiveTableOperations wraps a checked AlreadyExistsException from Hive into our own unchecked AlreadyExistsException.", "author": "aokolnychyi", "createdAt": "2020-06-19T23:55:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjkzMjgxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA4MDc2Nw==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443080767", "bodyText": "Also, because we may need the same wrapping logic in many places, it probably makes sense to define a function for that and reuse in all places (I mean in the utility class itself, not in tests).", "author": "aokolnychyi", "createdAt": "2020-06-19T23:57:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjkzMjgxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA4MDc3OA==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443080778", "bodyText": "That was the first of the two dilemmas I had. The second being the return types as java.util collections. Let me change all of them to unchecked exceptions.", "author": "raptond", "createdAt": "2020-06-19T23:57:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjkzMjgxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzE4MjExMQ==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443182111", "bodyText": "I have addressed this using a function ExceptionUtil.toUncheckedException.\n@aokolnychyi - Though I have created a class ExceptionUtil.java which does the mapping of specific checked exceptions to unchecked equivalents, I had to place the class inside the spark module. This is because the exceptions like org.apache.spark.sql.catalyst.analysis.NoSuchTableException depend on Spark libraries. Let me know if you want to move it to elsewhere.", "author": "raptond", "createdAt": "2020-06-21T05:03:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjkzMjgxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAzODMxMQ==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443038311", "bodyText": "Since there's no SparkTableUtil.scala in code anymore, I guess it would confuse people as they might look for SparkTableUtil.scala. Maybe just remove this comment?", "author": "rdsr", "createdAt": "2020-06-19T20:49:10Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java", "diffHunk": "@@ -0,0 +1,649 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathFilter;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.orc.OrcMetrics;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.relocated.com.google.common.base.Objects;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.MapPartitionsFunction;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTablePartition;\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog;\n+import org.apache.spark.sql.catalyst.expressions.Expression;\n+import org.apache.spark.sql.catalyst.expressions.NamedExpression;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan;\n+import scala.Function2;\n+import scala.Option;\n+import scala.Some;\n+import scala.Tuple2;\n+import scala.collection.JavaConverters;\n+import scala.runtime.AbstractPartialFunction;\n+\n+import static org.apache.spark.sql.functions.col;\n+\n+/**\n+ * Java version of the original SparkTableUtil.scala", "originalCommit": "67ba0019b7f043db5049f30d1ee4ffce865dad35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA4MTI0MA==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443081240", "bodyText": "I was planning to give the link to the git blob ref of SparkTableUtil.scala and missed it. What do you think about that?", "author": "raptond", "createdAt": "2020-06-20T00:01:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAzODMxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzE4MjI4Ng==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443182286", "bodyText": "The previous release tag of SparkTableUtil.scala is added to the comment - 556754c#diff-531684170cfef0f1493ced85a0c8dbfcR95-R97\nI had thought people could trace the history of the file from the link. I'm ok to remove it as well. @rdsr", "author": "raptond", "createdAt": "2020-06-21T05:07:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAzODMxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkwMTQxNg==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443901416", "bodyText": "sounds good to me.", "author": "rdsr", "createdAt": "2020-06-23T00:51:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAzODMxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA4MTIyMA==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443081220", "bodyText": "nit: let's remove final from the args to be consistent with other places as we don't normally mark args as final and checkstyle should already catch if we try to modify args.", "author": "aokolnychyi", "createdAt": "2020-06-20T00:01:18Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java", "diffHunk": "@@ -0,0 +1,649 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathFilter;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.orc.OrcMetrics;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.relocated.com.google.common.base.Objects;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.MapPartitionsFunction;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTablePartition;\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog;\n+import org.apache.spark.sql.catalyst.expressions.Expression;\n+import org.apache.spark.sql.catalyst.expressions.NamedExpression;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan;\n+import scala.Function2;\n+import scala.Option;\n+import scala.Some;\n+import scala.Tuple2;\n+import scala.collection.JavaConverters;\n+import scala.runtime.AbstractPartialFunction;\n+\n+import static org.apache.spark.sql.functions.col;\n+\n+/**\n+ * Java version of the original SparkTableUtil.scala\n+ */\n+public class SparkTableUtil {\n+\n+  private static final PathFilter HIDDEN_PATH_FILTER =\n+      p -> !p.getName().startsWith(\"_\") && !p.getName().startsWith(\".\");\n+\n+  private SparkTableUtil() {\n+  }\n+\n+  /**\n+   * Returns a DataFrame with a row for each partition in the table.\n+   *\n+   * The DataFrame has 3 columns, partition key (a=1/b=2), partition location, and format\n+   * (avro or parquet).\n+   *\n+   * @param spark a Spark session\n+   * @param table a table name and (optional) database\n+   * @return a DataFrame of the table's partitions\n+   */\n+  public static Dataset<Row> partitionDF(SparkSession spark, String table)\n+      throws NoSuchDatabaseException, NoSuchTableException, ParseException {\n+\n+    List<SparkPartition> partitions = getPartitions(spark, table);\n+    return spark.createDataFrame(partitions, SparkPartition.class).toDF(\"partition\", \"uri\", \"format\");\n+  }\n+\n+  /**\n+   * Returns a DataFrame with a row for each partition that matches the specified 'expression'.\n+   *\n+   * @param spark      a Spark session.\n+   * @param table      name of the table.\n+   * @param expression The expression whose matching partitions are returned.\n+   * @return a DataFrame of the table partitions.\n+   */\n+  public static Dataset<Row> partitionDFByFilter(\n+      SparkSession spark,\n+      String table,\n+      String expression) throws ParseException, NoSuchTableException, NoSuchDatabaseException {\n+\n+    List<SparkPartition> partitions = getPartitionsByFilter(spark, table, expression);\n+    return spark.createDataFrame(partitions, SparkPartition.class).toDF(\"partition\", \"uri\", \"format\");\n+  }\n+\n+  /**\n+   * Returns all partitions in the table.\n+   *\n+   * @param spark a Spark session\n+   * @param table a table name and (optional) database\n+   * @return all table's partitions\n+   */\n+  public static List<SparkPartition> getPartitions(SparkSession spark, String table)\n+      throws NoSuchTableException, NoSuchDatabaseException, ParseException {\n+\n+    TableIdentifier tableIdent = spark.sessionState().sqlParser().parseTableIdentifier(table);\n+    return getPartitions(spark, tableIdent);\n+  }\n+\n+  /**\n+   * Returns all partitions in the table.\n+   *\n+   * @param spark      a Spark session\n+   * @param tableIdent a table identifier\n+   * @return all table's partitions\n+   */\n+  public static List<SparkPartition> getPartitions(SparkSession spark, TableIdentifier tableIdent)\n+      throws NoSuchTableException, NoSuchDatabaseException {\n+    SessionCatalog catalog = spark.sessionState().catalog();\n+    CatalogTable catalogTable = catalog.getTableMetadata(tableIdent);\n+\n+    return JavaConverters\n+        .seqAsJavaListConverter(catalog.listPartitions(tableIdent, Option.empty()))\n+        .asJava()\n+        .stream()\n+        .map(catalogPartition -> toSparkPartition(catalogPartition, catalogTable))\n+        .collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Returns partitions that match the specified 'predicate'.\n+   *\n+   * @param spark     a Spark session\n+   * @param table     a table name and (optional) database\n+   * @param predicate a predicate on partition columns\n+   * @return matching table's partitions\n+   */\n+  public static List<SparkPartition> getPartitionsByFilter(SparkSession spark, String table, String predicate)\n+      throws ParseException, NoSuchTableException, NoSuchDatabaseException {\n+\n+    TableIdentifier tableIdent = spark.sessionState().sqlParser().parseTableIdentifier(table);\n+    Expression unresolvedPredicateExpr = spark.sessionState().sqlParser().parseExpression(predicate);\n+    Expression resolvedPredicateExpr = resolveAttrs(spark, table, unresolvedPredicateExpr);\n+    return getPartitionsByFilter(spark, tableIdent, resolvedPredicateExpr);\n+  }\n+\n+  /**\n+   * Returns partitions that match the specified 'predicate'.\n+   *\n+   * @param spark         a Spark session\n+   * @param tableIdent    a table identifier\n+   * @param predicateExpr a predicate expression on partition columns\n+   * @return matching table's partitions\n+   */\n+  public static List<SparkPartition> getPartitionsByFilter(\n+      SparkSession spark,\n+      TableIdentifier tableIdent,\n+      Expression predicateExpr) throws NoSuchTableException, NoSuchDatabaseException {\n+\n+    SessionCatalog catalog = spark.sessionState().catalog();\n+    CatalogTable catalogTable = catalog.getTableMetadata(tableIdent);\n+\n+    final Expression resolvedPredicateExpr;\n+    if (!predicateExpr.resolved()) {\n+      resolvedPredicateExpr = resolveAttrs(spark, tableIdent.quotedString(), predicateExpr);\n+    } else {\n+      resolvedPredicateExpr = predicateExpr;\n+    }\n+\n+    return JavaConverters.seqAsJavaListConverter(\n+        catalog.listPartitionsByFilter(\n+            tableIdent,\n+            JavaConverters\n+                .collectionAsScalaIterableConverter(ImmutableList.of(resolvedPredicateExpr))\n+                .asScala()\n+                .toSeq()))\n+        .asJava()\n+        .stream()\n+        .map(catalogPartition -> toSparkPartition(catalogPartition, catalogTable))\n+        .collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Returns the data files in a partition by listing the partition location.\n+   *\n+   * For Parquet and ORC partitions, this will read metrics from the file footer. For Avro partitions,\n+   * metrics are set to null.\n+   *\n+   * @param partition     a partition\n+   * @param conf          a serializable Hadoop conf\n+   * @param metricsConfig a metrics conf\n+   * @return a List of [[DataFile]]\n+   */\n+  public static List<DataFile> listPartition(\n+      SparkPartition partition,\n+      PartitionSpec spec,\n+      SerializableConfiguration conf,\n+      MetricsConfig metricsConfig) throws IOException {\n+\n+    return listPartition(\n+        partition.values,\n+        partition.uri,\n+        partition.format,\n+        spec,\n+        conf.get(),\n+        metricsConfig);\n+  }\n+\n+  /**\n+   * Returns the data files in a partition by listing the partition location.\n+   *\n+   * For Parquet and ORC partitions, this will read metrics from the file footer. For Avro partitions,\n+   * metrics are set to null.\n+   *\n+   * @param partition     partition key, e.g., \"a=1/b=2\"\n+   * @param uri           partition location URI\n+   * @param format        partition format, avro or parquet\n+   * @param conf          a Hadoop conf\n+   * @param metricsConfig a metrics conf\n+   * @return a List of [[DataFile]]\n+   */\n+  public static List<DataFile> listPartition(\n+      Map<String, String> partition,\n+      String uri,\n+      String format,\n+      PartitionSpec spec,\n+      Configuration conf,\n+      MetricsConfig metricsConfig) throws IOException {\n+\n+    if (format.contains(\"avro\")) {\n+      return listAvroPartition(partition, uri, spec, conf);\n+    } else if (format.contains(\"parquet\")) {\n+      return listParquetPartition(partition, uri, spec, conf, metricsConfig);\n+    } else if (format.contains(\"orc\")) {\n+      // TODO: use MetricsConfig in listOrcPartition\n+      return listOrcPartition(partition, uri, spec, conf);\n+    } else {\n+      throw new UnsupportedOperationException(\"Unknown partition format: \" + format);\n+    }\n+  }\n+\n+  private static List<DataFile> listAvroPartition(\n+      Map<String, String> partitionPath,\n+      String partitionUri,\n+      PartitionSpec spec,\n+      Configuration conf) throws IOException {\n+\n+    Path partition = new Path(partitionUri);\n+    FileSystem fs = partition.getFileSystem(conf);\n+    return Arrays.stream(fs.listStatus(partition, HIDDEN_PATH_FILTER))\n+        .filter(FileStatus::isFile)\n+        .map(stat -> {\n+          Metrics metrics = new Metrics(-1L, null, null, null);\n+          String partitionKey = spec.fields().stream()\n+              .map(PartitionField::name)\n+              .map(name -> String.format(\"%s=%s\", name, partitionPath.get(name)))\n+              .collect(Collectors.joining(\"/\"));\n+\n+          return DataFiles.builder(spec)\n+              .withPath(stat.getPath().toString())\n+              .withFormat(\"avro\")\n+              .withFileSizeInBytes(stat.getLen())\n+              .withMetrics(metrics)\n+              .withPartitionPath(partitionKey)\n+              .build();\n+\n+        }).collect(Collectors.toList());\n+  }\n+\n+  private static List<DataFile> listParquetPartition(\n+      Map<String, String> partitionPath,\n+      String partitionUri,\n+      PartitionSpec spec,\n+      Configuration conf,\n+      MetricsConfig metricsSpec) throws IOException {\n+\n+    Path partition = new Path(partitionUri);\n+    FileSystem fs = partition.getFileSystem(conf);\n+\n+    return Arrays.stream(fs.listStatus(partition, HIDDEN_PATH_FILTER))\n+        .filter(FileStatus::isFile)\n+        .map(stat -> {\n+          final Metrics metrics;\n+          try {\n+            //noinspection deprecation\n+            metrics = ParquetUtil.footerMetrics(ParquetFileReader.readFooter(conf, stat), metricsSpec);\n+          } catch (IOException e) {\n+            throw new RuntimeException(\"Unable to read footer metrics for: \" + stat.getPath(), e);\n+          }\n+          String partitionKey = spec.fields().stream()\n+              .map(PartitionField::name)\n+              .map(name -> String.format(\"%s=%s\", name, partitionPath.get(name)))\n+              .collect(Collectors.joining(\"/\"));\n+\n+          return DataFiles.builder(spec)\n+              .withPath(stat.getPath().toString())\n+              .withFormat(\"parquet\")\n+              .withFileSizeInBytes(stat.getLen())\n+              .withMetrics(metrics)\n+              .withPartitionPath(partitionKey)\n+              .build();\n+\n+        }).collect(Collectors.toList());\n+  }\n+\n+  private static List<DataFile> listOrcPartition(\n+      Map<String, String> partitionPath,\n+      String partitionUri,\n+      PartitionSpec spec,\n+      Configuration conf) throws IOException {\n+\n+    Path partition = new Path(partitionUri);\n+    FileSystem fs = partition.getFileSystem(conf);\n+\n+    return Arrays.stream(fs.listStatus(partition, HIDDEN_PATH_FILTER))\n+        .filter(FileStatus::isFile)\n+        .map(stat -> {\n+          Metrics metrics = OrcMetrics.fromInputFile(HadoopInputFile.fromPath(stat.getPath(), conf));\n+          String partitionKey = spec.fields().stream()\n+              .map(PartitionField::name)\n+              .map(name -> String.format(\"%s=%s\", name, partitionPath.get(name)))\n+              .collect(Collectors.joining(\"/\"));\n+\n+          return DataFiles.builder(spec)\n+              .withPath(stat.getPath().toString())\n+              .withFormat(\"orc\")\n+              .withFileSizeInBytes(stat.getLen())\n+              .withMetrics(metrics)\n+              .withPartitionPath(partitionKey)\n+              .build();\n+\n+        }).collect(Collectors.toList());\n+  }\n+\n+  private static SparkPartition toSparkPartition(CatalogTablePartition partition, CatalogTable table) {\n+    Option<URI> locationUri = partition.storage().locationUri();\n+    Option<String> serde = partition.storage().serde();\n+\n+    Preconditions.checkArgument(locationUri.nonEmpty(), \"Partition URI should be defined\");\n+    Preconditions.checkArgument(serde.nonEmpty() || table.provider().nonEmpty(),\n+        \"Partition format should be defined\");\n+\n+    String uri = String.valueOf(locationUri.get());\n+    String format = serde.nonEmpty() ? serde.get() : table.provider().get();\n+\n+    return new SparkPartition(\n+        JavaConverters.mapAsJavaMapConverter(partition.spec()).asJava(),\n+        uri,\n+        format\n+    );\n+  }\n+\n+  private static Expression resolveAttrs(SparkSession spark, String table, Expression expr) {\n+    final Function2<String, String, Object> resolver = spark.sessionState().analyzer().resolver();\n+    final LogicalPlan plan = spark.table(table).queryExecution().analyzed();\n+    return expr.transform(new AbstractPartialFunction<Expression, Expression>() {\n+      @Override\n+      public Expression apply(Expression attr) {\n+        UnresolvedAttribute unresolvedAttribute = (UnresolvedAttribute) attr;\n+        Option<NamedExpression> namedExpressionOption = plan.resolve(unresolvedAttribute.nameParts(), resolver);\n+        if (namedExpressionOption.isDefined()) {\n+          return (Expression) namedExpressionOption.get();\n+        } else {\n+          throw new IllegalArgumentException(\n+              String.format(\"Could not resolve %s using columns: %s\", attr, plan.output()));\n+        }\n+      }\n+\n+      @Override\n+      public boolean isDefinedAt(Expression attr) {\n+        return attr instanceof UnresolvedAttribute;\n+      }\n+    });\n+  }\n+\n+  private static Iterator<ManifestFile> buildManifest(\n+      SerializableConfiguration conf,\n+      PartitionSpec spec,\n+      String basePath,\n+      Iterator<Tuple2<String, DataFile>> fileTuples) {\n+    try {\n+      if (fileTuples.hasNext()) {\n+        FileIO io = new HadoopFileIO(conf.get());\n+        TaskContext ctx = TaskContext.get();\n+        Path location =\n+            new Path(basePath, String.format(\"stage-%d-task-%d-manifest\", ctx.stageId(), ctx.taskAttemptId()));\n+        OutputFile outputFile = io.newOutputFile(FileFormat.AVRO.addExtension(location.toString()));\n+        ManifestWriter<DataFile> writer = ManifestFiles.write(spec, outputFile);\n+        try {\n+          fileTuples.forEachRemaining(fileTuple -> writer.add(fileTuple._2));\n+        } finally {\n+          writer.close();\n+        }\n+        ManifestFile manifestFile = writer.toManifestFile();\n+        return ImmutableList.of(manifestFile).iterator();\n+      } else {\n+        return Collections.emptyIterator();\n+      }\n+    } catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+  }\n+\n+  /**\n+   * Import files from an existing Spark table to an Iceberg table.\n+   *\n+   * The import uses the Spark session to get table metadata. It assumes no\n+   * operation is going on the original and target table and thus is not\n+   * thread-safe.\n+   *\n+   * @param spark            a Spark session\n+   * @param sourceTableIdent an identifier of the source Spark table\n+   * @param targetTable      an Iceberg table where to import the data\n+   * @param stagingDir       a staging directory to store temporary manifest files\n+   */\n+  public static void importSparkTable(\n+      SparkSession spark,\n+      TableIdentifier sourceTableIdent,\n+      Table targetTable,\n+      String stagingDir) throws AnalysisException, IOException {\n+\n+    SessionCatalog catalog = spark.sessionState().catalog();\n+\n+    String db = sourceTableIdent.database().nonEmpty() ?\n+        sourceTableIdent.database().get() :\n+        catalog.getCurrentDatabase();\n+    TableIdentifier sourceTableIdentWithDB = new TableIdentifier(sourceTableIdent.table(), Some.apply(db));\n+\n+    if (!catalog.tableExists(sourceTableIdentWithDB)) {\n+      throw new org.apache.iceberg.exceptions.NoSuchTableException(\n+          String.format(\"Table %s does not exist\", sourceTableIdentWithDB));\n+    }\n+\n+    PartitionSpec spec = SparkSchemaUtil.specForTable(spark, sourceTableIdentWithDB.unquotedString());\n+\n+    if (spec == PartitionSpec.unpartitioned()) {\n+      importUnpartitionedSparkTable(spark, sourceTableIdentWithDB, targetTable);\n+    } else {\n+      List<SparkPartition> sourceTablePartitions = getPartitions(spark, sourceTableIdent);\n+      importSparkPartitions(spark, sourceTablePartitions, targetTable, spec, stagingDir);\n+    }\n+  }\n+\n+  private static void importUnpartitionedSparkTable(\n+      SparkSession spark,\n+      TableIdentifier sourceTableIdent,\n+      Table targetTable) throws AnalysisException, IOException {\n+\n+    CatalogTable sourceTable = spark.sessionState().catalog().getTableMetadata(sourceTableIdent);\n+    Option<String> format =\n+        sourceTable.storage().serde().nonEmpty() ? sourceTable.storage().serde() : sourceTable.provider();\n+    Preconditions.checkArgument(format.nonEmpty(), \"Could not determine table format\");\n+\n+    Map<String, String> partition = Collections.emptyMap();\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    Configuration conf = spark.sessionState().newHadoopConf();\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(targetTable.properties());\n+\n+    List<DataFile> files = listPartition(\n+        partition,\n+        sourceTable.location().toString(),\n+        format.get(),\n+        spec,\n+        conf,\n+        metricsConfig);\n+\n+    AppendFiles append = targetTable.newAppend();\n+    files.forEach(append::appendFile);\n+    append.commit();\n+  }\n+\n+  /**\n+   * Import files from given partitions to an Iceberg table.\n+   *\n+   * @param spark       a Spark session\n+   * @param partitions  partitions to import\n+   * @param targetTable an Iceberg table where to import the data\n+   * @param spec        a partition spec\n+   * @param stagingDir  a staging directory to store temporary manifest files\n+   */\n+  public static void importSparkPartitions(\n+      SparkSession spark,\n+      List<SparkPartition> partitions,\n+      Table targetTable,\n+      PartitionSpec spec,\n+      String stagingDir) {\n+\n+    Configuration conf = spark.sessionState().newHadoopConf();\n+    SerializableConfiguration serializableConf = new SerializableConfiguration(conf);\n+    int parallelism = Math.min(partitions.size(), spark.sessionState().conf().parallelPartitionDiscoveryParallelism());\n+    int numShufflePartitions = spark.sessionState().conf().numShufflePartitions();\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(targetTable.properties());\n+\n+    JavaSparkContext sparkContext = JavaSparkContext.fromSparkContext(spark.sparkContext());\n+    JavaRDD<SparkPartition> partitionRDD = sparkContext.parallelize(partitions, parallelism);\n+\n+    Dataset<SparkPartition> partitionDS = spark.createDataset(\n+        partitionRDD.rdd(),\n+        Encoders.javaSerialization(SparkPartition.class));\n+\n+    List<ManifestFile> manifests = partitionDS\n+        .flatMap((FlatMapFunction<SparkPartition, DataFile>) sparkPartition ->\n+                listPartition(\n+                    sparkPartition,\n+                    spec,\n+                    serializableConf,\n+                    metricsConfig).iterator(),\n+            Encoders.javaSerialization(DataFile.class))\n+        .repartition(numShufflePartitions)\n+        .map((MapFunction<DataFile, Tuple2<String, DataFile>>) file ->\n+                Tuple2.apply(file.path().toString(), file),\n+            Encoders.tuple(Encoders.STRING(), Encoders.javaSerialization(DataFile.class)))\n+        .orderBy(col(\"_1\"))\n+        .mapPartitions(\n+            (MapPartitionsFunction<Tuple2<String, DataFile>, ManifestFile>) fileTuple ->\n+                buildManifest(\n+                    serializableConf,\n+                    spec,\n+                    stagingDir,\n+                    fileTuple),\n+            Encoders.javaSerialization(ManifestFile.class))\n+        .collectAsList();\n+\n+    try {\n+      boolean snapshotIdInheritanceEnabled = PropertyUtil.propertyAsBoolean(\n+          targetTable.properties(),\n+          TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED,\n+          TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED_DEFAULT);\n+\n+      AppendFiles append = targetTable.newAppend();\n+      manifests.forEach(append::appendManifest);\n+      append.commit();\n+\n+      if (!snapshotIdInheritanceEnabled) {\n+        // delete original manifests as they were rewritten before the commit\n+        deleteManifests(targetTable.io(), manifests);\n+      }\n+    } catch (Throwable e) {\n+      deleteManifests(targetTable.io(), manifests);\n+      throw e;\n+    }\n+  }\n+\n+  private static void deleteManifests(FileIO io, List<ManifestFile> manifests) {\n+    Tasks.foreach(manifests)\n+        .noRetry()\n+        .suppressFailureWhenFinished()\n+        .run(item -> io.deleteFile(item.path()));\n+  }\n+\n+  /**\n+   * Class representing a table partition.\n+   */\n+  public static class SparkPartition implements Serializable {\n+    private final Map<String, String> values;\n+    private final String uri;\n+    private final String format;\n+\n+    public SparkPartition(final Map<String, String> values, final String uri, final String format) {", "originalCommit": "67ba0019b7f043db5049f30d1ee4ffce865dad35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA4MTU2OA==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443081568", "bodyText": "nit: final", "author": "aokolnychyi", "createdAt": "2020-06-20T00:03:55Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java", "diffHunk": "@@ -0,0 +1,649 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathFilter;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.orc.OrcMetrics;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.relocated.com.google.common.base.Objects;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.MapPartitionsFunction;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTablePartition;\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog;\n+import org.apache.spark.sql.catalyst.expressions.Expression;\n+import org.apache.spark.sql.catalyst.expressions.NamedExpression;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan;\n+import scala.Function2;\n+import scala.Option;\n+import scala.Some;\n+import scala.Tuple2;\n+import scala.collection.JavaConverters;\n+import scala.runtime.AbstractPartialFunction;\n+\n+import static org.apache.spark.sql.functions.col;\n+\n+/**\n+ * Java version of the original SparkTableUtil.scala\n+ */\n+public class SparkTableUtil {\n+\n+  private static final PathFilter HIDDEN_PATH_FILTER =\n+      p -> !p.getName().startsWith(\"_\") && !p.getName().startsWith(\".\");\n+\n+  private SparkTableUtil() {\n+  }\n+\n+  /**\n+   * Returns a DataFrame with a row for each partition in the table.\n+   *\n+   * The DataFrame has 3 columns, partition key (a=1/b=2), partition location, and format\n+   * (avro or parquet).\n+   *\n+   * @param spark a Spark session\n+   * @param table a table name and (optional) database\n+   * @return a DataFrame of the table's partitions\n+   */\n+  public static Dataset<Row> partitionDF(SparkSession spark, String table)\n+      throws NoSuchDatabaseException, NoSuchTableException, ParseException {\n+\n+    List<SparkPartition> partitions = getPartitions(spark, table);\n+    return spark.createDataFrame(partitions, SparkPartition.class).toDF(\"partition\", \"uri\", \"format\");\n+  }\n+\n+  /**\n+   * Returns a DataFrame with a row for each partition that matches the specified 'expression'.\n+   *\n+   * @param spark      a Spark session.\n+   * @param table      name of the table.\n+   * @param expression The expression whose matching partitions are returned.\n+   * @return a DataFrame of the table partitions.\n+   */\n+  public static Dataset<Row> partitionDFByFilter(\n+      SparkSession spark,\n+      String table,\n+      String expression) throws ParseException, NoSuchTableException, NoSuchDatabaseException {\n+\n+    List<SparkPartition> partitions = getPartitionsByFilter(spark, table, expression);\n+    return spark.createDataFrame(partitions, SparkPartition.class).toDF(\"partition\", \"uri\", \"format\");\n+  }\n+\n+  /**\n+   * Returns all partitions in the table.\n+   *\n+   * @param spark a Spark session\n+   * @param table a table name and (optional) database\n+   * @return all table's partitions\n+   */\n+  public static List<SparkPartition> getPartitions(SparkSession spark, String table)\n+      throws NoSuchTableException, NoSuchDatabaseException, ParseException {\n+\n+    TableIdentifier tableIdent = spark.sessionState().sqlParser().parseTableIdentifier(table);\n+    return getPartitions(spark, tableIdent);\n+  }\n+\n+  /**\n+   * Returns all partitions in the table.\n+   *\n+   * @param spark      a Spark session\n+   * @param tableIdent a table identifier\n+   * @return all table's partitions\n+   */\n+  public static List<SparkPartition> getPartitions(SparkSession spark, TableIdentifier tableIdent)\n+      throws NoSuchTableException, NoSuchDatabaseException {\n+    SessionCatalog catalog = spark.sessionState().catalog();\n+    CatalogTable catalogTable = catalog.getTableMetadata(tableIdent);\n+\n+    return JavaConverters\n+        .seqAsJavaListConverter(catalog.listPartitions(tableIdent, Option.empty()))\n+        .asJava()\n+        .stream()\n+        .map(catalogPartition -> toSparkPartition(catalogPartition, catalogTable))\n+        .collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Returns partitions that match the specified 'predicate'.\n+   *\n+   * @param spark     a Spark session\n+   * @param table     a table name and (optional) database\n+   * @param predicate a predicate on partition columns\n+   * @return matching table's partitions\n+   */\n+  public static List<SparkPartition> getPartitionsByFilter(SparkSession spark, String table, String predicate)\n+      throws ParseException, NoSuchTableException, NoSuchDatabaseException {\n+\n+    TableIdentifier tableIdent = spark.sessionState().sqlParser().parseTableIdentifier(table);\n+    Expression unresolvedPredicateExpr = spark.sessionState().sqlParser().parseExpression(predicate);\n+    Expression resolvedPredicateExpr = resolveAttrs(spark, table, unresolvedPredicateExpr);\n+    return getPartitionsByFilter(spark, tableIdent, resolvedPredicateExpr);\n+  }\n+\n+  /**\n+   * Returns partitions that match the specified 'predicate'.\n+   *\n+   * @param spark         a Spark session\n+   * @param tableIdent    a table identifier\n+   * @param predicateExpr a predicate expression on partition columns\n+   * @return matching table's partitions\n+   */\n+  public static List<SparkPartition> getPartitionsByFilter(\n+      SparkSession spark,\n+      TableIdentifier tableIdent,\n+      Expression predicateExpr) throws NoSuchTableException, NoSuchDatabaseException {\n+\n+    SessionCatalog catalog = spark.sessionState().catalog();\n+    CatalogTable catalogTable = catalog.getTableMetadata(tableIdent);\n+\n+    final Expression resolvedPredicateExpr;", "originalCommit": "67ba0019b7f043db5049f30d1ee4ffce865dad35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA4MTc4Nw==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443081787", "bodyText": "We have RuntimeIOException in Iceberg if we want to get rid of this checked exception too.", "author": "aokolnychyi", "createdAt": "2020-06-20T00:05:53Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java", "diffHunk": "@@ -0,0 +1,649 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathFilter;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.orc.OrcMetrics;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.relocated.com.google.common.base.Objects;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.MapPartitionsFunction;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTablePartition;\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog;\n+import org.apache.spark.sql.catalyst.expressions.Expression;\n+import org.apache.spark.sql.catalyst.expressions.NamedExpression;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan;\n+import scala.Function2;\n+import scala.Option;\n+import scala.Some;\n+import scala.Tuple2;\n+import scala.collection.JavaConverters;\n+import scala.runtime.AbstractPartialFunction;\n+\n+import static org.apache.spark.sql.functions.col;\n+\n+/**\n+ * Java version of the original SparkTableUtil.scala\n+ */\n+public class SparkTableUtil {\n+\n+  private static final PathFilter HIDDEN_PATH_FILTER =\n+      p -> !p.getName().startsWith(\"_\") && !p.getName().startsWith(\".\");\n+\n+  private SparkTableUtil() {\n+  }\n+\n+  /**\n+   * Returns a DataFrame with a row for each partition in the table.\n+   *\n+   * The DataFrame has 3 columns, partition key (a=1/b=2), partition location, and format\n+   * (avro or parquet).\n+   *\n+   * @param spark a Spark session\n+   * @param table a table name and (optional) database\n+   * @return a DataFrame of the table's partitions\n+   */\n+  public static Dataset<Row> partitionDF(SparkSession spark, String table)\n+      throws NoSuchDatabaseException, NoSuchTableException, ParseException {\n+\n+    List<SparkPartition> partitions = getPartitions(spark, table);\n+    return spark.createDataFrame(partitions, SparkPartition.class).toDF(\"partition\", \"uri\", \"format\");\n+  }\n+\n+  /**\n+   * Returns a DataFrame with a row for each partition that matches the specified 'expression'.\n+   *\n+   * @param spark      a Spark session.\n+   * @param table      name of the table.\n+   * @param expression The expression whose matching partitions are returned.\n+   * @return a DataFrame of the table partitions.\n+   */\n+  public static Dataset<Row> partitionDFByFilter(\n+      SparkSession spark,\n+      String table,\n+      String expression) throws ParseException, NoSuchTableException, NoSuchDatabaseException {\n+\n+    List<SparkPartition> partitions = getPartitionsByFilter(spark, table, expression);\n+    return spark.createDataFrame(partitions, SparkPartition.class).toDF(\"partition\", \"uri\", \"format\");\n+  }\n+\n+  /**\n+   * Returns all partitions in the table.\n+   *\n+   * @param spark a Spark session\n+   * @param table a table name and (optional) database\n+   * @return all table's partitions\n+   */\n+  public static List<SparkPartition> getPartitions(SparkSession spark, String table)\n+      throws NoSuchTableException, NoSuchDatabaseException, ParseException {\n+\n+    TableIdentifier tableIdent = spark.sessionState().sqlParser().parseTableIdentifier(table);\n+    return getPartitions(spark, tableIdent);\n+  }\n+\n+  /**\n+   * Returns all partitions in the table.\n+   *\n+   * @param spark      a Spark session\n+   * @param tableIdent a table identifier\n+   * @return all table's partitions\n+   */\n+  public static List<SparkPartition> getPartitions(SparkSession spark, TableIdentifier tableIdent)\n+      throws NoSuchTableException, NoSuchDatabaseException {\n+    SessionCatalog catalog = spark.sessionState().catalog();\n+    CatalogTable catalogTable = catalog.getTableMetadata(tableIdent);\n+\n+    return JavaConverters\n+        .seqAsJavaListConverter(catalog.listPartitions(tableIdent, Option.empty()))\n+        .asJava()\n+        .stream()\n+        .map(catalogPartition -> toSparkPartition(catalogPartition, catalogTable))\n+        .collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Returns partitions that match the specified 'predicate'.\n+   *\n+   * @param spark     a Spark session\n+   * @param table     a table name and (optional) database\n+   * @param predicate a predicate on partition columns\n+   * @return matching table's partitions\n+   */\n+  public static List<SparkPartition> getPartitionsByFilter(SparkSession spark, String table, String predicate)\n+      throws ParseException, NoSuchTableException, NoSuchDatabaseException {\n+\n+    TableIdentifier tableIdent = spark.sessionState().sqlParser().parseTableIdentifier(table);\n+    Expression unresolvedPredicateExpr = spark.sessionState().sqlParser().parseExpression(predicate);\n+    Expression resolvedPredicateExpr = resolveAttrs(spark, table, unresolvedPredicateExpr);\n+    return getPartitionsByFilter(spark, tableIdent, resolvedPredicateExpr);\n+  }\n+\n+  /**\n+   * Returns partitions that match the specified 'predicate'.\n+   *\n+   * @param spark         a Spark session\n+   * @param tableIdent    a table identifier\n+   * @param predicateExpr a predicate expression on partition columns\n+   * @return matching table's partitions\n+   */\n+  public static List<SparkPartition> getPartitionsByFilter(\n+      SparkSession spark,\n+      TableIdentifier tableIdent,\n+      Expression predicateExpr) throws NoSuchTableException, NoSuchDatabaseException {\n+\n+    SessionCatalog catalog = spark.sessionState().catalog();\n+    CatalogTable catalogTable = catalog.getTableMetadata(tableIdent);\n+\n+    final Expression resolvedPredicateExpr;\n+    if (!predicateExpr.resolved()) {\n+      resolvedPredicateExpr = resolveAttrs(spark, tableIdent.quotedString(), predicateExpr);\n+    } else {\n+      resolvedPredicateExpr = predicateExpr;\n+    }\n+\n+    return JavaConverters.seqAsJavaListConverter(\n+        catalog.listPartitionsByFilter(\n+            tableIdent,\n+            JavaConverters\n+                .collectionAsScalaIterableConverter(ImmutableList.of(resolvedPredicateExpr))\n+                .asScala()\n+                .toSeq()))\n+        .asJava()\n+        .stream()\n+        .map(catalogPartition -> toSparkPartition(catalogPartition, catalogTable))\n+        .collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Returns the data files in a partition by listing the partition location.\n+   *\n+   * For Parquet and ORC partitions, this will read metrics from the file footer. For Avro partitions,\n+   * metrics are set to null.\n+   *\n+   * @param partition     a partition\n+   * @param conf          a serializable Hadoop conf\n+   * @param metricsConfig a metrics conf\n+   * @return a List of [[DataFile]]\n+   */\n+  public static List<DataFile> listPartition(\n+      SparkPartition partition,\n+      PartitionSpec spec,\n+      SerializableConfiguration conf,\n+      MetricsConfig metricsConfig) throws IOException {", "originalCommit": "67ba0019b7f043db5049f30d1ee4ffce865dad35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA4MTk3OQ==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443081979", "bodyText": "Does this comment still apply in Java? Isn't it specific to Scalastyle?", "author": "aokolnychyi", "createdAt": "2020-06-20T00:07:42Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java", "diffHunk": "@@ -0,0 +1,649 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathFilter;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.orc.OrcMetrics;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.relocated.com.google.common.base.Objects;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.MapPartitionsFunction;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTablePartition;\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog;\n+import org.apache.spark.sql.catalyst.expressions.Expression;\n+import org.apache.spark.sql.catalyst.expressions.NamedExpression;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan;\n+import scala.Function2;\n+import scala.Option;\n+import scala.Some;\n+import scala.Tuple2;\n+import scala.collection.JavaConverters;\n+import scala.runtime.AbstractPartialFunction;\n+\n+import static org.apache.spark.sql.functions.col;\n+\n+/**\n+ * Java version of the original SparkTableUtil.scala\n+ */\n+public class SparkTableUtil {\n+\n+  private static final PathFilter HIDDEN_PATH_FILTER =\n+      p -> !p.getName().startsWith(\"_\") && !p.getName().startsWith(\".\");\n+\n+  private SparkTableUtil() {\n+  }\n+\n+  /**\n+   * Returns a DataFrame with a row for each partition in the table.\n+   *\n+   * The DataFrame has 3 columns, partition key (a=1/b=2), partition location, and format\n+   * (avro or parquet).\n+   *\n+   * @param spark a Spark session\n+   * @param table a table name and (optional) database\n+   * @return a DataFrame of the table's partitions\n+   */\n+  public static Dataset<Row> partitionDF(SparkSession spark, String table)\n+      throws NoSuchDatabaseException, NoSuchTableException, ParseException {\n+\n+    List<SparkPartition> partitions = getPartitions(spark, table);\n+    return spark.createDataFrame(partitions, SparkPartition.class).toDF(\"partition\", \"uri\", \"format\");\n+  }\n+\n+  /**\n+   * Returns a DataFrame with a row for each partition that matches the specified 'expression'.\n+   *\n+   * @param spark      a Spark session.\n+   * @param table      name of the table.\n+   * @param expression The expression whose matching partitions are returned.\n+   * @return a DataFrame of the table partitions.\n+   */\n+  public static Dataset<Row> partitionDFByFilter(\n+      SparkSession spark,\n+      String table,\n+      String expression) throws ParseException, NoSuchTableException, NoSuchDatabaseException {\n+\n+    List<SparkPartition> partitions = getPartitionsByFilter(spark, table, expression);\n+    return spark.createDataFrame(partitions, SparkPartition.class).toDF(\"partition\", \"uri\", \"format\");\n+  }\n+\n+  /**\n+   * Returns all partitions in the table.\n+   *\n+   * @param spark a Spark session\n+   * @param table a table name and (optional) database\n+   * @return all table's partitions\n+   */\n+  public static List<SparkPartition> getPartitions(SparkSession spark, String table)\n+      throws NoSuchTableException, NoSuchDatabaseException, ParseException {\n+\n+    TableIdentifier tableIdent = spark.sessionState().sqlParser().parseTableIdentifier(table);\n+    return getPartitions(spark, tableIdent);\n+  }\n+\n+  /**\n+   * Returns all partitions in the table.\n+   *\n+   * @param spark      a Spark session\n+   * @param tableIdent a table identifier\n+   * @return all table's partitions\n+   */\n+  public static List<SparkPartition> getPartitions(SparkSession spark, TableIdentifier tableIdent)\n+      throws NoSuchTableException, NoSuchDatabaseException {\n+    SessionCatalog catalog = spark.sessionState().catalog();\n+    CatalogTable catalogTable = catalog.getTableMetadata(tableIdent);\n+\n+    return JavaConverters\n+        .seqAsJavaListConverter(catalog.listPartitions(tableIdent, Option.empty()))\n+        .asJava()\n+        .stream()\n+        .map(catalogPartition -> toSparkPartition(catalogPartition, catalogTable))\n+        .collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Returns partitions that match the specified 'predicate'.\n+   *\n+   * @param spark     a Spark session\n+   * @param table     a table name and (optional) database\n+   * @param predicate a predicate on partition columns\n+   * @return matching table's partitions\n+   */\n+  public static List<SparkPartition> getPartitionsByFilter(SparkSession spark, String table, String predicate)\n+      throws ParseException, NoSuchTableException, NoSuchDatabaseException {\n+\n+    TableIdentifier tableIdent = spark.sessionState().sqlParser().parseTableIdentifier(table);\n+    Expression unresolvedPredicateExpr = spark.sessionState().sqlParser().parseExpression(predicate);\n+    Expression resolvedPredicateExpr = resolveAttrs(spark, table, unresolvedPredicateExpr);\n+    return getPartitionsByFilter(spark, tableIdent, resolvedPredicateExpr);\n+  }\n+\n+  /**\n+   * Returns partitions that match the specified 'predicate'.\n+   *\n+   * @param spark         a Spark session\n+   * @param tableIdent    a table identifier\n+   * @param predicateExpr a predicate expression on partition columns\n+   * @return matching table's partitions\n+   */\n+  public static List<SparkPartition> getPartitionsByFilter(\n+      SparkSession spark,\n+      TableIdentifier tableIdent,\n+      Expression predicateExpr) throws NoSuchTableException, NoSuchDatabaseException {\n+\n+    SessionCatalog catalog = spark.sessionState().catalog();\n+    CatalogTable catalogTable = catalog.getTableMetadata(tableIdent);\n+\n+    final Expression resolvedPredicateExpr;\n+    if (!predicateExpr.resolved()) {\n+      resolvedPredicateExpr = resolveAttrs(spark, tableIdent.quotedString(), predicateExpr);\n+    } else {\n+      resolvedPredicateExpr = predicateExpr;\n+    }\n+\n+    return JavaConverters.seqAsJavaListConverter(\n+        catalog.listPartitionsByFilter(\n+            tableIdent,\n+            JavaConverters\n+                .collectionAsScalaIterableConverter(ImmutableList.of(resolvedPredicateExpr))\n+                .asScala()\n+                .toSeq()))\n+        .asJava()\n+        .stream()\n+        .map(catalogPartition -> toSparkPartition(catalogPartition, catalogTable))\n+        .collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Returns the data files in a partition by listing the partition location.\n+   *\n+   * For Parquet and ORC partitions, this will read metrics from the file footer. For Avro partitions,\n+   * metrics are set to null.\n+   *\n+   * @param partition     a partition\n+   * @param conf          a serializable Hadoop conf\n+   * @param metricsConfig a metrics conf\n+   * @return a List of [[DataFile]]\n+   */\n+  public static List<DataFile> listPartition(\n+      SparkPartition partition,\n+      PartitionSpec spec,\n+      SerializableConfiguration conf,\n+      MetricsConfig metricsConfig) throws IOException {\n+\n+    return listPartition(\n+        partition.values,\n+        partition.uri,\n+        partition.format,\n+        spec,\n+        conf.get(),\n+        metricsConfig);\n+  }\n+\n+  /**\n+   * Returns the data files in a partition by listing the partition location.\n+   *\n+   * For Parquet and ORC partitions, this will read metrics from the file footer. For Avro partitions,\n+   * metrics are set to null.\n+   *\n+   * @param partition     partition key, e.g., \"a=1/b=2\"\n+   * @param uri           partition location URI\n+   * @param format        partition format, avro or parquet\n+   * @param conf          a Hadoop conf\n+   * @param metricsConfig a metrics conf\n+   * @return a List of [[DataFile]]\n+   */\n+  public static List<DataFile> listPartition(\n+      Map<String, String> partition,\n+      String uri,\n+      String format,\n+      PartitionSpec spec,\n+      Configuration conf,\n+      MetricsConfig metricsConfig) throws IOException {\n+\n+    if (format.contains(\"avro\")) {\n+      return listAvroPartition(partition, uri, spec, conf);\n+    } else if (format.contains(\"parquet\")) {\n+      return listParquetPartition(partition, uri, spec, conf, metricsConfig);\n+    } else if (format.contains(\"orc\")) {\n+      // TODO: use MetricsConfig in listOrcPartition\n+      return listOrcPartition(partition, uri, spec, conf);\n+    } else {\n+      throw new UnsupportedOperationException(\"Unknown partition format: \" + format);\n+    }\n+  }\n+\n+  private static List<DataFile> listAvroPartition(\n+      Map<String, String> partitionPath,\n+      String partitionUri,\n+      PartitionSpec spec,\n+      Configuration conf) throws IOException {\n+\n+    Path partition = new Path(partitionUri);\n+    FileSystem fs = partition.getFileSystem(conf);\n+    return Arrays.stream(fs.listStatus(partition, HIDDEN_PATH_FILTER))\n+        .filter(FileStatus::isFile)\n+        .map(stat -> {\n+          Metrics metrics = new Metrics(-1L, null, null, null);\n+          String partitionKey = spec.fields().stream()\n+              .map(PartitionField::name)\n+              .map(name -> String.format(\"%s=%s\", name, partitionPath.get(name)))\n+              .collect(Collectors.joining(\"/\"));\n+\n+          return DataFiles.builder(spec)\n+              .withPath(stat.getPath().toString())\n+              .withFormat(\"avro\")\n+              .withFileSizeInBytes(stat.getLen())\n+              .withMetrics(metrics)\n+              .withPartitionPath(partitionKey)\n+              .build();\n+\n+        }).collect(Collectors.toList());\n+  }\n+\n+  private static List<DataFile> listParquetPartition(\n+      Map<String, String> partitionPath,\n+      String partitionUri,\n+      PartitionSpec spec,\n+      Configuration conf,\n+      MetricsConfig metricsSpec) throws IOException {\n+\n+    Path partition = new Path(partitionUri);\n+    FileSystem fs = partition.getFileSystem(conf);\n+\n+    return Arrays.stream(fs.listStatus(partition, HIDDEN_PATH_FILTER))\n+        .filter(FileStatus::isFile)\n+        .map(stat -> {\n+          final Metrics metrics;\n+          try {\n+            //noinspection deprecation", "originalCommit": "67ba0019b7f043db5049f30d1ee4ffce865dad35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzE4MjM0Mw==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443182343", "bodyText": "This was an intellij suppression for the deprecated usage of ParquetFileReader.readFooter. I removed it now.", "author": "raptond", "createdAt": "2020-06-21T05:08:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA4MTk3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA4MjAxMQ==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443082011", "bodyText": "It can be RuntimeIOException", "author": "aokolnychyi", "createdAt": "2020-06-20T00:07:59Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java", "diffHunk": "@@ -0,0 +1,649 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathFilter;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.orc.OrcMetrics;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.relocated.com.google.common.base.Objects;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.MapPartitionsFunction;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTablePartition;\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog;\n+import org.apache.spark.sql.catalyst.expressions.Expression;\n+import org.apache.spark.sql.catalyst.expressions.NamedExpression;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan;\n+import scala.Function2;\n+import scala.Option;\n+import scala.Some;\n+import scala.Tuple2;\n+import scala.collection.JavaConverters;\n+import scala.runtime.AbstractPartialFunction;\n+\n+import static org.apache.spark.sql.functions.col;\n+\n+/**\n+ * Java version of the original SparkTableUtil.scala\n+ */\n+public class SparkTableUtil {\n+\n+  private static final PathFilter HIDDEN_PATH_FILTER =\n+      p -> !p.getName().startsWith(\"_\") && !p.getName().startsWith(\".\");\n+\n+  private SparkTableUtil() {\n+  }\n+\n+  /**\n+   * Returns a DataFrame with a row for each partition in the table.\n+   *\n+   * The DataFrame has 3 columns, partition key (a=1/b=2), partition location, and format\n+   * (avro or parquet).\n+   *\n+   * @param spark a Spark session\n+   * @param table a table name and (optional) database\n+   * @return a DataFrame of the table's partitions\n+   */\n+  public static Dataset<Row> partitionDF(SparkSession spark, String table)\n+      throws NoSuchDatabaseException, NoSuchTableException, ParseException {\n+\n+    List<SparkPartition> partitions = getPartitions(spark, table);\n+    return spark.createDataFrame(partitions, SparkPartition.class).toDF(\"partition\", \"uri\", \"format\");\n+  }\n+\n+  /**\n+   * Returns a DataFrame with a row for each partition that matches the specified 'expression'.\n+   *\n+   * @param spark      a Spark session.\n+   * @param table      name of the table.\n+   * @param expression The expression whose matching partitions are returned.\n+   * @return a DataFrame of the table partitions.\n+   */\n+  public static Dataset<Row> partitionDFByFilter(\n+      SparkSession spark,\n+      String table,\n+      String expression) throws ParseException, NoSuchTableException, NoSuchDatabaseException {\n+\n+    List<SparkPartition> partitions = getPartitionsByFilter(spark, table, expression);\n+    return spark.createDataFrame(partitions, SparkPartition.class).toDF(\"partition\", \"uri\", \"format\");\n+  }\n+\n+  /**\n+   * Returns all partitions in the table.\n+   *\n+   * @param spark a Spark session\n+   * @param table a table name and (optional) database\n+   * @return all table's partitions\n+   */\n+  public static List<SparkPartition> getPartitions(SparkSession spark, String table)\n+      throws NoSuchTableException, NoSuchDatabaseException, ParseException {\n+\n+    TableIdentifier tableIdent = spark.sessionState().sqlParser().parseTableIdentifier(table);\n+    return getPartitions(spark, tableIdent);\n+  }\n+\n+  /**\n+   * Returns all partitions in the table.\n+   *\n+   * @param spark      a Spark session\n+   * @param tableIdent a table identifier\n+   * @return all table's partitions\n+   */\n+  public static List<SparkPartition> getPartitions(SparkSession spark, TableIdentifier tableIdent)\n+      throws NoSuchTableException, NoSuchDatabaseException {\n+    SessionCatalog catalog = spark.sessionState().catalog();\n+    CatalogTable catalogTable = catalog.getTableMetadata(tableIdent);\n+\n+    return JavaConverters\n+        .seqAsJavaListConverter(catalog.listPartitions(tableIdent, Option.empty()))\n+        .asJava()\n+        .stream()\n+        .map(catalogPartition -> toSparkPartition(catalogPartition, catalogTable))\n+        .collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Returns partitions that match the specified 'predicate'.\n+   *\n+   * @param spark     a Spark session\n+   * @param table     a table name and (optional) database\n+   * @param predicate a predicate on partition columns\n+   * @return matching table's partitions\n+   */\n+  public static List<SparkPartition> getPartitionsByFilter(SparkSession spark, String table, String predicate)\n+      throws ParseException, NoSuchTableException, NoSuchDatabaseException {\n+\n+    TableIdentifier tableIdent = spark.sessionState().sqlParser().parseTableIdentifier(table);\n+    Expression unresolvedPredicateExpr = spark.sessionState().sqlParser().parseExpression(predicate);\n+    Expression resolvedPredicateExpr = resolveAttrs(spark, table, unresolvedPredicateExpr);\n+    return getPartitionsByFilter(spark, tableIdent, resolvedPredicateExpr);\n+  }\n+\n+  /**\n+   * Returns partitions that match the specified 'predicate'.\n+   *\n+   * @param spark         a Spark session\n+   * @param tableIdent    a table identifier\n+   * @param predicateExpr a predicate expression on partition columns\n+   * @return matching table's partitions\n+   */\n+  public static List<SparkPartition> getPartitionsByFilter(\n+      SparkSession spark,\n+      TableIdentifier tableIdent,\n+      Expression predicateExpr) throws NoSuchTableException, NoSuchDatabaseException {\n+\n+    SessionCatalog catalog = spark.sessionState().catalog();\n+    CatalogTable catalogTable = catalog.getTableMetadata(tableIdent);\n+\n+    final Expression resolvedPredicateExpr;\n+    if (!predicateExpr.resolved()) {\n+      resolvedPredicateExpr = resolveAttrs(spark, tableIdent.quotedString(), predicateExpr);\n+    } else {\n+      resolvedPredicateExpr = predicateExpr;\n+    }\n+\n+    return JavaConverters.seqAsJavaListConverter(\n+        catalog.listPartitionsByFilter(\n+            tableIdent,\n+            JavaConverters\n+                .collectionAsScalaIterableConverter(ImmutableList.of(resolvedPredicateExpr))\n+                .asScala()\n+                .toSeq()))\n+        .asJava()\n+        .stream()\n+        .map(catalogPartition -> toSparkPartition(catalogPartition, catalogTable))\n+        .collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Returns the data files in a partition by listing the partition location.\n+   *\n+   * For Parquet and ORC partitions, this will read metrics from the file footer. For Avro partitions,\n+   * metrics are set to null.\n+   *\n+   * @param partition     a partition\n+   * @param conf          a serializable Hadoop conf\n+   * @param metricsConfig a metrics conf\n+   * @return a List of [[DataFile]]\n+   */\n+  public static List<DataFile> listPartition(\n+      SparkPartition partition,\n+      PartitionSpec spec,\n+      SerializableConfiguration conf,\n+      MetricsConfig metricsConfig) throws IOException {\n+\n+    return listPartition(\n+        partition.values,\n+        partition.uri,\n+        partition.format,\n+        spec,\n+        conf.get(),\n+        metricsConfig);\n+  }\n+\n+  /**\n+   * Returns the data files in a partition by listing the partition location.\n+   *\n+   * For Parquet and ORC partitions, this will read metrics from the file footer. For Avro partitions,\n+   * metrics are set to null.\n+   *\n+   * @param partition     partition key, e.g., \"a=1/b=2\"\n+   * @param uri           partition location URI\n+   * @param format        partition format, avro or parquet\n+   * @param conf          a Hadoop conf\n+   * @param metricsConfig a metrics conf\n+   * @return a List of [[DataFile]]\n+   */\n+  public static List<DataFile> listPartition(\n+      Map<String, String> partition,\n+      String uri,\n+      String format,\n+      PartitionSpec spec,\n+      Configuration conf,\n+      MetricsConfig metricsConfig) throws IOException {\n+\n+    if (format.contains(\"avro\")) {\n+      return listAvroPartition(partition, uri, spec, conf);\n+    } else if (format.contains(\"parquet\")) {\n+      return listParquetPartition(partition, uri, spec, conf, metricsConfig);\n+    } else if (format.contains(\"orc\")) {\n+      // TODO: use MetricsConfig in listOrcPartition\n+      return listOrcPartition(partition, uri, spec, conf);\n+    } else {\n+      throw new UnsupportedOperationException(\"Unknown partition format: \" + format);\n+    }\n+  }\n+\n+  private static List<DataFile> listAvroPartition(\n+      Map<String, String> partitionPath,\n+      String partitionUri,\n+      PartitionSpec spec,\n+      Configuration conf) throws IOException {\n+\n+    Path partition = new Path(partitionUri);\n+    FileSystem fs = partition.getFileSystem(conf);\n+    return Arrays.stream(fs.listStatus(partition, HIDDEN_PATH_FILTER))\n+        .filter(FileStatus::isFile)\n+        .map(stat -> {\n+          Metrics metrics = new Metrics(-1L, null, null, null);\n+          String partitionKey = spec.fields().stream()\n+              .map(PartitionField::name)\n+              .map(name -> String.format(\"%s=%s\", name, partitionPath.get(name)))\n+              .collect(Collectors.joining(\"/\"));\n+\n+          return DataFiles.builder(spec)\n+              .withPath(stat.getPath().toString())\n+              .withFormat(\"avro\")\n+              .withFileSizeInBytes(stat.getLen())\n+              .withMetrics(metrics)\n+              .withPartitionPath(partitionKey)\n+              .build();\n+\n+        }).collect(Collectors.toList());\n+  }\n+\n+  private static List<DataFile> listParquetPartition(\n+      Map<String, String> partitionPath,\n+      String partitionUri,\n+      PartitionSpec spec,\n+      Configuration conf,\n+      MetricsConfig metricsSpec) throws IOException {\n+\n+    Path partition = new Path(partitionUri);\n+    FileSystem fs = partition.getFileSystem(conf);\n+\n+    return Arrays.stream(fs.listStatus(partition, HIDDEN_PATH_FILTER))\n+        .filter(FileStatus::isFile)\n+        .map(stat -> {\n+          final Metrics metrics;\n+          try {\n+            //noinspection deprecation\n+            metrics = ParquetUtil.footerMetrics(ParquetFileReader.readFooter(conf, stat), metricsSpec);\n+          } catch (IOException e) {\n+            throw new RuntimeException(\"Unable to read footer metrics for: \" + stat.getPath(), e);", "originalCommit": "67ba0019b7f043db5049f30d1ee4ffce865dad35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA4MjIzMA==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443082230", "bodyText": "Will it make sense to split this into two lines?\nString suffix = String.format(\"stage-%d-task-%d-manifest\", ctx.stageId(), ctx.taskAttemptId());\nPath location = new Path(basePath, suffix);", "author": "aokolnychyi", "createdAt": "2020-06-20T00:10:09Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java", "diffHunk": "@@ -0,0 +1,649 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathFilter;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.orc.OrcMetrics;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.relocated.com.google.common.base.Objects;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.MapPartitionsFunction;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTablePartition;\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog;\n+import org.apache.spark.sql.catalyst.expressions.Expression;\n+import org.apache.spark.sql.catalyst.expressions.NamedExpression;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan;\n+import scala.Function2;\n+import scala.Option;\n+import scala.Some;\n+import scala.Tuple2;\n+import scala.collection.JavaConverters;\n+import scala.runtime.AbstractPartialFunction;\n+\n+import static org.apache.spark.sql.functions.col;\n+\n+/**\n+ * Java version of the original SparkTableUtil.scala\n+ */\n+public class SparkTableUtil {\n+\n+  private static final PathFilter HIDDEN_PATH_FILTER =\n+      p -> !p.getName().startsWith(\"_\") && !p.getName().startsWith(\".\");\n+\n+  private SparkTableUtil() {\n+  }\n+\n+  /**\n+   * Returns a DataFrame with a row for each partition in the table.\n+   *\n+   * The DataFrame has 3 columns, partition key (a=1/b=2), partition location, and format\n+   * (avro or parquet).\n+   *\n+   * @param spark a Spark session\n+   * @param table a table name and (optional) database\n+   * @return a DataFrame of the table's partitions\n+   */\n+  public static Dataset<Row> partitionDF(SparkSession spark, String table)\n+      throws NoSuchDatabaseException, NoSuchTableException, ParseException {\n+\n+    List<SparkPartition> partitions = getPartitions(spark, table);\n+    return spark.createDataFrame(partitions, SparkPartition.class).toDF(\"partition\", \"uri\", \"format\");\n+  }\n+\n+  /**\n+   * Returns a DataFrame with a row for each partition that matches the specified 'expression'.\n+   *\n+   * @param spark      a Spark session.\n+   * @param table      name of the table.\n+   * @param expression The expression whose matching partitions are returned.\n+   * @return a DataFrame of the table partitions.\n+   */\n+  public static Dataset<Row> partitionDFByFilter(\n+      SparkSession spark,\n+      String table,\n+      String expression) throws ParseException, NoSuchTableException, NoSuchDatabaseException {\n+\n+    List<SparkPartition> partitions = getPartitionsByFilter(spark, table, expression);\n+    return spark.createDataFrame(partitions, SparkPartition.class).toDF(\"partition\", \"uri\", \"format\");\n+  }\n+\n+  /**\n+   * Returns all partitions in the table.\n+   *\n+   * @param spark a Spark session\n+   * @param table a table name and (optional) database\n+   * @return all table's partitions\n+   */\n+  public static List<SparkPartition> getPartitions(SparkSession spark, String table)\n+      throws NoSuchTableException, NoSuchDatabaseException, ParseException {\n+\n+    TableIdentifier tableIdent = spark.sessionState().sqlParser().parseTableIdentifier(table);\n+    return getPartitions(spark, tableIdent);\n+  }\n+\n+  /**\n+   * Returns all partitions in the table.\n+   *\n+   * @param spark      a Spark session\n+   * @param tableIdent a table identifier\n+   * @return all table's partitions\n+   */\n+  public static List<SparkPartition> getPartitions(SparkSession spark, TableIdentifier tableIdent)\n+      throws NoSuchTableException, NoSuchDatabaseException {\n+    SessionCatalog catalog = spark.sessionState().catalog();\n+    CatalogTable catalogTable = catalog.getTableMetadata(tableIdent);\n+\n+    return JavaConverters\n+        .seqAsJavaListConverter(catalog.listPartitions(tableIdent, Option.empty()))\n+        .asJava()\n+        .stream()\n+        .map(catalogPartition -> toSparkPartition(catalogPartition, catalogTable))\n+        .collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Returns partitions that match the specified 'predicate'.\n+   *\n+   * @param spark     a Spark session\n+   * @param table     a table name and (optional) database\n+   * @param predicate a predicate on partition columns\n+   * @return matching table's partitions\n+   */\n+  public static List<SparkPartition> getPartitionsByFilter(SparkSession spark, String table, String predicate)\n+      throws ParseException, NoSuchTableException, NoSuchDatabaseException {\n+\n+    TableIdentifier tableIdent = spark.sessionState().sqlParser().parseTableIdentifier(table);\n+    Expression unresolvedPredicateExpr = spark.sessionState().sqlParser().parseExpression(predicate);\n+    Expression resolvedPredicateExpr = resolveAttrs(spark, table, unresolvedPredicateExpr);\n+    return getPartitionsByFilter(spark, tableIdent, resolvedPredicateExpr);\n+  }\n+\n+  /**\n+   * Returns partitions that match the specified 'predicate'.\n+   *\n+   * @param spark         a Spark session\n+   * @param tableIdent    a table identifier\n+   * @param predicateExpr a predicate expression on partition columns\n+   * @return matching table's partitions\n+   */\n+  public static List<SparkPartition> getPartitionsByFilter(\n+      SparkSession spark,\n+      TableIdentifier tableIdent,\n+      Expression predicateExpr) throws NoSuchTableException, NoSuchDatabaseException {\n+\n+    SessionCatalog catalog = spark.sessionState().catalog();\n+    CatalogTable catalogTable = catalog.getTableMetadata(tableIdent);\n+\n+    final Expression resolvedPredicateExpr;\n+    if (!predicateExpr.resolved()) {\n+      resolvedPredicateExpr = resolveAttrs(spark, tableIdent.quotedString(), predicateExpr);\n+    } else {\n+      resolvedPredicateExpr = predicateExpr;\n+    }\n+\n+    return JavaConverters.seqAsJavaListConverter(\n+        catalog.listPartitionsByFilter(\n+            tableIdent,\n+            JavaConverters\n+                .collectionAsScalaIterableConverter(ImmutableList.of(resolvedPredicateExpr))\n+                .asScala()\n+                .toSeq()))\n+        .asJava()\n+        .stream()\n+        .map(catalogPartition -> toSparkPartition(catalogPartition, catalogTable))\n+        .collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Returns the data files in a partition by listing the partition location.\n+   *\n+   * For Parquet and ORC partitions, this will read metrics from the file footer. For Avro partitions,\n+   * metrics are set to null.\n+   *\n+   * @param partition     a partition\n+   * @param conf          a serializable Hadoop conf\n+   * @param metricsConfig a metrics conf\n+   * @return a List of [[DataFile]]\n+   */\n+  public static List<DataFile> listPartition(\n+      SparkPartition partition,\n+      PartitionSpec spec,\n+      SerializableConfiguration conf,\n+      MetricsConfig metricsConfig) throws IOException {\n+\n+    return listPartition(\n+        partition.values,\n+        partition.uri,\n+        partition.format,\n+        spec,\n+        conf.get(),\n+        metricsConfig);\n+  }\n+\n+  /**\n+   * Returns the data files in a partition by listing the partition location.\n+   *\n+   * For Parquet and ORC partitions, this will read metrics from the file footer. For Avro partitions,\n+   * metrics are set to null.\n+   *\n+   * @param partition     partition key, e.g., \"a=1/b=2\"\n+   * @param uri           partition location URI\n+   * @param format        partition format, avro or parquet\n+   * @param conf          a Hadoop conf\n+   * @param metricsConfig a metrics conf\n+   * @return a List of [[DataFile]]\n+   */\n+  public static List<DataFile> listPartition(\n+      Map<String, String> partition,\n+      String uri,\n+      String format,\n+      PartitionSpec spec,\n+      Configuration conf,\n+      MetricsConfig metricsConfig) throws IOException {\n+\n+    if (format.contains(\"avro\")) {\n+      return listAvroPartition(partition, uri, spec, conf);\n+    } else if (format.contains(\"parquet\")) {\n+      return listParquetPartition(partition, uri, spec, conf, metricsConfig);\n+    } else if (format.contains(\"orc\")) {\n+      // TODO: use MetricsConfig in listOrcPartition\n+      return listOrcPartition(partition, uri, spec, conf);\n+    } else {\n+      throw new UnsupportedOperationException(\"Unknown partition format: \" + format);\n+    }\n+  }\n+\n+  private static List<DataFile> listAvroPartition(\n+      Map<String, String> partitionPath,\n+      String partitionUri,\n+      PartitionSpec spec,\n+      Configuration conf) throws IOException {\n+\n+    Path partition = new Path(partitionUri);\n+    FileSystem fs = partition.getFileSystem(conf);\n+    return Arrays.stream(fs.listStatus(partition, HIDDEN_PATH_FILTER))\n+        .filter(FileStatus::isFile)\n+        .map(stat -> {\n+          Metrics metrics = new Metrics(-1L, null, null, null);\n+          String partitionKey = spec.fields().stream()\n+              .map(PartitionField::name)\n+              .map(name -> String.format(\"%s=%s\", name, partitionPath.get(name)))\n+              .collect(Collectors.joining(\"/\"));\n+\n+          return DataFiles.builder(spec)\n+              .withPath(stat.getPath().toString())\n+              .withFormat(\"avro\")\n+              .withFileSizeInBytes(stat.getLen())\n+              .withMetrics(metrics)\n+              .withPartitionPath(partitionKey)\n+              .build();\n+\n+        }).collect(Collectors.toList());\n+  }\n+\n+  private static List<DataFile> listParquetPartition(\n+      Map<String, String> partitionPath,\n+      String partitionUri,\n+      PartitionSpec spec,\n+      Configuration conf,\n+      MetricsConfig metricsSpec) throws IOException {\n+\n+    Path partition = new Path(partitionUri);\n+    FileSystem fs = partition.getFileSystem(conf);\n+\n+    return Arrays.stream(fs.listStatus(partition, HIDDEN_PATH_FILTER))\n+        .filter(FileStatus::isFile)\n+        .map(stat -> {\n+          final Metrics metrics;\n+          try {\n+            //noinspection deprecation\n+            metrics = ParquetUtil.footerMetrics(ParquetFileReader.readFooter(conf, stat), metricsSpec);\n+          } catch (IOException e) {\n+            throw new RuntimeException(\"Unable to read footer metrics for: \" + stat.getPath(), e);\n+          }\n+          String partitionKey = spec.fields().stream()\n+              .map(PartitionField::name)\n+              .map(name -> String.format(\"%s=%s\", name, partitionPath.get(name)))\n+              .collect(Collectors.joining(\"/\"));\n+\n+          return DataFiles.builder(spec)\n+              .withPath(stat.getPath().toString())\n+              .withFormat(\"parquet\")\n+              .withFileSizeInBytes(stat.getLen())\n+              .withMetrics(metrics)\n+              .withPartitionPath(partitionKey)\n+              .build();\n+\n+        }).collect(Collectors.toList());\n+  }\n+\n+  private static List<DataFile> listOrcPartition(\n+      Map<String, String> partitionPath,\n+      String partitionUri,\n+      PartitionSpec spec,\n+      Configuration conf) throws IOException {\n+\n+    Path partition = new Path(partitionUri);\n+    FileSystem fs = partition.getFileSystem(conf);\n+\n+    return Arrays.stream(fs.listStatus(partition, HIDDEN_PATH_FILTER))\n+        .filter(FileStatus::isFile)\n+        .map(stat -> {\n+          Metrics metrics = OrcMetrics.fromInputFile(HadoopInputFile.fromPath(stat.getPath(), conf));\n+          String partitionKey = spec.fields().stream()\n+              .map(PartitionField::name)\n+              .map(name -> String.format(\"%s=%s\", name, partitionPath.get(name)))\n+              .collect(Collectors.joining(\"/\"));\n+\n+          return DataFiles.builder(spec)\n+              .withPath(stat.getPath().toString())\n+              .withFormat(\"orc\")\n+              .withFileSizeInBytes(stat.getLen())\n+              .withMetrics(metrics)\n+              .withPartitionPath(partitionKey)\n+              .build();\n+\n+        }).collect(Collectors.toList());\n+  }\n+\n+  private static SparkPartition toSparkPartition(CatalogTablePartition partition, CatalogTable table) {\n+    Option<URI> locationUri = partition.storage().locationUri();\n+    Option<String> serde = partition.storage().serde();\n+\n+    Preconditions.checkArgument(locationUri.nonEmpty(), \"Partition URI should be defined\");\n+    Preconditions.checkArgument(serde.nonEmpty() || table.provider().nonEmpty(),\n+        \"Partition format should be defined\");\n+\n+    String uri = String.valueOf(locationUri.get());\n+    String format = serde.nonEmpty() ? serde.get() : table.provider().get();\n+\n+    return new SparkPartition(\n+        JavaConverters.mapAsJavaMapConverter(partition.spec()).asJava(),\n+        uri,\n+        format\n+    );\n+  }\n+\n+  private static Expression resolveAttrs(SparkSession spark, String table, Expression expr) {\n+    final Function2<String, String, Object> resolver = spark.sessionState().analyzer().resolver();\n+    final LogicalPlan plan = spark.table(table).queryExecution().analyzed();\n+    return expr.transform(new AbstractPartialFunction<Expression, Expression>() {\n+      @Override\n+      public Expression apply(Expression attr) {\n+        UnresolvedAttribute unresolvedAttribute = (UnresolvedAttribute) attr;\n+        Option<NamedExpression> namedExpressionOption = plan.resolve(unresolvedAttribute.nameParts(), resolver);\n+        if (namedExpressionOption.isDefined()) {\n+          return (Expression) namedExpressionOption.get();\n+        } else {\n+          throw new IllegalArgumentException(\n+              String.format(\"Could not resolve %s using columns: %s\", attr, plan.output()));\n+        }\n+      }\n+\n+      @Override\n+      public boolean isDefinedAt(Expression attr) {\n+        return attr instanceof UnresolvedAttribute;\n+      }\n+    });\n+  }\n+\n+  private static Iterator<ManifestFile> buildManifest(\n+      SerializableConfiguration conf,\n+      PartitionSpec spec,\n+      String basePath,\n+      Iterator<Tuple2<String, DataFile>> fileTuples) {\n+    try {\n+      if (fileTuples.hasNext()) {\n+        FileIO io = new HadoopFileIO(conf.get());\n+        TaskContext ctx = TaskContext.get();\n+        Path location =\n+            new Path(basePath, String.format(\"stage-%d-task-%d-manifest\", ctx.stageId(), ctx.taskAttemptId()));", "originalCommit": "67ba0019b7f043db5049f30d1ee4ffce865dad35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA4MjM4Nw==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443082387", "bodyText": "nit: final", "author": "aokolnychyi", "createdAt": "2020-06-20T00:11:31Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java", "diffHunk": "@@ -0,0 +1,649 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathFilter;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.orc.OrcMetrics;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.relocated.com.google.common.base.Objects;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.MapPartitionsFunction;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTablePartition;\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog;\n+import org.apache.spark.sql.catalyst.expressions.Expression;\n+import org.apache.spark.sql.catalyst.expressions.NamedExpression;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan;\n+import scala.Function2;\n+import scala.Option;\n+import scala.Some;\n+import scala.Tuple2;\n+import scala.collection.JavaConverters;\n+import scala.runtime.AbstractPartialFunction;\n+\n+import static org.apache.spark.sql.functions.col;\n+\n+/**\n+ * Java version of the original SparkTableUtil.scala\n+ */\n+public class SparkTableUtil {\n+\n+  private static final PathFilter HIDDEN_PATH_FILTER =\n+      p -> !p.getName().startsWith(\"_\") && !p.getName().startsWith(\".\");\n+\n+  private SparkTableUtil() {\n+  }\n+\n+  /**\n+   * Returns a DataFrame with a row for each partition in the table.\n+   *\n+   * The DataFrame has 3 columns, partition key (a=1/b=2), partition location, and format\n+   * (avro or parquet).\n+   *\n+   * @param spark a Spark session\n+   * @param table a table name and (optional) database\n+   * @return a DataFrame of the table's partitions\n+   */\n+  public static Dataset<Row> partitionDF(SparkSession spark, String table)\n+      throws NoSuchDatabaseException, NoSuchTableException, ParseException {\n+\n+    List<SparkPartition> partitions = getPartitions(spark, table);\n+    return spark.createDataFrame(partitions, SparkPartition.class).toDF(\"partition\", \"uri\", \"format\");\n+  }\n+\n+  /**\n+   * Returns a DataFrame with a row for each partition that matches the specified 'expression'.\n+   *\n+   * @param spark      a Spark session.\n+   * @param table      name of the table.\n+   * @param expression The expression whose matching partitions are returned.\n+   * @return a DataFrame of the table partitions.\n+   */\n+  public static Dataset<Row> partitionDFByFilter(\n+      SparkSession spark,\n+      String table,\n+      String expression) throws ParseException, NoSuchTableException, NoSuchDatabaseException {\n+\n+    List<SparkPartition> partitions = getPartitionsByFilter(spark, table, expression);\n+    return spark.createDataFrame(partitions, SparkPartition.class).toDF(\"partition\", \"uri\", \"format\");\n+  }\n+\n+  /**\n+   * Returns all partitions in the table.\n+   *\n+   * @param spark a Spark session\n+   * @param table a table name and (optional) database\n+   * @return all table's partitions\n+   */\n+  public static List<SparkPartition> getPartitions(SparkSession spark, String table)\n+      throws NoSuchTableException, NoSuchDatabaseException, ParseException {\n+\n+    TableIdentifier tableIdent = spark.sessionState().sqlParser().parseTableIdentifier(table);\n+    return getPartitions(spark, tableIdent);\n+  }\n+\n+  /**\n+   * Returns all partitions in the table.\n+   *\n+   * @param spark      a Spark session\n+   * @param tableIdent a table identifier\n+   * @return all table's partitions\n+   */\n+  public static List<SparkPartition> getPartitions(SparkSession spark, TableIdentifier tableIdent)\n+      throws NoSuchTableException, NoSuchDatabaseException {\n+    SessionCatalog catalog = spark.sessionState().catalog();\n+    CatalogTable catalogTable = catalog.getTableMetadata(tableIdent);\n+\n+    return JavaConverters\n+        .seqAsJavaListConverter(catalog.listPartitions(tableIdent, Option.empty()))\n+        .asJava()\n+        .stream()\n+        .map(catalogPartition -> toSparkPartition(catalogPartition, catalogTable))\n+        .collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Returns partitions that match the specified 'predicate'.\n+   *\n+   * @param spark     a Spark session\n+   * @param table     a table name and (optional) database\n+   * @param predicate a predicate on partition columns\n+   * @return matching table's partitions\n+   */\n+  public static List<SparkPartition> getPartitionsByFilter(SparkSession spark, String table, String predicate)\n+      throws ParseException, NoSuchTableException, NoSuchDatabaseException {\n+\n+    TableIdentifier tableIdent = spark.sessionState().sqlParser().parseTableIdentifier(table);\n+    Expression unresolvedPredicateExpr = spark.sessionState().sqlParser().parseExpression(predicate);\n+    Expression resolvedPredicateExpr = resolveAttrs(spark, table, unresolvedPredicateExpr);\n+    return getPartitionsByFilter(spark, tableIdent, resolvedPredicateExpr);\n+  }\n+\n+  /**\n+   * Returns partitions that match the specified 'predicate'.\n+   *\n+   * @param spark         a Spark session\n+   * @param tableIdent    a table identifier\n+   * @param predicateExpr a predicate expression on partition columns\n+   * @return matching table's partitions\n+   */\n+  public static List<SparkPartition> getPartitionsByFilter(\n+      SparkSession spark,\n+      TableIdentifier tableIdent,\n+      Expression predicateExpr) throws NoSuchTableException, NoSuchDatabaseException {\n+\n+    SessionCatalog catalog = spark.sessionState().catalog();\n+    CatalogTable catalogTable = catalog.getTableMetadata(tableIdent);\n+\n+    final Expression resolvedPredicateExpr;\n+    if (!predicateExpr.resolved()) {\n+      resolvedPredicateExpr = resolveAttrs(spark, tableIdent.quotedString(), predicateExpr);\n+    } else {\n+      resolvedPredicateExpr = predicateExpr;\n+    }\n+\n+    return JavaConverters.seqAsJavaListConverter(\n+        catalog.listPartitionsByFilter(\n+            tableIdent,\n+            JavaConverters\n+                .collectionAsScalaIterableConverter(ImmutableList.of(resolvedPredicateExpr))\n+                .asScala()\n+                .toSeq()))\n+        .asJava()\n+        .stream()\n+        .map(catalogPartition -> toSparkPartition(catalogPartition, catalogTable))\n+        .collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Returns the data files in a partition by listing the partition location.\n+   *\n+   * For Parquet and ORC partitions, this will read metrics from the file footer. For Avro partitions,\n+   * metrics are set to null.\n+   *\n+   * @param partition     a partition\n+   * @param conf          a serializable Hadoop conf\n+   * @param metricsConfig a metrics conf\n+   * @return a List of [[DataFile]]\n+   */\n+  public static List<DataFile> listPartition(\n+      SparkPartition partition,\n+      PartitionSpec spec,\n+      SerializableConfiguration conf,\n+      MetricsConfig metricsConfig) throws IOException {\n+\n+    return listPartition(\n+        partition.values,\n+        partition.uri,\n+        partition.format,\n+        spec,\n+        conf.get(),\n+        metricsConfig);\n+  }\n+\n+  /**\n+   * Returns the data files in a partition by listing the partition location.\n+   *\n+   * For Parquet and ORC partitions, this will read metrics from the file footer. For Avro partitions,\n+   * metrics are set to null.\n+   *\n+   * @param partition     partition key, e.g., \"a=1/b=2\"\n+   * @param uri           partition location URI\n+   * @param format        partition format, avro or parquet\n+   * @param conf          a Hadoop conf\n+   * @param metricsConfig a metrics conf\n+   * @return a List of [[DataFile]]\n+   */\n+  public static List<DataFile> listPartition(\n+      Map<String, String> partition,\n+      String uri,\n+      String format,\n+      PartitionSpec spec,\n+      Configuration conf,\n+      MetricsConfig metricsConfig) throws IOException {\n+\n+    if (format.contains(\"avro\")) {\n+      return listAvroPartition(partition, uri, spec, conf);\n+    } else if (format.contains(\"parquet\")) {\n+      return listParquetPartition(partition, uri, spec, conf, metricsConfig);\n+    } else if (format.contains(\"orc\")) {\n+      // TODO: use MetricsConfig in listOrcPartition\n+      return listOrcPartition(partition, uri, spec, conf);\n+    } else {\n+      throw new UnsupportedOperationException(\"Unknown partition format: \" + format);\n+    }\n+  }\n+\n+  private static List<DataFile> listAvroPartition(\n+      Map<String, String> partitionPath,\n+      String partitionUri,\n+      PartitionSpec spec,\n+      Configuration conf) throws IOException {\n+\n+    Path partition = new Path(partitionUri);\n+    FileSystem fs = partition.getFileSystem(conf);\n+    return Arrays.stream(fs.listStatus(partition, HIDDEN_PATH_FILTER))\n+        .filter(FileStatus::isFile)\n+        .map(stat -> {\n+          Metrics metrics = new Metrics(-1L, null, null, null);\n+          String partitionKey = spec.fields().stream()\n+              .map(PartitionField::name)\n+              .map(name -> String.format(\"%s=%s\", name, partitionPath.get(name)))\n+              .collect(Collectors.joining(\"/\"));\n+\n+          return DataFiles.builder(spec)\n+              .withPath(stat.getPath().toString())\n+              .withFormat(\"avro\")\n+              .withFileSizeInBytes(stat.getLen())\n+              .withMetrics(metrics)\n+              .withPartitionPath(partitionKey)\n+              .build();\n+\n+        }).collect(Collectors.toList());\n+  }\n+\n+  private static List<DataFile> listParquetPartition(\n+      Map<String, String> partitionPath,\n+      String partitionUri,\n+      PartitionSpec spec,\n+      Configuration conf,\n+      MetricsConfig metricsSpec) throws IOException {\n+\n+    Path partition = new Path(partitionUri);\n+    FileSystem fs = partition.getFileSystem(conf);\n+\n+    return Arrays.stream(fs.listStatus(partition, HIDDEN_PATH_FILTER))\n+        .filter(FileStatus::isFile)\n+        .map(stat -> {\n+          final Metrics metrics;", "originalCommit": "67ba0019b7f043db5049f30d1ee4ffce865dad35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA4MjQwOQ==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443082409", "bodyText": "nit: finals", "author": "aokolnychyi", "createdAt": "2020-06-20T00:11:42Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java", "diffHunk": "@@ -0,0 +1,649 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathFilter;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.orc.OrcMetrics;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.relocated.com.google.common.base.Objects;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.MapPartitionsFunction;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTablePartition;\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog;\n+import org.apache.spark.sql.catalyst.expressions.Expression;\n+import org.apache.spark.sql.catalyst.expressions.NamedExpression;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan;\n+import scala.Function2;\n+import scala.Option;\n+import scala.Some;\n+import scala.Tuple2;\n+import scala.collection.JavaConverters;\n+import scala.runtime.AbstractPartialFunction;\n+\n+import static org.apache.spark.sql.functions.col;\n+\n+/**\n+ * Java version of the original SparkTableUtil.scala\n+ */\n+public class SparkTableUtil {\n+\n+  private static final PathFilter HIDDEN_PATH_FILTER =\n+      p -> !p.getName().startsWith(\"_\") && !p.getName().startsWith(\".\");\n+\n+  private SparkTableUtil() {\n+  }\n+\n+  /**\n+   * Returns a DataFrame with a row for each partition in the table.\n+   *\n+   * The DataFrame has 3 columns, partition key (a=1/b=2), partition location, and format\n+   * (avro or parquet).\n+   *\n+   * @param spark a Spark session\n+   * @param table a table name and (optional) database\n+   * @return a DataFrame of the table's partitions\n+   */\n+  public static Dataset<Row> partitionDF(SparkSession spark, String table)\n+      throws NoSuchDatabaseException, NoSuchTableException, ParseException {\n+\n+    List<SparkPartition> partitions = getPartitions(spark, table);\n+    return spark.createDataFrame(partitions, SparkPartition.class).toDF(\"partition\", \"uri\", \"format\");\n+  }\n+\n+  /**\n+   * Returns a DataFrame with a row for each partition that matches the specified 'expression'.\n+   *\n+   * @param spark      a Spark session.\n+   * @param table      name of the table.\n+   * @param expression The expression whose matching partitions are returned.\n+   * @return a DataFrame of the table partitions.\n+   */\n+  public static Dataset<Row> partitionDFByFilter(\n+      SparkSession spark,\n+      String table,\n+      String expression) throws ParseException, NoSuchTableException, NoSuchDatabaseException {\n+\n+    List<SparkPartition> partitions = getPartitionsByFilter(spark, table, expression);\n+    return spark.createDataFrame(partitions, SparkPartition.class).toDF(\"partition\", \"uri\", \"format\");\n+  }\n+\n+  /**\n+   * Returns all partitions in the table.\n+   *\n+   * @param spark a Spark session\n+   * @param table a table name and (optional) database\n+   * @return all table's partitions\n+   */\n+  public static List<SparkPartition> getPartitions(SparkSession spark, String table)\n+      throws NoSuchTableException, NoSuchDatabaseException, ParseException {\n+\n+    TableIdentifier tableIdent = spark.sessionState().sqlParser().parseTableIdentifier(table);\n+    return getPartitions(spark, tableIdent);\n+  }\n+\n+  /**\n+   * Returns all partitions in the table.\n+   *\n+   * @param spark      a Spark session\n+   * @param tableIdent a table identifier\n+   * @return all table's partitions\n+   */\n+  public static List<SparkPartition> getPartitions(SparkSession spark, TableIdentifier tableIdent)\n+      throws NoSuchTableException, NoSuchDatabaseException {\n+    SessionCatalog catalog = spark.sessionState().catalog();\n+    CatalogTable catalogTable = catalog.getTableMetadata(tableIdent);\n+\n+    return JavaConverters\n+        .seqAsJavaListConverter(catalog.listPartitions(tableIdent, Option.empty()))\n+        .asJava()\n+        .stream()\n+        .map(catalogPartition -> toSparkPartition(catalogPartition, catalogTable))\n+        .collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Returns partitions that match the specified 'predicate'.\n+   *\n+   * @param spark     a Spark session\n+   * @param table     a table name and (optional) database\n+   * @param predicate a predicate on partition columns\n+   * @return matching table's partitions\n+   */\n+  public static List<SparkPartition> getPartitionsByFilter(SparkSession spark, String table, String predicate)\n+      throws ParseException, NoSuchTableException, NoSuchDatabaseException {\n+\n+    TableIdentifier tableIdent = spark.sessionState().sqlParser().parseTableIdentifier(table);\n+    Expression unresolvedPredicateExpr = spark.sessionState().sqlParser().parseExpression(predicate);\n+    Expression resolvedPredicateExpr = resolveAttrs(spark, table, unresolvedPredicateExpr);\n+    return getPartitionsByFilter(spark, tableIdent, resolvedPredicateExpr);\n+  }\n+\n+  /**\n+   * Returns partitions that match the specified 'predicate'.\n+   *\n+   * @param spark         a Spark session\n+   * @param tableIdent    a table identifier\n+   * @param predicateExpr a predicate expression on partition columns\n+   * @return matching table's partitions\n+   */\n+  public static List<SparkPartition> getPartitionsByFilter(\n+      SparkSession spark,\n+      TableIdentifier tableIdent,\n+      Expression predicateExpr) throws NoSuchTableException, NoSuchDatabaseException {\n+\n+    SessionCatalog catalog = spark.sessionState().catalog();\n+    CatalogTable catalogTable = catalog.getTableMetadata(tableIdent);\n+\n+    final Expression resolvedPredicateExpr;\n+    if (!predicateExpr.resolved()) {\n+      resolvedPredicateExpr = resolveAttrs(spark, tableIdent.quotedString(), predicateExpr);\n+    } else {\n+      resolvedPredicateExpr = predicateExpr;\n+    }\n+\n+    return JavaConverters.seqAsJavaListConverter(\n+        catalog.listPartitionsByFilter(\n+            tableIdent,\n+            JavaConverters\n+                .collectionAsScalaIterableConverter(ImmutableList.of(resolvedPredicateExpr))\n+                .asScala()\n+                .toSeq()))\n+        .asJava()\n+        .stream()\n+        .map(catalogPartition -> toSparkPartition(catalogPartition, catalogTable))\n+        .collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Returns the data files in a partition by listing the partition location.\n+   *\n+   * For Parquet and ORC partitions, this will read metrics from the file footer. For Avro partitions,\n+   * metrics are set to null.\n+   *\n+   * @param partition     a partition\n+   * @param conf          a serializable Hadoop conf\n+   * @param metricsConfig a metrics conf\n+   * @return a List of [[DataFile]]\n+   */\n+  public static List<DataFile> listPartition(\n+      SparkPartition partition,\n+      PartitionSpec spec,\n+      SerializableConfiguration conf,\n+      MetricsConfig metricsConfig) throws IOException {\n+\n+    return listPartition(\n+        partition.values,\n+        partition.uri,\n+        partition.format,\n+        spec,\n+        conf.get(),\n+        metricsConfig);\n+  }\n+\n+  /**\n+   * Returns the data files in a partition by listing the partition location.\n+   *\n+   * For Parquet and ORC partitions, this will read metrics from the file footer. For Avro partitions,\n+   * metrics are set to null.\n+   *\n+   * @param partition     partition key, e.g., \"a=1/b=2\"\n+   * @param uri           partition location URI\n+   * @param format        partition format, avro or parquet\n+   * @param conf          a Hadoop conf\n+   * @param metricsConfig a metrics conf\n+   * @return a List of [[DataFile]]\n+   */\n+  public static List<DataFile> listPartition(\n+      Map<String, String> partition,\n+      String uri,\n+      String format,\n+      PartitionSpec spec,\n+      Configuration conf,\n+      MetricsConfig metricsConfig) throws IOException {\n+\n+    if (format.contains(\"avro\")) {\n+      return listAvroPartition(partition, uri, spec, conf);\n+    } else if (format.contains(\"parquet\")) {\n+      return listParquetPartition(partition, uri, spec, conf, metricsConfig);\n+    } else if (format.contains(\"orc\")) {\n+      // TODO: use MetricsConfig in listOrcPartition\n+      return listOrcPartition(partition, uri, spec, conf);\n+    } else {\n+      throw new UnsupportedOperationException(\"Unknown partition format: \" + format);\n+    }\n+  }\n+\n+  private static List<DataFile> listAvroPartition(\n+      Map<String, String> partitionPath,\n+      String partitionUri,\n+      PartitionSpec spec,\n+      Configuration conf) throws IOException {\n+\n+    Path partition = new Path(partitionUri);\n+    FileSystem fs = partition.getFileSystem(conf);\n+    return Arrays.stream(fs.listStatus(partition, HIDDEN_PATH_FILTER))\n+        .filter(FileStatus::isFile)\n+        .map(stat -> {\n+          Metrics metrics = new Metrics(-1L, null, null, null);\n+          String partitionKey = spec.fields().stream()\n+              .map(PartitionField::name)\n+              .map(name -> String.format(\"%s=%s\", name, partitionPath.get(name)))\n+              .collect(Collectors.joining(\"/\"));\n+\n+          return DataFiles.builder(spec)\n+              .withPath(stat.getPath().toString())\n+              .withFormat(\"avro\")\n+              .withFileSizeInBytes(stat.getLen())\n+              .withMetrics(metrics)\n+              .withPartitionPath(partitionKey)\n+              .build();\n+\n+        }).collect(Collectors.toList());\n+  }\n+\n+  private static List<DataFile> listParquetPartition(\n+      Map<String, String> partitionPath,\n+      String partitionUri,\n+      PartitionSpec spec,\n+      Configuration conf,\n+      MetricsConfig metricsSpec) throws IOException {\n+\n+    Path partition = new Path(partitionUri);\n+    FileSystem fs = partition.getFileSystem(conf);\n+\n+    return Arrays.stream(fs.listStatus(partition, HIDDEN_PATH_FILTER))\n+        .filter(FileStatus::isFile)\n+        .map(stat -> {\n+          final Metrics metrics;\n+          try {\n+            //noinspection deprecation\n+            metrics = ParquetUtil.footerMetrics(ParquetFileReader.readFooter(conf, stat), metricsSpec);\n+          } catch (IOException e) {\n+            throw new RuntimeException(\"Unable to read footer metrics for: \" + stat.getPath(), e);\n+          }\n+          String partitionKey = spec.fields().stream()\n+              .map(PartitionField::name)\n+              .map(name -> String.format(\"%s=%s\", name, partitionPath.get(name)))\n+              .collect(Collectors.joining(\"/\"));\n+\n+          return DataFiles.builder(spec)\n+              .withPath(stat.getPath().toString())\n+              .withFormat(\"parquet\")\n+              .withFileSizeInBytes(stat.getLen())\n+              .withMetrics(metrics)\n+              .withPartitionPath(partitionKey)\n+              .build();\n+\n+        }).collect(Collectors.toList());\n+  }\n+\n+  private static List<DataFile> listOrcPartition(\n+      Map<String, String> partitionPath,\n+      String partitionUri,\n+      PartitionSpec spec,\n+      Configuration conf) throws IOException {\n+\n+    Path partition = new Path(partitionUri);\n+    FileSystem fs = partition.getFileSystem(conf);\n+\n+    return Arrays.stream(fs.listStatus(partition, HIDDEN_PATH_FILTER))\n+        .filter(FileStatus::isFile)\n+        .map(stat -> {\n+          Metrics metrics = OrcMetrics.fromInputFile(HadoopInputFile.fromPath(stat.getPath(), conf));\n+          String partitionKey = spec.fields().stream()\n+              .map(PartitionField::name)\n+              .map(name -> String.format(\"%s=%s\", name, partitionPath.get(name)))\n+              .collect(Collectors.joining(\"/\"));\n+\n+          return DataFiles.builder(spec)\n+              .withPath(stat.getPath().toString())\n+              .withFormat(\"orc\")\n+              .withFileSizeInBytes(stat.getLen())\n+              .withMetrics(metrics)\n+              .withPartitionPath(partitionKey)\n+              .build();\n+\n+        }).collect(Collectors.toList());\n+  }\n+\n+  private static SparkPartition toSparkPartition(CatalogTablePartition partition, CatalogTable table) {\n+    Option<URI> locationUri = partition.storage().locationUri();\n+    Option<String> serde = partition.storage().serde();\n+\n+    Preconditions.checkArgument(locationUri.nonEmpty(), \"Partition URI should be defined\");\n+    Preconditions.checkArgument(serde.nonEmpty() || table.provider().nonEmpty(),\n+        \"Partition format should be defined\");\n+\n+    String uri = String.valueOf(locationUri.get());\n+    String format = serde.nonEmpty() ? serde.get() : table.provider().get();\n+\n+    return new SparkPartition(\n+        JavaConverters.mapAsJavaMapConverter(partition.spec()).asJava(),\n+        uri,\n+        format\n+    );\n+  }\n+\n+  private static Expression resolveAttrs(SparkSession spark, String table, Expression expr) {\n+    final Function2<String, String, Object> resolver = spark.sessionState().analyzer().resolver();", "originalCommit": "67ba0019b7f043db5049f30d1ee4ffce865dad35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9ca4e18ccc358de60e379f4ac694705fe205c524", "url": "https://github.com/apache/iceberg/commit/9ca4e18ccc358de60e379f4ac694705fe205c524", "message": "Convert SparkTableUtil.scala to SparkTableUtil.java\n\nAnd cleanup the scala build parts.", "committedDate": "2020-06-21T02:48:46Z", "type": "commit"}, {"oid": "556754c65bf1d9cd97bb8e51263f8c7db8a87b95", "url": "https://github.com/apache/iceberg/commit/556754c65bf1d9cd97bb8e51263f8c7db8a87b95", "message": "Address review comments", "committedDate": "2020-06-21T02:48:47Z", "type": "forcePushed"}, {"oid": "b0ee44eec8a1833f7b08b3b012aac2a1c8c81ae8", "url": "https://github.com/apache/iceberg/commit/b0ee44eec8a1833f7b08b3b012aac2a1c8c81ae8", "message": "Address review comments", "committedDate": "2020-06-21T07:02:20Z", "type": "commit"}, {"oid": "b0ee44eec8a1833f7b08b3b012aac2a1c8c81ae8", "url": "https://github.com/apache/iceberg/commit/b0ee44eec8a1833f7b08b3b012aac2a1c8c81ae8", "message": "Address review comments", "committedDate": "2020-06-21T07:02:20Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkwMjUyNw==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443902527", "bodyText": "Looks like we don't wrap NoSuchDatabaseException that we get from Spark. I am not sure there is a good alternative as NoSuchNamespaceException sounds a bit too generic to be applied in this case. Probably, wrapping into RuntimeException like we do now is the most straightforward option.", "author": "aokolnychyi", "createdAt": "2020-06-23T00:55:55Z", "path": "spark/src/main/java/org/apache/iceberg/spark/ExceptionUtil.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.io.IOException;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.spark.sql.AnalysisException;\n+\n+public class ExceptionUtil {\n+\n+  private ExceptionUtil() {\n+  }\n+\n+  /**\n+   * Converts Checked Exceptions to Unchecked Exceptions.\n+   *\n+   * @param exception the exception object expected to be unchecked.\n+   * @return - checked exception.\n+   */\n+  public static RuntimeException toUncheckedException(Throwable exception) {\n+\n+    if (exception instanceof RuntimeException) {\n+      return (RuntimeException) exception;\n+\n+    } else if (exception instanceof org.apache.spark.sql.catalyst.analysis.NoSuchTableException) {", "originalCommit": "b0ee44eec8a1833f7b08b3b012aac2a1c8c81ae8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkwOTIzMA==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443909230", "bodyText": "NoSuchDatabaseException to NoSuchNamespaceException  is actually a miss.\n#1126 (comment) did mention about converting\nNoSuchDatabaseException, NoSuchTableException, ParseException to NoSuchNamespaceException, NoSuchTableException, and ValidationException respectively.", "author": "raptond", "createdAt": "2020-06-23T01:22:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkwMjUyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkwMzk5Mg==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443903992", "bodyText": "Originally, I thought about a private method in SparkTableUtil that would accept a lambda but having a separate class probably makes more sense as we can reuse this work in the actions in the future. I am not sure about the name, though. Since we are handling only Spark exceptions, maybe it makes sense to call it SparkExceptionUtil. I'll be interested to hear other opinions on this.", "author": "aokolnychyi", "createdAt": "2020-06-23T01:01:43Z", "path": "spark/src/main/java/org/apache/iceberg/spark/ExceptionUtil.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.io.IOException;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.spark.sql.AnalysisException;\n+\n+public class ExceptionUtil {", "originalCommit": "b0ee44eec8a1833f7b08b3b012aac2a1c8c81ae8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkxNTYxNw==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443915617", "bodyText": "SparkExceptionUtil does make sense. I will wait for any consensus and rename it. Rest of the comments are addressed.", "author": "raptond", "createdAt": "2020-06-23T01:48:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkwMzk5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIwNTQxOQ==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r446205419", "bodyText": "Renamed ExceptionUtil to SparkExceptionUtil", "author": "raptond", "createdAt": "2020-06-26T14:05:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkwMzk5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkwNDM5MQ==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443904391", "bodyText": "nit: I think the javadoc for the arg and return value is the opposite to what is actually done as we converted checked exceptions into unchecked.", "author": "aokolnychyi", "createdAt": "2020-06-23T01:03:10Z", "path": "spark/src/main/java/org/apache/iceberg/spark/ExceptionUtil.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.io.IOException;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.spark.sql.AnalysisException;\n+\n+public class ExceptionUtil {\n+\n+  private ExceptionUtil() {\n+  }\n+\n+  /**\n+   * Converts Checked Exceptions to Unchecked Exceptions.\n+   *\n+   * @param exception the exception object expected to be unchecked.\n+   * @return - checked exception.", "originalCommit": "b0ee44eec8a1833f7b08b3b012aac2a1c8c81ae8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA3NjcwMw==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r445076703", "bodyText": "Also nit: we don't add - to Javadoc", "author": "rdblue", "createdAt": "2020-06-24T18:05:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkwNDM5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIwNjE4OA==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r446206188", "bodyText": "Fixed this.", "author": "raptond", "createdAt": "2020-06-26T14:06:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkwNDM5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkwNTgxMQ==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443905811", "bodyText": "nit: looks like we don't need this change anymore", "author": "aokolnychyi", "createdAt": "2020-06-23T01:08:57Z", "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtilWithInMemoryCatalog.java", "diffHunk": "@@ -135,7 +134,8 @@ public void testImportUnpartitionedTable() throws IOException {\n   }\n \n   @Test\n-  public void testImportPartitionedTable() throws IOException {\n+  public void testImportPartitionedTable()", "originalCommit": "b0ee44eec8a1833f7b08b3b012aac2a1c8c81ae8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkwNTg0MQ==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r443905841", "bodyText": "nit: looks like we don't need this change anymore", "author": "aokolnychyi", "createdAt": "2020-06-23T01:09:03Z", "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtilWithInMemoryCatalog.java", "diffHunk": "@@ -232,7 +232,8 @@ public void testImportPartitions() throws IOException {\n   }\n \n   @Test\n-  public void testImportPartitionsWithSnapshotInheritance() throws IOException {\n+  public void testImportPartitionsWithSnapshotInheritance()", "originalCommit": "b0ee44eec8a1833f7b08b3b012aac2a1c8c81ae8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "c026816473e4511878e4419c108af9b43b7c3067", "url": "https://github.com/apache/iceberg/commit/c026816473e4511878e4419c108af9b43b7c3067", "message": "Additional review comments addressed", "committedDate": "2020-06-23T01:46:51Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA3NjQ0OA==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r445076448", "bodyText": "Nit: we prefer sentence case instead of capitalizing all nouns.", "author": "rdblue", "createdAt": "2020-06-24T18:04:45Z", "path": "spark/src/main/java/org/apache/iceberg/spark/ExceptionUtil.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.io.IOException;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.spark.sql.AnalysisException;\n+\n+public class ExceptionUtil {\n+\n+  private ExceptionUtil() {\n+  }\n+\n+  /**\n+   * Converts Checked Exceptions to Unchecked Exceptions.", "originalCommit": "b0ee44eec8a1833f7b08b3b012aac2a1c8c81ae8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA3NzQyMQ==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r445077421", "bodyText": "I'd like to have more context here, instead of just wrapping without an exception message. Can you add a string message to add to the new exception?", "author": "rdblue", "createdAt": "2020-06-24T18:06:36Z", "path": "spark/src/main/java/org/apache/iceberg/spark/ExceptionUtil.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.io.IOException;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.spark.sql.AnalysisException;\n+\n+public class ExceptionUtil {\n+\n+  private ExceptionUtil() {\n+  }\n+\n+  /**\n+   * Converts Checked Exceptions to Unchecked Exceptions.\n+   *\n+   * @param exception the exception object expected to be unchecked.\n+   * @return - checked exception.\n+   */\n+  public static RuntimeException toUncheckedException(Throwable exception) {", "originalCommit": "b0ee44eec8a1833f7b08b3b012aac2a1c8c81ae8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIxMzAxMg==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r446213012", "bodyText": "This is taken care now. Additionally to add the manifest file path also to the exception message when doing buildManifest, I had to move the exception handling inside. Ref: SparkTableUtil.java R430-R434\nThis resulted in a try-with-resources block with a reference to the writer, as the writer.toManifestFile should happen only after closing the writer.\nString outputPath = FileFormat.AVRO.addExtension(location.toString());\nOutputFile outputFile = io.newOutputFile(outputPath);\nManifestWriter<DataFile> writer = ManifestFiles.write(spec, outputFile);\n\ntry (ManifestWriter<DataFile> writerRef = writer) {\n  fileTuples.forEachRemaining(fileTuple -> writerRef.add(fileTuple._2));\n} catch (IOException e) {\n  throw SparkExceptionUtil.toUncheckedException(e, \"Unable to close the manifest writer: %s\", outputPath);\n}\n\nManifestFile manifestFile = writer.toManifestFile();\nreturn ImmutableList.of(manifestFile).iterator();\n\nLet me know if the above is alright. The other option I had was the below or throw from the finally block. Both looked quite odd.\ntry {\n  try {\n    fileTuples.forEachRemaining(fileTuple -> writer.add(fileTuple._2));\n  } finally {\n    writer.close();\n  }\n} catch (IOException e) {\n  throw SparkExceptionUtil.toUncheckedException(e, \"Unable to close the manifest writer: %s\", outputPath);\n}", "author": "raptond", "createdAt": "2020-06-26T14:18:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA3NzQyMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0MTg4NQ==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r447341885", "bodyText": "I like your idea of using a ref. I've used the other pattern in some places and it's more awkward. I'll probably use the ref approach from now on.", "author": "rdblue", "createdAt": "2020-06-30T00:47:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA3NzQyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA3ODY5MA==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r445078690", "bodyText": "Nit: we don't start methods with empty lines.", "author": "rdblue", "createdAt": "2020-06-24T18:08:59Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java", "diffHunk": "@@ -0,0 +1,682 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathFilter;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.orc.OrcMetrics;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.relocated.com.google.common.base.Objects;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.MapPartitionsFunction;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTablePartition;\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog;\n+import org.apache.spark.sql.catalyst.expressions.Expression;\n+import org.apache.spark.sql.catalyst.expressions.NamedExpression;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan;\n+import scala.Function2;\n+import scala.Option;\n+import scala.Some;\n+import scala.Tuple2;\n+import scala.collection.JavaConverters;\n+import scala.runtime.AbstractPartialFunction;\n+\n+import static org.apache.spark.sql.functions.col;\n+\n+/**\n+ * Java version of the original SparkTableUtil.scala\n+ * https://github.com/apache/iceberg/blob/apache-iceberg-0.8.0-incubating/spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala\n+ */\n+public class SparkTableUtil {\n+\n+  private static final PathFilter HIDDEN_PATH_FILTER =\n+      p -> !p.getName().startsWith(\"_\") && !p.getName().startsWith(\".\");\n+\n+  private SparkTableUtil() {\n+  }\n+\n+  /**\n+   * Returns a DataFrame with a row for each partition in the table.\n+   *\n+   * The DataFrame has 3 columns, partition key (a=1/b=2), partition location, and format\n+   * (avro or parquet).\n+   *\n+   * @param spark a Spark session\n+   * @param table a table name and (optional) database\n+   * @return a DataFrame of the table's partitions\n+   */\n+  public static Dataset<Row> partitionDF(SparkSession spark, String table) {\n+", "originalCommit": "c026816473e4511878e4419c108af9b43b7c3067", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA4MDEwMg==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r445080102", "bodyText": "Style: could you leave arguments on the same line?\nIf the line is too long, you can either wrap and indent to the start of the argument list, or start the entire argument list at 2 indentations (4 spaces) on the next line. Either one of these:\n  public void method(Arg arg1,\n                     Arg arg2) { ... }\n\n  public void method(\n      Arg arg1, Arg arg2) { ... }", "author": "rdblue", "createdAt": "2020-06-24T18:11:32Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java", "diffHunk": "@@ -0,0 +1,682 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathFilter;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.orc.OrcMetrics;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.relocated.com.google.common.base.Objects;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.MapPartitionsFunction;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTablePartition;\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog;\n+import org.apache.spark.sql.catalyst.expressions.Expression;\n+import org.apache.spark.sql.catalyst.expressions.NamedExpression;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan;\n+import scala.Function2;\n+import scala.Option;\n+import scala.Some;\n+import scala.Tuple2;\n+import scala.collection.JavaConverters;\n+import scala.runtime.AbstractPartialFunction;\n+\n+import static org.apache.spark.sql.functions.col;\n+\n+/**\n+ * Java version of the original SparkTableUtil.scala\n+ * https://github.com/apache/iceberg/blob/apache-iceberg-0.8.0-incubating/spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala\n+ */\n+public class SparkTableUtil {\n+\n+  private static final PathFilter HIDDEN_PATH_FILTER =\n+      p -> !p.getName().startsWith(\"_\") && !p.getName().startsWith(\".\");\n+\n+  private SparkTableUtil() {\n+  }\n+\n+  /**\n+   * Returns a DataFrame with a row for each partition in the table.\n+   *\n+   * The DataFrame has 3 columns, partition key (a=1/b=2), partition location, and format\n+   * (avro or parquet).\n+   *\n+   * @param spark a Spark session\n+   * @param table a table name and (optional) database\n+   * @return a DataFrame of the table's partitions\n+   */\n+  public static Dataset<Row> partitionDF(SparkSession spark, String table) {\n+\n+    List<SparkPartition> partitions = getPartitions(spark, table);\n+    return spark.createDataFrame(partitions, SparkPartition.class).toDF(\"partition\", \"uri\", \"format\");\n+  }\n+\n+  /**\n+   * Returns a DataFrame with a row for each partition that matches the specified 'expression'.\n+   *\n+   * @param spark      a Spark session.\n+   * @param table      name of the table.\n+   * @param expression The expression whose matching partitions are returned.\n+   * @return a DataFrame of the table partitions.\n+   */\n+  public static Dataset<Row> partitionDFByFilter(\n+      SparkSession spark,\n+      String table,\n+      String expression) {", "originalCommit": "c026816473e4511878e4419c108af9b43b7c3067", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA4MDg5OQ==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r445080899", "bodyText": "Could this conversion be in a separate statement to avoid so much indentation?", "author": "rdblue", "createdAt": "2020-06-24T18:12:58Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java", "diffHunk": "@@ -0,0 +1,682 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathFilter;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.orc.OrcMetrics;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.relocated.com.google.common.base.Objects;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.MapPartitionsFunction;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTablePartition;\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog;\n+import org.apache.spark.sql.catalyst.expressions.Expression;\n+import org.apache.spark.sql.catalyst.expressions.NamedExpression;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan;\n+import scala.Function2;\n+import scala.Option;\n+import scala.Some;\n+import scala.Tuple2;\n+import scala.collection.JavaConverters;\n+import scala.runtime.AbstractPartialFunction;\n+\n+import static org.apache.spark.sql.functions.col;\n+\n+/**\n+ * Java version of the original SparkTableUtil.scala\n+ * https://github.com/apache/iceberg/blob/apache-iceberg-0.8.0-incubating/spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala\n+ */\n+public class SparkTableUtil {\n+\n+  private static final PathFilter HIDDEN_PATH_FILTER =\n+      p -> !p.getName().startsWith(\"_\") && !p.getName().startsWith(\".\");\n+\n+  private SparkTableUtil() {\n+  }\n+\n+  /**\n+   * Returns a DataFrame with a row for each partition in the table.\n+   *\n+   * The DataFrame has 3 columns, partition key (a=1/b=2), partition location, and format\n+   * (avro or parquet).\n+   *\n+   * @param spark a Spark session\n+   * @param table a table name and (optional) database\n+   * @return a DataFrame of the table's partitions\n+   */\n+  public static Dataset<Row> partitionDF(SparkSession spark, String table) {\n+\n+    List<SparkPartition> partitions = getPartitions(spark, table);\n+    return spark.createDataFrame(partitions, SparkPartition.class).toDF(\"partition\", \"uri\", \"format\");\n+  }\n+\n+  /**\n+   * Returns a DataFrame with a row for each partition that matches the specified 'expression'.\n+   *\n+   * @param spark      a Spark session.\n+   * @param table      name of the table.\n+   * @param expression The expression whose matching partitions are returned.\n+   * @return a DataFrame of the table partitions.\n+   */\n+  public static Dataset<Row> partitionDFByFilter(\n+      SparkSession spark,\n+      String table,\n+      String expression) {\n+\n+    List<SparkPartition> partitions = getPartitionsByFilter(spark, table, expression);\n+    return spark.createDataFrame(partitions, SparkPartition.class).toDF(\"partition\", \"uri\", \"format\");\n+  }\n+\n+  /**\n+   * Returns all partitions in the table.\n+   *\n+   * @param spark a Spark session\n+   * @param table a table name and (optional) database\n+   * @return all table's partitions\n+   */\n+  public static List<SparkPartition> getPartitions(SparkSession spark, String table) {\n+\n+    try {\n+      TableIdentifier tableIdent = spark.sessionState().sqlParser().parseTableIdentifier(table);\n+      return getPartitions(spark, tableIdent);\n+    } catch (ParseException e) {\n+      throw ExceptionUtil.toUncheckedException(e);\n+    }\n+  }\n+\n+  /**\n+   * Returns all partitions in the table.\n+   *\n+   * @param spark      a Spark session\n+   * @param tableIdent a table identifier\n+   * @return all table's partitions\n+   */\n+  public static List<SparkPartition> getPartitions(SparkSession spark, TableIdentifier tableIdent) {\n+\n+    try {\n+      SessionCatalog catalog = spark.sessionState().catalog();\n+      CatalogTable catalogTable = catalog.getTableMetadata(tableIdent);\n+\n+      return JavaConverters\n+          .seqAsJavaListConverter(catalog.listPartitions(tableIdent, Option.empty()))\n+          .asJava()\n+          .stream()\n+          .map(catalogPartition -> toSparkPartition(catalogPartition, catalogTable))\n+          .collect(Collectors.toList());\n+    } catch (NoSuchDatabaseException | NoSuchTableException e) {\n+      throw ExceptionUtil.toUncheckedException(e);\n+    }\n+  }\n+\n+  /**\n+   * Returns partitions that match the specified 'predicate'.\n+   *\n+   * @param spark     a Spark session\n+   * @param table     a table name and (optional) database\n+   * @param predicate a predicate on partition columns\n+   * @return matching table's partitions\n+   */\n+  public static List<SparkPartition> getPartitionsByFilter(SparkSession spark, String table, String predicate) {\n+\n+    try {\n+      TableIdentifier tableIdent = spark.sessionState().sqlParser().parseTableIdentifier(table);\n+      Expression unresolvedPredicateExpr = spark.sessionState().sqlParser().parseExpression(predicate);\n+      Expression resolvedPredicateExpr = resolveAttrs(spark, table, unresolvedPredicateExpr);\n+      return getPartitionsByFilter(spark, tableIdent, resolvedPredicateExpr);\n+    } catch (ParseException e) {\n+      throw ExceptionUtil.toUncheckedException(e);\n+    }\n+  }\n+\n+  /**\n+   * Returns partitions that match the specified 'predicate'.\n+   *\n+   * @param spark         a Spark session\n+   * @param tableIdent    a table identifier\n+   * @param predicateExpr a predicate expression on partition columns\n+   * @return matching table's partitions\n+   */\n+  public static List<SparkPartition> getPartitionsByFilter(\n+      SparkSession spark,\n+      TableIdentifier tableIdent,\n+      Expression predicateExpr) {\n+\n+    try {\n+      SessionCatalog catalog = spark.sessionState().catalog();\n+      CatalogTable catalogTable = catalog.getTableMetadata(tableIdent);\n+\n+      Expression resolvedPredicateExpr;\n+      if (!predicateExpr.resolved()) {\n+        resolvedPredicateExpr = resolveAttrs(spark, tableIdent.quotedString(), predicateExpr);\n+      } else {\n+        resolvedPredicateExpr = predicateExpr;\n+      }\n+\n+      return JavaConverters.seqAsJavaListConverter(\n+          catalog.listPartitionsByFilter(\n+              tableIdent,\n+              JavaConverters\n+                  .collectionAsScalaIterableConverter(ImmutableList.of(resolvedPredicateExpr))\n+                  .asScala()\n+                  .toSeq()))", "originalCommit": "c026816473e4511878e4419c108af9b43b7c3067", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA4MTEwOQ==", "url": "https://github.com/apache/iceberg/pull/1126#discussion_r445081109", "bodyText": "Generally, single method calls should be on a couple of lines, not an argument on each line.", "author": "rdblue", "createdAt": "2020-06-24T18:13:22Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java", "diffHunk": "@@ -0,0 +1,682 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.net.URI;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathFilter;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.orc.OrcMetrics;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.relocated.com.google.common.base.Objects;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.MapPartitionsFunction;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTablePartition;\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog;\n+import org.apache.spark.sql.catalyst.expressions.Expression;\n+import org.apache.spark.sql.catalyst.expressions.NamedExpression;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan;\n+import scala.Function2;\n+import scala.Option;\n+import scala.Some;\n+import scala.Tuple2;\n+import scala.collection.JavaConverters;\n+import scala.runtime.AbstractPartialFunction;\n+\n+import static org.apache.spark.sql.functions.col;\n+\n+/**\n+ * Java version of the original SparkTableUtil.scala\n+ * https://github.com/apache/iceberg/blob/apache-iceberg-0.8.0-incubating/spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala\n+ */\n+public class SparkTableUtil {\n+\n+  private static final PathFilter HIDDEN_PATH_FILTER =\n+      p -> !p.getName().startsWith(\"_\") && !p.getName().startsWith(\".\");\n+\n+  private SparkTableUtil() {\n+  }\n+\n+  /**\n+   * Returns a DataFrame with a row for each partition in the table.\n+   *\n+   * The DataFrame has 3 columns, partition key (a=1/b=2), partition location, and format\n+   * (avro or parquet).\n+   *\n+   * @param spark a Spark session\n+   * @param table a table name and (optional) database\n+   * @return a DataFrame of the table's partitions\n+   */\n+  public static Dataset<Row> partitionDF(SparkSession spark, String table) {\n+\n+    List<SparkPartition> partitions = getPartitions(spark, table);\n+    return spark.createDataFrame(partitions, SparkPartition.class).toDF(\"partition\", \"uri\", \"format\");\n+  }\n+\n+  /**\n+   * Returns a DataFrame with a row for each partition that matches the specified 'expression'.\n+   *\n+   * @param spark      a Spark session.\n+   * @param table      name of the table.\n+   * @param expression The expression whose matching partitions are returned.\n+   * @return a DataFrame of the table partitions.\n+   */\n+  public static Dataset<Row> partitionDFByFilter(\n+      SparkSession spark,\n+      String table,\n+      String expression) {\n+\n+    List<SparkPartition> partitions = getPartitionsByFilter(spark, table, expression);\n+    return spark.createDataFrame(partitions, SparkPartition.class).toDF(\"partition\", \"uri\", \"format\");\n+  }\n+\n+  /**\n+   * Returns all partitions in the table.\n+   *\n+   * @param spark a Spark session\n+   * @param table a table name and (optional) database\n+   * @return all table's partitions\n+   */\n+  public static List<SparkPartition> getPartitions(SparkSession spark, String table) {\n+\n+    try {\n+      TableIdentifier tableIdent = spark.sessionState().sqlParser().parseTableIdentifier(table);\n+      return getPartitions(spark, tableIdent);\n+    } catch (ParseException e) {\n+      throw ExceptionUtil.toUncheckedException(e);\n+    }\n+  }\n+\n+  /**\n+   * Returns all partitions in the table.\n+   *\n+   * @param spark      a Spark session\n+   * @param tableIdent a table identifier\n+   * @return all table's partitions\n+   */\n+  public static List<SparkPartition> getPartitions(SparkSession spark, TableIdentifier tableIdent) {\n+\n+    try {\n+      SessionCatalog catalog = spark.sessionState().catalog();\n+      CatalogTable catalogTable = catalog.getTableMetadata(tableIdent);\n+\n+      return JavaConverters\n+          .seqAsJavaListConverter(catalog.listPartitions(tableIdent, Option.empty()))\n+          .asJava()\n+          .stream()\n+          .map(catalogPartition -> toSparkPartition(catalogPartition, catalogTable))\n+          .collect(Collectors.toList());\n+    } catch (NoSuchDatabaseException | NoSuchTableException e) {\n+      throw ExceptionUtil.toUncheckedException(e);\n+    }\n+  }\n+\n+  /**\n+   * Returns partitions that match the specified 'predicate'.\n+   *\n+   * @param spark     a Spark session\n+   * @param table     a table name and (optional) database\n+   * @param predicate a predicate on partition columns\n+   * @return matching table's partitions\n+   */\n+  public static List<SparkPartition> getPartitionsByFilter(SparkSession spark, String table, String predicate) {\n+\n+    try {\n+      TableIdentifier tableIdent = spark.sessionState().sqlParser().parseTableIdentifier(table);\n+      Expression unresolvedPredicateExpr = spark.sessionState().sqlParser().parseExpression(predicate);\n+      Expression resolvedPredicateExpr = resolveAttrs(spark, table, unresolvedPredicateExpr);\n+      return getPartitionsByFilter(spark, tableIdent, resolvedPredicateExpr);\n+    } catch (ParseException e) {\n+      throw ExceptionUtil.toUncheckedException(e);\n+    }\n+  }\n+\n+  /**\n+   * Returns partitions that match the specified 'predicate'.\n+   *\n+   * @param spark         a Spark session\n+   * @param tableIdent    a table identifier\n+   * @param predicateExpr a predicate expression on partition columns\n+   * @return matching table's partitions\n+   */\n+  public static List<SparkPartition> getPartitionsByFilter(\n+      SparkSession spark,\n+      TableIdentifier tableIdent,\n+      Expression predicateExpr) {\n+\n+    try {\n+      SessionCatalog catalog = spark.sessionState().catalog();\n+      CatalogTable catalogTable = catalog.getTableMetadata(tableIdent);\n+\n+      Expression resolvedPredicateExpr;\n+      if (!predicateExpr.resolved()) {\n+        resolvedPredicateExpr = resolveAttrs(spark, tableIdent.quotedString(), predicateExpr);\n+      } else {\n+        resolvedPredicateExpr = predicateExpr;\n+      }\n+\n+      return JavaConverters.seqAsJavaListConverter(\n+          catalog.listPartitionsByFilter(\n+              tableIdent,\n+              JavaConverters\n+                  .collectionAsScalaIterableConverter(ImmutableList.of(resolvedPredicateExpr))\n+                  .asScala()\n+                  .toSeq()))\n+          .asJava()\n+          .stream()\n+          .map(catalogPartition -> toSparkPartition(catalogPartition, catalogTable))\n+          .collect(Collectors.toList());\n+    } catch (NoSuchDatabaseException | NoSuchTableException e) {\n+      throw ExceptionUtil.toUncheckedException(e);\n+    }\n+  }\n+\n+  /**\n+   * Returns the data files in a partition by listing the partition location.\n+   *\n+   * For Parquet and ORC partitions, this will read metrics from the file footer. For Avro partitions,\n+   * metrics are set to null.\n+   *\n+   * @param partition     a partition\n+   * @param conf          a serializable Hadoop conf\n+   * @param metricsConfig a metrics conf\n+   * @return a List of [[DataFile]]\n+   */\n+  public static List<DataFile> listPartition(\n+      SparkPartition partition,\n+      PartitionSpec spec,\n+      SerializableConfiguration conf,\n+      MetricsConfig metricsConfig) {\n+\n+    return listPartition(\n+        partition.values,\n+        partition.uri,\n+        partition.format,\n+        spec,\n+        conf.get(),\n+        metricsConfig);", "originalCommit": "c026816473e4511878e4419c108af9b43b7c3067", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "7eb020f0e0b6817cd4d5eeb9a9c387cbbf3bcc00", "url": "https://github.com/apache/iceberg/commit/7eb020f0e0b6817cd4d5eeb9a9c387cbbf3bcc00", "message": "More review comments addressed", "committedDate": "2020-06-26T14:03:44Z", "type": "commit"}]}