{"pr_number": 1430, "pr_title": "API: Fix Metrics serialization", "pr_createdAt": "2020-09-07T10:12:50Z", "pr_url": "https://github.com/apache/iceberg/pull/1430", "timeline": [{"oid": "41a8e959afd0989fe282db5dc898c4a46b6d040c", "url": "https://github.com/apache/iceberg/commit/41a8e959afd0989fe282db5dc898c4a46b6d040c", "message": "Core: Fix Metrics serialization", "committedDate": "2020-09-07T10:10:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAxODYwNg==", "url": "https://github.com/apache/iceberg/pull/1430#discussion_r485018606", "bodyText": "I think this is one of those possibly dangerous things if the underlying implementation of the byte buffer isn't backed by an array. I believe the safe way to handle this is\nbyte[] b = new byte[bb.remaining()];\nbb.get(b);\n\nhttp://errorprone.info/bugpattern/ByteBufferBackingArray", "author": "RussellSpitzer", "createdAt": "2020-09-08T15:41:02Z", "path": "api/src/main/java/org/apache/iceberg/Metrics.java", "diffHunk": "@@ -120,4 +124,83 @@ public Long recordCount() {\n   public Map<Integer, ByteBuffer> upperBounds() {\n     return upperBounds;\n   }\n+\n+  /**\n+   * Implemented the method to enable serialization of ByteBuffers.\n+   * @param out The stream where to write\n+   * @throws IOException On serialization error\n+   */\n+  private void writeObject(ObjectOutputStream out) throws IOException {\n+    out.writeObject(rowCount);\n+    out.writeObject(columnSizes);\n+    out.writeObject(valueCounts);\n+    out.writeObject(nullValueCounts);\n+\n+    writeByteBufferMap(out, lowerBounds);\n+    writeByteBufferMap(out, upperBounds);\n+  }\n+\n+  private static void writeByteBufferMap(ObjectOutputStream out, Map<Integer, ByteBuffer> byteBufferMap)\n+      throws IOException {\n+    if (byteBufferMap == null) {\n+      out.writeInt(-1);\n+\n+    } else {\n+      // Write the size\n+      out.writeInt(byteBufferMap.size());\n+\n+      for (Map.Entry<Integer, ByteBuffer> entry : byteBufferMap.entrySet()) {\n+        // Write the key\n+        out.writeObject(entry.getKey());\n+\n+        // Write the value\n+        if (entry.getValue() != null) {\n+          out.writeObject(entry.getValue().array());", "originalCommit": "41a8e959afd0989fe282db5dc898c4a46b6d040c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAyNzA1NQ==", "url": "https://github.com/apache/iceberg/pull/1430#discussion_r485027055", "bodyText": "I do not know how this ByteBuffer is used. I was considering the proposed solution, but I am not entirely sure that we only want to store the remaining part of the ByteBuffer, so I would like to know the opinions of people more familiar with the Iceberg usages of this stuff.\nMaybe a check for specific implementation and error message for other options would be better. What do you think?", "author": "pvary", "createdAt": "2020-09-08T15:53:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAxODYwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAzNTE4Mg==", "url": "https://github.com/apache/iceberg/pull/1430#discussion_r485035182", "bodyText": "Maybe just checking the hasArray and checking that the offset is 0 would be enough. And throw an exception in other cases. Maybe we can avoid the costly buffer allocation. Or use them only when they are needed.\nGood that you brought this up. Started to fix this warning, but then got sidetracked \ud83d\ude04", "author": "pvary", "createdAt": "2020-09-08T16:05:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAxODYwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTUzOTY3OQ==", "url": "https://github.com/apache/iceberg/pull/1430#discussion_r485539679", "bodyText": "@RussellSpitzer: Fixed with your suggested changes. I have chickened out from using different serialization method for different buffers because I think the added complexity might not worth the gained performance.\nCould you please recheck the change?\nThanks,\nPeter", "author": "pvary", "createdAt": "2020-09-09T11:32:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAxODYwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg4NzMyNQ==", "url": "https://github.com/apache/iceberg/pull/1430#discussion_r485887325", "bodyText": "lgtm!", "author": "RussellSpitzer", "createdAt": "2020-09-09T20:00:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAxODYwNg=="}], "type": "inlineReview"}, {"oid": "61a49ea813c47622cb7c666c8428c2acea3f39da", "url": "https://github.com/apache/iceberg/commit/61a49ea813c47622cb7c666c8428c2acea3f39da", "message": "Fix ByteBuffer serialization issue.", "committedDate": "2020-09-09T11:30:26Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTcyOTY1OQ==", "url": "https://github.com/apache/iceberg/pull/1430#discussion_r485729659", "bodyText": "Seems like we can probably just get away with a simple new java.util.HashMap(), no need for guava as a dependency for this class.", "author": "fbocse", "createdAt": "2020-09-09T16:04:57Z", "path": "api/src/main/java/org/apache/iceberg/Metrics.java", "diffHunk": "@@ -120,4 +124,90 @@ public Long recordCount() {\n   public Map<Integer, ByteBuffer> upperBounds() {\n     return upperBounds;\n   }\n+\n+  /**\n+   * Implemented the method to enable serialization of ByteBuffers.\n+   * @param out The stream where to write\n+   * @throws IOException On serialization error\n+   */\n+  private void writeObject(ObjectOutputStream out) throws IOException {\n+    out.writeObject(rowCount);\n+    out.writeObject(columnSizes);\n+    out.writeObject(valueCounts);\n+    out.writeObject(nullValueCounts);\n+\n+    writeByteBufferMap(out, lowerBounds);\n+    writeByteBufferMap(out, upperBounds);\n+  }\n+\n+  private static void writeByteBufferMap(ObjectOutputStream out, Map<Integer, ByteBuffer> byteBufferMap)\n+      throws IOException {\n+    if (byteBufferMap == null) {\n+      out.writeInt(-1);\n+\n+    } else {\n+      // Write the size\n+      out.writeInt(byteBufferMap.size());\n+\n+      for (Map.Entry<Integer, ByteBuffer> entry : byteBufferMap.entrySet()) {\n+        // Write the key\n+        out.writeObject(entry.getKey());\n+\n+        // Write the value\n+        if (entry.getValue() != null) {\n+          // Copy the actual values from the buffer\n+          ByteBuffer bb = entry.getValue();\n+          byte[] bytes = new byte[bb.remaining()];\n+          bb.get(bytes);\n+          bb.position(bb.position() - bytes.length); // Restores the buffer position\n+\n+          // Write out the data\n+          out.writeObject(bytes);\n+        } else {\n+          out.writeObject(null);\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Implemented the method to enable deserialization of ByteBuffers.\n+   * @param in The stream to read from\n+   * @throws IOException On serialization error\n+   * @throws ClassNotFoundException If the class is not found\n+   */\n+  private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException {\n+    rowCount = (Long) in.readObject();\n+    columnSizes = (Map<Integer, Long>) in.readObject();\n+    valueCounts = (Map<Integer, Long>) in.readObject();\n+    nullValueCounts = (Map<Integer, Long>) in.readObject();\n+\n+    lowerBounds = readByteBufferMap(in);\n+    upperBounds = readByteBufferMap(in);\n+  }\n+\n+  private static Map<Integer, ByteBuffer> readByteBufferMap(ObjectInputStream in)\n+      throws IOException, ClassNotFoundException {\n+    int size = in.readInt();\n+\n+    if (size == -1) {\n+      return null;\n+\n+    } else {\n+      Map<Integer, ByteBuffer> result = Maps.newHashMap();", "originalCommit": "61a49ea813c47622cb7c666c8428c2acea3f39da", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTgxNjAwNg==", "url": "https://github.com/apache/iceberg/pull/1430#discussion_r485816006", "bodyText": "Good point.\nRemoved guava dependency.\nThanks,\nPeter", "author": "pvary", "createdAt": "2020-09-09T18:07:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTcyOTY1OQ=="}], "type": "inlineReview"}, {"oid": "1737739a6424af07aecef6c38ce1669a0482ab42", "url": "https://github.com/apache/iceberg/commit/1737739a6424af07aecef6c38ce1669a0482ab42", "message": "Removed guava dependency", "committedDate": "2020-09-09T18:06:00Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU4NTYyMw==", "url": "https://github.com/apache/iceberg/pull/1430#discussion_r487585623", "bodyText": "We have a ByteBuffers util located in the core module at org.apache.iceberg.util.ByteBuffers that handles the appropriate copying and restoration of ByteBuffers.\nPlease consider using that for consistency and for easier refactoring of ByteBuffer issues in the future. Here's a link to that class: https://github.com/apache/iceberg/blob/master/core/src/main/java/org/apache/iceberg/util/ByteBuffers.java", "author": "kbendick", "createdAt": "2020-09-13T22:32:56Z", "path": "api/src/main/java/org/apache/iceberg/Metrics.java", "diffHunk": "@@ -120,4 +124,90 @@ public Long recordCount() {\n   public Map<Integer, ByteBuffer> upperBounds() {\n     return upperBounds;\n   }\n+\n+  /**\n+   * Implemented the method to enable serialization of ByteBuffers.\n+   * @param out The stream where to write\n+   * @throws IOException On serialization error\n+   */\n+  private void writeObject(ObjectOutputStream out) throws IOException {\n+    out.writeObject(rowCount);\n+    out.writeObject(columnSizes);\n+    out.writeObject(valueCounts);\n+    out.writeObject(nullValueCounts);\n+\n+    writeByteBufferMap(out, lowerBounds);\n+    writeByteBufferMap(out, upperBounds);\n+  }\n+\n+  private static void writeByteBufferMap(ObjectOutputStream out, Map<Integer, ByteBuffer> byteBufferMap)\n+      throws IOException {\n+    if (byteBufferMap == null) {\n+      out.writeInt(-1);\n+\n+    } else {\n+      // Write the size\n+      out.writeInt(byteBufferMap.size());\n+\n+      for (Map.Entry<Integer, ByteBuffer> entry : byteBufferMap.entrySet()) {\n+        // Write the key\n+        out.writeObject(entry.getKey());\n+\n+        // Write the value\n+        if (entry.getValue() != null) {\n+          // Copy the actual values from the buffer\n+          ByteBuffer bb = entry.getValue();\n+          byte[] bytes = new byte[bb.remaining()];\n+          bb.get(bytes);\n+          bb.position(bb.position() - bytes.length); // Restores the buffer position", "originalCommit": "1737739a6424af07aecef6c38ce1669a0482ab42", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU5NTEyNQ==", "url": "https://github.com/apache/iceberg/pull/1430#discussion_r487595125", "bodyText": "I agree with @kbendick. You can call ByteBuffers.toByteArray instead of adding the logic to read here.", "author": "rdblue", "createdAt": "2020-09-14T00:00:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU4NTYyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Nzc3NDE2Ng==", "url": "https://github.com/apache/iceberg/pull/1430#discussion_r487774166", "bodyText": "Thanks @kbendick! Have not noticed that class.\nHad to refactor that and move it to the API package to reuse it. Please check if you agree with the move, or you think there is a better place for it.\nThanks,\nPeter", "author": "pvary", "createdAt": "2020-09-14T09:26:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU4NTYyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU4NzI5MA==", "url": "https://github.com/apache/iceberg/pull/1430#discussion_r487587290", "bodyText": "Is this serialization format, e.g. rowCount, columnSizes, valueCounts, nullValueCounts and then the lowerBounds followed by upperBounds currently used anywhere else?\nIf so, perhaps there's a utility class that can be used?\nIf not, I personally think that this serialization format should be  documented somewhere. We might need to include the format version in it as well, like we do for other places (where the new v2 format specification will be what is used to support row level deletes).\nAdditionally, would it make sense to move this serialization logic into a utility class if there's nothing that currently meets this need?\nFor example, type conversions to get the lower bounds value from the ByteBuffer are currently done in org.apache.iceberg.types.Conversion#fromByteBuffer as mentioned elsehwere in this file. Here's a link to the code, perhaps something there might make this task easier / more standardized: https://github.com/apache/iceberg/blob/master/api/src/main/java/org/apache/iceberg/types/Conversions.java\nAnd if there's not a utility class, would it make sense to add one (like how the actual conversion of upperBounds and lowerBounds from their ByteBuffers is deletegated to the Conversions class?", "author": "kbendick", "createdAt": "2020-09-13T22:51:08Z", "path": "api/src/main/java/org/apache/iceberg/Metrics.java", "diffHunk": "@@ -120,4 +124,90 @@ public Long recordCount() {\n   public Map<Integer, ByteBuffer> upperBounds() {\n     return upperBounds;\n   }\n+\n+  /**\n+   * Implemented the method to enable serialization of ByteBuffers.\n+   * @param out The stream where to write\n+   * @throws IOException On serialization error\n+   */\n+  private void writeObject(ObjectOutputStream out) throws IOException {\n+    out.writeObject(rowCount);\n+    out.writeObject(columnSizes);\n+    out.writeObject(valueCounts);\n+    out.writeObject(nullValueCounts);\n+\n+    writeByteBufferMap(out, lowerBounds);\n+    writeByteBufferMap(out, upperBounds);\n+  }", "originalCommit": "1737739a6424af07aecef6c38ce1669a0482ab42", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU4ODkxNg==", "url": "https://github.com/apache/iceberg/pull/1430#discussion_r487588916", "bodyText": "Oh, actually, I spoke too soon. There is documentation on the serialization of types when storing them as lower bounds / upper bounds in the maps of manifest files, namely in the data_files struct:  https://iceberg.apache.org/spec/#manifests\nThere's also documentation on what single values can be stored in the upper bounds / lower bounds byte buffer maps and how they are serialized: https://iceberg.apache.org/spec/#appendix-d-single-value-serialization\nPerhaps something from that documentation page can be used to make this serialization choice  inline with the existing stuff? Or at the least I would recommend that documentation be added, though I think that you might want to consider submitting this PR to the dev mailing list at dv@iceberg.apache.org, as I feel that likely some people will have an opinion about the introduction of an additional standard for serialization (especially given that it's part of the public facing API). You can see the dev mailing list archive here: https://www.mail-archive.com/dev@iceberg.apache.org/", "author": "kbendick", "createdAt": "2020-09-13T23:06:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU4NzI5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU5NDUyNA==", "url": "https://github.com/apache/iceberg/pull/1430#discussion_r487594524", "bodyText": "This isn't really a serialization format that needs to be specified or reusable (besides maybe writeByteBufferMap). Java serialization should only be used within a running job that has just one version of the Iceberg library, so what matters is that a test can write an object and read an identical one.\nWe don't want to go too far down the path of documenting things like this that we don't intend to support as a format.", "author": "rdblue", "createdAt": "2020-09-13T23:55:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU4NzI5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU5NDgzNQ==", "url": "https://github.com/apache/iceberg/pull/1430#discussion_r487594835", "bodyText": "Don't we need to flush the object output stream?", "author": "rdblue", "createdAt": "2020-09-13T23:58:15Z", "path": "api/src/test/java/org/apache/iceberg/TestMetricsSerialization.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.nio.ByteBuffer;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+public class TestMetricsSerialization {\n+\n+  @Test\n+  public void testSerialization() throws IOException, ClassNotFoundException {\n+    Metrics original = generateMetrics();\n+\n+    byte[] serialized = serialize(original);\n+    Metrics result = deserialize(serialized);\n+\n+    assertEquals(original, result);\n+  }\n+\n+  @Test\n+  public void testSerializationWithNulls() throws IOException, ClassNotFoundException {\n+    Metrics original = generateMetricsWithNulls();\n+\n+    byte[] serialized = serialize(original);\n+    Metrics result = deserialize(serialized);\n+\n+    assertEquals(original, result);\n+  }\n+\n+  private static byte[] serialize(Metrics metrics) throws IOException {\n+    try (ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream()) {\n+      ObjectOutputStream objectOutputStream = new ObjectOutputStream(byteArrayOutputStream);\n+      objectOutputStream.writeObject(metrics);", "originalCommit": "1737739a6424af07aecef6c38ce1669a0482ab42", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Nzc3NzY2Ng==", "url": "https://github.com/apache/iceberg/pull/1430#discussion_r487777666", "bodyText": "Good catch! Thanks!\nDone", "author": "pvary", "createdAt": "2020-09-14T09:32:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU5NDgzNQ=="}], "type": "inlineReview"}, {"oid": "bc2ae6951a44f9798517c601350c936f057c6a07", "url": "https://github.com/apache/iceberg/commit/bc2ae6951a44f9798517c601350c936f057c6a07", "message": "Used ByteBuffers.toByteArray to get the byte[] from the buffers.", "committedDate": "2020-09-14T09:24:07Z", "type": "commit"}, {"oid": "e41675b841efbdaf1fdeddd34b6b95d0262c29e4", "url": "https://github.com/apache/iceberg/commit/e41675b841efbdaf1fdeddd34b6b95d0262c29e4", "message": "Flushing the stream in the tests to prevent possible flakiness", "committedDate": "2020-09-14T09:31:29Z", "type": "commit"}]}