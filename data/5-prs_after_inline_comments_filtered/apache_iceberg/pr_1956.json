{"pr_number": 1956, "pr_title": "Flink: Upgrade version from 1.11.0 to 1.12.1", "pr_createdAt": "2020-12-18T04:34:44Z", "pr_url": "https://github.com/apache/iceberg/pull/1956", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjUzMDU3Mg==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r546530572", "bodyText": "I checked the flink codebase,  all the RowData's TypeSerializer related classes are not expose to be public, they are marked as Internal.  The only way I can think of is:  Use a compactbility utility to load InternalTypeInfo  or RowDataTypeInfo class if there's exist one.", "author": "openinx", "createdAt": "2020-12-21T06:28:00Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -91,7 +91,7 @@ public static Builder forRow(DataStream<Row> input, TableSchema tableSchema) {\n     DataType[] fieldDataTypes = tableSchema.getFieldDataTypes();\n \n     DataFormatConverters.RowConverter rowConverter = new DataFormatConverters.RowConverter(fieldDataTypes);\n-    return builderFor(input, rowConverter::toInternal, RowDataTypeInfo.of(rowType))\n+    return builderFor(input, rowConverter::toInternal, InternalTypeInfo.of(rowType))", "originalCommit": "6c3caf41a3f1930355f8eaec1189a4e7a5e6c583", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjgxMjc2Mg==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r546812762", "bodyText": "I think this is the shim layer approach that others suggested.", "author": "stevenzwu", "createdAt": "2020-12-21T16:46:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjUzMDU3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzI3MzY2Mg==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r547273662", "bodyText": "Em, It's true.", "author": "openinx", "createdAt": "2020-12-22T13:20:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjUzMDU3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjUzNDI5OQ==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r546534299", "bodyText": "It's strange here, because I saw the TableColumn is marked as PublicEvolving, but after released flink 1.12.0 it did not have any Interface compatibility guarantee.  At least, it should marked as deprecated, and keep it a major release.", "author": "openinx", "createdAt": "2020-12-21T06:41:33Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java", "diffHunk": "@@ -83,8 +83,8 @@ public void testGetTable() {\n             Types.NestedField.optional(1, \"strV\", Types.StringType.get())));\n     Assert.assertEquals(\n         Arrays.asList(\n-            TableColumn.of(\"id\", DataTypes.BIGINT()),\n-            TableColumn.of(\"strV\", DataTypes.STRING())),\n+            TableColumn.physical(\"id\", DataTypes.BIGINT()),", "originalCommit": "6c3caf41a3f1930355f8eaec1189a4e7a5e6c583", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjA0NjU5MQ==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r566046591", "bodyText": "@stevenzwu  I think we could change to use the loaded iceberg schema to accomplish the schema validation, so that we don't have to use the flink's TableSchema.\nopeninx@2a63571#diff-4c56ab08c19464dbe3351f71fc39345ee031a282b3e8dc1b107cbe9a1964d105R81", "author": "openinx", "createdAt": "2021-01-28T12:13:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjUzNDI5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjIyODE5OA==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r566228198", "bodyText": "thx a lot for the suggestion. will update", "author": "stevenzwu", "createdAt": "2021-01-28T16:24:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjUzNDI5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzY4ODc5Ng==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r567688796", "bodyText": "sorry, this API break was a mistake and will be fixed in the next bugfix release\nhttps://issues.apache.org/jira/browse/FLINK-21226", "author": "twalthr", "createdAt": "2021-02-01T09:49:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjUzNDI5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzcyNjY0Ng==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r567726646", "bodyText": "It's OK,  we've planned to just upgrade the flink 1.12 because we needs the newly introduced API from 1.12.0 to develop our unified iceberg flink source/sink.  Thanks @twalthr for the work.", "author": "openinx", "createdAt": "2021-02-01T10:47:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjUzNDI5OQ=="}], "type": "inlineReview"}, {"oid": "2c721b14acecc12f211bafebdf14c8fe0734d86a", "url": "https://github.com/apache/iceberg/commit/2c721b14acecc12f211bafebdf14c8fe0734d86a", "message": "define MiniClusterWithClientResource @ClassRule directly so that we can disable CoreOptions.CHECK_LEAKED_CLASSLOADER", "committedDate": "2020-12-23T00:42:12Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTQ3NTA0Mw==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r549475043", "bodyText": "Would it make sense to use 2 here (or any value greater than 1)? At my work, we've found that some people's code runs into issues once it's distributed. I highly doubt that's likely to be the case here, given that typically those users were relatively new end users of flink, but it still might be helpful to run the tests with non-local shuffles (not in the same JVM). We could do 2 and then halve the number of slots per task manager.\nYour call, but since this has been helpful for some people in my organization I thought I'd mention it.", "author": "kbendick", "createdAt": "2020-12-28T20:18:24Z", "path": "flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSink.java", "diffHunk": "@@ -46,21 +49,36 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.junit.Assert;\n import org.junit.Before;\n-import org.junit.Rule;\n+import org.junit.ClassRule;\n import org.junit.Test;\n import org.junit.rules.TemporaryFolder;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n @RunWith(Parameterized.class)\n-public class TestFlinkIcebergSink extends AbstractTestBase {\n+public class TestFlinkIcebergSink extends TestBaseUtils {\n   private static final TypeInformation<Row> ROW_TYPE_INFO = new RowTypeInfo(\n       SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n   private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n       SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n \n-  @Rule\n-  public TemporaryFolder tempFolder = new TemporaryFolder();\n+  private static final int DEFAULT_PARALLELISM = 4;\n+\n+  private static final org.apache.flink.configuration.Configuration config =\n+      new org.apache.flink.configuration.Configuration()\n+          // disable classloader check as Avro may cache class/object in the serializers.\n+          .set(CoreOptions.CHECK_LEAKED_CLASSLOADER, false);\n+\n+  @ClassRule\n+  public static MiniClusterWithClientResource miniClusterResource = new MiniClusterWithClientResource(\n+      new MiniClusterResourceConfiguration.Builder()\n+          .setNumberTaskManagers(1)", "originalCommit": "2c721b14acecc12f211bafebdf14c8fe0734d86a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTkzNDQ2OA==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r549934468", "bodyText": "it seems that most Flink unit test with MiniCluster are using one taskmanager. I would stick with the convention.\nOn the other hand, this code is added to disable CHECK_LEAKED_CLASSLOADER. If we can fix that problem, we can stick with the AbstractTestBase base class", "author": "stevenzwu", "createdAt": "2020-12-30T04:49:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTQ3NTA0Mw=="}], "type": "inlineReview"}, {"oid": "65089cf003edf477c3a6dd615e34fcdd8ee96807", "url": "https://github.com/apache/iceberg/commit/65089cf003edf477c3a6dd615e34fcdd8ee96807", "message": "disable CoreOptions.CHECK_LEAKED_CLASSLOADER for FlinkTestBase", "committedDate": "2020-12-31T00:44:18Z", "type": "forcePushed"}, {"oid": "ebbf155e3aee84c64d28e11101988be017e14134", "url": "https://github.com/apache/iceberg/commit/ebbf155e3aee84c64d28e11101988be017e14134", "message": "disable CoreOptions.CHECK_LEAKED_CLASSLOADER for FlinkTestBase", "committedDate": "2020-12-31T00:54:03Z", "type": "forcePushed"}, {"oid": "70235e302af747a9ab8828324477d404911b1eb2", "url": "https://github.com/apache/iceberg/commit/70235e302af747a9ab8828324477d404911b1eb2", "message": "disable CoreOptions.CHECK_LEAKED_CLASSLOADER for FlinkTestBase", "committedDate": "2021-01-02T19:24:29Z", "type": "forcePushed"}, {"oid": "0f2c7977b05a4638811c296c8843dd3e8e56628c", "url": "https://github.com/apache/iceberg/commit/0f2c7977b05a4638811c296c8843dd3e8e56628c", "message": "disable CoreOptions.CHECK_LEAKED_CLASSLOADER for FlinkTestBase", "committedDate": "2021-01-03T02:31:30Z", "type": "forcePushed"}, {"oid": "558e3f5996ecc092b04142df560858ab530e53dd", "url": "https://github.com/apache/iceberg/commit/558e3f5996ecc092b04142df560858ab530e53dd", "message": "disable CoreOptions.CHECK_LEAKED_CLASSLOADER for FlinkTestBase", "committedDate": "2021-01-04T22:58:58Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY3OTcwNw==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r551679707", "bodyText": "I read the javadoc about  TableResult again.  The correct way to execute sql and get the results is:\nTableResult result = tEnv.execute(\"select ...\");\n// using try-with-resources statement\ntry (CloseableIterator<Row> it = result.collect()) {\n        it... // collect same data\n}\nThen I think we don't have to call c.getJobExecutionResult().get() here. How about removing  the line101 ~ line 106 and the following part use try-with-resources statement ?", "author": "openinx", "createdAt": "2021-01-05T02:21:31Z", "path": "flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java", "diffHunk": "@@ -76,7 +99,7 @@ protected TableEnvironment getTableEnv() {\n     TableResult tableResult = getTableEnv().executeSql(String.format(query, args));\n     tableResult.getJobClient().ifPresent(c -> {\n       try {\n-        c.getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();\n+        c.getJobExecutionResult().get();", "originalCommit": "558e3f5996ecc092b04142df560858ab530e53dd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTcwNDY3OQ==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r551704679", "bodyText": "I was wondering the same question regarding c.getJobExecutionResult().get(). Updated the PR per your suggestion.", "author": "stevenzwu", "createdAt": "2021-01-05T04:02:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY3OTcwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDI4NzcwMA==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r554287700", "bodyText": "BTW, removing the JobClientgetJobExecutionResult().get() works well for Flink 1.12. It seems that it doesn't work with Flink 1.11 when I tried it in the Flink test refactor PR", "author": "stevenzwu", "createdAt": "2021-01-09T04:10:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY3OTcwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY4MDUyOA==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r551680528", "bodyText": "Similar issues  here.  ( see https://github.com/apache/iceberg/pull/1956/files#r551679707 )", "author": "openinx", "createdAt": "2021-01-05T02:23:47Z", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java", "diffHunk": "@@ -107,7 +121,14 @@ public void testResiduals() throws Exception {\n   }\n \n   private List<Row> executeSQL(String sql) {\n-    return Lists.newArrayList(tEnv.executeSql(sql).collect());\n+    CloseableIterator<Row> iter = getTableEnv().executeSql(sql).collect();", "originalCommit": "558e3f5996ecc092b04142df560858ab530e53dd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTcwNDcyNg==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r551704726", "bodyText": "updated", "author": "stevenzwu", "createdAt": "2021-01-05T04:02:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY4MDUyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY4MTEzNw==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r551681137", "bodyText": "Why do we need to change here ?", "author": "openinx", "createdAt": "2021-01-05T02:25:47Z", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java", "diffHunk": "@@ -52,12 +53,25 @@ public TestFlinkScanSql(String fileFormat) {\n   @Override\n   public void before() throws IOException {\n     super.before();\n-    tEnv = TableEnvironment.create(EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build());\n-    tEnv.executeSql(String.format(\n+    getTableEnv().executeSql(String.format(", "originalCommit": "558e3f5996ecc092b04142df560858ab530e53dd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY4MzM2NQ==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r551683365", "bodyText": "this is to avoid creating the table environment for each test method. it is the same pattern used by FlinkTestBase.", "author": "stevenzwu", "createdAt": "2021-01-05T02:33:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY4MTEzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjA2MDE0Nw==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r566060147", "bodyText": "OK, that sounds good to me.\nNit: here we could just use the executeSQL methods now.   Maybe we could just rename the executeSQL(String sql) as sql(String query, Object... args), so that we could have the same usage that is similar to the FlinkTestBase#sql.", "author": "openinx", "createdAt": "2021-01-28T12:34:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY4MTEzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjIyMDM3Ng==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r566220376", "bodyText": "really like the suggestion. will update", "author": "stevenzwu", "createdAt": "2021-01-28T16:14:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY4MTEzNw=="}], "type": "inlineReview"}, {"oid": "4f1f1d48b2dc1148b27db2bb16abe3b8a94f3563", "url": "https://github.com/apache/iceberg/commit/4f1f1d48b2dc1148b27db2bb16abe3b8a94f3563", "message": "use try-with-resources statement for collecting table result", "committedDate": "2021-01-05T03:59:09Z", "type": "forcePushed"}, {"oid": "f33e94c29845ffee9a2586098a942182f965a623", "url": "https://github.com/apache/iceberg/commit/f33e94c29845ffee9a2586098a942182f965a623", "message": "use try-with-resources statement for collecting table result", "committedDate": "2021-01-06T22:11:07Z", "type": "forcePushed"}, {"oid": "f9d4dd6047b0328993818d687cf8193c2f6f26d6", "url": "https://github.com/apache/iceberg/commit/f9d4dd6047b0328993818d687cf8193c2f6f26d6", "message": "use try-with-resources statement for collecting table result", "committedDate": "2021-01-17T00:34:12Z", "type": "forcePushed"}, {"oid": "98d9f6ad511b47b554ca14ae5d7fa8931161b3bf", "url": "https://github.com/apache/iceberg/commit/98d9f6ad511b47b554ca14ae5d7fa8931161b3bf", "message": "update TestFlinkTableSourc with Flink 1.12 upgrade as the behavior changed: (1) like '%' or '%%' now matches null (2) NOT IN filter push down may not maintain the same order", "committedDate": "2021-01-19T01:22:44Z", "type": "forcePushed"}, {"oid": "4b9baeb07babb634a5d717df180666ece2792edf", "url": "https://github.com/apache/iceberg/commit/4b9baeb07babb634a5d717df180666ece2792edf", "message": "update Flink version from 1.12.0 to 1.12.1", "committedDate": "2021-01-26T23:29:29Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjA0NjczNg==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r566046736", "bodyText": "ditto.", "author": "openinx", "createdAt": "2021-01-28T12:13:45Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java", "diffHunk": "@@ -107,7 +107,7 @@ public void testRenameTable() {\n         () -> getTableEnv().from(\"tl\")\n     );\n     Assert.assertEquals(\n-        Collections.singletonList(TableColumn.of(\"id\", DataTypes.BIGINT())),\n+        Collections.singletonList(TableColumn.physical(\"id\", DataTypes.BIGINT())),", "originalCommit": "4b9baeb07babb634a5d717df180666ece2792edf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2Njc5MTc2Nw==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r566791767", "bodyText": "Here,  we could use the similar way to avoid to use TableColumn#physical. openinx@2a63571#diff-4c56ab08c19464dbe3351f71fc39345ee031a282b3e8dc1b107cbe9a1964d105R102", "author": "openinx", "createdAt": "2021-01-29T12:36:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjA0NjczNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODI3OTA4Nw==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r568279087", "bodyText": "@stevenzwu ,  do you think  this issue need to fix in this PR ? I think we'd better to...", "author": "openinx", "createdAt": "2021-02-02T02:27:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjA0NjczNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjA0NzYyOQ==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r566047629", "bodyText": "Why do we need to change here ?  in the new flink 1.12.1 release,  the null data will be hit the LIKE '%%' ?", "author": "openinx", "createdAt": "2021-01-28T12:15:29Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -617,12 +617,14 @@ public void testFilterNotPushDownLike() {\n     explainNoPushDown = getTableEnv().explainSql(sqlNoPushDown);\n     Assert.assertFalse(\"Explain should not contain FilterPushDown\",\n         explainNoPushDown.contains(expectedFilterPushDownExplain));\n+\n     sqlNoPushDown = \"SELECT * FROM  \" + TABLE_NAME + \"  WHERE data LIKE '%%' \";\n     resultLike = sql(sqlNoPushDown);\n-    Assert.assertEquals(\"Should have 2 records\", 2, resultLike.size());\n+    Assert.assertEquals(\"Should have 3 records\", 3, resultLike.size());", "originalCommit": "4b9baeb07babb634a5d717df180666ece2792edf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjE3OTE4Mg==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r566179182", "bodyText": "yeah. there are a couple of small behavior changes in 1.12. I also confirmed with zhangjun0x01\n98d9f6a", "author": "stevenzwu", "createdAt": "2021-01-28T15:23:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjA0NzYyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjUyMTU5Mw==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r566521593", "bodyText": "I tested it again. In hive 2.3.7, the sql select * from mytest where data like '%'  , data is null will not match. In flink 1.12, it will return the value of data is null. I have not found the related flink jira for this", "author": "zhangjun0x01", "createdAt": "2021-01-29T01:34:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjA0NzYyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjA1Nzc2OQ==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r566057769", "bodyText": "I'd prefer to create our iceberg's RowDataCloner (which is similar to the RowDataConvert ),  so that we could get ride of this flink internal InternalSerializers.\nRelying on flink internal interfaces that do not promise compatibility should be avoided as much as possible in subsequent development codes.", "author": "openinx", "createdAt": "2021-01-28T12:30:30Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestHelpers.java", "diffHunk": "@@ -62,9 +61,8 @@ private TestHelpers() {\n   }\n \n   public static RowData copyRowData(RowData from, RowType rowType) {\n-    ExecutionConfig config = new ExecutionConfig();\n     TypeSerializer[] fieldSerializers = rowType.getChildren().stream()\n-        .map((LogicalType type) -> InternalSerializers.create(type, config))\n+        .map((LogicalType type) -> InternalSerializers.create(type))", "originalCommit": "4b9baeb07babb634a5d717df180666ece2792edf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjM0MjU3OQ==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r566342579", "bodyText": "It will be great if we can achieve the goal. Looking at the behavior of clone, I am wondering if we will need to reinvent a lot of specific serializers (like BooleanSerializer) that InternalSerializers eventually depends on. Those type serializers are also internal just like InternalSerializers.\nIdeally, it will be great if RowData interface provides a public clone API. Actually, in the FLIP-27 source, we need need to clone RowData since RowDataIterator returns a reused RowData object.", "author": "stevenzwu", "createdAt": "2021-01-28T19:11:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjA1Nzc2OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2Njc5NjY2OQ==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r566796669", "bodyText": "Here we will use this to copy a RowData into a newly created GenericRowData, that means we don't have to use the complex TypeSerializer ( that is used for copying binary data for BinaryRowData).   I think it will be good to remove the internal API if possible,  so I'd recommend to use the  similar RowDataConverter way to copy a totally new java objects for each fields.\nBTW, I've discussed with flink sql team (@wuchong),   the future apache flink will provides RowData copy utility to acomplish this RowData clone..", "author": "openinx", "createdAt": "2021-01-29T12:45:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjA1Nzc2OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2Njc5ODE5Mw==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r566798193", "bodyText": "Of course, we could open separate PR to address the InternalSerializers issue if you think it's necessary.", "author": "openinx", "createdAt": "2021-01-29T12:48:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjA1Nzc2OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjkyNTgwOA==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r566925808", "bodyText": "Created issue-2184 to track this issue. probably a separate PR is better.", "author": "stevenzwu", "createdAt": "2021-01-29T16:05:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjA1Nzc2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjA2MTgxMQ==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r566061811", "bodyText": "Nit: How about just import the org.apache.flink.configuration.Configuration class and use:\nprivate static final Configuration config = new Configuration().set(CoreOptions.CHECK_LEAKED_CLASSLOADER, false);\nhere.", "author": "openinx", "createdAt": "2021-01-28T12:37:36Z", "path": "flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java", "diffHunk": "@@ -34,8 +36,28 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.junit.AfterClass;\n import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.rules.TemporaryFolder;\n \n-public abstract class FlinkTestBase extends AbstractTestBase {\n+public abstract class FlinkTestBase extends TestBaseUtils {\n+\n+  private static final int DEFAULT_PARALLELISM = 4;\n+\n+  private static final org.apache.flink.configuration.Configuration config =", "originalCommit": "4b9baeb07babb634a5d717df180666ece2792edf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjE4MDEzNg==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r566180136", "bodyText": "will fix", "author": "stevenzwu", "createdAt": "2021-01-28T15:24:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjA2MTgxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjA2NDI0MQ==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r566064241", "bodyText": "As this InternalTypeInfo is the flink internal API that we have to depends on in the flink+iceberg integration work,  I'd like to move this InternalTypeInfo.of(RowType rowType) into a separate small utility methods,  in case of changing every places that refer this internal class.\nopeninx@2a63571#diff-e4d1084ea160d030f3a225948d224678436b79eb5a94df230c71cdc7327f99d8R43", "author": "openinx", "createdAt": "2021-01-28T12:41:52Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -33,7 +33,7 @@\n import org.apache.flink.table.api.TableSchema;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.data.util.DataFormatConverters;\n-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;", "originalCommit": "4b9baeb07babb634a5d717df180666ece2792edf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjI0MDkyNQ==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r566240925", "bodyText": "will do assuming you meant just consolidation of calls to Flink Internal and PublicEvolve APIs. we don't need to support both Flink 1.11 and 1.12 in the compatibility util class.", "author": "stevenzwu", "createdAt": "2021-01-28T16:41:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjA2NDI0MQ=="}], "type": "inlineReview"}, {"oid": "976cc67f1030b9f938bed214b9041f167d8a71be", "url": "https://github.com/apache/iceberg/commit/976cc67f1030b9f938bed214b9041f167d8a71be", "message": "address the review comments", "committedDate": "2021-01-28T19:17:30Z", "type": "forcePushed"}, {"oid": "5dd3ea888dc0c439a428742407bd4b48483ed9e8", "url": "https://github.com/apache/iceberg/commit/5dd3ea888dc0c439a428742407bd4b48483ed9e8", "message": "fix javadoc", "committedDate": "2021-01-28T22:43:45Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjgyMzg0MQ==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r566823841", "bodyText": "I think we need to disable the CoreOptions.CHECK_LEAKED_CLASSLOADER here  ?  Seems like we usually validation the iceberg tables results once we've terminated the flink job.", "author": "openinx", "createdAt": "2021-01-29T13:32:31Z", "path": "flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java", "diffHunk": "@@ -34,8 +37,27 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.junit.AfterClass;\n import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.rules.TemporaryFolder;\n \n-public abstract class FlinkTestBase extends AbstractTestBase {\n+public abstract class FlinkTestBase extends TestBaseUtils {\n+\n+  private static final int DEFAULT_PARALLELISM = 4;\n+\n+  private static final Configuration config = new Configuration()\n+          // disable classloader check as Avro may cache class/object in the serializers.\n+          .set(CoreOptions.CHECK_LEAKED_CLASSLOADER, false);", "originalCommit": "5dd3ea888dc0c439a428742407bd4b48483ed9e8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjkxODAzNg==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r566918036", "bodyText": "It is already disabled here. We need to disable this check for other tests that are affected by this. CI tests were passing after disabling here. It started to fail again after I rebased with latest master.", "author": "stevenzwu", "createdAt": "2021-01-29T15:54:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjgyMzg0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzAxNzM1Mg==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r567017352", "bodyText": "This is getting really tricky to debug. Can't reproduce it locally. Hence, can't set up break point or add debug logging to custom built Flink.\nI tried to disable CoreOptions.CHECK_LEAKED_CLASSLOADER for all MiniClulster tests. That doesn't help.", "author": "stevenzwu", "createdAt": "2021-01-29T18:37:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjgyMzg0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzcyNzI5OQ==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r567727299", "bodyText": "OK, I'd see how to reproduce those class loader issues.", "author": "openinx", "createdAt": "2021-02-01T10:48:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjgyMzg0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2Nzc2NTg0Nw==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r567765847", "bodyText": "There're still some ITCases which did not disable the CHECK_LEAKED_CLASSLOADER in current patch, so I made this commit: openinx@c8c2c8e to re-trigger the travis CI again, let's see what will it say: https://github.com/openinx/incubator-iceberg/runs/1806279011", "author": "openinx", "createdAt": "2021-02-01T11:52:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjgyMzg0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2Nzc2NjgzMQ==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r567766831", "bodyText": "If the travis CI pass, then @stevenzwu, you can apply this commit into this PR to retry this travis CI:\ncurl -s https://github.com/openinx/incubator-iceberg/commit/c8c2c8e8e72fd1b2051f1f946bb54c968e86c81d.patch | git am", "author": "openinx", "createdAt": "2021-02-01T11:54:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjgyMzg0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzgwODUzNw==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r567808537", "bodyText": "Seems all unit tests are passed https://github.com/openinx/incubator-iceberg/runs/1806279011 ! @stevenzwu , you could add this commit and then retry the travis CI again.", "author": "openinx", "createdAt": "2021-02-01T13:04:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjgyMzg0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODA0MjU0MA==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r568042540", "bodyText": "@openinx thx a lot for identifying the missing integration tests that require disabling classloader check. I didn't realize the need to search for StreamExecutionEnvironment.getExecutionEnvironment.\nBTW, I also added MiniClusterWithClientResource to TestFlinkIcebergSinkV2, which might help speed up a tiny bit.", "author": "stevenzwu", "createdAt": "2021-02-01T18:23:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjgyMzg0MQ=="}], "type": "inlineReview"}, {"oid": "8fd95cb99da61ead466d20000d248d7a3d5bf92c", "url": "https://github.com/apache/iceberg/commit/8fd95cb99da61ead466d20000d248d7a3d5bf92c", "message": "create our own MiniClusterTestBase class that disable classloader check, because Flink tests can still use Flink's classloader to read and validate table result after the Flink job is terminated.", "committedDate": "2021-02-01T16:39:54Z", "type": "forcePushed"}, {"oid": "ae99bc30db9de35ae016c952c3ba90a39b4d4307", "url": "https://github.com/apache/iceberg/commit/ae99bc30db9de35ae016c952c3ba90a39b4d4307", "message": "create our own MiniClusterBase class that disable classloader check, because Flink tests can still use Flink's classloader to read and validate table result after the Flink job is terminated.", "committedDate": "2021-02-01T17:06:35Z", "type": "forcePushed"}, {"oid": "75f0b00d3be0149bc34ca934516fe1d652047f45", "url": "https://github.com/apache/iceberg/commit/75f0b00d3be0149bc34ca934516fe1d652047f45", "message": "upgrade flink version from 1.11.0 to 1.12.0", "committedDate": "2021-02-01T21:45:03Z", "type": "commit"}, {"oid": "60b43928a2693de0b5949f5804b2c95796fc119d", "url": "https://github.com/apache/iceberg/commit/60b43928a2693de0b5949f5804b2c95796fc119d", "message": "only create table env once and close the table result itertor in TestFlinkScanSql", "committedDate": "2021-02-01T21:45:03Z", "type": "commit"}, {"oid": "1af9e61b48d337bc233bda5d66d85b6830ab72c1", "url": "https://github.com/apache/iceberg/commit/1af9e61b48d337bc233bda5d66d85b6830ab72c1", "message": "disable CoreOptions.CHECK_LEAKED_CLASSLOADER for FlinkTestBase", "committedDate": "2021-02-01T21:45:03Z", "type": "commit"}, {"oid": "ed59526a1a1fd23416349dfd2e4c65d1e34dcdfa", "url": "https://github.com/apache/iceberg/commit/ed59526a1a1fd23416349dfd2e4c65d1e34dcdfa", "message": "use try-with-resources statement for collecting table result", "committedDate": "2021-02-01T21:45:03Z", "type": "commit"}, {"oid": "b1289ea124283357c73ef3818a2546c31b8aa3d1", "url": "https://github.com/apache/iceberg/commit/b1289ea124283357c73ef3818a2546c31b8aa3d1", "message": "update TestFlinkTableSourc with Flink 1.12 upgrade as the behavior changed: (1) like '%' or '%%' now matches null (2) NOT IN filter push down may not maintain the same order", "committedDate": "2021-02-01T21:45:03Z", "type": "commit"}, {"oid": "f9f81e48ca3927ada56098260f51dd47c3b03c13", "url": "https://github.com/apache/iceberg/commit/f9f81e48ca3927ada56098260f51dd47c3b03c13", "message": "update Flink version from 1.12.0 to 1.12.1", "committedDate": "2021-02-01T21:45:45Z", "type": "commit"}, {"oid": "94dab69cb88f93fdcb5568f8460b8887c39c867f", "url": "https://github.com/apache/iceberg/commit/94dab69cb88f93fdcb5568f8460b8887c39c867f", "message": "address the review comments", "committedDate": "2021-02-01T21:45:45Z", "type": "commit"}, {"oid": "39a6fef4e7b220ffaf02084ea20ca411929175a4", "url": "https://github.com/apache/iceberg/commit/39a6fef4e7b220ffaf02084ea20ca411929175a4", "message": "fix javadoc", "committedDate": "2021-02-01T21:45:45Z", "type": "commit"}, {"oid": "744a8273f406d6c146b6237303ae4123d5486da7", "url": "https://github.com/apache/iceberg/commit/744a8273f406d6c146b6237303ae4123d5486da7", "message": "create our own MiniClusterBase class that disable classloader check, because Flink tests can still use Flink's classloader to read and validate table result after the Flink job is terminated.", "committedDate": "2021-02-01T21:45:45Z", "type": "commit"}, {"oid": "a7c983b96bd909ae6db1320c3eab0ed770813278", "url": "https://github.com/apache/iceberg/commit/a7c983b96bd909ae6db1320c3eab0ed770813278", "message": "enable TestFlinkCatalogTable#testCreateTableIfNotExists now we are on Flink 1.12.\n\nHave to adjust the test in a couple of places.", "committedDate": "2021-02-01T21:45:45Z", "type": "forcePushed"}, {"oid": "b05c00e1a311396400e277bdb07d024e5fb0781f", "url": "https://github.com/apache/iceberg/commit/b05c00e1a311396400e277bdb07d024e5fb0781f", "message": "enable TestFlinkCatalogTable#testCreateTableIfNotExists now we are on Flink 1.12.\n\nHave to adjust the test in a couple of places.", "committedDate": "2021-02-01T23:06:06Z", "type": "commit"}, {"oid": "b05c00e1a311396400e277bdb07d024e5fb0781f", "url": "https://github.com/apache/iceberg/commit/b05c00e1a311396400e277bdb07d024e5fb0781f", "message": "enable TestFlinkCatalogTable#testCreateTableIfNotExists now we are on Flink 1.12.\n\nHave to adjust the test in a couple of places.", "committedDate": "2021-02-01T23:06:06Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODI3NzgxNA==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r568277814", "bodyText": "Nit: in apache iceberg, we usually use the unified Lists.newArrayLists to create a new ArrayList.", "author": "openinx", "createdAt": "2021-02-02T02:24:19Z", "path": "flink/src/test/java/org/apache/iceberg/flink/FlinkCatalogTestBase.java", "diffHunk": "@@ -119,6 +123,12 @@ protected String warehouseRoot() {\n     }\n   }\n \n+  protected String getFullQualifiedTableName(String tableName) {\n+    final List<String> levels = new ArrayList<>(Arrays.asList(icebergNamespace.levels()));", "originalCommit": "b05c00e1a311396400e277bdb07d024e5fb0781f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODI4NTc3MA==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r568285770", "bodyText": "will fix", "author": "stevenzwu", "createdAt": "2021-02-02T02:48:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODI3NzgxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODI3OTMyNw==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r568279327", "bodyText": "Thanks a lot for the unit tests improvement !", "author": "openinx", "createdAt": "2021-02-02T02:28:37Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java", "diffHunk": "@@ -136,13 +133,23 @@ public void testCreateTableIfNotExists() {\n \n     sql(\"DROP TABLE tl\");\n     AssertHelpers.assertThrows(\"Table 'tl' should be dropped\",\n-        NoSuchTableException.class, \"Table does not exist: db.tl\", () -> table(\"tl\"));\n+        NoSuchTableException.class,\n+        \"Table does not exist: \" + getFullQualifiedTableName(\"tl\"),\n+        () -> table(\"tl\"));\n \n-    sql(\"CREATE TABLE IF NO EXISTS tl(id BIGINT)\");\n+    sql(\"CREATE TABLE IF NOT EXISTS tl(id BIGINT)\");\n     Assert.assertEquals(Maps.newHashMap(), table(\"tl\").properties());\n \n-    sql(\"CREATE TABLE IF NOT EXISTS tl(id BIGINT) WITH ('location'='/tmp/location')\");\n-    Assert.assertEquals(\"Should still be the old table.\", Maps.newHashMap(), table(\"tl\").properties());\n+    final String uuid = UUID.randomUUID().toString();", "originalCommit": "b05c00e1a311396400e277bdb07d024e5fb0781f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODI4NDA5Mg==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r568284092", "bodyText": "I saw the TestFlinkIcebergSinkV2 also defines the similar mini cluster resources, then how about make it into a small methods so that the TestFlinkIcebergSinkV2 could reuse it ?   For future defined mini cluster resource, we'd better also reuse this one because it will be easy to forget to disable this CHECK_LEAKED_CLASSLOADER switch for developers.\n  @ClassRule\n  public static MiniClusterWithClientResource miniClusterResource = createMiniClusterResource();\n\n  @ClassRule\n  public static final TemporaryFolder TEMPORARY_FOLDER = new TemporaryFolder();\n\n  public static MiniClusterWithClientResource createMiniClusterResource() {\n    return new MiniClusterWithClientResource(\n        new MiniClusterResourceConfiguration.Builder()\n            .setNumberTaskManagers(1)\n            .setNumberSlotsPerTaskManager(DEFAULT_PARALLELISM)\n            .setConfiguration(CONFIG)\n            .build());\n  }\nThe TestFlinkIcebergSinkV2 could just use:\n@ClassRule\npublic static MiniClusterWithClientResource miniClusterResource = MiniClusterBase.createMiniClusterResource();", "author": "openinx", "createdAt": "2021-02-02T02:43:17Z", "path": "flink/src/test/java/org/apache/iceberg/flink/MiniClusterBase.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.configuration.CoreOptions;\n+import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration;\n+import org.apache.flink.test.util.MiniClusterWithClientResource;\n+import org.apache.flink.test.util.TestBaseUtils;\n+import org.junit.ClassRule;\n+import org.junit.rules.TemporaryFolder;\n+\n+/**\n+ * It will start a mini cluster with classloader.check-leaked-classloader=false, so that we won't break the unit tests\n+ * because of the class loader leak issue. In our iceberg integration tests, there're some that will assert the results\n+ * after finished the flink jobs, so actually we may access the class loader that has been closed by the flink task\n+ * managers if we enable the switch classloader.check-leaked-classloader by default.\n+ */\n+public class MiniClusterBase extends TestBaseUtils {\n+\n+  private static final int DEFAULT_PARALLELISM = 4;\n+\n+  public static final Configuration CONFIG = new Configuration()\n+      // disable classloader check as Avro may cache class/object in the serializers.\n+      .set(CoreOptions.CHECK_LEAKED_CLASSLOADER, false);\n+\n+  @ClassRule\n+  public static MiniClusterWithClientResource miniClusterResource = new MiniClusterWithClientResource(", "originalCommit": "b05c00e1a311396400e277bdb07d024e5fb0781f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODI4NTkxMQ==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r568285911", "bodyText": "That is a great idea. will update", "author": "stevenzwu", "createdAt": "2021-02-02T02:48:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODI4NDA5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODI4NDUwMA==", "url": "https://github.com/apache/iceberg/pull/1956#discussion_r568284500", "bodyText": "Thanks a lot for this.", "author": "openinx", "createdAt": "2021-02-02T02:44:39Z", "path": "flink/src/main/java/org/apache/iceberg/flink/actions/Actions.java", "diffHunk": "@@ -37,7 +43,7 @@ public static Actions forTable(StreamExecutionEnvironment env, Table table) {\n   }\n \n   public static Actions forTable(Table table) {\n-    return new Actions(StreamExecutionEnvironment.getExecutionEnvironment(), table);\n+    return new Actions(StreamExecutionEnvironment.getExecutionEnvironment(CONFIG), table);", "originalCommit": "b05c00e1a311396400e277bdb07d024e5fb0781f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b5ce94825e5cea71089c667568f314ef122ca8d3", "url": "https://github.com/apache/iceberg/commit/b5ce94825e5cea71089c667568f314ef122ca8d3", "message": "Address latest review comments", "committedDate": "2021-02-02T04:54:24Z", "type": "commit"}]}