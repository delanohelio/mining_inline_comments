{"pr_number": 1064, "pr_title": "Add DeleteFile and manifest reader and writer for deletes", "pr_createdAt": "2020-05-25T21:57:51Z", "pr_url": "https://github.com/apache/iceberg/pull/1064", "timeline": [{"oid": "1152bc110d7b69741f4545ac04c2d9fe40584a22", "url": "https://github.com/apache/iceberg/commit/1152bc110d7b69741f4545ac04c2d9fe40584a22", "message": "Extract ContentFile from DataFile and add type param to ManifestEntry.", "committedDate": "2020-05-25T23:04:38Z", "type": "forcePushed"}, {"oid": "3dd65213bea1513bae7753c9873df7f109d1ca82", "url": "https://github.com/apache/iceberg/commit/3dd65213bea1513bae7753c9873df7f109d1ca82", "message": "Extract ContentFile from DataFile and add type param to ManifestEntry.", "committedDate": "2020-05-25T23:06:44Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjMyNTg4NA==", "url": "https://github.com/apache/iceberg/pull/1064#discussion_r432325884", "bodyText": "I see the comment says that the BaseFile is the base class for DataFile and DeleteFile,  is it suitable to make the FileContent use FileContent.DATA by default ?  Just curious.", "author": "openinx", "createdAt": "2020-05-29T08:13:16Z", "path": "core/src/main/java/org/apache/iceberg/BaseFile.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.base.MoreObjects;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.avro.specific.SpecificData;\n+import org.apache.iceberg.avro.AvroSchemaUtil;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+\n+/**\n+ * Base class for both {@link DataFile} and {@link DeleteFile}.\n+ */\n+abstract class BaseFile<F>\n+    implements ContentFile<F>, IndexedRecord, StructLike, SpecificData.SchemaConstructable, Serializable {\n+  static final Types.StructType EMPTY_STRUCT_TYPE = Types.StructType.of();\n+  static final PartitionData EMPTY_PARTITION_DATA = new PartitionData(EMPTY_STRUCT_TYPE) {\n+    @Override\n+    public PartitionData copy() {\n+      return this; // this does not change\n+    }\n+  };\n+\n+  private int[] fromProjectionPos;\n+  private Types.StructType partitionType;\n+\n+  private FileContent content = FileContent.DATA;", "originalCommit": "9183fa21cf05db9a8f0bc78a94d0ec23358a8856", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4MDAxNg==", "url": "https://github.com/apache/iceberg/pull/1064#discussion_r432580016", "bodyText": "I am also curious whether we can rely on content() defined in ContentFile as both DeleteFile and DataFile override that.", "author": "aokolnychyi", "createdAt": "2020-05-29T15:54:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjMyNTg4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjY0Mjg4OA==", "url": "https://github.com/apache/iceberg/pull/1064#discussion_r432642888", "bodyText": "The FileContent for DeleteFile could be either POSITION_DELETES or EQUALITY_DELETES so we need to store it. DataFile always overrides it though.", "author": "rdblue", "createdAt": "2020-05-29T17:47:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjMyNTg4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjY0Mjk2Ng==", "url": "https://github.com/apache/iceberg/pull/1064#discussion_r432642966", "bodyText": "We actually need a field as DeleteFiles can be either positional or equality.", "author": "aokolnychyi", "createdAt": "2020-05-29T17:47:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjMyNTg4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjMzMDIyOQ==", "url": "https://github.com/apache/iceberg/pull/1064#discussion_r432330229", "bodyText": "I guess we should return this.content.id() here ?", "author": "openinx", "createdAt": "2020-05-29T08:21:45Z", "path": "core/src/main/java/org/apache/iceberg/BaseFile.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.base.MoreObjects;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.avro.specific.SpecificData;\n+import org.apache.iceberg.avro.AvroSchemaUtil;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+\n+/**\n+ * Base class for both {@link DataFile} and {@link DeleteFile}.\n+ */\n+abstract class BaseFile<F>\n+    implements ContentFile<F>, IndexedRecord, StructLike, SpecificData.SchemaConstructable, Serializable {\n+  static final Types.StructType EMPTY_STRUCT_TYPE = Types.StructType.of();\n+  static final PartitionData EMPTY_PARTITION_DATA = new PartitionData(EMPTY_STRUCT_TYPE) {\n+    @Override\n+    public PartitionData copy() {\n+      return this; // this does not change\n+    }\n+  };\n+\n+  private int[] fromProjectionPos;\n+  private Types.StructType partitionType;\n+\n+  private FileContent content = FileContent.DATA;\n+  private String filePath = null;\n+  private FileFormat format = null;\n+  private PartitionData partitionData = null;\n+  private Long recordCount = null;\n+  private long fileSizeInBytes = -1L;\n+\n+  // optional fields\n+  private Map<Integer, Long> columnSizes = null;\n+  private Map<Integer, Long> valueCounts = null;\n+  private Map<Integer, Long> nullValueCounts = null;\n+  private Map<Integer, ByteBuffer> lowerBounds = null;\n+  private Map<Integer, ByteBuffer> upperBounds = null;\n+  private List<Long> splitOffsets = null;\n+  private byte[] keyMetadata = null;\n+\n+  // cached schema\n+  private transient org.apache.avro.Schema avroSchema = null;\n+\n+  /**\n+   * Used by Avro reflection to instantiate this class when reading manifest files.\n+   */\n+  BaseFile(org.apache.avro.Schema avroSchema) {\n+    this.avroSchema = avroSchema;\n+\n+    Types.StructType schema = AvroSchemaUtil.convert(avroSchema).asNestedType().asStructType();\n+\n+    // partition type may be null if the field was not projected\n+    Type partType = schema.fieldType(\"partition\");\n+    if (partType != null) {\n+      this.partitionType = partType.asNestedType().asStructType();\n+    } else {\n+      this.partitionType = EMPTY_STRUCT_TYPE;\n+    }\n+\n+    List<Types.NestedField> fields = schema.fields();\n+    List<Types.NestedField> allFields = DataFile.getType(partitionType).fields();\n+    this.fromProjectionPos = new int[fields.size()];\n+    for (int i = 0; i < fromProjectionPos.length; i += 1) {\n+      boolean found = false;\n+      for (int j = 0; j < allFields.size(); j += 1) {\n+        if (fields.get(i).fieldId() == allFields.get(j).fieldId()) {\n+          found = true;\n+          fromProjectionPos[i] = j;\n+        }\n+      }\n+\n+      if (!found) {\n+        throw new IllegalArgumentException(\"Cannot find projected field: \" + fields.get(i));\n+      }\n+    }\n+\n+    this.partitionData = new PartitionData(partitionType);\n+  }\n+\n+  BaseFile(FileContent content, String filePath, FileFormat format,\n+           PartitionData partition, long fileSizeInBytes, long recordCount,\n+           Map<Integer, Long> columnSizes, Map<Integer, Long> valueCounts, Map<Integer, Long> nullValueCounts,\n+           Map<Integer, ByteBuffer> lowerBounds, Map<Integer, ByteBuffer> upperBounds, List<Long> splitOffsets,\n+           ByteBuffer keyMetadata) {\n+    this.content = content;\n+    this.filePath = filePath;\n+    this.format = format;\n+\n+    // this constructor is used by DataFiles.Builder, which passes null for unpartitioned data\n+    if (partition == null) {\n+      this.partitionData = EMPTY_PARTITION_DATA;\n+      this.partitionType = EMPTY_PARTITION_DATA.getPartitionType();\n+    } else {\n+      this.partitionData = partition;\n+      this.partitionType = partition.getPartitionType();\n+    }\n+\n+    // this will throw NPE if metrics.recordCount is null\n+    this.recordCount = recordCount;\n+    this.fileSizeInBytes = fileSizeInBytes;\n+    this.columnSizes = columnSizes;\n+    this.valueCounts = valueCounts;\n+    this.nullValueCounts = nullValueCounts;\n+    this.lowerBounds = SerializableByteBufferMap.wrap(lowerBounds);\n+    this.upperBounds = SerializableByteBufferMap.wrap(upperBounds);\n+    this.splitOffsets = copy(splitOffsets);\n+    this.keyMetadata = ByteBuffers.toByteArray(keyMetadata);\n+  }\n+\n+  /**\n+   * Copy constructor.\n+   *\n+   * @param toCopy a generic data file to copy.\n+   * @param fullCopy whether to copy all fields or to drop column-level stats\n+   */\n+  BaseFile(BaseFile<F> toCopy, boolean fullCopy) {\n+    this.content = toCopy.content;\n+    this.filePath = toCopy.filePath;\n+    this.format = toCopy.format;\n+    this.partitionData = toCopy.partitionData.copy();\n+    this.partitionType = toCopy.partitionType;\n+    this.recordCount = toCopy.recordCount;\n+    this.fileSizeInBytes = toCopy.fileSizeInBytes;\n+    if (fullCopy) {\n+      // TODO: support lazy conversion to/from map\n+      this.columnSizes = copy(toCopy.columnSizes);\n+      this.valueCounts = copy(toCopy.valueCounts);\n+      this.nullValueCounts = copy(toCopy.nullValueCounts);\n+      this.lowerBounds = SerializableByteBufferMap.wrap(copy(toCopy.lowerBounds));\n+      this.upperBounds = SerializableByteBufferMap.wrap(copy(toCopy.upperBounds));\n+    } else {\n+      this.columnSizes = null;\n+      this.valueCounts = null;\n+      this.nullValueCounts = null;\n+      this.lowerBounds = null;\n+      this.upperBounds = null;\n+    }\n+    this.fromProjectionPos = toCopy.fromProjectionPos;\n+    this.keyMetadata = toCopy.keyMetadata == null ? null : Arrays.copyOf(toCopy.keyMetadata, toCopy.keyMetadata.length);\n+    this.splitOffsets = copy(toCopy.splitOffsets);\n+  }\n+\n+  /**\n+   * Constructor for Java serialization.\n+   */\n+  BaseFile() {\n+  }\n+\n+  @Override\n+  public org.apache.avro.Schema getSchema() {\n+    if (avroSchema == null) {\n+      this.avroSchema = getAvroSchema(partitionType);\n+    }\n+    return avroSchema;\n+  }\n+\n+  @Override\n+  @SuppressWarnings(\"unchecked\")\n+  public void put(int i, Object value) {\n+    int pos = i;\n+    // if the schema was projected, map the incoming ordinal to the expected one\n+    if (fromProjectionPos != null) {\n+      pos = fromProjectionPos[i];\n+    }\n+    switch (pos) {\n+      case 0:\n+        this.content = value != null ? FileContent.values()[(Integer) value] : FileContent.DATA;\n+        return;\n+      case 1:\n+        // always coerce to String for Serializable\n+        this.filePath = value.toString();\n+        return;\n+      case 2:\n+        this.format = FileFormat.valueOf(value.toString());\n+        return;\n+      case 3:\n+        this.partitionData = (PartitionData) value;\n+        return;\n+      case 4:\n+        this.recordCount = (Long) value;\n+        return;\n+      case 5:\n+        this.fileSizeInBytes = (Long) value;\n+        return;\n+      case 6:\n+        this.columnSizes = (Map<Integer, Long>) value;\n+        return;\n+      case 7:\n+        this.valueCounts = (Map<Integer, Long>) value;\n+        return;\n+      case 8:\n+        this.nullValueCounts = (Map<Integer, Long>) value;\n+        return;\n+      case 9:\n+        this.lowerBounds = SerializableByteBufferMap.wrap((Map<Integer, ByteBuffer>) value);\n+        return;\n+      case 10:\n+        this.upperBounds = SerializableByteBufferMap.wrap((Map<Integer, ByteBuffer>) value);\n+        return;\n+      case 11:\n+        this.keyMetadata = ByteBuffers.toByteArray((ByteBuffer) value);\n+        return;\n+      case 12:\n+        this.splitOffsets = (List<Long>) value;\n+        return;\n+      default:\n+        // ignore the object, it must be from a newer version of the format\n+    }\n+  }\n+\n+  @Override\n+  public <T> void set(int pos, T value) {\n+    put(pos, value);\n+  }\n+\n+  @Override\n+  public Object get(int i) {\n+    int pos = i;\n+    // if the schema was projected, map the incoming ordinal to the expected one\n+    if (fromProjectionPos != null) {\n+      pos = fromProjectionPos[i];\n+    }\n+    switch (pos) {\n+      case 0:\n+        return FileContent.DATA.id();", "originalCommit": "9183fa21cf05db9a8f0bc78a94d0ec23358a8856", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjYzOTkzNQ==", "url": "https://github.com/apache/iceberg/pull/1064#discussion_r432639935", "bodyText": "This is only used on read to get a container to reuse, so it doesn't affect correctness. Writes all go through the wrappers, which is why tests pass. I'll fix this to avoid confusion.", "author": "rdblue", "createdAt": "2020-05-29T17:41:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjMzMDIyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjMzMTUyNA==", "url": "https://github.com/apache/iceberg/pull/1064#discussion_r432331524", "bodyText": "Missing a FileContent field here I guess.", "author": "openinx", "createdAt": "2020-05-29T08:24:06Z", "path": "core/src/main/java/org/apache/iceberg/BaseFile.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.base.MoreObjects;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.avro.specific.SpecificData;\n+import org.apache.iceberg.avro.AvroSchemaUtil;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+\n+/**\n+ * Base class for both {@link DataFile} and {@link DeleteFile}.\n+ */\n+abstract class BaseFile<F>\n+    implements ContentFile<F>, IndexedRecord, StructLike, SpecificData.SchemaConstructable, Serializable {\n+  static final Types.StructType EMPTY_STRUCT_TYPE = Types.StructType.of();\n+  static final PartitionData EMPTY_PARTITION_DATA = new PartitionData(EMPTY_STRUCT_TYPE) {\n+    @Override\n+    public PartitionData copy() {\n+      return this; // this does not change\n+    }\n+  };\n+\n+  private int[] fromProjectionPos;\n+  private Types.StructType partitionType;\n+\n+  private FileContent content = FileContent.DATA;\n+  private String filePath = null;\n+  private FileFormat format = null;\n+  private PartitionData partitionData = null;\n+  private Long recordCount = null;\n+  private long fileSizeInBytes = -1L;\n+\n+  // optional fields\n+  private Map<Integer, Long> columnSizes = null;\n+  private Map<Integer, Long> valueCounts = null;\n+  private Map<Integer, Long> nullValueCounts = null;\n+  private Map<Integer, ByteBuffer> lowerBounds = null;\n+  private Map<Integer, ByteBuffer> upperBounds = null;\n+  private List<Long> splitOffsets = null;\n+  private byte[] keyMetadata = null;\n+\n+  // cached schema\n+  private transient org.apache.avro.Schema avroSchema = null;\n+\n+  /**\n+   * Used by Avro reflection to instantiate this class when reading manifest files.\n+   */\n+  BaseFile(org.apache.avro.Schema avroSchema) {\n+    this.avroSchema = avroSchema;\n+\n+    Types.StructType schema = AvroSchemaUtil.convert(avroSchema).asNestedType().asStructType();\n+\n+    // partition type may be null if the field was not projected\n+    Type partType = schema.fieldType(\"partition\");\n+    if (partType != null) {\n+      this.partitionType = partType.asNestedType().asStructType();\n+    } else {\n+      this.partitionType = EMPTY_STRUCT_TYPE;\n+    }\n+\n+    List<Types.NestedField> fields = schema.fields();\n+    List<Types.NestedField> allFields = DataFile.getType(partitionType).fields();\n+    this.fromProjectionPos = new int[fields.size()];\n+    for (int i = 0; i < fromProjectionPos.length; i += 1) {\n+      boolean found = false;\n+      for (int j = 0; j < allFields.size(); j += 1) {\n+        if (fields.get(i).fieldId() == allFields.get(j).fieldId()) {\n+          found = true;\n+          fromProjectionPos[i] = j;\n+        }\n+      }\n+\n+      if (!found) {\n+        throw new IllegalArgumentException(\"Cannot find projected field: \" + fields.get(i));\n+      }\n+    }\n+\n+    this.partitionData = new PartitionData(partitionType);\n+  }\n+\n+  BaseFile(FileContent content, String filePath, FileFormat format,\n+           PartitionData partition, long fileSizeInBytes, long recordCount,\n+           Map<Integer, Long> columnSizes, Map<Integer, Long> valueCounts, Map<Integer, Long> nullValueCounts,\n+           Map<Integer, ByteBuffer> lowerBounds, Map<Integer, ByteBuffer> upperBounds, List<Long> splitOffsets,\n+           ByteBuffer keyMetadata) {\n+    this.content = content;\n+    this.filePath = filePath;\n+    this.format = format;\n+\n+    // this constructor is used by DataFiles.Builder, which passes null for unpartitioned data\n+    if (partition == null) {\n+      this.partitionData = EMPTY_PARTITION_DATA;\n+      this.partitionType = EMPTY_PARTITION_DATA.getPartitionType();\n+    } else {\n+      this.partitionData = partition;\n+      this.partitionType = partition.getPartitionType();\n+    }\n+\n+    // this will throw NPE if metrics.recordCount is null\n+    this.recordCount = recordCount;\n+    this.fileSizeInBytes = fileSizeInBytes;\n+    this.columnSizes = columnSizes;\n+    this.valueCounts = valueCounts;\n+    this.nullValueCounts = nullValueCounts;\n+    this.lowerBounds = SerializableByteBufferMap.wrap(lowerBounds);\n+    this.upperBounds = SerializableByteBufferMap.wrap(upperBounds);\n+    this.splitOffsets = copy(splitOffsets);\n+    this.keyMetadata = ByteBuffers.toByteArray(keyMetadata);\n+  }\n+\n+  /**\n+   * Copy constructor.\n+   *\n+   * @param toCopy a generic data file to copy.\n+   * @param fullCopy whether to copy all fields or to drop column-level stats\n+   */\n+  BaseFile(BaseFile<F> toCopy, boolean fullCopy) {\n+    this.content = toCopy.content;\n+    this.filePath = toCopy.filePath;\n+    this.format = toCopy.format;\n+    this.partitionData = toCopy.partitionData.copy();\n+    this.partitionType = toCopy.partitionType;\n+    this.recordCount = toCopy.recordCount;\n+    this.fileSizeInBytes = toCopy.fileSizeInBytes;\n+    if (fullCopy) {\n+      // TODO: support lazy conversion to/from map\n+      this.columnSizes = copy(toCopy.columnSizes);\n+      this.valueCounts = copy(toCopy.valueCounts);\n+      this.nullValueCounts = copy(toCopy.nullValueCounts);\n+      this.lowerBounds = SerializableByteBufferMap.wrap(copy(toCopy.lowerBounds));\n+      this.upperBounds = SerializableByteBufferMap.wrap(copy(toCopy.upperBounds));\n+    } else {\n+      this.columnSizes = null;\n+      this.valueCounts = null;\n+      this.nullValueCounts = null;\n+      this.lowerBounds = null;\n+      this.upperBounds = null;\n+    }\n+    this.fromProjectionPos = toCopy.fromProjectionPos;\n+    this.keyMetadata = toCopy.keyMetadata == null ? null : Arrays.copyOf(toCopy.keyMetadata, toCopy.keyMetadata.length);\n+    this.splitOffsets = copy(toCopy.splitOffsets);\n+  }\n+\n+  /**\n+   * Constructor for Java serialization.\n+   */\n+  BaseFile() {\n+  }\n+\n+  @Override\n+  public org.apache.avro.Schema getSchema() {\n+    if (avroSchema == null) {\n+      this.avroSchema = getAvroSchema(partitionType);\n+    }\n+    return avroSchema;\n+  }\n+\n+  @Override\n+  @SuppressWarnings(\"unchecked\")\n+  public void put(int i, Object value) {\n+    int pos = i;\n+    // if the schema was projected, map the incoming ordinal to the expected one\n+    if (fromProjectionPos != null) {\n+      pos = fromProjectionPos[i];\n+    }\n+    switch (pos) {\n+      case 0:\n+        this.content = value != null ? FileContent.values()[(Integer) value] : FileContent.DATA;\n+        return;\n+      case 1:\n+        // always coerce to String for Serializable\n+        this.filePath = value.toString();\n+        return;\n+      case 2:\n+        this.format = FileFormat.valueOf(value.toString());\n+        return;\n+      case 3:\n+        this.partitionData = (PartitionData) value;\n+        return;\n+      case 4:\n+        this.recordCount = (Long) value;\n+        return;\n+      case 5:\n+        this.fileSizeInBytes = (Long) value;\n+        return;\n+      case 6:\n+        this.columnSizes = (Map<Integer, Long>) value;\n+        return;\n+      case 7:\n+        this.valueCounts = (Map<Integer, Long>) value;\n+        return;\n+      case 8:\n+        this.nullValueCounts = (Map<Integer, Long>) value;\n+        return;\n+      case 9:\n+        this.lowerBounds = SerializableByteBufferMap.wrap((Map<Integer, ByteBuffer>) value);\n+        return;\n+      case 10:\n+        this.upperBounds = SerializableByteBufferMap.wrap((Map<Integer, ByteBuffer>) value);\n+        return;\n+      case 11:\n+        this.keyMetadata = ByteBuffers.toByteArray((ByteBuffer) value);\n+        return;\n+      case 12:\n+        this.splitOffsets = (List<Long>) value;\n+        return;\n+      default:\n+        // ignore the object, it must be from a newer version of the format\n+    }\n+  }\n+\n+  @Override\n+  public <T> void set(int pos, T value) {\n+    put(pos, value);\n+  }\n+\n+  @Override\n+  public Object get(int i) {\n+    int pos = i;\n+    // if the schema was projected, map the incoming ordinal to the expected one\n+    if (fromProjectionPos != null) {\n+      pos = fromProjectionPos[i];\n+    }\n+    switch (pos) {\n+      case 0:\n+        return FileContent.DATA.id();\n+      case 1:\n+        return filePath;\n+      case 2:\n+        return format != null ? format.toString() : null;\n+      case 3:\n+        return partitionData;\n+      case 4:\n+        return recordCount;\n+      case 5:\n+        return fileSizeInBytes;\n+      case 6:\n+        return columnSizes;\n+      case 7:\n+        return valueCounts;\n+      case 8:\n+        return nullValueCounts;\n+      case 9:\n+        return lowerBounds;\n+      case 10:\n+        return upperBounds;\n+      case 11:\n+        return keyMetadata != null ? ByteBuffer.wrap(keyMetadata) : null;\n+      case 12:\n+        return splitOffsets;\n+      default:\n+        throw new UnsupportedOperationException(\"Unknown field ordinal: \" + pos);\n+    }\n+  }\n+\n+  @Override\n+  public <T> T get(int pos, Class<T> javaClass) {\n+    return javaClass.cast(get(pos));\n+  }\n+\n+  @Override\n+  public int size() {\n+    return DataFile.getType(EMPTY_STRUCT_TYPE).fields().size();\n+  }\n+\n+  public FileContent content() {\n+    return content;\n+  }\n+\n+  public CharSequence path() {\n+    return filePath;\n+  }\n+\n+  public FileFormat format() {\n+    return format;\n+  }\n+\n+  public StructLike partition() {\n+    return partitionData;\n+  }\n+\n+  public long recordCount() {\n+    return recordCount;\n+  }\n+\n+  public long fileSizeInBytes() {\n+    return fileSizeInBytes;\n+  }\n+\n+  public Map<Integer, Long> columnSizes() {\n+    return columnSizes;\n+  }\n+\n+  public Map<Integer, Long> valueCounts() {\n+    return valueCounts;\n+  }\n+\n+  public Map<Integer, Long> nullValueCounts() {\n+    return nullValueCounts;\n+  }\n+\n+  public Map<Integer, ByteBuffer> lowerBounds() {\n+    return lowerBounds;\n+  }\n+\n+  public Map<Integer, ByteBuffer> upperBounds() {\n+    return upperBounds;\n+  }\n+\n+  public ByteBuffer keyMetadata() {\n+    return keyMetadata != null ? ByteBuffer.wrap(keyMetadata) : null;\n+  }\n+\n+  public List<Long> splitOffsets() {\n+    return splitOffsets;\n+  }\n+\n+  private static <K, V> Map<K, V> copy(Map<K, V> map) {\n+    if (map != null) {\n+      Map<K, V> copy = Maps.newHashMapWithExpectedSize(map.size());\n+      copy.putAll(map);\n+      return Collections.unmodifiableMap(copy);\n+    }\n+    return null;\n+  }\n+\n+  private static <E> List<E> copy(List<E> list) {\n+    if (list != null) {\n+      List<E> copy = Lists.newArrayListWithExpectedSize(list.size());\n+      copy.addAll(list);\n+      return Collections.unmodifiableList(copy);\n+    }\n+    return null;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return MoreObjects.toStringHelper(this)\n+        .add(\"file_path\", filePath)", "originalCommit": "9183fa21cf05db9a8f0bc78a94d0ec23358a8856", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjY2ODc1MA==", "url": "https://github.com/apache/iceberg/pull/1064#discussion_r432668750", "bodyText": "Added.", "author": "rdblue", "createdAt": "2020-05-29T18:40:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjMzMTUyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjMzMjM5OA==", "url": "https://github.com/apache/iceberg/pull/1064#discussion_r432332398", "bodyText": "Nice ..", "author": "openinx", "createdAt": "2020-05-29T08:25:43Z", "path": "core/src/main/java/org/apache/iceberg/BaseManifestReader.java", "diffHunk": "@@ -47,17 +47,18 @@\n /**\n  * Base reader for data and delete manifest files.\n  *\n- * @param <T> The Java class of files returned by this reader.\n+ * @param <F> The Java class of files returned by this reader.\n  * @param <ThisT> The Java class of this reader, returned by configuration methods.\n  */\n-abstract class BaseManifestReader<T, ThisT> extends CloseableGroup implements CloseableIterable<T> {\n+abstract class BaseManifestReader<F extends ContentFile<F>, ThisT>\n+    extends CloseableGroup implements CloseableIterable<F> {\n   static final ImmutableList<String> ALL_COLUMNS = ImmutableList.of(\"*\");\n   private static final Set<String> STATS_COLUMNS = Sets.newHashSet(\n       \"value_counts\", \"null_value_counts\", \"lower_bounds\", \"upper_bounds\");\n \n   protected enum FileType {\n     DATA_FILES(GenericDataFile.class.getName()),\n-    DELETE_FILES(\"...\");\n+    DELETE_FILES(GenericDeleteFile.class.getName());", "originalCommit": "9183fa21cf05db9a8f0bc78a94d0ec23358a8856", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjMzODYzNw==", "url": "https://github.com/apache/iceberg/pull/1064#discussion_r432338637", "bodyText": "For my understanding,  the DATA manifest & DELETE manifest could share the same read / write path so I think we could use the common reader+writer.  Is there any other reason that we need to make them separate paths ?", "author": "openinx", "createdAt": "2020-05-29T08:37:22Z", "path": "core/src/main/java/org/apache/iceberg/ManifestFiles.java", "diffHunk": "@@ -55,6 +55,8 @@ public static ManifestReader read(ManifestFile manifest, FileIO io) {\n    * @return a {@link ManifestReader}\n    */\n   public static ManifestReader read(ManifestFile manifest, FileIO io, Map<Integer, PartitionSpec> specsById) {\n+    Preconditions.checkArgument(manifest.content() == ManifestContent.DATA,", "originalCommit": "9183fa21cf05db9a8f0bc78a94d0ec23358a8856", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjY0MTU1MA==", "url": "https://github.com/apache/iceberg/pull/1064#discussion_r432641550", "bodyText": "They have the same schema to keep the format simpler, but in the APIs we want to keep them separate by using DataFile and DeleteFile types. The readers and writers mostly share the same code, but are separate so that they can use the correct file type interface.", "author": "rdblue", "createdAt": "2020-05-29T17:44:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjMzODYzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4MzUzOQ==", "url": "https://github.com/apache/iceberg/pull/1064#discussion_r432583539", "bodyText": "Do we keep GenericDataFile on purpose here?", "author": "aokolnychyi", "createdAt": "2020-05-29T16:00:13Z", "path": "core/src/main/java/org/apache/iceberg/BaseFile.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.base.MoreObjects;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.avro.specific.SpecificData;\n+import org.apache.iceberg.avro.AvroSchemaUtil;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+\n+/**\n+ * Base class for both {@link DataFile} and {@link DeleteFile}.\n+ */\n+abstract class BaseFile<F>\n+    implements ContentFile<F>, IndexedRecord, StructLike, SpecificData.SchemaConstructable, Serializable {\n+  static final Types.StructType EMPTY_STRUCT_TYPE = Types.StructType.of();\n+  static final PartitionData EMPTY_PARTITION_DATA = new PartitionData(EMPTY_STRUCT_TYPE) {\n+    @Override\n+    public PartitionData copy() {\n+      return this; // this does not change\n+    }\n+  };\n+\n+  private int[] fromProjectionPos;\n+  private Types.StructType partitionType;\n+\n+  private FileContent content = FileContent.DATA;\n+  private String filePath = null;\n+  private FileFormat format = null;\n+  private PartitionData partitionData = null;\n+  private Long recordCount = null;\n+  private long fileSizeInBytes = -1L;\n+\n+  // optional fields\n+  private Map<Integer, Long> columnSizes = null;\n+  private Map<Integer, Long> valueCounts = null;\n+  private Map<Integer, Long> nullValueCounts = null;\n+  private Map<Integer, ByteBuffer> lowerBounds = null;\n+  private Map<Integer, ByteBuffer> upperBounds = null;\n+  private List<Long> splitOffsets = null;\n+  private byte[] keyMetadata = null;\n+\n+  // cached schema\n+  private transient org.apache.avro.Schema avroSchema = null;\n+\n+  /**\n+   * Used by Avro reflection to instantiate this class when reading manifest files.\n+   */\n+  BaseFile(org.apache.avro.Schema avroSchema) {\n+    this.avroSchema = avroSchema;\n+\n+    Types.StructType schema = AvroSchemaUtil.convert(avroSchema).asNestedType().asStructType();\n+\n+    // partition type may be null if the field was not projected\n+    Type partType = schema.fieldType(\"partition\");\n+    if (partType != null) {\n+      this.partitionType = partType.asNestedType().asStructType();\n+    } else {\n+      this.partitionType = EMPTY_STRUCT_TYPE;\n+    }\n+\n+    List<Types.NestedField> fields = schema.fields();\n+    List<Types.NestedField> allFields = DataFile.getType(partitionType).fields();\n+    this.fromProjectionPos = new int[fields.size()];\n+    for (int i = 0; i < fromProjectionPos.length; i += 1) {\n+      boolean found = false;\n+      for (int j = 0; j < allFields.size(); j += 1) {\n+        if (fields.get(i).fieldId() == allFields.get(j).fieldId()) {\n+          found = true;\n+          fromProjectionPos[i] = j;\n+        }\n+      }\n+\n+      if (!found) {\n+        throw new IllegalArgumentException(\"Cannot find projected field: \" + fields.get(i));\n+      }\n+    }\n+\n+    this.partitionData = new PartitionData(partitionType);\n+  }\n+\n+  BaseFile(FileContent content, String filePath, FileFormat format,\n+           PartitionData partition, long fileSizeInBytes, long recordCount,\n+           Map<Integer, Long> columnSizes, Map<Integer, Long> valueCounts, Map<Integer, Long> nullValueCounts,\n+           Map<Integer, ByteBuffer> lowerBounds, Map<Integer, ByteBuffer> upperBounds, List<Long> splitOffsets,\n+           ByteBuffer keyMetadata) {\n+    this.content = content;\n+    this.filePath = filePath;\n+    this.format = format;\n+\n+    // this constructor is used by DataFiles.Builder, which passes null for unpartitioned data\n+    if (partition == null) {\n+      this.partitionData = EMPTY_PARTITION_DATA;\n+      this.partitionType = EMPTY_PARTITION_DATA.getPartitionType();\n+    } else {\n+      this.partitionData = partition;\n+      this.partitionType = partition.getPartitionType();\n+    }\n+\n+    // this will throw NPE if metrics.recordCount is null\n+    this.recordCount = recordCount;\n+    this.fileSizeInBytes = fileSizeInBytes;\n+    this.columnSizes = columnSizes;\n+    this.valueCounts = valueCounts;\n+    this.nullValueCounts = nullValueCounts;\n+    this.lowerBounds = SerializableByteBufferMap.wrap(lowerBounds);\n+    this.upperBounds = SerializableByteBufferMap.wrap(upperBounds);\n+    this.splitOffsets = copy(splitOffsets);\n+    this.keyMetadata = ByteBuffers.toByteArray(keyMetadata);\n+  }\n+\n+  /**\n+   * Copy constructor.\n+   *\n+   * @param toCopy a generic data file to copy.\n+   * @param fullCopy whether to copy all fields or to drop column-level stats\n+   */\n+  BaseFile(BaseFile<F> toCopy, boolean fullCopy) {\n+    this.content = toCopy.content;\n+    this.filePath = toCopy.filePath;\n+    this.format = toCopy.format;\n+    this.partitionData = toCopy.partitionData.copy();\n+    this.partitionType = toCopy.partitionType;\n+    this.recordCount = toCopy.recordCount;\n+    this.fileSizeInBytes = toCopy.fileSizeInBytes;\n+    if (fullCopy) {\n+      // TODO: support lazy conversion to/from map\n+      this.columnSizes = copy(toCopy.columnSizes);\n+      this.valueCounts = copy(toCopy.valueCounts);\n+      this.nullValueCounts = copy(toCopy.nullValueCounts);\n+      this.lowerBounds = SerializableByteBufferMap.wrap(copy(toCopy.lowerBounds));\n+      this.upperBounds = SerializableByteBufferMap.wrap(copy(toCopy.upperBounds));\n+    } else {\n+      this.columnSizes = null;\n+      this.valueCounts = null;\n+      this.nullValueCounts = null;\n+      this.lowerBounds = null;\n+      this.upperBounds = null;\n+    }\n+    this.fromProjectionPos = toCopy.fromProjectionPos;\n+    this.keyMetadata = toCopy.keyMetadata == null ? null : Arrays.copyOf(toCopy.keyMetadata, toCopy.keyMetadata.length);\n+    this.splitOffsets = copy(toCopy.splitOffsets);\n+  }\n+\n+  /**\n+   * Constructor for Java serialization.\n+   */\n+  BaseFile() {\n+  }\n+\n+  @Override\n+  public org.apache.avro.Schema getSchema() {\n+    if (avroSchema == null) {\n+      this.avroSchema = getAvroSchema(partitionType);\n+    }\n+    return avroSchema;\n+  }\n+\n+  @Override\n+  @SuppressWarnings(\"unchecked\")\n+  public void put(int i, Object value) {\n+    int pos = i;\n+    // if the schema was projected, map the incoming ordinal to the expected one\n+    if (fromProjectionPos != null) {\n+      pos = fromProjectionPos[i];\n+    }\n+    switch (pos) {\n+      case 0:\n+        this.content = value != null ? FileContent.values()[(Integer) value] : FileContent.DATA;\n+        return;\n+      case 1:\n+        // always coerce to String for Serializable\n+        this.filePath = value.toString();\n+        return;\n+      case 2:\n+        this.format = FileFormat.valueOf(value.toString());\n+        return;\n+      case 3:\n+        this.partitionData = (PartitionData) value;\n+        return;\n+      case 4:\n+        this.recordCount = (Long) value;\n+        return;\n+      case 5:\n+        this.fileSizeInBytes = (Long) value;\n+        return;\n+      case 6:\n+        this.columnSizes = (Map<Integer, Long>) value;\n+        return;\n+      case 7:\n+        this.valueCounts = (Map<Integer, Long>) value;\n+        return;\n+      case 8:\n+        this.nullValueCounts = (Map<Integer, Long>) value;\n+        return;\n+      case 9:\n+        this.lowerBounds = SerializableByteBufferMap.wrap((Map<Integer, ByteBuffer>) value);\n+        return;\n+      case 10:\n+        this.upperBounds = SerializableByteBufferMap.wrap((Map<Integer, ByteBuffer>) value);\n+        return;\n+      case 11:\n+        this.keyMetadata = ByteBuffers.toByteArray((ByteBuffer) value);\n+        return;\n+      case 12:\n+        this.splitOffsets = (List<Long>) value;\n+        return;\n+      default:\n+        // ignore the object, it must be from a newer version of the format\n+    }\n+  }\n+\n+  @Override\n+  public <T> void set(int pos, T value) {\n+    put(pos, value);\n+  }\n+\n+  @Override\n+  public Object get(int i) {\n+    int pos = i;\n+    // if the schema was projected, map the incoming ordinal to the expected one\n+    if (fromProjectionPos != null) {\n+      pos = fromProjectionPos[i];\n+    }\n+    switch (pos) {\n+      case 0:\n+        return FileContent.DATA.id();\n+      case 1:\n+        return filePath;\n+      case 2:\n+        return format != null ? format.toString() : null;\n+      case 3:\n+        return partitionData;\n+      case 4:\n+        return recordCount;\n+      case 5:\n+        return fileSizeInBytes;\n+      case 6:\n+        return columnSizes;\n+      case 7:\n+        return valueCounts;\n+      case 8:\n+        return nullValueCounts;\n+      case 9:\n+        return lowerBounds;\n+      case 10:\n+        return upperBounds;\n+      case 11:\n+        return keyMetadata != null ? ByteBuffer.wrap(keyMetadata) : null;\n+      case 12:\n+        return splitOffsets;\n+      default:\n+        throw new UnsupportedOperationException(\"Unknown field ordinal: \" + pos);\n+    }\n+  }\n+\n+  @Override\n+  public <T> T get(int pos, Class<T> javaClass) {\n+    return javaClass.cast(get(pos));\n+  }\n+\n+  @Override\n+  public int size() {\n+    return DataFile.getType(EMPTY_STRUCT_TYPE).fields().size();\n+  }\n+\n+  public FileContent content() {\n+    return content;\n+  }\n+\n+  public CharSequence path() {\n+    return filePath;\n+  }\n+\n+  public FileFormat format() {\n+    return format;\n+  }\n+\n+  public StructLike partition() {\n+    return partitionData;\n+  }\n+\n+  public long recordCount() {\n+    return recordCount;\n+  }\n+\n+  public long fileSizeInBytes() {\n+    return fileSizeInBytes;\n+  }\n+\n+  public Map<Integer, Long> columnSizes() {\n+    return columnSizes;\n+  }\n+\n+  public Map<Integer, Long> valueCounts() {\n+    return valueCounts;\n+  }\n+\n+  public Map<Integer, Long> nullValueCounts() {\n+    return nullValueCounts;\n+  }\n+\n+  public Map<Integer, ByteBuffer> lowerBounds() {\n+    return lowerBounds;\n+  }\n+\n+  public Map<Integer, ByteBuffer> upperBounds() {\n+    return upperBounds;\n+  }\n+\n+  public ByteBuffer keyMetadata() {\n+    return keyMetadata != null ? ByteBuffer.wrap(keyMetadata) : null;\n+  }\n+\n+  public List<Long> splitOffsets() {\n+    return splitOffsets;\n+  }\n+\n+  private static <K, V> Map<K, V> copy(Map<K, V> map) {\n+    if (map != null) {\n+      Map<K, V> copy = Maps.newHashMapWithExpectedSize(map.size());\n+      copy.putAll(map);\n+      return Collections.unmodifiableMap(copy);\n+    }\n+    return null;\n+  }\n+\n+  private static <E> List<E> copy(List<E> list) {\n+    if (list != null) {\n+      List<E> copy = Lists.newArrayListWithExpectedSize(list.size());\n+      copy.addAll(list);\n+      return Collections.unmodifiableList(copy);\n+    }\n+    return null;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return MoreObjects.toStringHelper(this)\n+        .add(\"file_path\", filePath)\n+        .add(\"file_format\", format)\n+        .add(\"partition\", partitionData)\n+        .add(\"record_count\", recordCount)\n+        .add(\"file_size_in_bytes\", fileSizeInBytes)\n+        .add(\"column_sizes\", columnSizes)\n+        .add(\"value_counts\", valueCounts)\n+        .add(\"null_value_counts\", nullValueCounts)\n+        .add(\"lower_bounds\", lowerBounds)\n+        .add(\"upper_bounds\", upperBounds)\n+        .add(\"key_metadata\", keyMetadata == null ? \"null\" : \"(redacted)\")\n+        .add(\"split_offsets\", splitOffsets == null ? \"null\" : splitOffsets)\n+        .toString();\n+  }\n+\n+  private static org.apache.avro.Schema getAvroSchema(Types.StructType partitionType) {\n+    Types.StructType type = DataFile.getType(partitionType);\n+    return AvroSchemaUtil.convert(type, ImmutableMap.of(\n+        type, GenericDataFile.class.getName(),", "originalCommit": "9183fa21cf05db9a8f0bc78a94d0ec23358a8856", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjY0NzY3MA==", "url": "https://github.com/apache/iceberg/pull/1064#discussion_r432647670", "bodyText": "This is used by getSchema, which should probably be overridden. It doesn't matter for correctness, but it would be nice to use the right record name.", "author": "rdblue", "createdAt": "2020-05-29T17:56:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4MzUzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjY2ODM2Ng==", "url": "https://github.com/apache/iceberg/pull/1064#discussion_r432668366", "bodyText": "Moving this into subclasses.", "author": "rdblue", "createdAt": "2020-05-29T18:39:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4MzUzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4NjY2OQ==", "url": "https://github.com/apache/iceberg/pull/1064#discussion_r432586669", "bodyText": "nit: I think this can be on a single line now (i.e. iterator())", "author": "aokolnychyi", "createdAt": "2020-05-29T16:05:38Z", "path": "core/src/main/java/org/apache/iceberg/BaseManifestReader.java", "diffHunk": "@@ -218,13 +219,12 @@ public ThisT caseSensitive(boolean isCaseSensitive) {\n    * @return an Iterator of DataFile. Makes defensive copies of files before returning\n    */\n   @Override\n-  @SuppressWarnings(\"unchecked\")\n-  public CloseableIterator<T> iterator() {\n+  public CloseableIterator<F> iterator() {\n     if (dropStats(rowFilter, columns)) {\n-      return (CloseableIterator<T>) CloseableIterable.transform(liveEntries(), e -> e.file().copyWithoutStats())\n+      return CloseableIterable.transform(liveEntries(), e -> e.file().copyWithoutStats())", "originalCommit": "9183fa21cf05db9a8f0bc78a94d0ec23358a8856", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjY2ODYwMg==", "url": "https://github.com/apache/iceberg/pull/1064#discussion_r432668602", "bodyText": "Fixed.", "author": "rdblue", "createdAt": "2020-05-29T18:39:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4NjY2OQ=="}], "type": "inlineReview"}, {"oid": "5ed51eaec32559019754bb880b3094c71181e440", "url": "https://github.com/apache/iceberg/commit/5ed51eaec32559019754bb880b3094c71181e440", "message": "Split DataFile into DataFile and DeleteFile.", "committedDate": "2020-05-29T18:44:55Z", "type": "commit"}, {"oid": "6edd6022b227e71fb37b3a54ad087e3d74002e5c", "url": "https://github.com/apache/iceberg/commit/6edd6022b227e71fb37b3a54ad087e3d74002e5c", "message": "Fix FileType in BaseManifestReader.", "committedDate": "2020-05-29T18:44:55Z", "type": "commit"}, {"oid": "66535cd0ee97798aacced1f51ff7e0138ab25d33", "url": "https://github.com/apache/iceberg/commit/66535cd0ee97798aacced1f51ff7e0138ab25d33", "message": "Add DeleteManifestReader.", "committedDate": "2020-05-29T18:44:55Z", "type": "commit"}, {"oid": "ed2a15636b53c0064f2d623480729d999c0575b4", "url": "https://github.com/apache/iceberg/commit/ed2a15636b53c0064f2d623480729d999c0575b4", "message": "Add delete manifest writer.", "committedDate": "2020-05-29T18:44:55Z", "type": "commit"}, {"oid": "3d1ee652568a7e26dd31c9d9b46da694ef510970", "url": "https://github.com/apache/iceberg/commit/3d1ee652568a7e26dd31c9d9b46da694ef510970", "message": "Remove wrapper from ManifestEntry.", "committedDate": "2020-05-29T18:44:55Z", "type": "commit"}, {"oid": "0f4e0634d3b4dbba80892a4af66dfa7322c755f8", "url": "https://github.com/apache/iceberg/commit/0f4e0634d3b4dbba80892a4af66dfa7322c755f8", "message": "Extract ContentFile from DataFile and add type param to ManifestEntry.", "committedDate": "2020-05-29T18:44:55Z", "type": "commit"}, {"oid": "e45a0d781b19e0460d6f859097fa047061b8ce98", "url": "https://github.com/apache/iceberg/commit/e45a0d781b19e0460d6f859097fa047061b8ce98", "message": "Minor fixes.", "committedDate": "2020-05-29T18:44:55Z", "type": "commit"}, {"oid": "6a796c5bc02528d38084ee9cfbfcf44e567d28cb", "url": "https://github.com/apache/iceberg/commit/6a796c5bc02528d38084ee9cfbfcf44e567d28cb", "message": "Update for review comments.", "committedDate": "2020-05-29T18:44:55Z", "type": "commit"}, {"oid": "6a796c5bc02528d38084ee9cfbfcf44e567d28cb", "url": "https://github.com/apache/iceberg/commit/6a796c5bc02528d38084ee9cfbfcf44e567d28cb", "message": "Update for review comments.", "committedDate": "2020-05-29T18:44:55Z", "type": "forcePushed"}]}