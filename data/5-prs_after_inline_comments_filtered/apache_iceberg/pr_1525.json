{"pr_number": 1525, "pr_title": "Provide API and Implementation for Creating Iceberg Tables from Spark", "pr_createdAt": "2020-09-28T21:23:17Z", "pr_url": "https://github.com/apache/iceberg/pull/1525", "timeline": [{"oid": "ef12445542ab93764e54bea36f61fd9890d4b449", "url": "https://github.com/apache/iceberg/commit/ef12445542ab93764e54bea36f61fd9890d4b449", "message": "Refactor out Spark 3 Migration", "committedDate": "2020-12-09T16:51:06Z", "type": "commit"}, {"oid": "af9ed97fa4e0047f1dd6ae8b6d979d5f67447e2e", "url": "https://github.com/apache/iceberg/commit/af9ed97fa4e0047f1dd6ae8b6d979d5f67447e2e", "message": "Fixup from Rebase", "committedDate": "2020-12-09T16:51:06Z", "type": "commit"}, {"oid": "bd330688dd052df82f3f52dfdb10d667ec407127", "url": "https://github.com/apache/iceberg/commit/bd330688dd052df82f3f52dfdb10d667ec407127", "message": "Fix CheckStyle Errors", "committedDate": "2020-12-09T16:51:06Z", "type": "commit"}, {"oid": "f4e456ed4fdf42fba99dbc095a21d900b468c8df", "url": "https://github.com/apache/iceberg/commit/f4e456ed4fdf42fba99dbc095a21d900b468c8df", "message": "Add GC_ENABLED false Flag to Snapshot Tables", "committedDate": "2020-12-09T16:51:06Z", "type": "commit"}, {"oid": "a8898ccffa97466d16d0eede6303f52aa4644c7a", "url": "https://github.com/apache/iceberg/commit/a8898ccffa97466d16d0eede6303f52aa4644c7a", "message": "Fix Source Catalog Requirements", "committedDate": "2020-12-09T16:51:06Z", "type": "commit"}, {"oid": "f25e4731774e3ff1587a984fcfd067f434a49cfa", "url": "https://github.com/apache/iceberg/commit/f25e4731774e3ff1587a984fcfd067f434a49cfa", "message": "Various Reviewer Comments", "committedDate": "2020-12-09T16:51:06Z", "type": "commit"}, {"oid": "bf50967bb9ab9344b3c242c274cc9fab58c8b115", "url": "https://github.com/apache/iceberg/commit/bf50967bb9ab9344b3c242c274cc9fab58c8b115", "message": "Review Comments and Changes", "committedDate": "2020-12-09T19:34:36Z", "type": "commit"}, {"oid": "bf50967bb9ab9344b3c242c274cc9fab58c8b115", "url": "https://github.com/apache/iceberg/commit/bf50967bb9ab9344b3c242c274cc9fab58c8b115", "message": "Review Comments and Changes", "committedDate": "2020-12-09T19:34:36Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc2OTg4Mg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539769882", "bodyText": "Nit: indentation is off.", "author": "rdblue", "createdAt": "2020-12-10T01:17:47Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String LOCATION = \"location\";\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  protected static final List<String> EXCLUDED_PROPERTIES =\n+      ImmutableList.of(\"path\", \"transient_lastDdlTime\", \"serialization.format\");\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final TableCatalog sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private final Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                     CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) this.sourceCatalog.loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected TableCatalog sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableIdent() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableIdent() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private void validateSourceTable() {\n+    String sourceTableProvider = sourceCatalogTable.provider().get().toLowerCase(Locale.ROOT);\n+    Preconditions.checkArgument(ALLOWED_SOURCES.contains(sourceTableProvider),\n+          \"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider);", "originalCommit": "bf50967bb9ab9344b3c242c274cc9fab58c8b115", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc3MDM3Ng==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539770376", "bodyText": "I thought this was going to be changed to the Iceberg NoSuchNamespaceException? Same with the other one. Can we throw the right Iceberg exception so that callers can catch them?", "author": "rdblue", "createdAt": "2020-12-10T01:19:25Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String LOCATION = \"location\";\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  protected static final List<String> EXCLUDED_PROPERTIES =\n+      ImmutableList.of(\"path\", \"transient_lastDdlTime\", \"serialization.format\");\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final TableCatalog sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private final Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                     CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) this.sourceCatalog.loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected TableCatalog sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableIdent() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableIdent() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private void validateSourceTable() {\n+    String sourceTableProvider = sourceCatalogTable.provider().get().toLowerCase(Locale.ROOT);\n+    Preconditions.checkArgument(ALLOWED_SOURCES.contains(sourceTableProvider),\n+          \"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider);\n+    Preconditions.checkArgument(!sourceCatalogTable.storage().locationUri().isEmpty(),\n+        \"Cannot create an Iceberg table from a source without an explicit location\");\n+  }\n+\n+  private StagingTableCatalog checkDestinationCatalog(CatalogPlugin catalog) {\n+    Preconditions.checkArgument(catalog instanceof SparkSessionCatalog || catalog instanceof SparkCatalog,\n+        \"Cannot create Iceberg table in non Iceberg Catalog. \" +\n+            \"Catalog %s was of class %s but %s or %s are required\",\n+        catalog.name(), catalog.getClass(), SparkSessionCatalog.class.getName(), SparkCatalog.class.getName());\n+\n+    return (StagingTableCatalog) catalog;\n+  }\n+\n+  protected StagedTable stageDestTable() {\n+    try {\n+      Map<String, String> props = targetTableProps();\n+      StructType schema = sourceTable.schema();\n+      Transform[] partitioning = sourceTable.partitioning();\n+      return destCatalog.stageCreate(destTableIdent, schema, partitioning, props);\n+    } catch (NoSuchNamespaceException e) {\n+      throw new IllegalArgumentException(\"Cannot create a new table in a namespace which does not exist\", e);", "originalCommit": "bf50967bb9ab9344b3c242c274cc9fab58c8b115", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc3MDk3NA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539770974", "bodyText": "And include the namespace that does not exist.", "author": "rdblue", "createdAt": "2020-12-10T01:20:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc3MDM3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc4MDIzNw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539780237", "bodyText": "Sorry I didn't know what you meant by \"iceberg exception\" i meant to ask but forgot", "author": "RussellSpitzer", "createdAt": "2020-12-10T01:45:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc3MDM3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc3MDY4OQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539770689", "bodyText": "Same here. I think these should be the right Iceberg exceptions. And, the exception message here should include the table identifier that could not be found.", "author": "rdblue", "createdAt": "2020-12-10T01:20:09Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableIdent().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableIdent().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableIdent(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);", "originalCommit": "bf50967bb9ab9344b3c242c274cc9fab58c8b115", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc3MDg2MQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539770861", "bodyText": "This should similarly include the name of the table that already exists.", "author": "rdblue", "createdAt": "2020-12-10T01:20:39Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableIdent().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableIdent().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableIdent(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);", "originalCommit": "bf50967bb9ab9344b3c242c274cc9fab58c8b115", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc3MTA0Mg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539771042", "bodyText": "Please include the destination table name in the error message when this is updated, too.", "author": "rdblue", "createdAt": "2020-12-10T01:21:10Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String LOCATION = \"location\";\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  protected static final List<String> EXCLUDED_PROPERTIES =\n+      ImmutableList.of(\"path\", \"transient_lastDdlTime\", \"serialization.format\");\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final TableCatalog sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private final Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                     CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) this.sourceCatalog.loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected TableCatalog sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableIdent() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableIdent() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private void validateSourceTable() {\n+    String sourceTableProvider = sourceCatalogTable.provider().get().toLowerCase(Locale.ROOT);\n+    Preconditions.checkArgument(ALLOWED_SOURCES.contains(sourceTableProvider),\n+          \"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider);\n+    Preconditions.checkArgument(!sourceCatalogTable.storage().locationUri().isEmpty(),\n+        \"Cannot create an Iceberg table from a source without an explicit location\");\n+  }\n+\n+  private StagingTableCatalog checkDestinationCatalog(CatalogPlugin catalog) {\n+    Preconditions.checkArgument(catalog instanceof SparkSessionCatalog || catalog instanceof SparkCatalog,\n+        \"Cannot create Iceberg table in non Iceberg Catalog. \" +\n+            \"Catalog %s was of class %s but %s or %s are required\",\n+        catalog.name(), catalog.getClass(), SparkSessionCatalog.class.getName(), SparkCatalog.class.getName());\n+\n+    return (StagingTableCatalog) catalog;\n+  }\n+\n+  protected StagedTable stageDestTable() {\n+    try {\n+      Map<String, String> props = targetTableProps();\n+      StructType schema = sourceTable.schema();\n+      Transform[] partitioning = sourceTable.partitioning();\n+      return destCatalog.stageCreate(destTableIdent, schema, partitioning, props);\n+    } catch (NoSuchNamespaceException e) {\n+      throw new IllegalArgumentException(\"Cannot create a new table in a namespace which does not exist\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Destination table already exists\", e);", "originalCommit": "bf50967bb9ab9344b3c242c274cc9fab58c8b115", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc3MTg1OQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539771859", "bodyText": "The return value of this needs to be a SparkTable for both uses, so I think it should return a SparkTable to avoid unchecked casts in the callers.", "author": "rdblue", "createdAt": "2020-12-10T01:23:10Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String LOCATION = \"location\";\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  protected static final List<String> EXCLUDED_PROPERTIES =\n+      ImmutableList.of(\"path\", \"transient_lastDdlTime\", \"serialization.format\");\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final TableCatalog sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private final Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                     CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) this.sourceCatalog.loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected TableCatalog sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableIdent() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableIdent() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private void validateSourceTable() {\n+    String sourceTableProvider = sourceCatalogTable.provider().get().toLowerCase(Locale.ROOT);\n+    Preconditions.checkArgument(ALLOWED_SOURCES.contains(sourceTableProvider),\n+          \"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider);\n+    Preconditions.checkArgument(!sourceCatalogTable.storage().locationUri().isEmpty(),\n+        \"Cannot create an Iceberg table from a source without an explicit location\");\n+  }\n+\n+  private StagingTableCatalog checkDestinationCatalog(CatalogPlugin catalog) {\n+    Preconditions.checkArgument(catalog instanceof SparkSessionCatalog || catalog instanceof SparkCatalog,\n+        \"Cannot create Iceberg table in non Iceberg Catalog. \" +\n+            \"Catalog %s was of class %s but %s or %s are required\",\n+        catalog.name(), catalog.getClass(), SparkSessionCatalog.class.getName(), SparkCatalog.class.getName());\n+\n+    return (StagingTableCatalog) catalog;\n+  }\n+\n+  protected StagedTable stageDestTable() {", "originalCommit": "bf50967bb9ab9344b3c242c274cc9fab58c8b115", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc4Nzc0NA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539787744", "bodyText": "It also needs to be StagedTable :/ since we need to commit changes (and possible abort changes) so I think we either need a cast to StagedTable or to SparkTable", "author": "RussellSpitzer", "createdAt": "2020-12-10T02:03:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc3MTg1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDI4NzQxNQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540287415", "bodyText": "Can it be StagedSparkTable that also extends SparkTable?", "author": "aokolnychyi", "createdAt": "2020-12-10T15:59:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc3MTg1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDI5NDY5MA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540294690", "bodyText": "Didn't see we had that, yeah we should use that", "author": "RussellSpitzer", "createdAt": "2020-12-10T16:09:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc3MTg1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc3Mzg3OA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539773878", "bodyText": "Migrate catches any exceptions from the abort and logs them, but allows the original exception to propagate. I think this should do the same.", "author": "rdblue", "createdAt": "2020-12-10T01:28:44Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableIdent, CatalogPlugin destCatalog,\n+                       Identifier destTableIdent) {\n+    super(spark, sourceCatalog, sourceTableIdent, destCatalog, destTableIdent);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable = stageDestTable();\n+    Table icebergTable = ((SparkTable) stagedTable).table();\n+    // TODO Check table location here against source location\n+\n+    ensureNameMappingPresent(icebergTable);\n+\n+    boolean threw = true;\n+    try {\n+      String stagingLocation = getMetadataLocation(icebergTable);\n+      LOG.info(\"Beginning snapshot of {} to {} using metadata location {}\", sourceTableIdent(), destTableIdent(),\n+          stagingLocation);\n+\n+      TableIdentifier v1TableIdentifier = v1SourceTable().identifier();\n+      SparkTableUtil.importSparkTable(spark(), v1TableIdentifier, icebergTable, stagingLocation);\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {\n+        LOG.error(\"Error when attempting to commit snapshot changes, rolling back\");\n+        stagedTable.abortStagedChanges();", "originalCommit": "bf50967bb9ab9344b3c242c274cc9fab58c8b115", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc3NDI4OA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539774288", "bodyText": "The checks here should be Preconditions checks as well.", "author": "rdblue", "createdAt": "2020-12-10T01:29:41Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableIdent, CatalogPlugin destCatalog,\n+                       Identifier destTableIdent) {\n+    super(spark, sourceCatalog, sourceTableIdent, destCatalog, destTableIdent);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable = stageDestTable();\n+    Table icebergTable = ((SparkTable) stagedTable).table();\n+    // TODO Check table location here against source location\n+\n+    ensureNameMappingPresent(icebergTable);\n+\n+    boolean threw = true;\n+    try {\n+      String stagingLocation = getMetadataLocation(icebergTable);\n+      LOG.info(\"Beginning snapshot of {} to {} using metadata location {}\", sourceTableIdent(), destTableIdent(),\n+          stagingLocation);\n+\n+      TableIdentifier v1TableIdentifier = v1SourceTable().identifier();\n+      SparkTableUtil.importSparkTable(spark(), v1TableIdentifier, icebergTable, stagingLocation);\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {\n+        LOG.error(\"Error when attempting to commit snapshot changes, rolling back\");\n+        stagedTable.abortStagedChanges();\n+      }\n+    }\n+\n+    Snapshot snapshot = icebergTable.currentSnapshot();\n+    long numMigratedFiles = Long.parseLong(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+    LOG.info(\"Successfully loaded Iceberg metadata for {} files\", numMigratedFiles);\n+    return numMigratedFiles;\n+  }\n+\n+  @Override\n+  protected Map<String, String> targetTableProps() {\n+    Map<String, String> properties = Maps.newHashMap();\n+\n+    // Remove any possible location properties from origin properties\n+    properties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());\n+    properties.remove(LOCATION);\n+    properties.remove(TableProperties.WRITE_METADATA_LOCATION);\n+    properties.remove(TableProperties.WRITE_NEW_DATA_LOCATION);\n+\n+    EXCLUDED_PROPERTIES.forEach(properties::remove);\n+    properties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n+    properties.put(TableProperties.GC_ENABLED, \"false\");\n+    properties.put(\"snapshot\", \"true\");\n+    properties.putAll(additionalProperties());\n+\n+    // Don't use the default location for the destination table if an alternate has be set\n+    if (destTableLocation != null) {\n+      properties.put(LOCATION, destTableLocation);\n+    }\n+\n+    return properties;\n+  }\n+\n+  @Override\n+  protected TableCatalog checkSourceCatalog(CatalogPlugin catalog) {\n+    // Currently the Import code relies on being able to look up the table in the session code\n+    if (!(catalog.name().equals(\"spark_catalog\"))) {\n+      throw new IllegalArgumentException(String.format(\n+          \"Cannot snapshot a table that isn't in spark_catalog, the session catalog. \" +\n+              \"Found source catalog %s\", catalog.name()));\n+    }\n+\n+    if (!(catalog instanceof TableCatalog)) {\n+      throw new IllegalArgumentException(String.format(\n+          \"Cannot snapshot a table from a non-table catalog %s. Catalog has class of %s.\", catalog.name(),\n+          catalog.getClass().toString()\n+      ));\n+    }", "originalCommit": "bf50967bb9ab9344b3c242c274cc9fab58c8b115", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "a99c8216da0f2adc74bc8b2498599fd4228bee63", "url": "https://github.com/apache/iceberg/commit/a99c8216da0f2adc74bc8b2498599fd4228bee63", "message": "Change Exception Handling", "committedDate": "2020-12-10T15:56:21Z", "type": "commit"}, {"oid": "a99c8216da0f2adc74bc8b2498599fd4228bee63", "url": "https://github.com/apache/iceberg/commit/a99c8216da0f2adc74bc8b2498599fd4228bee63", "message": "Change Exception Handling", "committedDate": "2020-12-10T15:56:21Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDI4OTU5Ng==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540289596", "bodyText": "nit: this on the right side seems redundant", "author": "aokolnychyi", "createdAt": "2020-12-10T16:02:40Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NoSuchNamespaceException;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String LOCATION = \"location\";\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  protected static final List<String> EXCLUDED_PROPERTIES =\n+      ImmutableList.of(\"path\", \"transient_lastDdlTime\", \"serialization.format\");\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final TableCatalog sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private final Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                     CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) this.sourceCatalog.loadTable(sourceTableIdent);", "originalCommit": "a99c8216da0f2adc74bc8b2498599fd4228bee63", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDI5NzE1Nw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540297157", "bodyText": "\"sourceCatalog\" is a CatalogPlugin, \"this.sourceCatalog\" is a TableCatalog. I could change the procedure args though", "author": "RussellSpitzer", "createdAt": "2020-12-10T16:12:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDI4OTU5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMwNDIyMA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540304220", "bodyText": "Got it, not a bid deal.", "author": "aokolnychyi", "createdAt": "2020-12-10T16:20:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDI4OTU5Ng=="}], "type": "inlineReview"}, {"oid": "c746b86385bae9f8ecfb847bf87612e377672978", "url": "https://github.com/apache/iceberg/commit/c746b86385bae9f8ecfb847bf87612e377672978", "message": "Use StagedSparkTable", "committedDate": "2020-12-10T16:16:13Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMwMDI5Mw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540300293", "bodyText": "Do we also exclude Iceberg related props? WRITE_NEW_DATA_LOCATION and WRITE_METADATA_LOCATION?", "author": "aokolnychyi", "createdAt": "2020-12-10T16:16:30Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NoSuchNamespaceException;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String LOCATION = \"location\";\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  protected static final List<String> EXCLUDED_PROPERTIES =", "originalCommit": "a99c8216da0f2adc74bc8b2498599fd4228bee63", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMwMTQ0OQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540301449", "bodyText": "Only for Snapshot, these are specifically the excluded properties from the original table metadata", "author": "RussellSpitzer", "createdAt": "2020-12-10T16:17:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMwMDI5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMyNDY4NA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540324684", "bodyText": "+1", "author": "aokolnychyi", "createdAt": "2020-12-10T16:43:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMwMDI5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMwNTc1Ng==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540305756", "bodyText": "nit: Do we add quotes around names or no? I am fine either way but having it consistent is good.", "author": "aokolnychyi", "createdAt": "2020-12-10T16:22:40Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NoSuchNamespaceException;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.source.StagedSparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String LOCATION = \"location\";\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  protected static final List<String> EXCLUDED_PROPERTIES =\n+      ImmutableList.of(\"path\", \"transient_lastDdlTime\", \"serialization.format\");\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final TableCatalog sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private final Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                     CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) this.sourceCatalog.loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (org.apache.spark.sql.catalyst.analysis.NoSuchTableException e) {\n+      throw new NoSuchTableException(\"Cannot not find source table %s\", sourceTableIdent);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected TableCatalog sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableIdent() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableIdent() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private void validateSourceTable() {\n+    String sourceTableProvider = sourceCatalogTable.provider().get().toLowerCase(Locale.ROOT);\n+    Preconditions.checkArgument(ALLOWED_SOURCES.contains(sourceTableProvider),\n+        \"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider);\n+    Preconditions.checkArgument(!sourceCatalogTable.storage().locationUri().isEmpty(),\n+        \"Cannot create an Iceberg table from a source without an explicit location\");\n+  }\n+\n+  private StagingTableCatalog checkDestinationCatalog(CatalogPlugin catalog) {\n+    Preconditions.checkArgument(catalog instanceof SparkSessionCatalog || catalog instanceof SparkCatalog,\n+        \"Cannot create Iceberg table in non Iceberg Catalog. Catalog %s was of class %s but %s or %s are required\",", "originalCommit": "c746b86385bae9f8ecfb847bf87612e377672978", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMxNzExMw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540317113", "bodyText": "we definitely add them in other places, I wasn't consistent in my reapplication though (I also think we aren't doing it consistently throughout the codebase but that's another story)", "author": "RussellSpitzer", "createdAt": "2020-12-10T16:35:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMwNTc1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDM2MjE4Ng==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540362186", "bodyText": "I think that names should be internally quoted, not externally. So [\"a\", \" \", \"b\"] should be a.` `.b.", "author": "rdblue", "createdAt": "2020-12-10T17:33:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMwNTc1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMwOTY3Nw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540309677", "bodyText": "nit: catalog.getClass() seems to be missing getName", "author": "aokolnychyi", "createdAt": "2020-12-10T16:26:43Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NoSuchNamespaceException;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.source.StagedSparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String LOCATION = \"location\";\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  protected static final List<String> EXCLUDED_PROPERTIES =\n+      ImmutableList.of(\"path\", \"transient_lastDdlTime\", \"serialization.format\");\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final TableCatalog sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private final Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                     CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) this.sourceCatalog.loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (org.apache.spark.sql.catalyst.analysis.NoSuchTableException e) {\n+      throw new NoSuchTableException(\"Cannot not find source table %s\", sourceTableIdent);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected TableCatalog sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableIdent() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableIdent() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private void validateSourceTable() {\n+    String sourceTableProvider = sourceCatalogTable.provider().get().toLowerCase(Locale.ROOT);\n+    Preconditions.checkArgument(ALLOWED_SOURCES.contains(sourceTableProvider),\n+        \"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider);\n+    Preconditions.checkArgument(!sourceCatalogTable.storage().locationUri().isEmpty(),\n+        \"Cannot create an Iceberg table from a source without an explicit location\");\n+  }\n+\n+  private StagingTableCatalog checkDestinationCatalog(CatalogPlugin catalog) {\n+    Preconditions.checkArgument(catalog instanceof SparkSessionCatalog || catalog instanceof SparkCatalog,\n+        \"Cannot create Iceberg table in non Iceberg Catalog. Catalog %s was of class %s but %s or %s are required\",\n+        catalog.name(), catalog.getClass(), SparkSessionCatalog.class.getName(), SparkCatalog.class.getName());", "originalCommit": "c746b86385bae9f8ecfb847bf87612e377672978", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMxMzQxNA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540313414", "bodyText": "nit: should we define this one closer to the block where it is used? Like next to stagedTable?", "author": "aokolnychyi", "createdAt": "2020-12-10T16:30:53Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.StagedSparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;", "originalCommit": "c746b86385bae9f8ecfb847bf87612e377672978", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMxNTE4Nw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540315187", "bodyText": "nit: missing quotes", "author": "aokolnychyi", "createdAt": "2020-12-10T16:33:15Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.StagedSparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableIdent().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableIdent().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableIdent(), backupIdentifier);\n+    } catch (org.apache.spark.sql.catalyst.analysis.NoSuchTableException e) {\n+      throw new NoSuchTableException(\"Cannot find table '%s' to migrate\", sourceTableIdent());\n+    } catch (org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException e) {\n+      throw new AlreadyExistsException(\"Cannot rename migration source '%s' to backup name '%s'.\" +\n+          \" Backup table already exists.\", sourceTableIdent(), backupIdentifier);\n+    }\n+\n+    StagedSparkTable stagedTable = null;\n+    boolean threw = true;\n+    try {\n+      stagedTable = stageDestTable();\n+      icebergTable = stagedTable.table();\n+\n+      ensureNameMappingPresent(icebergTable);\n+\n+      String stagingLocation = getMetadataLocation(icebergTable);\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableIdent(), stagingLocation);\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);\n+      TableIdentifier v1BackupIdentifier = new TableIdentifier(backupIdentifier.name(), backupNamespace);\n+      SparkTableUtil.importSparkTable(spark(), v1BackupIdentifier, icebergTable, stagingLocation);\n+\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {\n+        LOG.error(\"Error when attempting perform migration changes, aborting table creation and restoring backup.\");\n+\n+        try {\n+          stagedTable.abortStagedChanges();\n+        } catch (Exception abortException) {\n+          LOG.error(\"Cannot abort staged changes\", abortException);\n+        }\n+\n+        try {\n+          destCatalog().renameTable(backupIdentifier, sourceTableIdent());\n+        } catch (org.apache.spark.sql.catalyst.analysis.NoSuchTableException nstException) {\n+          throw new NoSuchTableException(\"Cannot restore backup '%s', the backup cannot be found\", backupIdentifier);\n+        } catch (org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException taeException) {\n+          throw new AlreadyExistsException(\"Cannot restore backup, a table with the original name \" +\n+              \"exists. The backup can be found with the name %s\", backupIdentifier);", "originalCommit": "c746b86385bae9f8ecfb847bf87612e377672978", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMxODA3OQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540318079", "bodyText": "Why use threw and finally instead of catch? Do we also re-throw the original exception?", "author": "aokolnychyi", "createdAt": "2020-12-10T16:36:18Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.StagedSparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableIdent().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableIdent().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableIdent(), backupIdentifier);\n+    } catch (org.apache.spark.sql.catalyst.analysis.NoSuchTableException e) {\n+      throw new NoSuchTableException(\"Cannot find table '%s' to migrate\", sourceTableIdent());\n+    } catch (org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException e) {\n+      throw new AlreadyExistsException(\"Cannot rename migration source '%s' to backup name '%s'.\" +\n+          \" Backup table already exists.\", sourceTableIdent(), backupIdentifier);\n+    }\n+\n+    StagedSparkTable stagedTable = null;\n+    boolean threw = true;\n+    try {\n+      stagedTable = stageDestTable();\n+      icebergTable = stagedTable.table();\n+\n+      ensureNameMappingPresent(icebergTable);\n+\n+      String stagingLocation = getMetadataLocation(icebergTable);\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableIdent(), stagingLocation);\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);\n+      TableIdentifier v1BackupIdentifier = new TableIdentifier(backupIdentifier.name(), backupNamespace);\n+      SparkTableUtil.importSparkTable(spark(), v1BackupIdentifier, icebergTable, stagingLocation);\n+\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {", "originalCommit": "c746b86385bae9f8ecfb847bf87612e377672978", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMyMjAzOQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540322039", "bodyText": "Well, it is finally, never mind.", "author": "aokolnychyi", "createdAt": "2020-12-10T16:40:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMxODA3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMyMDM5OQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540320399", "bodyText": "nit: quotes?", "author": "aokolnychyi", "createdAt": "2020-12-10T16:38:33Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.StagedSparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableIdent().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableIdent().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableIdent(), backupIdentifier);\n+    } catch (org.apache.spark.sql.catalyst.analysis.NoSuchTableException e) {\n+      throw new NoSuchTableException(\"Cannot find table '%s' to migrate\", sourceTableIdent());\n+    } catch (org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException e) {\n+      throw new AlreadyExistsException(\"Cannot rename migration source '%s' to backup name '%s'.\" +\n+          \" Backup table already exists.\", sourceTableIdent(), backupIdentifier);\n+    }\n+\n+    StagedSparkTable stagedTable = null;\n+    boolean threw = true;\n+    try {\n+      stagedTable = stageDestTable();\n+      icebergTable = stagedTable.table();\n+\n+      ensureNameMappingPresent(icebergTable);\n+\n+      String stagingLocation = getMetadataLocation(icebergTable);\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableIdent(), stagingLocation);\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);\n+      TableIdentifier v1BackupIdentifier = new TableIdentifier(backupIdentifier.name(), backupNamespace);\n+      SparkTableUtil.importSparkTable(spark(), v1BackupIdentifier, icebergTable, stagingLocation);\n+\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {\n+        LOG.error(\"Error when attempting perform migration changes, aborting table creation and restoring backup.\");\n+\n+        try {\n+          stagedTable.abortStagedChanges();\n+        } catch (Exception abortException) {\n+          LOG.error(\"Cannot abort staged changes\", abortException);\n+        }\n+\n+        try {\n+          destCatalog().renameTable(backupIdentifier, sourceTableIdent());\n+        } catch (org.apache.spark.sql.catalyst.analysis.NoSuchTableException nstException) {\n+          throw new NoSuchTableException(\"Cannot restore backup '%s', the backup cannot be found\", backupIdentifier);\n+        } catch (org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException taeException) {\n+          throw new AlreadyExistsException(\"Cannot restore backup, a table with the original name \" +\n+              \"exists. The backup can be found with the name %s\", backupIdentifier);\n+        }\n+      }\n+    }\n+\n+    Snapshot snapshot = icebergTable.currentSnapshot();\n+    long numMigratedFiles = Long.parseLong(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+    LOG.info(\"Successfully loaded Iceberg metadata for {} files\", numMigratedFiles);\n+    return numMigratedFiles;\n+  }\n+\n+  @Override\n+  protected Map<String, String> targetTableProps() {\n+    Map<String, String> properties = Maps.newHashMap();\n+    properties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());\n+    EXCLUDED_PROPERTIES.forEach(properties::remove);\n+    properties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n+    properties.put(\"migrated\", \"true\");\n+    properties.putAll(additionalProperties());\n+    return properties;\n+  }\n+\n+  @Override\n+  protected TableCatalog checkSourceCatalog(CatalogPlugin catalog) {\n+    // Currently the Import code relies on being able to look up the table in the session code\n+    Preconditions.checkArgument(catalog instanceof SparkSessionCatalog,\n+        \"Cannot migrate a table from a non-Iceberg Spark Session Catalog. Found %s of class %s as the source catalog.\",", "originalCommit": "c746b86385bae9f8ecfb847bf87612e377672978", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMyMzc0Mg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540323742", "bodyText": "+1", "author": "aokolnychyi", "createdAt": "2020-12-10T16:41:54Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.StagedSparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableIdent, CatalogPlugin destCatalog,\n+                       Identifier destTableIdent) {\n+    super(spark, sourceCatalog, sourceTableIdent, destCatalog, destTableIdent);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedSparkTable stagedTable = stageDestTable();\n+    Table icebergTable = stagedTable.table();\n+    // TODO Check table location here against source location\n+\n+    ensureNameMappingPresent(icebergTable);\n+\n+    boolean threw = true;\n+    try {\n+      String stagingLocation = getMetadataLocation(icebergTable);\n+      LOG.info(\"Beginning snapshot of {} to {} using metadata location {}\", sourceTableIdent(), destTableIdent(),\n+          stagingLocation);\n+\n+      TableIdentifier v1TableIdentifier = v1SourceTable().identifier();\n+      SparkTableUtil.importSparkTable(spark(), v1TableIdentifier, icebergTable, stagingLocation);\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {\n+        LOG.error(\"Error when attempting to commit snapshot changes, rolling back\");\n+\n+        try {\n+          stagedTable.abortStagedChanges();\n+        } catch (Exception abortException) {\n+          LOG.error(\"Cannot abort staged changes\", abortException);\n+        }\n+      }\n+    }\n+\n+    Snapshot snapshot = icebergTable.currentSnapshot();\n+    long numMigratedFiles = Long.parseLong(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+    LOG.info(\"Successfully loaded Iceberg metadata for {} files\", numMigratedFiles);\n+    return numMigratedFiles;\n+  }\n+\n+  @Override\n+  protected Map<String, String> targetTableProps() {\n+    Map<String, String> properties = Maps.newHashMap();\n+\n+    // Remove any possible location properties from origin properties\n+    properties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());", "originalCommit": "c746b86385bae9f8ecfb847bf87612e377672978", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "a4bbc7af123e2f003941f3b3fb6aab3fc39821b3", "url": "https://github.com/apache/iceberg/commit/a4bbc7af123e2f003941f3b3fb6aab3fc39821b3", "message": "Final review nits", "committedDate": "2020-12-10T18:45:23Z", "type": "commit"}, {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39", "url": "https://github.com/apache/iceberg/commit/a23fa89b2d5a2be61af72836656e4bf3322c9c39", "message": "Provide API and Implementation for Creating Iceberg Tables from Spark\n\nPreviously the SparkUtil class provided an importSparkTable command but\nthis command suffered from a few shortcomings. It had a difficult api and\ndid not work directly with DSV2 Iceberg Catalogs. To provide a simpler interface\nthat shares a similar pattern to currently available Spark Actions we introduce\nCreateActions and MigrateAction.\n\nThrough this command we can both migrate a Table as well as make a snapshot\nof an existing table.\n\nGeneral usage:\n\n  CreateActions\n    .createIcebergTable(spark, dest)\n    .fromSourceTable(source)\n    .withNewTableLocation(location)\n    .execute()", "committedDate": "2020-09-28T22:11:42Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1NjQyNQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r496356425", "bodyText": "EDIT: I goofed. We're creating iceberg tables from Spark tables. So my 1st suggestion should be ignored entirely. Of course spark tables won't have hidden partition specs as that's an iceberg concept and we're using this to convert tables from spark to iceberg \ud83e\udd26 . Is it possible to re-use a spark table's bucket based partitioning? I've never personally used it due to the small files problem it can generate, but are our hash functions for bucketing so different from the Spark bucketing (or some other issue) that we can't make it work - like, in a follow up PR?\n--- Original ---\nMy understanding is that we're only searching for non-hidden partitions here because that's all that's possible to create from a table derived from catalyst's TableIdentifier which has no knowledge of hidden partitions and uses Hive style partitioning instead. That being said, I do still think that the JavaDoc could use an update emphasizing this. How about something like this...\n/**\n * Given a Spark table identifier, determine the PartitionSpec, which will be either\n * an identity or unpartitioned PartitionSpec based on the original table's hive-style\n * partition columns.\n */\n\nI'd love to hear other suggestions, but to me the JavaDoc seems ... somehow missing something. But I'm not quite sure what it is. \ud83e\udd14", "author": "kbendick", "createdAt": "2020-09-29T03:27:42Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkSchemaUtil.java", "diffHunk": "@@ -63,6 +65,21 @@ public static Schema schemaForTable(SparkSession spark, String name) {\n     return new Schema(converted.asNestedType().asStructType().fields());\n   }\n \n+  /**\n+   * Given a Spark table identifier, determine the PartitionSpec.\n+   * @param spark the SparkSession which contains the identifier\n+   * @param table a TableIdentifier, if the namespace is left blank the catalog().currentDatabase() will be used\n+   * @return a IcebergPartitionSpec representing the partitioning of the Spark table\n+   * @throws AnalysisException if thrown by the Spark catalog\n+   */\n+  public static PartitionSpec specForTable(SparkSession spark, TableIdentifier table) throws AnalysisException {\n+    String db = table.database().nonEmpty() ? table.database().get() : spark.catalog().currentDatabase();\n+    PartitionSpec spec = identitySpec(\n+        schemaForTable(spark, table.unquotedString()),\n+        spark.catalog().listColumns(db, table.table()).collectAsList());\n+    return spec == null ? PartitionSpec.unpartitioned() : spec;", "originalCommit": "a23fa89b2d5a2be61af72836656e4bf3322c9c39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc2NDU0OQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r496764549", "bodyText": "Definitely want this to be more clear, I think your documentation is correct, either the table is already partitioned with identity transforms, if so we need to know which, or it has no partitioning information.", "author": "RussellSpitzer", "createdAt": "2020-09-29T14:28:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1NjQyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzEyODIwMA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r497128200", "bodyText": "So I've thought about this more, and I think that what might be missing / what is bothering me is honestly just an empty line between the doc comment and the section for params. \ud83d\ude05\nSo definitely file that one under nits.", "author": "kbendick", "createdAt": "2020-09-29T23:15:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1NjQyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM2NDc0MA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r496364740", "bodyText": "I think that you can get the table DDL via a utility function in catalyst, and then there's a function that returns a schema from DDL. However, if this is returning an Iceberg schema then do please ignore me :)", "author": "kbendick", "createdAt": "2020-09-29T04:02:30Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkSchemaUtil.java", "diffHunk": "@@ -63,6 +65,21 @@ public static Schema schemaForTable(SparkSession spark, String name) {\n     return new Schema(converted.asNestedType().asStructType().fields());\n   }\n \n+  /**\n+   * Given a Spark table identifier, determine the PartitionSpec.\n+   * @param spark the SparkSession which contains the identifier\n+   * @param table a TableIdentifier, if the namespace is left blank the catalog().currentDatabase() will be used\n+   * @return a IcebergPartitionSpec representing the partitioning of the Spark table\n+   * @throws AnalysisException if thrown by the Spark catalog\n+   */\n+  public static PartitionSpec specForTable(SparkSession spark, TableIdentifier table) throws AnalysisException {\n+    String db = table.database().nonEmpty() ? table.database().get() : spark.catalog().currentDatabase();\n+    PartitionSpec spec = identitySpec(\n+        schemaForTable(spark, table.unquotedString()),", "originalCommit": "a23fa89b2d5a2be61af72836656e4bf3322c9c39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc2NDkyNw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r496764927", "bodyText": "Yeah this is the IcebergSchema :)", "author": "RussellSpitzer", "createdAt": "2020-09-29T14:29:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM2NDc0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzEyNzQ3NQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r497127475", "bodyText": "I figured once I typed it out. I should really start deleting my comments once I come to the answer rubber duck debugging with myself in the github comments \ud83d\ude05 .", "author": "kbendick", "createdAt": "2020-09-29T23:14:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM2NDc0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM3NjUxMg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r496376512", "bodyText": "Can we create a separate PR for this and merge it in sooner? I imagine this PR will take longer to merge because of its large scope, but this seems useful now.", "author": "kbendick", "createdAt": "2020-09-29T04:23:40Z", "path": "spark/src/test/java/org/apache/iceberg/spark/SparkTestBase.java", "diffHunk": "@@ -63,7 +63,9 @@ public static void startMetastoreAndSpark() {\n \n   @AfterClass\n   public static void stopMetastoreAndSpark() {\n-    catalog.close();\n+    if (catalog != null) {\n+      catalog.close();\n+    }", "originalCommit": "a23fa89b2d5a2be61af72836656e4bf3322c9c39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njc2NTQ4Ng==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r496765486", "bodyText": "Sure, I just added this in because my tests were yelling at me all the time because I like to throw random kill codes around when debugging. I decided to just make sure it would stop bothering me.", "author": "RussellSpitzer", "createdAt": "2020-09-29T14:30:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM3NjUxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzEyOTI0Ng==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r497129246", "bodyText": "I can create a separate PR to handle this now if you don't mind.", "author": "kbendick", "createdAt": "2020-09-29T23:16:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM3NjUxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzEzNDgwOQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r497134809", "bodyText": "@RussellSpitzer I created a PR that cherry picks just this change as this PR's scope is pretty large and this small change can be merged in much more quickly: #1529", "author": "kbendick", "createdAt": "2020-09-29T23:24:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM3NjUxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY2OTk4NA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r500669984", "bodyText": "#1529 was merged.", "author": "rdblue", "createdAt": "2020-10-07T00:29:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM3NjUxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY2OTUyNA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r500669524", "bodyText": "I don't think that we should leak the v1 Spark API (TableIdentifier) in a util class like this. What about passing database and name separately? Then the caller is responsible for adding database, which avoids the need to use v1 spark.catalog() to default it.", "author": "rdblue", "createdAt": "2020-10-07T00:28:06Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkSchemaUtil.java", "diffHunk": "@@ -63,6 +65,21 @@ public static Schema schemaForTable(SparkSession spark, String name) {\n     return new Schema(converted.asNestedType().asStructType().fields());\n   }\n \n+  /**\n+   * Given a Spark table identifier, determine the PartitionSpec.\n+   * @param spark the SparkSession which contains the identifier\n+   * @param table a TableIdentifier, if the namespace is left blank the catalog().currentDatabase() will be used\n+   * @return a IcebergPartitionSpec representing the partitioning of the Spark table\n+   * @throws AnalysisException if thrown by the Spark catalog\n+   */\n+  public static PartitionSpec specForTable(SparkSession spark, TableIdentifier table) throws AnalysisException {", "originalCommit": "a23fa89b2d5a2be61af72836656e4bf3322c9c39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQ5MzAwMg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r503493002", "bodyText": "Sure, the main issue is that the old version just always used the string literal \"default\" no matter what. Which I don't think is the right thing to do.", "author": "RussellSpitzer", "createdAt": "2020-10-12T19:34:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY2OTUyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQ5NDk1Mg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r503494952", "bodyText": "I actually am not using this method any more so I'll drop it and revert back to \"default in the base specForTable", "author": "RussellSpitzer", "createdAt": "2020-10-12T19:38:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY2OTUyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3MDU1MA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r500670550", "bodyText": "I don't think that spark.catalog().currentDatabase() is correct. I thought that spark.catalog() always returns the built-in v1 catalog.", "author": "rdblue", "createdAt": "2020-10-07T00:31:47Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/CreateActions.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+\n+public class CreateActions {\n+  private final Identifier newTableName;\n+  private final SparkSession spark;\n+\n+  private CreateActions(SparkSession spark, Identifier newTableName) {\n+    this.spark = spark;\n+    this.newTableName = newTableName;\n+  }\n+\n+  /**\n+   * Create a new Iceberg table in the Iceberg SparkSession catalog based on an\n+   * existing non-iceberg table.\n+   * @param spark the session to use for interacting with Spark\n+   * @param newTableName the string representation of the multipart identifier for new table's name\n+   */\n+  public static CreateActions createIcebergTable(SparkSession spark, String newTableName) {\n+    Identifier identifier = Spark3Util.parseIdentifier(spark, newTableName);\n+    if (identifier.namespace() == null || identifier.namespace().length == 0) {\n+      identifier = Identifier.of(new String[] {spark.catalog().currentDatabase()}, identifier.name());\n+    }\n+    return createIcebergTable(spark, identifier);\n+  }\n+\n+  /**\n+   * Create a new Iceberg table in the Iceberg SparkSession catalog based on an\n+   * existing non-iceberg table.\n+   * @param spark the session to use for interacting with Spark\n+   * @param newTableName the Spark catalog Identifier for new table's name\n+   */\n+  public static CreateActions createIcebergTable(SparkSession spark, Identifier newTableName) {\n+    return new CreateActions(spark, newTableName);\n+  }\n+\n+  /**\n+   * Existing table to be used for creating the Iceberg Table\n+   * @param existingTable the string representation of an existing table\n+   * @return a Spark Action to perform the migration\n+   */\n+  public MigrateAction fromSourceTable(String existingTable) {\n+    Identifier identifier = Spark3Util.parseIdentifier(spark, existingTable);\n+    if (identifier.namespace() == null || identifier.namespace().length == 0) {\n+      identifier = Identifier.of(new String[] {spark.catalog().currentDatabase()}, identifier.name());", "originalCommit": "a23fa89b2d5a2be61af72836656e4bf3322c9c39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI5NzQ4NA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r504297484", "bodyText": "Removed this entirely, we now require a Catalog and Identifier to remove any ambiguity here. We parse this pair from the name as a multipart identifier, see new utility methods.", "author": "RussellSpitzer", "createdAt": "2020-10-13T22:35:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3MDU1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3MTIyOQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r500671229", "bodyText": "Is it possible to use the default V2SessionCatalog and Iceberg's non-session catalog?", "author": "rdblue", "createdAt": "2020-10-07T00:34:22Z", "path": "spark3/src/test/java/org/apache/iceberg/spark/actions/TestCreateActions.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.CreateActions;\n+import org.apache.iceberg.spark.MigrateAction;\n+import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.source.SimpleRecord;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runners.Parameterized;\n+\n+public class TestCreateActions extends SparkCatalogTestBase {\n+\n+  // Only valid for IcebergV2Catalog\n+  @Parameterized.Parameters\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"spark_catalog\", SparkSessionCatalog.class.getName(), ImmutableMap.of(", "originalCommit": "a23fa89b2d5a2be61af72836656e4bf3322c9c39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI5Njk5Ng==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r504296996", "bodyText": "Yes added in test for this, but removed the Hadoop one which I think may not work for either Snapshot or Migrate because of directory requirements. I believe this was passing previously because some of the names were colliding.", "author": "RussellSpitzer", "createdAt": "2020-10-13T22:34:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3MTIyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3MTQwNA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r500671404", "bodyText": "This doesn't look qualified?", "author": "rdblue", "createdAt": "2020-10-07T00:34:57Z", "path": "spark3/src/test/java/org/apache/iceberg/spark/actions/TestCreateActions.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.CreateActions;\n+import org.apache.iceberg.spark.MigrateAction;\n+import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.source.SimpleRecord;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runners.Parameterized;\n+\n+public class TestCreateActions extends SparkCatalogTestBase {\n+\n+  // Only valid for IcebergV2Catalog\n+  @Parameterized.Parameters\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"spark_catalog\", SparkSessionCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\",\n+            \"parquet-enabled\", \"true\",\n+            \"cache-enabled\", \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n+        )},\n+        new Object[] {\"spark_catalog\", SparkSessionCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hadoop\",\n+            \"default-namespace\", \"default\",\n+            \"parquet-enabled\", \"true\",\n+            \"cache-enabled\", \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n+        )}\n+    };\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  String qualifiedTableName = \"baseTable\";", "originalCommit": "a23fa89b2d5a2be61af72836656e4bf3322c9c39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI5NzA2OA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r504297068", "bodyText": "It was once :P, renamed", "author": "RussellSpitzer", "createdAt": "2020-10-13T22:34:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3MTQwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3MzU1Mw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r500673553", "bodyText": "I think this is referred to as \"snapshot\" in the tests, right? (From the SNAPSHOT TABLE command?)\nAnd the other one is a \"migrate\"?", "author": "rdblue", "createdAt": "2020-10-07T00:43:20Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/MigrateAction.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import jline.internal.Log;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.actions.Action;\n+import org.apache.iceberg.actions.ExpireSnapshotsAction;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogManager;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This action will migrate a known table in a Spark Catalog that is not an Iceberg table into an Iceberg table.\n+ * The created new table will be able to interact with and modify files in the original table.\n+ *\n+ * There are two main code paths\n+ *   - Creating a brand new iceberg table or replacing an existing Iceberg table\n+ *   This pathway will use a staged table to stage the creation or replacement, only committing after\n+ *   import has succeeded.", "originalCommit": "a23fa89b2d5a2be61af72836656e4bf3322c9c39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3NDMyOA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r500674328", "bodyText": "How does the user know which one is going to happen? Table names are passed into all of the methods in CreateActions.", "author": "rdblue", "createdAt": "2020-10-07T00:46:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3MzU1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQ5ODUyMQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r503498521", "bodyText": "Changed up the api to be much more explicit about what is happening.", "author": "RussellSpitzer", "createdAt": "2020-10-12T19:48:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3MzU1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3MzgzNA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r500673834", "bodyText": "We normally put constructors before public methods. Minor, but this was harder to find than needed.", "author": "rdblue", "createdAt": "2020-10-07T00:44:26Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/MigrateAction.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import jline.internal.Log;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.actions.Action;\n+import org.apache.iceberg.actions.ExpireSnapshotsAction;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogManager;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This action will migrate a known table in a Spark Catalog that is not an Iceberg table into an Iceberg table.\n+ * The created new table will be able to interact with and modify files in the original table.\n+ *\n+ * There are two main code paths\n+ *   - Creating a brand new iceberg table or replacing an existing Iceberg table\n+ *   This pathway will use a staged table to stage the creation or replacement, only committing after\n+ *   import has succeeded.\n+ *\n+ *   - Replacing a table in the Session Catalog with an Iceberg Table with the same name\n+ *   This pathway will create a temporary table with a different name. This replacement table will\n+ *   be committed upon a successful import. Then the original session catalog entry will be dropped\n+ *   and the new replacement table renamed to take its place.\n+ */\n+public class MigrateAction implements Action<Long> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  private static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  private static final String REPLACEMENT_NAME = \"_REPLACEMENT_\";\n+\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final CatalogTable sourceTable;\n+  private final String sourceTableLocation;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final Boolean sessionCatalogReplacement;\n+  private final SparkSessionCatalog destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private String destDataLocation;\n+  private String destMetadataLocation;\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  /**\n+   * Creates an Iceberg Location at a given location instead of using the location\n+   * provided by the source table. New metadata and data files will be added to this\n+   * new location and further operations will not effect the source table.\n+   *\n+   * Use this if you would like to experiment with Iceberg without changing\n+   * your original files.\n+   * @param newLocation the base directory for the new Iceberg Table\n+   * @return this for chaining\n+   */\n+  public MigrateAction withNewTableLocation(String newLocation) {\n+    this.destDataLocation = newLocation;\n+    this.destMetadataLocation = newLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+    return this;\n+  }\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with\n+   * the same key name will be overwritten.\n+   * @param properties a map of properties to be included\n+   * @return this for chaining\n+   */\n+  public MigrateAction withAdditionalProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  /**\n+   * Adds an additional property to the newly created Iceberg Table. Any properties\n+   * with the same key name will be overwritten.\n+   * @param key the key of the property to add\n+   * @param value the value of the property to add\n+   */\n+  public MigrateAction withAdditionalProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  public MigrateAction(SparkSession spark, Identifier destTableName, Identifier sourceTableName) {", "originalCommit": "a23fa89b2d5a2be61af72836656e4bf3322c9c39", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3NTQ5NQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r500675495", "bodyText": "I'm finding it hard to follow when this would migrate a table vs snapshot a table, and it also requires a separate class.\nDid you consider an API using verbs directly? I'm imagining something like this:\nActions.migrate(\"db.table\").execute();\nActions.snapshot(\"db.table\").as(\"db.table_iceberg\").execute();\n\n// maybe even create like?\nActions.createTable(\"db.table\").like(\"db.table_hive\").execute()\nWe could add methods as well, but those verbs correspond to the SQL that we expose and we haven't needed anything more complicated than that.", "author": "rdblue", "createdAt": "2020-10-07T00:50:47Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/CreateActions.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+\n+public class CreateActions {", "originalCommit": "a23fa89b2d5a2be61af72836656e4bf3322c9c39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzM2Mjk2NA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r503362964", "bodyText": "If we wanted the most limited api I think we want\nMigrate - Takes one arg (identifier)\nSnapshot - Takes two args (identifier source, identifier dest, location)\nI'm a little worried about having a \"snapshot\" verb with  \"as\" since I feel like it also requires an \"at\" for a location and then I have to do a runtime check to make sure they are both used unless I make a series of chaining classes.\nSnapshotStart ---  .as(identifier) -->  SnapshotWithDest --- at(location) --> CreateAction\nAlthough I'm ok with that too, that's probably more complicated for us, but more difficult for an end user to use incorrectly or get an unexpected result from.\nThis would be the minimal setup then, I'm not sure what createTable like would do?", "author": "RussellSpitzer", "createdAt": "2020-10-12T15:12:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3NTQ5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQ5NjA4Ng==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r503496086", "bodyText": "I took a more locked down approach to this for the mean time. We only expose a\nActions.migrate\nActions.snapshot\nTo the public, this class instead becomes package protected with a few public methods,\nThose inherited from Action\nwithProperty/ies : for adding table properties\nas : for changing the destination", "author": "RussellSpitzer", "createdAt": "2020-10-12T19:41:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3NTQ5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3NjM2Ng==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r500676366", "bodyText": "Does this mean that only Spark tables are supported and not Hive tables? I don't think that Hive tables have providers.", "author": "rdblue", "createdAt": "2020-10-07T00:54:17Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/MigrateAction.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import jline.internal.Log;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.actions.Action;\n+import org.apache.iceberg.actions.ExpireSnapshotsAction;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogManager;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This action will migrate a known table in a Spark Catalog that is not an Iceberg table into an Iceberg table.\n+ * The created new table will be able to interact with and modify files in the original table.\n+ *\n+ * There are two main code paths\n+ *   - Creating a brand new iceberg table or replacing an existing Iceberg table\n+ *   This pathway will use a staged table to stage the creation or replacement, only committing after\n+ *   import has succeeded.\n+ *\n+ *   - Replacing a table in the Session Catalog with an Iceberg Table with the same name\n+ *   This pathway will create a temporary table with a different name. This replacement table will\n+ *   be committed upon a successful import. Then the original session catalog entry will be dropped\n+ *   and the new replacement table renamed to take its place.\n+ */\n+public class MigrateAction implements Action<Long> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  private static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  private static final String REPLACEMENT_NAME = \"_REPLACEMENT_\";\n+\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final CatalogTable sourceTable;\n+  private final String sourceTableLocation;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final Boolean sessionCatalogReplacement;\n+  private final SparkSessionCatalog destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private String destDataLocation;\n+  private String destMetadataLocation;\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  /**\n+   * Creates an Iceberg Location at a given location instead of using the location\n+   * provided by the source table. New metadata and data files will be added to this\n+   * new location and further operations will not effect the source table.\n+   *\n+   * Use this if you would like to experiment with Iceberg without changing\n+   * your original files.\n+   * @param newLocation the base directory for the new Iceberg Table\n+   * @return this for chaining\n+   */\n+  public MigrateAction withNewTableLocation(String newLocation) {\n+    this.destDataLocation = newLocation;\n+    this.destMetadataLocation = newLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+    return this;\n+  }\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with\n+   * the same key name will be overwritten.\n+   * @param properties a map of properties to be included\n+   * @return this for chaining\n+   */\n+  public MigrateAction withAdditionalProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  /**\n+   * Adds an additional property to the newly created Iceberg Table. Any properties\n+   * with the same key name will be overwritten.\n+   * @param key the key of the property to add\n+   * @param value the value of the property to add\n+   */\n+  public MigrateAction withAdditionalProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  public MigrateAction(SparkSession spark, Identifier destTableName, Identifier sourceTableName) {\n+    this.spark = spark;\n+    this.sourceTableName = sourceTableName;\n+    this.destTableName = destTableName;\n+    this.destCatalog = getSparkSessionCatalogOrFail(spark, destTableName);\n+\n+    try {\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable =\n+        spark.sessionState().catalog().getTableMetadata(Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (NoSuchTableException | NoSuchDatabaseException e) {\n+      throw new IllegalArgumentException(String.format(\"Could not find source table %s\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceTable, ALLOWED_SOURCES);\n+\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+    this.sessionCatalogReplacement = !sourceTableProvider.equals(\"iceberg\") && sourceTableName.equals(destTableName);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceTable.storage().locationUri().get());\n+    this.destDataLocation = sourceTableLocation;\n+    this.destMetadataLocation = sourceTableLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable, Set<String> supportedSourceTableProviders) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);", "originalCommit": "a23fa89b2d5a2be61af72836656e4bf3322c9c39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzUwNjA1NA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r503506054", "bodyText": "Added in a Hive test (at least I think so, used CREATE EXTERNAL TABLE LOCATION ....) which I though triggered the hive path, works for this.", "author": "RussellSpitzer", "createdAt": "2020-10-12T20:01:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3NjM2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzU2MjU1MQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r503562551", "bodyText": "The provider for hive tables is \"hive\", doublechecked", "author": "RussellSpitzer", "createdAt": "2020-10-12T22:19:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3NjM2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3NzA1Mg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r500677052", "bodyText": "Style nit: Both \"Spark\" and \"OrFail\" are implied, so I think they are just making this method name longer. It could be getSessionCatalog.", "author": "rdblue", "createdAt": "2020-10-07T00:56:58Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/MigrateAction.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import jline.internal.Log;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.actions.Action;\n+import org.apache.iceberg.actions.ExpireSnapshotsAction;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogManager;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This action will migrate a known table in a Spark Catalog that is not an Iceberg table into an Iceberg table.\n+ * The created new table will be able to interact with and modify files in the original table.\n+ *\n+ * There are two main code paths\n+ *   - Creating a brand new iceberg table or replacing an existing Iceberg table\n+ *   This pathway will use a staged table to stage the creation or replacement, only committing after\n+ *   import has succeeded.\n+ *\n+ *   - Replacing a table in the Session Catalog with an Iceberg Table with the same name\n+ *   This pathway will create a temporary table with a different name. This replacement table will\n+ *   be committed upon a successful import. Then the original session catalog entry will be dropped\n+ *   and the new replacement table renamed to take its place.\n+ */\n+public class MigrateAction implements Action<Long> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  private static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  private static final String REPLACEMENT_NAME = \"_REPLACEMENT_\";\n+\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final CatalogTable sourceTable;\n+  private final String sourceTableLocation;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final Boolean sessionCatalogReplacement;\n+  private final SparkSessionCatalog destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private String destDataLocation;\n+  private String destMetadataLocation;\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  /**\n+   * Creates an Iceberg Location at a given location instead of using the location\n+   * provided by the source table. New metadata and data files will be added to this\n+   * new location and further operations will not effect the source table.\n+   *\n+   * Use this if you would like to experiment with Iceberg without changing\n+   * your original files.\n+   * @param newLocation the base directory for the new Iceberg Table\n+   * @return this for chaining\n+   */\n+  public MigrateAction withNewTableLocation(String newLocation) {\n+    this.destDataLocation = newLocation;\n+    this.destMetadataLocation = newLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+    return this;\n+  }\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with\n+   * the same key name will be overwritten.\n+   * @param properties a map of properties to be included\n+   * @return this for chaining\n+   */\n+  public MigrateAction withAdditionalProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  /**\n+   * Adds an additional property to the newly created Iceberg Table. Any properties\n+   * with the same key name will be overwritten.\n+   * @param key the key of the property to add\n+   * @param value the value of the property to add\n+   */\n+  public MigrateAction withAdditionalProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  public MigrateAction(SparkSession spark, Identifier destTableName, Identifier sourceTableName) {\n+    this.spark = spark;\n+    this.sourceTableName = sourceTableName;\n+    this.destTableName = destTableName;\n+    this.destCatalog = getSparkSessionCatalogOrFail(spark, destTableName);\n+\n+    try {\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable =\n+        spark.sessionState().catalog().getTableMetadata(Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (NoSuchTableException | NoSuchDatabaseException e) {\n+      throw new IllegalArgumentException(String.format(\"Could not find source table %s\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceTable, ALLOWED_SOURCES);\n+\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+    this.sessionCatalogReplacement = !sourceTableProvider.equals(\"iceberg\") && sourceTableName.equals(destTableName);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceTable.storage().locationUri().get());\n+    this.destDataLocation = sourceTableLocation;\n+    this.destMetadataLocation = sourceTableLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable, Set<String> supportedSourceTableProviders) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!supportedSourceTableProviders.contains(sourceTableProvider)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider));\n+    }\n+    if (sourceTable.storage().locationUri().isEmpty()) {\n+      throw new IllegalArgumentException(\"Cannot create an Iceberg table from a source without an explicit location\");\n+    }\n+  }\n+\n+  private static Map<String, String> extraIcebergTableProps(String tableLocation, String metadataLocation) {\n+    return ImmutableMap.of(\n+        TableProperties.WRITE_METADATA_LOCATION, metadataLocation,\n+        TableProperties.WRITE_NEW_DATA_LOCATION, tableLocation,\n+        \"migrated\", \"true\");\n+  }\n+\n+  private static SparkSessionCatalog getSparkSessionCatalogOrFail(", "originalCommit": "a23fa89b2d5a2be61af72836656e4bf3322c9c39", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3NzQzMA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r500677430", "bodyText": "I think everything after the staged table is created should be in the try block so that any exception thrown will roll back any changes that were made.", "author": "rdblue", "createdAt": "2020-10-07T00:58:34Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/MigrateAction.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import jline.internal.Log;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.actions.Action;\n+import org.apache.iceberg.actions.ExpireSnapshotsAction;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogManager;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This action will migrate a known table in a Spark Catalog that is not an Iceberg table into an Iceberg table.\n+ * The created new table will be able to interact with and modify files in the original table.\n+ *\n+ * There are two main code paths\n+ *   - Creating a brand new iceberg table or replacing an existing Iceberg table\n+ *   This pathway will use a staged table to stage the creation or replacement, only committing after\n+ *   import has succeeded.\n+ *\n+ *   - Replacing a table in the Session Catalog with an Iceberg Table with the same name\n+ *   This pathway will create a temporary table with a different name. This replacement table will\n+ *   be committed upon a successful import. Then the original session catalog entry will be dropped\n+ *   and the new replacement table renamed to take its place.\n+ */\n+public class MigrateAction implements Action<Long> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  private static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  private static final String REPLACEMENT_NAME = \"_REPLACEMENT_\";\n+\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final CatalogTable sourceTable;\n+  private final String sourceTableLocation;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final Boolean sessionCatalogReplacement;\n+  private final SparkSessionCatalog destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private String destDataLocation;\n+  private String destMetadataLocation;\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  /**\n+   * Creates an Iceberg Location at a given location instead of using the location\n+   * provided by the source table. New metadata and data files will be added to this\n+   * new location and further operations will not effect the source table.\n+   *\n+   * Use this if you would like to experiment with Iceberg without changing\n+   * your original files.\n+   * @param newLocation the base directory for the new Iceberg Table\n+   * @return this for chaining\n+   */\n+  public MigrateAction withNewTableLocation(String newLocation) {\n+    this.destDataLocation = newLocation;\n+    this.destMetadataLocation = newLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+    return this;\n+  }\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with\n+   * the same key name will be overwritten.\n+   * @param properties a map of properties to be included\n+   * @return this for chaining\n+   */\n+  public MigrateAction withAdditionalProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  /**\n+   * Adds an additional property to the newly created Iceberg Table. Any properties\n+   * with the same key name will be overwritten.\n+   * @param key the key of the property to add\n+   * @param value the value of the property to add\n+   */\n+  public MigrateAction withAdditionalProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  public MigrateAction(SparkSession spark, Identifier destTableName, Identifier sourceTableName) {\n+    this.spark = spark;\n+    this.sourceTableName = sourceTableName;\n+    this.destTableName = destTableName;\n+    this.destCatalog = getSparkSessionCatalogOrFail(spark, destTableName);\n+\n+    try {\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable =\n+        spark.sessionState().catalog().getTableMetadata(Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (NoSuchTableException | NoSuchDatabaseException e) {\n+      throw new IllegalArgumentException(String.format(\"Could not find source table %s\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceTable, ALLOWED_SOURCES);\n+\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+    this.sessionCatalogReplacement = !sourceTableProvider.equals(\"iceberg\") && sourceTableName.equals(destTableName);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceTable.storage().locationUri().get());\n+    this.destDataLocation = sourceTableLocation;\n+    this.destMetadataLocation = sourceTableLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable, Set<String> supportedSourceTableProviders) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!supportedSourceTableProviders.contains(sourceTableProvider)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider));\n+    }\n+    if (sourceTable.storage().locationUri().isEmpty()) {\n+      throw new IllegalArgumentException(\"Cannot create an Iceberg table from a source without an explicit location\");\n+    }\n+  }\n+\n+  private static Map<String, String> extraIcebergTableProps(String tableLocation, String metadataLocation) {\n+    return ImmutableMap.of(\n+        TableProperties.WRITE_METADATA_LOCATION, metadataLocation,\n+        TableProperties.WRITE_NEW_DATA_LOCATION, tableLocation,\n+        \"migrated\", \"true\");\n+  }\n+\n+  private static SparkSessionCatalog getSparkSessionCatalogOrFail(\n+      SparkSession spark,\n+      Identifier destTableName) {\n+    CatalogPlugin sessionCat;\n+    String[] destNamespace = destTableName.namespace();\n+    if (destNamespace != null && destNamespace.length > 0 &&\n+        spark.sessionState().catalogManager().isCatalogRegistered(destNamespace[0])) {\n+      sessionCat = spark.sessionState().catalogManager().catalog(destTableName.namespace()[0]);\n+    } else {\n+      sessionCat = spark.sessionState().catalogManager().catalog(CatalogManager.SESSION_CATALOG_NAME());\n+    }\n+    if (!(sessionCat instanceof SparkSessionCatalog)) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non Iceberg Catalog. Catalog %s was of class\" +\n+          \" %s but %s is required\", sessionCat.name(), sessionCat.getClass(), SparkSessionCatalog.class.getName()));\n+    }\n+    return (SparkSessionCatalog) sessionCat;\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Map<String, String> newTableProperties = new ImmutableMap.Builder<String, String>()\n+        .put(TableCatalog.PROP_PROVIDER, \"iceberg\")\n+        .putAll(JavaConverters.mapAsJavaMapConverter(sourceTable.properties()).asJava())\n+        .putAll(extraIcebergTableProps(destDataLocation, destMetadataLocation))\n+        .putAll(additionalProperties)\n+        .build();\n+\n+    StagedTable stagedTable;\n+    try {\n+      if (sessionCatalogReplacement) {\n+        /*\n+         * Spark Session Catalog cannot stage a replacement of a Session table with an Iceberg Table.\n+         * To workaround this we create a replcamement table which is renamed after it\n+         * is successfully constructed.\n+         */\n+        stagedTable = destCatalog.stageCreateOrReplace(Identifier.of(\n+            destTableName.namespace(),\n+            destTableName.name() + REPLACEMENT_NAME),\n+            sourceTable.schema(),\n+            Spark3Util.toTransforms(sourcePartitionSpec), newTableProperties);\n+      } else {\n+        stagedTable = destCatalog.stageCreateOrReplace(destTableName, sourceTable.schema(),\n+            Spark3Util.toTransforms(sourcePartitionSpec), newTableProperties);\n+      }\n+    } catch (NoSuchNamespaceException e) {\n+      throw new IllegalArgumentException(\"Cannot create a new table in a namespace which does not exist\", e);\n+    }\n+\n+    String stagingLocation = destMetadataLocation;\n+    Table icebergTable = ((SparkTable) stagedTable).table();\n+\n+    LOG.info(\"Beginning migration of {} to {}\", sourceTableName, destTableName);\n+    SparkTableUtil.importSparkTable(spark, Spark3Util.toTableIdentifier(sourceTableName), icebergTable,\n+        stagingLocation);\n+\n+    Snapshot snapshot = icebergTable.currentSnapshot();\n+    long numMigratedFiles = Long.valueOf(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+\n+    try {\n+      stagedTable.commitStagedChanges();", "originalCommit": "a23fa89b2d5a2be61af72836656e4bf3322c9c39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI5NjYxNw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r504296617", "bodyText": "Sounds good, fixed", "author": "RussellSpitzer", "createdAt": "2020-10-13T22:33:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3NzQzMA=="}], "type": "inlineReview"}, {"oid": "85112ca46e2439b586b5970eb2f4bf09079eec24", "url": "https://github.com/apache/iceberg/commit/85112ca46e2439b586b5970eb2f4bf09079eec24", "message": "Fix tests\n\nHadoop Catalog may be fundamentally incompatible with our actions here\nsince it requires for both migrate and snapshot since it requires a specific\ndirectory structure matching with the identifier. It's also difficult to\nexplicitly detect when a Hadoop catalog is in use because that information\nis private in both of our Catalog Implementations.", "committedDate": "2020-10-13T22:31:12Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI5ODM4OQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r504298389", "bodyText": "Base class for Actions that are common to both Spark 2 and 3", "author": "RussellSpitzer", "createdAt": "2020-10-13T22:38:33Z", "path": "spark/src/main/java/org/apache/iceberg/actions/CommonActions.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import org.apache.iceberg.Table;\n+import org.apache.spark.sql.SparkSession;\n+\n+public abstract class CommonActions {\n+\n+  private SparkSession spark;\n+  private Table table;\n+\n+  protected CommonActions(SparkSession spark, Table table) {", "originalCommit": "85112ca46e2439b586b5970eb2f4bf09079eec24", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI5OTYxMg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r504299612", "bodyText": "This change has the unfortunate consequence that there is now an Actions in both Spark2 and Spark3 modules, which means that our abstract tests which use both have to delegate their \"Actions.forTable\" method to their implementations.", "author": "RussellSpitzer", "createdAt": "2020-10-13T22:42:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI5ODM4OQ=="}], "type": "inlineReview"}, {"oid": "d250de2c822bb43f16ff3c133fffe2e43c85d8d2", "url": "https://github.com/apache/iceberg/commit/d250de2c822bb43f16ff3c133fffe2e43c85d8d2", "message": "Fix tests\n\nHadoop Catalog may be fundamentally incompatible with our actions here\nsince it requires for both migrate and snapshot since it requires a specific\ndirectory structure matching with the identifier. It's also difficult to\nexplicitly detect when a Hadoop catalog is in use because that information\nis private in both of our Catalog Implementations.", "committedDate": "2020-10-13T22:39:59Z", "type": "forcePushed"}, {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8", "url": "https://github.com/apache/iceberg/commit/216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8", "message": "Implement new Actions on reflection based Actions", "committedDate": "2020-10-23T17:46:41Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY4OTY0MQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515689641", "bodyText": "Typo: Should be \"migrate\" not \"snapshot\".", "author": "rdblue", "createdAt": "2020-11-01T23:46:18Z", "path": "spark/src/main/java/org/apache/iceberg/actions/Actions.java", "diffHunk": "@@ -77,6 +82,81 @@ public ExpireSnapshotsAction expireSnapshots() {\n     return new ExpireSnapshotsAction(spark, table);\n   }\n \n+  /**\n+   * Converts the provided table into an Iceberg table in place. The table will no longer be represented by it's\n+   * previous provider in the session catalog and a new metadata directory will be created at the table's location.\n+   *\n+   * @param tableName Table to be converted\n+   * @return Action to perform migration\n+   */\n+  public static CreateAction migrate(String tableName) {\n+    try {\n+      return DynMethods.builder(\"migrate\")\n+          .impl(implClass(), String.class).buildStaticChecked()\n+          .invoke(tableName);\n+    } catch (NoSuchMethodException ex) {\n+      throw new UnsupportedOperationException(\"Snapshot is not implemented for this version of Spark\");", "originalCommit": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY4OTk1Nw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515689957", "bodyText": "The doc here is really specific to Spark, but I don't think there is a need for it. How about something like \"The table will no longer be accessible using the previous implementation\"?\nAlso, new data will be written to the data directory to avoid breaking any copies of the table. When we migrate, we leave a db.table_hive copy that can be renamed back in place to roll back the operation. The user is responsible for dropping the _hive copy or doing renames to rollback. Not sure how much of that applies to this PR though.", "author": "rdblue", "createdAt": "2020-11-01T23:49:37Z", "path": "spark/src/main/java/org/apache/iceberg/actions/Actions.java", "diffHunk": "@@ -77,6 +82,81 @@ public ExpireSnapshotsAction expireSnapshots() {\n     return new ExpireSnapshotsAction(spark, table);\n   }\n \n+  /**\n+   * Converts the provided table into an Iceberg table in place. The table will no longer be represented by it's\n+   * previous provider in the session catalog and a new metadata directory will be created at the table's location.", "originalCommit": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDczOTEyMQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r520739121", "bodyText": "I'll consider the fall back behavior in the action itself, I think we could leave another copy behind but I think that probably should be a user operation rather than a automatic part of migrate. I could be convinced otherwise though \ud83e\udd37", "author": "RussellSpitzer", "createdAt": "2020-11-10T17:25:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY4OTk1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjIzNzIzMw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536237233", "bodyText": "This seems to be still open.", "author": "aokolnychyi", "createdAt": "2020-12-04T16:51:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY4OTk1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI0NDYxMw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536244613", "bodyText": "woops only converted the lower one, this one is fixed too now", "author": "RussellSpitzer", "createdAt": "2020-12-04T17:02:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY4OTk1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MDA3Mg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515690072", "bodyText": "Same here, should be \"migrate\".", "author": "rdblue", "createdAt": "2020-11-01T23:50:28Z", "path": "spark/src/main/java/org/apache/iceberg/actions/Actions.java", "diffHunk": "@@ -77,6 +82,81 @@ public ExpireSnapshotsAction expireSnapshots() {\n     return new ExpireSnapshotsAction(spark, table);\n   }\n \n+  /**\n+   * Converts the provided table into an Iceberg table in place. The table will no longer be represented by it's\n+   * previous provider in the session catalog and a new metadata directory will be created at the table's location.\n+   *\n+   * @param tableName Table to be converted\n+   * @return Action to perform migration\n+   */\n+  public static CreateAction migrate(String tableName) {\n+    try {\n+      return DynMethods.builder(\"migrate\")\n+          .impl(implClass(), String.class).buildStaticChecked()\n+          .invoke(tableName);\n+    } catch (NoSuchMethodException ex) {\n+      throw new UnsupportedOperationException(\"Snapshot is not implemented for this version of Spark\");\n+    }\n+  }\n+\n+  /**\n+   * Converts the provided table into an Iceberg table in place. The table will no longer be represented by it's\n+   * previous provider in the session catalog and a new metadata directory will be created at the table's location.\n+   *\n+   * @param tableName Table to be converted\n+   * @param spark     Spark session to use for looking up table\n+   * @return Action to perform migration\n+   */\n+  public static CreateAction migrate(SparkSession spark, String tableName) {\n+    try {\n+      return DynMethods.builder(\"migrate\")\n+          .impl(implClass(), SparkSession.class, String.class).buildStaticChecked()\n+          .invoke(spark, tableName);\n+    } catch (NoSuchMethodException ex) {\n+      throw new UnsupportedOperationException(\"Snapshot is not implemented for this version of Spark\");", "originalCommit": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MDQxMg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515690412", "bodyText": "The wording in this Javadoc and the argument names seem a little confusing to me. I think it should be \"Creates a new Iceberg table that is a snapshot of the given source table\", then \"The new table can be altered, . . .\". Referring to the table that gets created as a table should help users understand what is happening a bit better than referring to it as a \"snapshot\", which has a conflicting meaning when used as a noun. Here, I think we want to stick to using it as a verb.\nSince the \"source\" and \"dest\" are tables, I'd use \"sourceName\" or \"sourceTable\" and \"destName\" or \"destTable\".", "author": "rdblue", "createdAt": "2020-11-01T23:54:05Z", "path": "spark/src/main/java/org/apache/iceberg/actions/Actions.java", "diffHunk": "@@ -77,6 +82,81 @@ public ExpireSnapshotsAction expireSnapshots() {\n     return new ExpireSnapshotsAction(spark, table);\n   }\n \n+  /**\n+   * Converts the provided table into an Iceberg table in place. The table will no longer be represented by it's\n+   * previous provider in the session catalog and a new metadata directory will be created at the table's location.\n+   *\n+   * @param tableName Table to be converted\n+   * @return Action to perform migration\n+   */\n+  public static CreateAction migrate(String tableName) {\n+    try {\n+      return DynMethods.builder(\"migrate\")\n+          .impl(implClass(), String.class).buildStaticChecked()\n+          .invoke(tableName);\n+    } catch (NoSuchMethodException ex) {\n+      throw new UnsupportedOperationException(\"Snapshot is not implemented for this version of Spark\");\n+    }\n+  }\n+\n+  /**\n+   * Converts the provided table into an Iceberg table in place. The table will no longer be represented by it's\n+   * previous provider in the session catalog and a new metadata directory will be created at the table's location.\n+   *\n+   * @param tableName Table to be converted\n+   * @param spark     Spark session to use for looking up table\n+   * @return Action to perform migration\n+   */\n+  public static CreateAction migrate(SparkSession spark, String tableName) {\n+    try {\n+      return DynMethods.builder(\"migrate\")\n+          .impl(implClass(), SparkSession.class, String.class).buildStaticChecked()\n+          .invoke(spark, tableName);\n+    } catch (NoSuchMethodException ex) {\n+      throw new UnsupportedOperationException(\"Snapshot is not implemented for this version of Spark\");\n+    }\n+  }\n+\n+  /**\n+   * Creates an independent Iceberg snapshot of a given table. The Snapshot can be altered, appended or deleted without\n+   * causing any change to the original table the snapshot is based on. New data and metadata will be created in the\n+   * location passed to this method.\n+   *\n+   * @param sourceId Snapshot's source data\n+   * @param destId   Name of the new snapshot", "originalCommit": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MDQ4Nw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515690487", "bodyText": "It may be helpful to link to the CreateAction API rather than just calling out that it is an Action.", "author": "rdblue", "createdAt": "2020-11-01T23:54:35Z", "path": "spark/src/main/java/org/apache/iceberg/actions/Actions.java", "diffHunk": "@@ -77,6 +82,81 @@ public ExpireSnapshotsAction expireSnapshots() {\n     return new ExpireSnapshotsAction(spark, table);\n   }\n \n+  /**\n+   * Converts the provided table into an Iceberg table in place. The table will no longer be represented by it's\n+   * previous provider in the session catalog and a new metadata directory will be created at the table's location.\n+   *\n+   * @param tableName Table to be converted\n+   * @return Action to perform migration\n+   */\n+  public static CreateAction migrate(String tableName) {\n+    try {\n+      return DynMethods.builder(\"migrate\")\n+          .impl(implClass(), String.class).buildStaticChecked()\n+          .invoke(tableName);\n+    } catch (NoSuchMethodException ex) {\n+      throw new UnsupportedOperationException(\"Snapshot is not implemented for this version of Spark\");\n+    }\n+  }\n+\n+  /**\n+   * Converts the provided table into an Iceberg table in place. The table will no longer be represented by it's\n+   * previous provider in the session catalog and a new metadata directory will be created at the table's location.\n+   *\n+   * @param tableName Table to be converted\n+   * @param spark     Spark session to use for looking up table\n+   * @return Action to perform migration\n+   */\n+  public static CreateAction migrate(SparkSession spark, String tableName) {\n+    try {\n+      return DynMethods.builder(\"migrate\")\n+          .impl(implClass(), SparkSession.class, String.class).buildStaticChecked()\n+          .invoke(spark, tableName);\n+    } catch (NoSuchMethodException ex) {\n+      throw new UnsupportedOperationException(\"Snapshot is not implemented for this version of Spark\");\n+    }\n+  }\n+\n+  /**\n+   * Creates an independent Iceberg snapshot of a given table. The Snapshot can be altered, appended or deleted without\n+   * causing any change to the original table the snapshot is based on. New data and metadata will be created in the\n+   * location passed to this method.\n+   *\n+   * @param sourceId Snapshot's source data\n+   * @param destId   Name of the new snapshot\n+   * @param location Location for metadata and new data for the Snapshot\n+   * @return Action to perform snapshot", "originalCommit": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MDgzMA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515690830", "bodyText": "Actions.migrate returns a CreateAction, but as seems to change the operation from an in-place migration to something unclear. Is the original table retained if the name doesn't conflict?\nSimilarly, I don't think there is a need for this with Actions.snapshot because snapshot accepts a destination table name. When I suggested as, my intent was to use it as a way to pass the destination table name for snapshot. But if you can't create a SnapshotAction without a table name we don't need this.", "author": "rdblue", "createdAt": "2020-11-01T23:57:46Z", "path": "spark/src/main/java/org/apache/iceberg/actions/CreateAction.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+\n+interface CreateAction extends Action<Long> {\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with\n+   * the same key name will be overwritten.\n+   * @param properties a map of properties to be included\n+   * @return this for chaining\n+   */\n+  CreateAction withAdditionalProperties(Map<String, String> properties);\n+\n+  /**\n+   * Adds an additional property to the newly created Iceberg Table. Any properties\n+   * with the same key name will be overwritten.\n+   * @param key the key of the property to add\n+   * @param value the value of the property to add\n+   * @return this for chaining\n+   */\n+  CreateAction withAdditionalProperty(String key, String value);\n+\n+  /**\n+   * Changes the name of the new Iceberg table we are making to the name passed\n+   * to this method.\n+   * @param newName The new name of the table to be made\n+   * @return this for chaining\n+   */\n+  CreateAction as(String newName);", "originalCommit": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDc0OTA0Mg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r520749042", "bodyText": "I can remove it, for snapshot it doesn't make sense but for Migrate it allowed you to use the same location for data as the original table but using a different catalog identifier, it wouldn't remove the old identifier if it didn't match. That may be of limited utility so I'll drop it.", "author": "RussellSpitzer", "createdAt": "2020-11-10T17:40:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MDgzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MDkyMQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515690921", "bodyText": "Why not set and setAll? Is \"additional\" more clear?", "author": "rdblue", "createdAt": "2020-11-01T23:58:46Z", "path": "spark/src/main/java/org/apache/iceberg/actions/CreateAction.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+\n+interface CreateAction extends Action<Long> {\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with\n+   * the same key name will be overwritten.\n+   * @param properties a map of properties to be included\n+   * @return this for chaining\n+   */\n+  CreateAction withAdditionalProperties(Map<String, String> properties);\n+\n+  /**\n+   * Adds an additional property to the newly created Iceberg Table. Any properties\n+   * with the same key name will be overwritten.\n+   * @param key the key of the property to add\n+   * @param value the value of the property to add\n+   * @return this for chaining\n+   */\n+  CreateAction withAdditionalProperty(String key, String value);", "originalCommit": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjEyNDQ4Mw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r516124483", "bodyText": "This was actually part of an earlier discussion I had with @aokolnychyi, We were discussing whether we should be preserving the properties set in the origin table for the operation. I had \"additional\" here because I settled on a behavior we had previously be using internally which copied over whatever properties the original source originally had.\nI can go back to \"set and setAll\" but I would like to hear your opinion on copying properties from the origins.", "author": "RussellSpitzer", "createdAt": "2020-11-02T17:10:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MDkyMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIxMDY0MA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r516210640", "bodyText": "I think both migrate and snapshot should copy everything from the original table, including properties. That's one reason why I like set: it makes no guarantees about the other table's properties, only that the given key/value will be set in the new table.", "author": "rdblue", "createdAt": "2020-11-02T19:39:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MDkyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MTMxOQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515691319", "bodyText": "Can we avoid Scala in this API to avoid breaking changes? I think converting to List immediately and then calling this method is going to be better.", "author": "rdblue", "createdAt": "2020-11-02T00:02:03Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -543,4 +550,63 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) {\n+    try {\n+      return catalogAndIdentifier(spark, spark.sessionState().sqlParser().parseMultipartIdentifier(name));\n+    } catch (ParseException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot parse identifier %s\", name), e);\n+    }\n+  }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param spark Spark session to use for resolution\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, Seq<String> nameParts) {", "originalCommit": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjEyNTA5Mg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r516125092", "bodyText": "Already upset about that Scala 2.13 change :) Yeah no problem, I'll stay Java only in message signatures with the exception of Spark Specific classes.", "author": "RussellSpitzer", "createdAt": "2020-11-02T17:11:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MTMxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MTU2Ng==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515691566", "bodyText": "Why is this needed?", "author": "rdblue", "createdAt": "2020-11-02T00:04:25Z", "path": "spark3/src/test/java/org/apache/iceberg/actions/TestCreateActions.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.source.SimpleRecord;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runners.Parameterized;\n+\n+public class TestCreateActions extends SparkCatalogTestBase {\n+\n+  // Only valid for IcebergV2Catalog - Hadoop Catalog does not support Staged Tables\n+  @Parameterized.Parameters(name = \"Catalog Name {0} - Options {2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"spark_catalog\", SparkSessionCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\",\n+            \"parquet-enabled\", \"true\",\n+            \"cache-enabled\", \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n+        )},\n+        new Object[] { \"testhive\", SparkCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\"\n+        )}\n+    };\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  String baseTableName = \"baseTable\";\n+  File tableDir;\n+  String tableLocation;\n+  final String implementation;\n+  final String type;\n+  final TableCatalog catalog;\n+\n+  public TestCreateActions(\n+      String catalogName,\n+      String implementation,\n+      Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+    this.catalog = (TableCatalog) spark.sessionState().catalogManager().catalog(catalogName);\n+    this.implementation = implementation;\n+    this.type = config.get(\"type\");\n+  }\n+\n+  @Before\n+  public void before() {\n+    try {\n+      this.tableDir = temp.newFolder();\n+    } catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+    this.tableLocation = tableDir.toURI().toString();\n+\n+    spark.conf().set(\"hive.exec.dynamic.partition\", \"true\");\n+    spark.conf().set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\");\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", baseTableName));\n+\n+\n+    List<SimpleRecord> expected = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\"),\n+        new SimpleRecord(2, \"b\"),\n+        new SimpleRecord(3, \"c\")\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, SimpleRecord.class);\n+\n+    df.select(\"id\", \"data\").orderBy(\"data\").write()\n+        .mode(\"append\")\n+        .option(\"path\", tableLocation)\n+        .saveAsTable(baseTableName);\n+  }\n+\n+  @After\n+  public void after() throws IOException {\n+    // Drop the hive table.\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", baseTableName));\n+  }\n+\n+\n+  @Test\n+  public void testMigratePartitioned() throws Exception {\n+    String dest = uniqueName(\"iceberg_migrate_partitioned\");\n+    String source = uniqueName(\"test_migrate_partitioned_table\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet PARTITIONED BY (id) location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.migrate(source).as(dest),\n+        3);\n+  }\n+\n+  @Test\n+  public void testMigrateUnpartitioned() throws Exception {\n+    String dest = uniqueName(\"iceberg_migrate_unpartitioned\");\n+    String source = uniqueName(\"test_migrate_unpartitioned_table\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.migrate(source).as(dest),\n+        2);\n+  }\n+\n+  @Test\n+  public void testMigrateReplace() throws Exception {\n+    // We can't do a replacement unless we have overridden the spark_catalog\n+    if (catalog.name().equals(\"spark_catalog\")) {\n+      String source = uniqueName(uniqueName(\"iceberg_migrate_replace\"));\n+      testCreate(source,\n+          source,\n+          \"CREATE TABLE %s using parquet PARTITIONED BY (id) location '%s' AS SELECT * FROM %s\",\n+          () -> Actions.migrate(source),\n+          3);\n+    }\n+  }\n+\n+  @Test\n+  public void testSnapshotPartitioned() throws Exception {\n+    File location = temp.newFolder();\n+    String dest = uniqueName(\"iceberg_snapshot_partitioned\");\n+    String source = uniqueName(\"test_snapshot_partitioned_table\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet PARTITIONED BY (id) location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.snapshot(source, dest, location.toString()),\n+        3);\n+    testIsolatedSnapshot(source, dest);\n+  }\n+\n+  @Test\n+  public void testSnapshotUnpartitioned() throws Exception {\n+    File location = temp.newFolder();\n+    String dest = uniqueName(\"iceberg_snapshot_unpartitioned\");\n+    String source = uniqueName(\"test_snapshot_unpartitioned_table\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.snapshot(source, dest, location.toString()),\n+        2);\n+    testIsolatedSnapshot(source, dest);\n+  }\n+\n+  @Test\n+  public void testSnapshotHiveTable() throws Exception {\n+    File location = temp.newFolder();\n+    String dest = uniqueName(\"iceberg_snapshot_hive_table\");\n+    String source = uniqueName(\"snapshot_hive_table\");\n+    testCreate(source,\n+        dest,\n+        String.format(\"CREATE EXTERNAL TABLE %s (id Int, data String) STORED AS parquet LOCATION '%s'\", source,\n+            tableLocation),\n+        () -> Actions.snapshot(source, dest, location.toString()),\n+        3);\n+    testIsolatedSnapshot(source, dest);\n+  }\n+\n+  @Test\n+  public void testProperties() throws Exception {\n+    String dest = uniqueName(\"iceberg_properties\");\n+    String source = uniqueName(\"test_properties_table\");\n+    Map<String, String> props = Maps.newHashMap();\n+    props.put(\"city\", \"New Orleans\");\n+    props.put(\"note\", \"Jazz\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.migrate(source).as(dest)\n+        .withAdditionalProperty(\"dogs\", \"sundance\")\n+        .withAdditionalProperties(props),\n+        2);\n+\n+    SparkTable table = loadTable(dest);\n+\n+\n+    Map<String, String> expectedProps = Maps.newHashMap();\n+    expectedProps.putAll(props);\n+    expectedProps.put(\"dogs\", \"sundance\");\n+\n+    for (Map.Entry<String, String> entry : expectedProps.entrySet()) {\n+      Assert.assertTrue(\n+          \"Created table missing property \" + entry.getKey(),\n+          table.properties().containsKey(entry.getKey()));\n+      Assert.assertEquals(\"Property value is not the expected value\",\n+          entry.getValue(), table.properties().get(entry.getKey()));\n+    }\n+  }\n+\n+  private SparkTable loadTable(String name) throws NoSuchTableException {\n+    return (SparkTable) catalog.loadTable(Spark3Util.catalogAndIdentifier(spark, name).identifier());\n+  }\n+\n+  // Creates a table, runs the migration command and checks the results.\n+  private void testCreate(String source, String dest, String sqlCreate, Supplier<CreateAction> action,\n+      long expectedMigratedFiles) throws\n+      Exception {\n+\n+    File location = temp.newFolder();\n+\n+    spark.sql(String.format(sqlCreate, source, location, baseTableName));\n+\n+    long migratedFiles = action.get().execute();\n+\n+    SparkTable table = loadTable(dest);\n+\n+    Assert.assertEquals(\"Provider should be iceberg\", \"iceberg\",\n+        table.properties().get(TableCatalog.PROP_PROVIDER));\n+    Assert.assertEquals(\"Expected number of migrated files\", expectedMigratedFiles, migratedFiles);\n+    Assert.assertEquals(\"Expected rows in table \", 3, spark.table(dest).count());\n+  }\n+\n+  // Inserts records into the destination, makes sure those records exist and source table is unchanged\n+  private void testIsolatedSnapshot(String source, String dest) {\n+    List<Row> expected = spark.sql(String.format(\"SELECT * FROM %s\", source)).collectAsList();\n+\n+    List<SimpleRecord> extraData = Lists.newArrayList(\n+        new SimpleRecord(4, \"d\")\n+    );\n+    Dataset<Row> df = spark.createDataFrame(extraData, SimpleRecord.class);\n+    df.write().format(\"iceberg\").mode(\"append\").saveAsTable(dest);\n+\n+    List<Row> result = spark.sql(String.format(\"SELECT * FROM %s\", source)).collectAsList();\n+    Assert.assertEquals(\"No additional rows should be added to the original table\", expected.size(),\n+        result.size());\n+\n+    List<Row> snapshot = spark.sql(String.format(\"SELECT * FROM %s WHERE id = 4 AND data = 'd'\", dest)).collectAsList();\n+    Assert.assertEquals(\"Added row not found in snapshot\", 1, snapshot.size());\n+  }\n+\n+  private String uniqueName(String source) {", "originalCommit": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjEzNDczNg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r516134736", "bodyText": "This is mostly because the tests take way too long when I was running them locally because of HiveMetastore retries. When every test had a unique name I didn't have to drop tables and I wouldn't have to wait for the metastore to check for 2 minutes to see whether the table existed when I called DROP IF EXISTS.\nThis is one of the reasons I brought up the retry length a while back, I just couldn't efficiently test this code on my local machine.", "author": "RussellSpitzer", "createdAt": "2020-11-02T17:22:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MTU2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIwOTc4OQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r516209789", "bodyText": "I don't think a DROP IF EXISTS should take 2 minutes with the retry. That sounds like something in the catalog is broken. The retry should only happen if the metadata file is known, but can't be loaded. In that case, the table does exist.\nMaybe the test before/after methods are dropping the temporary files before dropping the table?", "author": "rdblue", "createdAt": "2020-11-02T19:37:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MTU2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MTc3NQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515691775", "bodyText": "What is IcebergV2Catalog? It looks like SparkCatalog supports staged tables with both Hadoop and Hive catalogs.", "author": "rdblue", "createdAt": "2020-11-02T00:06:32Z", "path": "spark3/src/test/java/org/apache/iceberg/actions/TestCreateActions.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.source.SimpleRecord;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runners.Parameterized;\n+\n+public class TestCreateActions extends SparkCatalogTestBase {\n+\n+  // Only valid for IcebergV2Catalog - Hadoop Catalog does not support Staged Tables", "originalCommit": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjEzMTM4OQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r516131389", "bodyText": "Yeah I can probably allow this and test it better. I was running into issues because the HadoopOps fails if you manually specify a location for metadata or data since that violates the table structure. Since the current implementation always manually specifies these locations it breaks with the Hadoop Catalog if you don't pick out the exact right location when snapshotting.\nLet me go back and see if I can make that less confusing and support the Hadoop backed Catalog better", "author": "RussellSpitzer", "createdAt": "2020-11-02T17:18:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MTc3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIwOTE0Mw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r516209143", "bodyText": "It sounds like creating a snapshot table should just use the new table's default location rather than requiring a location. Why did you choose to require a location?", "author": "rdblue", "createdAt": "2020-11-02T19:36:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MTc3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDU1MjQ2NQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r524552465", "bodyText": "Just had an internal user asking the same thing, gonna allow both.", "author": "RussellSpitzer", "createdAt": "2020-11-16T20:28:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MTc3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MTgwMw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515691803", "bodyText": "Are these private?", "author": "rdblue", "createdAt": "2020-11-02T00:06:43Z", "path": "spark3/src/test/java/org/apache/iceberg/actions/TestCreateActions.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.source.SimpleRecord;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runners.Parameterized;\n+\n+public class TestCreateActions extends SparkCatalogTestBase {\n+\n+  // Only valid for IcebergV2Catalog - Hadoop Catalog does not support Staged Tables\n+  @Parameterized.Parameters(name = \"Catalog Name {0} - Options {2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"spark_catalog\", SparkSessionCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\",\n+            \"parquet-enabled\", \"true\",\n+            \"cache-enabled\", \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n+        )},\n+        new Object[] { \"testhive\", SparkCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\"\n+        )}\n+    };\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  String baseTableName = \"baseTable\";", "originalCommit": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MTg1MQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515691851", "bodyText": "Nit: extra newline", "author": "rdblue", "createdAt": "2020-11-02T00:07:03Z", "path": "spark3/src/test/java/org/apache/iceberg/actions/TestCreateActions.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.source.SimpleRecord;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runners.Parameterized;\n+\n+public class TestCreateActions extends SparkCatalogTestBase {\n+\n+  // Only valid for IcebergV2Catalog - Hadoop Catalog does not support Staged Tables\n+  @Parameterized.Parameters(name = \"Catalog Name {0} - Options {2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"spark_catalog\", SparkSessionCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\",\n+            \"parquet-enabled\", \"true\",\n+            \"cache-enabled\", \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n+        )},\n+        new Object[] { \"testhive\", SparkCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\"\n+        )}\n+    };\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  String baseTableName = \"baseTable\";\n+  File tableDir;\n+  String tableLocation;\n+  final String implementation;\n+  final String type;\n+  final TableCatalog catalog;\n+\n+  public TestCreateActions(\n+      String catalogName,\n+      String implementation,\n+      Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+    this.catalog = (TableCatalog) spark.sessionState().catalogManager().catalog(catalogName);\n+    this.implementation = implementation;\n+    this.type = config.get(\"type\");\n+  }\n+\n+  @Before\n+  public void before() {\n+    try {\n+      this.tableDir = temp.newFolder();\n+    } catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+    this.tableLocation = tableDir.toURI().toString();\n+\n+    spark.conf().set(\"hive.exec.dynamic.partition\", \"true\");\n+    spark.conf().set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\");\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", baseTableName));\n+\n+", "originalCommit": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MTk1MQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515691951", "bodyText": "Can you use Assume for this instead of if? That way it shows up as a skipped case.", "author": "rdblue", "createdAt": "2020-11-02T00:08:17Z", "path": "spark3/src/test/java/org/apache/iceberg/actions/TestCreateActions.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.source.SimpleRecord;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runners.Parameterized;\n+\n+public class TestCreateActions extends SparkCatalogTestBase {\n+\n+  // Only valid for IcebergV2Catalog - Hadoop Catalog does not support Staged Tables\n+  @Parameterized.Parameters(name = \"Catalog Name {0} - Options {2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"spark_catalog\", SparkSessionCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\",\n+            \"parquet-enabled\", \"true\",\n+            \"cache-enabled\", \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n+        )},\n+        new Object[] { \"testhive\", SparkCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\"\n+        )}\n+    };\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  String baseTableName = \"baseTable\";\n+  File tableDir;\n+  String tableLocation;\n+  final String implementation;\n+  final String type;\n+  final TableCatalog catalog;\n+\n+  public TestCreateActions(\n+      String catalogName,\n+      String implementation,\n+      Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+    this.catalog = (TableCatalog) spark.sessionState().catalogManager().catalog(catalogName);\n+    this.implementation = implementation;\n+    this.type = config.get(\"type\");\n+  }\n+\n+  @Before\n+  public void before() {\n+    try {\n+      this.tableDir = temp.newFolder();\n+    } catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+    this.tableLocation = tableDir.toURI().toString();\n+\n+    spark.conf().set(\"hive.exec.dynamic.partition\", \"true\");\n+    spark.conf().set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\");\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", baseTableName));\n+\n+\n+    List<SimpleRecord> expected = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\"),\n+        new SimpleRecord(2, \"b\"),\n+        new SimpleRecord(3, \"c\")\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, SimpleRecord.class);\n+\n+    df.select(\"id\", \"data\").orderBy(\"data\").write()\n+        .mode(\"append\")\n+        .option(\"path\", tableLocation)\n+        .saveAsTable(baseTableName);\n+  }\n+\n+  @After\n+  public void after() throws IOException {\n+    // Drop the hive table.\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", baseTableName));\n+  }\n+\n+\n+  @Test\n+  public void testMigratePartitioned() throws Exception {\n+    String dest = uniqueName(\"iceberg_migrate_partitioned\");\n+    String source = uniqueName(\"test_migrate_partitioned_table\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet PARTITIONED BY (id) location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.migrate(source).as(dest),\n+        3);\n+  }\n+\n+  @Test\n+  public void testMigrateUnpartitioned() throws Exception {\n+    String dest = uniqueName(\"iceberg_migrate_unpartitioned\");\n+    String source = uniqueName(\"test_migrate_unpartitioned_table\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.migrate(source).as(dest),\n+        2);\n+  }\n+\n+  @Test\n+  public void testMigrateReplace() throws Exception {\n+    // We can't do a replacement unless we have overridden the spark_catalog\n+    if (catalog.name().equals(\"spark_catalog\")) {", "originalCommit": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjEzMTk5Mw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r516131993", "bodyText": "Yep, just learned about Assume last week. Will implement it here!", "author": "RussellSpitzer", "createdAt": "2020-11-02T17:19:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MTk1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MjgyNQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515692825", "bodyText": "I like the code reuse that these tests have, but I don't think that the pattern of a single method like this is very readable. For example, it isn't clear what expectedMigratedFiles is until you read this, but it differs between test cases.\nI think a better pattern for reuse is to use separate methods that are well named. These tests have a great example of what I'm talking about with testIsolatedSnapshot. You could add a boolean to testCreate for whether this method should call testIsolatedSnapshot, but it is more readable to simply put the call in the test case directly.\nI think a similar option, assertMigratedFileCount, would be an improvement over passing an unlabelled number. Similarly, building a method to create and initialize the source table with a name rather than passing a create statement would help readability in the tests.", "author": "rdblue", "createdAt": "2020-11-02T00:15:41Z", "path": "spark3/src/test/java/org/apache/iceberg/actions/TestCreateActions.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.source.SimpleRecord;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runners.Parameterized;\n+\n+public class TestCreateActions extends SparkCatalogTestBase {\n+\n+  // Only valid for IcebergV2Catalog - Hadoop Catalog does not support Staged Tables\n+  @Parameterized.Parameters(name = \"Catalog Name {0} - Options {2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"spark_catalog\", SparkSessionCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\",\n+            \"parquet-enabled\", \"true\",\n+            \"cache-enabled\", \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n+        )},\n+        new Object[] { \"testhive\", SparkCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\"\n+        )}\n+    };\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  String baseTableName = \"baseTable\";\n+  File tableDir;\n+  String tableLocation;\n+  final String implementation;\n+  final String type;\n+  final TableCatalog catalog;\n+\n+  public TestCreateActions(\n+      String catalogName,\n+      String implementation,\n+      Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+    this.catalog = (TableCatalog) spark.sessionState().catalogManager().catalog(catalogName);\n+    this.implementation = implementation;\n+    this.type = config.get(\"type\");\n+  }\n+\n+  @Before\n+  public void before() {\n+    try {\n+      this.tableDir = temp.newFolder();\n+    } catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+    this.tableLocation = tableDir.toURI().toString();\n+\n+    spark.conf().set(\"hive.exec.dynamic.partition\", \"true\");\n+    spark.conf().set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\");\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", baseTableName));\n+\n+\n+    List<SimpleRecord> expected = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\"),\n+        new SimpleRecord(2, \"b\"),\n+        new SimpleRecord(3, \"c\")\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, SimpleRecord.class);\n+\n+    df.select(\"id\", \"data\").orderBy(\"data\").write()\n+        .mode(\"append\")\n+        .option(\"path\", tableLocation)\n+        .saveAsTable(baseTableName);\n+  }\n+\n+  @After\n+  public void after() throws IOException {\n+    // Drop the hive table.\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", baseTableName));\n+  }\n+\n+\n+  @Test\n+  public void testMigratePartitioned() throws Exception {\n+    String dest = uniqueName(\"iceberg_migrate_partitioned\");\n+    String source = uniqueName(\"test_migrate_partitioned_table\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet PARTITIONED BY (id) location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.migrate(source).as(dest),\n+        3);\n+  }\n+\n+  @Test\n+  public void testMigrateUnpartitioned() throws Exception {\n+    String dest = uniqueName(\"iceberg_migrate_unpartitioned\");\n+    String source = uniqueName(\"test_migrate_unpartitioned_table\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.migrate(source).as(dest),\n+        2);\n+  }\n+\n+  @Test\n+  public void testMigrateReplace() throws Exception {\n+    // We can't do a replacement unless we have overridden the spark_catalog\n+    if (catalog.name().equals(\"spark_catalog\")) {\n+      String source = uniqueName(uniqueName(\"iceberg_migrate_replace\"));\n+      testCreate(source,\n+          source,\n+          \"CREATE TABLE %s using parquet PARTITIONED BY (id) location '%s' AS SELECT * FROM %s\",\n+          () -> Actions.migrate(source),\n+          3);\n+    }\n+  }\n+\n+  @Test\n+  public void testSnapshotPartitioned() throws Exception {\n+    File location = temp.newFolder();\n+    String dest = uniqueName(\"iceberg_snapshot_partitioned\");\n+    String source = uniqueName(\"test_snapshot_partitioned_table\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet PARTITIONED BY (id) location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.snapshot(source, dest, location.toString()),\n+        3);\n+    testIsolatedSnapshot(source, dest);\n+  }\n+\n+  @Test\n+  public void testSnapshotUnpartitioned() throws Exception {\n+    File location = temp.newFolder();\n+    String dest = uniqueName(\"iceberg_snapshot_unpartitioned\");\n+    String source = uniqueName(\"test_snapshot_unpartitioned_table\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.snapshot(source, dest, location.toString()),\n+        2);\n+    testIsolatedSnapshot(source, dest);\n+  }\n+\n+  @Test\n+  public void testSnapshotHiveTable() throws Exception {\n+    File location = temp.newFolder();\n+    String dest = uniqueName(\"iceberg_snapshot_hive_table\");\n+    String source = uniqueName(\"snapshot_hive_table\");\n+    testCreate(source,\n+        dest,\n+        String.format(\"CREATE EXTERNAL TABLE %s (id Int, data String) STORED AS parquet LOCATION '%s'\", source,\n+            tableLocation),\n+        () -> Actions.snapshot(source, dest, location.toString()),\n+        3);\n+    testIsolatedSnapshot(source, dest);\n+  }\n+\n+  @Test\n+  public void testProperties() throws Exception {\n+    String dest = uniqueName(\"iceberg_properties\");\n+    String source = uniqueName(\"test_properties_table\");\n+    Map<String, String> props = Maps.newHashMap();\n+    props.put(\"city\", \"New Orleans\");\n+    props.put(\"note\", \"Jazz\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.migrate(source).as(dest)\n+        .withAdditionalProperty(\"dogs\", \"sundance\")\n+        .withAdditionalProperties(props),\n+        2);\n+\n+    SparkTable table = loadTable(dest);\n+\n+\n+    Map<String, String> expectedProps = Maps.newHashMap();\n+    expectedProps.putAll(props);\n+    expectedProps.put(\"dogs\", \"sundance\");\n+\n+    for (Map.Entry<String, String> entry : expectedProps.entrySet()) {\n+      Assert.assertTrue(\n+          \"Created table missing property \" + entry.getKey(),\n+          table.properties().containsKey(entry.getKey()));\n+      Assert.assertEquals(\"Property value is not the expected value\",\n+          entry.getValue(), table.properties().get(entry.getKey()));\n+    }\n+  }\n+\n+  private SparkTable loadTable(String name) throws NoSuchTableException {\n+    return (SparkTable) catalog.loadTable(Spark3Util.catalogAndIdentifier(spark, name).identifier());\n+  }\n+\n+  // Creates a table, runs the migration command and checks the results.\n+  private void testCreate(String source, String dest, String sqlCreate, Supplier<CreateAction> action,\n+      long expectedMigratedFiles) throws", "originalCommit": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjEzMjY3MA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r516132670", "bodyText": "Sgtm", "author": "RussellSpitzer", "createdAt": "2020-11-02T17:20:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MjgyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MzM3Mg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515693372", "bodyText": "Session catalog?\nDo we have a way around this restriction? In our implementation, we load the source table using our Hive or Spark table implementation and check that it is what we expect. Then we use that implementation to load the partitions. Would we similarly require a v2 table implementation to make this catalog agnostic?\n(This isn't a blocker, just curious to hear your ideas.)", "author": "rdblue", "createdAt": "2020-11-02T00:20:08Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,290 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import jline.internal.Log;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This action will migrate a known table in a Spark Catalog that is not an Iceberg table into an Iceberg table.\n+ * The created new table will be able to interact with and modify files in the original table.\n+ *\n+ * There are two main code paths\n+ *   - Creating a brand new iceberg table or replacing an existing Iceberg table\n+ *   This pathway will use a staged table to stage the creation or replacement, only committing after\n+ *   import has succeeded.\n+ *\n+ *   - Replacing a table in the Session Catalog with an Iceberg Table with the same name\n+ *   This pathway will create a temporary table with a different name. This replacement table will\n+ *   be committed upon a successful import. Then the original session catalog entry will be dropped\n+ *   and the new replacement table renamed to take its place.\n+ */\n+class Spark3CreateAction implements CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3CreateAction.class);\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  private static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  private static final String REPLACEMENT_NAME = \"_REPLACEMENT_\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final CatalogTable sourceTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final Boolean sessionCatalogReplacement;\n+  private final CatalogPlugin destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private String destDataLocation;\n+  private String destMetadataLocation;\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName,\n+                       CatalogPlugin destCatalog,  Identifier destTableName) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableName = sourceTableName;\n+    this.destCatalog = destCatalog;\n+    this.destTableName = destTableName;\n+\n+    try {\n+      String sourceString = String.join(\".\", sourceTableName.namespace()) + \".\" + sourceTableName.name();\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, sourceString);\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable = spark.sessionState().catalog().getTableMetadata(Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (NoSuchTableException | NoSuchDatabaseException e) {\n+      throw new IllegalArgumentException(String.format(\"Could not find source table %s\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceTable, ALLOWED_SOURCES);\n+\n+    this.sessionCatalogReplacement = isSessionCatalogReplacement();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceTable.storage().locationUri().get());\n+    this.destDataLocation = sourceTableLocation;\n+    this.destMetadataLocation = sourceTableLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+  }\n+\n+  private boolean isSessionCatalogReplacement() {\n+    boolean sourceIceberg = sourceTable.provider().get().toLowerCase(Locale.ROOT).equals(\"iceberg\");\n+    boolean sameCatalog = sourceCatalog == destCatalog;\n+    boolean sameIdentifier = sourceTableName.name().equals(destTableName.name()) &&\n+        Arrays.equals(sourceTableName.namespace(), destTableName.namespace());\n+    return !sourceIceberg && sameCatalog && sameIdentifier;\n+  }\n+\n+\n+  /**\n+   * Creates the Iceberg data and metadata at a given location instead of the source table\n+   * location. New metadata and data files will be added to this\n+   * new location and further operations will not effect the source table.\n+   *\n+   * Use this if you would like to experiment with Iceberg without changing\n+   * your original files.\n+   * @param newLocation the base directory for the new Iceberg Table\n+   * @return this for chaining\n+   */\n+  CreateAction asSnapshotAtLocation(String newLocation) {\n+    this.destDataLocation = newLocation;\n+    this.destMetadataLocation = newLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+    return this;\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagingTableCatalog stagingCatalog = checkDestinationCatalog(destCatalog);\n+    Map<String, String> newTableProperties = new ImmutableMap.Builder<String, String>()\n+        .put(TableCatalog.PROP_PROVIDER, \"iceberg\")\n+        .putAll(JavaConverters.mapAsJavaMapConverter(sourceTable.properties()).asJava())\n+        .putAll(extraIcebergTableProps(destDataLocation, destMetadataLocation))\n+        .putAll(additionalProperties)\n+        .build();\n+\n+    StagedTable stagedTable;\n+    try {\n+      if (sessionCatalogReplacement) {\n+        /*\n+         * Spark Session Catalog cannot stage a replacement of a Session table with an Iceberg Table.\n+         * To workaround this we create a replacement table which is renamed after it\n+         * is successfully constructed.\n+         */\n+        stagedTable = stagingCatalog.stageCreate(Identifier.of(\n+            destTableName.namespace(),\n+            destTableName.name() + REPLACEMENT_NAME),\n+            sourceTable.schema(),\n+            Spark3Util.toTransforms(sourcePartitionSpec), newTableProperties);\n+      } else {\n+        stagedTable = stagingCatalog.stageCreate(destTableName, sourceTable.schema(),\n+            Spark3Util.toTransforms(sourcePartitionSpec), newTableProperties);\n+      }\n+    } catch (NoSuchNamespaceException e) {\n+      throw new IllegalArgumentException(\"Cannot create a new table in a namespace which does not exist\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Destination table already exists\", e);\n+    }\n+\n+    String stagingLocation = destMetadataLocation;\n+    Table icebergTable = ((SparkTable) stagedTable).table();\n+\n+    LOG.info(\"Beginning migration of {} to {}\", sourceTableName, destTableName);\n+    long numMigratedFiles = 0;\n+    try {\n+      SparkTableUtil.importSparkTable(spark, Spark3Util.toTableIdentifier(sourceTableName), icebergTable,\n+          stagingLocation);\n+\n+      Snapshot snapshot = icebergTable.currentSnapshot();\n+      numMigratedFiles = Long.valueOf(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+\n+      stagedTable.commitStagedChanges();\n+      LOG.info(\"Successfully loaded Iceberg metadata for {} files\", numMigratedFiles);\n+    } catch (Exception e) {\n+      LOG.error(\"Error when attempting to commit migration changes, rolling back\", e);\n+      stagedTable.abortStagedChanges();\n+      throw e;\n+    }\n+\n+    if (sessionCatalogReplacement) {\n+      Identifier replacementTable = Identifier.of(destTableName.namespace(), destTableName.name() + REPLACEMENT_NAME);\n+      try {\n+        stagingCatalog.dropTable(destTableName);\n+        stagingCatalog.renameTable(replacementTable, destTableName);\n+      } catch (NoSuchTableException e) {\n+        LOG.error(\"Cannot migrate, replacement table is missing. Attempting to recreate source table\", e);\n+        try {\n+          stagingCatalog.createTable(sourceTableName, sourceTable.schema(),\n+              Spark3Util.toTransforms(sourcePartitionSpec), JavaConverters.mapAsJavaMap(sourceTable.properties()));\n+        } catch (TableAlreadyExistsException tableAlreadyExistsException) {\n+          Log.error(\"Cannot recreate source table. Source table has already been recreated\", e);\n+          throw new RuntimeException(e);\n+        } catch (NoSuchNamespaceException noSuchNamespaceException) {\n+          Log.error(\"Cannot recreate source table. Source namespace has been removed, cannot recreate\", e);\n+          throw new RuntimeException(e);\n+        }\n+      } catch (TableAlreadyExistsException e) {\n+        Log.error(\"Cannot migrate, Source table was recreated before replacement could be moved. \" +\n+            \"Attempting to remove replacement table.\", e);\n+        stagingCatalog.dropTable(replacementTable);\n+        stagedTable.abortStagedChanges();\n+      }\n+    }\n+\n+    return numMigratedFiles;\n+  }\n+\n+  @Override\n+  public CreateAction withAdditionalProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withAdditionalProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction as(String newName) {\n+    Spark3Util.CatalogAndIdentifier newDest = Spark3Util.catalogAndIdentifier(spark, newName);\n+    return new Spark3CreateAction(spark, sourceCatalog, sourceTableName, newDest.catalog(), newDest.identifier())\n+        .withAdditionalProperties(this.additionalProperties);\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable, Set<String> supportedSourceTableProviders) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!supportedSourceTableProviders.contains(sourceTableProvider)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider));\n+    }\n+    if (sourceTable.storage().locationUri().isEmpty()) {\n+      throw new IllegalArgumentException(\"Cannot create an Iceberg table from a source without an explicit location\");\n+    }\n+  }\n+\n+  private static Map<String, String> extraIcebergTableProps(String tableLocation, String metadataLocation) {\n+    return ImmutableMap.of(\n+        TableProperties.WRITE_METADATA_LOCATION, metadataLocation,\n+        TableProperties.WRITE_NEW_DATA_LOCATION, tableLocation,\n+        \"migrated\", \"true\");\n+  }\n+\n+  private static StagingTableCatalog checkDestinationCatalog(CatalogPlugin catalog) {\n+    if (!(catalog instanceof SparkSessionCatalog) && !(catalog instanceof SparkCatalog)) {\n+      throw new IllegalArgumentException(String.format(\"Cannot create Iceberg table in non Iceberg Catalog. \" +\n+              \"Catalog %s was of class %s but %s or %s are required\", catalog.name(), catalog.getClass(),\n+          SparkSessionCatalog.class.getName(), SparkCatalog.class.getName()));\n+    }\n+    return (StagingTableCatalog) catalog;\n+  }\n+\n+  private CatalogPlugin checkSourceCatalog(CatalogPlugin catalog) {\n+    // Currently the Import code relies on being able to look up the table in the session code", "originalCommit": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjEyODY4OQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r516128689", "bodyText": "The limitation here is based on how the current SparkUtil.import code is written. It can't handle DSV2 catalogs or things like that so I thought this would be the easiest approach for now.\nI think what we should do is modify that utility function to be less SparkCatalog specific, or at least have ways of calling it that describe a table based on partitions (like in your code) and other properties directly rather than assuming it is a \"table name\" that can be looked up in the catalog.\nThere other approach I was thinking about was writing a version of the function which is just all in on Spark3 and uses the CatalogV2 table api. We already have a few discussions going on in issues about how we are going to deal with this for other Actions which all have similar limitations at the moment.", "author": "RussellSpitzer", "createdAt": "2020-11-02T17:15:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MzM3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwMjMwNA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r526502304", "bodyText": "I think we can change SparkTableUtil if we need to. Can be done later, though.", "author": "aokolnychyi", "createdAt": "2020-11-18T23:59:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MzM3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5Mzc3Mw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515693773", "bodyText": "This excludes Hive tables that don't use Spark's provider, including those created with STORED AS parquet instead of USING parquet. I don't think that is necessary. Isn't the only requirement that all of the partitions are a supported format?", "author": "rdblue", "createdAt": "2020-11-02T00:23:53Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,290 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import jline.internal.Log;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This action will migrate a known table in a Spark Catalog that is not an Iceberg table into an Iceberg table.\n+ * The created new table will be able to interact with and modify files in the original table.\n+ *\n+ * There are two main code paths\n+ *   - Creating a brand new iceberg table or replacing an existing Iceberg table\n+ *   This pathway will use a staged table to stage the creation or replacement, only committing after\n+ *   import has succeeded.\n+ *\n+ *   - Replacing a table in the Session Catalog with an Iceberg Table with the same name\n+ *   This pathway will create a temporary table with a different name. This replacement table will\n+ *   be committed upon a successful import. Then the original session catalog entry will be dropped\n+ *   and the new replacement table renamed to take its place.\n+ */\n+class Spark3CreateAction implements CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3CreateAction.class);\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  private static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  private static final String REPLACEMENT_NAME = \"_REPLACEMENT_\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final CatalogTable sourceTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final Boolean sessionCatalogReplacement;\n+  private final CatalogPlugin destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private String destDataLocation;\n+  private String destMetadataLocation;\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName,\n+                       CatalogPlugin destCatalog,  Identifier destTableName) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableName = sourceTableName;\n+    this.destCatalog = destCatalog;\n+    this.destTableName = destTableName;\n+\n+    try {\n+      String sourceString = String.join(\".\", sourceTableName.namespace()) + \".\" + sourceTableName.name();\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, sourceString);\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable = spark.sessionState().catalog().getTableMetadata(Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (NoSuchTableException | NoSuchDatabaseException e) {\n+      throw new IllegalArgumentException(String.format(\"Could not find source table %s\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceTable, ALLOWED_SOURCES);\n+\n+    this.sessionCatalogReplacement = isSessionCatalogReplacement();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceTable.storage().locationUri().get());\n+    this.destDataLocation = sourceTableLocation;\n+    this.destMetadataLocation = sourceTableLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+  }\n+\n+  private boolean isSessionCatalogReplacement() {\n+    boolean sourceIceberg = sourceTable.provider().get().toLowerCase(Locale.ROOT).equals(\"iceberg\");\n+    boolean sameCatalog = sourceCatalog == destCatalog;\n+    boolean sameIdentifier = sourceTableName.name().equals(destTableName.name()) &&\n+        Arrays.equals(sourceTableName.namespace(), destTableName.namespace());\n+    return !sourceIceberg && sameCatalog && sameIdentifier;\n+  }\n+\n+\n+  /**\n+   * Creates the Iceberg data and metadata at a given location instead of the source table\n+   * location. New metadata and data files will be added to this\n+   * new location and further operations will not effect the source table.\n+   *\n+   * Use this if you would like to experiment with Iceberg without changing\n+   * your original files.\n+   * @param newLocation the base directory for the new Iceberg Table\n+   * @return this for chaining\n+   */\n+  CreateAction asSnapshotAtLocation(String newLocation) {\n+    this.destDataLocation = newLocation;\n+    this.destMetadataLocation = newLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+    return this;\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagingTableCatalog stagingCatalog = checkDestinationCatalog(destCatalog);\n+    Map<String, String> newTableProperties = new ImmutableMap.Builder<String, String>()\n+        .put(TableCatalog.PROP_PROVIDER, \"iceberg\")\n+        .putAll(JavaConverters.mapAsJavaMapConverter(sourceTable.properties()).asJava())\n+        .putAll(extraIcebergTableProps(destDataLocation, destMetadataLocation))\n+        .putAll(additionalProperties)\n+        .build();\n+\n+    StagedTable stagedTable;\n+    try {\n+      if (sessionCatalogReplacement) {\n+        /*\n+         * Spark Session Catalog cannot stage a replacement of a Session table with an Iceberg Table.\n+         * To workaround this we create a replacement table which is renamed after it\n+         * is successfully constructed.\n+         */\n+        stagedTable = stagingCatalog.stageCreate(Identifier.of(\n+            destTableName.namespace(),\n+            destTableName.name() + REPLACEMENT_NAME),\n+            sourceTable.schema(),\n+            Spark3Util.toTransforms(sourcePartitionSpec), newTableProperties);\n+      } else {\n+        stagedTable = stagingCatalog.stageCreate(destTableName, sourceTable.schema(),\n+            Spark3Util.toTransforms(sourcePartitionSpec), newTableProperties);\n+      }\n+    } catch (NoSuchNamespaceException e) {\n+      throw new IllegalArgumentException(\"Cannot create a new table in a namespace which does not exist\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Destination table already exists\", e);\n+    }\n+\n+    String stagingLocation = destMetadataLocation;\n+    Table icebergTable = ((SparkTable) stagedTable).table();\n+\n+    LOG.info(\"Beginning migration of {} to {}\", sourceTableName, destTableName);\n+    long numMigratedFiles = 0;\n+    try {\n+      SparkTableUtil.importSparkTable(spark, Spark3Util.toTableIdentifier(sourceTableName), icebergTable,\n+          stagingLocation);\n+\n+      Snapshot snapshot = icebergTable.currentSnapshot();\n+      numMigratedFiles = Long.valueOf(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+\n+      stagedTable.commitStagedChanges();\n+      LOG.info(\"Successfully loaded Iceberg metadata for {} files\", numMigratedFiles);\n+    } catch (Exception e) {\n+      LOG.error(\"Error when attempting to commit migration changes, rolling back\", e);\n+      stagedTable.abortStagedChanges();\n+      throw e;\n+    }\n+\n+    if (sessionCatalogReplacement) {\n+      Identifier replacementTable = Identifier.of(destTableName.namespace(), destTableName.name() + REPLACEMENT_NAME);\n+      try {\n+        stagingCatalog.dropTable(destTableName);\n+        stagingCatalog.renameTable(replacementTable, destTableName);\n+      } catch (NoSuchTableException e) {\n+        LOG.error(\"Cannot migrate, replacement table is missing. Attempting to recreate source table\", e);\n+        try {\n+          stagingCatalog.createTable(sourceTableName, sourceTable.schema(),\n+              Spark3Util.toTransforms(sourcePartitionSpec), JavaConverters.mapAsJavaMap(sourceTable.properties()));\n+        } catch (TableAlreadyExistsException tableAlreadyExistsException) {\n+          Log.error(\"Cannot recreate source table. Source table has already been recreated\", e);\n+          throw new RuntimeException(e);\n+        } catch (NoSuchNamespaceException noSuchNamespaceException) {\n+          Log.error(\"Cannot recreate source table. Source namespace has been removed, cannot recreate\", e);\n+          throw new RuntimeException(e);\n+        }\n+      } catch (TableAlreadyExistsException e) {\n+        Log.error(\"Cannot migrate, Source table was recreated before replacement could be moved. \" +\n+            \"Attempting to remove replacement table.\", e);\n+        stagingCatalog.dropTable(replacementTable);\n+        stagedTable.abortStagedChanges();\n+      }\n+    }\n+\n+    return numMigratedFiles;\n+  }\n+\n+  @Override\n+  public CreateAction withAdditionalProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withAdditionalProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction as(String newName) {\n+    Spark3Util.CatalogAndIdentifier newDest = Spark3Util.catalogAndIdentifier(spark, newName);\n+    return new Spark3CreateAction(spark, sourceCatalog, sourceTableName, newDest.catalog(), newDest.identifier())\n+        .withAdditionalProperties(this.additionalProperties);\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable, Set<String> supportedSourceTableProviders) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);", "originalCommit": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDM4NzkxOA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r524387918", "bodyText": "Yeah let me double check this.", "author": "RussellSpitzer", "createdAt": "2020-11-16T16:13:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5Mzc3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDU1MzA0MQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r524553041", "bodyText": "So this is ok, Managed tables return a provider of hive.", "author": "RussellSpitzer", "createdAt": "2020-11-16T20:30:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5Mzc3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDU1MzE1NQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r524553155", "bodyText": "Also added managed hive table tests", "author": "RussellSpitzer", "createdAt": "2020-11-16T20:30:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5Mzc3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5NDAwMw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515694003", "bodyText": "Should this use a static set of table providers instead of passing them in?", "author": "rdblue", "createdAt": "2020-11-02T00:25:33Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,290 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import jline.internal.Log;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This action will migrate a known table in a Spark Catalog that is not an Iceberg table into an Iceberg table.\n+ * The created new table will be able to interact with and modify files in the original table.\n+ *\n+ * There are two main code paths\n+ *   - Creating a brand new iceberg table or replacing an existing Iceberg table\n+ *   This pathway will use a staged table to stage the creation or replacement, only committing after\n+ *   import has succeeded.\n+ *\n+ *   - Replacing a table in the Session Catalog with an Iceberg Table with the same name\n+ *   This pathway will create a temporary table with a different name. This replacement table will\n+ *   be committed upon a successful import. Then the original session catalog entry will be dropped\n+ *   and the new replacement table renamed to take its place.\n+ */\n+class Spark3CreateAction implements CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3CreateAction.class);\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  private static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  private static final String REPLACEMENT_NAME = \"_REPLACEMENT_\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final CatalogTable sourceTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final Boolean sessionCatalogReplacement;\n+  private final CatalogPlugin destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private String destDataLocation;\n+  private String destMetadataLocation;\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName,\n+                       CatalogPlugin destCatalog,  Identifier destTableName) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableName = sourceTableName;\n+    this.destCatalog = destCatalog;\n+    this.destTableName = destTableName;\n+\n+    try {\n+      String sourceString = String.join(\".\", sourceTableName.namespace()) + \".\" + sourceTableName.name();\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, sourceString);\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable = spark.sessionState().catalog().getTableMetadata(Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (NoSuchTableException | NoSuchDatabaseException e) {\n+      throw new IllegalArgumentException(String.format(\"Could not find source table %s\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceTable, ALLOWED_SOURCES);\n+\n+    this.sessionCatalogReplacement = isSessionCatalogReplacement();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceTable.storage().locationUri().get());\n+    this.destDataLocation = sourceTableLocation;\n+    this.destMetadataLocation = sourceTableLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+  }\n+\n+  private boolean isSessionCatalogReplacement() {\n+    boolean sourceIceberg = sourceTable.provider().get().toLowerCase(Locale.ROOT).equals(\"iceberg\");\n+    boolean sameCatalog = sourceCatalog == destCatalog;\n+    boolean sameIdentifier = sourceTableName.name().equals(destTableName.name()) &&\n+        Arrays.equals(sourceTableName.namespace(), destTableName.namespace());\n+    return !sourceIceberg && sameCatalog && sameIdentifier;\n+  }\n+\n+\n+  /**\n+   * Creates the Iceberg data and metadata at a given location instead of the source table\n+   * location. New metadata and data files will be added to this\n+   * new location and further operations will not effect the source table.\n+   *\n+   * Use this if you would like to experiment with Iceberg without changing\n+   * your original files.\n+   * @param newLocation the base directory for the new Iceberg Table\n+   * @return this for chaining\n+   */\n+  CreateAction asSnapshotAtLocation(String newLocation) {\n+    this.destDataLocation = newLocation;\n+    this.destMetadataLocation = newLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+    return this;\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagingTableCatalog stagingCatalog = checkDestinationCatalog(destCatalog);\n+    Map<String, String> newTableProperties = new ImmutableMap.Builder<String, String>()\n+        .put(TableCatalog.PROP_PROVIDER, \"iceberg\")\n+        .putAll(JavaConverters.mapAsJavaMapConverter(sourceTable.properties()).asJava())\n+        .putAll(extraIcebergTableProps(destDataLocation, destMetadataLocation))\n+        .putAll(additionalProperties)\n+        .build();\n+\n+    StagedTable stagedTable;\n+    try {\n+      if (sessionCatalogReplacement) {\n+        /*\n+         * Spark Session Catalog cannot stage a replacement of a Session table with an Iceberg Table.\n+         * To workaround this we create a replacement table which is renamed after it\n+         * is successfully constructed.\n+         */\n+        stagedTable = stagingCatalog.stageCreate(Identifier.of(\n+            destTableName.namespace(),\n+            destTableName.name() + REPLACEMENT_NAME),\n+            sourceTable.schema(),\n+            Spark3Util.toTransforms(sourcePartitionSpec), newTableProperties);\n+      } else {\n+        stagedTable = stagingCatalog.stageCreate(destTableName, sourceTable.schema(),\n+            Spark3Util.toTransforms(sourcePartitionSpec), newTableProperties);\n+      }\n+    } catch (NoSuchNamespaceException e) {\n+      throw new IllegalArgumentException(\"Cannot create a new table in a namespace which does not exist\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Destination table already exists\", e);\n+    }\n+\n+    String stagingLocation = destMetadataLocation;\n+    Table icebergTable = ((SparkTable) stagedTable).table();\n+\n+    LOG.info(\"Beginning migration of {} to {}\", sourceTableName, destTableName);\n+    long numMigratedFiles = 0;\n+    try {\n+      SparkTableUtil.importSparkTable(spark, Spark3Util.toTableIdentifier(sourceTableName), icebergTable,\n+          stagingLocation);\n+\n+      Snapshot snapshot = icebergTable.currentSnapshot();\n+      numMigratedFiles = Long.valueOf(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+\n+      stagedTable.commitStagedChanges();\n+      LOG.info(\"Successfully loaded Iceberg metadata for {} files\", numMigratedFiles);\n+    } catch (Exception e) {\n+      LOG.error(\"Error when attempting to commit migration changes, rolling back\", e);\n+      stagedTable.abortStagedChanges();\n+      throw e;\n+    }\n+\n+    if (sessionCatalogReplacement) {\n+      Identifier replacementTable = Identifier.of(destTableName.namespace(), destTableName.name() + REPLACEMENT_NAME);\n+      try {\n+        stagingCatalog.dropTable(destTableName);\n+        stagingCatalog.renameTable(replacementTable, destTableName);\n+      } catch (NoSuchTableException e) {\n+        LOG.error(\"Cannot migrate, replacement table is missing. Attempting to recreate source table\", e);\n+        try {\n+          stagingCatalog.createTable(sourceTableName, sourceTable.schema(),\n+              Spark3Util.toTransforms(sourcePartitionSpec), JavaConverters.mapAsJavaMap(sourceTable.properties()));\n+        } catch (TableAlreadyExistsException tableAlreadyExistsException) {\n+          Log.error(\"Cannot recreate source table. Source table has already been recreated\", e);\n+          throw new RuntimeException(e);\n+        } catch (NoSuchNamespaceException noSuchNamespaceException) {\n+          Log.error(\"Cannot recreate source table. Source namespace has been removed, cannot recreate\", e);\n+          throw new RuntimeException(e);\n+        }\n+      } catch (TableAlreadyExistsException e) {\n+        Log.error(\"Cannot migrate, Source table was recreated before replacement could be moved. \" +\n+            \"Attempting to remove replacement table.\", e);\n+        stagingCatalog.dropTable(replacementTable);\n+        stagedTable.abortStagedChanges();\n+      }\n+    }\n+\n+    return numMigratedFiles;\n+  }\n+\n+  @Override\n+  public CreateAction withAdditionalProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withAdditionalProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction as(String newName) {\n+    Spark3Util.CatalogAndIdentifier newDest = Spark3Util.catalogAndIdentifier(spark, newName);\n+    return new Spark3CreateAction(spark, sourceCatalog, sourceTableName, newDest.catalog(), newDest.identifier())\n+        .withAdditionalProperties(this.additionalProperties);\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable, Set<String> supportedSourceTableProviders) {", "originalCommit": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDM4NzQ0Mg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r524387442", "bodyText": "Yep, switching.", "author": "RussellSpitzer", "createdAt": "2020-11-16T16:12:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5NDAwMw=="}], "type": "inlineReview"}, {"oid": "683b7e5265adf67b1e24a25c008e2f289f1d97ad", "url": "https://github.com/apache/iceberg/commit/683b7e5265adf67b1e24a25c008e2f289f1d97ad", "message": "Provide API and Implementation for Creating Iceberg Tables from Spark\n\nPreviously the SparkUtil class provided an importSparkTable command but\nthis command suffered from a few shortcomings. It had a difficult api and\ndid not work directly with DSV2 Iceberg Catalogs. To provide a simpler interface\nthat shares a similar pattern to currently available Spark Actions we introduce\nActions.migrate and Actions.snapshot.\n\nThrough these commands we can both migrate a Table as well as make a snapshot\nof an existing table.", "committedDate": "2020-11-17T18:30:39Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ1NzY1MQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r526457651", "bodyText": "Well, it seems we rely too much on reflection here. When we decided to use reflection to split actions, we did not have these static methods and it was reasonable since the scope of the change was smaller compared to introducing BaseActions. I am no longer sure reflection is a good idea here as making these methods work with reflection is more complicated than having BaseActions. On top, we don't have compile time checks. Users will call these methods in Spark 2 and will get runtime exceptions. I'd prefer to not expose those methods at all.", "author": "aokolnychyi", "createdAt": "2020-11-18T22:12:53Z", "path": "spark/src/main/java/org/apache/iceberg/actions/Actions.java", "diffHunk": "@@ -77,6 +82,120 @@ public ExpireSnapshotsAction expireSnapshots() {\n     return new ExpireSnapshotsAction(spark, table);\n   }\n \n+  /**\n+   * Converts the provided table into an Iceberg table in place. The table will no longer be represented by it's\n+   * previous provider in the session catalog and a new metadata directory will be created at the table's location.\n+   *\n+   * @param tableName Table to be converted\n+   * @return {@link CreateAction} to perform migration\n+   */\n+  public static CreateAction migrate(String tableName) {", "originalCommit": "683b7e5265adf67b1e24a25c008e2f289f1d97ad", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ1ODUxMg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r526458512", "bodyText": "What are your thoughts on this, @RussellSpitzer @rdblue?\nI think it is not too late to reconsider this.", "author": "aokolnychyi", "createdAt": "2020-11-18T22:14:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ1NzY1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ2NTYxNQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r526465615", "bodyText": "I think not implementing these in Spark 2 is definitely a reason to reconsider this.\nWhat about somewhere in the middle? We could introduce an API that isn't static, then call its methods from the static ones here. Then we just need an implementation class, which we could load dynamically.", "author": "rdblue", "createdAt": "2020-11-18T22:28:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ1NzY1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3ODQ1Mg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r526478452", "bodyText": "I still prefer having BaseActions with an Actions per module. I think having two classes each which determine their implementation at runtime is not much better than all the static method reflection currently in the PR. The user experience would be worse, since there is a weird middle method to call, and we still would have runtime exceptions for bad method calls.\nMaybe I don't understand the API that isn't static plan.", "author": "RussellSpitzer", "createdAt": "2020-11-18T22:56:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ1NzY1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjIzODkxOA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536238918", "bodyText": "I think the last time we chatted having SparkActions or moving current actions to org.apache.iceberg.spark was one of the most promising ideas due to its simplicity.\nI agree it is not something we should address in this PR but I'd try to solve it before 0.11.", "author": "aokolnychyi", "createdAt": "2020-12-04T16:53:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ1NzY1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3ODk3MA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r526478970", "bodyText": "If we don't assign an explicit location while snapshotting, how are we going to validate the new location is different compared to the existing table location?", "author": "aokolnychyi", "createdAt": "2020-11-18T22:57:14Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import jline.internal.Log;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This action will migrate a known table in a Spark Catalog that is not an Iceberg table into an Iceberg table.\n+ * The created new table will be able to interact with and modify files in the original table.\n+ *\n+ * There are two main code paths\n+ *   - Creating a brand new iceberg table or replacing an existing Iceberg table\n+ *   This pathway will use a staged table to stage the creation or replacement, only committing after\n+ *   import has succeeded.\n+ *\n+ *   - Replacing a table in the Session Catalog with an Iceberg Table of the same name.\n+ *   This pathway will first create a temporary table with a different name. This replacement table will\n+ *   be committed upon a successful import. Then the original session catalog entry will be dropped\n+ *   and the new replacement table renamed to take its place.\n+ */\n+class Spark3CreateAction implements CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3CreateAction.class);\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  private static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  private static final String REPLACEMENT_NAME = \"_REPLACEMENT_\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final CatalogTable sourceTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final Boolean sessionCatalogReplacement;\n+  private final CatalogPlugin destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private String destDataLocation;\n+  private String destMetadataLocation;\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName,\n+                       CatalogPlugin destCatalog,  Identifier destTableName) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableName = sourceTableName;\n+    this.destCatalog = destCatalog;\n+    this.destTableName = destTableName;\n+\n+    try {\n+      String sourceString = String.join(\".\", sourceTableName.namespace()) + \".\" + sourceTableName.name();\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, sourceString);\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable = spark.sessionState().catalog().getTableMetadata(Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (NoSuchTableException | NoSuchDatabaseException e) {\n+      throw new IllegalArgumentException(String.format(\"Could not find source table %s\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceTable);\n+\n+    this.sessionCatalogReplacement = isSessionCatalogReplacement();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceTable.storage().locationUri().get());\n+    this.destDataLocation = sourceTableLocation;\n+    this.destMetadataLocation = sourceTableLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+  }\n+\n+  private boolean isSessionCatalogReplacement() {\n+    boolean sourceIceberg = sourceTable.provider().get().toLowerCase(Locale.ROOT).equals(\"iceberg\");\n+    boolean sameCatalog = sourceCatalog == destCatalog;\n+    boolean sameIdentifier = sourceTableName.name().equals(destTableName.name()) &&\n+        Arrays.equals(sourceTableName.namespace(), destTableName.namespace());\n+    return !sourceIceberg && sameCatalog && sameIdentifier;\n+  }\n+\n+\n+  /**\n+   * Creates the Iceberg data and metadata at a given location instead of the source table\n+   * location. New metadata and data files will be added to this\n+   * new location and further operations will not effect the source table.\n+   *\n+   * @param newLocation the base directory for the new Iceberg Table\n+   * @return this for chaining\n+   */\n+  CreateAction asSnapshotAtLocation(String newLocation) {\n+    Preconditions.checkArgument(!newLocation.equals(sourceTableLocation), \"Cannot create a snapshot with the\" +\n+        \"same data location as the source table. To place new files in the source table directory use the migrate \" +\n+        \"command.\");\n+    this.destDataLocation = newLocation;\n+    this.destMetadataLocation = newLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+    return this;\n+  }\n+\n+  /**\n+   * Creates the Iceberg data and metadata at the catalog default location for the\n+   * new table.\n+   *\n+   * @return this for chaining\n+   */\n+  CreateAction asSnapshotAtDefaultLocation() {", "originalCommit": "683b7e5265adf67b1e24a25c008e2f289f1d97ad", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ5MTc1Mw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r526491753", "bodyText": "I don't think there is a way we can actually know what the Metastore does, so there is a possibility that a user's default location could be the same", "author": "RussellSpitzer", "createdAt": "2020-11-18T23:31:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3ODk3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU1Mjk5Nw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r526552997", "bodyText": "I feel like asking for an explicit location in case of snapshot is safer but I can be convinced otherwise.", "author": "aokolnychyi", "createdAt": "2020-11-19T02:32:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3ODk3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3OTM1OQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r526479359", "bodyText": "I am not sure this is correct. Shouldn't we be assigning the table location, not data location?", "author": "aokolnychyi", "createdAt": "2020-11-18T22:58:08Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import jline.internal.Log;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This action will migrate a known table in a Spark Catalog that is not an Iceberg table into an Iceberg table.\n+ * The created new table will be able to interact with and modify files in the original table.\n+ *\n+ * There are two main code paths\n+ *   - Creating a brand new iceberg table or replacing an existing Iceberg table\n+ *   This pathway will use a staged table to stage the creation or replacement, only committing after\n+ *   import has succeeded.\n+ *\n+ *   - Replacing a table in the Session Catalog with an Iceberg Table of the same name.\n+ *   This pathway will first create a temporary table with a different name. This replacement table will\n+ *   be committed upon a successful import. Then the original session catalog entry will be dropped\n+ *   and the new replacement table renamed to take its place.\n+ */\n+class Spark3CreateAction implements CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3CreateAction.class);\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  private static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  private static final String REPLACEMENT_NAME = \"_REPLACEMENT_\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final CatalogTable sourceTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final Boolean sessionCatalogReplacement;\n+  private final CatalogPlugin destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private String destDataLocation;\n+  private String destMetadataLocation;\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName,\n+                       CatalogPlugin destCatalog,  Identifier destTableName) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableName = sourceTableName;\n+    this.destCatalog = destCatalog;\n+    this.destTableName = destTableName;\n+\n+    try {\n+      String sourceString = String.join(\".\", sourceTableName.namespace()) + \".\" + sourceTableName.name();\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, sourceString);\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable = spark.sessionState().catalog().getTableMetadata(Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (NoSuchTableException | NoSuchDatabaseException e) {\n+      throw new IllegalArgumentException(String.format(\"Could not find source table %s\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceTable);\n+\n+    this.sessionCatalogReplacement = isSessionCatalogReplacement();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceTable.storage().locationUri().get());\n+    this.destDataLocation = sourceTableLocation;\n+    this.destMetadataLocation = sourceTableLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+  }\n+\n+  private boolean isSessionCatalogReplacement() {\n+    boolean sourceIceberg = sourceTable.provider().get().toLowerCase(Locale.ROOT).equals(\"iceberg\");\n+    boolean sameCatalog = sourceCatalog == destCatalog;\n+    boolean sameIdentifier = sourceTableName.name().equals(destTableName.name()) &&\n+        Arrays.equals(sourceTableName.namespace(), destTableName.namespace());\n+    return !sourceIceberg && sameCatalog && sameIdentifier;\n+  }\n+\n+\n+  /**\n+   * Creates the Iceberg data and metadata at a given location instead of the source table\n+   * location. New metadata and data files will be added to this\n+   * new location and further operations will not effect the source table.\n+   *\n+   * @param newLocation the base directory for the new Iceberg Table\n+   * @return this for chaining\n+   */\n+  CreateAction asSnapshotAtLocation(String newLocation) {\n+    Preconditions.checkArgument(!newLocation.equals(sourceTableLocation), \"Cannot create a snapshot with the\" +\n+        \"same data location as the source table. To place new files in the source table directory use the migrate \" +\n+        \"command.\");\n+    this.destDataLocation = newLocation;", "originalCommit": "683b7e5265adf67b1e24a25c008e2f289f1d97ad", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwMTI3Mg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r526501272", "bodyText": "We have to check both the data and metadata locations eventually to ensure users are not changing it through table properties.", "author": "aokolnychyi", "createdAt": "2020-11-18T23:56:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3OTM1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ5NDIwNw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r526494207", "bodyText": "setAll?", "author": "aokolnychyi", "createdAt": "2020-11-18T23:37:35Z", "path": "spark/src/main/java/org/apache/iceberg/actions/CreateAction.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+\n+interface CreateAction extends Action<Long> {\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with\n+   * the same key name will be overwritten.\n+   * @param properties a map of properties to be included\n+   * @return this for chaining\n+   */\n+  CreateAction set(Map<String, String> properties);", "originalCommit": "683b7e5265adf67b1e24a25c008e2f289f1d97ad", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwMTg2Mg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r526501862", "bodyText": "Is it enough to check if it is instance of BaseCatalog we introduced recently?", "author": "aokolnychyi", "createdAt": "2020-11-18T23:57:56Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import jline.internal.Log;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This action will migrate a known table in a Spark Catalog that is not an Iceberg table into an Iceberg table.\n+ * The created new table will be able to interact with and modify files in the original table.\n+ *\n+ * There are two main code paths\n+ *   - Creating a brand new iceberg table or replacing an existing Iceberg table\n+ *   This pathway will use a staged table to stage the creation or replacement, only committing after\n+ *   import has succeeded.\n+ *\n+ *   - Replacing a table in the Session Catalog with an Iceberg Table of the same name.\n+ *   This pathway will first create a temporary table with a different name. This replacement table will\n+ *   be committed upon a successful import. Then the original session catalog entry will be dropped\n+ *   and the new replacement table renamed to take its place.\n+ */\n+class Spark3CreateAction implements CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3CreateAction.class);\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  private static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  private static final String REPLACEMENT_NAME = \"_REPLACEMENT_\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final CatalogTable sourceTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final Boolean sessionCatalogReplacement;\n+  private final CatalogPlugin destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private String destDataLocation;\n+  private String destMetadataLocation;\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName,\n+                       CatalogPlugin destCatalog,  Identifier destTableName) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableName = sourceTableName;\n+    this.destCatalog = destCatalog;\n+    this.destTableName = destTableName;\n+\n+    try {\n+      String sourceString = String.join(\".\", sourceTableName.namespace()) + \".\" + sourceTableName.name();\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, sourceString);\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable = spark.sessionState().catalog().getTableMetadata(Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (NoSuchTableException | NoSuchDatabaseException e) {\n+      throw new IllegalArgumentException(String.format(\"Could not find source table %s\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceTable);\n+\n+    this.sessionCatalogReplacement = isSessionCatalogReplacement();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceTable.storage().locationUri().get());\n+    this.destDataLocation = sourceTableLocation;\n+    this.destMetadataLocation = sourceTableLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+  }\n+\n+  private boolean isSessionCatalogReplacement() {\n+    boolean sourceIceberg = sourceTable.provider().get().toLowerCase(Locale.ROOT).equals(\"iceberg\");\n+    boolean sameCatalog = sourceCatalog == destCatalog;\n+    boolean sameIdentifier = sourceTableName.name().equals(destTableName.name()) &&\n+        Arrays.equals(sourceTableName.namespace(), destTableName.namespace());\n+    return !sourceIceberg && sameCatalog && sameIdentifier;\n+  }\n+\n+\n+  /**\n+   * Creates the Iceberg data and metadata at a given location instead of the source table\n+   * location. New metadata and data files will be added to this\n+   * new location and further operations will not effect the source table.\n+   *\n+   * @param newLocation the base directory for the new Iceberg Table\n+   * @return this for chaining\n+   */\n+  CreateAction asSnapshotAtLocation(String newLocation) {\n+    Preconditions.checkArgument(!newLocation.equals(sourceTableLocation), \"Cannot create a snapshot with the\" +\n+        \"same data location as the source table. To place new files in the source table directory use the migrate \" +\n+        \"command.\");\n+    this.destDataLocation = newLocation;\n+    this.destMetadataLocation = newLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+    return this;\n+  }\n+\n+  /**\n+   * Creates the Iceberg data and metadata at the catalog default location for the\n+   * new table.\n+   *\n+   * @return this for chaining\n+   */\n+  CreateAction asSnapshotAtDefaultLocation() {\n+    this.destDataLocation = null;\n+    this.destMetadataLocation = null;\n+    return this;\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagingTableCatalog stagingCatalog = checkDestinationCatalog(destCatalog);\n+    Map<String, String> newTableProperties = new ImmutableMap.Builder<String, String>()\n+        .put(TableCatalog.PROP_PROVIDER, \"iceberg\")\n+        .putAll(JavaConverters.mapAsJavaMapConverter(sourceTable.properties()).asJava())\n+        .putAll(extraIcebergTableProps(destDataLocation, destMetadataLocation))\n+        .putAll(additionalProperties)\n+        .build();\n+\n+    StagedTable stagedTable;\n+    try {\n+      if (sessionCatalogReplacement) {\n+        /*\n+         * Spark Session Catalog cannot stage a replacement of a Session table with an Iceberg Table.\n+         * To workaround this we create a replacement table which is renamed after it\n+         * is successfully constructed.\n+         */\n+        stagedTable = stagingCatalog.stageCreate(Identifier.of(\n+            destTableName.namespace(),\n+            destTableName.name() + REPLACEMENT_NAME),\n+            sourceTable.schema(),\n+            Spark3Util.toTransforms(sourcePartitionSpec), newTableProperties);\n+      } else {\n+        stagedTable = stagingCatalog.stageCreate(destTableName, sourceTable.schema(),\n+            Spark3Util.toTransforms(sourcePartitionSpec), newTableProperties);\n+      }\n+    } catch (NoSuchNamespaceException e) {\n+      throw new IllegalArgumentException(\"Cannot create a new table in a namespace which does not exist\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Destination table already exists\", e);\n+    }\n+\n+    Table icebergTable = ((SparkTable) stagedTable).table();\n+\n+    String stagingLocation;\n+    if (destMetadataLocation != null) {\n+      stagingLocation = destMetadataLocation;\n+    } else {\n+      stagingLocation = ((SparkTable) stagedTable).table().location() + \"/\" + ICEBERG_METADATA_FOLDER;\n+    }\n+\n+    LOG.info(\"Beginning migration of {} to {} using metadata location {}\", sourceTableName, destTableName,\n+        stagingLocation);\n+\n+    long numMigratedFiles;\n+    try {\n+      SparkTableUtil.importSparkTable(spark, Spark3Util.toTableIdentifier(sourceTableName), icebergTable,\n+          stagingLocation);\n+\n+      Snapshot snapshot = icebergTable.currentSnapshot();\n+      numMigratedFiles = Long.valueOf(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+\n+      stagedTable.commitStagedChanges();\n+      LOG.info(\"Successfully loaded Iceberg metadata for {} files\", numMigratedFiles);\n+    } catch (Exception e) {\n+      LOG.error(\"Error when attempting to commit migration changes, rolling back\", e);\n+      stagedTable.abortStagedChanges();\n+      throw e;\n+    }\n+\n+    if (sessionCatalogReplacement) {\n+      Identifier replacementTable = Identifier.of(destTableName.namespace(), destTableName.name() + REPLACEMENT_NAME);\n+      try {\n+        stagingCatalog.dropTable(destTableName);\n+        stagingCatalog.renameTable(replacementTable, destTableName);\n+      } catch (NoSuchTableException e) {\n+        LOG.error(\"Cannot migrate, replacement table is missing. Attempting to recreate source table\", e);\n+        try {\n+          stagingCatalog.createTable(sourceTableName, sourceTable.schema(),\n+              Spark3Util.toTransforms(sourcePartitionSpec), JavaConverters.mapAsJavaMap(sourceTable.properties()));\n+        } catch (TableAlreadyExistsException tableAlreadyExistsException) {\n+          Log.error(\"Cannot recreate source table. Source table has already been recreated\", e);\n+          throw new RuntimeException(e);\n+        } catch (NoSuchNamespaceException noSuchNamespaceException) {\n+          Log.error(\"Cannot recreate source table. Source namespace has been removed, cannot recreate\", e);\n+          throw new RuntimeException(e);\n+        }\n+      } catch (TableAlreadyExistsException e) {\n+        Log.error(\"Cannot migrate, Source table was recreated before replacement could be moved. \" +\n+            \"Attempting to remove replacement table.\", e);\n+        stagingCatalog.dropTable(replacementTable);\n+        stagedTable.abortStagedChanges();\n+      }\n+    }\n+\n+    return numMigratedFiles;\n+  }\n+\n+  @Override\n+  public CreateAction set(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction set(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!ALLOWED_SOURCES.contains(sourceTableProvider)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider));\n+    }\n+    if (sourceTable.storage().locationUri().isEmpty()) {\n+      throw new IllegalArgumentException(\"Cannot create an Iceberg table from a source without an explicit location\");\n+    }\n+  }\n+\n+  private static Map<String, String> extraIcebergTableProps(String tableLocation, String metadataLocation) {\n+    if (tableLocation != null && metadataLocation != null) {\n+      return ImmutableMap.of(\n+          TableProperties.WRITE_METADATA_LOCATION, metadataLocation,\n+          TableProperties.WRITE_NEW_DATA_LOCATION, tableLocation,\n+          \"migrated\", \"true\"\n+      );\n+    } else {\n+      return ImmutableMap.of(\"migrated\", \"true\");\n+    }\n+  }\n+\n+  private static StagingTableCatalog checkDestinationCatalog(CatalogPlugin catalog) {\n+    if (!(catalog instanceof SparkSessionCatalog) && !(catalog instanceof SparkCatalog)) {", "originalCommit": "683b7e5265adf67b1e24a25c008e2f289f1d97ad", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwMjIxNQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r526502215", "bodyText": "Yep, Sorry this PR has been in progress for a very long time, I haven't kept up with all the other changes.", "author": "RussellSpitzer", "createdAt": "2020-11-18T23:58:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwMTg2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTYzMDQxNQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r531630415", "bodyText": "We can't actually do this with the current package structure since baseCatalog is package private in Spark and this is in Actions. We could always move spark's action package though", "author": "RussellSpitzer", "createdAt": "2020-11-27T14:24:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwMTg2Mg=="}], "type": "inlineReview"}, {"oid": "9d521f386c5cd6ca1e5b8b70d2c5cb411eddcae9", "url": "https://github.com/apache/iceberg/commit/9d521f386c5cd6ca1e5b8b70d2c5cb411eddcae9", "message": "Get all Tests Working Again", "committedDate": "2020-11-25T18:24:03Z", "type": "forcePushed"}, {"oid": "2e6e20a19fd81569bd296799b19b6b5ae612d19d", "url": "https://github.com/apache/iceberg/commit/2e6e20a19fd81569bd296799b19b6b5ae612d19d", "message": "Fixup from Rebase", "committedDate": "2020-11-28T17:49:28Z", "type": "forcePushed"}, {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "url": "https://github.com/apache/iceberg/commit/09f329bc39e8a2a1b2b5f925c580a2177833510b", "message": "Fix Source Catalog Requirements", "committedDate": "2020-12-03T19:58:57Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc3NzcyOA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535777728", "bodyText": "specForTable is a bit hacky. At a minimum, it seems strange that this would create a string table name just for specForTable to split on . immediately.\nSince the source table is loaded as a v1 table just after this, why not use the schema and partition fields from the CatalogTable instead?\nWe should probably also deprecate specForTable if we are going to maintain this, since this can be a much better utility for conversion.", "author": "rdblue", "createdAt": "2020-12-04T01:54:05Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName,\n+                       CatalogPlugin destCatalog,  Identifier destTableName) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableName = sourceTableName;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableName = destTableName;\n+\n+    try {\n+      String sourceString = String.join(\".\", sourceTableName.namespace()) + \".\" + sourceTableName.name();\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, sourceString);", "originalCommit": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI0ODE3Nw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536248177", "bodyText": "This isn't actually used, I forgot to remove it, we now use the Transforms out of the V1Table representation", "author": "RussellSpitzer", "createdAt": "2020-12-04T17:10:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc3NzcyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc3ODUzMg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535778532", "bodyText": "Why not use TableCatalog?", "author": "rdblue", "createdAt": "2020-12-04T01:56:24Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName,\n+                       CatalogPlugin destCatalog,  Identifier destTableName) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableName = sourceTableName;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableName = destTableName;\n+\n+    try {\n+      String sourceString = String.join(\".\", sourceTableName.namespace()) + \".\" + sourceTableName.name();\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, sourceString);\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable = (V1Table) ((TableCatalog) sourceCatalog).loadTable(sourceTableName);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableName), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use a non-v1 table %s as a source\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceCatalogTable);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction setAll(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction set(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected CatalogPlugin sourceCatalog() {", "originalCommit": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI1MDU1Ng==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536250556", "bodyText": "Sure we can do that", "author": "RussellSpitzer", "createdAt": "2020-12-04T17:14:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc3ODUzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI3MDg2MA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536270860", "bodyText": "+1", "author": "aokolnychyi", "createdAt": "2020-12-04T17:47:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc3ODUzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc3OTAwNA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535779004", "bodyText": "How is hive handled? Do we just assume that the partitions are a supported format?", "author": "rdblue", "createdAt": "2020-12-04T01:57:38Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");", "originalCommit": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI2OTc2Mw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536269763", "bodyText": "I think our SparkUtil handles it while getting partitions.\n    Preconditions.checkArgument(locationUri.nonEmpty(), \"Partition URI should be defined\");\n    Preconditions.checkArgument(serde.nonEmpty() || table.provider().nonEmpty(),\n        \"Partition format should be defined\");\n\n    String uri = Util.uriToString(locationUri.get());\n    String format = serde.nonEmpty() ? serde.get() : table.provider().get();", "author": "aokolnychyi", "createdAt": "2020-12-04T17:45:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc3OTAwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI3MDA3Mw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536270073", "bodyText": "@RussellSpitzer, do we have a test for this?\nWe used to have one internally.\n        withSQLConf(\n          \"hive.exec.dynamic.partition\" -> \"true\",\n          \"hive.exec.dynamic.partition.mode\" -> \"nonstrict\") {\n\n          sql(\n            s\"\"\"CREATE TABLE t (\n               |  id integer,\n               |  name string\n               |)\n               |PARTITIONED BY (dept string, subdept string)\n               |STORED AS PARQUET\n               |LOCATION '$dir'\"\"\".stripMargin)", "author": "aokolnychyi", "createdAt": "2020-12-04T17:46:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc3OTAwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM3NjU0NQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538376545", "bodyText": "I see a test for this one. It looks correct to me.", "author": "aokolnychyi", "createdAt": "2020-12-08T13:36:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc3OTAwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc4MDA5OQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535780099", "bodyText": "It would probably be better to use a mutable map because this one will reject duplicate properties instead of overwriting.\nYou can also use ImmutableMap.builder() instead of supplying the key and value types.", "author": "rdblue", "createdAt": "2020-12-04T02:00:50Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    try {\n+      Map<String, String> newTableProperties = new ImmutableMap.Builder<String, String>()", "originalCommit": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI1Mjc4Ng==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536252786", "bodyText": "Switched to mutable map", "author": "RussellSpitzer", "createdAt": "2020-12-04T17:18:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc4MDA5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc4MDMyNA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535780324", "bodyText": "Can't this use stagedTable properties instead?", "author": "rdblue", "createdAt": "2020-12-04T02:01:27Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    try {\n+      Map<String, String> newTableProperties = new ImmutableMap.Builder<String, String>()\n+          .put(TableCatalog.PROP_PROVIDER, \"iceberg\")\n+          .putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava())\n+          .putAll(tableLocationProperties(sourceTableLocation()))\n+          .putAll(additionalProperties())\n+          .build();\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(\n+          destTableName().namespace(),\n+          destTableName().name()),\n+          v1SourceTable().schema(),\n+          sourcePartitionSpec(),\n+          newTableProperties);\n+\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {", "originalCommit": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI3MDgyNA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536270824", "bodyText": "Sure will change", "author": "RussellSpitzer", "createdAt": "2020-12-04T17:47:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc4MDMyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc4MDQ5Ng==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535780496", "bodyText": "Since we know the properties are in the table, should we look up this location from properties?", "author": "rdblue", "createdAt": "2020-12-04T02:01:52Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    try {\n+      Map<String, String> newTableProperties = new ImmutableMap.Builder<String, String>()\n+          .put(TableCatalog.PROP_PROVIDER, \"iceberg\")\n+          .putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava())\n+          .putAll(tableLocationProperties(sourceTableLocation()))\n+          .putAll(additionalProperties())\n+          .build();\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(\n+          destTableName().namespace(),\n+          destTableName().name()),\n+          v1SourceTable().schema(),\n+          sourcePartitionSpec(),\n+          newTableProperties);\n+\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        applyDefaultTableNameMapping(icebergTable);\n+      }\n+\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;", "originalCommit": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI3NDI1Nw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536274257", "bodyText": "Sure we can do that here, but not in Snapshot Action where it might not be set", "author": "RussellSpitzer", "createdAt": "2020-12-04T17:53:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc4MDQ5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc4MDc5MQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535780791", "bodyText": "This is another place where we might want to update our own API rather than creating a special instance (v2BackupIdentifier) to pass through.", "author": "rdblue", "createdAt": "2020-12-04T02:02:42Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    try {\n+      Map<String, String> newTableProperties = new ImmutableMap.Builder<String, String>()\n+          .put(TableCatalog.PROP_PROVIDER, \"iceberg\")\n+          .putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava())\n+          .putAll(tableLocationProperties(sourceTableLocation()))\n+          .putAll(additionalProperties())\n+          .build();\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(\n+          destTableName().namespace(),\n+          destTableName().name()),\n+          v1SourceTable().schema(),\n+          sourcePartitionSpec(),\n+          newTableProperties);\n+\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        applyDefaultTableNameMapping(icebergTable);\n+      }\n+\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableName(), stagingLocation);\n+\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);\n+      TableIdentifier v1BackupIdentifier = new TableIdentifier(backupIdentifier.name(), backupNamespace);\n+      SparkTableUtil.importSparkTable(spark(), v1BackupIdentifier, icebergTable, stagingLocation);", "originalCommit": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5MjMzOQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535792339", "bodyText": "I think it is usually better to do work like this in a finally block using a threw boolean:\nboolean threw = true;\ntry {\n  // something that might fail\n  threw = false;\n} finally {\n  if (threw) {\n    // clean up\n  }\n}\nThat has the advantage that whatever happened in the block, you get the correct exception rather than needing to throw a generic RuntimeException which would make it difficult to use this action in a higher-level application. Also, there are technically throwables that are not Exception.", "author": "rdblue", "createdAt": "2020-12-04T02:37:01Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    try {\n+      Map<String, String> newTableProperties = new ImmutableMap.Builder<String, String>()\n+          .put(TableCatalog.PROP_PROVIDER, \"iceberg\")\n+          .putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava())\n+          .putAll(tableLocationProperties(sourceTableLocation()))\n+          .putAll(additionalProperties())\n+          .build();\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(\n+          destTableName().namespace(),\n+          destTableName().name()),\n+          v1SourceTable().schema(),\n+          sourcePartitionSpec(),\n+          newTableProperties);\n+\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        applyDefaultTableNameMapping(icebergTable);\n+      }\n+\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableName(), stagingLocation);\n+\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);\n+      TableIdentifier v1BackupIdentifier = new TableIdentifier(backupIdentifier.name(), backupNamespace);\n+      SparkTableUtil.importSparkTable(spark(), v1BackupIdentifier, icebergTable, stagingLocation);\n+\n+      stagedTable.commitStagedChanges();\n+    } catch (Exception e) {", "originalCommit": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI5MjA1NA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536292054", "bodyText": "Sounds good, i'll change it around", "author": "RussellSpitzer", "createdAt": "2020-12-04T18:23:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5MjMzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5MjUyMA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535792520", "bodyText": "Since the changes are only staged, this should happen after restoring the backup table, in case there is a failure in the abort.", "author": "rdblue", "createdAt": "2020-12-04T02:37:34Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    try {\n+      Map<String, String> newTableProperties = new ImmutableMap.Builder<String, String>()\n+          .put(TableCatalog.PROP_PROVIDER, \"iceberg\")\n+          .putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava())\n+          .putAll(tableLocationProperties(sourceTableLocation()))\n+          .putAll(additionalProperties())\n+          .build();\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(\n+          destTableName().namespace(),\n+          destTableName().name()),\n+          v1SourceTable().schema(),\n+          sourcePartitionSpec(),\n+          newTableProperties);\n+\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        applyDefaultTableNameMapping(icebergTable);\n+      }\n+\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableName(), stagingLocation);\n+\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);\n+      TableIdentifier v1BackupIdentifier = new TableIdentifier(backupIdentifier.name(), backupNamespace);\n+      SparkTableUtil.importSparkTable(spark(), v1BackupIdentifier, icebergTable, stagingLocation);\n+\n+      stagedTable.commitStagedChanges();\n+    } catch (Exception e) {\n+      LOG.error(\"Error when attempting perform migration changes, aborting table creation and restoring backup\", e);\n+\n+      try {\n+        if (stagedTable != null) {\n+          stagedTable.abortStagedChanges();", "originalCommit": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI5MjczNg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536292736", "bodyText": "Sure, that order does make more sense", "author": "RussellSpitzer", "createdAt": "2020-12-04T18:24:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5MjUyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMxNDQyOA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540314428", "bodyText": "Is this one actually resolved?", "author": "aokolnychyi", "createdAt": "2020-12-10T16:32:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5MjUyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMxNDY4MA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540314680", "bodyText": "Well, it is in try block, never mind.", "author": "aokolnychyi", "createdAt": "2020-12-10T16:32:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5MjUyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMxOTY0Mw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540319643", "bodyText": "I switched it back after we decided we catch the exception and just log it. But we can move it back", "author": "RussellSpitzer", "createdAt": "2020-12-10T16:37:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5MjUyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5MjkzOA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535792938", "bodyText": "I think of \"apply\" as using the name mapping. What about \"add\" instead because this creates it and adds it to the table?", "author": "rdblue", "createdAt": "2020-12-04T02:38:54Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName,\n+                       CatalogPlugin destCatalog,  Identifier destTableName) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableName = sourceTableName;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableName = destTableName;\n+\n+    try {\n+      String sourceString = String.join(\".\", sourceTableName.namespace()) + \".\" + sourceTableName.name();\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, sourceString);\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable = (V1Table) ((TableCatalog) sourceCatalog).loadTable(sourceTableName);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableName), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use a non-v1 table %s as a source\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceCatalogTable);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction setAll(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction set(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected CatalogPlugin sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableName() {\n+    return sourceTableName;\n+  }\n+\n+  protected Transform[] sourcePartitionSpec() {\n+    return sourceTable.partitioning();\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableName() {\n+    return destTableName;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!ALLOWED_SOURCES.contains(sourceTableProvider)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider));\n+    }\n+\n+    if (sourceTable.storage().locationUri().isEmpty()) {\n+      throw new IllegalArgumentException(\"Cannot create an Iceberg table from a source without an explicit location\");\n+    }\n+  }\n+\n+  protected static Map<String, String> tableLocationProperties(String tableLocation) {\n+    return ImmutableMap.of(\n+        TableProperties.WRITE_METADATA_LOCATION, tableLocation + \"/\" + ICEBERG_METADATA_FOLDER,\n+        TableProperties.WRITE_NEW_DATA_LOCATION, tableLocation\n+    );\n+  }\n+\n+  protected static void applyDefaultTableNameMapping(Table table) {", "originalCommit": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI1MTUyNQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536251525", "bodyText": "sounds good to me", "author": "RussellSpitzer", "createdAt": "2020-12-04T17:16:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5MjkzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI5NjI4Ng==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536296286", "bodyText": "I think assign would also fit here.", "author": "aokolnychyi", "createdAt": "2020-12-04T18:30:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5MjkzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI5OTYzNQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536299635", "bodyText": "I like assign better, i'm gonna go with that", "author": "RussellSpitzer", "createdAt": "2020-12-04T18:36:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5MjkzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5MzE5OQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535793199", "bodyText": "Here as well, this will fail if the properties conflict. Since we know that Iceberg won't modify them it is safe to use a mutable map.", "author": "rdblue", "createdAt": "2020-12-04T02:39:42Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private final String destTableLocation;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName) {\n+    super(spark, sourceCatalog, sourceTableName, destCatalog, destTableName);\n+    // Cannot check if the table location that would be generated by the catalog will match the source\n+    this.destTableLocation = null;\n+  }\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName, String destTableLocation) {\n+    super(spark, sourceCatalog, sourceTableName, destCatalog, destTableName);\n+    Preconditions.checkArgument(!sourceTableLocation().equals(destTableLocation),\n+        \"Cannot create snapshot where destination location is the same as the source location. This\" +\n+            \"would cause a mixing of original table created and snapshot created files.\");\n+    this.destTableLocation = destTableLocation;\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable;\n+    Table icebergTable;\n+\n+    try {\n+      stagedTable = destCatalog().stageCreate(destTableName(), v1SourceTable().schema(),\n+          sourcePartitionSpec(), buildPropertyMap());\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        applyDefaultTableNameMapping(icebergTable);\n+      }\n+    } catch (TableAlreadyExistsException taeException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because a table already exists with that name\",\n+          taeException);\n+    } catch (NoSuchNamespaceException nsnException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because the namespace given does not exist\",\n+          nsnException);\n+    }\n+\n+    try {\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;\n+      LOG.info(\"Beginning snapshot of {} to {} using metadata location {}\", sourceTableName(), destTableName(),\n+          stagingLocation);\n+\n+      TableIdentifier v1TableIdentifier = v1SourceTable().identifier();\n+      SparkTableUtil.importSparkTable(spark(), v1TableIdentifier, icebergTable, stagingLocation);\n+      stagedTable.commitStagedChanges();\n+\n+    } catch (Exception e) {\n+      LOG.error(\"Error when attempting to commit snapshot changes, rolling back\", e);\n+      if (stagedTable != null) {\n+        stagedTable.abortStagedChanges();\n+      }\n+      throw e;\n+    }\n+\n+    long numMigratedFiles;\n+    Snapshot snapshot = icebergTable.currentSnapshot();\n+    numMigratedFiles = Long.valueOf(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+    LOG.info(\"Successfully loaded Iceberg metadata for {} files\", numMigratedFiles);\n+    return numMigratedFiles;\n+  }\n+\n+  private Map<String, String> buildPropertyMap() {\n+    ImmutableMap.Builder<String, String> propBuilder = ImmutableMap.<String, String>builder()", "originalCommit": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI5NjE5Ng==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536296196", "bodyText": "Sounds good to me", "author": "RussellSpitzer", "createdAt": "2020-12-04T18:30:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5MzE5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5MzQ2NQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535793465", "bodyText": "Can you add more whitespace between control flow blocks in this PR?", "author": "rdblue", "createdAt": "2020-12-04T02:40:32Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/SparkActions.java", "diffHunk": "@@ -20,10 +20,75 @@\n package org.apache.iceberg.actions;\n \n import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.Spark3Util;\n import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n \n-class SparkActions extends Actions {\n+public class SparkActions extends Actions {\n   protected SparkActions(SparkSession spark, Table table) {\n     super(spark, table);\n   }\n+\n+  public static CreateAction migrate(String tableName) {\n+    return migrate(SparkSession.active(), tableName);\n+  }\n+\n+  public static CreateAction migrate(SparkSession spark, String tableName) {\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n+    try {\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, tableName);\n+    } catch (ParseException e) {\n+      throw new IllegalArgumentException(\"Cannot parse migrate target\", e);\n+    }\n+\n+    return new Spark3MigrateAction(spark, catalogAndIdentifier.catalog(), catalogAndIdentifier.identifier());\n+  }\n+\n+  public static CreateAction snapshot(String sourceId, String destId) {\n+    return snapshot(SparkSession.active(), sourceId, destId);\n+  }\n+\n+  public static CreateAction snapshot(SparkSession spark, String sourceId, String destId) {\n+    Spark3Util.CatalogAndIdentifier sourceIdent;\n+    try {\n+      sourceIdent = Spark3Util.catalogAndIdentifier(spark, sourceId);\n+    } catch (ParseException e) {\n+      throw new IllegalArgumentException(\"Cannot parse snapshot source\", e);\n+    }", "originalCommit": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5NDE5MA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535794190", "bodyText": "Why is the location passed to snapshot? That seems like something that should be set on the action instead because it isn't required.", "author": "rdblue", "createdAt": "2020-12-04T02:42:40Z", "path": "spark3/src/test/java/org/apache/iceberg/actions/TestCreateActions.java", "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.io.filefilter.TrueFileFilter;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.source.SimpleRecord;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runners.Parameterized;\n+import scala.Some;\n+import scala.collection.Seq;\n+\n+public class TestCreateActions extends SparkCatalogTestBase {\n+  private static final String CREATE_PARTITIONED_PARQUET = \"CREATE TABLE %s (id INT, data STRING) \" +\n+      \"using parquet PARTITIONED BY (id) LOCATION '%s'\";\n+  private static final String CREATE_PARQUET = \"CREATE TABLE %s (id INT, data STRING) \" +\n+      \"using parquet LOCATION '%s'\";\n+  private static final String CREATE_HIVE_EXTERNAL_PARQUET = \"CREATE EXTERNAL TABLE %s (data STRING) \" +\n+      \"PARTITIONED BY (id INT) STORED AS parquet LOCATION '%s'\";\n+  private static final String CREATE_HIVE_PARQUET = \"CREATE TABLE %s (data STRING) \" +\n+      \"PARTITIONED BY (id INT) STORED AS parquet\";\n+\n+  private static final String NAMESPACE = \"default\";\n+\n+  @Parameterized.Parameters(name = \"Catalog Name {0} - Options {2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"spark_catalog\", SparkSessionCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\",\n+            \"parquet-enabled\", \"true\",\n+            \"cache-enabled\", \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n+        )},\n+        new Object[] {\"spark_catalog\", SparkSessionCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hadoop\",\n+            \"default-namespace\", \"default\",\n+            \"parquet-enabled\", \"true\",\n+            \"cache-enabled\", \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n+        )},\n+        new Object[] { \"testhive\", SparkCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\"\n+        )},\n+        new Object[] { \"testhadoop\", SparkCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hadoop\",\n+            \"default-namespace\", \"default\"\n+        )}\n+    };\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private String baseTableName = \"baseTable\";\n+  private File tableDir;\n+  private String tableLocation;\n+  private final String type;\n+  private final TableCatalog catalog;\n+\n+  public TestCreateActions(\n+      String catalogName,\n+      String implementation,\n+      Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+    this.catalog = (TableCatalog) spark.sessionState().catalogManager().catalog(catalogName);\n+    this.type = config.get(\"type\");\n+  }\n+\n+  @Before\n+  public void before() {\n+    try {\n+      this.tableDir = temp.newFolder();\n+    } catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+    this.tableLocation = tableDir.toURI().toString();\n+\n+    spark.conf().set(\"hive.exec.dynamic.partition\", \"true\");\n+    spark.conf().set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\");\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", baseTableName));\n+\n+    List<SimpleRecord> expected = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\"),\n+        new SimpleRecord(2, \"b\"),\n+        new SimpleRecord(3, \"c\")\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, SimpleRecord.class);\n+\n+    df.select(\"id\", \"data\").orderBy(\"data\").write()\n+        .mode(\"append\")\n+        .option(\"path\", tableLocation)\n+        .saveAsTable(baseTableName);\n+  }\n+\n+  @After\n+  public void after() throws IOException {\n+    // Drop the hive table.\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", baseTableName));\n+  }\n+\n+  @Test\n+  public void testMigratePartitioned() throws Exception {\n+    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n+    Assume.assumeTrue(\"Can only migrate from Spark Session Catalog\", catalog.name().equals(\"spark_catalog\"));\n+    String source = sourceName(\"test_migrate_partitioned_table\");\n+    String dest = source;\n+    createSourceTable(CREATE_PARTITIONED_PARQUET, source);\n+    assertMigratedFileCount(Actions.migrate(source), source, dest);\n+  }\n+\n+  @Test\n+  public void testMigrateUnpartitioned() throws Exception {\n+    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n+    Assume.assumeTrue(\"Can only migrate from Spark Session Catalog\", catalog.name().equals(\"spark_catalog\"));\n+    String source = sourceName(\"test_migrate_unpartitioned_table\");\n+    String dest = source;\n+    createSourceTable(CREATE_PARQUET, source);\n+    assertMigratedFileCount(Actions.migrate(source), source, dest);\n+  }\n+\n+  @Test\n+  public void testSnapshotPartitioned() throws Exception {\n+    Assume.assumeTrue(\"Cannot snapshot with arbitrary location in a hadoop based catalog\",\n+        !type.equals(\"hadoop\"));\n+    File location = temp.newFolder();\n+    String source = sourceName(\"test_snapshot_partitioned_table\");\n+    String dest = destName(\"iceberg_snapshot_partitioned\");\n+    createSourceTable(CREATE_PARTITIONED_PARQUET, source);\n+    assertMigratedFileCount(Actions.snapshot(source, dest, location.toString()), source, dest);\n+    assertIsolatedSnapshot(source, dest);\n+  }\n+\n+  @Test\n+  public void testSnapshotUnpartitioned() throws Exception {\n+    Assume.assumeTrue(\"Cannot snapshot with arbitrary location in a hadoop based catalog\",\n+        !type.equals(\"hadoop\"));\n+    File location = temp.newFolder();\n+    String source = sourceName(\"test_snapshot_unpartitioned_table\");\n+    String dest = destName(\"iceberg_snapshot_unpartitioned\");\n+    createSourceTable(CREATE_PARQUET, source);\n+    assertMigratedFileCount(Actions.snapshot(source, dest, location.toString()), source, dest);\n+    assertIsolatedSnapshot(source, dest);\n+  }\n+\n+  @Test\n+  public void testSnapshotHiveTable() throws Exception {\n+    Assume.assumeTrue(\"Cannot snapshot with arbitrary location in a hadoop based catalog\",\n+        !type.equals(\"hadoop\"));\n+    File location = temp.newFolder();\n+    String source = sourceName(\"snapshot_hive_table\");\n+    String dest = destName(\"iceberg_snapshot_hive_table\");\n+    createSourceTable(CREATE_HIVE_EXTERNAL_PARQUET, source);\n+    assertMigratedFileCount(Actions.snapshot(source, dest, location.toString()), source, dest);\n+    assertIsolatedSnapshot(source, dest);\n+  }\n+\n+  @Test\n+  public void testMigrateHiveTable() throws Exception {\n+    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n+    String source = sourceName(\"migrate_hive_table\");\n+    String dest = source;\n+    createSourceTable(CREATE_HIVE_EXTERNAL_PARQUET, source);\n+    assertMigratedFileCount(Actions.migrate(source), source, dest);\n+  }\n+\n+  @Test\n+  public void testSnapshotManagedHiveTable() throws Exception {\n+    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n+    File location = temp.newFolder();\n+    String source = sourceName(\"snapshot_managed_hive_table\");\n+    String dest = destName(\"iceberg_snapshot_managed_hive_table\");\n+    createSourceTable(CREATE_HIVE_PARQUET, source);\n+    assertMigratedFileCount(Actions.snapshot(source, dest, location.toString()), source, dest);", "originalCommit": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI5NzA4Mw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536297083", "bodyText": "Changed the interfaces around so it is no longer part of the static method", "author": "RussellSpitzer", "createdAt": "2020-12-04T18:32:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5NDE5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5NDUxMw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535794513", "bodyText": "I would probably remove this. I don't see a need to supply location this way and it adds complexity with an additional dynamic method call.", "author": "rdblue", "createdAt": "2020-12-04T02:43:37Z", "path": "spark/src/main/java/org/apache/iceberg/actions/Actions.java", "diffHunk": "@@ -77,6 +82,120 @@ public ExpireSnapshotsAction expireSnapshots() {\n     return new ExpireSnapshotsAction(spark, table);\n   }\n \n+  /**\n+   * Converts the provided table into an Iceberg table in place. The table will no longer be represented by it's\n+   * previous provider in the session catalog and a new metadata directory will be created at the table's location.\n+   *\n+   * @param tableName Table to be converted\n+   * @return {@link CreateAction} to perform migration\n+   */\n+  public static CreateAction migrate(String tableName) {\n+    try {\n+      return DynMethods.builder(\"migrate\")\n+          .impl(implClass(), String.class).buildStaticChecked()\n+          .invoke(tableName);\n+    } catch (NoSuchMethodException ex) {\n+      throw new UnsupportedOperationException(\"Migrate is not implemented for this version of Spark\");\n+    }\n+  }\n+\n+  /**\n+   * Converts the provided table into an Iceberg table in place. The table will no longer be accesible by it's\n+   * previous implementation\n+   *\n+   * @param tableName Table to be converted\n+   * @param spark     Spark session to use for looking up table\n+   * @return {@link CreateAction} to perform migration\n+   */\n+  public static CreateAction migrate(SparkSession spark, String tableName) {\n+    try {\n+      return DynMethods.builder(\"migrate\")\n+          .impl(implClass(), SparkSession.class, String.class).buildStaticChecked()\n+          .invoke(spark, tableName);\n+    } catch (NoSuchMethodException ex) {\n+      throw new UnsupportedOperationException(\"Migrate is not implemented for this version of Spark\");\n+    }\n+  }\n+\n+  /**\n+   * Creates an independent Iceberg table based on a given table. The new Iceberg table can be altered, appended or\n+   * deleted without causing any change to the original. New data and metadata will be created in the default\n+   * location for tables of this name in the destination catalog.\n+   *\n+   * @param sourceTable Original table which is the basis for the new Iceberg table\n+   * @param destTable   New Iceberg table being created\n+   * @return {@link CreateAction} to perform snapshot\n+   */\n+  public static CreateAction snapshot(SparkSession spark, String sourceTable, String destTable) {\n+    try {\n+      return DynMethods.builder(\"snapshot\")\n+          .impl(implClass(), SparkSession.class, String.class, String.class).buildStaticChecked()\n+          .invoke(spark, sourceTable, destTable);\n+    } catch (NoSuchMethodException ex) {\n+      throw new UnsupportedOperationException(\"Snapshot is not implemented for this version of Spark\");\n+    }\n+  }\n+\n+  /**\n+   * Creates an independent Iceberg table based on a given table. The new Iceberg table can be altered, appended or\n+   * deleted without causing any change to the original. New data and metadata will be created in the default\n+   * location for tables of this name in the destination catalog.\n+   *\n+   * @param sourceTable Original table which is the basis for the new Iceberg table\n+   * @param destTable   New Iceberg table being created\n+   * @return {@link CreateAction} to perform snapshot\n+   */\n+  public static CreateAction snapshot(String sourceTable, String destTable) {\n+    try {\n+      return DynMethods.builder(\"snapshot\")\n+          .impl(implClass(), String.class, String.class).buildStaticChecked()\n+          .invoke(sourceTable, destTable);\n+    } catch (NoSuchMethodException ex) {\n+      throw new UnsupportedOperationException(\"Snapshot is not implemented for this version of Spark\");\n+    }\n+  }\n+\n+  /**\n+   * Creates an independent Iceberg table based on a given table. The new Iceberg table can be altered, appended or\n+   * deleted without causing any change to the original. New data and metadata will be created in the\n+   * new location passed to this method.\n+   *\n+   * @param sourceTable Original table which is the basis for the new Iceberg table\n+   * @param destTable   New Iceberg table being created\n+   * @param location Location for metadata and new data for the new table\n+   * @return {@link CreateAction} to perform snapshot\n+   */\n+  public static CreateAction snapshot(String sourceTable, String destTable, String location) {", "originalCommit": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI0NDk2OQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536244969", "bodyText": "Removed, added another Interface SnapshotAction, with \"withLocation\" method for supplying location", "author": "RussellSpitzer", "createdAt": "2020-12-04T17:03:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5NDUxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI2Mjc3MQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536262771", "bodyText": "Should it be withProperties like we have in Catalog.Builder?", "author": "aokolnychyi", "createdAt": "2020-12-04T17:34:14Z", "path": "spark/src/main/java/org/apache/iceberg/actions/CreateAction.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+\n+interface CreateAction extends Action<Long> {\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with\n+   * the same key name will be overwritten.\n+   * @param properties a map of properties to be included\n+   * @return this for chaining\n+   */\n+  CreateAction setAll(Map<String, String> properties);", "originalCommit": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI2Mjk3OQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536262979", "bodyText": "And withProperty here?", "author": "aokolnychyi", "createdAt": "2020-12-04T17:34:32Z", "path": "spark/src/main/java/org/apache/iceberg/actions/CreateAction.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+\n+interface CreateAction extends Action<Long> {\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with\n+   * the same key name will be overwritten.\n+   * @param properties a map of properties to be included\n+   * @return this for chaining\n+   */\n+  CreateAction setAll(Map<String, String> properties);\n+\n+  /**\n+   * Adds an additional property to the newly created Iceberg Table. Any properties\n+   * with the same key name will be overwritten.\n+   * @param key the key of the property to add\n+   * @param value the value of the property to add\n+   * @return this for chaining\n+   */\n+  CreateAction set(String key, String value);", "originalCommit": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI3MTg4Nw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536271887", "bodyText": "nit: formatting is off", "author": "aokolnychyi", "createdAt": "2020-12-04T17:49:14Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName,\n+                       CatalogPlugin destCatalog,  Identifier destTableName) {", "originalCommit": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI4NDczNg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536284736", "bodyText": "Seems like it will be possible to snapshot a non-Iceberg Hive table as Iceberg Hadoop table. I thought for a sec we should disable such cases but there is probably no good reason. We can snapshot tables from other catalogs.", "author": "aokolnychyi", "createdAt": "2020-12-04T18:10:31Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;", "originalCommit": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI5MDkxMA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536290910", "bodyText": "This must always be TableCatalog, right? Should we validate this early and call checkSourceCatalog on it? The validation method can be void and can operate on TableCatalog.", "author": "aokolnychyi", "createdAt": "2020-12-04T18:21:12Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;", "originalCommit": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI5ODEzMQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536298131", "bodyText": "Ryan also suggested this, I changed it", "author": "RussellSpitzer", "createdAt": "2020-12-04T18:34:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI5MDkxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM3NzkzMA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538377930", "bodyText": "@RussellSpitzer, this is yet to be updated? I see we changed the return type of the method but not the field.", "author": "aokolnychyi", "createdAt": "2020-12-08T13:37:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI5MDkxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQwODk1OA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539408958", "bodyText": "Missed the field here", "author": "RussellSpitzer", "createdAt": "2020-12-09T15:35:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI5MDkxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI5MjQ5MQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536292491", "bodyText": "nit: sourceTableIdent?", "author": "aokolnychyi", "createdAt": "2020-12-04T18:23:59Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;", "originalCommit": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI5MzY5MA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536293690", "bodyText": "nit: destTableIdent?", "author": "aokolnychyi", "createdAt": "2020-12-04T18:26:12Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableName;", "originalCommit": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjMxNDc0MQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536314741", "bodyText": "Why do we have this method in parent class? Isn't the location config specific to the operation type? We have to set the data location to the table location only in MIGRATE.", "author": "aokolnychyi", "createdAt": "2020-12-04T19:03:32Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName,\n+                       CatalogPlugin destCatalog,  Identifier destTableName) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableName = sourceTableName;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableName = destTableName;\n+\n+    try {\n+      String sourceString = String.join(\".\", sourceTableName.namespace()) + \".\" + sourceTableName.name();\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, sourceString);\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable = (V1Table) ((TableCatalog) sourceCatalog).loadTable(sourceTableName);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableName), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use a non-v1 table %s as a source\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceCatalogTable);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction setAll(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction set(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected CatalogPlugin sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableName() {\n+    return sourceTableName;\n+  }\n+\n+  protected Transform[] sourcePartitionSpec() {\n+    return sourceTable.partitioning();\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableName() {\n+    return destTableName;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!ALLOWED_SOURCES.contains(sourceTableProvider)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider));\n+    }\n+\n+    if (sourceTable.storage().locationUri().isEmpty()) {\n+      throw new IllegalArgumentException(\"Cannot create an Iceberg table from a source without an explicit location\");\n+    }\n+  }\n+\n+  protected static Map<String, String> tableLocationProperties(String tableLocation) {", "originalCommit": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjM1NjgzOA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536356838", "bodyText": "And with Snapshot with a manually specified location", "author": "RussellSpitzer", "createdAt": "2020-12-04T20:19:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjMxNDc0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQxMjg5Mg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538412892", "bodyText": "I am not sure I follow. Why do we have to force a specific layout if the user specifies a location in SNAPSHOT?\nIt should be OK to specify a custom metadata and data location in the SNAPSHOT command but we are forcing the layout here. In my view, this should only apply to MIGRATE where we must keep the old layout.", "author": "aokolnychyi", "createdAt": "2020-12-08T14:09:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjMxNDc0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQzNTk5Ng==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538435996", "bodyText": "This only applies if the user also uses .withLocation in SnapshotAction, if \"destLocation\" is not set then these properties are not set.", "author": "RussellSpitzer", "createdAt": "2020-12-08T14:31:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjMxNDc0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODUyODQ3MA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538528470", "bodyText": "Correct but why? Why cannot we set a location and the data and metadata folder in SNAPSHOT?", "author": "aokolnychyi", "createdAt": "2020-12-08T15:54:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjMxNDc0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc0NTY3Nw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538745677", "bodyText": "I don't think this is needed either way.\nIn our MIGRATE command, we set the metadata location so that we can create the Iceberg table and then rename it into place. Because it doesn't have the correct name to begin with, its location would be different so we set the metadata location to ensure the metadata is created under the correct location. I think we also didn't support custom table locations at the time I wrote it.\nNow, tables support custom table locations in create and this is going to use a staged table created to create in place. I think as long as the table is created from the exiting table's location for MIGRATE, or created with the requested location for SNAPSHOT, everything should work fine without the additional table properties.", "author": "rdblue", "createdAt": "2020-12-08T19:27:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjMxNDc0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwNzQ4NA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538807484", "bodyText": "@rdblue, what about keeping the old layout after migration? I agree nothing is required for SNAPSHOT but I think we should keep the existing layout of data after migration (i.e. data location for new data should point to the root table location).", "author": "aokolnychyi", "createdAt": "2020-12-08T21:06:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjMxNDc0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgxNzA3NA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538817074", "bodyText": "I would recommend against that because it modifies the state of the partitions in the old table. We use the data prefix so that the old table can be kept unmodified in case the operation needs to be rolled back, even after a write. Writing data into the old folders makes it live in the old table.", "author": "rdblue", "createdAt": "2020-12-08T21:22:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjMxNDc0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgyMzQyMA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538823420", "bodyText": "Well, it probably depends on the use case. Some people may actually need a way to move back without loosing writes after all snapshots are expired. Also, it is a bit surprising for folks less familiar with Iceberg that the data is written to data folder by default.", "author": "aokolnychyi", "createdAt": "2020-12-08T21:33:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjMxNDc0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgyNTAzMg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538825032", "bodyText": "Rolling back the table should be also possible even if we write the data to the same location. We can call rollback and then expire/remove orphan files. That said, I will be okay if we allow extra properties to be set during MIGRATE and document this properly.", "author": "aokolnychyi", "createdAt": "2020-12-08T21:35:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjMxNDc0MQ=="}], "type": "inlineReview"}, {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "url": "https://github.com/apache/iceberg/commit/e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "message": "Various Reviewer Comments", "committedDate": "2020-12-04T21:01:07Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM3MDIyMA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538370220", "bodyText": "I am fine with returning a number of imported files but if someone has any ideas about other useful info, we may consider adding a result class.", "author": "aokolnychyi", "createdAt": "2020-12-08T13:30:23Z", "path": "spark/src/main/java/org/apache/iceberg/actions/CreateAction.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+\n+interface CreateAction extends Action<Long> {", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM3MTA1Nw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538371057", "bodyText": "We used to return the number of imported partitions but we cannot compute it easily if snapshot id inheritance is enabled.", "author": "aokolnychyi", "createdAt": "2020-12-08T13:31:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM3MDIyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM3MjI5MQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538372291", "bodyText": "nit: extra line", "author": "aokolnychyi", "createdAt": "2020-12-08T13:32:25Z", "path": "spark/src/main/java/org/apache/iceberg/actions/CreateAction.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+\n+interface CreateAction extends Action<Long> {\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with\n+   * the same key name will be overwritten.\n+   * @param properties a map of properties to be included\n+   * @return this for chaining\n+   */\n+  CreateAction withProperties(Map<String, String> properties);\n+\n+  /**\n+   * Adds an additional property to the newly created Iceberg Table. Any properties\n+   * with the same key name will be overwritten.\n+   * @param key the key of the property to add\n+   * @param value the value of the property to add\n+   * @return this for chaining\n+   */\n+  CreateAction withProperty(String key, String value);\n+", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM3NTQwMA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538375400", "bodyText": "nit: Table -> table", "author": "aokolnychyi", "createdAt": "2020-12-08T13:35:29Z", "path": "spark/src/main/java/org/apache/iceberg/actions/CreateAction.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+\n+interface CreateAction extends Action<Long> {\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM3NTUwMA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538375500", "bodyText": "nit: Table -> table", "author": "aokolnychyi", "createdAt": "2020-12-08T13:35:34Z", "path": "spark/src/main/java/org/apache/iceberg/actions/CreateAction.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+\n+interface CreateAction extends Action<Long> {\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with\n+   * the same key name will be overwritten.\n+   * @param properties a map of properties to be included\n+   * @return this for chaining\n+   */\n+  CreateAction withProperties(Map<String, String> properties);\n+\n+  /**\n+   * Adds an additional property to the newly created Iceberg Table. Any properties", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM3ODYzNA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538378634", "bodyText": "nit: formatting is off", "author": "aokolnychyi", "createdAt": "2020-12-08T13:38:31Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                       CatalogPlugin destCatalog, Identifier destTableIdent) {", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM4MzUwMw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538383503", "bodyText": "Can we mark it final?", "author": "aokolnychyi", "createdAt": "2020-12-08T13:43:18Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM5ODk5Ng==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538398996", "bodyText": "It looks like having a separate method to check if it is an Iceberg catalog would make it more readable than having two negations.\nOr we could use ValidationException. It also gives formatting for free.\n    ValidationException.check(\n        catalog instanceof SparkSessionCatalog || catalog instanceof SparkCatalog,\n        \"Unsupported destination catalog. Catalog %s is %s but must be either %s or %s\",\n        catalog.name(), catalog.getClass().getName(), SparkSessionCatalog.class.getName(), SparkCatalog.class.getName());", "author": "aokolnychyi", "createdAt": "2020-12-08T13:56:21Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                       CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) ((TableCatalog) sourceCatalog).loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable(sourceCatalogTable);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected CatalogPlugin sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableName() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected Transform[] sourcePartitionSpec() {\n+    return sourceTable.partitioning();\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableName() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!ALLOWED_SOURCES.contains(sourceTableProvider)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider));\n+    }\n+\n+    if (sourceTable.storage().locationUri().isEmpty()) {\n+      throw new IllegalArgumentException(\"Cannot create an Iceberg table from a source without an explicit location\");\n+    }\n+  }\n+\n+  protected static Map<String, String> tableLocationProperties(String tableLocation) {\n+    return ImmutableMap.of(\n+        TableProperties.WRITE_METADATA_LOCATION, tableLocation + \"/\" + ICEBERG_METADATA_FOLDER,\n+        TableProperties.WRITE_NEW_DATA_LOCATION, tableLocation\n+    );\n+  }\n+\n+  protected static void assignDefaultTableNameMapping(Table table) {\n+    NameMapping nameMapping = MappingUtil.create(table.schema());\n+    String nameMappingJson = NameMappingParser.toJson(nameMapping);\n+    table.updateProperties().set(TableProperties.DEFAULT_NAME_MAPPING, nameMappingJson).commit();\n+  }\n+\n+  private static StagingTableCatalog checkDestinationCatalog(CatalogPlugin catalog) {\n+    if (!(catalog instanceof SparkSessionCatalog) && !(catalog instanceof SparkCatalog)) {", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQwMDA1Ng==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538400056", "bodyText": "Using ValidationException seems pretty clean. What do you think, @RussellSpitzer?", "author": "aokolnychyi", "createdAt": "2020-12-08T13:57:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM5ODk5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQwMzU1NA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538403554", "bodyText": "Or shall we even use Preconditions.checkArgument? I think that would be better.", "author": "aokolnychyi", "createdAt": "2020-12-08T14:00:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM5ODk5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQwMDg0Mg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538400842", "bodyText": "Why static?", "author": "aokolnychyi", "createdAt": "2020-12-08T13:57:46Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                       CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) ((TableCatalog) sourceCatalog).loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable(sourceCatalogTable);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected CatalogPlugin sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableName() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected Transform[] sourcePartitionSpec() {\n+    return sourceTable.partitioning();\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableName() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!ALLOWED_SOURCES.contains(sourceTableProvider)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider));\n+    }\n+\n+    if (sourceTable.storage().locationUri().isEmpty()) {\n+      throw new IllegalArgumentException(\"Cannot create an Iceberg table from a source without an explicit location\");\n+    }\n+  }\n+\n+  protected static Map<String, String> tableLocationProperties(String tableLocation) {\n+    return ImmutableMap.of(\n+        TableProperties.WRITE_METADATA_LOCATION, tableLocation + \"/\" + ICEBERG_METADATA_FOLDER,\n+        TableProperties.WRITE_NEW_DATA_LOCATION, tableLocation\n+    );\n+  }\n+\n+  protected static void assignDefaultTableNameMapping(Table table) {\n+    NameMapping nameMapping = MappingUtil.create(table.schema());\n+    String nameMappingJson = NameMappingParser.toJson(nameMapping);\n+    table.updateProperties().set(TableProperties.DEFAULT_NAME_MAPPING, nameMappingJson).commit();\n+  }\n+\n+  private static StagingTableCatalog checkDestinationCatalog(CatalogPlugin catalog) {", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc0NjYxNA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538746614", "bodyText": "I think it makes sense to keep the API for child implementations small. We can always refactor to expose it, but if we don't need to then private static makes sense to me.", "author": "rdblue", "createdAt": "2020-12-08T19:29:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQwMDg0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwODczNg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538808736", "bodyText": "private sounds good to me but I am not sure why it needs to be static as it is used in non-static context.", "author": "aokolnychyi", "createdAt": "2020-12-08T21:08:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQwMDg0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQwMTM3NQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538401375", "bodyText": "Same here. Why static?", "author": "aokolnychyi", "createdAt": "2020-12-08T13:58:15Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                       CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) ((TableCatalog) sourceCatalog).loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable(sourceCatalogTable);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected CatalogPlugin sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableName() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected Transform[] sourcePartitionSpec() {\n+    return sourceTable.partitioning();\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableName() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!ALLOWED_SOURCES.contains(sourceTableProvider)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider));\n+    }\n+\n+    if (sourceTable.storage().locationUri().isEmpty()) {\n+      throw new IllegalArgumentException(\"Cannot create an Iceberg table from a source without an explicit location\");\n+    }\n+  }\n+\n+  protected static Map<String, String> tableLocationProperties(String tableLocation) {\n+    return ImmutableMap.of(\n+        TableProperties.WRITE_METADATA_LOCATION, tableLocation + \"/\" + ICEBERG_METADATA_FOLDER,\n+        TableProperties.WRITE_NEW_DATA_LOCATION, tableLocation\n+    );\n+  }\n+\n+  protected static void assignDefaultTableNameMapping(Table table) {", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQzMTUxOA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539431518", "bodyText": "sorry force of habit. I just want my companion classes :(", "author": "RussellSpitzer", "createdAt": "2020-12-09T16:01:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQwMTM3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQwNjcxNA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538406714", "bodyText": "I think it will be more readable to use Preconditions here.\n    Preconditions.checkArgument(ALLOWED_SOURCES.contains(sourceTableProvider),\n        \"Cannot create an Iceberg table from source provider: %s\",\n        sourceTableProvider);", "author": "aokolnychyi", "createdAt": "2020-12-08T14:03:34Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                       CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) ((TableCatalog) sourceCatalog).loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable(sourceCatalogTable);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected CatalogPlugin sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableName() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected Transform[] sourcePartitionSpec() {\n+    return sourceTable.partitioning();\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableName() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!ALLOWED_SOURCES.contains(sourceTableProvider)) {", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQwNjg3NQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538406875", "bodyText": "same here", "author": "aokolnychyi", "createdAt": "2020-12-08T14:03:43Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                       CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) ((TableCatalog) sourceCatalog).loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable(sourceCatalogTable);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected CatalogPlugin sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableName() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected Transform[] sourcePartitionSpec() {\n+    return sourceTable.partitioning();\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableName() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!ALLOWED_SOURCES.contains(sourceTableProvider)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider));\n+    }\n+\n+    if (sourceTable.storage().locationUri().isEmpty()) {", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ2OTMxMg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538469312", "bodyText": "It looks like the staging part is same in both. Can we factor this out into the parent class?\n  protected StagedTable stageDestTable() {\n    try {\n      Map<String, String> props = targetTableProps();\n      StructType schema = sourceTable.schema();\n      Transform[] partitioning = sourceTable.partitioning();\n      return destCatalog.stageCreate(destTableIdent, schema, partitioning, props);\n    } catch (NoSuchNamespaceException e) {\n      throw new IllegalArgumentException(\"Cannot create a new table in a namespace which does not exist\", e);\n    } catch (TableAlreadyExistsException e) {\n      throw new IllegalArgumentException(\"Destination table already exists\", e);\n    }\n  }\n\n  protected abstract Map<String, String> targetTableProps();\n\n\nThen we can override targetTableProps in each subclass and make sure we add extra metadata like migrated or snapshot flags if we want to.", "author": "aokolnychyi", "createdAt": "2020-12-08T15:01:30Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    boolean threw = true;\n+    try {\n+      Map<String, String> newTableProperties = new HashMap<>();\n+      newTableProperties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n+      newTableProperties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());\n+      newTableProperties.putAll(tableLocationProperties(sourceTableLocation()));\n+      newTableProperties.putAll(additionalProperties());\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ0NjY3MA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539446670", "bodyText": "Sounds good to me", "author": "RussellSpitzer", "createdAt": "2020-12-09T16:19:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ2OTMxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ3NjMxMw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538476313", "bodyText": "I don't think this is correct. I think we should allow custom data and metadata folders in SNAPSHOT.\nHere is how get the metadata location internally:\n  def getMetadataLocation(table: Table): String = {\n    table.properties.getOrDefault(\n      TableProperties.WRITE_METADATA_LOCATION,\n      table.location + \"/\" + ICEBERG_METADATA_FOLDER)\n  }\n\nI believe we should do the same here.", "author": "aokolnychyi", "createdAt": "2020-12-08T15:08:00Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName) {\n+    super(spark, sourceCatalog, sourceTableName, destCatalog, destTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable;\n+    Table icebergTable;\n+\n+    try {\n+      stagedTable = destCatalog().stageCreate(destTableName(), v1SourceTable().schema(),\n+          sourcePartitionSpec(), buildPropertyMap());\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+    } catch (TableAlreadyExistsException taeException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because a table already exists with that name\",\n+          taeException);\n+    } catch (NoSuchNamespaceException nsnException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because the namespace given does not exist\",\n+          nsnException);\n+    }\n+\n+    boolean threw = true;\n+    try {\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc1NTM3NQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538755375", "bodyText": "I don't really have an opinion on whether we need to customize this in snapshot tables. I think for migration it is a good idea. For snapshots, I'd probably opt to keep it simple.", "author": "rdblue", "createdAt": "2020-12-08T19:42:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ3NjMxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgxMjQxMQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538812411", "bodyText": "I think the simplest option for us is not to do anything for SNAPSHOT and let users define all properties including data and metadata location. That's why we need the logic in getMetadataLocation to pick the correct metadata location for staging.", "author": "aokolnychyi", "createdAt": "2020-12-08T21:14:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ3NjMxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ3ODQ0Ng==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538478446", "bodyText": "How about we add a check that the table location, as well as metadata and data locations, do not interfere with the original table? This way we don't have to know the default table location the catalog will assign.", "author": "aokolnychyi", "createdAt": "2020-12-08T15:09:50Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName) {\n+    super(spark, sourceCatalog, sourceTableName, destCatalog, destTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable;\n+    Table icebergTable;\n+\n+    try {\n+      stagedTable = destCatalog().stageCreate(destTableName(), v1SourceTable().schema(),\n+          sourcePartitionSpec(), buildPropertyMap());\n+      icebergTable = ((SparkTable) stagedTable).table();", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ3ODkwMw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538478903", "bodyText": "We can just add a TODO now and address it later as the probability of hitting this is low.", "author": "aokolnychyi", "createdAt": "2020-12-08T15:10:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ3ODQ0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ4MDUxNQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538480515", "bodyText": "I think stagedTable is never null and this check is redundant.", "author": "aokolnychyi", "createdAt": "2020-12-08T15:11:43Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName) {\n+    super(spark, sourceCatalog, sourceTableName, destCatalog, destTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable;\n+    Table icebergTable;\n+\n+    try {\n+      stagedTable = destCatalog().stageCreate(destTableName(), v1SourceTable().schema(),\n+          sourcePartitionSpec(), buildPropertyMap());\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+    } catch (TableAlreadyExistsException taeException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because a table already exists with that name\",\n+          taeException);\n+    } catch (NoSuchNamespaceException nsnException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because the namespace given does not exist\",\n+          nsnException);\n+    }\n+\n+    boolean threw = true;\n+    try {\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;\n+      LOG.info(\"Beginning snapshot of {} to {} using metadata location {}\", sourceTableName(), destTableName(),\n+          stagingLocation);\n+\n+      TableIdentifier v1TableIdentifier = v1SourceTable().identifier();\n+      SparkTableUtil.importSparkTable(spark(), v1TableIdentifier, icebergTable, stagingLocation);\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {\n+        LOG.error(\"Error when attempting to commit snapshot changes, rolling back\");\n+        if (stagedTable != null) {", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ4MTcyOQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538481729", "bodyText": "Why cannot we just do the rollback in a catch clause? This seems a bit too complicated to me.", "author": "aokolnychyi", "createdAt": "2020-12-08T15:12:50Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName) {\n+    super(spark, sourceCatalog, sourceTableName, destCatalog, destTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable;\n+    Table icebergTable;\n+\n+    try {\n+      stagedTable = destCatalog().stageCreate(destTableName(), v1SourceTable().schema(),\n+          sourcePartitionSpec(), buildPropertyMap());\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+    } catch (TableAlreadyExistsException taeException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because a table already exists with that name\",\n+          taeException);\n+    } catch (NoSuchNamespaceException nsnException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because the namespace given does not exist\",\n+          nsnException);\n+    }\n+\n+    boolean threw = true;\n+    try {\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;\n+      LOG.info(\"Beginning snapshot of {} to {} using metadata location {}\", sourceTableName(), destTableName(),\n+          stagingLocation);\n+\n+      TableIdentifier v1TableIdentifier = v1SourceTable().identifier();\n+      SparkTableUtil.importSparkTable(spark(), v1TableIdentifier, icebergTable, stagingLocation);\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ2NzE2Mw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539467163", "bodyText": "This was recommend by @rdblue to insure the original exceptions are propagated as is while still doing cleanup", "author": "RussellSpitzer", "createdAt": "2020-12-09T16:43:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ4MTcyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ4MjU1NQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538482555", "bodyText": "I think it can be Long.parseLong to avoid boxing.", "author": "aokolnychyi", "createdAt": "2020-12-08T15:13:36Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName) {\n+    super(spark, sourceCatalog, sourceTableName, destCatalog, destTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable;\n+    Table icebergTable;\n+\n+    try {\n+      stagedTable = destCatalog().stageCreate(destTableName(), v1SourceTable().schema(),\n+          sourcePartitionSpec(), buildPropertyMap());\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+    } catch (TableAlreadyExistsException taeException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because a table already exists with that name\",\n+          taeException);\n+    } catch (NoSuchNamespaceException nsnException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because the namespace given does not exist\",\n+          nsnException);\n+    }\n+\n+    boolean threw = true;\n+    try {\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;\n+      LOG.info(\"Beginning snapshot of {} to {} using metadata location {}\", sourceTableName(), destTableName(),\n+          stagingLocation);\n+\n+      TableIdentifier v1TableIdentifier = v1SourceTable().identifier();\n+      SparkTableUtil.importSparkTable(spark(), v1TableIdentifier, icebergTable, stagingLocation);\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {\n+        LOG.error(\"Error when attempting to commit snapshot changes, rolling back\");\n+        if (stagedTable != null) {\n+          stagedTable.abortStagedChanges();\n+        }\n+      }\n+    }\n+\n+    long numMigratedFiles;\n+    Snapshot snapshot = icebergTable.currentSnapshot();\n+    numMigratedFiles = Long.valueOf(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ4MzE2MQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538483161", "bodyText": "Can we define the var on the same line? It should fit.", "author": "aokolnychyi", "createdAt": "2020-12-08T15:14:09Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName) {\n+    super(spark, sourceCatalog, sourceTableName, destCatalog, destTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable;\n+    Table icebergTable;\n+\n+    try {\n+      stagedTable = destCatalog().stageCreate(destTableName(), v1SourceTable().schema(),\n+          sourcePartitionSpec(), buildPropertyMap());\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+    } catch (TableAlreadyExistsException taeException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because a table already exists with that name\",\n+          taeException);\n+    } catch (NoSuchNamespaceException nsnException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because the namespace given does not exist\",\n+          nsnException);\n+    }\n+\n+    boolean threw = true;\n+    try {\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;\n+      LOG.info(\"Beginning snapshot of {} to {} using metadata location {}\", sourceTableName(), destTableName(),\n+          stagingLocation);\n+\n+      TableIdentifier v1TableIdentifier = v1SourceTable().identifier();\n+      SparkTableUtil.importSparkTable(spark(), v1TableIdentifier, icebergTable, stagingLocation);\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {\n+        LOG.error(\"Error when attempting to commit snapshot changes, rolling back\");\n+        if (stagedTable != null) {\n+          stagedTable.abortStagedChanges();\n+        }\n+      }\n+    }\n+\n+    long numMigratedFiles;", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ4NTAwNQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538485005", "bodyText": "I think we miss   at the end of this line. I'd also split it in between sentences.", "author": "aokolnychyi", "createdAt": "2020-12-08T15:15:53Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName) {\n+    super(spark, sourceCatalog, sourceTableName, destCatalog, destTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable;\n+    Table icebergTable;\n+\n+    try {\n+      stagedTable = destCatalog().stageCreate(destTableName(), v1SourceTable().schema(),\n+          sourcePartitionSpec(), buildPropertyMap());\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+    } catch (TableAlreadyExistsException taeException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because a table already exists with that name\",\n+          taeException);\n+    } catch (NoSuchNamespaceException nsnException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because the namespace given does not exist\",\n+          nsnException);\n+    }\n+\n+    boolean threw = true;\n+    try {\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;\n+      LOG.info(\"Beginning snapshot of {} to {} using metadata location {}\", sourceTableName(), destTableName(),\n+          stagingLocation);\n+\n+      TableIdentifier v1TableIdentifier = v1SourceTable().identifier();\n+      SparkTableUtil.importSparkTable(spark(), v1TableIdentifier, icebergTable, stagingLocation);\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {\n+        LOG.error(\"Error when attempting to commit snapshot changes, rolling back\");\n+        if (stagedTable != null) {\n+          stagedTable.abortStagedChanges();\n+        }\n+      }\n+    }\n+\n+    long numMigratedFiles;\n+    Snapshot snapshot = icebergTable.currentSnapshot();\n+    numMigratedFiles = Long.valueOf(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+    LOG.info(\"Successfully loaded Iceberg metadata for {} files\", numMigratedFiles);\n+    return numMigratedFiles;\n+  }\n+\n+  private Map<String, String> buildPropertyMap() {\n+    Map<String, String> properties = new HashMap<>();\n+    properties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n+    properties.put(TableProperties.GC_ENABLED, \"false\");\n+    properties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());\n+    properties.putAll(additionalProperties());\n+\n+    // Don't use the default location for the destination table if an alternate has be set\n+    if (destTableLocation != null) {\n+      properties.putAll(tableLocationProperties(destTableLocation));\n+    }\n+\n+    return properties;\n+  }\n+\n+  @Override\n+  protected TableCatalog checkSourceCatalog(CatalogPlugin catalog) {\n+    // Currently the Import code relies on being able to look up the table in the session code\n+    if (!(catalog.name().equals(\"spark_catalog\"))) {\n+      throw new IllegalArgumentException(String.format(\n+          \"Cannot snapshot a table that isn't in spark_catalog, the session catalog. \" +\n+              \"Found source catalog %s\", catalog.name()));\n+    }\n+\n+    if (!(catalog instanceof TableCatalog)) {\n+      throw new IllegalArgumentException(String.format(\n+          \"Cannot snapshot a table from a non-table catalog %s. Catalog has class of %s.\", catalog.name(),\n+          catalog.getClass().toString()\n+      ));\n+    }\n+\n+    return (TableCatalog) catalog;\n+  }\n+\n+  @Override\n+  public SnapshotAction withLocation(String location) {\n+    Preconditions.checkArgument(!sourceTableLocation().equals(location),\n+        \"Cannot create snapshot where destination location is the same as the source location. This\" +", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ4OTk1Nw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538489957", "bodyText": "Do we need to remove any properties, @rdblue? I think we remove at least path internally.", "author": "aokolnychyi", "createdAt": "2020-12-08T15:20:40Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    boolean threw = true;\n+    try {\n+      Map<String, String> newTableProperties = new HashMap<>();\n+      newTableProperties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc0ODU5Ng==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538748596", "bodyText": "Looks like we remove \"path\", \"transient_lastDdlTime\", and \"serialization.format\". We also add this property, and one that signals that it was migrated from Hive, \"migrated-from-hive\" -> \"true\".", "author": "rdblue", "createdAt": "2020-12-08T19:32:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ4OTk1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwOTQ2OQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538809469", "bodyText": "I think this would work well with targetTableProps proposed here.", "author": "aokolnychyi", "createdAt": "2020-12-08T21:09:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ4OTk1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwOTY1OA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538809658", "bodyText": "Having some extra metadata about the original table helps.", "author": "aokolnychyi", "createdAt": "2020-12-08T21:10:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ4OTk1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ5MzMzMg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538493332", "bodyText": "Do we always have a namespace?", "author": "aokolnychyi", "createdAt": "2020-12-08T15:23:43Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    boolean threw = true;\n+    try {\n+      Map<String, String> newTableProperties = new HashMap<>();\n+      newTableProperties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n+      newTableProperties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());\n+      newTableProperties.putAll(tableLocationProperties(sourceTableLocation()));\n+      newTableProperties.putAll(additionalProperties());\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(\n+          destTableName().namespace(),\n+          destTableName().name()),\n+          v1SourceTable().schema(),\n+          sourcePartitionSpec(),\n+          newTableProperties);\n+\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!stagedTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+\n+      String stagingLocation = stagedTable.properties().get(TableProperties.WRITE_METADATA_LOCATION);\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableName(), stagingLocation);\n+\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc0OTU4Nw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538749587", "bodyText": "Yes, because the source catalog is always the session catalog.", "author": "rdblue", "createdAt": "2020-12-08T19:33:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ5MzMzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ5NDY1Ng==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538494656", "bodyText": "nit: sourceTableIdent and destTableIdent", "author": "aokolnychyi", "createdAt": "2020-12-08T15:24:52Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName) {", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ5NTM3NQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538495375", "bodyText": "nit: destTableIdent?", "author": "aokolnychyi", "createdAt": "2020-12-08T15:25:28Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                       CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) ((TableCatalog) sourceCatalog).loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable(sourceCatalogTable);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected CatalogPlugin sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableName() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected Transform[] sourcePartitionSpec() {\n+    return sourceTable.partitioning();\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableName() {", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ5NTY0OA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538495648", "bodyText": "nit: sourceTableIdent?", "author": "aokolnychyi", "createdAt": "2020-12-08T15:25:42Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                       CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) ((TableCatalog) sourceCatalog).loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable(sourceCatalogTable);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected CatalogPlugin sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableName() {", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ5NjU5NA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538496594", "bodyText": "If we move staging to the parent class, this would also be simplified. It is a bit hard to follow now.", "author": "aokolnychyi", "createdAt": "2020-12-08T15:26:40Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    boolean threw = true;\n+    try {\n+      Map<String, String> newTableProperties = new HashMap<>();\n+      newTableProperties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n+      newTableProperties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());\n+      newTableProperties.putAll(tableLocationProperties(sourceTableLocation()));\n+      newTableProperties.putAll(additionalProperties());\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(\n+          destTableName().namespace(),\n+          destTableName().name()),\n+          v1SourceTable().schema(),\n+          sourcePartitionSpec(),\n+          newTableProperties);\n+\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!stagedTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+\n+      String stagingLocation = stagedTable.properties().get(TableProperties.WRITE_METADATA_LOCATION);\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableName(), stagingLocation);\n+\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);\n+      TableIdentifier v1BackupIdentifier = new TableIdentifier(backupIdentifier.name(), backupNamespace);\n+      SparkTableUtil.importSparkTable(spark(), v1BackupIdentifier, icebergTable, stagingLocation);\n+\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+\n+    } catch (TableAlreadyExistsException tableAlreadyExistsException) {", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ0ODI1Mg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539448252", "bodyText": "This will make the errors much more difficult to reason about. Since in \"Snapshot\" we will report \"table already exists\" which might make sense, but in \"migrate\" it makes no sense since our migration has no destination table as it's actually an internal error, ie the table go renamed back during the operation", "author": "RussellSpitzer", "createdAt": "2020-12-09T16:21:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ5NjU5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ5NzQ0NQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538497445", "bodyText": "nit: parseLong", "author": "aokolnychyi", "createdAt": "2020-12-08T15:27:21Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    boolean threw = true;\n+    try {\n+      Map<String, String> newTableProperties = new HashMap<>();\n+      newTableProperties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n+      newTableProperties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());\n+      newTableProperties.putAll(tableLocationProperties(sourceTableLocation()));\n+      newTableProperties.putAll(additionalProperties());\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(\n+          destTableName().namespace(),\n+          destTableName().name()),\n+          v1SourceTable().schema(),\n+          sourcePartitionSpec(),\n+          newTableProperties);\n+\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!stagedTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+\n+      String stagingLocation = stagedTable.properties().get(TableProperties.WRITE_METADATA_LOCATION);\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableName(), stagingLocation);\n+\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);\n+      TableIdentifier v1BackupIdentifier = new TableIdentifier(backupIdentifier.name(), backupNamespace);\n+      SparkTableUtil.importSparkTable(spark(), v1BackupIdentifier, icebergTable, stagingLocation);\n+\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+\n+    } catch (TableAlreadyExistsException tableAlreadyExistsException) {\n+      throw new IllegalArgumentException(\"Cannot migrate because a table was created with the same name as the \" +\n+          \"original was created after we renamed the original table to a backup identifier.\",\n+          tableAlreadyExistsException);\n+    } catch (NoSuchNamespaceException noSuchNamespaceException) {\n+      throw new IllegalArgumentException(\"Cannot migrate because the namespace for the table no longer exists after\" +\n+          \"we renamed the original table to a backup identifier.\",\n+          noSuchNamespaceException);\n+    } finally {\n+\n+      if (threw) {\n+        LOG.error(\"Error when attempting perform migration changes, aborting table creation and restoring backup.\");\n+        try {\n+          destCatalog().renameTable(backupIdentifier, sourceTableName());\n+        } catch (NoSuchTableException nstException) {\n+          throw new IllegalArgumentException(\"Cannot restore backup, the backup cannot be found\", nstException);\n+        } catch (TableAlreadyExistsException taeException) {\n+          throw new IllegalArgumentException(String.format(\"Cannot restore backup, a table with the original name \" +\n+              \"exists. The backup can be found with the name %s\", backupIdentifier.toString()), taeException);\n+        }\n+\n+        try {\n+          if (stagedTable != null) {\n+            stagedTable.abortStagedChanges();\n+          }\n+        } catch (Exception abortException) {\n+          LOG.error(\"Unable to abort staged changes\", abortException);\n+        }\n+      }\n+    }\n+\n+    Snapshot snapshot = icebergTable.currentSnapshot();\n+    long numMigratedFiles = Long.valueOf(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODUwMDMwMQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538500301", "bodyText": "Do we really want to throw a new exception rather than rethrwoing the original exception?\nI kind of liked our internal utility tryWithSafeFinallyAndFailureCallbacks that we copied from Spark for such things.", "author": "aokolnychyi", "createdAt": "2020-12-08T15:29:51Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    boolean threw = true;\n+    try {\n+      Map<String, String> newTableProperties = new HashMap<>();\n+      newTableProperties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n+      newTableProperties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());\n+      newTableProperties.putAll(tableLocationProperties(sourceTableLocation()));\n+      newTableProperties.putAll(additionalProperties());\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(\n+          destTableName().namespace(),\n+          destTableName().name()),\n+          v1SourceTable().schema(),\n+          sourcePartitionSpec(),\n+          newTableProperties);\n+\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!stagedTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+\n+      String stagingLocation = stagedTable.properties().get(TableProperties.WRITE_METADATA_LOCATION);\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableName(), stagingLocation);\n+\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);\n+      TableIdentifier v1BackupIdentifier = new TableIdentifier(backupIdentifier.name(), backupNamespace);\n+      SparkTableUtil.importSparkTable(spark(), v1BackupIdentifier, icebergTable, stagingLocation);\n+\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+\n+    } catch (TableAlreadyExistsException tableAlreadyExistsException) {\n+      throw new IllegalArgumentException(\"Cannot migrate because a table was created with the same name as the \" +\n+          \"original was created after we renamed the original table to a backup identifier.\",\n+          tableAlreadyExistsException);\n+    } catch (NoSuchNamespaceException noSuchNamespaceException) {\n+      throw new IllegalArgumentException(\"Cannot migrate because the namespace for the table no longer exists after\" +\n+          \"we renamed the original table to a backup identifier.\",\n+          noSuchNamespaceException);\n+    } finally {\n+\n+      if (threw) {\n+        LOG.error(\"Error when attempting perform migration changes, aborting table creation and restoring backup.\");\n+        try {\n+          destCatalog().renameTable(backupIdentifier, sourceTableName());\n+        } catch (NoSuchTableException nstException) {\n+          throw new IllegalArgumentException(\"Cannot restore backup, the backup cannot be found\", nstException);", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc1MTkwNw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538751907", "bodyText": "I don't think so. I think this should log and allow the original exception to propagate. It would also be nice to add these as suppressed exceptions (not a blocker).\nThese should also no prevent the abort from happening below.", "author": "rdblue", "createdAt": "2020-12-08T19:37:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODUwMDMwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgxMDc1OA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538810758", "bodyText": "@RussellSpitzer, our tryWithSafeFinallyAndFailureCallbacks does add suppressed exceptions. We can address this in a follow-up but we need to fix other major points.", "author": "aokolnychyi", "createdAt": "2020-12-08T21:12:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODUwMDMwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc3Mjc1Mg==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539772752", "bodyText": "I think we should log the errors for now and update to add suppressed exceptions later.\nSo for now, I think the runtime exceptions here should be changed to log messages.", "author": "rdblue", "createdAt": "2020-12-10T01:25:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODUwMDMwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc0MzAyMw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538743023", "bodyText": "Not a blocker, but we will probably want to relax this constraint when we support direct access to Hive catalogs. Provider is specific to Spark, so if we were to load directly nothing would detect the provider. And in any case, if the provider is \"hive\", it's basically delegating to the partition-level checks. This is nice to catch problems early, though.", "author": "rdblue", "createdAt": "2020-12-08T19:23:33Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                       CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) ((TableCatalog) sourceCatalog).loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable(sourceCatalogTable);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected CatalogPlugin sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableName() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected Transform[] sourcePartitionSpec() {\n+    return sourceTable.partitioning();\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableName() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!ALLOWED_SOURCES.contains(sourceTableProvider)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider));", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc1MTA0NA==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538751044", "bodyText": "Instead of throwing IllegalArgumentException, what about throwing the Iceberg exception for these cases?", "author": "rdblue", "createdAt": "2020-12-08T19:36:04Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    boolean threw = true;\n+    try {\n+      Map<String, String> newTableProperties = new HashMap<>();\n+      newTableProperties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n+      newTableProperties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());\n+      newTableProperties.putAll(tableLocationProperties(sourceTableLocation()));\n+      newTableProperties.putAll(additionalProperties());\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(\n+          destTableName().namespace(),\n+          destTableName().name()),\n+          v1SourceTable().schema(),\n+          sourcePartitionSpec(),\n+          newTableProperties);\n+\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!stagedTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+\n+      String stagingLocation = stagedTable.properties().get(TableProperties.WRITE_METADATA_LOCATION);\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableName(), stagingLocation);\n+\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);\n+      TableIdentifier v1BackupIdentifier = new TableIdentifier(backupIdentifier.name(), backupNamespace);\n+      SparkTableUtil.importSparkTable(spark(), v1BackupIdentifier, icebergTable, stagingLocation);\n+\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+\n+    } catch (TableAlreadyExistsException tableAlreadyExistsException) {\n+      throw new IllegalArgumentException(\"Cannot migrate because a table was created with the same name as the \" +\n+          \"original was created after we renamed the original table to a backup identifier.\",\n+          tableAlreadyExistsException);\n+    } catch (NoSuchNamespaceException noSuchNamespaceException) {\n+      throw new IllegalArgumentException(\"Cannot migrate because the namespace for the table no longer exists after\" +", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc1MjU5Nw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538752597", "bodyText": "Precondition here as well.", "author": "rdblue", "createdAt": "2020-12-08T19:38:29Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    boolean threw = true;\n+    try {\n+      Map<String, String> newTableProperties = new HashMap<>();\n+      newTableProperties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n+      newTableProperties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());\n+      newTableProperties.putAll(tableLocationProperties(sourceTableLocation()));\n+      newTableProperties.putAll(additionalProperties());\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(\n+          destTableName().namespace(),\n+          destTableName().name()),\n+          v1SourceTable().schema(),\n+          sourcePartitionSpec(),\n+          newTableProperties);\n+\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!stagedTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+\n+      String stagingLocation = stagedTable.properties().get(TableProperties.WRITE_METADATA_LOCATION);\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableName(), stagingLocation);\n+\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);\n+      TableIdentifier v1BackupIdentifier = new TableIdentifier(backupIdentifier.name(), backupNamespace);\n+      SparkTableUtil.importSparkTable(spark(), v1BackupIdentifier, icebergTable, stagingLocation);\n+\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+\n+    } catch (TableAlreadyExistsException tableAlreadyExistsException) {\n+      throw new IllegalArgumentException(\"Cannot migrate because a table was created with the same name as the \" +\n+          \"original was created after we renamed the original table to a backup identifier.\",\n+          tableAlreadyExistsException);\n+    } catch (NoSuchNamespaceException noSuchNamespaceException) {\n+      throw new IllegalArgumentException(\"Cannot migrate because the namespace for the table no longer exists after\" +\n+          \"we renamed the original table to a backup identifier.\",\n+          noSuchNamespaceException);\n+    } finally {\n+\n+      if (threw) {\n+        LOG.error(\"Error when attempting perform migration changes, aborting table creation and restoring backup.\");\n+        try {\n+          destCatalog().renameTable(backupIdentifier, sourceTableName());\n+        } catch (NoSuchTableException nstException) {\n+          throw new IllegalArgumentException(\"Cannot restore backup, the backup cannot be found\", nstException);\n+        } catch (TableAlreadyExistsException taeException) {\n+          throw new IllegalArgumentException(String.format(\"Cannot restore backup, a table with the original name \" +\n+              \"exists. The backup can be found with the name %s\", backupIdentifier.toString()), taeException);\n+        }\n+\n+        try {\n+          if (stagedTable != null) {\n+            stagedTable.abortStagedChanges();\n+          }\n+        } catch (Exception abortException) {\n+          LOG.error(\"Unable to abort staged changes\", abortException);\n+        }\n+      }\n+    }\n+\n+    Snapshot snapshot = icebergTable.currentSnapshot();\n+    long numMigratedFiles = Long.valueOf(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+    LOG.info(\"Successfully loaded Iceberg metadata for {} files\", numMigratedFiles);\n+    return numMigratedFiles;\n+  }\n+\n+  @Override\n+  protected TableCatalog checkSourceCatalog(CatalogPlugin catalog) {\n+    // Currently the Import code relies on being able to look up the table in the session code\n+    if (!(catalog instanceof SparkSessionCatalog)) {\n+      throw new IllegalArgumentException(String.format(", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc1NDQ4Ng==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538754486", "bodyText": "Since this logic is the same in both places, this could be moved into the assign function as ensureNameMappingPresent or something.", "author": "rdblue", "createdAt": "2020-12-08T19:41:34Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName) {\n+    super(spark, sourceCatalog, sourceTableName, destCatalog, destTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable;\n+    Table icebergTable;\n+\n+    try {\n+      stagedTable = destCatalog().stageCreate(destTableName(), v1SourceTable().schema(),\n+          sourcePartitionSpec(), buildPropertyMap());\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ2NTQ2Nw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539465467", "bodyText": "sgtm", "author": "RussellSpitzer", "createdAt": "2020-12-09T16:41:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc1NDQ4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc1NTg2OQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538755869", "bodyText": "Nit: prefer Maps.newHashMap().", "author": "rdblue", "createdAt": "2020-12-08T19:43:45Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName) {\n+    super(spark, sourceCatalog, sourceTableName, destCatalog, destTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable;\n+    Table icebergTable;\n+\n+    try {\n+      stagedTable = destCatalog().stageCreate(destTableName(), v1SourceTable().schema(),\n+          sourcePartitionSpec(), buildPropertyMap());\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+    } catch (TableAlreadyExistsException taeException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because a table already exists with that name\",\n+          taeException);\n+    } catch (NoSuchNamespaceException nsnException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because the namespace given does not exist\",\n+          nsnException);\n+    }\n+\n+    boolean threw = true;\n+    try {\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;\n+      LOG.info(\"Beginning snapshot of {} to {} using metadata location {}\", sourceTableName(), destTableName(),\n+          stagingLocation);\n+\n+      TableIdentifier v1TableIdentifier = v1SourceTable().identifier();\n+      SparkTableUtil.importSparkTable(spark(), v1TableIdentifier, icebergTable, stagingLocation);\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {\n+        LOG.error(\"Error when attempting to commit snapshot changes, rolling back\");\n+        if (stagedTable != null) {\n+          stagedTable.abortStagedChanges();\n+        }\n+      }\n+    }\n+\n+    long numMigratedFiles;\n+    Snapshot snapshot = icebergTable.currentSnapshot();\n+    numMigratedFiles = Long.valueOf(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+    LOG.info(\"Successfully loaded Iceberg metadata for {} files\", numMigratedFiles);\n+    return numMigratedFiles;\n+  }\n+\n+  private Map<String, String> buildPropertyMap() {\n+    Map<String, String> properties = new HashMap<>();", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc1ODcyMQ==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538758721", "bodyText": "There are quite a few try/catch blocks for this. Maybe add a version that doesn't throw ParseException and pass in a context string to form the error message. Also, the error message should include the table name that failed parsing:\npublic static CatalogAndIdentifier catalogAndIdentifier(String description, SparkSession spark, String tableName) {\n  try {\n    return catalogAndIdentifier(spark, tableName);\n  } catch (ParseException e) {\n    throw new IllegalArgumentException(\"Cannot parse \" + description + \": \" + tableName, e);\n  }\n}", "author": "rdblue", "createdAt": "2020-12-08T19:48:27Z", "path": "spark3/src/main/java/org/apache/iceberg/actions/SparkActions.java", "diffHunk": "@@ -20,10 +20,54 @@\n package org.apache.iceberg.actions;\n \n import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.Spark3Util;\n import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n \n-class SparkActions extends Actions {\n+public class SparkActions extends Actions {\n   protected SparkActions(SparkSession spark, Table table) {\n     super(spark, table);\n   }\n+\n+  public static CreateAction migrate(String tableName) {\n+    return migrate(SparkSession.active(), tableName);\n+  }\n+\n+  public static CreateAction migrate(SparkSession spark, String tableName) {\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n+    try {\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, tableName);\n+    } catch (ParseException e) {\n+      throw new IllegalArgumentException(\"Cannot parse migrate target\", e);", "originalCommit": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTUwMTE4Nw==", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539501187", "bodyText": "Switched to using the method added by Anton in the other PR", "author": "RussellSpitzer", "createdAt": "2020-12-09T17:26:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc1ODcyMQ=="}], "type": "inlineReview"}, {"oid": "a6b02d26dae1a1c3e6182b86646c832dd2a37640", "url": "https://github.com/apache/iceberg/commit/a6b02d26dae1a1c3e6182b86646c832dd2a37640", "message": "Provide API and Implementation for Creating Iceberg Tables from Spark\n\nPreviously the SparkUtil class provided an importSparkTable command but\nthis command suffered from a few shortcomings. It had a difficult api and\ndid not work directly with DSV2 Iceberg Catalogs. To provide a simpler interface\nthat shares a similar pattern to currently available Spark Actions we introduce\nActions.migrate and Actions.snapshot.\n\nThrough these commands we can both migrate a Table as well as make a snapshot\nof an existing table.", "committedDate": "2020-12-09T16:51:06Z", "type": "commit"}]}