{"pr_number": 1573, "pr_title": "Add AWS subproject and initial S3FileIO implementation", "pr_createdAt": "2020-10-09T23:17:15Z", "pr_url": "https://github.com/apache/iceberg/pull/1573", "timeline": [{"oid": "3f17216d3d3f08148dd74f6f86efd659b4ec8e97", "url": "https://github.com/apache/iceberg/commit/3f17216d3d3f08148dd74f6f86efd659b4ec8e97", "message": "Add AWS subproject and initial S3FileIO implementation", "committedDate": "2020-10-09T23:09:39Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcxNTY0NQ==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r502715645", "bodyText": "I believe this is another of our shaded dependencies, so we'll need to swap it for the relocated version", "author": "RussellSpitzer", "createdAt": "2020-10-09T23:52:04Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3InputStream.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.AmazonS3URI;\n+import com.amazonaws.services.s3.model.GetObjectRequest;\n+import com.amazonaws.services.s3.model.S3ObjectInputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import org.apache.commons.compress.utils.IOUtils;", "originalCommit": "3f17216d3d3f08148dd74f6f86efd659b4ec8e97", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcxNjY2NQ==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r502716665", "bodyText": "<property name=\"illegalPkgs\" value=\"org.elasticsearch.common.base, com.clearspring.analytics.util, org.spark_project.guava, org.sparkproject.guava, org.glassfish.jersey.internal.guava, org.apache.commons.compress.utils\"/>\n            <message key=\"import.illegal\" value=\"Must not import repackaged classes.\"/>\n        </module>\n\nI think our message here is a little wrong in check style, but I believe we are avoiding importing Spark packaged classes?", "author": "RussellSpitzer", "createdAt": "2020-10-09T23:58:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcxNTY0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc0NDI3Nw==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r502744277", "bodyText": "Thanks for pointing this out.  I actually removed this dependency.", "author": "danielcweeks", "createdAt": "2020-10-10T04:25:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcxNTY0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyMDcwMw==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r502720703", "bodyText": "Should this be a lazy client?\n  private transient AmazonS3 client;\n  public AmazonS3 client() {\n    if (client == null) {\n      this.client = s3.get();\n    }\n    return client;\n  }", "author": "rdblue", "createdAt": "2020-10-10T00:24:50Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3FileIO.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.AmazonS3URI;\n+import com.amazonaws.services.s3.model.AmazonS3Exception;\n+import com.amazonaws.services.s3.model.ObjectMetadata;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Objects;\n+import org.apache.http.HttpStatus;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.io.PositionOutputStream;\n+import org.apache.iceberg.io.SeekableInputStream;\n+import org.apache.iceberg.util.SerializableSupplier;\n+\n+/**\n+ * FileIO implementation backed by S3.  Locations used must follow the conventions\n+ * for AmazonS3URIs (e.g. s3://bucket/path/..).\n+ */\n+public class S3FileIO implements FileIO, Serializable {\n+  private final SerializableSupplier<AmazonS3> s3;\n+\n+  public S3FileIO(SerializableSupplier<AmazonS3> s3) {\n+    this.s3 = s3;\n+  }\n+\n+  @Override\n+  public InputFile newInputFile(String path) {\n+    return new S3InputFile(new AmazonS3URI(path));\n+  }\n+\n+  @Override\n+  public OutputFile newOutputFile(String path) {\n+    return new S3OutputFile(new AmazonS3URI(path));\n+  }\n+\n+  @Override\n+  public void deleteFile(String path) {\n+    AmazonS3URI location = new AmazonS3URI(path);\n+    s3.get().deleteObject(location.getBucket(), location.getKey());", "originalCommit": "3f17216d3d3f08148dd74f6f86efd659b4ec8e97", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc0NDE4Nw==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r502744187", "bodyText": "That's probably true in a sense, but I had expected that to be done by the supplier.  Something we should consider as you may want different clients for different sources/buckets/roles, etc.", "author": "danielcweeks", "createdAt": "2020-10-10T04:24:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyMDcwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc0NjEzNA==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r502746134", "bodyText": "Never mind, that make total sense to do.  I added it.", "author": "danielcweeks", "createdAt": "2020-10-10T04:50:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyMDcwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyMTMyNg==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r502721326", "bodyText": "We should probably implement this.", "author": "rdblue", "createdAt": "2020-10-10T00:29:27Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3FileIO.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.AmazonS3URI;\n+import com.amazonaws.services.s3.model.AmazonS3Exception;\n+import com.amazonaws.services.s3.model.ObjectMetadata;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Objects;\n+import org.apache.http.HttpStatus;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.io.PositionOutputStream;\n+import org.apache.iceberg.io.SeekableInputStream;\n+import org.apache.iceberg.util.SerializableSupplier;\n+\n+/**\n+ * FileIO implementation backed by S3.  Locations used must follow the conventions\n+ * for AmazonS3URIs (e.g. s3://bucket/path/..).\n+ */\n+public class S3FileIO implements FileIO, Serializable {\n+  private final SerializableSupplier<AmazonS3> s3;\n+\n+  public S3FileIO(SerializableSupplier<AmazonS3> s3) {\n+    this.s3 = s3;\n+  }\n+\n+  @Override\n+  public InputFile newInputFile(String path) {\n+    return new S3InputFile(new AmazonS3URI(path));\n+  }\n+\n+  @Override\n+  public OutputFile newOutputFile(String path) {\n+    return new S3OutputFile(new AmazonS3URI(path));\n+  }\n+\n+  @Override\n+  public void deleteFile(String path) {\n+    AmazonS3URI location = new AmazonS3URI(path);\n+    s3.get().deleteObject(location.getBucket(), location.getKey());\n+  }\n+\n+  public class S3InputFile implements InputFile {\n+    private final AmazonS3URI location;\n+    private ObjectMetadata metadata;\n+\n+    public S3InputFile(AmazonS3URI location) {\n+      this.location = location;\n+    }\n+\n+    /**\n+     * Note: this may be stale if file was deleted since metadata is cached\n+     * for size/existence checks.\n+     *\n+     * @return content length\n+     */\n+    @Override\n+    public long getLength() {\n+      return Objects.requireNonNull(getObjectMetadata()).getContentLength();\n+    }\n+\n+    @Override\n+    public SeekableInputStream newStream() {\n+      return new S3InputStream(s3.get(), location);\n+    }\n+\n+    @Override\n+    public String location() {\n+      return location.toString();\n+    }\n+\n+    /**\n+     * Note: this may be stale if file was deleted since metadata is cached\n+     * for size/existence checks.\n+     *\n+     * @return flag\n+     */\n+    @Override\n+    public boolean exists() {\n+      return getObjectMetadata() != null;\n+    }\n+\n+    @Override\n+    public String toString() {\n+      return location.toString();\n+    }\n+\n+    private ObjectMetadata getObjectMetadata() {\n+      if (metadata == null) {\n+        try {\n+          metadata = s3.get().getObjectMetadata(location.getBucket(), location.getKey());\n+        } catch (AmazonS3Exception e) {\n+          if (e.getStatusCode() != HttpStatus.SC_NOT_FOUND) {\n+            throw e; // return null if 404 Not Found, otherwise rethrow\n+          }\n+        }\n+      }\n+\n+      return metadata;\n+    }\n+  }\n+\n+  public class S3OutputFile implements OutputFile {\n+    private final AmazonS3URI location;\n+\n+    public S3OutputFile(AmazonS3URI location) {\n+      this.location = location;\n+    }\n+\n+    @Override\n+    public PositionOutputStream create() {\n+      throw new UnsupportedOperationException();", "originalCommit": "3f17216d3d3f08148dd74f6f86efd659b4ec8e97", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc0NjM2NA==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r502746364", "bodyText": "A couple points on this.  I don't see this actually used anywhere except in some tests.\nSecond point is, I'm not sure we can implement the stated contract around this behavior.  With S3 we cannot guarantee an atomic create (there are both collisions as well as inconsistency).\nI feel like for now we should throw so if in the future there is an expectation around that behavior, it will break and we can address it then.", "author": "danielcweeks", "createdAt": "2020-10-10T04:53:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyMTMyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE4NTY4Mg==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r511185682", "bodyText": "This is okay for now, but we should eventually add it. There isn't an atomicity guarantee about the final file here. It just creates an output stream for the file that will create the file on close. That close can fail if the file was created concurrently. I think the only guarantee here is that the file didn't exist when this method is called.", "author": "rdblue", "createdAt": "2020-10-23T22:42:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyMTMyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyMTYzOQ==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r502721639", "bodyText": "Why not implement it anyway? All we need to do is delegate to stream, right?", "author": "rdblue", "createdAt": "2020-10-10T00:31:49Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3InputStream.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.AmazonS3URI;\n+import com.amazonaws.services.s3.model.GetObjectRequest;\n+import com.amazonaws.services.s3.model.S3ObjectInputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import org.apache.commons.compress.utils.IOUtils;\n+import org.apache.iceberg.io.SeekableInputStream;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class S3InputStream extends SeekableInputStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(S3InputStream.class);\n+\n+  private final StackTraceElement[] createStack;\n+  private final AmazonS3 s3;\n+  private final AmazonS3URI location;\n+\n+  private S3ObjectInputStream stream;\n+  private long pos = 0;\n+  private long next = 0;\n+  private boolean closed = false;\n+\n+  private int skipSize = 1024 * 1024;\n+\n+  public S3InputStream(AmazonS3 s3, AmazonS3URI location) {\n+    this.s3 = s3;\n+    this.location = location;\n+\n+    createStack = Thread.currentThread().getStackTrace();\n+  }\n+\n+  @Override\n+  public long getPos() {\n+    return pos;\n+  }\n+\n+  @Override\n+  public void seek(long newPos) {\n+    Preconditions.checkState(!closed, \"already closed\");\n+    Preconditions.checkArgument(newPos >= 0, \"position is negative: %s\", newPos);\n+\n+    // this allows a seek beyond the end of the stream but the next read will fail\n+    next = newPos;\n+  }\n+\n+  @Override\n+  public int read() {\n+    // This stream is wrapped with BufferedInputStream, so this method should never be called\n+    throw new UnsupportedOperationException();", "originalCommit": "3f17216d3d3f08148dd74f6f86efd659b4ec8e97", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyMTg1MA==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r502721850", "bodyText": "One argument would be that if you're hitting this, you're doing something wrong (and risk poor performance). Throwing here is a form of defensive programming.", "author": "jacques-n", "createdAt": "2020-10-10T00:33:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyMTYzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyMzU4NA==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r502723584", "bodyText": "@jacques-n, isn't the underlying stream buffered, though? Why require wrapping this one in a BufferedStream as well?\nThere are a few places where we use read() directly, assuming the data is buffered. Like in the vectorized reader base class: https://github.com/apache/iceberg/blob/master/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/BaseVectorizedParquetValuesReader.java#L147\nA one significant speedup we saw when working on vectorization was avoiding code in Parquet that would read into a 4- or 8-byte buffer and then access each byte individual to reconstruct numeric values. Maybe there is a better way that what we're currently doing though?", "author": "rdblue", "createdAt": "2020-10-10T00:48:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyMTYzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNTQ2NQ==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r502725465", "bodyText": "Good questions. I haven't looked at the underlying reader to confirm so it may be a moot point.\nGenerally, in our other performance work we've tried to move away from InputStream entirely. For example, we expose this as the only input mechanism for streams.\nThen higher levels consumers should be preallocating space based on say, page size, do a bulk pull from the underlying system (say s3) and then do all byte reads using unsafe byte reads or similar (as opposed to an inputstream stack). In those cases, (hopefully) you can use the appropriate intrinsic operations.\nFor reference, the impl for S3 of the above are in these two classes: Using AWS Sync Client, Useing AWS Async Client\nFeel free to grab anything that seems useful. Unfortunately, we don't actually use the latter true async implementation because we've found the AWS Java SDk2 async api for S3 to be very buggy (or at least it was when were last testing it). In cases for Azure and similar, we actually chose to use the relatively strong AsyncHttpClient as you can find here.", "author": "jacques-n", "createdAt": "2020-10-10T01:04:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyMTYzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc0NzAzNA==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r502747034", "bodyText": "@jacques-n, we have some similar optimizations as well and I think they would be great follow ons to an initial implementation.  For example, we have a vectored-io implementation for Parquet that will only do bounded range gets from S3, which is more efficient for connections and limits any buffered reads by the sdk past the intended range.\nHowever, for now, I think we should keep this relatively straightforward so we have a working initial point.\nI'll definitely take a look at the sync and async clients, those look promising.", "author": "danielcweeks", "createdAt": "2020-10-10T05:02:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyMTYzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2MDk5OQ==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r502760999", "bodyText": "I'm in agreement on getting a working initial implementation in and then optimizing it later. We use similar optimizations when reading / writing directly to S3 from plain old java programs.\nThis might somewhat go against the grain of how FileIO works, but if there were a filesystem concept like exists in some of the python code, we could make use of the multipart file upload manager and then upload based on the configured block size, which makes reads from S3 (when reading along the same block size) significantly more performant. It also means that all file uploads have a final commit operation (such that they all succeed or don't). I guess with Iceberg that final commit operation isn't really necessary though, as it's ok for tables to have files in them that don't belong as they will be filtered out by the metadata.\nBut I'm still hugely in favor of making multipart uploads work for us. As well as a more performant way to download only the portion of the file thats needed (e.g. the footer, etc).", "author": "kbendick", "createdAt": "2020-10-10T07:53:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyMTYzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgyMTM3Mw==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r502821373", "bodyText": "@danielcweeks, totally agreed on my comments around patterns not being important notes for this patch. I was only meaning to share some of the ways we've moved away from inputstream in our code.", "author": "jacques-n", "createdAt": "2020-10-10T19:00:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyMTYzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzM4NzA1OA==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r503387058", "bodyText": "@kbendick I think a few other threads on here have touched on some of your points.  We do want to do progressive multipart upload as a follow up.\nFor the read, I think there a few different paths forward as well.  We've implemented a vectored-io to enable more performant reads, and as @jacques-n mentioned, there are other optimizations we can look into for the read path.  These get a little more complicated and may affect some upstream dependencies (like parquet), so I expect to evolve this initial approach.", "author": "danielcweeks", "createdAt": "2020-10-12T15:53:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyMTYzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc4MjE5Mg==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r504782192", "bodyText": "So, back to @rdblue's original comment, I did a little digging to see what is underlying the stream from the aws sdk and there is buffering happening via a SessionInputBuffer on the request.  I'm going to go ahead and implement the read, but I also agree with @jacques-n that it's probably an issue if this ends up being called for performance reasons.", "author": "danielcweeks", "createdAt": "2020-10-14T15:42:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyMTYzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyMjIyMQ==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r502722221", "bodyText": "I'd prefer not to create a direct dependency on commons-compress just for IOUtils.skip. It should be easy enough to write a skipFully method.", "author": "rdblue", "createdAt": "2020-10-10T00:36:32Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3InputStream.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.AmazonS3URI;\n+import com.amazonaws.services.s3.model.GetObjectRequest;\n+import com.amazonaws.services.s3.model.S3ObjectInputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import org.apache.commons.compress.utils.IOUtils;", "originalCommit": "3f17216d3d3f08148dd74f6f86efd659b4ec8e97", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc0NDA3Mg==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r502744072", "bodyText": "Yeah, I actually removed this and when back to what the original PrestoS3FileSystem implementation does (also added credit).", "author": "danielcweeks", "createdAt": "2020-10-10T04:23:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyMjIyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc1MzA5OA==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r502753098", "bodyText": "This feels somewhat unnecessary just for the warning. In the linked examples from @jacques-n, they use thread name instead via Thread.currentThread().getName();. Would that be a cleaner approach? What value does having the stack trace from when the S3InputStream was instantiated bring?", "author": "kbendick", "createdAt": "2020-10-10T06:20:21Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3InputStream.java", "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.AmazonS3URI;\n+import com.amazonaws.services.s3.model.GetObjectRequest;\n+import com.amazonaws.services.s3.model.S3ObjectInputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import org.apache.iceberg.io.SeekableInputStream;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class S3InputStream extends SeekableInputStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(S3InputStream.class);\n+\n+  private final StackTraceElement[] createStack;\n+  private final AmazonS3 s3;\n+  private final AmazonS3URI location;\n+\n+  private S3ObjectInputStream stream;\n+  private long pos = 0;\n+  private long next = 0;\n+  private boolean closed = false;\n+\n+  private int skipSize = 1024 * 1024;\n+\n+  public S3InputStream(AmazonS3 s3, AmazonS3URI location) {\n+    this.s3 = s3;\n+    this.location = location;\n+\n+    createStack = Thread.currentThread().getStackTrace();", "originalCommit": "200a8d7eec2947c115556cf2e1332de52d754bff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgyOTc4Mg==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r502829782", "bodyText": "This is used to show you where a leaked stream was created. I've found it helpful for tracking down paths with missing close calls.", "author": "rdblue", "createdAt": "2020-10-10T20:36:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc1MzA5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2MTczOA==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r502761738", "bodyText": "Do you intend to call the helper function you've written closeStream here or otherwise close or abort the S3ObjectInputStream? Seems like the input stream and any on going http requests won't get appropriately aborted / closed otherwise.", "author": "kbendick", "createdAt": "2020-10-10T08:01:37Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3InputStream.java", "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.AmazonS3URI;\n+import com.amazonaws.services.s3.model.GetObjectRequest;\n+import com.amazonaws.services.s3.model.S3ObjectInputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import org.apache.iceberg.io.SeekableInputStream;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class S3InputStream extends SeekableInputStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(S3InputStream.class);\n+\n+  private final StackTraceElement[] createStack;\n+  private final AmazonS3 s3;\n+  private final AmazonS3URI location;\n+\n+  private S3ObjectInputStream stream;\n+  private long pos = 0;\n+  private long next = 0;\n+  private boolean closed = false;\n+\n+  private int skipSize = 1024 * 1024;\n+\n+  public S3InputStream(AmazonS3 s3, AmazonS3URI location) {\n+    this.s3 = s3;\n+    this.location = location;\n+\n+    createStack = Thread.currentThread().getStackTrace();\n+  }\n+\n+  @Override\n+  public long getPos() {\n+    return pos;\n+  }\n+\n+  @Override\n+  public void seek(long newPos) {\n+    Preconditions.checkState(!closed, \"already closed\");\n+    Preconditions.checkArgument(newPos >= 0, \"position is negative: %s\", newPos);\n+\n+    // this allows a seek beyond the end of the stream but the next read will fail\n+    next = newPos;\n+  }\n+\n+  @Override\n+  public int read() {\n+    // This stream is wrapped with BufferedInputStream, so this method should never be called\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public int read(byte[] b, int off, int len) throws IOException {\n+    Preconditions.checkState(!closed, \"already closed\");\n+    positionStream();\n+\n+    int bytesRead = stream.read(b, off, len);\n+    pos += bytesRead;\n+    next += bytesRead;\n+\n+    return bytesRead;\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    super.close();\n+    closed = true;", "originalCommit": "200a8d7eec2947c115556cf2e1332de52d754bff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzM4NDY4Mw==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r503384683", "bodyText": "Great catch.  Yes, it should be aborted in the close.", "author": "danielcweeks", "createdAt": "2020-10-12T15:49:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2MTczOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2MjI2Mg==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r502762262", "bodyText": "Should a null check be added here to ensure that the previous stream was closed / aborted? I see that you're using this function immediately after calling closeStream, which has the null check. However, I worry that in a future refactor the null check between closeStream and openStream will be lost and then resources will be potentially leaked. But this is not a blocker since it does currently have the check between them.", "author": "kbendick", "createdAt": "2020-10-10T08:07:11Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3InputStream.java", "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.AmazonS3URI;\n+import com.amazonaws.services.s3.model.GetObjectRequest;\n+import com.amazonaws.services.s3.model.S3ObjectInputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import org.apache.iceberg.io.SeekableInputStream;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class S3InputStream extends SeekableInputStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(S3InputStream.class);\n+\n+  private final StackTraceElement[] createStack;\n+  private final AmazonS3 s3;\n+  private final AmazonS3URI location;\n+\n+  private S3ObjectInputStream stream;\n+  private long pos = 0;\n+  private long next = 0;\n+  private boolean closed = false;\n+\n+  private int skipSize = 1024 * 1024;\n+\n+  public S3InputStream(AmazonS3 s3, AmazonS3URI location) {\n+    this.s3 = s3;\n+    this.location = location;\n+\n+    createStack = Thread.currentThread().getStackTrace();\n+  }\n+\n+  @Override\n+  public long getPos() {\n+    return pos;\n+  }\n+\n+  @Override\n+  public void seek(long newPos) {\n+    Preconditions.checkState(!closed, \"already closed\");\n+    Preconditions.checkArgument(newPos >= 0, \"position is negative: %s\", newPos);\n+\n+    // this allows a seek beyond the end of the stream but the next read will fail\n+    next = newPos;\n+  }\n+\n+  @Override\n+  public int read() {\n+    // This stream is wrapped with BufferedInputStream, so this method should never be called\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public int read(byte[] b, int off, int len) throws IOException {\n+    Preconditions.checkState(!closed, \"already closed\");\n+    positionStream();\n+\n+    int bytesRead = stream.read(b, off, len);\n+    pos += bytesRead;\n+    next += bytesRead;\n+\n+    return bytesRead;\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    super.close();\n+    closed = true;\n+  }\n+\n+  private void positionStream() throws IOException {\n+    if ((stream != null) && (next == pos)) {\n+      // already at specified position\n+      return;\n+    }\n+\n+    if ((stream != null) && (next > pos)) {\n+      // seeking forwards\n+      long skip = next - pos;\n+      if (skip <= Math.max(stream.available(), skipSize)) {\n+        // already buffered or seek is small enough\n+        LOG.debug(\"Read-through seek for {} to offset {}\", location, next);\n+        try {\n+          if (stream.skip(skip) == skip) {\n+            pos = next;\n+            return;\n+          }\n+        } catch (IOException ignored) {\n+          // will retry by re-opening the stream\n+        }\n+      }\n+    }\n+\n+    // close the stream and open at desired position\n+    LOG.debug(\"Seek with new stream for {} to offset {}\", location, next);\n+    pos = next;\n+    closeStream();\n+    openStream();\n+  }\n+\n+  private void openStream() {\n+    GetObjectRequest request = new GetObjectRequest(location.getBucket(), location.getKey())\n+        .withRange(pos, Long.MAX_VALUE);\n+    stream = s3.getObject(request).getObjectContent();", "originalCommit": "200a8d7eec2947c115556cf2e1332de52d754bff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc5MjgzNg==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r504792836", "bodyText": "So, I changed this a little bit so that the closing is handled just prior to opening the new stream, so that there's less chance it would be leaked in the future.  I also updated to not call the sream.abort() because that is actually handled by the underlying stream and will abort if necessary.  Now we are just calling close(), which is probably more standard.", "author": "danielcweeks", "createdAt": "2020-10-14T15:57:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2MjI2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE4MzU2Nw==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r511183567", "bodyText": "Minor: it's hard to follow the builders within builders within builders. Might be easier to separate them out into local variables.", "author": "rdblue", "createdAt": "2020-10-23T22:34:40Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3FileIO.java", "diffHunk": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import com.amazonaws.services.s3.AmazonS3URI;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Objects;\n+import org.apache.http.HttpStatus;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.io.PositionOutputStream;\n+import org.apache.iceberg.io.SeekableInputStream;\n+import org.apache.iceberg.util.SerializableSupplier;\n+import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.Delete;\n+import software.amazon.awssdk.services.s3.model.DeleteObjectsRequest;\n+import software.amazon.awssdk.services.s3.model.HeadObjectRequest;\n+import software.amazon.awssdk.services.s3.model.HeadObjectResponse;\n+import software.amazon.awssdk.services.s3.model.ObjectIdentifier;\n+import software.amazon.awssdk.services.s3.model.S3Exception;\n+\n+/**\n+ * FileIO implementation backed by S3.  Locations used must follow the conventions for URIs (e.g. s3://bucket/path/..).\n+ */\n+public class S3FileIO implements FileIO, Serializable {\n+  private final SerializableSupplier<S3Client> s3;\n+  private transient S3Client client;\n+\n+  public S3FileIO(SerializableSupplier<S3Client> s3) {\n+    this.s3 = s3;\n+  }\n+\n+  @Override\n+  public InputFile newInputFile(String path) {\n+    return new S3InputFile(new AmazonS3URI(path));\n+  }\n+\n+  @Override\n+  public OutputFile newOutputFile(String path) {\n+    return new S3OutputFile(new AmazonS3URI(path));\n+  }\n+\n+  @Override\n+  public void deleteFile(String path) {\n+    AmazonS3URI location = new AmazonS3URI(path);\n+    client().deleteObjects(DeleteObjectsRequest.builder()\n+        .bucket(location.getBucket())\n+        .delete(Delete.builder()\n+            .objects(ObjectIdentifier.builder()\n+                .key(location.getKey())\n+                .build())\n+            .build())\n+        .build());", "originalCommit": "3c948aa465c01dbe8159117b95725e3c36a7d76b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE4NDczMQ==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r511184731", "bodyText": "This is going to throw a NullPointerException if the object doesn't exist because getObjectMetadata returns null when it receives a 404. I think it would be better for getObjectMetadata to throw NotFoundException with a friendly error message that includes the location.", "author": "rdblue", "createdAt": "2020-10-23T22:39:02Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3FileIO.java", "diffHunk": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import com.amazonaws.services.s3.AmazonS3URI;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.Objects;\n+import org.apache.http.HttpStatus;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.io.PositionOutputStream;\n+import org.apache.iceberg.io.SeekableInputStream;\n+import org.apache.iceberg.util.SerializableSupplier;\n+import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.Delete;\n+import software.amazon.awssdk.services.s3.model.DeleteObjectsRequest;\n+import software.amazon.awssdk.services.s3.model.HeadObjectRequest;\n+import software.amazon.awssdk.services.s3.model.HeadObjectResponse;\n+import software.amazon.awssdk.services.s3.model.ObjectIdentifier;\n+import software.amazon.awssdk.services.s3.model.S3Exception;\n+\n+/**\n+ * FileIO implementation backed by S3.  Locations used must follow the conventions for URIs (e.g. s3://bucket/path/..).\n+ */\n+public class S3FileIO implements FileIO, Serializable {\n+  private final SerializableSupplier<S3Client> s3;\n+  private transient S3Client client;\n+\n+  public S3FileIO(SerializableSupplier<S3Client> s3) {\n+    this.s3 = s3;\n+  }\n+\n+  @Override\n+  public InputFile newInputFile(String path) {\n+    return new S3InputFile(new AmazonS3URI(path));\n+  }\n+\n+  @Override\n+  public OutputFile newOutputFile(String path) {\n+    return new S3OutputFile(new AmazonS3URI(path));\n+  }\n+\n+  @Override\n+  public void deleteFile(String path) {\n+    AmazonS3URI location = new AmazonS3URI(path);\n+    client().deleteObjects(DeleteObjectsRequest.builder()\n+        .bucket(location.getBucket())\n+        .delete(Delete.builder()\n+            .objects(ObjectIdentifier.builder()\n+                .key(location.getKey())\n+                .build())\n+            .build())\n+        .build());\n+  }\n+\n+  private S3Client client() {\n+    if (client == null) {\n+      client = s3.get();\n+    }\n+    return client;\n+  }\n+\n+  public class S3InputFile implements InputFile {\n+    private final AmazonS3URI location;\n+    private HeadObjectResponse metadata;\n+\n+    public S3InputFile(AmazonS3URI location) {\n+      this.location = location;\n+    }\n+\n+    /**\n+     * Note: this may be stale if file was deleted since metadata is cached for size/existence checks.\n+     *\n+     * @return content length\n+     */\n+    @Override\n+    public long getLength() {\n+      return Objects.requireNonNull(getObjectMetadata()).contentLength();", "originalCommit": "3c948aa465c01dbe8159117b95725e3c36a7d76b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ4OTQ1OA==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r511489458", "bodyText": "This is pretty much how presto does this, but I've reworked the error handling so that the underlying S3 exception will propagate for getLength() if it's called on something that doesn't exist (or there is some other error).", "author": "danielcweeks", "createdAt": "2020-10-24T16:28:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE4NDczMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE4NjE0Mw==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r511186143", "bodyText": "Later, we will probably want to pass in these configuration options.", "author": "rdblue", "createdAt": "2020-10-23T22:44:32Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3InputStream.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import com.amazonaws.services.s3.AmazonS3URI;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.Arrays;\n+import org.apache.iceberg.io.SeekableInputStream;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import software.amazon.awssdk.core.sync.ResponseTransformer;\n+import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.GetObjectRequest;\n+\n+public class S3InputStream extends SeekableInputStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(S3InputStream.class);\n+\n+  private final StackTraceElement[] createStack;\n+  private final S3Client s3;\n+  private final AmazonS3URI location;\n+\n+  private InputStream stream;\n+  private long pos = 0;\n+  private long next = 0;\n+  private boolean closed = false;\n+\n+  private int skipSize = 1024 * 1024;", "originalCommit": "3c948aa465c01dbe8159117b95725e3c36a7d76b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ4OTY1MQ==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r511489651", "bodyText": "Yes, this was the only configurable property at this point, so I didn't want to introduce the configuration options, but as we address more functionality (e.g. progressive multipart upload) we will need to find a way to deal with the property configuration as there will be many more options.", "author": "danielcweeks", "createdAt": "2020-10-24T16:30:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE4NjE0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE4Njk1Mw==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r511186953", "bodyText": "This also needs to update pos and next.", "author": "rdblue", "createdAt": "2020-10-23T22:47:50Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3InputStream.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import com.amazonaws.services.s3.AmazonS3URI;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.Arrays;\n+import org.apache.iceberg.io.SeekableInputStream;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import software.amazon.awssdk.core.sync.ResponseTransformer;\n+import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.GetObjectRequest;\n+\n+public class S3InputStream extends SeekableInputStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(S3InputStream.class);\n+\n+  private final StackTraceElement[] createStack;\n+  private final S3Client s3;\n+  private final AmazonS3URI location;\n+\n+  private InputStream stream;\n+  private long pos = 0;\n+  private long next = 0;\n+  private boolean closed = false;\n+\n+  private int skipSize = 1024 * 1024;\n+\n+  public S3InputStream(S3Client s3, AmazonS3URI location) {\n+    this.s3 = s3;\n+    this.location = location;\n+\n+    createStack = Thread.currentThread().getStackTrace();\n+  }\n+\n+  @Override\n+  public long getPos() {\n+    return pos;\n+  }\n+\n+  @Override\n+  public void seek(long newPos) {\n+    Preconditions.checkState(!closed, \"already closed\");\n+    Preconditions.checkArgument(newPos >= 0, \"position is negative: %s\", newPos);\n+\n+    // this allows a seek beyond the end of the stream but the next read will fail\n+    next = newPos;\n+  }\n+\n+  @Override\n+  public int read() throws IOException {\n+    return stream.read();", "originalCommit": "3c948aa465c01dbe8159117b95725e3c36a7d76b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ4OTkxNw==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r511489917", "bodyText": "Nice catch, clearly didn't look closely since we don't use this.  I'll add some tests as well just to make sure it's covered.", "author": "danielcweeks", "createdAt": "2020-10-24T16:33:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE4Njk1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE4NzIxMg==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r511187212", "bodyText": "Nit: we mostly use sentence case for error messages, and try to include the intent: \"Cannot read: already closed\"", "author": "rdblue", "createdAt": "2020-10-23T22:48:55Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3InputStream.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import com.amazonaws.services.s3.AmazonS3URI;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.Arrays;\n+import org.apache.iceberg.io.SeekableInputStream;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import software.amazon.awssdk.core.sync.ResponseTransformer;\n+import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.GetObjectRequest;\n+\n+public class S3InputStream extends SeekableInputStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(S3InputStream.class);\n+\n+  private final StackTraceElement[] createStack;\n+  private final S3Client s3;\n+  private final AmazonS3URI location;\n+\n+  private InputStream stream;\n+  private long pos = 0;\n+  private long next = 0;\n+  private boolean closed = false;\n+\n+  private int skipSize = 1024 * 1024;\n+\n+  public S3InputStream(S3Client s3, AmazonS3URI location) {\n+    this.s3 = s3;\n+    this.location = location;\n+\n+    createStack = Thread.currentThread().getStackTrace();\n+  }\n+\n+  @Override\n+  public long getPos() {\n+    return pos;\n+  }\n+\n+  @Override\n+  public void seek(long newPos) {\n+    Preconditions.checkState(!closed, \"already closed\");\n+    Preconditions.checkArgument(newPos >= 0, \"position is negative: %s\", newPos);\n+\n+    // this allows a seek beyond the end of the stream but the next read will fail\n+    next = newPos;\n+  }\n+\n+  @Override\n+  public int read() throws IOException {\n+    return stream.read();\n+  }\n+\n+  @Override\n+  public int read(byte[] b, int off, int len) throws IOException {\n+    Preconditions.checkState(!closed, \"already closed\");", "originalCommit": "3c948aa465c01dbe8159117b95725e3c36a7d76b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MDAwNg==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r511490006", "bodyText": "Updated message.", "author": "danielcweeks", "createdAt": "2020-10-24T16:34:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE4NzIxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE4Nzk1OA==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r511187958", "bodyText": "Shouldn't this be Math.min? If the stream has 5MB left and the skip size is 1MB, then a 2MB skip should not read through.", "author": "rdblue", "createdAt": "2020-10-23T22:51:49Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3InputStream.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import com.amazonaws.services.s3.AmazonS3URI;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.Arrays;\n+import org.apache.iceberg.io.SeekableInputStream;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import software.amazon.awssdk.core.sync.ResponseTransformer;\n+import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.GetObjectRequest;\n+\n+public class S3InputStream extends SeekableInputStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(S3InputStream.class);\n+\n+  private final StackTraceElement[] createStack;\n+  private final S3Client s3;\n+  private final AmazonS3URI location;\n+\n+  private InputStream stream;\n+  private long pos = 0;\n+  private long next = 0;\n+  private boolean closed = false;\n+\n+  private int skipSize = 1024 * 1024;\n+\n+  public S3InputStream(S3Client s3, AmazonS3URI location) {\n+    this.s3 = s3;\n+    this.location = location;\n+\n+    createStack = Thread.currentThread().getStackTrace();\n+  }\n+\n+  @Override\n+  public long getPos() {\n+    return pos;\n+  }\n+\n+  @Override\n+  public void seek(long newPos) {\n+    Preconditions.checkState(!closed, \"already closed\");\n+    Preconditions.checkArgument(newPos >= 0, \"position is negative: %s\", newPos);\n+\n+    // this allows a seek beyond the end of the stream but the next read will fail\n+    next = newPos;\n+  }\n+\n+  @Override\n+  public int read() throws IOException {\n+    return stream.read();\n+  }\n+\n+  @Override\n+  public int read(byte[] b, int off, int len) throws IOException {\n+    Preconditions.checkState(!closed, \"already closed\");\n+    positionStream();\n+\n+    int bytesRead = stream.read(b, off, len);\n+    pos += bytesRead;\n+    next += bytesRead;\n+\n+    return bytesRead;\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    super.close();\n+    closed = true;\n+    closeStream();\n+  }\n+\n+  private void positionStream() throws IOException {\n+    if ((stream != null) && (next == pos)) {\n+      // already at specified position\n+      return;\n+    }\n+\n+    if ((stream != null) && (next > pos)) {\n+      // seeking forwards\n+      long skip = next - pos;\n+      if (skip <= Math.max(stream.available(), skipSize)) {", "originalCommit": "3c948aa465c01dbe8159117b95725e3c36a7d76b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MDQxOQ==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r511490419", "bodyText": "I think you might be misinterpreting what stream.available() means here.  That should be the number of bytes that can be skipped or read without incurring another blocking io call (not how much is left in the stream).  In this context, that would likely mean that the aws sdk has already internally buffered more than the amount we want to skip, so it would be wasteful to close the current stream and initiate a new request.  In this case want to make sure that the skip is less than the max of buffered or skip limit.", "author": "danielcweeks", "createdAt": "2020-10-24T16:39:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE4Nzk1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE4OTMzNw==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r511189337", "bodyText": "InputStream.skip doesn't guarantee that the bytes skipped is equal to the request: \"The skip method may, for a variety of reasons, end up skipping over some smaller number of bytes, possibly 0.\"\nIn this case, if anything other than the expected number of bytes is skipped, then this will fall back to reconnecting.\nI don't think that's the desired behavior because S3 may already be sending bytes to the reader (and it will be faster to read through) but the skip returns early because not as many bytes as requested were immediately available. I think that this should probably use a skipFully method that loops and skips until there are no more bytes to read through.", "author": "rdblue", "createdAt": "2020-10-23T22:58:00Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3InputStream.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import com.amazonaws.services.s3.AmazonS3URI;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.Arrays;\n+import org.apache.iceberg.io.SeekableInputStream;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import software.amazon.awssdk.core.sync.ResponseTransformer;\n+import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.GetObjectRequest;\n+\n+public class S3InputStream extends SeekableInputStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(S3InputStream.class);\n+\n+  private final StackTraceElement[] createStack;\n+  private final S3Client s3;\n+  private final AmazonS3URI location;\n+\n+  private InputStream stream;\n+  private long pos = 0;\n+  private long next = 0;\n+  private boolean closed = false;\n+\n+  private int skipSize = 1024 * 1024;\n+\n+  public S3InputStream(S3Client s3, AmazonS3URI location) {\n+    this.s3 = s3;\n+    this.location = location;\n+\n+    createStack = Thread.currentThread().getStackTrace();\n+  }\n+\n+  @Override\n+  public long getPos() {\n+    return pos;\n+  }\n+\n+  @Override\n+  public void seek(long newPos) {\n+    Preconditions.checkState(!closed, \"already closed\");\n+    Preconditions.checkArgument(newPos >= 0, \"position is negative: %s\", newPos);\n+\n+    // this allows a seek beyond the end of the stream but the next read will fail\n+    next = newPos;\n+  }\n+\n+  @Override\n+  public int read() throws IOException {\n+    return stream.read();\n+  }\n+\n+  @Override\n+  public int read(byte[] b, int off, int len) throws IOException {\n+    Preconditions.checkState(!closed, \"already closed\");\n+    positionStream();\n+\n+    int bytesRead = stream.read(b, off, len);\n+    pos += bytesRead;\n+    next += bytesRead;\n+\n+    return bytesRead;\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    super.close();\n+    closed = true;\n+    closeStream();\n+  }\n+\n+  private void positionStream() throws IOException {\n+    if ((stream != null) && (next == pos)) {\n+      // already at specified position\n+      return;\n+    }\n+\n+    if ((stream != null) && (next > pos)) {\n+      // seeking forwards\n+      long skip = next - pos;\n+      if (skip <= Math.max(stream.available(), skipSize)) {\n+        // already buffered or seek is small enough\n+        LOG.debug(\"Read-through seek for {} to offset {}\", location, next);\n+        try {\n+          if (stream.skip(skip) == skip) {", "originalCommit": "3c948aa465c01dbe8159117b95725e3c36a7d76b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MDYxOA==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r511490618", "bodyText": "I actually agree with this and flagged it when I saw what presto was doing.  I believe the Presto implementation relies skip skipping correctly or it just falls back to closing the stream and opening a new one (which seems really wasteful).\nI'll update this to skip fully.", "author": "danielcweeks", "createdAt": "2020-10-24T16:41:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE4OTMzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE4OTU3Mw==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r511189573", "bodyText": "This is weird. The format for this argument is \"bytes=(num)-\"?", "author": "rdblue", "createdAt": "2020-10-23T22:59:05Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3InputStream.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import com.amazonaws.services.s3.AmazonS3URI;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.Arrays;\n+import org.apache.iceberg.io.SeekableInputStream;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import software.amazon.awssdk.core.sync.ResponseTransformer;\n+import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.GetObjectRequest;\n+\n+public class S3InputStream extends SeekableInputStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(S3InputStream.class);\n+\n+  private final StackTraceElement[] createStack;\n+  private final S3Client s3;\n+  private final AmazonS3URI location;\n+\n+  private InputStream stream;\n+  private long pos = 0;\n+  private long next = 0;\n+  private boolean closed = false;\n+\n+  private int skipSize = 1024 * 1024;\n+\n+  public S3InputStream(S3Client s3, AmazonS3URI location) {\n+    this.s3 = s3;\n+    this.location = location;\n+\n+    createStack = Thread.currentThread().getStackTrace();\n+  }\n+\n+  @Override\n+  public long getPos() {\n+    return pos;\n+  }\n+\n+  @Override\n+  public void seek(long newPos) {\n+    Preconditions.checkState(!closed, \"already closed\");\n+    Preconditions.checkArgument(newPos >= 0, \"position is negative: %s\", newPos);\n+\n+    // this allows a seek beyond the end of the stream but the next read will fail\n+    next = newPos;\n+  }\n+\n+  @Override\n+  public int read() throws IOException {\n+    return stream.read();\n+  }\n+\n+  @Override\n+  public int read(byte[] b, int off, int len) throws IOException {\n+    Preconditions.checkState(!closed, \"already closed\");\n+    positionStream();\n+\n+    int bytesRead = stream.read(b, off, len);\n+    pos += bytesRead;\n+    next += bytesRead;\n+\n+    return bytesRead;\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    super.close();\n+    closed = true;\n+    closeStream();\n+  }\n+\n+  private void positionStream() throws IOException {\n+    if ((stream != null) && (next == pos)) {\n+      // already at specified position\n+      return;\n+    }\n+\n+    if ((stream != null) && (next > pos)) {\n+      // seeking forwards\n+      long skip = next - pos;\n+      if (skip <= Math.max(stream.available(), skipSize)) {\n+        // already buffered or seek is small enough\n+        LOG.debug(\"Read-through seek for {} to offset {}\", location, next);\n+        try {\n+          if (stream.skip(skip) == skip) {\n+            pos = next;\n+            return;\n+          }\n+        } catch (IOException ignored) {\n+          // will retry by re-opening the stream\n+        }\n+      }\n+    }\n+\n+    // close the stream and open at desired position\n+    LOG.debug(\"Seek with new stream for {} to offset {}\", location, next);\n+    pos = next;\n+    openStream();\n+  }\n+\n+  private void openStream() throws IOException {\n+    GetObjectRequest request = GetObjectRequest.builder()\n+        .bucket(location.getBucket())\n+        .key(location.getKey())\n+        .range(String.format(\"bytes=%s-\", pos))", "originalCommit": "3c948aa465c01dbe8159117b95725e3c36a7d76b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTQ5MDg2MA==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r511490860", "bodyText": "Yeah, I made a comment about this above as well. The v2 way they handle range via the API is based off of rfc2616-sec14 and the API only exposes the ability to set this string.  It feels very awkward compared to the v1 API that exposes the range as long with options for start and end of range.  Weird, but I don't see a better option.", "author": "danielcweeks", "createdAt": "2020-10-24T16:44:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE4OTU3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE5MDA3NQ==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r511190075", "bodyText": "The stream is always non-null. Should we remove this check?", "author": "rdblue", "createdAt": "2020-10-23T23:01:14Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import com.amazonaws.services.s3.AmazonS3URI;\n+import java.io.BufferedOutputStream;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.util.Arrays;\n+import org.apache.iceberg.io.PositionOutputStream;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import software.amazon.awssdk.core.sync.RequestBody;\n+import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.PutObjectRequest;\n+\n+public class S3OutputStream extends PositionOutputStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(S3OutputStream.class);\n+\n+  private final StackTraceElement[] createStack;\n+  private final S3Client s3;\n+  private final AmazonS3URI location;\n+\n+  private final OutputStream stream;\n+  private final File stagingFile;\n+  private long pos = 0;\n+\n+  private boolean closed = false;\n+\n+  public S3OutputStream(S3Client s3, AmazonS3URI location) throws IOException {\n+    this.s3 = s3;\n+    this.location = location;\n+\n+    createStack = Thread.currentThread().getStackTrace();\n+    stagingFile = File.createTempFile(\"s3fileio-\", \".tmp\");\n+    stream = new BufferedOutputStream(new FileOutputStream(stagingFile));\n+\n+    stagingFile.deleteOnExit();\n+  }\n+\n+  @Override\n+  public long getPos() {\n+    return pos;\n+  }\n+\n+  @Override\n+  public void flush() throws IOException {\n+    stream.flush();\n+  }\n+\n+  @Override\n+  public void write(int b) throws IOException {\n+    stream.write(b);\n+    pos += 1;\n+  }\n+\n+  @Override\n+  public void write(byte[] b, int off, int len) throws IOException {\n+    stream.write(b, off, len);\n+    pos += len;\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    super.close();\n+    closed = true;\n+\n+    try {\n+      if (stream != null) {", "originalCommit": "3c948aa465c01dbe8159117b95725e3c36a7d76b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA2OTUzMQ==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r512069531", "bodyText": "yes, done", "author": "danielcweeks", "createdAt": "2020-10-26T15:52:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE5MDA3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE5MTkzOA==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r511191938", "bodyText": "I think this needs to test the different read methods, not just read with a buffer. It also should test the values returned by getPos because those are critical to skipping correctly for read-through seeks.", "author": "rdblue", "createdAt": "2020-10-23T23:09:13Z", "path": "aws/src/test/java/org/apache/iceberg/aws/s3/S3InputStreamTest.java", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import com.adobe.testing.s3mock.junit4.S3MockRule;\n+import com.amazonaws.services.s3.AmazonS3URI;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.Arrays;\n+import java.util.Random;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.iceberg.io.SeekableInputStream;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import software.amazon.awssdk.core.sync.RequestBody;\n+import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.CreateBucketRequest;\n+import software.amazon.awssdk.services.s3.model.PutObjectRequest;\n+\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertThrows;\n+\n+public class S3InputStreamTest {\n+  @ClassRule\n+  public static final S3MockRule S3_MOCK_RULE = S3MockRule.builder().silent().build();\n+\n+  private final S3Client s3 = S3_MOCK_RULE.createS3ClientV2();\n+  private final Random random = new Random(1);\n+\n+  @Before\n+  public void before() {\n+    s3.createBucket(CreateBucketRequest.builder().bucket(\"bucket\").build());\n+  }\n+\n+  @Test\n+  public void testRead() throws Exception {\n+    AmazonS3URI uri = new AmazonS3URI(\"s3://bucket/path/to/read.dat\");\n+    byte[] expected = randomData(1024 * 1024);\n+\n+    writeS3Data(uri, expected);\n+\n+    try (InputStream in = new S3InputStream(s3, uri)) {\n+      byte[] actual = IOUtils.readFully(in, expected.length);\n+      assertArrayEquals(expected, actual);\n+    }", "originalCommit": "3c948aa465c01dbe8159117b95725e3c36a7d76b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjE3NDg4OQ==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r512174889", "bodyText": "I added a number of additional tests including backseeks, read-through, and skip seeks and reads using both implemented paths.  This was actually good because it caught an issue with our lazy position updates for the streams.", "author": "danielcweeks", "createdAt": "2020-10-26T18:20:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE5MTkzOA=="}], "type": "inlineReview"}, {"oid": "57744a815314b175145edac59519f119e0e50e0f", "url": "https://github.com/apache/iceberg/commit/57744a815314b175145edac59519f119e0e50e0f", "message": "Squash changes for reviews", "committedDate": "2020-10-26T18:18:57Z", "type": "forcePushed"}, {"oid": "146737b2dc1fce7f3029f65c0ab6a6a7617088d1", "url": "https://github.com/apache/iceberg/commit/146737b2dc1fce7f3029f65c0ab6a6a7617088d1", "message": "Squash changes for reviews", "committedDate": "2020-10-26T19:40:18Z", "type": "forcePushed"}, {"oid": "dece93ecfa9ec6ba47fc89096228c5ef88164756", "url": "https://github.com/apache/iceberg/commit/dece93ecfa9ec6ba47fc89096228c5ef88164756", "message": "Squash changes for reviews", "committedDate": "2020-10-27T15:19:26Z", "type": "forcePushed"}, {"oid": "04bfdbfd7dc0f9fc2605898880384b12229c5c92", "url": "https://github.com/apache/iceberg/commit/04bfdbfd7dc0f9fc2605898880384b12229c5c92", "message": "Squash changes for reviews", "committedDate": "2020-10-27T19:16:41Z", "type": "commit"}, {"oid": "04bfdbfd7dc0f9fc2605898880384b12229c5c92", "url": "https://github.com/apache/iceberg/commit/04bfdbfd7dc0f9fc2605898880384b12229c5c92", "message": "Squash changes for reviews", "committedDate": "2020-10-27T19:16:41Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYwOTEzNQ==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r514609135", "bodyText": "What is the reasoning behind having a serializable S3Client supplier? Since you are already marking the s3 client as transient, why not just have:\npublic S3FileIO() {\n  client = S3Client.create();\n}\nAlso the AWS client is hard to serialize. You need to serialize things like credential information that might expire after encode and decode. It seems hard to use this constructor unless we also provide an actual implementation of SerializableSupplier<S3Client.", "author": "jackye1995", "createdAt": "2020-10-29T22:43:46Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3FileIO.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import com.amazonaws.services.s3.AmazonS3URI;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import org.apache.http.HttpStatus;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.io.PositionOutputStream;\n+import org.apache.iceberg.io.SeekableInputStream;\n+import org.apache.iceberg.util.SerializableSupplier;\n+import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.Delete;\n+import software.amazon.awssdk.services.s3.model.DeleteObjectsRequest;\n+import software.amazon.awssdk.services.s3.model.HeadObjectRequest;\n+import software.amazon.awssdk.services.s3.model.HeadObjectResponse;\n+import software.amazon.awssdk.services.s3.model.ObjectIdentifier;\n+import software.amazon.awssdk.services.s3.model.S3Exception;\n+\n+/**\n+ * FileIO implementation backed by S3.  Locations used must follow the conventions for URIs (e.g. s3://bucket/path/..).\n+ */\n+public class S3FileIO implements FileIO, Serializable {\n+  private final SerializableSupplier<S3Client> s3;\n+  private transient S3Client client;\n+\n+  public S3FileIO(SerializableSupplier<S3Client> s3) {", "originalCommit": "04bfdbfd7dc0f9fc2605898880384b12229c5c92", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYyOTc5MA==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r514629790", "bodyText": "The SerializableSupplier is there because the client is hard to serialize, as you mentioned. The supplier can carry information needed to create a client and is itself serializable instead.\nWe need to serialize the FileIO because it is sent to tasks in Spark and Flink. If the FileIO can be sent but then can't instantiate a configured client, then it won't work as intended.", "author": "rdblue", "createdAt": "2020-10-29T23:45:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYwOTEzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDY1NTMwMA==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r514655300", "bodyText": "Sorry I wasn't clear in the comment, I understand the serialization aspect of this, and what I am proposing is to have a default behavior that does not serialize the client, and simply recreate it if it is null:\n    private final SerializableSupplier<S3Client> s3;\n    private transient S3Client client;\n\n    public S3FileIO() {\n        this.s3 = null;\n    }\n\n    public S3FileIO(SerializableSupplier<S3Client> s3) {\n        this.s3 = s3;\n    }\n\n    private S3Client client() {\n        if (client == null) {\n            if (s3 != null) {\n                client = s3.get();\n            } else {\n                client = S3Client.create();\n            }\n        }\n        return client;\n    }\nThe reason is because the environment that deserializes the object is very likely to have the AWS configurations for the client to pick up and re-initialize itself. This is the situation for all AWS compute including EC2, EMR, ECS, Lambda. By having this default, most users can just use it as new S3FileIO() without the need to implement a serialization supplier.\nThe only situation left is the users with customized components such as a customized credential provider or HTTP client. In those cases, the current constructor can be used with a customized supplier.", "author": "jackye1995", "createdAt": "2020-10-30T01:09:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYwOTEzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIxNTcxOQ==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r515215719", "bodyText": "It makes sense to me to have a constructor that doesn't require the client supplier.", "author": "rdblue", "createdAt": "2020-10-30T16:16:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYwOTEzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk1ODY1Ng==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r516958656", "bodyText": "I'll add the default constructor.", "author": "danielcweeks", "createdAt": "2020-11-03T21:13:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYwOTEzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYwOTgwNw==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r514609807", "bodyText": "Is it better to not nest this class inside S3FileIO? In this way it is more consistent with HadoopInputFile", "author": "jackye1995", "createdAt": "2020-10-29T22:45:29Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3FileIO.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import com.amazonaws.services.s3.AmazonS3URI;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import org.apache.http.HttpStatus;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.io.PositionOutputStream;\n+import org.apache.iceberg.io.SeekableInputStream;\n+import org.apache.iceberg.util.SerializableSupplier;\n+import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.Delete;\n+import software.amazon.awssdk.services.s3.model.DeleteObjectsRequest;\n+import software.amazon.awssdk.services.s3.model.HeadObjectRequest;\n+import software.amazon.awssdk.services.s3.model.HeadObjectResponse;\n+import software.amazon.awssdk.services.s3.model.ObjectIdentifier;\n+import software.amazon.awssdk.services.s3.model.S3Exception;\n+\n+/**\n+ * FileIO implementation backed by S3.  Locations used must follow the conventions for URIs (e.g. s3://bucket/path/..).\n+ */\n+public class S3FileIO implements FileIO, Serializable {\n+  private final SerializableSupplier<S3Client> s3;\n+  private transient S3Client client;\n+\n+  public S3FileIO(SerializableSupplier<S3Client> s3) {\n+    this.s3 = s3;\n+  }\n+\n+  @Override\n+  public InputFile newInputFile(String path) {\n+    return new S3InputFile(new AmazonS3URI(path));\n+  }\n+\n+  @Override\n+  public OutputFile newOutputFile(String path) {\n+    return new S3OutputFile(new AmazonS3URI(path));\n+  }\n+\n+  @Override\n+  public void deleteFile(String path) {\n+    AmazonS3URI location = new AmazonS3URI(path);\n+    ObjectIdentifier objectIdentifier = ObjectIdentifier.builder().key(location.getKey()).build();\n+    Delete delete = Delete.builder().objects(objectIdentifier).build();\n+    DeleteObjectsRequest deleteRequest =\n+        DeleteObjectsRequest.builder().bucket(location.getBucket()).delete(delete).build();\n+\n+    client().deleteObjects(deleteRequest);\n+  }\n+\n+  private S3Client client() {\n+    if (client == null) {\n+      client = s3.get();\n+    }\n+    return client;\n+  }\n+\n+  public class S3InputFile implements InputFile {", "originalCommit": "04bfdbfd7dc0f9fc2605898880384b12229c5c92", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDY1Njg1Mw==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r514656853", "bodyText": "And same comment for S3OutputFile", "author": "jackye1995", "createdAt": "2020-10-30T01:11:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYwOTgwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk1NTEwNg==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r516955106", "bodyText": "I don't think there's any need to be consistent across these implementations.  Also in this case they share the client from S3FileIO, which would lend itself an inner class implementation.  I feel like this is more a stylistic difference and with how little code is involved in the inner classes, I find it more cohesive.", "author": "danielcweeks", "createdAt": "2020-11-03T21:05:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYwOTgwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAwMDA1Mw==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r517000053", "bodyText": "Ok, so after saying that, I did end up splitting it out due to implementing create causing me to rethink some of the shared functionality between input/output.  It got a bit more complicated and was probably more logic than should be embedded as inner classes.  They're now split out.", "author": "danielcweeks", "createdAt": "2020-11-03T22:46:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYwOTgwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYxMDc3Mg==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r514610772", "bodyText": "FileIO already implements Serializable, no need to declare it again.", "author": "jackye1995", "createdAt": "2020-10-29T22:48:12Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3FileIO.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import com.amazonaws.services.s3.AmazonS3URI;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import org.apache.http.HttpStatus;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.io.PositionOutputStream;\n+import org.apache.iceberg.io.SeekableInputStream;\n+import org.apache.iceberg.util.SerializableSupplier;\n+import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.Delete;\n+import software.amazon.awssdk.services.s3.model.DeleteObjectsRequest;\n+import software.amazon.awssdk.services.s3.model.HeadObjectRequest;\n+import software.amazon.awssdk.services.s3.model.HeadObjectResponse;\n+import software.amazon.awssdk.services.s3.model.ObjectIdentifier;\n+import software.amazon.awssdk.services.s3.model.S3Exception;\n+\n+/**\n+ * FileIO implementation backed by S3.  Locations used must follow the conventions for URIs (e.g. s3://bucket/path/..).\n+ */\n+public class S3FileIO implements FileIO, Serializable {", "originalCommit": "04bfdbfd7dc0f9fc2605898880384b12229c5c92", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk1NDcwNg==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r516954706", "bodyText": "While it is not required, there's some difference of opinion on this point.  In cases where we intend it to be serialized (as opposed to simply inheriting that property), some prefer to mark it explicitly for clarity.  Seems like Ryan agrees with your view, so I'll remove it.", "author": "danielcweeks", "createdAt": "2020-11-03T21:05:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYxMDc3Mg=="}], "type": "inlineReview"}, {"oid": "d2ae12190424e6822745b17d89f6396cde83f7ba", "url": "https://github.com/apache/iceberg/commit/d2ae12190424e6822745b17d89f6396cde83f7ba", "message": "Add support for create.  Remove dependency on sdk v1.", "committedDate": "2020-11-03T22:44:57Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAyNjcyMQ==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r517026721", "bodyText": "Why throw IOException if the only place this constructor is called catches it and throws UncheckedIOException?", "author": "rdblue", "createdAt": "2020-11-04T00:07:19Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import java.io.BufferedOutputStream;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.util.Arrays;\n+import org.apache.iceberg.io.PositionOutputStream;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import software.amazon.awssdk.core.sync.RequestBody;\n+import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.PutObjectRequest;\n+\n+public class S3OutputStream extends PositionOutputStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(S3OutputStream.class);\n+\n+  private final StackTraceElement[] createStack;\n+  private final S3Client s3;\n+  private final S3URI location;\n+\n+  private final OutputStream stream;\n+  private final File stagingFile;\n+  private long pos = 0;\n+\n+  private boolean closed = false;\n+\n+  public S3OutputStream(S3Client s3, S3URI location) throws IOException {", "originalCommit": "d2ae12190424e6822745b17d89f6396cde83f7ba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkyNTY4OQ==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r518925689", "bodyText": "This is just more of a convention, which I think makes sense to leave this way.  IO Streams typically throw the IOException (which is why this does because we're wrapping the buffered streams).  However, the OutputFile interface is really what's forcing the change in behavior because create() and createAndOverwrite() do not throw IOException and expect the an unchecked exception.", "author": "danielcweeks", "createdAt": "2020-11-06T18:21:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAyNjcyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAyNjk4Ng==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r517026986", "bodyText": "Is there any need to make this class public? If not, then I'd prefer to keep it package-private to avoid more breakable APIs than necessary.", "author": "rdblue", "createdAt": "2020-11-04T00:08:25Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import java.io.BufferedOutputStream;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.util.Arrays;\n+import org.apache.iceberg.io.PositionOutputStream;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import software.amazon.awssdk.core.sync.RequestBody;\n+import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.PutObjectRequest;\n+\n+public class S3OutputStream extends PositionOutputStream {", "originalCommit": "d2ae12190424e6822745b17d89f6396cde83f7ba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkyNjAxNQ==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r518926015", "bodyText": "I'll change the protection levels.", "author": "danielcweeks", "createdAt": "2020-11-06T18:21:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAyNjk4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAyNzE4Nw==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r517027187", "bodyText": "Similar to the comment on S3OutputFile, I'd like to make as much of the internals as possible package-private to avoid needing to support direct use.", "author": "rdblue", "createdAt": "2020-11-04T00:09:11Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/BaseS3File.java", "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import software.amazon.awssdk.http.HttpStatusCode;\n+import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.HeadObjectRequest;\n+import software.amazon.awssdk.services.s3.model.HeadObjectResponse;\n+import software.amazon.awssdk.services.s3.model.S3Exception;\n+\n+public abstract class BaseS3File {", "originalCommit": "d2ae12190424e6822745b17d89f6396cde83f7ba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkyNjY3MQ==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r518926671", "bodyText": "done", "author": "danielcweeks", "createdAt": "2020-11-06T18:22:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAyNzE4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAyNzM3Mg==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r517027372", "bodyText": "This class already has location() above. How about a package-private uri method?", "author": "rdblue", "createdAt": "2020-11-04T00:09:39Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/BaseS3File.java", "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import software.amazon.awssdk.http.HttpStatusCode;\n+import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.HeadObjectRequest;\n+import software.amazon.awssdk.services.s3.model.HeadObjectResponse;\n+import software.amazon.awssdk.services.s3.model.S3Exception;\n+\n+public abstract class BaseS3File {\n+  private final S3Client client;\n+  private final S3URI location;\n+  private HeadObjectResponse metadata;\n+\n+  public BaseS3File(S3Client client, S3URI location) {\n+    this.client = client;\n+    this.location = location;\n+  }\n+\n+  public String location() {\n+    return location.toString();\n+  }\n+\n+  public S3Client getClient() {\n+    return client;\n+  }\n+\n+  public S3URI getLocation() {", "originalCommit": "d2ae12190424e6822745b17d89f6396cde83f7ba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkzODMwMQ==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r518938301", "bodyText": "updated", "author": "danielcweeks", "createdAt": "2020-11-06T18:45:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAyNzM3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAyNzUxOA==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r517027518", "bodyText": "Could this be package-private client()? (We avoid using get in names because it has little value and is only used in Java conventions.)", "author": "rdblue", "createdAt": "2020-11-04T00:10:14Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/BaseS3File.java", "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import software.amazon.awssdk.http.HttpStatusCode;\n+import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.HeadObjectRequest;\n+import software.amazon.awssdk.services.s3.model.HeadObjectResponse;\n+import software.amazon.awssdk.services.s3.model.S3Exception;\n+\n+public abstract class BaseS3File {\n+  private final S3Client client;\n+  private final S3URI location;\n+  private HeadObjectResponse metadata;\n+\n+  public BaseS3File(S3Client client, S3URI location) {\n+    this.client = client;\n+    this.location = location;\n+  }\n+\n+  public String location() {\n+    return location.toString();\n+  }\n+\n+  public S3Client getClient() {", "originalCommit": "d2ae12190424e6822745b17d89f6396cde83f7ba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkzODQxNg==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r518938416", "bodyText": "updated", "author": "danielcweeks", "createdAt": "2020-11-06T18:46:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAyNzUxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAyOTMxNQ==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r517029315", "bodyText": "Unfortunately, Java's URI has a lot of problems with character escaping. and it's really hard to reconstruct some or all of a URI from the getter methods that it exposes. The uriToString method that we use actually avoids calling URI#toString and instead uses new Path(uri).toString().\nI think it would be better to parse the URI without it. Since we expect a bucket in the URI authority, we can be fairly strict with parsing rules:\n\nSplit by :// to get scheme and bucket/key\nSplit bucket/key by the first / to get bucket and key\nValidate scheme is s3 or equivalent (s3n, s3a)", "author": "rdblue", "createdAt": "2020-11-04T00:16:52Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3URI.java", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import java.net.URI;\n+\n+public class S3URI {\n+  private final URI uri;\n+\n+  public S3URI(URI uri) {\n+    this.uri = uri;\n+  }\n+\n+  public S3URI(String uri) {\n+    this(URI.create(uri));\n+  }\n+\n+  public String bucket() {\n+    return uri.getAuthority();\n+  }\n+\n+  public String key() {\n+    return uri.getPath().startsWith(\"/\") ? uri.getPath().replaceFirst(\"/\", \"\") : uri.getPath();", "originalCommit": "d2ae12190424e6822745b17d89f6396cde83f7ba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkzNjE1NA==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r518936154", "bodyText": "I'm changed this to a custom S3URI implementation that does not rely on URI.  It does basic validation for scheme with know supported types (https, s3, s3a, s3n).  It will also strip query and fragment (some patterns like externally signed urls or other services could leak those into the location).  Beyond that it does no actual validation since the S3 client will do that.", "author": "danielcweeks", "createdAt": "2020-11-06T18:41:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAyOTMxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAzMDI3Ng==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r517030276", "bodyText": "Should this also call write(int)?", "author": "rdblue", "createdAt": "2020-11-04T00:20:26Z", "path": "aws/src/test/java/org/apache/iceberg/aws/s3/S3OutputStreamTest.java", "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import com.adobe.testing.s3mock.junit4.S3MockRule;\n+import java.io.IOException;\n+import java.util.Random;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import software.amazon.awssdk.core.ResponseBytes;\n+import software.amazon.awssdk.core.sync.ResponseTransformer;\n+import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.CreateBucketRequest;\n+import software.amazon.awssdk.services.s3.model.GetObjectRequest;\n+import software.amazon.awssdk.services.s3.model.GetObjectResponse;\n+\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class S3OutputStreamTest {\n+  @ClassRule\n+  public static final S3MockRule S3_MOCK_RULE = S3MockRule.builder().silent().build();\n+\n+  private final S3Client s3 = S3_MOCK_RULE.createS3ClientV2();\n+  private final Random random = new Random(1);\n+\n+  @Before\n+  public void before() {\n+    s3.createBucket(CreateBucketRequest.builder().bucket(\"bucket\").build());\n+  }\n+\n+  @Test\n+  public void getPos() throws IOException {\n+    S3URI uri = new S3URI(\"s3://bucket/path/to/pos.dat\");\n+    int writeSize = 1024;\n+\n+    try (S3OutputStream stream = new S3OutputStream(s3, uri)) {\n+      stream.write(new byte[writeSize]);", "originalCommit": "d2ae12190424e6822745b17d89f6396cde83f7ba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkzNjM3Ng==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r518936376", "bodyText": "Added a separate test for write(int)", "author": "danielcweeks", "createdAt": "2020-11-06T18:41:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAzMDI3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAzMTE4OQ==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r517031189", "bodyText": "This only tests read-through seek. Can you add a test for close/open seek?", "author": "rdblue", "createdAt": "2020-11-04T00:23:47Z", "path": "aws/src/test/java/org/apache/iceberg/aws/s3/S3InputStreamTest.java", "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.aws.s3;\n+\n+import com.adobe.testing.s3mock.junit4.S3MockRule;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Random;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.iceberg.io.SeekableInputStream;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import software.amazon.awssdk.core.sync.RequestBody;\n+import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.CreateBucketRequest;\n+import software.amazon.awssdk.services.s3.model.PutObjectRequest;\n+\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertThrows;\n+\n+public class S3InputStreamTest {\n+  @ClassRule\n+  public static final S3MockRule S3_MOCK_RULE = S3MockRule.builder().silent().build();\n+\n+  private final S3Client s3 = S3_MOCK_RULE.createS3ClientV2();\n+  private final Random random = new Random(1);\n+\n+  @Before\n+  public void before() {\n+    s3.createBucket(CreateBucketRequest.builder().bucket(\"bucket\").build());\n+  }\n+\n+  @Test\n+  public void testRead() throws Exception {\n+    S3URI uri = new S3URI(\"s3://bucket/path/to/read.dat\");\n+    int dataSize = 1024 * 1024 * 10;\n+    byte[] data = randomData(dataSize);\n+\n+    writeS3Data(uri, data);\n+\n+    try (SeekableInputStream in = new S3InputStream(s3, uri)) {\n+      int readSize = 1024;\n+      byte [] actual = new byte[readSize];\n+\n+      readAndCheck(in, in.getPos(), readSize, data, false);\n+      readAndCheck(in, in.getPos(), readSize, data, true);\n+\n+      // Seek forward in current stream\n+      int seekSize = 1024;\n+      readAndCheck(in, in.getPos() + seekSize, readSize, data, false);\n+      readAndCheck(in, in.getPos() + seekSize, readSize, data, true);\n+\n+      // Buffered read\n+      readAndCheck(in, in.getPos(), readSize, data, true);\n+      readAndCheck(in, in.getPos(), readSize, data, false);\n+\n+      // Seek with new stream\n+      long seekNewStreamPosition = 2 * 1024 * 1024;\n+      readAndCheck(in, in.getPos() + seekNewStreamPosition, readSize, data, true);\n+      readAndCheck(in, in.getPos() + seekNewStreamPosition, readSize, data, false);\n+\n+      // Backseek and read\n+      readAndCheck(in, 0, readSize, data, true);\n+      readAndCheck(in, 0, readSize, data, false);\n+    }\n+  }\n+\n+  private void readAndCheck(SeekableInputStream in, long rangeStart, int size, byte [] original, boolean buffered)\n+      throws IOException {\n+    in.seek(rangeStart);\n+    assertEquals(rangeStart, in.getPos());\n+\n+    long rangeEnd = rangeStart + size;\n+    byte [] actual = new byte[size];\n+\n+    if (buffered) {\n+      IOUtils.readFully(in, actual);\n+    } else {\n+      int read = 0;\n+      while (read < size) {\n+        actual[read++] = (byte) in.read();\n+      }\n+    }\n+\n+    assertEquals(rangeEnd, in.getPos());\n+    assertArrayEquals(Arrays.copyOfRange(original, (int) rangeStart, (int) rangeEnd), actual);\n+  }\n+\n+  @Test\n+  public void testClose() throws Exception {\n+    S3URI uri = new S3URI(\"s3://bucket/path/to/closed.dat\");\n+    SeekableInputStream closed = new S3InputStream(s3, uri);\n+    closed.close();\n+    assertThrows(IllegalStateException.class, () -> closed.seek(0));\n+  }\n+\n+  @Test\n+  public void testSeek() throws Exception {\n+    S3URI uri = new S3URI(\"s3://bucket/path/to/seek.dat\");\n+    byte[] expected = randomData(1024 * 1024);\n+\n+    writeS3Data(uri, expected);\n+\n+    try (SeekableInputStream in = new S3InputStream(s3, uri)) {\n+      in.seek(expected.length / 2);\n+      byte[] actual = IOUtils.readFully(in, expected.length / 2);\n+      assertArrayEquals(Arrays.copyOfRange(expected, expected.length / 2, expected.length), actual);\n+    }", "originalCommit": "d2ae12190424e6822745b17d89f6396cde83f7ba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAzMTgyMg==", "url": "https://github.com/apache/iceberg/pull/1573#discussion_r517031822", "bodyText": "Looks like this is done in the read test.", "author": "rdblue", "createdAt": "2020-11-04T00:26:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAzMTE4OQ=="}], "type": "inlineReview"}, {"oid": "1ff6188660a75032b5446d8f549d21e20ede478f", "url": "https://github.com/apache/iceberg/commit/1ff6188660a75032b5446d8f549d21e20ede478f", "message": "Update S3URI implementation and address comments", "committedDate": "2020-11-06T18:45:02Z", "type": "commit"}, {"oid": "bca7c8fffa139e0b6769082297141e6b5167cb55", "url": "https://github.com/apache/iceberg/commit/bca7c8fffa139e0b6769082297141e6b5167cb55", "message": "Merge branch 'master' into s3fileio", "committedDate": "2020-11-06T19:00:30Z", "type": "commit"}, {"oid": "91cc94971df5df6764ed131ea4069e57f47fe2a4", "url": "https://github.com/apache/iceberg/commit/91cc94971df5df6764ed131ea4069e57f47fe2a4", "message": "Fix errorprone check", "committedDate": "2020-11-06T20:08:50Z", "type": "commit"}, {"oid": "0847d4c34e6314f67e99487f92fb7b69ca9f48ab", "url": "https://github.com/apache/iceberg/commit/0847d4c34e6314f67e99487f92fb7b69ca9f48ab", "message": "Fix checkstyle", "committedDate": "2020-11-06T21:40:45Z", "type": "commit"}, {"oid": "8ba2796fd778f99d8b451e2ace696dc4cf68a31c", "url": "https://github.com/apache/iceberg/commit/8ba2796fd778f99d8b451e2ace696dc4cf68a31c", "message": "Checkstyle fixes", "committedDate": "2020-11-06T23:42:27Z", "type": "commit"}, {"oid": "42bed1eac6c482e78e05c6a09282f83c71e82b13", "url": "https://github.com/apache/iceberg/commit/42bed1eac6c482e78e05c6a09282f83c71e82b13", "message": "Merge branch 'master' into s3fileio", "committedDate": "2020-11-07T01:40:28Z", "type": "commit"}]}