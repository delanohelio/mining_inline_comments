{"pr_number": 1623, "pr_title": "Flink:  Add a rewrite datafile action for flink", "pr_createdAt": "2020-10-17T07:22:54Z", "pr_url": "https://github.com/apache/iceberg/pull/1623", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzA1MzI5Nw==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507053297", "bodyText": "Hi ZhangJun, Thanks for your work. Seems the class only used in Flink and the logic here already exists in Spark part. I think it's better to split this pr into 2 parts, one is refactor current rewrite code and the other is 'add a rewrite action for flink'.", "author": "simonsssu", "createdAt": "2020-10-18T09:06:07Z", "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class RewriteDataFilesActionBase<ThisT, R> extends BaseSnapshotUpdateAction<ThisT, R> {", "originalCommit": "0124e3e236f6163551064c43a42a205be2cbd0f6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzA2MzYyOA==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507063628", "bodyText": "hi,simonsssu.thanks for your suggestion. I have opened a new PR #1624  and extracted some common logic. I will do rebase in PR #1624 is merged", "author": "zhangjun0x01", "createdAt": "2020-10-18T09:39:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzA1MzI5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzA1MzU3OQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507053579", "bodyText": "Useless blank line.", "author": "simonsssu", "createdAt": "2020-10-18T09:07:06Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final PartitionSpec spec;\n+  private final Map<String, String> properties;\n+  private final FileFormat format;\n+  private final LocationProvider locations;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final Table table;\n+\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n+                         FileIO io, EncryptionManager encryptionManager) {\n+    this.table = table;\n+    this.schema = table.schema();\n+    this.spec = spec;\n+    this.locations = table.locationProvider();\n+    this.properties = table.properties();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = table.properties().get(DEFAULT_NAME_MAPPING);\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    taskWriterFactory = new RowDataTaskWriterFactory(table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        table.io(),\n+        table.encryption(),\n+        Long.MAX_VALUE,\n+        format,\n+        table.properties());\n+  }\n+\n+\n+  public List<DataFile> rewriteDataForTasks(DataSet<CombinedScanTask> dataSet) throws Exception {\n+\n+    RewriteMap map = new RewriteMap(schema, nameMapping, io, caseSensitive,\n+        encryptionManager,\n+        taskWriterFactory);\n+\n+    DataSet<List<DataFile>> ds = dataSet.map(map);\n+    return ds.collect().stream()\n+        .flatMap(Collection::stream)\n+        .collect(Collectors.toList());\n+  }\n+\n+  public static class RewriteMap extends RichMapFunction<CombinedScanTask, List<DataFile>> {\n+\n+    private transient TaskWriter<RowData> writer;\n+    private transient int subTaskId;\n+    private transient int attemptId;\n+\n+    private final Schema schema;\n+    private final String nameMapping;\n+    private final FileIO io;\n+    private final boolean caseSensitive;\n+    private final EncryptionManager encryptionManager;\n+    private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+    public RewriteMap(Schema schema, String nameMapping, FileIO io, boolean caseSensitive,\n+                      EncryptionManager encryptionManager,\n+                      TaskWriterFactory<RowData> taskWriterFactory) {\n+      this.schema = schema;\n+      this.nameMapping = nameMapping;\n+      this.io = io;\n+      this.caseSensitive = caseSensitive;\n+      this.encryptionManager = encryptionManager;\n+      this.taskWriterFactory = taskWriterFactory;\n+    }\n+\n+    @Override\n+    public void open(Configuration parameters) {\n+      this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+      this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+      // Initialize the task writer factory.\n+      this.taskWriterFactory.initialize(subTaskId, attemptId);\n+\n+    }\n+", "originalCommit": "0124e3e236f6163551064c43a42a205be2cbd0f6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzA1NjkyMg==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507056922", "bodyText": "Can we use a try-with-resource here?  I think it will simply the code to handle exception.", "author": "simonsssu", "createdAt": "2020-10-18T09:18:48Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final PartitionSpec spec;\n+  private final Map<String, String> properties;\n+  private final FileFormat format;\n+  private final LocationProvider locations;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final Table table;\n+\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n+                         FileIO io, EncryptionManager encryptionManager) {\n+    this.table = table;\n+    this.schema = table.schema();\n+    this.spec = spec;\n+    this.locations = table.locationProvider();\n+    this.properties = table.properties();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = table.properties().get(DEFAULT_NAME_MAPPING);\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    taskWriterFactory = new RowDataTaskWriterFactory(table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        table.io(),\n+        table.encryption(),\n+        Long.MAX_VALUE,\n+        format,\n+        table.properties());\n+  }\n+\n+\n+  public List<DataFile> rewriteDataForTasks(DataSet<CombinedScanTask> dataSet) throws Exception {\n+\n+    RewriteMap map = new RewriteMap(schema, nameMapping, io, caseSensitive,\n+        encryptionManager,\n+        taskWriterFactory);\n+\n+    DataSet<List<DataFile>> ds = dataSet.map(map);\n+    return ds.collect().stream()\n+        .flatMap(Collection::stream)\n+        .collect(Collectors.toList());\n+  }\n+\n+  public static class RewriteMap extends RichMapFunction<CombinedScanTask, List<DataFile>> {\n+\n+    private transient TaskWriter<RowData> writer;\n+    private transient int subTaskId;\n+    private transient int attemptId;\n+\n+    private final Schema schema;\n+    private final String nameMapping;\n+    private final FileIO io;\n+    private final boolean caseSensitive;\n+    private final EncryptionManager encryptionManager;\n+    private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+    public RewriteMap(Schema schema, String nameMapping, FileIO io, boolean caseSensitive,\n+                      EncryptionManager encryptionManager,\n+                      TaskWriterFactory<RowData> taskWriterFactory) {\n+      this.schema = schema;\n+      this.nameMapping = nameMapping;\n+      this.io = io;\n+      this.caseSensitive = caseSensitive;\n+      this.encryptionManager = encryptionManager;\n+      this.taskWriterFactory = taskWriterFactory;\n+    }\n+\n+    @Override\n+    public void open(Configuration parameters) {\n+      this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+      this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+      // Initialize the task writer factory.\n+      this.taskWriterFactory.initialize(subTaskId, attemptId);\n+\n+    }\n+\n+\n+    @Override\n+    public List<DataFile> map(CombinedScanTask task) throws Exception {\n+      // Initialize the task writer.\n+      this.writer = taskWriterFactory.create();\n+      RowDataIterator iterator =", "originalCommit": "0124e3e236f6163551064c43a42a205be2cbd0f6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU5OTg3MQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507599871", "bodyText": "I refer to the spark code, but I didn't find a specific reason for doing so, so I copied it to here . I guess it may have encountered some special exceptions.", "author": "zhangjun0x01", "createdAt": "2020-10-19T09:23:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzA1NjkyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzUzNjg1OA==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507536858", "bodyText": "Could we just return  the Lists.newArrayList(writer.complete()) ?", "author": "openinx", "createdAt": "2020-10-19T07:44:16Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final PartitionSpec spec;\n+  private final Map<String, String> properties;\n+  private final FileFormat format;\n+  private final LocationProvider locations;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final Table table;\n+\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n+                         FileIO io, EncryptionManager encryptionManager) {\n+    this.table = table;\n+    this.schema = table.schema();\n+    this.spec = spec;\n+    this.locations = table.locationProvider();\n+    this.properties = table.properties();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = table.properties().get(DEFAULT_NAME_MAPPING);\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    taskWriterFactory = new RowDataTaskWriterFactory(table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        table.io(),\n+        table.encryption(),\n+        Long.MAX_VALUE,\n+        format,\n+        table.properties());\n+  }\n+\n+\n+  public List<DataFile> rewriteDataForTasks(DataSet<CombinedScanTask> dataSet) throws Exception {\n+\n+    RewriteMap map = new RewriteMap(schema, nameMapping, io, caseSensitive,\n+        encryptionManager,\n+        taskWriterFactory);\n+\n+    DataSet<List<DataFile>> ds = dataSet.map(map);\n+    return ds.collect().stream()\n+        .flatMap(Collection::stream)\n+        .collect(Collectors.toList());\n+  }\n+\n+  public static class RewriteMap extends RichMapFunction<CombinedScanTask, List<DataFile>> {\n+\n+    private transient TaskWriter<RowData> writer;\n+    private transient int subTaskId;\n+    private transient int attemptId;\n+\n+    private final Schema schema;\n+    private final String nameMapping;\n+    private final FileIO io;\n+    private final boolean caseSensitive;\n+    private final EncryptionManager encryptionManager;\n+    private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+    public RewriteMap(Schema schema, String nameMapping, FileIO io, boolean caseSensitive,\n+                      EncryptionManager encryptionManager,\n+                      TaskWriterFactory<RowData> taskWriterFactory) {\n+      this.schema = schema;\n+      this.nameMapping = nameMapping;\n+      this.io = io;\n+      this.caseSensitive = caseSensitive;\n+      this.encryptionManager = encryptionManager;\n+      this.taskWriterFactory = taskWriterFactory;\n+    }\n+\n+    @Override\n+    public void open(Configuration parameters) {\n+      this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+      this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+      // Initialize the task writer factory.\n+      this.taskWriterFactory.initialize(subTaskId, attemptId);\n+\n+    }\n+\n+\n+    @Override\n+    public List<DataFile> map(CombinedScanTask task) throws Exception {\n+      // Initialize the task writer.\n+      this.writer = taskWriterFactory.create();\n+      RowDataIterator iterator =\n+          new RowDataIterator(task, io, encryptionManager, schema, schema,\n+              nameMapping, caseSensitive);\n+      try {\n+        while (iterator.hasNext()) {\n+          RowData rowData = iterator.next();\n+          writer.write(rowData);\n+        }\n+        iterator.close();\n+        writer.close();\n+        return Lists.newArrayList(writer.complete()).stream()", "originalCommit": "0124e3e236f6163551064c43a42a205be2cbd0f6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzUzODg1MQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507538851", "bodyText": "The following writer.complete() will close firstly and then return the completed data file list, pls see the javadoc in TaskWriter:\n  /**\n   * Close the writer and get the completed data files.\n   *\n   * @return the completed data files of this task writer.\n   */\n  DataFile[] complete() throws IOException;\nSo here we don't have to call writer.close() now.", "author": "openinx", "createdAt": "2020-10-19T07:47:22Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final PartitionSpec spec;\n+  private final Map<String, String> properties;\n+  private final FileFormat format;\n+  private final LocationProvider locations;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final Table table;\n+\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n+                         FileIO io, EncryptionManager encryptionManager) {\n+    this.table = table;\n+    this.schema = table.schema();\n+    this.spec = spec;\n+    this.locations = table.locationProvider();\n+    this.properties = table.properties();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = table.properties().get(DEFAULT_NAME_MAPPING);\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    taskWriterFactory = new RowDataTaskWriterFactory(table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        table.io(),\n+        table.encryption(),\n+        Long.MAX_VALUE,\n+        format,\n+        table.properties());\n+  }\n+\n+\n+  public List<DataFile> rewriteDataForTasks(DataSet<CombinedScanTask> dataSet) throws Exception {\n+\n+    RewriteMap map = new RewriteMap(schema, nameMapping, io, caseSensitive,\n+        encryptionManager,\n+        taskWriterFactory);\n+\n+    DataSet<List<DataFile>> ds = dataSet.map(map);\n+    return ds.collect().stream()\n+        .flatMap(Collection::stream)\n+        .collect(Collectors.toList());\n+  }\n+\n+  public static class RewriteMap extends RichMapFunction<CombinedScanTask, List<DataFile>> {\n+\n+    private transient TaskWriter<RowData> writer;\n+    private transient int subTaskId;\n+    private transient int attemptId;\n+\n+    private final Schema schema;\n+    private final String nameMapping;\n+    private final FileIO io;\n+    private final boolean caseSensitive;\n+    private final EncryptionManager encryptionManager;\n+    private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+    public RewriteMap(Schema schema, String nameMapping, FileIO io, boolean caseSensitive,\n+                      EncryptionManager encryptionManager,\n+                      TaskWriterFactory<RowData> taskWriterFactory) {\n+      this.schema = schema;\n+      this.nameMapping = nameMapping;\n+      this.io = io;\n+      this.caseSensitive = caseSensitive;\n+      this.encryptionManager = encryptionManager;\n+      this.taskWriterFactory = taskWriterFactory;\n+    }\n+\n+    @Override\n+    public void open(Configuration parameters) {\n+      this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+      this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+      // Initialize the task writer factory.\n+      this.taskWriterFactory.initialize(subTaskId, attemptId);\n+\n+    }\n+\n+\n+    @Override\n+    public List<DataFile> map(CombinedScanTask task) throws Exception {\n+      // Initialize the task writer.\n+      this.writer = taskWriterFactory.create();\n+      RowDataIterator iterator =\n+          new RowDataIterator(task, io, encryptionManager, schema, schema,\n+              nameMapping, caseSensitive);\n+      try {\n+        while (iterator.hasNext()) {\n+          RowData rowData = iterator.next();\n+          writer.write(rowData);\n+        }\n+        iterator.close();\n+        writer.close();", "originalCommit": "0124e3e236f6163551064c43a42a205be2cbd0f6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzUzOTk2Mg==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507539962", "bodyText": "nit:  pls don't left a new empty line here.", "author": "openinx", "createdAt": "2020-10-19T07:49:15Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final PartitionSpec spec;\n+  private final Map<String, String> properties;\n+  private final FileFormat format;\n+  private final LocationProvider locations;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final Table table;\n+\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n+                         FileIO io, EncryptionManager encryptionManager) {\n+    this.table = table;\n+    this.schema = table.schema();\n+    this.spec = spec;\n+    this.locations = table.locationProvider();\n+    this.properties = table.properties();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = table.properties().get(DEFAULT_NAME_MAPPING);\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    taskWriterFactory = new RowDataTaskWriterFactory(table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        table.io(),\n+        table.encryption(),\n+        Long.MAX_VALUE,\n+        format,\n+        table.properties());\n+  }\n+\n+\n+  public List<DataFile> rewriteDataForTasks(DataSet<CombinedScanTask> dataSet) throws Exception {\n+\n+    RewriteMap map = new RewriteMap(schema, nameMapping, io, caseSensitive,\n+        encryptionManager,\n+        taskWriterFactory);\n+\n+    DataSet<List<DataFile>> ds = dataSet.map(map);\n+    return ds.collect().stream()\n+        .flatMap(Collection::stream)\n+        .collect(Collectors.toList());\n+  }\n+\n+  public static class RewriteMap extends RichMapFunction<CombinedScanTask, List<DataFile>> {\n+\n+    private transient TaskWriter<RowData> writer;\n+    private transient int subTaskId;\n+    private transient int attemptId;\n+\n+    private final Schema schema;\n+    private final String nameMapping;\n+    private final FileIO io;\n+    private final boolean caseSensitive;\n+    private final EncryptionManager encryptionManager;\n+    private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+    public RewriteMap(Schema schema, String nameMapping, FileIO io, boolean caseSensitive,\n+                      EncryptionManager encryptionManager,\n+                      TaskWriterFactory<RowData> taskWriterFactory) {\n+      this.schema = schema;\n+      this.nameMapping = nameMapping;\n+      this.io = io;\n+      this.caseSensitive = caseSensitive;\n+      this.encryptionManager = encryptionManager;\n+      this.taskWriterFactory = taskWriterFactory;\n+    }\n+\n+    @Override\n+    public void open(Configuration parameters) {\n+      this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+      this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+      // Initialize the task writer factory.\n+      this.taskWriterFactory.initialize(subTaskId, attemptId);\n+", "originalCommit": "0124e3e236f6163551064c43a42a205be2cbd0f6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU0NzUxNg==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507547516", "bodyText": "I use the iceberg checkstyle to format the code,Is it necessary to add this empty line  limit to checkstyle?", "author": "zhangjun0x01", "createdAt": "2020-10-19T08:01:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzUzOTk2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU0MTkyNg==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507541926", "bodyText": "Those variables spec, properties, locations are never used in this classes , do we need them here ?", "author": "openinx", "createdAt": "2020-10-19T07:52:27Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final PartitionSpec spec;", "originalCommit": "0124e3e236f6163551064c43a42a205be2cbd0f6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU0MjE4OQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507542189", "bodyText": "The same thing for table.", "author": "openinx", "createdAt": "2020-10-19T07:52:56Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final PartitionSpec spec;\n+  private final Map<String, String> properties;\n+  private final FileFormat format;\n+  private final LocationProvider locations;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final Table table;", "originalCommit": "0124e3e236f6163551064c43a42a205be2cbd0f6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU0MzM2NQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507543365", "bodyText": "Maybe it's time to move all the properties parser to a TablePropsUtil now.", "author": "openinx", "createdAt": "2020-10-19T07:54:54Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final PartitionSpec spec;\n+  private final Map<String, String> properties;\n+  private final FileFormat format;\n+  private final LocationProvider locations;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final Table table;\n+\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n+                         FileIO io, EncryptionManager encryptionManager) {\n+    this.table = table;\n+    this.schema = table.schema();\n+    this.spec = spec;\n+    this.locations = table.locationProvider();\n+    this.properties = table.properties();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = table.properties().get(DEFAULT_NAME_MAPPING);\n+\n+    String formatString = table.properties().getOrDefault(", "originalCommit": "0124e3e236f6163551064c43a42a205be2cbd0f6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU0NzYzOQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507547639", "bodyText": "The targetFileSizeBytes will always be unlimited ?  Pls set a configuration keys for it,  here is the discussion we had for spark compaction.", "author": "openinx", "createdAt": "2020-10-19T08:01:55Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final PartitionSpec spec;\n+  private final Map<String, String> properties;\n+  private final FileFormat format;\n+  private final LocationProvider locations;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final Table table;\n+\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n+                         FileIO io, EncryptionManager encryptionManager) {\n+    this.table = table;\n+    this.schema = table.schema();\n+    this.spec = spec;\n+    this.locations = table.locationProvider();\n+    this.properties = table.properties();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = table.properties().get(DEFAULT_NAME_MAPPING);\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    taskWriterFactory = new RowDataTaskWriterFactory(table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        table.io(),\n+        table.encryption(),\n+        Long.MAX_VALUE,", "originalCommit": "0124e3e236f6163551064c43a42a205be2cbd0f6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU2MTU2Nw==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507561567", "bodyText": "I have read this discussion, but my understanding is that the limit of targetFileSizeBytes is in the reader, not the writer.\nthe writer of the spark is here\n    TaskWriter<InternalRow> writer;\n    if (spec.fields().isEmpty()) {\n      writer = new UnpartitionedWriter<>(spec, format, appenderFactory, fileFactory, io.value(), Long.MAX_VALUE);\n    } else {\n      writer = new SparkPartitionedWriter(spec, format, appenderFactory, fileFactory, io.value(), Long.MAX_VALUE,\n          schema, structType);\n    }\n\nI set the targetFileSizeBytes in RewriteDataFilesActionBase#targetSizeInBytes method", "author": "zhangjun0x01", "createdAt": "2020-10-19T08:24:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU0NzYzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU4NzY0Mg==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507587642", "bodyText": "The conclusion should be here.  OK,  we only have to control the read split size and ignore the write target file size.", "author": "openinx", "createdAt": "2020-10-19T09:04:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU0NzYzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU2NTQxNA==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507565414", "bodyText": "nit: Do we need to switch to a new line here ?", "author": "openinx", "createdAt": "2020-10-19T08:30:40Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final PartitionSpec spec;\n+  private final Map<String, String> properties;\n+  private final FileFormat format;\n+  private final LocationProvider locations;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final Table table;\n+\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n+                         FileIO io, EncryptionManager encryptionManager) {\n+    this.table = table;\n+    this.schema = table.schema();\n+    this.spec = spec;\n+    this.locations = table.locationProvider();\n+    this.properties = table.properties();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = table.properties().get(DEFAULT_NAME_MAPPING);\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    taskWriterFactory = new RowDataTaskWriterFactory(table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        table.io(),\n+        table.encryption(),\n+        Long.MAX_VALUE,\n+        format,\n+        table.properties());\n+  }\n+\n+\n+  public List<DataFile> rewriteDataForTasks(DataSet<CombinedScanTask> dataSet) throws Exception {\n+\n+    RewriteMap map = new RewriteMap(schema, nameMapping, io, caseSensitive,\n+        encryptionManager,\n+        taskWriterFactory);\n+\n+    DataSet<List<DataFile>> ds = dataSet.map(map);\n+    return ds.collect().stream()\n+        .flatMap(Collection::stream)\n+        .collect(Collectors.toList());\n+  }\n+\n+  public static class RewriteMap extends RichMapFunction<CombinedScanTask, List<DataFile>> {\n+\n+    private transient TaskWriter<RowData> writer;\n+    private transient int subTaskId;\n+    private transient int attemptId;\n+\n+    private final Schema schema;\n+    private final String nameMapping;\n+    private final FileIO io;\n+    private final boolean caseSensitive;\n+    private final EncryptionManager encryptionManager;\n+    private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+    public RewriteMap(Schema schema, String nameMapping, FileIO io, boolean caseSensitive,\n+                      EncryptionManager encryptionManager,\n+                      TaskWriterFactory<RowData> taskWriterFactory) {\n+      this.schema = schema;\n+      this.nameMapping = nameMapping;\n+      this.io = io;\n+      this.caseSensitive = caseSensitive;\n+      this.encryptionManager = encryptionManager;\n+      this.taskWriterFactory = taskWriterFactory;\n+    }\n+\n+    @Override\n+    public void open(Configuration parameters) {\n+      this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+      this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+      // Initialize the task writer factory.\n+      this.taskWriterFactory.initialize(subTaskId, attemptId);\n+\n+    }\n+\n+\n+    @Override\n+    public List<DataFile> map(CombinedScanTask task) throws Exception {\n+      // Initialize the task writer.\n+      this.writer = taskWriterFactory.create();\n+      RowDataIterator iterator =\n+          new RowDataIterator(task, io, encryptionManager, schema, schema,\n+              nameMapping, caseSensitive);\n+      try {\n+        while (iterator.hasNext()) {\n+          RowData rowData = iterator.next();\n+          writer.write(rowData);\n+        }\n+        iterator.close();\n+        writer.close();\n+        return Lists.newArrayList(writer.complete()).stream()\n+            .collect(Collectors.toList());\n+      } catch (Throwable originalThrowable) {\n+        try {\n+          LOG.error(\"Aborting task\", originalThrowable);\n+\n+          LOG.error(\"Aborting commit for  (subTaskId {}, attemptId {})\",\n+              subTaskId, attemptId);", "originalCommit": "0124e3e236f6163551064c43a42a205be2cbd0f6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU2NTg5OQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507565899", "bodyText": "nit:  iterator is always not-null ?", "author": "openinx", "createdAt": "2020-10-19T08:31:22Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final PartitionSpec spec;\n+  private final Map<String, String> properties;\n+  private final FileFormat format;\n+  private final LocationProvider locations;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final Table table;\n+\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n+                         FileIO io, EncryptionManager encryptionManager) {\n+    this.table = table;\n+    this.schema = table.schema();\n+    this.spec = spec;\n+    this.locations = table.locationProvider();\n+    this.properties = table.properties();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = table.properties().get(DEFAULT_NAME_MAPPING);\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    taskWriterFactory = new RowDataTaskWriterFactory(table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        table.io(),\n+        table.encryption(),\n+        Long.MAX_VALUE,\n+        format,\n+        table.properties());\n+  }\n+\n+\n+  public List<DataFile> rewriteDataForTasks(DataSet<CombinedScanTask> dataSet) throws Exception {\n+\n+    RewriteMap map = new RewriteMap(schema, nameMapping, io, caseSensitive,\n+        encryptionManager,\n+        taskWriterFactory);\n+\n+    DataSet<List<DataFile>> ds = dataSet.map(map);\n+    return ds.collect().stream()\n+        .flatMap(Collection::stream)\n+        .collect(Collectors.toList());\n+  }\n+\n+  public static class RewriteMap extends RichMapFunction<CombinedScanTask, List<DataFile>> {\n+\n+    private transient TaskWriter<RowData> writer;\n+    private transient int subTaskId;\n+    private transient int attemptId;\n+\n+    private final Schema schema;\n+    private final String nameMapping;\n+    private final FileIO io;\n+    private final boolean caseSensitive;\n+    private final EncryptionManager encryptionManager;\n+    private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+    public RewriteMap(Schema schema, String nameMapping, FileIO io, boolean caseSensitive,\n+                      EncryptionManager encryptionManager,\n+                      TaskWriterFactory<RowData> taskWriterFactory) {\n+      this.schema = schema;\n+      this.nameMapping = nameMapping;\n+      this.io = io;\n+      this.caseSensitive = caseSensitive;\n+      this.encryptionManager = encryptionManager;\n+      this.taskWriterFactory = taskWriterFactory;\n+    }\n+\n+    @Override\n+    public void open(Configuration parameters) {\n+      this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+      this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+      // Initialize the task writer factory.\n+      this.taskWriterFactory.initialize(subTaskId, attemptId);\n+\n+    }\n+\n+\n+    @Override\n+    public List<DataFile> map(CombinedScanTask task) throws Exception {\n+      // Initialize the task writer.\n+      this.writer = taskWriterFactory.create();\n+      RowDataIterator iterator =\n+          new RowDataIterator(task, io, encryptionManager, schema, schema,\n+              nameMapping, caseSensitive);\n+      try {\n+        while (iterator.hasNext()) {\n+          RowData rowData = iterator.next();\n+          writer.write(rowData);\n+        }\n+        iterator.close();\n+        writer.close();\n+        return Lists.newArrayList(writer.complete()).stream()\n+            .collect(Collectors.toList());\n+      } catch (Throwable originalThrowable) {\n+        try {\n+          LOG.error(\"Aborting task\", originalThrowable);\n+\n+          LOG.error(\"Aborting commit for  (subTaskId {}, attemptId {})\",\n+              subTaskId, attemptId);\n+          if (iterator != null) {", "originalCommit": "0124e3e236f6163551064c43a42a205be2cbd0f6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU2NzkxNQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507567915", "bodyText": "Does this class need to be serializable ?  I checked the code, seems it won't need to be encoded or decoded for network transferring ?", "author": "openinx", "createdAt": "2020-10-19T08:34:27Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {", "originalCommit": "0124e3e236f6163551064c43a42a205be2cbd0f6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU3Mjg0NQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507572845", "bodyText": "I think here we'd better to throw the exception to upper layer ?  How about throwing the UncheckedIOException(e) ?", "author": "openinx", "createdAt": "2020-10-19T08:41:56Z", "path": "flink/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.api.java.ExecutionEnvironment;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.source.RowDataRewriter;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction extends\n+    RewriteDataFilesActionBase<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private ExecutionEnvironment env = null;\n+  private boolean caseSensitive;\n+\n+  public RewriteDataFilesAction(ExecutionEnvironment env, Table table) {\n+    super(table);\n+    this.env = env;\n+    caseSensitive = Boolean.parseBoolean(table.properties().getOrDefault(\"case-sensitive\", \"false\"));\n+  }\n+\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    Map<StructLikeWrapper, Collection<FileScanTask>> filteredGroupedTasks = getFilteredGroupedTasks();\n+\n+    // Nothing to rewrite if there's only one DataFile in each partition.\n+    if (filteredGroupedTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    List<DataFile> currentDataFiles = getCurrentDataFiles(filteredGroupedTasks);\n+\n+    List<CombinedScanTask> combinedScanTasks = getCombinedScanTasks(filteredGroupedTasks);\n+\n+    DataSet<CombinedScanTask> dataSet = env.fromCollection(combinedScanTasks);\n+    env.setParallelism(combinedScanTasks.size());\n+\n+    RowDataRewriter rowDataRewriter =\n+        new RowDataRewriter(getTable(), getTable().spec(), caseSensitive, getTable().io(), getEncryptionManager());\n+\n+    List<DataFile> addedDataFiles = null;\n+    try {\n+      addedDataFiles = rowDataRewriter.rewriteDataForTasks(dataSet);\n+    } catch (Exception e) {\n+      LOG.error(\"rewrite data files error\", e);", "originalCommit": "0124e3e236f6163551064c43a42a205be2cbd0f6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU4ODM1NQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507588355", "bodyText": "This exception comes from the DataSet#collect method. I looked at this method again. It throws a RuntimeException. I think we also throw a RuntimeException here. What do you think?", "author": "zhangjun0x01", "createdAt": "2020-10-19T09:05:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU3Mjg0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU5MTMyNw==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507591327", "bodyText": "Both RuntimeException  and UncheckedIOException works fine for me. Actually,  UncheckedIOException is also a RuntimeException.", "author": "openinx", "createdAt": "2020-10-19T09:09:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU3Mjg0NQ=="}], "type": "inlineReview"}, {"oid": "4f5c6db70a2e4571cf295caa7f1ed0f6fbadf9ff", "url": "https://github.com/apache/iceberg/commit/4f5c6db70a2e4571cf295caa7f1ed0f6fbadf9ff", "message": "Add a rewrite datafile action for flink", "committedDate": "2020-10-19T10:37:32Z", "type": "forcePushed"}, {"oid": "29d587c26c83215d22c89908e6c4d48e5fc03229", "url": "https://github.com/apache/iceberg/commit/29d587c26c83215d22c89908e6c4d48e5fc03229", "message": "Add a rewrite datafile action for flink", "committedDate": "2020-10-19T11:02:42Z", "type": "forcePushed"}, {"oid": "43083687475865c9d07c2a78287946fc43964ff3", "url": "https://github.com/apache/iceberg/commit/43083687475865c9d07c2a78287946fc43964ff3", "message": "Add a rewrite datafile action for flink\n\nrm PartitionSpec", "committedDate": "2020-10-19T11:21:55Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODIyMTg1NQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r508221855", "bodyText": "Do we really need this class ? Why not use org.apache.iceberg.util.PropertyUtil instead ?", "author": "simonsssu", "createdAt": "2020-10-20T05:39:43Z", "path": "flink/src/main/java/org/apache/iceberg/flink/TablePropertiesUtil.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*", "originalCommit": "43083687475865c9d07c2a78287946fc43964ff3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg1NzUwNg==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r509857506", "bodyText": "yes,we should use PropertyUtil, I had delete the TablePropertiesUtil.java ,I didn't find this PropertyUtil at first", "author": "zhangjun0x01", "createdAt": "2020-10-22T03:26:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODIyMTg1NQ=="}], "type": "inlineReview"}, {"oid": "d68fd2ee0a8fd2e5a7e2501fc99e79d77eb73e91", "url": "https://github.com/apache/iceberg/commit/d68fd2ee0a8fd2e5a7e2501fc99e79d77eb73e91", "message": "RewriteDataFilesAction for flink", "committedDate": "2020-10-22T03:20:20Z", "type": "forcePushed"}, {"oid": "4d0e1db3e2685e9c8f5cb9fb300f20a847df2955", "url": "https://github.com/apache/iceberg/commit/4d0e1db3e2685e9c8f5cb9fb300f20a847df2955", "message": "RewriteDataFilesAction for flink", "committedDate": "2020-10-22T03:57:13Z", "type": "forcePushed"}, {"oid": "f150a6c77b763e5fbfd51928c1dc679a948fc7c0", "url": "https://github.com/apache/iceberg/commit/f150a6c77b763e5fbfd51928c1dc679a948fc7c0", "message": "RewriteDataFilesAction for flink", "committedDate": "2020-10-26T10:51:38Z", "type": "forcePushed"}, {"oid": "f640e6e3646ed4062ee4457594511002b6d08969", "url": "https://github.com/apache/iceberg/commit/f640e6e3646ed4062ee4457594511002b6d08969", "message": "RewriteDataFilesAction for flink", "committedDate": "2020-10-28T06:08:26Z", "type": "forcePushed"}, {"oid": "cf65fed93538f5625e9c22b823c92b97b28b34a1", "url": "https://github.com/apache/iceberg/commit/cf65fed93538f5625e9c22b823c92b97b28b34a1", "message": "RewriteDataFilesAction for flink", "committedDate": "2020-10-28T06:20:33Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDc3NDY4Mg==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514774682", "bodyText": "why setFileIO ?  That's quite confusing,  it's more likely we are getting fileIO while we name it as setFileIO ?  I don't think we need to change this.", "author": "openinx", "createdAt": "2020-10-30T03:25:12Z", "path": "core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java", "diffHunk": "@@ -268,7 +272,7 @@ private void replaceDataFiles(Iterable<DataFile> deletedDataFiles, Iterable<Data\n     }\n   }\n \n-  protected abstract FileIO fileIO();\n+  protected abstract FileIO setFileIO();", "originalCommit": "cf65fed93538f5625e9c22b823c92b97b28b34a1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ1MjIyMg==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r515452222", "bodyText": "In the construction method of BaseRewriteDataFilesAction, we need to initialize fileIO and provide an abstract setter method. Other places, such as the rewriteDataForTasks method of spark and flink, need to get fileIO by call the getter method of fileIO, but there is no getter method currently. If we also call the fileIO() method to get fileIO, then the fileIO() method will be called multiple times, and SparkUtil.serializableFileIO() will be called multiple times.\nWe provide a setFileIO() method to ensure that the initialization of fileIO is only executed once. If we want to get the value of the fileIO field, call its getter method fileIO().", "author": "zhangjun0x01", "createdAt": "2020-10-31T04:02:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDc3NDY4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTcxODY0NQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r515718645", "bodyText": "I understand your points. I said it's confusing because its method name is setFileIO while would just return the FileIO without accepting any arguments,   it looks like a getFileIO.\nHow about loading the fileIO lazily as the previous abstraction PR comment ?\nprivate FileIO fileIO;\n\nprivate FileIO fileIO(){\n    if(fileIO==null){\n       this.fileIO = SparkUtil.serializableFileIO(table());\n    } \n    return this.fileIO;\n}", "author": "openinx", "createdAt": "2020-11-02T03:00:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDc3NDY4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDg5NjczMw==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514896733", "bodyText": "I'm thinking that we may need to move this class into org.apache.iceberg.flink.actions as we flink & spark both have the same class name Actions under the package org.apache.iceberg.actions,  then how to identify the class if someone want to execute the actions in the same project for both flink and spark ?", "author": "openinx", "createdAt": "2020-10-30T06:35:22Z", "path": "flink/src/main/java/org/apache/iceberg/actions/Actions.java", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;", "originalCommit": "cf65fed93538f5625e9c22b823c92b97b28b34a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDg5NzI3OA==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514897278", "bodyText": "nit: don't have to wrap the line here ..", "author": "openinx", "createdAt": "2020-10-30T06:37:19Z", "path": "flink/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.api.java.ExecutionEnvironment;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.source.RowDataRewriter;\n+import org.apache.iceberg.io.FileIO;\n+\n+public class RewriteDataFilesAction extends", "originalCommit": "cf65fed93538f5625e9c22b823c92b97b28b34a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDg5ODgyMw==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514898823", "bodyText": "nit: could we keep the same order between the constructor's argument assignment and fields definition ?  for example:\nprivate final int a;\nprivate final int b;\n\npublic Construct(int a, int b){\n    this.a = a;\n    this.b = b;\n}", "author": "openinx", "createdAt": "2020-10-30T06:42:53Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final FileFormat format;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, boolean caseSensitive, FileIO io, EncryptionManager encryptionManager) {\n+    this.schema = table.schema();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = PropertyUtil.propertyAsString(table.properties(), DEFAULT_NAME_MAPPING, null);\n+    String formatString = PropertyUtil.propertyAsString(table.properties(), TableProperties.DEFAULT_FILE_FORMAT,\n+        TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));", "originalCommit": "cf65fed93538f5625e9c22b823c92b97b28b34a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDg5OTkwNw==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514899907", "bodyText": "nit:  code format ?", "author": "openinx", "createdAt": "2020-10-30T06:46:37Z", "path": "core/src/main/java/org/apache/iceberg/util/PropertyUtil.java", "diffHunk": "@@ -52,4 +52,13 @@ public static long propertyAsLong(Map<String, String> properties,\n     }\n     return defaultValue;\n   }\n+\n+  public static String propertyAsString(Map<String, String> properties,\n+                                    String property, String defaultValue) {", "originalCommit": "cf65fed93538f5625e9c22b823c92b97b28b34a1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ4MzU1NA==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r515483554", "bodyText": "I may not understand what you mean too much. This code draws on other methods in this class, I think there is no checkstyle exception or other exceptions", "author": "zhangjun0x01", "createdAt": "2020-10-31T10:43:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDg5OTkwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTcxOTAzNw==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r515719037", "bodyText": "I mean the String property should be aligned with the Map<String, String> properties.\n  public static String propertyAsString(Map<String, String> properties,\n                                        String property, String defaultValue) {\n    String value = properties.get(property);\n    if (value != null) {\n      return properties.get(property);\n    }\n    return defaultValue;\n  }", "author": "openinx", "createdAt": "2020-11-02T03:03:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDg5OTkwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkwMDQ1Ng==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514900456", "bodyText": "nit: could we separate this format parser into a separate code block by left a new empty line ? That makes the code more clear.", "author": "openinx", "createdAt": "2020-10-30T06:48:28Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final FileFormat format;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, boolean caseSensitive, FileIO io, EncryptionManager encryptionManager) {\n+    this.schema = table.schema();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = PropertyUtil.propertyAsString(table.properties(), DEFAULT_NAME_MAPPING, null);\n+    String formatString = PropertyUtil.propertyAsString(table.properties(), TableProperties.DEFAULT_FILE_FORMAT,", "originalCommit": "cf65fed93538f5625e9c22b823c92b97b28b34a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkwMTY5OQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514901699", "bodyText": "we don't need to expose this class to public ? As the apache iceberg is a lib,  so we're tried to avoid to expose unnecessary internal classes or interfaces to public, so that users won't abuse them.", "author": "openinx", "createdAt": "2020-10-30T06:52:39Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter {", "originalCommit": "cf65fed93538f5625e9c22b823c92b97b28b34a1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ1MjYwOA==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r515452608", "bodyText": "RowDataRewriter and RewriteDataFilesAction are different packages. If RowDataRewriter is not public, then RewriteDataFilesAction will not be able to call RowDataRewriter", "author": "zhangjun0x01", "createdAt": "2020-10-31T04:07:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkwMTY5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTcxOTUxMA==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r515719510", "bodyText": "OK, we've put the RowDataRewriter in the org.apache.iceberg.flink.source package, while the RewriteDataFilesAction is in org.apache.iceberg.actions package.  So we need to make it public, sounds reasonable.", "author": "openinx", "createdAt": "2020-11-02T03:05:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkwMTY5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkwMjU4Mg==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514902582", "bodyText": "I think we've forgot to close the RowDataIterator if encountered any exception when iterate the records.  Pls handle this carefully because if close failed then we won't want to close it again in the catch block.", "author": "openinx", "createdAt": "2020-10-30T06:55:37Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final FileFormat format;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, boolean caseSensitive, FileIO io, EncryptionManager encryptionManager) {\n+    this.schema = table.schema();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = PropertyUtil.propertyAsString(table.properties(), DEFAULT_NAME_MAPPING, null);\n+    String formatString = PropertyUtil.propertyAsString(table.properties(), TableProperties.DEFAULT_FILE_FORMAT,\n+        TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    this.taskWriterFactory = new RowDataTaskWriterFactory(\n+        table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        io,\n+        encryptionManager,\n+        Long.MAX_VALUE,\n+        format,\n+        table.properties());\n+  }\n+\n+  public List<DataFile> rewriteDataForTasks(DataSet<CombinedScanTask> dataSet) throws Exception {\n+    RewriteMap map = new RewriteMap(schema, nameMapping, io, caseSensitive, encryptionManager, taskWriterFactory);\n+    DataSet<List<DataFile>> ds = dataSet.map(map);\n+    return ds.collect().stream().flatMap(Collection::stream).collect(Collectors.toList());\n+  }\n+\n+  public static class RewriteMap extends RichMapFunction<CombinedScanTask, List<DataFile>> {\n+\n+    private transient TaskWriter<RowData> writer;\n+    private transient int subTaskId;\n+    private transient int attemptId;\n+\n+    private final Schema schema;\n+    private final String nameMapping;\n+    private final FileIO io;\n+    private final boolean caseSensitive;\n+    private final EncryptionManager encryptionManager;\n+    private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+    public RewriteMap(Schema schema, String nameMapping, FileIO io, boolean caseSensitive,\n+                      EncryptionManager encryptionManager, TaskWriterFactory<RowData> taskWriterFactory) {\n+      this.schema = schema;\n+      this.nameMapping = nameMapping;\n+      this.io = io;\n+      this.caseSensitive = caseSensitive;\n+      this.encryptionManager = encryptionManager;\n+      this.taskWriterFactory = taskWriterFactory;\n+    }\n+\n+    @Override\n+    public void open(Configuration parameters) {\n+      this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+      this.attemptId = getRuntimeContext().getAttemptNumber();\n+      // Initialize the task writer factory.\n+      this.taskWriterFactory.initialize(subTaskId, attemptId);\n+    }\n+\n+    @Override\n+    public List<DataFile> map(CombinedScanTask task) throws Exception {\n+      // Initialize the task writer.\n+      this.writer = taskWriterFactory.create();\n+      RowDataIterator iterator =\n+          new RowDataIterator(task, io, encryptionManager, schema, schema, nameMapping, caseSensitive);\n+      try {\n+        while (iterator.hasNext()) {\n+          RowData rowData = iterator.next();\n+          writer.write(rowData);\n+        }\n+        iterator.close();\n+        return Lists.newArrayList(writer.complete());\n+      } catch (Throwable originalThrowable) {\n+        try {", "originalCommit": "cf65fed93538f5625e9c22b823c92b97b28b34a1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ4MjM5OQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r515482399", "bodyText": "I close the iterator in finally", "author": "zhangjun0x01", "createdAt": "2020-10-31T10:28:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkwMjU4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTcyMTUyMg==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r515721522", "bodyText": "nit: I think it's good to use the try-with-resource here as @simonsssu  suggested.\n      try (RowDataIterator iterator = new RowDataIterator(task, io, encryptionManager, schema, schema, nameMapping,\n          caseSensitive)) {\n        while (iterator.hasNext()) {\n          RowData rowData = iterator.next();\n          writer.write(rowData);\n        }\n        return Lists.newArrayList(writer.complete());\n      } catch (Throwable originalThrowable) {\n        try {\n          LOG.error(\"Aborting commit for  (subTaskId {}, attemptId {})\", subTaskId, attemptId);\n          writer.abort();\n          LOG.error(\"Aborted commit for  (subTaskId {}, attemptId {})\", subTaskId, attemptId);\n        } catch (Throwable inner) {\n          if (originalThrowable != inner) {\n            originalThrowable.addSuppressed(inner);\n            LOG.warn(\"Suppressing exception in catch: {}\", inner.getMessage(), inner);\n          }\n        }\n        if (originalThrowable instanceof Exception) {\n          throw originalThrowable;\n        } else {\n          throw new RuntimeException(originalThrowable);\n        }\n      }\n    }", "author": "openinx", "createdAt": "2020-11-02T03:16:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkwMjU4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkwMzA4OA==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514903088", "bodyText": "What's the reason that we need to supress the inner exception when it's different with the originalThrowable ?", "author": "openinx", "createdAt": "2020-10-30T06:57:25Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final FileFormat format;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, boolean caseSensitive, FileIO io, EncryptionManager encryptionManager) {\n+    this.schema = table.schema();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = PropertyUtil.propertyAsString(table.properties(), DEFAULT_NAME_MAPPING, null);\n+    String formatString = PropertyUtil.propertyAsString(table.properties(), TableProperties.DEFAULT_FILE_FORMAT,\n+        TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    this.taskWriterFactory = new RowDataTaskWriterFactory(\n+        table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        io,\n+        encryptionManager,\n+        Long.MAX_VALUE,\n+        format,\n+        table.properties());\n+  }\n+\n+  public List<DataFile> rewriteDataForTasks(DataSet<CombinedScanTask> dataSet) throws Exception {\n+    RewriteMap map = new RewriteMap(schema, nameMapping, io, caseSensitive, encryptionManager, taskWriterFactory);\n+    DataSet<List<DataFile>> ds = dataSet.map(map);\n+    return ds.collect().stream().flatMap(Collection::stream).collect(Collectors.toList());\n+  }\n+\n+  public static class RewriteMap extends RichMapFunction<CombinedScanTask, List<DataFile>> {\n+\n+    private transient TaskWriter<RowData> writer;\n+    private transient int subTaskId;\n+    private transient int attemptId;\n+\n+    private final Schema schema;\n+    private final String nameMapping;\n+    private final FileIO io;\n+    private final boolean caseSensitive;\n+    private final EncryptionManager encryptionManager;\n+    private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+    public RewriteMap(Schema schema, String nameMapping, FileIO io, boolean caseSensitive,\n+                      EncryptionManager encryptionManager, TaskWriterFactory<RowData> taskWriterFactory) {\n+      this.schema = schema;\n+      this.nameMapping = nameMapping;\n+      this.io = io;\n+      this.caseSensitive = caseSensitive;\n+      this.encryptionManager = encryptionManager;\n+      this.taskWriterFactory = taskWriterFactory;\n+    }\n+\n+    @Override\n+    public void open(Configuration parameters) {\n+      this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+      this.attemptId = getRuntimeContext().getAttemptNumber();\n+      // Initialize the task writer factory.\n+      this.taskWriterFactory.initialize(subTaskId, attemptId);\n+    }\n+\n+    @Override\n+    public List<DataFile> map(CombinedScanTask task) throws Exception {\n+      // Initialize the task writer.\n+      this.writer = taskWriterFactory.create();\n+      RowDataIterator iterator =\n+          new RowDataIterator(task, io, encryptionManager, schema, schema, nameMapping, caseSensitive);\n+      try {\n+        while (iterator.hasNext()) {\n+          RowData rowData = iterator.next();\n+          writer.write(rowData);\n+        }\n+        iterator.close();\n+        return Lists.newArrayList(writer.complete());\n+      } catch (Throwable originalThrowable) {\n+        try {\n+          LOG.error(\"Aborting commit for  (subTaskId {}, attemptId {})\", subTaskId, attemptId);\n+          writer.abort();\n+          LOG.error(\"Aborted commit for  (subTaskId {}, attemptId {})\", subTaskId, attemptId);\n+        } catch (Throwable inner) {\n+          if (originalThrowable != inner) {\n+            originalThrowable.addSuppressed(inner);", "originalCommit": "cf65fed93538f5625e9c22b823c92b97b28b34a1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ4MzEzMQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r515483131", "bodyText": "The code borrows spark, my understanding is that if an exception occurs during the writing, it enters the catch block, but writer.abort(); may also throw an exception, which will overwrite the original exception, so use originalThrowable.addSuppressed( inner);  record all exceptions to avoid losing the original exception", "author": "zhangjun0x01", "createdAt": "2020-10-31T10:37:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkwMzA4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkwNDA0OQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514904049", "bodyText": "nit:  could just use local variable ?  don't have to be a transient field.", "author": "openinx", "createdAt": "2020-10-30T07:00:49Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final FileFormat format;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, boolean caseSensitive, FileIO io, EncryptionManager encryptionManager) {\n+    this.schema = table.schema();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = PropertyUtil.propertyAsString(table.properties(), DEFAULT_NAME_MAPPING, null);\n+    String formatString = PropertyUtil.propertyAsString(table.properties(), TableProperties.DEFAULT_FILE_FORMAT,\n+        TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    this.taskWriterFactory = new RowDataTaskWriterFactory(\n+        table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        io,\n+        encryptionManager,\n+        Long.MAX_VALUE,\n+        format,\n+        table.properties());\n+  }\n+\n+  public List<DataFile> rewriteDataForTasks(DataSet<CombinedScanTask> dataSet) throws Exception {\n+    RewriteMap map = new RewriteMap(schema, nameMapping, io, caseSensitive, encryptionManager, taskWriterFactory);\n+    DataSet<List<DataFile>> ds = dataSet.map(map);\n+    return ds.collect().stream().flatMap(Collection::stream).collect(Collectors.toList());\n+  }\n+\n+  public static class RewriteMap extends RichMapFunction<CombinedScanTask, List<DataFile>> {\n+\n+    private transient TaskWriter<RowData> writer;", "originalCommit": "cf65fed93538f5625e9c22b823c92b97b28b34a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkwODU2Nw==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514908567", "bodyText": "I think we may need to refactor the SimpleDataUtil.assertTableRecords, because it will use Set to check the expected records, that means it will de-duplicate the same records automatically, and won't identify the case that have two <1, 'hello'> records.", "author": "openinx", "createdAt": "2020-10-30T07:15:22Z", "path": "flink/src/test/java/org/apache/iceberg/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.flink.FlinkCatalogTestBase;\n+import org.apache.iceberg.flink.SimpleDataUtil;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestRewriteDataFilesAction extends FlinkCatalogTestBase {\n+\n+  private static final String TABLE_NAME_UNPARTIITONED = \"test_table_unpartitioned\";\n+  private static final String TABLE_NAME_PARTIITONED = \"test_table_partitioned\";\n+  private final FileFormat format;\n+  private Table icebergTableUnPartitioned;\n+  private Table icebergTablePartitioned;\n+\n+  public TestRewriteDataFilesAction(String catalogName, String[] baseNamespace, FileFormat format) {\n+    super(catalogName, baseNamespace);\n+    this.format = format;\n+  }\n+\n+  @Parameterized.Parameters(name = \"catalogName={0}, baseNamespace={1}, format={2}\")\n+  public static Iterable<Object[]> parameters() {\n+    List<Object[]> parameters = Lists.newArrayList();\n+    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n+      for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n+        String catalogName = (String) catalogParams[0];\n+        String[] baseNamespace = (String[]) catalogParams[1];\n+        parameters.add(new Object[] {catalogName, baseNamespace, format});\n+      }\n+    }\n+    return parameters;\n+  }\n+\n+  @Before\n+  public void before() {\n+    super.before();\n+    sql(\"CREATE DATABASE %s\", flinkDatabase);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n+    sql(\"CREATE TABLE %s (id int, data varchar) with ('write.format.default'='%s')\", TABLE_NAME_UNPARTIITONED,\n+        format.name());\n+    icebergTableUnPartitioned = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace,\n+        TABLE_NAME_UNPARTIITONED));\n+\n+    sql(\"CREATE TABLE %s (id int, data varchar)  PARTITIONED BY (data) with ('write.format.default'='%s')\",\n+        TABLE_NAME_PARTIITONED, format.name());\n+    icebergTablePartitioned = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace,\n+        TABLE_NAME_PARTIITONED));\n+  }\n+\n+  @After\n+  public void clean() {\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, TABLE_NAME_UNPARTIITONED);\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, TABLE_NAME_PARTIITONED);\n+    sql(\"DROP DATABASE IF EXISTS %s\", flinkDatabase);\n+    super.clean();\n+  }\n+\n+  @Test\n+  public void testRewriteDataFilesEmptyTable() throws Exception {\n+    Assert.assertNull(\"Table must be empty\", icebergTableUnPartitioned.currentSnapshot());\n+    Actions.forTable(icebergTableUnPartitioned)\n+        .rewriteDataFiles()\n+        .execute();\n+    Assert.assertNull(\"Table must stay empty\", icebergTableUnPartitioned.currentSnapshot());\n+  }\n+\n+\n+  @Test\n+  public void testRewriteDataFilesUnpartitionedTable() throws Exception {\n+    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME_UNPARTIITONED);\n+    sql(\"INSERT INTO %s SELECT 2, 'world'\", TABLE_NAME_UNPARTIITONED);\n+\n+    icebergTableUnPartitioned.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = icebergTableUnPartitioned.newScan().planFiles();\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 2 data files before rewrite\", 2, dataFiles.size());\n+\n+    RewriteDataFilesActionResult result =\n+        Actions.forTable(icebergTableUnPartitioned)\n+            .rewriteDataFiles()\n+            .execute();\n+\n+    Assert.assertEquals(\"Action should rewrite 2 data files\", 2, result.deletedDataFiles().size());\n+    Assert.assertEquals(\"Action should add 1 data file\", 1, result.addedDataFiles().size());\n+\n+    icebergTableUnPartitioned.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks1 = icebergTableUnPartitioned.newScan().planFiles();\n+    List<DataFile> dataFiles1 = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 1 data files after rewrite\", 1, dataFiles1.size());\n+\n+    // Assert the table records as expected.\n+    SimpleDataUtil.assertTableRecords(icebergTableUnPartitioned, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"hello\"),\n+        SimpleDataUtil.createRecord(2, \"world\")\n+    ));\n+  }\n+\n+  @Test\n+  public void testRewriteDataFilesPartitionedTable() throws Exception {\n+    sql(\"INSERT INTO %s SELECT 1, 'hello' \", TABLE_NAME_PARTIITONED);\n+    sql(\"INSERT INTO %s SELECT 2, 'hello' \", TABLE_NAME_PARTIITONED);\n+    sql(\"INSERT INTO %s SELECT 3, 'world' \", TABLE_NAME_PARTIITONED);\n+    sql(\"INSERT INTO %s SELECT 4, 'world' \", TABLE_NAME_PARTIITONED);\n+\n+    icebergTablePartitioned.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = icebergTablePartitioned.newScan().planFiles();\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 4 data files before rewrite\", 4, dataFiles.size());\n+\n+    RewriteDataFilesActionResult result =\n+        Actions.forTable(icebergTablePartitioned)\n+            .rewriteDataFiles()\n+            .execute();\n+\n+    Assert.assertEquals(\"Action should rewrite 4 data files\", 4, result.deletedDataFiles().size());\n+    Assert.assertEquals(\"Action should add 2 data file\", 2, result.addedDataFiles().size());\n+\n+    icebergTablePartitioned.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks1 = icebergTablePartitioned.newScan().planFiles();\n+    List<DataFile> dataFiles1 = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 2 data files after rewrite\", 2, dataFiles1.size());\n+\n+    // Assert the table records as expected.\n+    SimpleDataUtil.assertTableRecords(icebergTablePartitioned, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"hello\"),\n+        SimpleDataUtil.createRecord(2, \"hello\"),\n+        SimpleDataUtil.createRecord(3, \"world\"),\n+        SimpleDataUtil.createRecord(4, \"world\")\n+    ));\n+  }\n+\n+\n+  @Test\n+  public void testRewriteDataFilesWithFilter() throws Exception {\n+    sql(\"INSERT INTO %s SELECT 1, 'hello' \", TABLE_NAME_PARTIITONED);\n+    sql(\"INSERT INTO %s SELECT 1, 'hello' \", TABLE_NAME_PARTIITONED);\n+    sql(\"INSERT INTO %s SELECT 1, 'world' \", TABLE_NAME_PARTIITONED);\n+    sql(\"INSERT INTO %s SELECT 2, 'world' \", TABLE_NAME_PARTIITONED);\n+    sql(\"INSERT INTO %s SELECT 3, 'world' \", TABLE_NAME_PARTIITONED);\n+\n+    icebergTablePartitioned.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = icebergTablePartitioned.newScan().planFiles();\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 5 data files before rewrite\", 5, dataFiles.size());\n+\n+    RewriteDataFilesActionResult result =\n+        Actions.forTable(icebergTablePartitioned)\n+            .rewriteDataFiles()\n+            .filter(Expressions.equal(\"id\", 1))\n+            .filter(Expressions.startsWith(\"data\", \"he\"))\n+            .execute();\n+\n+    Assert.assertEquals(\"Action should rewrite 2 data files\", 2, result.deletedDataFiles().size());\n+    Assert.assertEquals(\"Action should add 1 data file\", 1, result.addedDataFiles().size());\n+\n+    icebergTablePartitioned.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks1 = icebergTablePartitioned.newScan().planFiles();\n+    List<DataFile> dataFiles1 = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 4 data files after rewrite\", 4, dataFiles1.size());\n+\n+    // Assert the table records as expected.\n+    SimpleDataUtil.assertTableRecords(icebergTablePartitioned, Lists.newArrayList(", "originalCommit": "cf65fed93538f5625e9c22b823c92b97b28b34a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkwODk1Nw==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514908957", "bodyText": "How about use protected ?", "author": "openinx", "createdAt": "2020-10-30T07:16:39Z", "path": "flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java", "diffHunk": "@@ -72,7 +72,7 @@ protected TableEnvironment getTableEnv() {\n     return tEnv;\n   }\n \n-  List<Object[]> sql(String query, Object... args) {\n+  public List<Object[]> sql(String query, Object... args) {", "originalCommit": "cf65fed93538f5625e9c22b823c92b97b28b34a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkxMTkyOQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514911929", "bodyText": "I think we should also provide an unit test similar to the spark testRewriteLargeTableHasResiduals, that test the option ignoreResiduals.", "author": "openinx", "createdAt": "2020-10-30T07:25:10Z", "path": "flink/src/test/java/org/apache/iceberg/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.flink.FlinkCatalogTestBase;\n+import org.apache.iceberg.flink.SimpleDataUtil;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestRewriteDataFilesAction extends FlinkCatalogTestBase {", "originalCommit": "cf65fed93538f5625e9c22b823c92b97b28b34a1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ4ODA4NA==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r515488084", "bodyText": "done", "author": "zhangjun0x01", "createdAt": "2020-10-31T11:37:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkxMTkyOQ=="}], "type": "inlineReview"}, {"oid": "7548af2e07bd829f92d45645e073d5c06a1dfc88", "url": "https://github.com/apache/iceberg/commit/7548af2e07bd829f92d45645e073d5c06a1dfc88", "message": "RewriteDataFilesAction for flink", "committedDate": "2020-10-31T03:23:25Z", "type": "forcePushed"}, {"oid": "1ac5493b1017473a78831567c7e299c3644b416b", "url": "https://github.com/apache/iceberg/commit/1ac5493b1017473a78831567c7e299c3644b416b", "message": "RewriteDataFilesAction for flink", "committedDate": "2020-10-31T03:23:25Z", "type": "forcePushed"}, {"oid": "75143b1a306e0ec5ffd07c958f0bf5d01ec70dfd", "url": "https://github.com/apache/iceberg/commit/75143b1a306e0ec5ffd07c958f0bf5d01ec70dfd", "message": " RewriteDataFilesAction for flink", "committedDate": "2020-11-02T05:32:46Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkxNjA2OA==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r515916068", "bodyText": "nit: TABLE_NAME_UNPARTIITONED -> TABLE_NAME_UNPARTITIONED.\nTABLE_NAME_PARTIITONED has the similar issue.", "author": "openinx", "createdAt": "2020-11-02T11:42:22Z", "path": "flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+\n+package org.apache.iceberg.flink.actions;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.actions.RewriteDataFilesActionResult;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.flink.FlinkCatalogTestBase;\n+import org.apache.iceberg.flink.SimpleDataUtil;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.iceberg.flink.SimpleDataUtil.RECORD;\n+\n+@RunWith(Parameterized.class)\n+public class TestRewriteDataFilesAction extends FlinkCatalogTestBase {\n+\n+  private static final String TABLE_NAME_UNPARTIITONED = \"test_table_unpartitioned\";", "originalCommit": "75143b1a306e0ec5ffd07c958f0bf5d01ec70dfd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjM4MTA4Ng==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r516381086", "bodyText": "I updated it", "author": "zhangjun0x01", "createdAt": "2020-11-03T01:19:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkxNjA2OA=="}], "type": "inlineReview"}, {"oid": "e4c28953643c0b7f5f7f0a7336d0a7315bebeb12", "url": "https://github.com/apache/iceberg/commit/e4c28953643c0b7f5f7f0a7336d0a7315bebeb12", "message": "fix Typo", "committedDate": "2020-11-03T01:10:31Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQzOTExNA==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r516439114", "bodyText": "I think setParallelism may not works.\nWhat we want is a SparkContext.parallelize, but this setParallelism just set parallelism, records still in a single node, downstream operators only have computation on one node.\nCan you check this?", "author": "JingsongLi", "createdAt": "2020-11-03T05:50:07Z", "path": "flink/src/main/java/org/apache/iceberg/flink/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.actions;\n+\n+import java.util.List;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.api.java.ExecutionEnvironment;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.actions.BaseRewriteDataFilesAction;\n+import org.apache.iceberg.flink.source.RowDataRewriter;\n+import org.apache.iceberg.io.FileIO;\n+\n+public class RewriteDataFilesAction extends BaseRewriteDataFilesAction<RewriteDataFilesAction> {\n+\n+  private ExecutionEnvironment env;\n+\n+  public RewriteDataFilesAction(ExecutionEnvironment env, Table table) {\n+    super(table);\n+    this.env = env;\n+  }\n+\n+  @Override\n+  protected FileIO fileIO() {\n+    return table().io();\n+  }\n+\n+  @Override\n+  protected List<DataFile> rewriteDataForTasks(List<CombinedScanTask> combinedScanTasks) {\n+    DataSet<CombinedScanTask> dataSet = env.fromCollection(combinedScanTasks).setParallelism(combinedScanTasks.size());", "originalCommit": "e4c28953643c0b7f5f7f0a7336d0a7315bebeb12", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQ1NzY4MQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r516457681", "bodyText": "Thanks @zhangjun0x01 for your contribution, why do you want to use DataSet API? DataSet API is being deprecated.\nLooks like just need a parallelize and map and collect. I think DataStream(We can call it BoundedStream) can finish this.\nWhat do you think?\n\nHi,@JingsongLi\uff1a\nI look up the api of the DataStream, it seems  no collect method", "author": "zhangjun0x01", "createdAt": "2020-11-03T07:02:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQzOTExNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQ1ODQxOQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r516458419", "bodyText": "You can use DataStreamUtils.collect", "author": "JingsongLi", "createdAt": "2020-11-03T07:05:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQzOTExNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQ5Mzg2MA==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r516493860", "bodyText": "I updated the pr, use DataStream instead of DataSet , and set the parallelism to StreamExecutionEnvironment.\nbut setParallelism for  env.fromCollection(combinedScanTasks)  does not seem to have an impact on downstream operator,this is my test job graph for use dataset.   https://blog.csdn.net/zhangjun5965/article/details/109469798", "author": "zhangjun0x01", "createdAt": "2020-11-03T08:31:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQzOTExNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjUwMjg3MA==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r516502870", "bodyText": "Thanks for the update and check, I think you are right, the graph is good.", "author": "JingsongLi", "createdAt": "2020-11-03T08:48:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQzOTExNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjUwMzk2Ng==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r516503966", "bodyText": "I think about parallelism again.\nCan we not set parallelism? Flink is a push based model, which is not suitable for taking on too much parallelism.\nSo can we just let it as default parallelims?", "author": "JingsongLi", "createdAt": "2020-11-03T08:50:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQzOTExNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjUzNDAwOQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r516534009", "bodyText": "If we do not set the parallelism, we can use the default parallelism of flink. If the user wants to modify the parallelism to improve the execution speed, it can be set by the -p parameter. However, the user may not know how much parallelism to set\uff0cbecause users may not know how many data files to Rewrite each time. If the setting is too large, it may cause a waste of resources.\nSo I think we can  set a max parallelism. If the size of CombinedScanTask list  is less than max parallelism, we use the CombinedScanTask list size as the job parallelism, if the size  is greater than max parallelism\uff0cwe use the max parallelism which we set.\nWhat do you think?", "author": "zhangjun0x01", "createdAt": "2020-11-03T09:41:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQzOTExNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjUzODAxOQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r516538019", "bodyText": "Let me explain why we can't set parallelism directly:\nFlink is in pipeline mode by default, which means that all tasks must be started.\nIf we want to compact a large table, it may require thousands of parallelism, which leads to the need for a very large cluster.\nThis is similar to the Hive source parallelism inference: https://ci.apache.org/projects/flink/flink-docs-master/dev/table/hive/hive_read_write.html#source-parallelism-inference\nThe value should be used to infer parallelism, but we need a max parallelism.\nSo I agree with you very much. Let's add a max parallelism.", "author": "JingsongLi", "createdAt": "2020-11-03T09:47:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQzOTExNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjU0MTM2OA==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r516541368", "bodyText": "BTW, you should set parallelism for map operation instead of modifying env.", "author": "JingsongLi", "createdAt": "2020-11-03T09:52:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQzOTExNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjU3MzU3Mg==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r516573572", "bodyText": "I updated the pr, although we set the parallelism for StreamExecutionEnvironment, I found that the DataStreamUtils.collect operator only uses 1 parallelism, so set parallelism to map operator or StreamExecutionEnvironment seems to have the same effect, but set parallelism to map is better to understand .\nI submit an issue (#1660) to  set max parallelism a few days ago, I will open a new pr  for #1660 when this PR is merged", "author": "zhangjun0x01", "createdAt": "2020-11-03T10:44:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQzOTExNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjU5OTQxNw==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r516599417", "bodyText": "although we set the parallelism for StreamExecutionEnvironment, I found that the DataStreamUtils.collect operator only uses 1 parallelism, so set parallelism to map operator or StreamExecutionEnvironment seems to have the same effect, but set parallelism to map is better to understand.\n\nIt is better not to bring extra impact to env, because this env may be reused by users.\n\nI submit an issue (#1660) to set max parallelism a few days ago, I will open a new pr for #1660 when this PR is merged\n\nCan you modify in this PR? Or just not set parallelism in this PR?\nI think it is not good to bring user cluster risk to master code.", "author": "JingsongLi", "createdAt": "2020-11-03T11:30:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQzOTExNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA3NTY5Mg==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r517075692", "bodyText": "I  update the code , add the max parallelism in this PR.", "author": "zhangjun0x01", "createdAt": "2020-11-04T03:17:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQzOTExNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjYwMjAwNA==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r516602004", "bodyText": "If change to use datastream to submit this job,  then the argument dataSet should be renamed to dataStream ?", "author": "openinx", "createdAt": "2020-11-03T11:35:23Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final FileFormat format;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, boolean caseSensitive, FileIO io, EncryptionManager encryptionManager) {\n+    this.schema = table.schema();\n+    this.caseSensitive = caseSensitive;\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+    this.nameMapping = PropertyUtil.propertyAsString(table.properties(), DEFAULT_NAME_MAPPING, null);\n+\n+    String formatString = PropertyUtil.propertyAsString(table.properties(), TableProperties.DEFAULT_FILE_FORMAT,\n+        TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    this.taskWriterFactory = new RowDataTaskWriterFactory(\n+        table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        io,\n+        encryptionManager,\n+        Long.MAX_VALUE,\n+        format,\n+        table.properties());\n+  }\n+\n+  public List<DataFile> rewriteDataForTasks(DataStream<CombinedScanTask> dataSet, int parallelism) {", "originalCommit": "a408953c4a4d9a5e2ecca16aebf02d3c51fe2631", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA3NTczNg==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r517075736", "bodyText": "I updated it.", "author": "zhangjun0x01", "createdAt": "2020-11-04T03:17:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjYwMjAwNA=="}], "type": "inlineReview"}, {"oid": "5a0dcdf7001807ccd83be3d4caa49f8b5c077a64", "url": "https://github.com/apache/iceberg/commit/5a0dcdf7001807ccd83be3d4caa49f8b5c077a64", "message": "add max Parallelism for RewriteDataFilesAction", "committedDate": "2020-11-04T03:12:30Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE3ODQxMg==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r517178412", "bodyText": "Do we need to pass the whole List<CombinedScanTask> here ?  I think we only need the numOfScanTasks ?", "author": "openinx", "createdAt": "2020-11-04T08:43:53Z", "path": "core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java", "diffHunk": "@@ -268,6 +268,23 @@ private void replaceDataFiles(Iterable<DataFile> deletedDataFiles, Iterable<Data\n     }\n   }\n \n+  protected int getParallelism(int parallelism, List<CombinedScanTask> combinedScanTasks) {", "originalCommit": "5a0dcdf7001807ccd83be3d4caa49f8b5c077a64", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE4MDA4NQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r517180085", "bodyText": "btw,  it seems to be a method which was designed for flink, Better to move it to flink implementation class ?", "author": "openinx", "createdAt": "2020-11-04T08:46:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE3ODQxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE4MzY4NQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r517183685", "bodyText": "+1 I think we can move options to Flink too.", "author": "JingsongLi", "createdAt": "2020-11-04T08:52:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE3ODQxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzIxMDQ4MQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r517210481", "bodyText": "should we set the max parallelism for spark  RewriteDataFilesAction? I put the method to BaseRewriteDataFilesAction, because both flink and spark use this method", "author": "zhangjun0x01", "createdAt": "2020-11-04T09:36:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE3ODQxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzIxNTEwMQ==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r517215101", "bodyText": "Don't need, Spark can handle large parallelism in default...", "author": "JingsongLi", "createdAt": "2020-11-04T09:43:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE3ODQxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzI1MDM2Nw==", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r517250367", "bodyText": "I moved set max parallelism to flink RewriteDataFilesAction", "author": "zhangjun0x01", "createdAt": "2020-11-04T10:39:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE3ODQxMg=="}], "type": "inlineReview"}, {"oid": "d02e7420e06bd706b085c0d3266ce6327e35de05", "url": "https://github.com/apache/iceberg/commit/d02e7420e06bd706b085c0d3266ce6327e35de05", "message": "RewriteDataFilesAction for flink", "committedDate": "2020-11-04T10:31:09Z", "type": "commit"}, {"oid": "0ffd2609ff1f69236f8f197e69b0973d294623f1", "url": "https://github.com/apache/iceberg/commit/0ffd2609ff1f69236f8f197e69b0973d294623f1", "message": "RewriteDataFilesAction for flink", "committedDate": "2020-11-04T10:31:09Z", "type": "commit"}, {"oid": "b87a3eb76dce132c6ff3842090155133a78c7797", "url": "https://github.com/apache/iceberg/commit/b87a3eb76dce132c6ff3842090155133a78c7797", "message": "fix unit test", "committedDate": "2020-11-04T10:31:09Z", "type": "commit"}, {"oid": "1d19c039b6fd3b1462c8372912f9f552c79dc32d", "url": "https://github.com/apache/iceberg/commit/1d19c039b6fd3b1462c8372912f9f552c79dc32d", "message": "fix checkstyle", "committedDate": "2020-11-04T10:31:09Z", "type": "commit"}, {"oid": "607885e05518803d0c12d54d411423b9eceb270c", "url": "https://github.com/apache/iceberg/commit/607885e05518803d0c12d54d411423b9eceb270c", "message": " RewriteDataFilesAction for flink", "committedDate": "2020-11-04T10:31:09Z", "type": "commit"}, {"oid": "634e9c4612093a63facb3831d333b43789fcdd92", "url": "https://github.com/apache/iceberg/commit/634e9c4612093a63facb3831d333b43789fcdd92", "message": "fix Typo", "committedDate": "2020-11-04T10:31:09Z", "type": "commit"}, {"oid": "6b011aa525ec3c2509ab0ced2848f9c69fc810fd", "url": "https://github.com/apache/iceberg/commit/6b011aa525ec3c2509ab0ced2848f9c69fc810fd", "message": "use DataStream instead of dataset", "committedDate": "2020-11-04T10:31:09Z", "type": "commit"}, {"oid": "72c5a98524e84478cc0abc01773ff20aa572ab61", "url": "https://github.com/apache/iceberg/commit/72c5a98524e84478cc0abc01773ff20aa572ab61", "message": "set parallelism for map", "committedDate": "2020-11-04T10:31:09Z", "type": "commit"}, {"oid": "aa78d406bb49ba638e94132db2e9de109dbb1596", "url": "https://github.com/apache/iceberg/commit/aa78d406bb49ba638e94132db2e9de109dbb1596", "message": "add max Parallelism for RewriteDataFilesAction", "committedDate": "2020-11-04T10:31:09Z", "type": "commit"}, {"oid": "054ca38f36a9f85ff6cfce9f21c27ca8757a3593", "url": "https://github.com/apache/iceberg/commit/054ca38f36a9f85ff6cfce9f21c27ca8757a3593", "message": "move maxParallelism to flink RewriteDataFilesAction", "committedDate": "2020-11-04T10:31:09Z", "type": "commit"}, {"oid": "054ca38f36a9f85ff6cfce9f21c27ca8757a3593", "url": "https://github.com/apache/iceberg/commit/054ca38f36a9f85ff6cfce9f21c27ca8757a3593", "message": "move maxParallelism to flink RewriteDataFilesAction", "committedDate": "2020-11-04T10:31:09Z", "type": "forcePushed"}]}