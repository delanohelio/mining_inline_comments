{"pr_number": 1185, "pr_title": "Flink: Add the iceberg files committer to collect data files and commit to iceberg table.", "pr_createdAt": "2020-07-09T09:09:59Z", "pr_url": "https://github.com/apache/iceberg/pull/1185", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM2MjAwMQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r460362001", "bodyText": "Setting the uid to be a randomly generated number is not a good practice. Operator uid's should be stable, such that between intentional redeployments or simply on restarts due to lost task managers, Flink will know which operators each state belongs to.\nIf the uid is changed between restarts, state could be lost.\nYou can find more information about the use of operator uid's with flink's savepoints (intentional restarts) and why stateful operators should be assigned a stable uid here: https://ci.apache.org/projects/flink/flink-docs-stable/ops/upgrading.html#matching-operator-state", "author": "kbendick", "createdAt": "2020-07-25T04:14:39Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergDataStream.java", "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.UUID;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+\n+public class IcebergDataStream {\n+\n+  private static final String ICEBERG_STREAM_WRITER = \"Iceberg-Stream-Writer\";\n+  private static final String ICEBERG_FILES_COMMITTER = \"Iceberg-Files-Committer\";\n+\n+  private final DataStream<Row> dataStream;\n+  private final String path;\n+  private final Configuration conf;\n+  private final TableSchema tableSchema;\n+  private final Integer parallelism;\n+\n+  private IcebergDataStream(DataStream<Row> dataStream, String path,\n+                            Configuration conf, TableSchema tableSchema,\n+                            Integer parallelism) {\n+    this.dataStream = dataStream;\n+    this.path = path;\n+    this.conf = conf;\n+    this.tableSchema = tableSchema;\n+    this.parallelism = parallelism;\n+  }\n+\n+  public static Builder buildFor(DataStream<Row> dataStream) {\n+    return new Builder().dataStream(dataStream);\n+  }\n+\n+  public static class Builder {\n+    private DataStream<Row> dataStream;\n+    private String path;\n+    private Configuration conf;\n+    private TableSchema tableSchema;\n+    private Integer parallelism;\n+\n+    private Builder dataStream(DataStream<Row> newDataStream) {\n+      this.dataStream = newDataStream;\n+      return this;\n+    }\n+\n+    public Builder path(String newPath) {\n+      this.path = newPath;\n+      return this;\n+    }\n+\n+    public Builder config(Configuration newConfig) {\n+      this.conf = newConfig;\n+      return this;\n+    }\n+\n+    public Builder tableSchema(TableSchema newTableSchema) {\n+      this.tableSchema = newTableSchema;\n+      return this;\n+    }\n+\n+    public Builder parallelism(int newParallelism) {\n+      this.parallelism = newParallelism;\n+      return this;\n+    }\n+\n+    public IcebergDataStream build() {\n+      return new IcebergDataStream(dataStream, path, conf, tableSchema, parallelism);\n+    }\n+  }\n+\n+  public void append() {\n+    IcebergStreamWriter streamWriter = IcebergStreamWriter.createStreamWriter(path, tableSchema, conf);\n+    IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(path, conf);\n+\n+    SingleOutputStreamOperator<DataFile> operator = dataStream\n+        .transform(ICEBERG_STREAM_WRITER, DataFileTypeInfo.TYPE_INFO, streamWriter)\n+        .uid(UUID.randomUUID().toString());\n+\n+    if (parallelism != null && parallelism > 0) {\n+      operator.setParallelism(parallelism);\n+    }\n+\n+    operator.addSink(filesCommitter)\n+        .name(ICEBERG_FILES_COMMITTER)\n+        .uid(UUID.randomUUID().toString())", "originalCommit": "b5b518b4ddcb36d9219c7e53cf40704f3a2e0e4e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM2MjQ2NA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r460362464", "bodyText": "Most of the time, I simply assign the name as the uid. In Scala, I use an implicit class to assign name and uid at the same time with the same value. This assumes that the name of an operator is unique (which might not always be true, and is something that should be considered as a library author), but so far I've not run into issues with the handful of jobs that I oversee.", "author": "kbendick", "createdAt": "2020-07-25T04:18:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM2MjAwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzMDMxMA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r464830310", "bodyText": "pos[i] It seemed the fields of flink ddl must be the same sequence with the fields of iceberg schema . what if flink ddl misses some fields? I think partition keys' indices will be different from in iceberg schema", "author": "YesOrNo828", "createdAt": "2020-08-04T06:36:21Z", "path": "flink/src/main/java/org/apache/iceberg/flink/PartitionKey.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.flink.types.Row;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.transforms.Transform;\n+import org.apache.iceberg.types.Types;\n+\n+public class PartitionKey implements StructLike {\n+\n+  private final Object[] partitionTuple;\n+\n+  private PartitionKey(Object[] partitionTuple) {\n+    this.partitionTuple = partitionTuple;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (!(o instanceof PartitionKey)) {\n+      return false;\n+    }\n+\n+    PartitionKey that = (PartitionKey) o;\n+    return Arrays.equals(partitionTuple, that.partitionTuple);\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    return Arrays.hashCode(partitionTuple);\n+  }\n+\n+  @Override\n+  public int size() {\n+    return partitionTuple.length;\n+  }\n+\n+  @Override\n+  public <T> T get(int pos, Class<T> javaClass) {\n+    return javaClass.cast(partitionTuple[pos]);\n+  }\n+\n+  public Object[] getPartitionTuple() {\n+    return partitionTuple;\n+  }\n+\n+  @Override\n+  public <T> void set(int pos, T value) {\n+    partitionTuple[pos] = value;\n+  }\n+\n+  private static Map<Integer, Integer> buildFieldId2PosMap(Schema schema) {\n+    Map<Integer, Integer> fieldId2Position = Maps.newHashMap();\n+    List<Types.NestedField> nestedFields = schema.asStruct().fields();\n+    for (int i = 0; i < nestedFields.size(); i++) {\n+      fieldId2Position.put(nestedFields.get(i).fieldId(), i);\n+    }\n+    return fieldId2Position;\n+  }\n+\n+  public static Builder builder(PartitionSpec spec) {\n+    return new Builder(spec);\n+  }\n+\n+  public static class Builder {\n+    private final int size;\n+\n+    private final int[] pos;\n+    private final Transform[] transforms;\n+\n+    private Builder(PartitionSpec spec) {\n+      List<PartitionField> fields = spec.fields();\n+      this.size = fields.size();\n+      this.pos = new int[size];\n+      this.transforms = new Transform[size];\n+\n+      Map<Integer, Integer> fieldId2Pos = buildFieldId2PosMap(spec.schema());\n+\n+      for (int i = 0; i < size; i += 1) {\n+        PartitionField field = fields.get(i);\n+        Integer position = fieldId2Pos.get(field.sourceId());\n+        Preconditions.checkArgument(position != null,\n+            \"Field source id from PartitionSpec MUST exist in the original schema\");\n+        this.pos[i] = position;\n+        this.transforms[i] = field.transform();\n+      }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public PartitionKey build(Row row) {\n+      Object[] partitionTuple = new Object[size];\n+      for (int i = 0; i < partitionTuple.length; i += 1) {\n+        partitionTuple[i] = transforms[i].apply(row.getField(pos[i]));", "originalCommit": "b5b518b4ddcb36d9219c7e53cf40704f3a2e0e4e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzg0MDA0Mw==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r467840043", "bodyText": "I think this method parseMaxCommittedCheckpointId should be invoked when this job is stored.", "author": "YesOrNo828", "createdAt": "2020-08-10T11:25:31Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import org.apache.commons.compress.utils.Lists;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer;\n+import org.apache.flink.runtime.state.CheckpointListener;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends RichSinkFunction<DataFile> implements\n+    CheckpointListener, CheckpointedFunction {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed.checkpoint.id\";\n+\n+  private final String path;\n+  private final SerializableConfiguration conf;\n+\n+  private transient long maxCommittedCheckpointId;\n+  private transient NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint;\n+  private transient List<DataFile> dataFilesOfCurrentCheckpoint;\n+  private transient Table table;\n+\n+  // State for all checkpoints;\n+  private static final ListStateDescriptor<byte[]> STATE_DESCRIPTOR =\n+      new ListStateDescriptor<>(\"checkpoints-state\", BytePrimitiveArraySerializer.INSTANCE);\n+  private transient ListState<byte[]> checkpointsState;\n+\n+  IcebergFilesCommitter(String path, Configuration newConf) {\n+    this.path = path;\n+    this.conf = new SerializableConfiguration(newConf);\n+  }\n+\n+  @Override\n+  public void initializeState(FunctionInitializationContext context) throws Exception {\n+    table = TableUtil.findTable(path, conf.get());\n+    maxCommittedCheckpointId = parseMaxCommittedCheckpointId(table.currentSnapshot());", "originalCommit": "b5b518b4ddcb36d9219c7e53cf40704f3a2e0e4e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDM5OTk4Mw==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r470399983", "bodyText": "You are right.  here we could only initialize the max-checkpoint-id to -1,  and could not read the checkpoint id from iceberg table if not in a restore job.   Because we may have a table with 3 checkpoints and its max-committed-checkpoint id is 3,  then we stop the current flink job and start another flink job to continue to write this table,  its checkpoint id will start from 1, if we read max-commited-checkpoint id here then we will miss the first three checkpoint's data files in current flink job.", "author": "openinx", "createdAt": "2020-08-14T04:12:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzg0MDA0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDUyODEyMw==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r470528123", "bodyText": "Added a unit test to address this thing. https://github.com/apache/iceberg/pull/1185/files#diff-67c60f8d1a96e4583f5b53248df15bedR257", "author": "openinx", "createdAt": "2020-08-14T09:54:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzg0MDA0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjgxNjY5Nw==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r472816697", "bodyText": "Do you have any idea about if the snapshots were removed by another process, and this process commit a new snapshot to iceberg table, the flink committer will not get the correct maxCommittedCheckpointId .", "author": "YesOrNo828", "createdAt": "2020-08-19T07:43:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzg0MDA0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjg5NTI4MQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r472895281", "bodyText": "That means someone have expired the latest committed snapshot for a running flink job, usually we flink checkpoint interval will be several minutes, while the snapshot expire interval will be several days or weeks.  It's unlikely that we will expire the latest committed snapshot unless someone set the unreasonable intervals.\nOn the other hand,  even if someone removed the latest committed snapshot,  the flink job will write the data to iceberg table correctly unless we restore the flink job once snapshot was removed.", "author": "openinx", "createdAt": "2020-08-19T09:32:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzg0MDA0Mw=="}], "type": "inlineReview"}, {"oid": "65d7347af2d1cdba405a2b81d204f0148be75141", "url": "https://github.com/apache/iceberg/commit/65d7347af2d1cdba405a2b81d204f0148be75141", "message": "Flink: Add the iceberg files committer to collect data files and commit to iceberg table.", "committedDate": "2020-08-13T08:53:33Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTgwOTEwNQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r469809105", "bodyText": "Can we use Flink MapTypeInfo, and addAll to sorted map?", "author": "JingsongLi", "createdAt": "2020-08-13T09:08:58Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer;\n+import org.apache.flink.runtime.state.CheckpointListener;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends RichSinkFunction<DataFile> implements\n+    CheckpointListener, CheckpointedFunction {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed.checkpoint.id\";\n+\n+  private static final FlinkCatalogFactory CATALOG_FACTORY = new FlinkCatalogFactory();\n+\n+  private final String path;\n+  private final SerializableConfiguration conf;\n+  private final ImmutableMap<String, String> options;\n+\n+  private transient long maxCommittedCheckpointId;\n+  private transient NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint;\n+  private transient List<DataFile> dataFilesOfCurrentCheckpoint;\n+  private transient Table table;\n+\n+  // State for all checkpoints;\n+  private static final ListStateDescriptor<byte[]> STATE_DESCRIPTOR =\n+      new ListStateDescriptor<>(\"checkpoints-state\", BytePrimitiveArraySerializer.INSTANCE);", "originalCommit": "65d7347af2d1cdba405a2b81d204f0148be75141", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDUzMDU3Nw==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r470530577", "bodyText": "That sounds good, I've implemented it in this patch:  15dd8e0", "author": "openinx", "createdAt": "2020-08-14T09:59:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTgwOTEwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTgxMjc0MQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r469812741", "bodyText": "Why exclude maxCommittedCheckpointId and checkpointId? Just dataFilesPerCheckpoint.tailMap(checkpointId, true)? I don't understand why we need store previous checkpoint id.", "author": "JingsongLi", "createdAt": "2020-08-13T09:14:44Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer;\n+import org.apache.flink.runtime.state.CheckpointListener;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends RichSinkFunction<DataFile> implements\n+    CheckpointListener, CheckpointedFunction {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed.checkpoint.id\";\n+\n+  private static final FlinkCatalogFactory CATALOG_FACTORY = new FlinkCatalogFactory();\n+\n+  private final String path;\n+  private final SerializableConfiguration conf;\n+  private final ImmutableMap<String, String> options;\n+\n+  private transient long maxCommittedCheckpointId;\n+  private transient NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint;\n+  private transient List<DataFile> dataFilesOfCurrentCheckpoint;\n+  private transient Table table;\n+\n+  // State for all checkpoints;\n+  private static final ListStateDescriptor<byte[]> STATE_DESCRIPTOR =\n+      new ListStateDescriptor<>(\"checkpoints-state\", BytePrimitiveArraySerializer.INSTANCE);\n+  private transient ListState<byte[]> checkpointsState;\n+\n+  IcebergFilesCommitter(String path, Map<String, String> options, Configuration conf) {\n+    this.path = path;\n+    this.options = ImmutableMap.copyOf(options);\n+    this.conf = new SerializableConfiguration(conf);\n+  }\n+\n+  @Override\n+  public void initializeState(FunctionInitializationContext context) throws Exception {\n+    Catalog icebergCatalog = CATALOG_FACTORY.buildIcebergCatalog(path, options, conf.get());\n+    table = icebergCatalog.loadTable(TableIdentifier.parse(path));\n+    maxCommittedCheckpointId = parseMaxCommittedCheckpointId(table.currentSnapshot());\n+\n+    dataFilesPerCheckpoint = Maps.newTreeMap();\n+    dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+    checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      dataFilesPerCheckpoint = deserializeState(checkpointsState.get().iterator().next());\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(FunctionSnapshotContext context) throws Exception {\n+    long checkpointId = context.getCheckpointId();\n+    LOG.info(\"Start to flush snapshot state to state backend, table: {}, checkpointId: {}\", table, checkpointId);\n+\n+    // Update the checkpoint state.\n+    dataFilesPerCheckpoint.put(checkpointId, ImmutableList.copyOf(dataFilesOfCurrentCheckpoint));\n+\n+    // Reset the snapshot state to the latest state.\n+    checkpointsState.clear();\n+    checkpointsState.addAll(ImmutableList.of(serializeState(dataFilesPerCheckpoint)));\n+\n+    // Clear the local buffer for current checkpoint.\n+    dataFilesOfCurrentCheckpoint.clear();\n+  }\n+\n+  @Override\n+  public void notifyCheckpointComplete(long checkpointId) {\n+    NavigableMap<Long, List<DataFile>> pendingFileMap = dataFilesPerCheckpoint.tailMap(maxCommittedCheckpointId, false);", "originalCommit": "65d7347af2d1cdba405a2b81d204f0148be75141", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDM5NTY0OA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r470395648", "bodyText": "I've added few comments about those members here 6b43ec5.\n\nWhy exclude maxCommittedCheckpointId and checkpointId?\n\nThe maxCommittedCheckPointId 's data files have been committed to iceberg,  so no need to commit them again, otherwise the table will have redundant datas.  For current checkpointId,  we've flushed the dataFilesOfCurrentCheckpoint into dataFilesPerCheckpoint in snapshotState method,  so we did not exclude the files for current checkpointId.", "author": "openinx", "createdAt": "2020-08-14T03:54:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTgxMjc0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTIwMzkzNA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r471203934", "bodyText": "According to the current code, the data of the current checkpoint needs to be committed in the next cycle.\nThis should not be the ideal way, which can lead to excessive delay.\nif (maxCommittedCheckpointId >= checkpointId) {\n  return;\n}\nMap<Long, List<DataFile>> pendingFileMap = dataFilesPerCheckpoint.headMap(checkpointId, true);\nList<DataFile> pendingDataFiles = Lists.newArrayList();\nfor (List<DataFile> dataFiles : pendingFileMap.values()) {\n      pendingDataFiles.addAll(dataFiles);\n}\n-- Do AppendFiles committing\nmaxCommittedCheckpointId = checkpointId;\npendingFileMap.clear();\n\nThis is the process I imagined.", "author": "JingsongLi", "createdAt": "2020-08-17T02:40:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTgxMjc0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIwMzQ3Nw==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r472203477", "bodyText": "the data of the current checkpoint needs to be committed in the next cycle.\n\nThat's not correct. Since we've merged the dataFilesOfCurrentCheckpoint into dataFilesPerCheckpoint,  so the  dataFilesPerCheckpoint.tailMap(maxCommittedCheckpointId, false);  will contains all the uncommitted data files,  include the files from current checkpoint.\nAbout your described process:\nMap<Long, List<DataFile>> pendingFileMap = dataFilesPerCheckpoint.tailMap(checkpointId, true);\nThis seems won't  commit all the previous files which have committed with failure .  How about the data files between (maxCommittedCheckpointId, checkpointId) ?\nIn my thought,  the current process should be correct.  If there're some places I misunderstood, pls correct me,thanks.", "author": "openinx", "createdAt": "2020-08-18T13:38:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTgxMjc0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjc3NjYwMg==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r472776602", "bodyText": "Sorry, wrong reading. Should beheadMap instead of tailMap. You can think again about my above example.\nI got your point about tailMap. But it is incorrect too. Consider the case:\n\n1.snapshot(cpId)\n2.snapshot(cpId + 1)\n3.cpId success, notifyComplete(cpId)\n4.cpId + 1 success, notifyComplete(cpId + 1)\n\nIf there is a failure between 3 and 4, the data in table will be incorrect.", "author": "JingsongLi", "createdAt": "2020-08-19T06:55:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTgxMjc0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjg3NDc4MQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r472874781", "bodyText": "That's a great case (which I did not consider) to proof your point. Yes, we need the headMap(checkpointId, true) to avoid future data files being committed to iceberg table,  I've fixed this point in 598f235.", "author": "openinx", "createdAt": "2020-08-19T08:59:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTgxMjc0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzcxNTYzNA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473715634", "bodyText": "Please add parquet support, and also add tests.", "author": "JingsongLi", "createdAt": "2020-08-20T07:47:34Z", "path": "flink/src/main/java/org/apache/iceberg/flink/RowDataTaskWriterFactory.java", "diffHunk": "@@ -113,7 +114,7 @@ protected PartitionKey partition(RowData row) {\n     }\n   }\n \n-  private static class FlinkFileAppenderFactory implements FileAppenderFactory<RowData> {\n+  private static class FlinkFileAppenderFactory implements FileAppenderFactory<RowData>, Serializable {", "originalCommit": "cbb6ea209b2a63c91d7b179878d7841a2b56086f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc1ODQ1Mw==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473758453", "bodyText": "We need to wait until the parquet writer is available. #1272.  Now only the parquet readers get merged.", "author": "openinx", "createdAt": "2020-08-20T08:27:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzcxNTYzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzcxNjk1Mg==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473716952", "bodyText": "Maybe you can let FileAppenderFactory extends Serializable", "author": "JingsongLi", "createdAt": "2020-08-20T07:48:38Z", "path": "flink/src/main/java/org/apache/iceberg/flink/RowDataTaskWriterFactory.java", "diffHunk": "@@ -113,7 +114,7 @@ protected PartitionKey partition(RowData row) {\n     }\n   }\n \n-  private static class FlinkFileAppenderFactory implements FileAppenderFactory<RowData> {\n+  private static class FlinkFileAppenderFactory implements FileAppenderFactory<RowData>, Serializable {", "originalCommit": "cbb6ea209b2a63c91d7b179878d7841a2b56086f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzcxNzExOQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473717119", "bodyText": "Why?", "author": "JingsongLi", "createdAt": "2020-08-20T07:48:45Z", "path": "core/src/main/java/org/apache/iceberg/io/OutputFileFactory.java", "diffHunk": "@@ -27,7 +28,7 @@\n import org.apache.iceberg.encryption.EncryptedOutputFile;\n import org.apache.iceberg.encryption.EncryptionManager;\n \n-public class OutputFileFactory {\n+public class OutputFileFactory implements Serializable {", "originalCommit": "cbb6ea209b2a63c91d7b179878d7841a2b56086f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc3MTM0OA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473771348", "bodyText": "Because the IcebergStreamWriter will need the RowDataTaskWriterFactory to create TaskWriter,  it could pass it to TaskWriterFactory when constructing, that means all members inside RowDataTaskWriterFactory should be serializable (include itself), unfortunately OutputFileFactory is one of its member.", "author": "openinx", "createdAt": "2020-08-20T08:42:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzcxNzExOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc5MTM3NQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473791375", "bodyText": "outputFileFactory should be transient in RowDataTaskWriterFactory, it is inited in initialize.", "author": "JingsongLi", "createdAt": "2020-08-20T09:04:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzcxNzExOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzgxNTcxOA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473815718", "bodyText": "OK, it make sense.", "author": "openinx", "createdAt": "2020-08-20T09:34:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzcxNzExOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzcxOTE4MQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473719181", "bodyText": "If this is a user API, I think a class FlinkSink (Instead of IcebergSink?) is more suitable.\nAnd do you think we can let it be a builder kind class?", "author": "JingsongLi", "createdAt": "2020-08-20T07:50:16Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergSinkUtil.java", "diffHunk": "@@ -37,9 +45,32 @@\n import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n \n class IcebergSinkUtil {\n+\n   private IcebergSinkUtil() {\n   }\n \n+  @SuppressWarnings(\"unchecked\")\n+  static DataStreamSink<RowData> write(DataStream<RowData> inputStream,", "originalCommit": "cbb6ea209b2a63c91d7b179878d7841a2b56086f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzcyNTc2Nw==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473725767", "bodyText": "Actually, I think it is just flinkSchema.", "author": "JingsongLi", "createdAt": "2020-08-20T07:55:07Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergSinkUtil.java", "diffHunk": "@@ -37,9 +45,32 @@\n import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n \n class IcebergSinkUtil {\n+\n   private IcebergSinkUtil() {\n   }\n \n+  @SuppressWarnings(\"unchecked\")\n+  static DataStreamSink<RowData> write(DataStream<RowData> inputStream,\n+                                       Map<String, String> options,\n+                                       Configuration conf,\n+                                       String fullTableName,\n+                                       Table table,\n+                                       TableSchema requestedSchema) {", "originalCommit": "cbb6ea209b2a63c91d7b179878d7841a2b56086f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzcyODQxMg==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473728412", "bodyText": "Why not use jobId?", "author": "JingsongLi", "createdAt": "2020-08-20T07:57:09Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String GLOBAL_FILES_COMMITTER_UID = \"flink.files-committer.uid\";\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed.checkpoint.id\";\n+\n+  private static final FlinkCatalogFactory CATALOG_FACTORY = new FlinkCatalogFactory();\n+\n+  // It will have an unique identifier for one job.\n+  private final String filesCommitterUid;\n+  private final String fullTableName;\n+  private final SerializableConfiguration conf;\n+  private final ImmutableMap<String, String> options;\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private transient long maxCommittedCheckpointId;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+  private transient Table table;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(String filesCommitterUid, String fullTableName,", "originalCommit": "cbb6ea209b2a63c91d7b179878d7841a2b56086f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzczMDg2NA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473730864", "bodyText": "maxCommittedCheckpointId can be a local field?", "author": "JingsongLi", "createdAt": "2020-08-20T07:58:58Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String GLOBAL_FILES_COMMITTER_UID = \"flink.files-committer.uid\";\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed.checkpoint.id\";\n+\n+  private static final FlinkCatalogFactory CATALOG_FACTORY = new FlinkCatalogFactory();\n+\n+  // It will have an unique identifier for one job.\n+  private final String filesCommitterUid;\n+  private final String fullTableName;\n+  private final SerializableConfiguration conf;\n+  private final ImmutableMap<String, String> options;\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private transient long maxCommittedCheckpointId;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+  private transient Table table;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(String filesCommitterUid, String fullTableName,\n+                        Map<String, String> options, Configuration conf) {\n+    this.filesCommitterUid = filesCommitterUid;\n+    this.fullTableName = fullTableName;\n+    this.options = ImmutableMap.copyOf(options);\n+    this.conf = new SerializableConfiguration(conf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    Catalog icebergCatalog = CATALOG_FACTORY.buildIcebergCatalog(fullTableName, options, conf.get());\n+\n+    table = icebergCatalog.loadTable(TableIdentifier.parse(fullTableName));\n+    maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;", "originalCommit": "cbb6ea209b2a63c91d7b179878d7841a2b56086f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzczMzI5NA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473733294", "bodyText": "Maybe just maxCommittedCheckpointId != INIT_ID?", "author": "JingsongLi", "createdAt": "2020-08-20T08:00:52Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String GLOBAL_FILES_COMMITTER_UID = \"flink.files-committer.uid\";\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed.checkpoint.id\";\n+\n+  private static final FlinkCatalogFactory CATALOG_FACTORY = new FlinkCatalogFactory();\n+\n+  // It will have an unique identifier for one job.\n+  private final String filesCommitterUid;\n+  private final String fullTableName;\n+  private final SerializableConfiguration conf;\n+  private final ImmutableMap<String, String> options;\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private transient long maxCommittedCheckpointId;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+  private transient Table table;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(String filesCommitterUid, String fullTableName,\n+                        Map<String, String> options, Configuration conf) {\n+    this.filesCommitterUid = filesCommitterUid;\n+    this.fullTableName = fullTableName;\n+    this.options = ImmutableMap.copyOf(options);\n+    this.conf = new SerializableConfiguration(conf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    Catalog icebergCatalog = CATALOG_FACTORY.buildIcebergCatalog(fullTableName, options, conf.get());\n+\n+    table = icebergCatalog.loadTable(TableIdentifier.parse(fullTableName));\n+    maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n+\n+    checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, filesCommitterUid);\n+      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n+      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n+      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n+      // the iceberg table.\n+      Preconditions.checkArgument(maxCommittedCheckpointId > 0,", "originalCommit": "cbb6ea209b2a63c91d7b179878d7841a2b56086f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc2NDE0Mw==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473764143", "bodyText": "I checked the ZooKeeperCheckpointIDCounter and StandaloneCheckpointIDCounter.  Both of them should start from 1.", "author": "openinx", "createdAt": "2020-08-20T08:34:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzczMzI5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc5MjE4NA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473792184", "bodyText": "Yes, but I think maxCommittedCheckpointId != INIT_ID should be more reasonable.", "author": "JingsongLi", "createdAt": "2020-08-20T09:05:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzczMzI5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzczNjc0MA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473736740", "bodyText": "checkpointsState.add(dataFilesPerCheckpoint);", "author": "JingsongLi", "createdAt": "2020-08-20T08:03:42Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String GLOBAL_FILES_COMMITTER_UID = \"flink.files-committer.uid\";\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed.checkpoint.id\";\n+\n+  private static final FlinkCatalogFactory CATALOG_FACTORY = new FlinkCatalogFactory();\n+\n+  // It will have an unique identifier for one job.\n+  private final String filesCommitterUid;\n+  private final String fullTableName;\n+  private final SerializableConfiguration conf;\n+  private final ImmutableMap<String, String> options;\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private transient long maxCommittedCheckpointId;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+  private transient Table table;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(String filesCommitterUid, String fullTableName,\n+                        Map<String, String> options, Configuration conf) {\n+    this.filesCommitterUid = filesCommitterUid;\n+    this.fullTableName = fullTableName;\n+    this.options = ImmutableMap.copyOf(options);\n+    this.conf = new SerializableConfiguration(conf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    Catalog icebergCatalog = CATALOG_FACTORY.buildIcebergCatalog(fullTableName, options, conf.get());\n+\n+    table = icebergCatalog.loadTable(TableIdentifier.parse(fullTableName));\n+    maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n+\n+    checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, filesCommitterUid);\n+      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n+      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n+      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n+      // the iceberg table.\n+      Preconditions.checkArgument(maxCommittedCheckpointId > 0,\n+          \"There should be an existing iceberg snapshot for current flink job: %s\", filesCommitterUid);\n+\n+      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n+      // Only keep the uncommitted data files in the cache.\n+      dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext context) throws Exception {\n+    super.snapshotState(context);\n+    long checkpointId = context.getCheckpointId();\n+    LOG.info(\"Start to flush snapshot state to state backend, table: {}, checkpointId: {}\", table, checkpointId);\n+\n+    // Update the checkpoint state.\n+    dataFilesPerCheckpoint.put(checkpointId, ImmutableList.copyOf(dataFilesOfCurrentCheckpoint));\n+\n+    // Reset the snapshot state to the latest state.\n+    checkpointsState.clear();\n+    checkpointsState.addAll(ImmutableList.of(dataFilesPerCheckpoint));", "originalCommit": "cbb6ea209b2a63c91d7b179878d7841a2b56086f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc0MDQ3Mg==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473740472", "bodyText": "Maybe we should commit even pendingDataFiles is empty for MAX_COMMITTED_CHECKPOINT_ID.", "author": "JingsongLi", "createdAt": "2020-08-20T08:06:51Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String GLOBAL_FILES_COMMITTER_UID = \"flink.files-committer.uid\";\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed.checkpoint.id\";\n+\n+  private static final FlinkCatalogFactory CATALOG_FACTORY = new FlinkCatalogFactory();\n+\n+  // It will have an unique identifier for one job.\n+  private final String filesCommitterUid;\n+  private final String fullTableName;\n+  private final SerializableConfiguration conf;\n+  private final ImmutableMap<String, String> options;\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private transient long maxCommittedCheckpointId;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+  private transient Table table;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(String filesCommitterUid, String fullTableName,\n+                        Map<String, String> options, Configuration conf) {\n+    this.filesCommitterUid = filesCommitterUid;\n+    this.fullTableName = fullTableName;\n+    this.options = ImmutableMap.copyOf(options);\n+    this.conf = new SerializableConfiguration(conf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    Catalog icebergCatalog = CATALOG_FACTORY.buildIcebergCatalog(fullTableName, options, conf.get());\n+\n+    table = icebergCatalog.loadTable(TableIdentifier.parse(fullTableName));\n+    maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n+\n+    checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, filesCommitterUid);\n+      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n+      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n+      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n+      // the iceberg table.\n+      Preconditions.checkArgument(maxCommittedCheckpointId > 0,\n+          \"There should be an existing iceberg snapshot for current flink job: %s\", filesCommitterUid);\n+\n+      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n+      // Only keep the uncommitted data files in the cache.\n+      dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext context) throws Exception {\n+    super.snapshotState(context);\n+    long checkpointId = context.getCheckpointId();\n+    LOG.info(\"Start to flush snapshot state to state backend, table: {}, checkpointId: {}\", table, checkpointId);\n+\n+    // Update the checkpoint state.\n+    dataFilesPerCheckpoint.put(checkpointId, ImmutableList.copyOf(dataFilesOfCurrentCheckpoint));\n+\n+    // Reset the snapshot state to the latest state.\n+    checkpointsState.clear();\n+    checkpointsState.addAll(ImmutableList.of(dataFilesPerCheckpoint));\n+\n+    // Clear the local buffer for current checkpoint.\n+    dataFilesOfCurrentCheckpoint.clear();\n+  }\n+\n+  @Override\n+  public void notifyCheckpointComplete(long checkpointId) throws Exception {\n+    super.notifyCheckpointComplete(checkpointId);\n+    commitUpToCheckpoint(checkpointId);\n+  }\n+\n+  private void commitUpToCheckpoint(long checkpointId) {\n+    NavigableMap<Long, List<DataFile>> pendingFileMap = dataFilesPerCheckpoint.headMap(checkpointId, true);\n+\n+    List<DataFile> pendingDataFiles = Lists.newArrayList();\n+    for (List<DataFile> dataFiles : pendingFileMap.values()) {\n+      pendingDataFiles.addAll(dataFiles);\n+    }\n+\n+    if (!pendingDataFiles.isEmpty()) {\n+      AppendFiles appendFiles = table.newAppend();\n+      pendingDataFiles.forEach(appendFiles::appendFile);\n+      appendFiles.set(MAX_COMMITTED_CHECKPOINT_ID, Long.toString(checkpointId));\n+      appendFiles.set(GLOBAL_FILES_COMMITTER_UID, filesCommitterUid);\n+      appendFiles.commit();", "originalCommit": "cbb6ea209b2a63c91d7b179878d7841a2b56086f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzg5MjE3Mw==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473892173", "bodyText": "The MAX_COMMITTED_CHECKPOINT_ID  stored in iceberg table is mainly used for avoiding duplicated data files to be committed to iceberg table.  Now we're sure that there's no data files to commit,  advancing the max-committed-checkpoint-id don't have much value in my thought.", "author": "openinx", "createdAt": "2020-08-20T11:12:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc0MDQ3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDQyMDE3MA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r474420170", "bodyText": "I'm not sure whether the following scenarios are common: however, if there is no data continuously, the previous snapshot may have expired, and an error will be reported after the Flink job failover.\nContinue to write snapshots so that others or monitor can feel: Hey, I'm alive, although I don't write data.", "author": "JingsongLi", "createdAt": "2020-08-21T05:42:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc0MDQ3Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDQ2MTIyNA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r474461224", "bodyText": "The case you described should be a rare case,  but I agree with you that we'd better to update the max-committed-checkpoint-id, so that the failover won't fail.", "author": "openinx", "createdAt": "2020-08-21T07:19:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc0MDQ3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc0MTQ3OA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473741478", "bodyText": "Maybe should be checkState.", "author": "JingsongLi", "createdAt": "2020-08-20T08:08:04Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String GLOBAL_FILES_COMMITTER_UID = \"flink.files-committer.uid\";\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed.checkpoint.id\";\n+\n+  private static final FlinkCatalogFactory CATALOG_FACTORY = new FlinkCatalogFactory();\n+\n+  // It will have an unique identifier for one job.\n+  private final String filesCommitterUid;\n+  private final String fullTableName;\n+  private final SerializableConfiguration conf;\n+  private final ImmutableMap<String, String> options;\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private transient long maxCommittedCheckpointId;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+  private transient Table table;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(String filesCommitterUid, String fullTableName,\n+                        Map<String, String> options, Configuration conf) {\n+    this.filesCommitterUid = filesCommitterUid;\n+    this.fullTableName = fullTableName;\n+    this.options = ImmutableMap.copyOf(options);\n+    this.conf = new SerializableConfiguration(conf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    Catalog icebergCatalog = CATALOG_FACTORY.buildIcebergCatalog(fullTableName, options, conf.get());\n+\n+    table = icebergCatalog.loadTable(TableIdentifier.parse(fullTableName));\n+    maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n+\n+    checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, filesCommitterUid);\n+      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n+      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n+      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n+      // the iceberg table.\n+      Preconditions.checkArgument(maxCommittedCheckpointId > 0,", "originalCommit": "cbb6ea209b2a63c91d7b179878d7841a2b56086f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc0MzQxNA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473743414", "bodyText": "checkpoint-id?", "author": "JingsongLi", "createdAt": "2020-08-20T08:10:26Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String GLOBAL_FILES_COMMITTER_UID = \"flink.files-committer.uid\";\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed.checkpoint.id\";", "originalCommit": "cbb6ea209b2a63c91d7b179878d7841a2b56086f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc0MzU2NA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473743564", "bodyText": "uid -> job-id?", "author": "JingsongLi", "createdAt": "2020-08-20T08:10:37Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String GLOBAL_FILES_COMMITTER_UID = \"flink.files-committer.uid\";", "originalCommit": "cbb6ea209b2a63c91d7b179878d7841a2b56086f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc0NDM1NQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473744355", "bodyText": "Please name the sink.", "author": "JingsongLi", "createdAt": "2020-08-20T08:11:38Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergSinkUtil.java", "diffHunk": "@@ -37,9 +45,32 @@\n import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n \n class IcebergSinkUtil {\n+\n   private IcebergSinkUtil() {\n   }\n \n+  @SuppressWarnings(\"unchecked\")\n+  static DataStreamSink<RowData> write(DataStream<RowData> inputStream,\n+                                       Map<String, String> options,\n+                                       Configuration conf,\n+                                       String fullTableName,\n+                                       Table table,\n+                                       TableSchema requestedSchema) {\n+    IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, requestedSchema);\n+\n+    String filesCommitterUID = String.format(\"IcebergFilesCommitter-%s\", UUID.randomUUID().toString());\n+    IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(filesCommitterUID, fullTableName, options, conf);\n+\n+    DataStream<Void> returnStream = inputStream\n+        .transform(IcebergStreamWriter.class.getSimpleName(), TypeInformation.of(DataFile.class), streamWriter)\n+        .setParallelism(inputStream.getParallelism())\n+        .transform(IcebergFilesCommitter.class.getSimpleName(), Types.VOID, filesCommitter)\n+        .setParallelism(1)\n+        .setMaxParallelism(1);\n+\n+    return returnStream.addSink(new DiscardingSink()).setParallelism(1);", "originalCommit": "cbb6ea209b2a63c91d7b179878d7841a2b56086f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc0NjE2OQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r473746169", "bodyText": "I just notice IcebergStreamWriter does not set chaining strategy. Should add in the constructor of IcebergStreamWriter: setChainingStrategy(ChainingStrategy.ALWAYS);", "author": "JingsongLi", "createdAt": "2020-08-20T08:13:44Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergSinkUtil.java", "diffHunk": "@@ -37,9 +45,32 @@\n import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n \n class IcebergSinkUtil {\n+\n   private IcebergSinkUtil() {\n   }\n \n+  @SuppressWarnings(\"unchecked\")\n+  static DataStreamSink<RowData> write(DataStream<RowData> inputStream,\n+                                       Map<String, String> options,\n+                                       Configuration conf,\n+                                       String fullTableName,\n+                                       Table table,\n+                                       TableSchema requestedSchema) {\n+    IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, requestedSchema);", "originalCommit": "cbb6ea209b2a63c91d7b179878d7841a2b56086f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "20196f8100589c3d3f4a2cce0a740e4374608437", "url": "https://github.com/apache/iceberg/commit/20196f8100589c3d3f4a2cce0a740e4374608437", "message": "Use the TableLoader to load table lazily inside icebergFilesCommitter", "committedDate": "2020-08-21T03:06:02Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMyOTQwNw==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r475329407", "bodyText": "NIT:  I think these fields not have to be static.", "author": "JingsongLi", "createdAt": "2020-08-24T03:58:43Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkSink.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+public class FlinkSink {\n+\n+  private static final TypeInformation<DataFile> DATA_FILE_TYPE_INFO = TypeInformation.of(DataFile.class);", "originalCommit": "3fdffd7f08f236b5fc36278dd79bbdc92b05ccdf", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMzMDY0OA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r475330648", "bodyText": "Actually, the default value of maxCommittedCheckpointId can be null instead of INITIAL_CHECKPOINT_ID.", "author": "JingsongLi", "createdAt": "2020-08-24T04:04:34Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.ChainingStrategy;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+  // It will have an unique identifier for one job.\n+  private transient String flinkJobId;\n+  private transient Table table;\n+  private transient long maxCommittedCheckpointId;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(TableLoader tableLoader, Configuration hadoopConf) {\n+    this.tableLoader = tableLoader;\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+    setChainingStrategy(ChainingStrategy.ALWAYS);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    flinkJobId = getContainingTask().getEnvironment().getJobID().toString();\n+\n+    // Open the table loader and load the table.\n+    tableLoader.open(hadoopConf.get());\n+    table = tableLoader.loadTable();\n+\n+    checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);\n+      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n+      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n+      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n+      // the iceberg table.\n+      Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID,\n+          \"There should be an existing iceberg snapshot for current flink job: %s\", flinkJobId);\n+\n+      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n+      // Only keep the uncommitted data files in the cache.\n+      dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext context) throws Exception {\n+    super.snapshotState(context);\n+    long checkpointId = context.getCheckpointId();\n+    LOG.info(\"Start to flush snapshot state to state backend, table: {}, checkpointId: {}\", table, checkpointId);\n+\n+    // Update the checkpoint state.\n+    dataFilesPerCheckpoint.put(checkpointId, ImmutableList.copyOf(dataFilesOfCurrentCheckpoint));\n+\n+    // Reset the snapshot state to the latest state.\n+    checkpointsState.clear();\n+    checkpointsState.add(dataFilesPerCheckpoint);\n+\n+    // Clear the local buffer for current checkpoint.\n+    dataFilesOfCurrentCheckpoint.clear();\n+  }\n+\n+  @Override\n+  public void notifyCheckpointComplete(long checkpointId) throws Exception {\n+    super.notifyCheckpointComplete(checkpointId);\n+    // It's possible that we have the following events:\n+    //   1. snapshotState(ckpId);\n+    //   2. snapshotState(ckpId+1);\n+    //   3. notifyCheckpointComplete(ckpId+1);\n+    //   4. notifyCheckpointComplete(ckpId);\n+    // For step#4, we don't need to commit iceberg table again because in step#3 we've committed all the files,\n+    // Besides, we need to maintain the max-committed-checkpoint-id to be increasing.\n+    if (checkpointId > maxCommittedCheckpointId) {\n+      commitUpToCheckpoint(checkpointId);\n+      maxCommittedCheckpointId = checkpointId;\n+    }\n+  }\n+\n+  private void commitUpToCheckpoint(long checkpointId) {\n+    NavigableMap<Long, List<DataFile>> pendingFileMap = dataFilesPerCheckpoint.headMap(checkpointId, true);\n+\n+    List<DataFile> pendingDataFiles = Lists.newArrayList();\n+    for (List<DataFile> dataFiles : pendingFileMap.values()) {\n+      pendingDataFiles.addAll(dataFiles);\n+    }\n+\n+    AppendFiles appendFiles = table.newAppend();\n+    pendingDataFiles.forEach(appendFiles::appendFile);\n+    appendFiles.set(MAX_COMMITTED_CHECKPOINT_ID, Long.toString(checkpointId));\n+    appendFiles.set(FLINK_JOB_ID, flinkJobId);\n+    appendFiles.commit();\n+\n+    // Clear the committed data files from dataFilesPerCheckpoint.\n+    pendingFileMap.clear();\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<DataFile> element) {\n+    this.dataFilesOfCurrentCheckpoint.add(element.getValue());\n+  }\n+\n+  @Override\n+  public void endInput() {\n+    commitUpToCheckpoint(Long.MAX_VALUE);\n+  }\n+\n+  private static ListStateDescriptor<SortedMap<Long, List<DataFile>>> buildStateDescriptor() {\n+    Comparator<Long> longComparator = Comparators.forType(Types.LongType.get());\n+    // Construct a ListTypeInfo.\n+    ListTypeInfo<DataFile> dataFileListTypeInfo = new ListTypeInfo<>(TypeInformation.of(DataFile.class));\n+    // Construct a SortedMapTypeInfo.\n+    SortedMapTypeInfo<Long, List<DataFile>> sortedMapTypeInfo = new SortedMapTypeInfo<>(\n+        BasicTypeInfo.LONG_TYPE_INFO, dataFileListTypeInfo, longComparator\n+    );\n+    return new ListStateDescriptor<>(\"iceberg-files-committer-state\", sortedMapTypeInfo);\n+  }\n+\n+  static Long getMaxCommittedCheckpointId(Table table, String flinkJobId) {", "originalCommit": "3fdffd7f08f236b5fc36278dd79bbdc92b05ccdf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTM2OTc0OA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r475369748", "bodyText": "For me, initializing the maxCommittedCheckpointId  to  -1 seems simpler to handle, because when calling SortedMap#tailMap or SortedMap#headMap we don't need to check nullable. The comparasion in notifyCheckpointComplete is the similar point.", "author": "openinx", "createdAt": "2020-08-24T06:31:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMzMDY0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMzMTYwNQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r475331605", "bodyText": "We can let committer be a single node.", "author": "JingsongLi", "createdAt": "2020-08-24T04:08:58Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.ChainingStrategy;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+  // It will have an unique identifier for one job.\n+  private transient String flinkJobId;\n+  private transient Table table;\n+  private transient long maxCommittedCheckpointId;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(TableLoader tableLoader, Configuration hadoopConf) {\n+    this.tableLoader = tableLoader;\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+    setChainingStrategy(ChainingStrategy.ALWAYS);", "originalCommit": "3fdffd7f08f236b5fc36278dd79bbdc92b05ccdf", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMzMTgwMA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r475331800", "bodyText": "Better to close this tableLoader in the dispose.", "author": "JingsongLi", "createdAt": "2020-08-24T04:09:45Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.ChainingStrategy;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+  // It will have an unique identifier for one job.\n+  private transient String flinkJobId;\n+  private transient Table table;\n+  private transient long maxCommittedCheckpointId;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(TableLoader tableLoader, Configuration hadoopConf) {\n+    this.tableLoader = tableLoader;\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+    setChainingStrategy(ChainingStrategy.ALWAYS);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    flinkJobId = getContainingTask().getEnvironment().getJobID().toString();\n+\n+    // Open the table loader and load the table.\n+    tableLoader.open(hadoopConf.get());", "originalCommit": "3fdffd7f08f236b5fc36278dd79bbdc92b05ccdf", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "8d20dfe2774cd09d95b5edc7d48c3f528d06d408", "url": "https://github.com/apache/iceberg/commit/8d20dfe2774cd09d95b5edc7d48c3f528d06d408", "message": "Add parquet to the unit tests.", "committedDate": "2020-08-25T03:37:59Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjI5OTIwMg==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r476299202", "bodyText": "Since we already have TableLoader, will it be easier for user if we load table here ?", "author": "jrthe42", "createdAt": "2020-08-25T09:09:22Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkSink.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+public class FlinkSink {\n+\n+  private static final String ICEBERG_STREAM_WRITER_NAME = IcebergStreamWriter.class.getSimpleName();\n+  private static final String ICEBERG_FILES_COMMITTER_NAME = IcebergFilesCommitter.class.getSimpleName();\n+\n+  private FlinkSink() {\n+  }\n+\n+  public static Builder forRow(DataStream<Row> input) {\n+    return new Builder().forRow(input);\n+  }\n+\n+  public static Builder forRowData(DataStream<RowData> input) {\n+    return new Builder().forRowData(input);\n+  }\n+\n+  public static class Builder {\n+    private DataStream<Row> rowInput = null;\n+    private DataStream<RowData> rowDataInput = null;\n+    private TableLoader tableLoader;\n+    private Configuration hadoopConf;\n+    private Table table;\n+    private TableSchema tableSchema;\n+\n+    private Builder forRow(DataStream<Row> newRowInput) {\n+      this.rowInput = newRowInput;\n+      return this;\n+    }\n+\n+    private Builder forRowData(DataStream<RowData> newRowDataInput) {\n+      this.rowDataInput = newRowDataInput;\n+      return this;\n+    }\n+\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    public Builder tableLoader(TableLoader newTableLoader) {\n+      this.tableLoader = newTableLoader;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(Configuration newHadoopConf) {\n+      this.hadoopConf = newHadoopConf;\n+      return this;\n+    }\n+\n+    public Builder tableSchema(TableSchema newTableSchema) {\n+      this.tableSchema = newTableSchema;\n+      return this;\n+    }\n+\n+    private DataStream<RowData> convert() {\n+      Preconditions.checkArgument(rowInput != null, \"The DataStream<Row> to convert shouldn't be null\");\n+\n+      RowType rowType;\n+      DataType[] fieldDataTypes;\n+      if (tableSchema != null) {\n+        rowType = (RowType) tableSchema.toRowDataType().getLogicalType();\n+        fieldDataTypes = tableSchema.getFieldDataTypes();\n+      } else {\n+        rowType = FlinkSchemaUtil.convert(table.schema());\n+        fieldDataTypes = TypeConversions.fromLogicalToDataType(rowType.getChildren().toArray(new LogicalType[0]));\n+      }\n+\n+      DataFormatConverters.RowConverter rowConverter = new DataFormatConverters.RowConverter(fieldDataTypes);\n+\n+      return rowInput.map(rowConverter::toInternal, RowDataTypeInfo.of(rowType));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public DataStreamSink<RowData> build() {\n+      Preconditions.checkArgument(rowInput != null || rowDataInput != null,\n+          \"Should initialize input DataStream first with DataStream<Row> or DataStream<RowData>\");\n+      Preconditions.checkArgument(rowInput == null || rowDataInput == null,\n+          \"Could only initialize input DataStream with either DataStream<Row> or DataStream<RowData>\");\n+      Preconditions.checkNotNull(table, \"Table shouldn't be null\");", "originalCommit": "8d20dfe2774cd09d95b5edc7d48c3f528d06d408", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE4Njg4Mg==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r477186882", "bodyText": "The table is used to initialize IcebergStreamWriter  at client side,  while the tableLoader is used to load table at TaskManager side,   using the tableLoader for TaskManager to load table for  client side looks strange to me.", "author": "openinx", "createdAt": "2020-08-26T10:05:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjI5OTIwMg=="}], "type": "inlineReview"}, {"oid": "9f238d699c0c9976b7003f561cfaabd0e8e1f266", "url": "https://github.com/apache/iceberg/commit/9f238d699c0c9976b7003f561cfaabd0e8e1f266", "message": "Flink: Add the iceberg files committer to collect data files and commit to iceberg table.", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "47db75fb329d35b4f360613c60ee4cab144a3347", "url": "https://github.com/apache/iceberg/commit/47db75fb329d35b4f360613c60ee4cab144a3347", "message": "Add comments for the critical members", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "0fcf8c4374777fed5ac75a167832bb971105c5dc", "url": "https://github.com/apache/iceberg/commit/0fcf8c4374777fed5ac75a167832bb971105c5dc", "message": "Read the max-committed-checkpoint-id only when restoring the job.", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "2ca3e45201eb0dc2c0e623abf5fcc18859382fa1", "url": "https://github.com/apache/iceberg/commit/2ca3e45201eb0dc2c0e623abf5fcc18859382fa1", "message": "Fix the broken unit tests.", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "02ece7e607ba77f4d75a1556f5182291bc531db9", "url": "https://github.com/apache/iceberg/commit/02ece7e607ba77f4d75a1556f5182291bc531db9", "message": "Refactor to use the SortedMapTypeInfo", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "24c9e80cfcc3344385ef272b2a232d3556999e08", "url": "https://github.com/apache/iceberg/commit/24c9e80cfcc3344385ef272b2a232d3556999e08", "message": "Handle the cases that two different jobs write the the same table.", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "ad83367c3759de9fd17bfa8fce4fb5b4a810f57d", "url": "https://github.com/apache/iceberg/commit/ad83367c3759de9fd17bfa8fce4fb5b4a810f57d", "message": "Minor fix", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "6f5307eb20afac386d46b96bc10f1d01783a7d87", "url": "https://github.com/apache/iceberg/commit/6f5307eb20afac386d46b96bc10f1d01783a7d87", "message": "Make IcebergFilesCommitter extends from AbstractStreamOperator for committing txn for bounded stream", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "527ea4c0aa45063266b444dcb7869ae5b89bee71", "url": "https://github.com/apache/iceberg/commit/527ea4c0aa45063266b444dcb7869ae5b89bee71", "message": "Add the unit tests for ordered and disordered events between two continues checkpoints", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "8542b703382ed346a60d45ac18c9b9d78650a9e0", "url": "https://github.com/apache/iceberg/commit/8542b703382ed346a60d45ac18c9b9d78650a9e0", "message": "Fix the checkstyle", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "d14aa089b5e9efa346f54bdcd52b405c1a3a5792", "url": "https://github.com/apache/iceberg/commit/d14aa089b5e9efa346f54bdcd52b405c1a3a5792", "message": "Add a check to validate the parsed maxCommittedCheckpointId.", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "2cb965828d05efae43acd837f39c8465f0a382a9", "url": "https://github.com/apache/iceberg/commit/2cb965828d05efae43acd837f39c8465f0a382a9", "message": "Improve the flink data stream unit tests.", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "7c532604481f1f13b6b8fd0dcd4c83e91db49463", "url": "https://github.com/apache/iceberg/commit/7c532604481f1f13b6b8fd0dcd4c83e91db49463", "message": "Addressing comments.", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "951a0ceccb57f8666e7ff99e67fdb886111bc074", "url": "https://github.com/apache/iceberg/commit/951a0ceccb57f8666e7ff99e67fdb886111bc074", "message": "Minor fixes", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "65c192165655fc7557db8914bf852b8b4bd13ca6", "url": "https://github.com/apache/iceberg/commit/65c192165655fc7557db8914bf852b8b4bd13ca6", "message": "Introduce orc to the filesCommitter unit tests.", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "71585830938bf70ab867db249321be1dbddba3ca", "url": "https://github.com/apache/iceberg/commit/71585830938bf70ab867db249321be1dbddba3ca", "message": "Use the TableLoader to load table lazily inside icebergFilesCommitter", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "fbc30fcfe9ffbbf2699d33913533c7d68c0cfb88", "url": "https://github.com/apache/iceberg/commit/fbc30fcfe9ffbbf2699d33913533c7d68c0cfb88", "message": "Advancing the max-committed-checkpoint-id when there's no data files to commit.", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "35fa7d3852e91641719e10ca36f9316be566820f", "url": "https://github.com/apache/iceberg/commit/35fa7d3852e91641719e10ca36f9316be566820f", "message": "Fix the broken unit tests after ORC was introduced", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "3e3fa49d8b1447d7885fb8e3364452c6024abfb8", "url": "https://github.com/apache/iceberg/commit/3e3fa49d8b1447d7885fb8e3364452c6024abfb8", "message": "Keep the increased max-committed-checkpoint-id", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "ac9d1e24a9f304edf6307b8ec22411a2fc94f363", "url": "https://github.com/apache/iceberg/commit/ac9d1e24a9f304edf6307b8ec22411a2fc94f363", "message": "Refactor the FlinkSink builder to support both DataStream<Row> dan DataStream<RowData>", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "56d0ed6c6b8da245fe61647aac2d3ec30d508e6f", "url": "https://github.com/apache/iceberg/commit/56d0ed6c6b8da245fe61647aac2d3ec30d508e6f", "message": "Close the table loader once we disposed in iceberg files committer", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "d7b42c3647c0a9e8b7dab97bd49ff7612e6091c2", "url": "https://github.com/apache/iceberg/commit/d7b42c3647c0a9e8b7dab97bd49ff7612e6091c2", "message": "Addressing the comments", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "31d84365a4adf399551b7e45c5da4c1380a4d913", "url": "https://github.com/apache/iceberg/commit/31d84365a4adf399551b7e45c5da4c1380a4d913", "message": "Add parquet to the unit tests.", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "64f05738fd42237973a3937c67c88ec656527f57", "url": "https://github.com/apache/iceberg/commit/64f05738fd42237973a3937c67c88ec656527f57", "message": "Add unit tests for bounded stream", "committedDate": "2020-08-27T02:09:35Z", "type": "commit"}, {"oid": "826ef560fc8ad47cbc0bb991480481c720b3d1a1", "url": "https://github.com/apache/iceberg/commit/826ef560fc8ad47cbc0bb991480481c720b3d1a1", "message": "Rebase to master and fix the complie error", "committedDate": "2020-08-27T02:22:34Z", "type": "commit"}, {"oid": "826ef560fc8ad47cbc0bb991480481c720b3d1a1", "url": "https://github.com/apache/iceberg/commit/826ef560fc8ad47cbc0bb991480481c720b3d1a1", "message": "Rebase to master and fix the complie error", "committedDate": "2020-08-27T02:22:34Z", "type": "forcePushed"}, {"oid": "fd6a22e9d233952fac0bf838d22cf97470c469e9", "url": "https://github.com/apache/iceberg/commit/fd6a22e9d233952fac0bf838d22cf97470c469e9", "message": "Move the writer classes to sink package", "committedDate": "2020-08-27T10:23:36Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc2NzI1Mw==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478767253", "bodyText": "This isn't required for Spark. Could we add this to the FlinkFileAppenderFactory instead?", "author": "rdblue", "createdAt": "2020-08-28T00:34:18Z", "path": "core/src/main/java/org/apache/iceberg/io/FileAppenderFactory.java", "diffHunk": "@@ -19,14 +19,15 @@\n \n package org.apache.iceberg.io;\n \n+import java.io.Serializable;\n import org.apache.iceberg.FileFormat;\n \n /**\n  * Factory to create a new {@link FileAppender} to write records.\n  *\n  * @param <T> data type of the rows to append.\n  */\n-public interface FileAppenderFactory<T> {\n+public interface FileAppenderFactory<T> extends Serializable {", "originalCommit": "fd6a22e9d233952fac0bf838d22cf97470c469e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc5MDY4NA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478790684", "bodyText": "That sounds reasonable.", "author": "openinx", "createdAt": "2020-08-28T02:06:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc2NzI1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc2OTYzMg==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478769632", "bodyText": "Curious: are there any plans to support other object models? I know the original sink used Avro, which is supported by Avro and Parquet file formats.\nMaybe for that we need a better way to configure file formats with object models, though.", "author": "rdblue", "createdAt": "2020-08-28T00:43:50Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+public class FlinkSink {\n+\n+  private static final String ICEBERG_STREAM_WRITER_NAME = IcebergStreamWriter.class.getSimpleName();\n+  private static final String ICEBERG_FILES_COMMITTER_NAME = IcebergFilesCommitter.class.getSimpleName();\n+\n+  private FlinkSink() {\n+  }\n+\n+  public static Builder forRow(DataStream<Row> input) {\n+    return new Builder().forRow(input);", "originalCommit": "fd6a22e9d233952fac0bf838d22cf97470c469e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc5NDU3OQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478794579", "bodyText": "It will be quite easy to support other object models, we only need a MapFunction to convert the generic object to RowData and define what's the TypeInformation for RowData.  For example, we could create a separate builder interfaces for the generic object models on top of this builder:\n  public static <T> Builder builderFor(DataStream<T> input,\n                                       Function<T, RowData> converter,\n                                       TypeInformation<RowData> outputType) {\n    DataStream<RowData> dataStream = input.map(converter::apply, outputType);\n    return forRowData(dataStream);\n  }", "author": "openinx", "createdAt": "2020-08-28T02:22:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc2OTYzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQzMzc2OQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479433769", "bodyText": "That sounds okay to me. We can always extend this with a path for other record formats in our version. And I do think it would be much easier to do once we have a more pluggable method to specify object models and data formats.", "author": "rdblue", "createdAt": "2020-08-28T17:12:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc2OTYzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUyNzk0MQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479527941", "bodyText": "@openinx I would prefer to avoid the extra conversion. E.g. we use Avro for streaming data and the ParquetAvroWriter from Iceberg library for writer function. On the other hand, we can supply an Avro TaskWriterFactory to IcebergStreamWriter.", "author": "stevenzwu", "createdAt": "2020-08-28T20:49:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc2OTYzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3MDE1NQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478770155", "bodyText": "I don't think these messages are very helpful. The problem is that neither forRow or forRowData is called. Being specific about how to fix it (call one of those methods) would be a more helpful error.", "author": "rdblue", "createdAt": "2020-08-28T00:45:50Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+public class FlinkSink {\n+\n+  private static final String ICEBERG_STREAM_WRITER_NAME = IcebergStreamWriter.class.getSimpleName();\n+  private static final String ICEBERG_FILES_COMMITTER_NAME = IcebergFilesCommitter.class.getSimpleName();\n+\n+  private FlinkSink() {\n+  }\n+\n+  public static Builder forRow(DataStream<Row> input) {\n+    return new Builder().forRow(input);\n+  }\n+\n+  public static Builder forRowData(DataStream<RowData> input) {\n+    return new Builder().forRowData(input);\n+  }\n+\n+  public static class Builder {\n+    private DataStream<Row> rowInput = null;\n+    private DataStream<RowData> rowDataInput = null;\n+    private TableLoader tableLoader;\n+    private Configuration hadoopConf;\n+    private Table table;\n+    private TableSchema tableSchema;\n+\n+    private Builder forRow(DataStream<Row> newRowInput) {\n+      this.rowInput = newRowInput;\n+      return this;\n+    }\n+\n+    private Builder forRowData(DataStream<RowData> newRowDataInput) {\n+      this.rowDataInput = newRowDataInput;\n+      return this;\n+    }\n+\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    public Builder tableLoader(TableLoader newTableLoader) {\n+      this.tableLoader = newTableLoader;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(Configuration newHadoopConf) {\n+      this.hadoopConf = newHadoopConf;\n+      return this;\n+    }\n+\n+    public Builder tableSchema(TableSchema newTableSchema) {\n+      this.tableSchema = newTableSchema;\n+      return this;\n+    }\n+\n+    private DataStream<RowData> convert() {\n+      Preconditions.checkArgument(rowInput != null, \"The DataStream<Row> to convert shouldn't be null\");\n+\n+      RowType rowType;\n+      DataType[] fieldDataTypes;\n+      if (tableSchema != null) {\n+        rowType = (RowType) tableSchema.toRowDataType().getLogicalType();\n+        fieldDataTypes = tableSchema.getFieldDataTypes();\n+      } else {\n+        rowType = FlinkSchemaUtil.convert(table.schema());\n+        fieldDataTypes = TypeConversions.fromLogicalToDataType(rowType.getChildren().toArray(new LogicalType[0]));\n+      }\n+\n+      DataFormatConverters.RowConverter rowConverter = new DataFormatConverters.RowConverter(fieldDataTypes);\n+\n+      return rowInput.map(rowConverter::toInternal, RowDataTypeInfo.of(rowType));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public DataStreamSink<RowData> build() {\n+      Preconditions.checkArgument(rowInput != null || rowDataInput != null,\n+          \"Should initialize input DataStream first with DataStream<Row> or DataStream<RowData>\");", "originalCommit": "fd6a22e9d233952fac0bf838d22cf97470c469e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc5ODU3NQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478798575", "bodyText": "OK, let me make those more clear.", "author": "openinx", "createdAt": "2020-08-28T02:38:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3MDE1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3MDM2NA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478770364", "bodyText": "The preconditions in build ensure this is never the case. I'm fine keeping the check (up to you) but it seems odd to have it in a private method. Also, this relies on the check that table is not null when tableSchema is null.", "author": "rdblue", "createdAt": "2020-08-28T00:46:42Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+public class FlinkSink {\n+\n+  private static final String ICEBERG_STREAM_WRITER_NAME = IcebergStreamWriter.class.getSimpleName();\n+  private static final String ICEBERG_FILES_COMMITTER_NAME = IcebergFilesCommitter.class.getSimpleName();\n+\n+  private FlinkSink() {\n+  }\n+\n+  public static Builder forRow(DataStream<Row> input) {\n+    return new Builder().forRow(input);\n+  }\n+\n+  public static Builder forRowData(DataStream<RowData> input) {\n+    return new Builder().forRowData(input);\n+  }\n+\n+  public static class Builder {\n+    private DataStream<Row> rowInput = null;\n+    private DataStream<RowData> rowDataInput = null;\n+    private TableLoader tableLoader;\n+    private Configuration hadoopConf;\n+    private Table table;\n+    private TableSchema tableSchema;\n+\n+    private Builder forRow(DataStream<Row> newRowInput) {\n+      this.rowInput = newRowInput;\n+      return this;\n+    }\n+\n+    private Builder forRowData(DataStream<RowData> newRowDataInput) {\n+      this.rowDataInput = newRowDataInput;\n+      return this;\n+    }\n+\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    public Builder tableLoader(TableLoader newTableLoader) {\n+      this.tableLoader = newTableLoader;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(Configuration newHadoopConf) {\n+      this.hadoopConf = newHadoopConf;\n+      return this;\n+    }\n+\n+    public Builder tableSchema(TableSchema newTableSchema) {\n+      this.tableSchema = newTableSchema;\n+      return this;\n+    }\n+\n+    private DataStream<RowData> convert() {\n+      Preconditions.checkArgument(rowInput != null, \"The DataStream<Row> to convert shouldn't be null\");", "originalCommit": "fd6a22e9d233952fac0bf838d22cf97470c469e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc5NTAwMw==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478795003", "bodyText": "I think it could be removed now, because we've checked the nullable in the Builder#build.", "author": "openinx", "createdAt": "2020-08-28T02:24:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3MDM2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3MjQxMg==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478772412", "bodyText": "I think in the Netflix version, we track old checkpoint state by writing a manifest file and appending that manifest to the table. The advantage is that state is really small so the job can be blocked from committing for a long time (days) and can recover when connectivity to commit has been restored.\nWe use this to handle resilience when AWS regions can't talk to one another and our metastore is in a single region.", "author": "rdblue", "createdAt": "2020-08-28T00:55:01Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();", "originalCommit": "fd6a22e9d233952fac0bf838d22cf97470c469e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODgxNzE3Mw==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478817173", "bodyText": "+1 for this approach. There may be thousands of datafiles in a checkpoint for partitioned table.\n\nIn snapshotState, serialize files to Flink state VS. serialize files to manifest file, they should cost about the same.\nIn notifyCheckpointComplete, just commit a manifest file has better performance instead of commit data files.\nCan reduce the size of States, which are actually put into the memory of the job manager.\n\nAlthough it is rare that checkpoint fails for a long time, it can improve the robustness of the system.\nBut, this approach does lead to more code complexity.", "author": "JingsongLi", "createdAt": "2020-08-28T03:57:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3MjQxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODgxODM4NQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478818385", "bodyText": "Yeah,  I saw that it will archive all the finished data files into a manifest file and maintain only the manifest files in state backend. (https://github.com/apache/iceberg/pull/856/files#diff-0245903e25b2cace005c52b2c7b130bbR372) .   For my understanding, that's a great point to handle the case that we fail to commit to iceberg table frequently (because we'v buffered all the data files in the manifest files rather than flink state backend),  but I'm no sure whether it is a common case for most of users,  the commit failure should be easily  found if they separated the data files regions from the metastore region,  others also use this way to maitain their data and meta ?  or their states  are enough to maintain those uncommitted data files ?\nIf the commit failure is a common case,  I'd happy to contribute to make those data files buffered in a manifest file.   For now,  I think we can make the simple state way go firstly.  Does that make sense ?", "author": "openinx", "createdAt": "2020-08-28T04:03:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3MjQxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQzMjk0OA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479432948", "bodyText": "I completely agree that we should not block adding the sink until this is done. I just wanted to note that this is a good place to put some extra effort. I think that multi-region deployments are common and this can really help. We don't keep Kafka data for a long time, so it is important to us that Flink continues to consume Kafka data to eventually commit to the table, even if it can't commit the manifests right away.", "author": "rdblue", "createdAt": "2020-08-28T17:11:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3MjQxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQzNDQ1OA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479434458", "bodyText": "@stevenzwu ^^", "author": "rdblue", "createdAt": "2020-08-28T17:14:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3MjQxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ1NzEwNA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479457104", "bodyText": "Just to provide more context on why we need it. Our data warehouses (metastore) only lives in us-east-1 region, while Flink streaming jobs can run in 3 regions (us-east-1, us-west-2, and eu-west-1). Transient commits failures happen from time to time, which aren't big concerns. We are more concerned about extended outages (like a day) for whatever reason (us-east-1 outage, cross-region network issue, metastore service outage).  For high-parallelism or event time partitioned tables, there could be thousands or tens of thousands of files per checkpoint interval. This manifest approach allows the Flink jobs to handle those extended outages better.\nFlink operator list state can't handle large state well. I vaguely remember limit is 1 or 2 GBs. And it can get pretty slow when the list is large.", "author": "stevenzwu", "createdAt": "2020-08-28T18:01:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3MjQxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTg2NTQxMA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479865410", "bodyText": "The manifest approach sounds a good improvement to me, it's  a todo issue in the next improvement #1403.", "author": "openinx", "createdAt": "2020-08-31T03:06:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3MjQxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3MzQwMQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478773401", "bodyText": "Does this need to be closed?\nhttps://github.com/apache/iceberg/pull/1346/files#r474605890", "author": "rdblue", "createdAt": "2020-08-28T00:57:41Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+  // It will have an unique identifier for one job.\n+  private transient String flinkJobId;\n+  private transient Table table;\n+  private transient long maxCommittedCheckpointId;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(TableLoader tableLoader, Configuration hadoopConf) {\n+    this.tableLoader = tableLoader;\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    flinkJobId = getContainingTask().getEnvironment().getJobID().toString();\n+\n+    // Open the table loader and load the table.\n+    tableLoader.open(hadoopConf.get());", "originalCommit": "fd6a22e9d233952fac0bf838d22cf97470c469e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3NTEzMg==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478775132", "bodyText": "Nevermind, looks like this is done in dispose() -- here's a good example of where prefixing with this. would be more clear.", "author": "rdblue", "createdAt": "2020-08-28T01:04:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3MzQwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3NDA3OQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478774079", "bodyText": "Minor: I prefer to prefix field names with this. when setting instance fields, so it is obvious that it is not a local variable.", "author": "rdblue", "createdAt": "2020-08-28T00:59:55Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+  // It will have an unique identifier for one job.\n+  private transient String flinkJobId;\n+  private transient Table table;\n+  private transient long maxCommittedCheckpointId;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(TableLoader tableLoader, Configuration hadoopConf) {\n+    this.tableLoader = tableLoader;\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    flinkJobId = getContainingTask().getEnvironment().getJobID().toString();\n+\n+    // Open the table loader and load the table.\n+    tableLoader.open(hadoopConf.get());\n+    table = tableLoader.loadTable();\n+    maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;", "originalCommit": "fd6a22e9d233952fac0bf838d22cf97470c469e9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3NTQ5Mg==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478775492", "bodyText": "What does this do?", "author": "rdblue", "createdAt": "2020-08-28T01:05:38Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriter.java", "diffHunk": "@@ -34,15 +35,16 @@\n   private static final long serialVersionUID = 1L;\n \n   private final String fullTableName;\n+  private final TaskWriterFactory<T> taskWriterFactory;\n \n-  private transient TaskWriterFactory<T> taskWriterFactory;\n   private transient TaskWriter<T> writer;\n   private transient int subTaskId;\n   private transient int attemptId;\n \n   IcebergStreamWriter(String fullTableName, TaskWriterFactory<T> taskWriterFactory) {\n     this.fullTableName = fullTableName;\n     this.taskWriterFactory = taskWriterFactory;\n+    setChainingStrategy(ChainingStrategy.ALWAYS);", "originalCommit": "fd6a22e9d233952fac0bf838d22cf97470c469e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc5ODU0OQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478798549", "bodyText": "The chaining strategy definition:\n * Defines the chaining scheme for the operator. When an operator is chained to the\n * predecessor, it means that they run in the same thread. They become one operator\n * consisting of multiple steps.\n *\n * <p>The default value used by the StreamOperator is {@link #HEAD}, which means that\n * the operator is not chained to its predecessor. Most operators override this with\n * {@link #ALWAYS}, meaning they will be chained to predecessors whenever possible.\n\nWe should also set the chaining strategy of the writer, so as to avoid the network crossing, which will bring additional serialization cost.", "author": "JingsongLi", "createdAt": "2020-08-28T02:38:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3NTQ5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODgwMjE2NQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r478802165", "bodyText": "Copied the javadoc from ChainingStrategy here, it's used for chaining the operators to be co-located on the same thread for better performance.\n/**\n * Defines the chaining scheme for the operator. When an operator is chained to the\n * predecessor, it means that they run in the same thread. They become one operator\n * consisting of multiple steps.\n *\n * <p>The default value used by the StreamOperator is {@link #HEAD}, which means that\n * the operator is not chained to its predecessor. Most operators override this with\n * {@link #ALWAYS}, meaning they will be chained to predecessors whenever possible.\n */\n@PublicEvolving\npublic enum ChainingStrategy {\n\n\t/**\n\t * Operators will be eagerly chained whenever possible.\n\t *\n\t * <p>To optimize performance, it is generally a good practice to allow maximal\n\t * chaining and increase operator parallelism.\n\t */\n\tALWAYS,\n\n\t/**\n\t * The operator will not be chained to the preceding or succeeding operators.\n\t */\n\tNEVER,\n\n\t/**\n\t * The operator will not be chained to the predecessor, but successors may chain to this\n\t * operator.\n\t */\n\tHEAD\n}", "author": "openinx", "createdAt": "2020-08-28T02:54:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3NTQ5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQzMTE2Mg==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479431162", "bodyText": "Thanks! Nice to learn more about how Flink works.", "author": "rdblue", "createdAt": "2020-08-28T17:07:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc3NTQ5Mg=="}], "type": "inlineReview"}, {"oid": "78a0d362a21faf912496b1bb846ea5d435deb3a5", "url": "https://github.com/apache/iceberg/commit/78a0d362a21faf912496b1bb846ea5d435deb3a5", "message": "Addressing comments from Ryan", "committedDate": "2020-08-28T02:47:22Z", "type": "commit"}, {"oid": "65076d72d65ebadce31c5ac920460c9e0cd1ea64", "url": "https://github.com/apache/iceberg/commit/65076d72d65ebadce31c5ac920460c9e0cd1ea64", "message": "Minor changes: removing stale comments etc.", "committedDate": "2020-08-28T06:28:42Z", "type": "commit"}, {"oid": "675977b47d3702d40bc716a520f8cac49465e73c", "url": "https://github.com/apache/iceberg/commit/675977b47d3702d40bc716a520f8cac49465e73c", "message": "Refactor the FlinkSink API and provide detailed javadoc.", "committedDate": "2020-08-28T07:11:21Z", "type": "commit"}, {"oid": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871", "url": "https://github.com/apache/iceberg/commit/e674ed7182289f3c6132c0ca2e76f0fd2d0b6871", "message": "Fix the broken unit tests", "committedDate": "2020-08-28T10:02:30Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQzNTkxMw==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479435913", "bodyText": "Minor: it seems odd to provide the loader and the table. Couldn't the loader be called in the sink builder?", "author": "rdblue", "createdAt": "2020-08-28T17:17:14Z", "path": "flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSink.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.util.FiniteTestSource;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.flink.SimpleDataUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkIcebergSink extends AbstractTestBase {\n+  private static final Configuration CONF = new Configuration();\n+  private static final TypeInformation<Row> ROW_TYPE_INFO = new RowTypeInfo(\n+      SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n+      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+\n+  private String tablePath;\n+  private Table table;\n+  private StreamExecutionEnvironment env;\n+  private TableLoader tableLoader;\n+\n+  private final FileFormat format;\n+  private final int parallelism;\n+  private final boolean partitioned;\n+\n+  @Parameterized.Parameters(name = \"format={0}, parallelism = {1}, partitioned = {2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"avro\", 1, true},\n+        new Object[] {\"avro\", 1, false},\n+        new Object[] {\"avro\", 2, true},\n+        new Object[] {\"avro\", 2, false},\n+        new Object[] {\"orc\", 1, true},\n+        new Object[] {\"orc\", 1, false},\n+        new Object[] {\"orc\", 2, true},\n+        new Object[] {\"orc\", 2, false},\n+        new Object[] {\"parquet\", 1, true},\n+        new Object[] {\"parquet\", 1, false},\n+        new Object[] {\"parquet\", 2, true},\n+        new Object[] {\"parquet\", 2, false}\n+    };\n+  }\n+\n+  public TestFlinkIcebergSink(String format, int parallelism, boolean partitioned) {\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n+    this.partitioned = partitioned;\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File folder = tempFolder.newFolder();\n+    String warehouse = folder.getAbsolutePath();\n+\n+    tablePath = warehouse.concat(\"/test\");\n+    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdir());\n+\n+    Map<String, String> props = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    table = SimpleDataUtil.createTable(tablePath, props, partitioned);\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment()\n+        .enableCheckpointing(100)\n+        .setParallelism(parallelism)\n+        .setMaxParallelism(parallelism);\n+\n+    tableLoader = TableLoader.fromHadoopTable(tablePath);\n+  }\n+\n+  private List<RowData> convertToRowData(List<Row> rows) {\n+    return rows.stream().map(CONVERTER::toInternal).collect(Collectors.toList());\n+  }\n+\n+  @Test\n+  public void testWriteRowData() throws Exception {\n+    List<Row> rows = Lists.newArrayList(\n+        Row.of(1, \"hello\"),\n+        Row.of(2, \"world\"),\n+        Row.of(3, \"foo\")\n+    );\n+    DataStream<RowData> dataStream = env.addSource(new FiniteTestSource<>(rows), ROW_TYPE_INFO)\n+        .map(CONVERTER::toInternal, RowDataTypeInfo.of(SimpleDataUtil.ROW_TYPE));\n+\n+    FlinkSink.forRowData(dataStream)\n+        .table(table)\n+        .tableLoader(tableLoader)", "originalCommit": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ0ODkyOA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479448928", "bodyText": "there can be multiple Iceberg sinks in the same job. we probably should add the table identifier string suffix to make operator name and id unique. We have a unique sinkName within a job and we add the sinkName suffix for operator name.", "author": "stevenzwu", "createdAt": "2020-08-28T17:44:15Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -0,0 +1,220 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+public class FlinkSink {\n+\n+  private static final String ICEBERG_STREAM_WRITER_NAME = IcebergStreamWriter.class.getSimpleName();\n+  private static final String ICEBERG_FILES_COMMITTER_NAME = IcebergFilesCommitter.class.getSimpleName();\n+\n+  private FlinkSink() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from generic input data stream into iceberg table. We use\n+   * {@link RowData} inside the sink connector, so users need to provide a mapper function and a\n+   * {@link TypeInformation} to convert those generic records to a RowData DataStream.\n+   *\n+   * @param input      the generic source input data stream.\n+   * @param mapper     function to convert the generic data to {@link RowData}\n+   * @param outputType to define the {@link TypeInformation} for the input data.\n+   * @param <T>        the data type of records.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static <T> Builder builderFor(DataStream<T> input,\n+                                       MapFunction<T, RowData> mapper,\n+                                       TypeInformation<RowData> outputType) {\n+    DataStream<RowData> dataStream = input.map(mapper, outputType);\n+    return forRowData(dataStream);\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from input data stream with {@link Row}s into iceberg table. We use\n+   * {@link RowData} inside the sink connector, so users need to provide a {@link TableSchema} for builder to convert\n+   * those {@link Row}s to a {@link RowData} DataStream.\n+   *\n+   * @param input       the source input data stream with {@link Row}s.\n+   * @param tableSchema defines the {@link TypeInformation} for input data.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forRow(DataStream<Row> input, TableSchema tableSchema) {\n+    RowType rowType = (RowType) tableSchema.toRowDataType().getLogicalType();\n+    DataType[] fieldDataTypes = tableSchema.getFieldDataTypes();\n+\n+    DataFormatConverters.RowConverter rowConverter = new DataFormatConverters.RowConverter(fieldDataTypes);\n+    return builderFor(input, rowConverter::toInternal, RowDataTypeInfo.of(rowType))\n+        .tableSchema(tableSchema);\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from input data stream with {@link RowData}s into iceberg table.\n+   *\n+   * @param input the source input data stream with {@link RowData}s.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forRowData(DataStream<RowData> input) {\n+    return new Builder().forRowData(input);\n+  }\n+\n+  public static class Builder {\n+    private DataStream<RowData> rowDataInput = null;\n+    private TableLoader tableLoader;\n+    private Configuration hadoopConf;\n+    private Table table;\n+    private TableSchema tableSchema;\n+\n+    private Builder() {\n+    }\n+\n+    private Builder forRowData(DataStream<RowData> newRowDataInput) {\n+      this.rowDataInput = newRowDataInput;\n+      return this;\n+    }\n+\n+    /**\n+     * This iceberg {@link Table} instance is used for initializing {@link IcebergStreamWriter} which will write all\n+     * the records into {@link DataFile}s and emit them to downstream operator. Providing a table would avoid so many\n+     * table loading from each separate task.\n+     *\n+     * @param newTable the loaded iceberg table instance.\n+     * @return {@link Builder} to connect the iceberg table.\n+     */\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    /**\n+     * The table loader is used for loading tables in {@link IcebergFilesCommitter} lazily, we need this loader because\n+     * {@link Table} is not serializable and could not just use the loaded table from Builder#table in the remote task\n+     * manager.\n+     *\n+     * @param newTableLoader to load iceberg table inside tasks.\n+     * @return {@link Builder} to connect the iceberg table.\n+     */\n+    public Builder tableLoader(TableLoader newTableLoader) {\n+      this.tableLoader = newTableLoader;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(Configuration newHadoopConf) {\n+      this.hadoopConf = newHadoopConf;\n+      return this;\n+    }\n+\n+    public Builder tableSchema(TableSchema newTableSchema) {\n+      this.tableSchema = newTableSchema;\n+      return this;\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public DataStreamSink<RowData> build() {\n+      Preconditions.checkArgument(rowDataInput != null,\n+          \"Please use forRowData() to initialize the input DataStream.\");\n+      Preconditions.checkNotNull(table, \"Table shouldn't be null\");\n+      Preconditions.checkNotNull(tableLoader, \"Table loader shouldn't be null\");\n+      Preconditions.checkNotNull(hadoopConf, \"Hadoop configuration shouldn't be null\");\n+\n+      IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, tableSchema);\n+      IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf);\n+\n+      DataStream<Void> returnStream = rowDataInput\n+          .transform(ICEBERG_STREAM_WRITER_NAME, TypeInformation.of(DataFile.class), streamWriter)", "originalCommit": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTg2MjIwOA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479862208", "bodyText": "That sounds good to me,   it's necessary to support multiple iceberg sinks in the same job, I will open an issue and provide a patch with unit tests to address this thing.", "author": "openinx", "createdAt": "2020-08-31T02:51:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ0ODkyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ0OTU4OQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479449589", "bodyText": "committer is a stateful operator, we should probably explicitly set uid.", "author": "stevenzwu", "createdAt": "2020-08-28T17:45:37Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -0,0 +1,220 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+public class FlinkSink {\n+\n+  private static final String ICEBERG_STREAM_WRITER_NAME = IcebergStreamWriter.class.getSimpleName();\n+  private static final String ICEBERG_FILES_COMMITTER_NAME = IcebergFilesCommitter.class.getSimpleName();\n+\n+  private FlinkSink() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from generic input data stream into iceberg table. We use\n+   * {@link RowData} inside the sink connector, so users need to provide a mapper function and a\n+   * {@link TypeInformation} to convert those generic records to a RowData DataStream.\n+   *\n+   * @param input      the generic source input data stream.\n+   * @param mapper     function to convert the generic data to {@link RowData}\n+   * @param outputType to define the {@link TypeInformation} for the input data.\n+   * @param <T>        the data type of records.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static <T> Builder builderFor(DataStream<T> input,\n+                                       MapFunction<T, RowData> mapper,\n+                                       TypeInformation<RowData> outputType) {\n+    DataStream<RowData> dataStream = input.map(mapper, outputType);\n+    return forRowData(dataStream);\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from input data stream with {@link Row}s into iceberg table. We use\n+   * {@link RowData} inside the sink connector, so users need to provide a {@link TableSchema} for builder to convert\n+   * those {@link Row}s to a {@link RowData} DataStream.\n+   *\n+   * @param input       the source input data stream with {@link Row}s.\n+   * @param tableSchema defines the {@link TypeInformation} for input data.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forRow(DataStream<Row> input, TableSchema tableSchema) {\n+    RowType rowType = (RowType) tableSchema.toRowDataType().getLogicalType();\n+    DataType[] fieldDataTypes = tableSchema.getFieldDataTypes();\n+\n+    DataFormatConverters.RowConverter rowConverter = new DataFormatConverters.RowConverter(fieldDataTypes);\n+    return builderFor(input, rowConverter::toInternal, RowDataTypeInfo.of(rowType))\n+        .tableSchema(tableSchema);\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from input data stream with {@link RowData}s into iceberg table.\n+   *\n+   * @param input the source input data stream with {@link RowData}s.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forRowData(DataStream<RowData> input) {\n+    return new Builder().forRowData(input);\n+  }\n+\n+  public static class Builder {\n+    private DataStream<RowData> rowDataInput = null;\n+    private TableLoader tableLoader;\n+    private Configuration hadoopConf;\n+    private Table table;\n+    private TableSchema tableSchema;\n+\n+    private Builder() {\n+    }\n+\n+    private Builder forRowData(DataStream<RowData> newRowDataInput) {\n+      this.rowDataInput = newRowDataInput;\n+      return this;\n+    }\n+\n+    /**\n+     * This iceberg {@link Table} instance is used for initializing {@link IcebergStreamWriter} which will write all\n+     * the records into {@link DataFile}s and emit them to downstream operator. Providing a table would avoid so many\n+     * table loading from each separate task.\n+     *\n+     * @param newTable the loaded iceberg table instance.\n+     * @return {@link Builder} to connect the iceberg table.\n+     */\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    /**\n+     * The table loader is used for loading tables in {@link IcebergFilesCommitter} lazily, we need this loader because\n+     * {@link Table} is not serializable and could not just use the loaded table from Builder#table in the remote task\n+     * manager.\n+     *\n+     * @param newTableLoader to load iceberg table inside tasks.\n+     * @return {@link Builder} to connect the iceberg table.\n+     */\n+    public Builder tableLoader(TableLoader newTableLoader) {\n+      this.tableLoader = newTableLoader;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(Configuration newHadoopConf) {\n+      this.hadoopConf = newHadoopConf;\n+      return this;\n+    }\n+\n+    public Builder tableSchema(TableSchema newTableSchema) {\n+      this.tableSchema = newTableSchema;\n+      return this;\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public DataStreamSink<RowData> build() {\n+      Preconditions.checkArgument(rowDataInput != null,\n+          \"Please use forRowData() to initialize the input DataStream.\");\n+      Preconditions.checkNotNull(table, \"Table shouldn't be null\");\n+      Preconditions.checkNotNull(tableLoader, \"Table loader shouldn't be null\");\n+      Preconditions.checkNotNull(hadoopConf, \"Hadoop configuration shouldn't be null\");\n+\n+      IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, tableSchema);\n+      IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf);\n+\n+      DataStream<Void> returnStream = rowDataInput\n+          .transform(ICEBERG_STREAM_WRITER_NAME, TypeInformation.of(DataFile.class), streamWriter)\n+          .setParallelism(rowDataInput.getParallelism())\n+          .transform(ICEBERG_FILES_COMMITTER_NAME, Types.VOID, filesCommitter)", "originalCommit": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0OTc3OQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479549779", "bodyText": "never mind. saw earlier comment that this is a todo item.", "author": "stevenzwu", "createdAt": "2020-08-28T21:51:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ0OTU4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ1MDQxNg==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479450416", "bodyText": "curious about the reason why don't we make the committer a sink function and instead add a dummy DiscardingSink.\nConceptually, this writer-committer combo is the reverse/mirror of split enumerator-reader FLIP-27 source interface. It will be nice to run committer on jobmanager (similar to enumerator). This way, Iceberg sink won't change the nature of the embarrassingly-parallel DAG.", "author": "stevenzwu", "createdAt": "2020-08-28T17:47:25Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -0,0 +1,220 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+public class FlinkSink {\n+\n+  private static final String ICEBERG_STREAM_WRITER_NAME = IcebergStreamWriter.class.getSimpleName();\n+  private static final String ICEBERG_FILES_COMMITTER_NAME = IcebergFilesCommitter.class.getSimpleName();\n+\n+  private FlinkSink() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from generic input data stream into iceberg table. We use\n+   * {@link RowData} inside the sink connector, so users need to provide a mapper function and a\n+   * {@link TypeInformation} to convert those generic records to a RowData DataStream.\n+   *\n+   * @param input      the generic source input data stream.\n+   * @param mapper     function to convert the generic data to {@link RowData}\n+   * @param outputType to define the {@link TypeInformation} for the input data.\n+   * @param <T>        the data type of records.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static <T> Builder builderFor(DataStream<T> input,\n+                                       MapFunction<T, RowData> mapper,\n+                                       TypeInformation<RowData> outputType) {\n+    DataStream<RowData> dataStream = input.map(mapper, outputType);\n+    return forRowData(dataStream);\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from input data stream with {@link Row}s into iceberg table. We use\n+   * {@link RowData} inside the sink connector, so users need to provide a {@link TableSchema} for builder to convert\n+   * those {@link Row}s to a {@link RowData} DataStream.\n+   *\n+   * @param input       the source input data stream with {@link Row}s.\n+   * @param tableSchema defines the {@link TypeInformation} for input data.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forRow(DataStream<Row> input, TableSchema tableSchema) {\n+    RowType rowType = (RowType) tableSchema.toRowDataType().getLogicalType();\n+    DataType[] fieldDataTypes = tableSchema.getFieldDataTypes();\n+\n+    DataFormatConverters.RowConverter rowConverter = new DataFormatConverters.RowConverter(fieldDataTypes);\n+    return builderFor(input, rowConverter::toInternal, RowDataTypeInfo.of(rowType))\n+        .tableSchema(tableSchema);\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from input data stream with {@link RowData}s into iceberg table.\n+   *\n+   * @param input the source input data stream with {@link RowData}s.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forRowData(DataStream<RowData> input) {\n+    return new Builder().forRowData(input);\n+  }\n+\n+  public static class Builder {\n+    private DataStream<RowData> rowDataInput = null;\n+    private TableLoader tableLoader;\n+    private Configuration hadoopConf;\n+    private Table table;\n+    private TableSchema tableSchema;\n+\n+    private Builder() {\n+    }\n+\n+    private Builder forRowData(DataStream<RowData> newRowDataInput) {\n+      this.rowDataInput = newRowDataInput;\n+      return this;\n+    }\n+\n+    /**\n+     * This iceberg {@link Table} instance is used for initializing {@link IcebergStreamWriter} which will write all\n+     * the records into {@link DataFile}s and emit them to downstream operator. Providing a table would avoid so many\n+     * table loading from each separate task.\n+     *\n+     * @param newTable the loaded iceberg table instance.\n+     * @return {@link Builder} to connect the iceberg table.\n+     */\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    /**\n+     * The table loader is used for loading tables in {@link IcebergFilesCommitter} lazily, we need this loader because\n+     * {@link Table} is not serializable and could not just use the loaded table from Builder#table in the remote task\n+     * manager.\n+     *\n+     * @param newTableLoader to load iceberg table inside tasks.\n+     * @return {@link Builder} to connect the iceberg table.\n+     */\n+    public Builder tableLoader(TableLoader newTableLoader) {\n+      this.tableLoader = newTableLoader;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(Configuration newHadoopConf) {\n+      this.hadoopConf = newHadoopConf;\n+      return this;\n+    }\n+\n+    public Builder tableSchema(TableSchema newTableSchema) {\n+      this.tableSchema = newTableSchema;\n+      return this;\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public DataStreamSink<RowData> build() {\n+      Preconditions.checkArgument(rowDataInput != null,\n+          \"Please use forRowData() to initialize the input DataStream.\");\n+      Preconditions.checkNotNull(table, \"Table shouldn't be null\");\n+      Preconditions.checkNotNull(tableLoader, \"Table loader shouldn't be null\");\n+      Preconditions.checkNotNull(hadoopConf, \"Hadoop configuration shouldn't be null\");\n+\n+      IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, tableSchema);\n+      IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf);\n+\n+      DataStream<Void> returnStream = rowDataInput\n+          .transform(ICEBERG_STREAM_WRITER_NAME, TypeInformation.of(DataFile.class), streamWriter)\n+          .setParallelism(rowDataInput.getParallelism())\n+          .transform(ICEBERG_FILES_COMMITTER_NAME, Types.VOID, filesCommitter)\n+          .setParallelism(1)\n+          .setMaxParallelism(1);\n+\n+      return returnStream.addSink(new DiscardingSink())", "originalCommit": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTg2MTcxNQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479861715", "bodyText": "I think this would answer your question about why did we use an operator rather than sink function.", "author": "openinx", "createdAt": "2020-08-31T02:49:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ1MDQxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDIwODM1NQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r480208355", "bodyText": "@openinx thx. that does answer my question.\nStill adding a DiscardingSink may confuse users. It seems that we really need a unified/improved sink interface (similar to FLIP-27) to supported bounded input jobs in sink function.\nWhat about any other Flink sinks used by bounded streaming/batch job? Do they all have to go through this model?", "author": "stevenzwu", "createdAt": "2020-08-31T15:30:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ1MDQxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDYyNTA0Mw==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r480625043", "bodyText": "There're three cases: 1>  unbounded streaming job; 2> bounded streaming job;  3> batch job.  If users only need the unbounded streaming ability, then only need to implement the SinkFunction,  otherwise if need both unbounded streaming and bounded streaming  ability,  then we need to extend/implement the AbstractStreamOperator & BoundedOneInput  and add the DiscardingSink to the tail.  If want to batch ability, then  need to provide an OutputFormat implementation.  In future flink,  we will unify the case#2 and case#3 in one sink interface,  but for now we have to implement separately for the bounded streaming and batch cases.\nFlink hive connector is a good case, which have support case1, case2 and case3.  It also use the similar way to the current iceberg sink connector now.", "author": "openinx", "createdAt": "2020-09-01T02:33:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ1MDQxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTI3NTUwMg==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r481275502", "bodyText": "fair enough. we can't solve this problem until Flink improved sink interface.", "author": "stevenzwu", "createdAt": "2020-09-01T16:24:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ1MDQxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ1MTMzMQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479451331", "bodyText": "this is a good default value for writer parallelism. we have users who want to explicitly control the writer parallelism to control the number of written files. in the future, we may want to allow user to set parallelism in the builder.", "author": "stevenzwu", "createdAt": "2020-08-28T17:49:25Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -0,0 +1,220 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+public class FlinkSink {\n+\n+  private static final String ICEBERG_STREAM_WRITER_NAME = IcebergStreamWriter.class.getSimpleName();\n+  private static final String ICEBERG_FILES_COMMITTER_NAME = IcebergFilesCommitter.class.getSimpleName();\n+\n+  private FlinkSink() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from generic input data stream into iceberg table. We use\n+   * {@link RowData} inside the sink connector, so users need to provide a mapper function and a\n+   * {@link TypeInformation} to convert those generic records to a RowData DataStream.\n+   *\n+   * @param input      the generic source input data stream.\n+   * @param mapper     function to convert the generic data to {@link RowData}\n+   * @param outputType to define the {@link TypeInformation} for the input data.\n+   * @param <T>        the data type of records.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static <T> Builder builderFor(DataStream<T> input,\n+                                       MapFunction<T, RowData> mapper,\n+                                       TypeInformation<RowData> outputType) {\n+    DataStream<RowData> dataStream = input.map(mapper, outputType);\n+    return forRowData(dataStream);\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from input data stream with {@link Row}s into iceberg table. We use\n+   * {@link RowData} inside the sink connector, so users need to provide a {@link TableSchema} for builder to convert\n+   * those {@link Row}s to a {@link RowData} DataStream.\n+   *\n+   * @param input       the source input data stream with {@link Row}s.\n+   * @param tableSchema defines the {@link TypeInformation} for input data.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forRow(DataStream<Row> input, TableSchema tableSchema) {\n+    RowType rowType = (RowType) tableSchema.toRowDataType().getLogicalType();\n+    DataType[] fieldDataTypes = tableSchema.getFieldDataTypes();\n+\n+    DataFormatConverters.RowConverter rowConverter = new DataFormatConverters.RowConverter(fieldDataTypes);\n+    return builderFor(input, rowConverter::toInternal, RowDataTypeInfo.of(rowType))\n+        .tableSchema(tableSchema);\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to export the data from input data stream with {@link RowData}s into iceberg table.\n+   *\n+   * @param input the source input data stream with {@link RowData}s.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forRowData(DataStream<RowData> input) {\n+    return new Builder().forRowData(input);\n+  }\n+\n+  public static class Builder {\n+    private DataStream<RowData> rowDataInput = null;\n+    private TableLoader tableLoader;\n+    private Configuration hadoopConf;\n+    private Table table;\n+    private TableSchema tableSchema;\n+\n+    private Builder() {\n+    }\n+\n+    private Builder forRowData(DataStream<RowData> newRowDataInput) {\n+      this.rowDataInput = newRowDataInput;\n+      return this;\n+    }\n+\n+    /**\n+     * This iceberg {@link Table} instance is used for initializing {@link IcebergStreamWriter} which will write all\n+     * the records into {@link DataFile}s and emit them to downstream operator. Providing a table would avoid so many\n+     * table loading from each separate task.\n+     *\n+     * @param newTable the loaded iceberg table instance.\n+     * @return {@link Builder} to connect the iceberg table.\n+     */\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    /**\n+     * The table loader is used for loading tables in {@link IcebergFilesCommitter} lazily, we need this loader because\n+     * {@link Table} is not serializable and could not just use the loaded table from Builder#table in the remote task\n+     * manager.\n+     *\n+     * @param newTableLoader to load iceberg table inside tasks.\n+     * @return {@link Builder} to connect the iceberg table.\n+     */\n+    public Builder tableLoader(TableLoader newTableLoader) {\n+      this.tableLoader = newTableLoader;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(Configuration newHadoopConf) {\n+      this.hadoopConf = newHadoopConf;\n+      return this;\n+    }\n+\n+    public Builder tableSchema(TableSchema newTableSchema) {\n+      this.tableSchema = newTableSchema;\n+      return this;\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public DataStreamSink<RowData> build() {\n+      Preconditions.checkArgument(rowDataInput != null,\n+          \"Please use forRowData() to initialize the input DataStream.\");\n+      Preconditions.checkNotNull(table, \"Table shouldn't be null\");\n+      Preconditions.checkNotNull(tableLoader, \"Table loader shouldn't be null\");\n+      Preconditions.checkNotNull(hadoopConf, \"Hadoop configuration shouldn't be null\");\n+\n+      IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, tableSchema);\n+      IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf);\n+\n+      DataStream<Void> returnStream = rowDataInput\n+          .transform(ICEBERG_STREAM_WRITER_NAME, TypeInformation.of(DataFile.class), streamWriter)\n+          .setParallelism(rowDataInput.getParallelism())", "originalCommit": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTg2MzMyMg==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479863322", "bodyText": "Make sense to me,  could be a following issue.", "author": "openinx", "createdAt": "2020-08-31T02:56:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ1MTMzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUxNTQzOA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479515438", "bodyText": "nice. this is the reverse iteration I was looking for. we were just walking through the table.snapshots() iterable.", "author": "stevenzwu", "createdAt": "2020-08-28T20:16:48Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+  // It will have an unique identifier for one job.\n+  private transient String flinkJobId;\n+  private transient Table table;\n+  private transient long maxCommittedCheckpointId;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(TableLoader tableLoader, Configuration hadoopConf) {\n+    this.tableLoader = tableLoader;\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    this.flinkJobId = getContainingTask().getEnvironment().getJobID().toString();\n+\n+    // Open the table loader and load the table.\n+    this.tableLoader.open(hadoopConf.get());\n+    this.table = tableLoader.loadTable();\n+    this.maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n+\n+    this.checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      this.maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);\n+      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n+      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n+      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n+      // the iceberg table.\n+      Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID,\n+          \"There should be an existing iceberg snapshot for current flink job: %s\", flinkJobId);\n+\n+      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n+      // Only keep the uncommitted data files in the cache.\n+      this.dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext context) throws Exception {\n+    super.snapshotState(context);\n+    long checkpointId = context.getCheckpointId();\n+    LOG.info(\"Start to flush snapshot state to state backend, table: {}, checkpointId: {}\", table, checkpointId);\n+\n+    // Update the checkpoint state.\n+    dataFilesPerCheckpoint.put(checkpointId, ImmutableList.copyOf(dataFilesOfCurrentCheckpoint));\n+\n+    // Reset the snapshot state to the latest state.\n+    checkpointsState.clear();\n+    checkpointsState.add(dataFilesPerCheckpoint);\n+\n+    // Clear the local buffer for current checkpoint.\n+    dataFilesOfCurrentCheckpoint.clear();\n+  }\n+\n+  @Override\n+  public void notifyCheckpointComplete(long checkpointId) throws Exception {\n+    super.notifyCheckpointComplete(checkpointId);\n+    // It's possible that we have the following events:\n+    //   1. snapshotState(ckpId);\n+    //   2. snapshotState(ckpId+1);\n+    //   3. notifyCheckpointComplete(ckpId+1);\n+    //   4. notifyCheckpointComplete(ckpId);\n+    // For step#4, we don't need to commit iceberg table again because in step#3 we've committed all the files,\n+    // Besides, we need to maintain the max-committed-checkpoint-id to be increasing.\n+    if (checkpointId > maxCommittedCheckpointId) {\n+      commitUpToCheckpoint(checkpointId);\n+      this.maxCommittedCheckpointId = checkpointId;\n+    }\n+  }\n+\n+  private void commitUpToCheckpoint(long checkpointId) {\n+    NavigableMap<Long, List<DataFile>> pendingFileMap = dataFilesPerCheckpoint.headMap(checkpointId, true);\n+\n+    List<DataFile> pendingDataFiles = Lists.newArrayList();\n+    for (List<DataFile> dataFiles : pendingFileMap.values()) {\n+      pendingDataFiles.addAll(dataFiles);\n+    }\n+\n+    AppendFiles appendFiles = table.newAppend();\n+    pendingDataFiles.forEach(appendFiles::appendFile);\n+    appendFiles.set(MAX_COMMITTED_CHECKPOINT_ID, Long.toString(checkpointId));\n+    appendFiles.set(FLINK_JOB_ID, flinkJobId);\n+    appendFiles.commit();\n+\n+    // Clear the committed data files from dataFilesPerCheckpoint.\n+    pendingFileMap.clear();\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<DataFile> element) {\n+    this.dataFilesOfCurrentCheckpoint.add(element.getValue());\n+  }\n+\n+  @Override\n+  public void endInput() {\n+    // Flush the buffered data files into 'dataFilesPerCheckpoint' firstly.\n+    dataFilesPerCheckpoint.put(Long.MAX_VALUE, ImmutableList.copyOf(dataFilesOfCurrentCheckpoint));\n+    dataFilesOfCurrentCheckpoint.clear();\n+\n+    commitUpToCheckpoint(Long.MAX_VALUE);\n+  }\n+\n+  @Override\n+  public void dispose() throws Exception {\n+    if (tableLoader != null) {\n+      tableLoader.close();\n+    }\n+  }\n+\n+  private static ListStateDescriptor<SortedMap<Long, List<DataFile>>> buildStateDescriptor() {\n+    Comparator<Long> longComparator = Comparators.forType(Types.LongType.get());\n+    // Construct a ListTypeInfo.\n+    ListTypeInfo<DataFile> dataFileListTypeInfo = new ListTypeInfo<>(TypeInformation.of(DataFile.class));\n+    // Construct a SortedMapTypeInfo.\n+    SortedMapTypeInfo<Long, List<DataFile>> sortedMapTypeInfo = new SortedMapTypeInfo<>(\n+        BasicTypeInfo.LONG_TYPE_INFO, dataFileListTypeInfo, longComparator\n+    );\n+    return new ListStateDescriptor<>(\"iceberg-files-committer-state\", sortedMapTypeInfo);\n+  }\n+\n+  static long getMaxCommittedCheckpointId(Table table, String flinkJobId) {", "originalCommit": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0MTM0OQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479541349", "bodyText": "I am wondering if Flink guarantees the serialized execution for notifyCheckpointComplete?", "author": "stevenzwu", "createdAt": "2020-08-28T21:25:19Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+  // It will have an unique identifier for one job.\n+  private transient String flinkJobId;\n+  private transient Table table;\n+  private transient long maxCommittedCheckpointId;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(TableLoader tableLoader, Configuration hadoopConf) {\n+    this.tableLoader = tableLoader;\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    this.flinkJobId = getContainingTask().getEnvironment().getJobID().toString();\n+\n+    // Open the table loader and load the table.\n+    this.tableLoader.open(hadoopConf.get());\n+    this.table = tableLoader.loadTable();\n+    this.maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n+\n+    this.checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      this.maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);\n+      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n+      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n+      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n+      // the iceberg table.\n+      Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID,\n+          \"There should be an existing iceberg snapshot for current flink job: %s\", flinkJobId);\n+\n+      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n+      // Only keep the uncommitted data files in the cache.\n+      this.dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext context) throws Exception {\n+    super.snapshotState(context);\n+    long checkpointId = context.getCheckpointId();\n+    LOG.info(\"Start to flush snapshot state to state backend, table: {}, checkpointId: {}\", table, checkpointId);\n+\n+    // Update the checkpoint state.\n+    dataFilesPerCheckpoint.put(checkpointId, ImmutableList.copyOf(dataFilesOfCurrentCheckpoint));\n+\n+    // Reset the snapshot state to the latest state.\n+    checkpointsState.clear();\n+    checkpointsState.add(dataFilesPerCheckpoint);\n+\n+    // Clear the local buffer for current checkpoint.\n+    dataFilesOfCurrentCheckpoint.clear();\n+  }\n+\n+  @Override\n+  public void notifyCheckpointComplete(long checkpointId) throws Exception {", "originalCommit": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTg3NjQ1OA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479876458", "bodyText": "According to the current implementation, it should be, because task checkpoints are ordered and RPC messages are ordered. But it's better not to rely on this.", "author": "JingsongLi", "createdAt": "2020-08-31T04:03:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0MTM0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDI0NDYzNg==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r480244636", "bodyText": "thx. should we raise a jira to track the task?\n@JingsongLi you are saying that notifyCheckpointComplete and snapshotState are serialized by the same lock/mutex, right? Otherwise, I can see problem with the concurrent checkpoint handling.", "author": "stevenzwu", "createdAt": "2020-08-31T16:27:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0MTM0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU2NDg2Nw==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r480564867", "bodyText": "Yes. You don't need care about thread safety.", "author": "JingsongLi", "createdAt": "2020-09-01T01:39:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0MTM0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0NTk3OQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479545979", "bodyText": "We might have a problem if redeploying the Flink job from external checkpoint. It is a new flinkJobId in this case. maxCommittedCheckpointId will be -1.  As a result, we can commit those committed files again later.\nThe way we do de-dup is to generate a hash for the manifest file path and store the hash in snapshot summary. During restore, we use the hash to de-dup if the manifest file was committed or not.\n        List<String> hashes = new ArrayList<>(flinkManifestFiles.size());\n        AppendFiles appendFiles = transaction.newAppend();\n        for (FlinkManifestFile flinkManifestFile : flinkManifestFiles) {\n          appendFiles.appendManifest(flinkManifestFile);\n          hashes.add(flinkManifestFile.hash());\n        }\n        appendFiles.set(\n            COMMIT_MANIFEST_HASHES_KEY, FlinkManifestFileUtil.hashesListToString(hashes));", "author": "stevenzwu", "createdAt": "2020-08-28T21:39:11Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+  // It will have an unique identifier for one job.\n+  private transient String flinkJobId;\n+  private transient Table table;\n+  private transient long maxCommittedCheckpointId;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(TableLoader tableLoader, Configuration hadoopConf) {\n+    this.tableLoader = tableLoader;\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    this.flinkJobId = getContainingTask().getEnvironment().getJobID().toString();\n+\n+    // Open the table loader and load the table.\n+    this.tableLoader.open(hadoopConf.get());\n+    this.table = tableLoader.loadTable();\n+    this.maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n+\n+    this.checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      this.maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);", "originalCommit": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTg3NDYyMw==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479874623", "bodyText": "It's a great point that we could handle, I think we can attach both the flinkJobId and maxCommittedCheckPointId to the checkpoint state.  when in restoring path,  we read the flinkJobId and maxCommittedCheckpointId from states, and compare the current flink job id with flinkJobId from state.  If the job ids are matched,  then  it is surely be the case that restoring without redploying(case#1), otherwise it's the case you said (case#2).\nFor case#1, the current code should be correct.\nFor case#2, we should use the old flinkJobId from state to parse the maxCommittedCheckpointId in iceberg table, and use that checkpoint id to filter all the committed data files.\nDoes that make sense ?", "author": "openinx", "createdAt": "2020-08-31T03:54:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0NTk3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDIxNDcxMw==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r480214713", "bodyText": "I like the proposed solution. it should work.", "author": "stevenzwu", "createdAt": "2020-08-31T15:40:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0NTk3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU4NTg4OA==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479585888", "bodyText": "In our implementation, we immediately commit any uncommitted files upon restore to avoid waiting for another checkpoint cycle.", "author": "stevenzwu", "createdAt": "2020-08-29T00:45:39Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+  // It will have an unique identifier for one job.\n+  private transient String flinkJobId;\n+  private transient Table table;\n+  private transient long maxCommittedCheckpointId;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(TableLoader tableLoader, Configuration hadoopConf) {\n+    this.tableLoader = tableLoader;\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    this.flinkJobId = getContainingTask().getEnvironment().getJobID().toString();\n+\n+    // Open the table loader and load the table.\n+    this.tableLoader.open(hadoopConf.get());\n+    this.table = tableLoader.loadTable();\n+    this.maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n+\n+    this.checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      this.maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);\n+      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n+      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n+      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n+      // the iceberg table.\n+      Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID,\n+          \"There should be an existing iceberg snapshot for current flink job: %s\", flinkJobId);\n+\n+      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n+      // Only keep the uncommitted data files in the cache.\n+      this.dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));", "originalCommit": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTk1MTEyMg==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479951122", "bodyText": "Committing those uncommitted data files here immediately sounds good to me,  it should won't impact the correctness here.", "author": "openinx", "createdAt": "2020-08-31T07:37:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU4NTg4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDExNzIxNQ==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r480117215", "bodyText": "Considered this issue again,  if redeploying flink job from external checkpoint, then all data files from restoredDataFiles  are from the old flink job, which means its checkpoint id could be range from 1~N.  If we put all those files into dataFilesPerCheckpoint, that will introduce the problem:  it did not align with the checkpoint id with the new flink job,  saying when notifyCheckpointComplete(1) for the new flink job, it won't  commit all the old data files with checkpointId > 1 to iceberg table.  That's incorrect.\nSo it should commit those remaining uncommitted data files to iceberg immediately.", "author": "openinx", "createdAt": "2020-08-31T13:07:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU4NTg4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTI0OTY0Nw==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r481249647", "bodyText": "@openinx if the new job is redeployed from checkpoint N taken by the old job, checkpointId will start from N+1 for the new job. that is my observation of Flink behavior.", "author": "stevenzwu", "createdAt": "2020-09-01T15:55:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU4NTg4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTU4NzAxMg==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r481587012", "bodyText": "OK , checked the flink savepoint code here,  it was designed as you said. https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L1371 .\nThanks for the confirmation,  then I think committing those files in next checkpoint cycle  should also be OK, but there's no harmness for us to commit them immediately.", "author": "openinx", "createdAt": "2020-09-02T03:01:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU4NTg4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU4ODY2Mw==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479588663", "bodyText": "Just want to point out that this will commit if there is zero pending files. For us, we actually still want to commit in this case mainly to update the region watermark info.", "author": "stevenzwu", "createdAt": "2020-08-29T01:09:53Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+  // It will have an unique identifier for one job.\n+  private transient String flinkJobId;\n+  private transient Table table;\n+  private transient long maxCommittedCheckpointId;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(TableLoader tableLoader, Configuration hadoopConf) {\n+    this.tableLoader = tableLoader;\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    this.flinkJobId = getContainingTask().getEnvironment().getJobID().toString();\n+\n+    // Open the table loader and load the table.\n+    this.tableLoader.open(hadoopConf.get());\n+    this.table = tableLoader.loadTable();\n+    this.maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n+\n+    this.checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      this.maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);\n+      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n+      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n+      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n+      // the iceberg table.\n+      Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID,\n+          \"There should be an existing iceberg snapshot for current flink job: %s\", flinkJobId);\n+\n+      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n+      // Only keep the uncommitted data files in the cache.\n+      this.dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext context) throws Exception {\n+    super.snapshotState(context);\n+    long checkpointId = context.getCheckpointId();\n+    LOG.info(\"Start to flush snapshot state to state backend, table: {}, checkpointId: {}\", table, checkpointId);\n+\n+    // Update the checkpoint state.\n+    dataFilesPerCheckpoint.put(checkpointId, ImmutableList.copyOf(dataFilesOfCurrentCheckpoint));\n+\n+    // Reset the snapshot state to the latest state.\n+    checkpointsState.clear();\n+    checkpointsState.add(dataFilesPerCheckpoint);\n+\n+    // Clear the local buffer for current checkpoint.\n+    dataFilesOfCurrentCheckpoint.clear();\n+  }\n+\n+  @Override\n+  public void notifyCheckpointComplete(long checkpointId) throws Exception {\n+    super.notifyCheckpointComplete(checkpointId);\n+    // It's possible that we have the following events:\n+    //   1. snapshotState(ckpId);\n+    //   2. snapshotState(ckpId+1);\n+    //   3. notifyCheckpointComplete(ckpId+1);\n+    //   4. notifyCheckpointComplete(ckpId);\n+    // For step#4, we don't need to commit iceberg table again because in step#3 we've committed all the files,\n+    // Besides, we need to maintain the max-committed-checkpoint-id to be increasing.\n+    if (checkpointId > maxCommittedCheckpointId) {\n+      commitUpToCheckpoint(checkpointId);\n+      this.maxCommittedCheckpointId = checkpointId;\n+    }\n+  }\n+\n+  private void commitUpToCheckpoint(long checkpointId) {", "originalCommit": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTg3NTYxMg==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479875612", "bodyText": "That should be similar to this point from this comment, we actually are doing the logic you said and provided a unit test to address this thing.", "author": "openinx", "createdAt": "2020-08-31T03:58:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU4ODY2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU4OTY2Mw==", "url": "https://github.com/apache/iceberg/pull/1185#discussion_r479589663", "bodyText": "this probably gets complicated when we allow concurrent checkpoints. Committer can receive files from both checkpoint N and N+1. We probably need add the checkpointId in the DataFile. It might make sense to provide FlinkDataFile wrapper so that we can add Flink additional metadata.\nWe have FlinkDataFile in our implementation for transmitting low and high timestamps. Now thinking about this issue. Maybe we can include checkpointId too so that committer can distinguish data files from different checkpoints.\npublic class FlinkDataFile implements Serializable {\n  private final long lowWatermark;\n  private final long highWatermark;\n  private final DataFile dataFile;\n\nThis does imposes additional requirement on the writer. It needs to know the last/next checkpointId.\n\nfor job started without checkpoint, the last checkpointId is 0\nfor job started with checkpoint, now IcebergWriter needs to know the last checkpointId retored.\n\nI couldn't find the checkpointId from the restored context in the initializeState(context) method for either AbstractStreamOperator or RichSinkFunction. It will be nice if it can be exposed.\nAlternatively, we can store the nextCheckpointId in the operator state. However, they also have some problems.\n\noperator list state can't deal with rescale as new subtasks won't get state\noperator union list state is not scalable. Kafka source is suffering the scalability issue with union state.\n\nNote that we may flush file before checkpoint barrier comes. We have two use cases to need the rollover by time and file size.\n\nAs I mentioned in another comment, we have Flink streaming jobs running in 3 regions in AWS and data warehouses live only in us-east-1. There is a backend service monitor files in two other remote regions and lift/copy them back to the us-east-1 home region. S3 cross-region file copy has 5 GB limit. For that reason, we flushes files if the size reaches 4 GB.\nWhen we implement rough ordering for Iceberg source, we need event time alignment. In these cases, we use Kafka broker time as event time since our Iceberg source tries to emulate the rough ordering for Kafka source. One possible solution is to roll over the file when the min and max timestamp reached certain threshold (e.g. 5 minutes).", "author": "stevenzwu", "createdAt": "2020-08-29T01:18:51Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.SortedMap;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ListTypeInfo;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Comparators;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class IcebergFilesCommitter extends AbstractStreamOperator<Void>\n+    implements OneInputStreamOperator<DataFile, Void>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long INITIAL_CHECKPOINT_ID = -1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergFilesCommitter.class);\n+  private static final String FLINK_JOB_ID = \"flink.job-id\";\n+\n+  // The max checkpoint id we've committed to iceberg table. As the flink's checkpoint is always increasing, so we could\n+  // correctly commit all the data files whose checkpoint id is greater than the max committed one to iceberg table, for\n+  // avoiding committing the same data files twice. This id will be attached to iceberg's meta when committing the\n+  // iceberg transaction.\n+  private static final String MAX_COMMITTED_CHECKPOINT_ID = \"flink.max-committed-checkpoint-id\";\n+\n+  // TableLoader to load iceberg table lazily.\n+  private final TableLoader tableLoader;\n+  private final SerializableConfiguration hadoopConf;\n+\n+  // A sorted map to maintain the completed data files for each pending checkpointId (which have not been committed\n+  // to iceberg table). We need a sorted map here because there's possible that few checkpoints snapshot failed, for\n+  // example: the 1st checkpoint have 2 data files <1, <file0, file1>>, the 2st checkpoint have 1 data files\n+  // <2, <file3>>. Snapshot for checkpoint#1 interrupted because of network/disk failure etc, while we don't expect\n+  // any data loss in iceberg table. So we keep the finished files <1, <file0, file1>> in memory and retry to commit\n+  // iceberg table when the next checkpoint happen.\n+  private final NavigableMap<Long, List<DataFile>> dataFilesPerCheckpoint = Maps.newTreeMap();\n+\n+  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // 'dataFilesPerCheckpoint'.\n+  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+\n+  // It will have an unique identifier for one job.\n+  private transient String flinkJobId;\n+  private transient Table table;\n+  private transient long maxCommittedCheckpointId;\n+\n+  // All pending checkpoints states for this function.\n+  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n+  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+\n+  IcebergFilesCommitter(TableLoader tableLoader, Configuration hadoopConf) {\n+    this.tableLoader = tableLoader;\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+    this.flinkJobId = getContainingTask().getEnvironment().getJobID().toString();\n+\n+    // Open the table loader and load the table.\n+    this.tableLoader.open(hadoopConf.get());\n+    this.table = tableLoader.loadTable();\n+    this.maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n+\n+    this.checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    if (context.isRestored()) {\n+      this.maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);\n+      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n+      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n+      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n+      // the iceberg table.\n+      Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID,\n+          \"There should be an existing iceberg snapshot for current flink job: %s\", flinkJobId);\n+\n+      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n+      // Only keep the uncommitted data files in the cache.\n+      this.dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext context) throws Exception {\n+    super.snapshotState(context);\n+    long checkpointId = context.getCheckpointId();\n+    LOG.info(\"Start to flush snapshot state to state backend, table: {}, checkpointId: {}\", table, checkpointId);\n+\n+    // Update the checkpoint state.\n+    dataFilesPerCheckpoint.put(checkpointId, ImmutableList.copyOf(dataFilesOfCurrentCheckpoint));\n+\n+    // Reset the snapshot state to the latest state.\n+    checkpointsState.clear();\n+    checkpointsState.add(dataFilesPerCheckpoint);\n+\n+    // Clear the local buffer for current checkpoint.\n+    dataFilesOfCurrentCheckpoint.clear();\n+  }\n+\n+  @Override\n+  public void notifyCheckpointComplete(long checkpointId) throws Exception {\n+    super.notifyCheckpointComplete(checkpointId);\n+    // It's possible that we have the following events:\n+    //   1. snapshotState(ckpId);\n+    //   2. snapshotState(ckpId+1);\n+    //   3. notifyCheckpointComplete(ckpId+1);\n+    //   4. notifyCheckpointComplete(ckpId);\n+    // For step#4, we don't need to commit iceberg table again because in step#3 we've committed all the files,\n+    // Besides, we need to maintain the max-committed-checkpoint-id to be increasing.\n+    if (checkpointId > maxCommittedCheckpointId) {\n+      commitUpToCheckpoint(checkpointId);\n+      this.maxCommittedCheckpointId = checkpointId;\n+    }\n+  }\n+\n+  private void commitUpToCheckpoint(long checkpointId) {\n+    NavigableMap<Long, List<DataFile>> pendingFileMap = dataFilesPerCheckpoint.headMap(checkpointId, true);\n+\n+    List<DataFile> pendingDataFiles = Lists.newArrayList();\n+    for (List<DataFile> dataFiles : pendingFileMap.values()) {\n+      pendingDataFiles.addAll(dataFiles);\n+    }\n+\n+    AppendFiles appendFiles = table.newAppend();\n+    pendingDataFiles.forEach(appendFiles::appendFile);\n+    appendFiles.set(MAX_COMMITTED_CHECKPOINT_ID, Long.toString(checkpointId));\n+    appendFiles.set(FLINK_JOB_ID, flinkJobId);\n+    appendFiles.commit();\n+\n+    // Clear the committed data files from dataFilesPerCheckpoint.\n+    pendingFileMap.clear();\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<DataFile> element) {\n+    this.dataFilesOfCurrentCheckpoint.add(element.getValue());", "originalCommit": "e674ed7182289f3c6132c0ca2e76f0fd2d0b6871", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}