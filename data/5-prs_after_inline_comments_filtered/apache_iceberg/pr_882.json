{"pr_number": 882, "pr_title": "fix: Failed to get status issue because of s3 eventual consistency", "pr_createdAt": "2020-03-30T17:39:40Z", "pr_url": "https://github.com/apache/iceberg/pull/882", "timeline": [{"oid": "20b1ab45ce803f814be23667625b9414b0b7209d", "url": "https://github.com/apache/iceberg/commit/20b1ab45ce803f814be23667625b9414b0b7209d", "message": "fix: Failed to get status issue because of s3 eventual consistency\n\ns3a file will invoke HadoopInputFile.getStat when getting length of file.\nthis call might fail because of s3 eventual consistency.\n\ninstead use appender.length to get file size and call respective setter methods\nwhile creating instance of data file.", "committedDate": "2020-03-31T02:24:04Z", "type": "forcePushed"}, {"oid": "7916d8fb9b9a421f597a6b95526ad1642ac542ab", "url": "https://github.com/apache/iceberg/commit/7916d8fb9b9a421f597a6b95526ad1642ac542ab", "message": "fix: Failed to get status issue because of s3 eventual consistency\n\ns3a file will invoke HadoopInputFile.getStat when getting length of file.\nthis call might fail because of s3 eventual consistency.\n\ninstead use appender.length to get file size and call respective setter methods\nwhile creating instance of data file when configured.\n\nadded new datasource option \"use-writer-length-as-file-size\" to control behaviour.", "committedDate": "2020-03-31T17:47:09Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTc4OTA0NQ==", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401789045", "bodyText": "@rdblue additional 103 in writer length, can this be bug in writer factory which returns buffer size after flush?\n( no rush of merging this PR , I am trying to make sure changes are ok)", "author": "sudssf", "createdAt": "2020-04-01T17:32:39Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java", "diffHunk": "@@ -201,6 +201,36 @@ public void testSplitOptionsOverridesTableProperties() throws IOException {\n     Assert.assertEquals(\"Spark partitions should match\", 2, resultDf.javaRDD().getNumPartitions());\n   }\n \n+  @Test\n+  public void testSplitOptionsOverridesTablePropertiesWithWriterLength() throws IOException {\n+    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    Map<String, String> options = Maps.newHashMap();\n+    options.put(TableProperties.SPLIT_SIZE, String.valueOf(128L * 1024 * 1024)); // 128Mb\n+    tables.create(SCHEMA, spec, options, tableLocation);\n+\n+    List<SimpleRecord> expectedRecords = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\"),\n+        new SimpleRecord(2, \"b\")\n+    );\n+    Dataset<Row> originalDf = spark.createDataFrame(expectedRecords, SimpleRecord.class);\n+    originalDf.select(\"id\", \"data\").write()\n+        .format(\"iceberg\")\n+        .mode(\"append\")\n+        .option(\"use-writer-length-as-file-size\", true)\n+        .save(tableLocation);\n+\n+    Dataset<Row> resultDf = spark.read()\n+        .format(\"iceberg\")\n+        .option(\"split-size\", String.valueOf(611 + 103)) // 611 bytes is the size of SimpleRecord(1,\"a\")", "originalCommit": "7916d8fb9b9a421f597a6b95526ad1642ac542ab", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTgyMDc1OQ==", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401820759", "bodyText": "Does this happen for all formats, or just one?", "author": "rdblue", "createdAt": "2020-04-01T18:26:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTc4OTA0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk1NzQ3NA==", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401957474", "bodyText": "Do we know why this is happening? I would expect Parquet to return the correct size after close. We should find out what's going on.", "author": "rdblue", "createdAt": "2020-04-01T22:56:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTc4OTA0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2MDU5MA==", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401960590", "bodyText": "I think this happens only for parquet.\nhttps://github.com/apache/incubator-iceberg/blob/master/parquet/src/main/java/org/apache/iceberg/parquet/ParquetWriter.java#L142\nwriteStore seems to return non zero results for getBufferedSize after close.", "author": "sudssf", "createdAt": "2020-04-01T23:04:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTc4OTA0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTgyMDM5NA==", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401820394", "bodyText": "This shouldn't be a new option. Let's remove it.", "author": "rdblue", "createdAt": "2020-04-01T18:25:29Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Writer.java", "diffHunk": "@@ -114,6 +115,7 @@\n     long tableTargetFileSize = PropertyUtil.propertyAsLong(\n         table.properties(), WRITE_TARGET_FILE_SIZE_BYTES, WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n     this.targetFileSize = options.getLong(\"target-file-size-bytes\", tableTargetFileSize);\n+    this.useWriterLengthAsFileSize = options.getBoolean(\"use-writer-length-as-file-size\", false);", "originalCommit": "7916d8fb9b9a421f597a6b95526ad1642ac542ab", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTkzOTk3Ng==", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401939976", "bodyText": "@rdblue  do you suggestion always using writer length, instead of calling dataFileBuilder.withEncryptedOutputFile(currentFile) which internally calls getStatus for s3a?", "author": "sudssf", "createdAt": "2020-04-01T22:11:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTgyMDM5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk1NzMwOA==", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401957308", "bodyText": "Yes, I think it should always use the length reported by the writer.", "author": "rdblue", "createdAt": "2020-04-01T22:55:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTgyMDM5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2MDc0OQ==", "url": "https://github.com/apache/iceberg/pull/882#discussion_r401960749", "bodyText": "sounds good I will update PR.", "author": "sudssf", "createdAt": "2020-04-01T23:05:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTgyMDM5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjUyNjgwOQ==", "url": "https://github.com/apache/iceberg/pull/882#discussion_r402526809", "bodyText": "@rdblue please take a look at updated PR at your convenience.", "author": "sudssf", "createdAt": "2020-04-02T18:32:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTgyMDM5NA=="}], "type": "inlineReview"}, {"oid": "3c5e52e53cd4a87a5c7b637e4a7ab2e9a58df466", "url": "https://github.com/apache/iceberg/commit/3c5e52e53cd4a87a5c7b637e4a7ab2e9a58df466", "message": "fix: Failed to get status issue because of s3 eventual consistency\n\ns3a file will invoke HadoopInputFile.getStat when getting length of file.\nthis call might fail because of s3 eventual consistency.\n\ninstead use appender.length to get file size and call respective setter methods\nwhile creating instance of data file.", "committedDate": "2020-04-01T23:56:30Z", "type": "forcePushed"}, {"oid": "69de3f2fe3e5b871e42d13a5fece4e713f3b2e06", "url": "https://github.com/apache/iceberg/commit/69de3f2fe3e5b871e42d13a5fece4e713f3b2e06", "message": "fix: Failed to get status issue because of s3 eventual consistency\n\ns3a file will invoke HadoopInputFile.getStat when getting length of file.\nthis call might fail because of s3 eventual consistency.\n\ninstead use appender.length to get file size and call respective setter methods\nwhile creating instance of data file.", "committedDate": "2020-04-01T23:58:26Z", "type": "commit"}, {"oid": "69de3f2fe3e5b871e42d13a5fece4e713f3b2e06", "url": "https://github.com/apache/iceberg/commit/69de3f2fe3e5b871e42d13a5fece4e713f3b2e06", "message": "fix: Failed to get status issue because of s3 eventual consistency\n\ns3a file will invoke HadoopInputFile.getStat when getting length of file.\nthis call might fail because of s3 eventual consistency.\n\ninstead use appender.length to get file size and call respective setter methods\nwhile creating instance of data file.", "committedDate": "2020-04-01T23:58:26Z", "type": "forcePushed"}]}