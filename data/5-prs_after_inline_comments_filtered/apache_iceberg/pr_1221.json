{"pr_number": 1221, "pr_title": "Spark: Fix estimateStatistics when called without filters", "pr_createdAt": "2020-07-20T20:36:28Z", "pr_url": "https://github.com/apache/iceberg/pull/1221", "timeline": [{"oid": "ac5fc0fb393e68ab31cfd8d48d0bc3688e7ba0c8", "url": "https://github.com/apache/iceberg/commit/ac5fc0fb393e68ab31cfd8d48d0bc3688e7ba0c8", "message": "ISSUE-1220: add option to disable manifest reading during estimateStatistics call\n\nhttps://github.com/apache/iceberg/issues/1220\nthis commits add support for table property \"read.spark.disable-estimate-statistics\"\nwhich will return default value of table statistics and avoid reading manifest and data files\nto get table size and row count.\nthis feature is useful for spark2 as spark2 does not perform predicate pushdown before calling\nestimateStatistics which ends up scanning entire table.\nthis feature may be useful for spark3 if table manifest list is large even after predicate pushdown.", "committedDate": "2020-07-20T21:21:10Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcxMTUzOQ==", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r457711539", "bodyText": "I just had an idea for an alternative solution to this. What about detecting that there are no filters and instead returning a value based on the total-records value in snapshot metadata?\nUsually, estimating stats based on the number of rows and a guess for the size of a row is much better than using the actual size anyway. So if you can get the number of rows and come up with an estimate for the size of each row based on the table schema, then you wouldn't need to disable stats at all.", "author": "rdblue", "createdAt": "2020-07-20T21:49:52Z", "path": "spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -276,6 +280,9 @@ public void pruneColumns(StructType newRequestedSchema) {\n \n   @Override\n   public Statistics estimateStatistics() {\n+    if(disableEstimateStatistics) {\n+      return new Stats(Long.MAX_VALUE, Long.MAX_VALUE);\n+    }", "originalCommit": "ac5fc0fb393e68ab31cfd8d48d0bc3688e7ba0c8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzc1NjEyNw==", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r457756127", "bodyText": "thats really good idea, I thought about this too but I was under impression that we had to look data file metadata to get this information.\nI try to update PR using  total-records", "author": "sudssf", "createdAt": "2020-07-20T23:55:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcxMTUzOQ=="}], "type": "inlineReview"}, {"oid": "25699e6c2da39e1fe6c84970305cdcd07e7478c3", "url": "https://github.com/apache/iceberg/commit/25699e6c2da39e1fe6c84970305cdcd07e7478c3", "message": "ISSUE-1220: add option to disable manifest reading during estimateStatistics call\n\nhttps://github.com/apache/iceberg/issues/1220\nthis commits add support for table property \"read.spark.use-approximate-statistics\"\nwhich returns table statistics based on table rows and approximate row size and avoid reading manifest and data files\nto get table size and row count.\nthis feature is useful for spark2 as spark2 does not perform predicate pushdown before calling\nestimateStatistics which ends up scanning entire table.\nthis feature may be useful for spark3 if table manifest list is large even after predicate pushdown.", "committedDate": "2020-07-21T01:22:25Z", "type": "forcePushed"}, {"oid": "b73d189b2192cf2a47051af6550fc98d6a32c629", "url": "https://github.com/apache/iceberg/commit/b73d189b2192cf2a47051af6550fc98d6a32c629", "message": "ISSUE-1220: add option to disable manifest reading during estimateStatistics call\n\nhttps://github.com/apache/iceberg/issues/1220\nthis commits add support for table property \"read.spark.use-approximate-statistics\"\nwhich returns table statistics based on table rows and approximate row size and avoid reading manifest and data files\nto get table size and row count.\nthis feature is useful for spark2 as spark2 does not perform predicate pushdown before calling\nestimateStatistics which ends up scanning entire table.\nthis feature may be useful for spark3 if table manifest list is large even after predicate pushdown.", "committedDate": "2020-07-21T03:11:26Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIzODU3MQ==", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r458238571", "bodyText": "Nit: no need for a newline here.", "author": "rdblue", "createdAt": "2020-07-21T16:41:52Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/ReaderUtils.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * common reader code between spark2 and spark3 module\n+ */\n+public class ReaderUtils {\n+  private ReaderUtils() {\n+", "originalCommit": "b73d189b2192cf2a47051af6550fc98d6a32c629", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIzODY3Mg==", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r458238672", "bodyText": "Nit: newlines should be added between methods.", "author": "rdblue", "createdAt": "2020-07-21T16:42:02Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/ReaderUtils.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * common reader code between spark2 and spark3 module\n+ */\n+public class ReaderUtils {\n+  private ReaderUtils() {\n+\n+  }\n+  /**", "originalCommit": "b73d189b2192cf2a47051af6550fc98d6a32c629", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIzODg3MA==", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r458238870", "bodyText": "And no need for a newline between Javadoc and the documented method.", "author": "rdblue", "createdAt": "2020-07-21T16:42:18Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/ReaderUtils.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * common reader code between spark2 and spark3 module\n+ */\n+public class ReaderUtils {\n+  private ReaderUtils() {\n+\n+  }\n+  /**\n+   * guess table size based on table schema and total records.\n+   *\n+   * @param table iceberg table\n+   * @param totalRecords total records in the table\n+   * @return approxiate size based on table schema\n+   */\n+", "originalCommit": "b73d189b2192cf2a47051af6550fc98d6a32c629", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIzOTA5Ng==", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r458239096", "bodyText": "Please add newlines after control flow, like if/else or loops.", "author": "rdblue", "createdAt": "2020-07-21T16:42:36Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/ReaderUtils.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * common reader code between spark2 and spark3 module\n+ */\n+public class ReaderUtils {\n+  private ReaderUtils() {\n+\n+  }\n+  /**\n+   * guess table size based on table schema and total records.\n+   *\n+   * @param table iceberg table\n+   * @param totalRecords total records in the table\n+   * @return approxiate size based on table schema\n+   */\n+\n+  public static long approximateTableSize(Table table, long totalRecords) {\n+    if (totalRecords == Long.MAX_VALUE) {\n+      return totalRecords;\n+    }\n+    StructType type = SparkSchemaUtil.convert(table.schema());", "originalCommit": "b73d189b2192cf2a47051af6550fc98d6a32c629", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODI0MDI1Nw==", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r458240257", "bodyText": "I think this should pass in a StructType or Schema rather than the table. The table isn't really used.\nIt would also make sense to pass the value of lazyType() from the reader because that will account for column projection.", "author": "rdblue", "createdAt": "2020-07-21T16:44:28Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/ReaderUtils.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * common reader code between spark2 and spark3 module\n+ */\n+public class ReaderUtils {\n+  private ReaderUtils() {\n+\n+  }\n+  /**\n+   * guess table size based on table schema and total records.\n+   *\n+   * @param table iceberg table\n+   * @param totalRecords total records in the table\n+   * @return approxiate size based on table schema\n+   */\n+\n+  public static long approximateTableSize(Table table, long totalRecords) {", "originalCommit": "b73d189b2192cf2a47051af6550fc98d6a32c629", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODI0MTA1OA==", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r458241058", "bodyText": "I think it would be easier to use PropertyUtil.propertyAsLong instead of adding this method.", "author": "rdblue", "createdAt": "2020-07-21T16:45:35Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/ReaderUtils.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * common reader code between spark2 and spark3 module\n+ */\n+public class ReaderUtils {\n+  private ReaderUtils() {\n+\n+  }\n+  /**\n+   * guess table size based on table schema and total records.\n+   *\n+   * @param table iceberg table\n+   * @param totalRecords total records in the table\n+   * @return approxiate size based on table schema\n+   */\n+\n+  public static long approximateTableSize(Table table, long totalRecords) {\n+    if (totalRecords == Long.MAX_VALUE) {\n+      return totalRecords;\n+    }\n+    StructType type = SparkSchemaUtil.convert(table.schema());\n+    long approximateSize = 0;\n+    for (StructField sparkField : type.fields()) {\n+      approximateSize += sparkField.dataType().defaultSize();\n+    }\n+    return approximateSize * totalRecords;\n+  }\n+\n+  /**\n+   * get total records from table metadata using {@link SnapshotSummary.TOTAL_RECORDS_PROP}.\n+   *\n+   * @param table iceberg table\n+   * @return total records from table metadata\n+   */\n+\n+  public static long totalRecordsFromMetadata(Table table) {", "originalCommit": "b73d189b2192cf2a47051af6550fc98d6a32c629", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b8c4ea154f1db55bfcf5bd3b6c70cd56bac6ae4a", "url": "https://github.com/apache/iceberg/commit/b8c4ea154f1db55bfcf5bd3b6c70cd56bac6ae4a", "message": "ISSUE-1220: add option to disable manifest reading during estimateStatistics call\n\nhttps://github.com/apache/iceberg/issues/1220\nthis commits add support for table property \"read.spark.use-approximate-statistics\"\nwhich returns table statistics based on table rows and approximate row size and avoid reading manifest and data files\nto get table size and row count.\nthis feature is useful for spark2 as spark2 does not perform predicate pushdown before calling\nestimateStatistics which ends up scanning entire table.\nthis feature may be useful for spark3 if table manifest list is large even after predicate pushdown.", "committedDate": "2020-07-22T21:49:04Z", "type": "forcePushed"}, {"oid": "8713a1786e1e83a89becc53b0d66404beb26d7be", "url": "https://github.com/apache/iceberg/commit/8713a1786e1e83a89becc53b0d66404beb26d7be", "message": "ISSUE-1220: add option to disable manifest reading during estimateStatistics call\n\nhttps://github.com/apache/iceberg/issues/1220\nthis commits add support for table property \"read.spark.use-approximate-statistics\"\nwhich returns table statistics based on table rows and approximate row size and avoid reading manifest and data files\nto get table size and row count.\nthis feature is useful for spark2 as spark2 does not perform predicate pushdown before calling\nestimateStatistics which ends up scanning entire table.\nthis feature may be useful for spark3 if table manifest list is large even after predicate pushdown.", "committedDate": "2020-07-22T21:49:59Z", "type": "forcePushed"}, {"oid": "34c335975be32f3588fc93a6ae4de94acce7c9e3", "url": "https://github.com/apache/iceberg/commit/34c335975be32f3588fc93a6ae4de94acce7c9e3", "message": "ISSUE-1220: add option to disable manifest reading during estimateStatistics call\n\nhttps://github.com/apache/iceberg/issues/1220\nthis commits add support for table property \"read.spark.use-approximate-statistics\"\nwhich returns table statistics based on table rows and approximate row size and avoid reading manifest and data files\nto get table size and row count.\nthis feature is useful for spark2 as spark2 does not perform predicate pushdown before calling\nestimateStatistics which ends up scanning entire table.\nthis feature may be useful for spark3 if table manifest list is large even after predicate pushdown.", "committedDate": "2020-07-22T21:50:44Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NjA5Nw==", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r459166097", "bodyText": "Looks like this is a typo.", "author": "rdblue", "createdAt": "2020-07-23T01:07:56Z", "path": "spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -174,7 +175,7 @@ private Schema lazySchema() {\n   }\n \n   private Expression filterExpression() {\n-    if (filterExpressions != null) {\n+    if (filterExpressions != null ) {", "originalCommit": "34c335975be32f3588fc93a6ae4de94acce7c9e3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk4NDYwMA==", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r460984600", "bodyText": "ack", "author": "sudssf", "createdAt": "2020-07-27T15:41:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NjA5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NjMxNw==", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r459166317", "bodyText": "I don't think this test needs to be here. Tests for utility classes should be in their own files, and preferably in the same module as the utility class.", "author": "rdblue", "createdAt": "2020-07-23T01:08:55Z", "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkSchema24.java", "diffHunk": "@@ -19,5 +19,43 @@\n \n package org.apache.iceberg.spark.source;\n \n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n public class TestSparkSchema24 extends TestSparkSchema {\n+\n+  @Test\n+  public void testReaderUtils() throws IOException {", "originalCommit": "34c335975be32f3588fc93a6ae4de94acce7c9e3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk4NDkyNQ==", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r460984925", "bodyText": "ack", "author": "sudssf", "createdAt": "2020-07-27T15:41:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NjMxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NjQ4Mw==", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r459166483", "bodyText": "This doesn't need to change since the tests should be in the spark module instead of in a subclass of TestSparkSchema.", "author": "rdblue", "createdAt": "2020-07-23T01:09:37Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkSchema.java", "diffHunk": "@@ -46,12 +46,12 @@\n \n public abstract class TestSparkSchema {\n \n-  private static final Configuration CONF = new Configuration();\n-  private static final Schema SCHEMA = new Schema(\n+  protected static final Configuration CONF = new Configuration();\n+  protected static final Schema SCHEMA = new Schema(\n       optional(1, \"id\", Types.IntegerType.get()),\n       optional(2, \"data\", Types.StringType.get())\n   );\n-  private static SparkSession spark = null;\n+  protected static SparkSession spark = null;", "originalCommit": "34c335975be32f3588fc93a6ae4de94acce7c9e3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk4NDU0MA==", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r460984540", "bodyText": "ack", "author": "sudssf", "createdAt": "2020-07-27T15:41:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NjQ4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NzUwMA==", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r459167500", "bodyText": "I don't think the method being tested warrants a test that creates a table, writes data, etc. The method has 2 cases:\n\nnumRows is Long.MAX_VALUE -> return Long.MAX_VALUE\nnumRows is not -> multiply numRows by sizeEstimate(dataType)\n\nAll you need is 2 test cases: one for each possibility for numRows. The second case should make sure the estimate is sane.", "author": "rdblue", "createdAt": "2020-07-23T01:12:59Z", "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkSchema24.java", "diffHunk": "@@ -19,5 +19,43 @@\n \n package org.apache.iceberg.spark.source;\n \n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n public class TestSparkSchema24 extends TestSparkSchema {\n+\n+  @Test\n+  public void testReaderUtils() throws IOException {\n+    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    tables.create(SCHEMA, spec, null, tableLocation);\n+    List<SimpleRecord> expectedRecords = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\")\n+    );\n+    Dataset<Row> originalDf = spark.createDataFrame(expectedRecords, SimpleRecord.class);\n+    originalDf.select(\"id\", \"data\").write()\n+        .format(\"iceberg\")\n+        .mode(\"append\")\n+        .save(tableLocation);\n+\n+    Table table = tables.load(tableLocation);\n+    long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n+              SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n+    Assert.assertEquals(\"totalRecords match\", 1, totalRecords);\n+    long tableSize = ReaderUtils.approximateTableSize(SparkSchemaUtil.convert(table.schema()), totalRecords);\n+    Assert.assertEquals(\"table size matches with expected approximation\", 24, tableSize);", "originalCommit": "34c335975be32f3588fc93a6ae4de94acce7c9e3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk4NTAzNQ==", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r460985035", "bodyText": "added", "author": "sudssf", "createdAt": "2020-07-27T15:41:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NzUwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2ODEwNg==", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r459168106", "bodyText": "Minor: I don't see much value in having this debug statement.\nIf you were trying to debug estimateStatistics, I think you'd want to have a message for every call, and you would want it to show the expression as well as the resulting estimate. I think Spark already has those logs, though.", "author": "rdblue", "createdAt": "2020-07-23T01:15:35Z", "path": "spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -276,6 +277,13 @@ public void pruneColumns(StructType newRequestedSchema) {\n \n   @Override\n   public Statistics estimateStatistics() {\n+    if (filterExpressions == null || filterExpressions == Expressions.alwaysTrue()) {\n+      LOG.debug(\"using table metadata to estimate table statistics\");", "originalCommit": "34c335975be32f3588fc93a6ae4de94acce7c9e3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk4NDgzOA==", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r460984838", "bodyText": "ack", "author": "sudssf", "createdAt": "2020-07-27T15:41:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2ODEwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2OTMxMg==", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r459169312", "bodyText": "Could you move this method to SparkSchemaUtil.estimateSize? I think it makes sense to put it in the schema utility methods, since it relates to a Spark schema. Looks like we don't have a test suite for that, so you could start TestSparkSchemaUtil with the tests for this.", "author": "rdblue", "createdAt": "2020-07-23T01:20:34Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/ReaderUtils.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * common reader code between spark2 and spark3 module\n+ */\n+public class ReaderUtils {\n+  private ReaderUtils() {}\n+\n+  /**\n+   * guess table size based on table schema and total records.\n+   *\n+   * @param table        iceberg table\n+   * @param totalRecords total records in the table\n+   * @return approxiate size based on table schema\n+   */\n+  public static long approximateTableSize(StructType tableSchema, long totalRecords) {", "originalCommit": "34c335975be32f3588fc93a6ae4de94acce7c9e3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk4NDQ2NA==", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r460984464", "bodyText": "ack this makes more sense thanks for suggestion", "author": "sudssf", "createdAt": "2020-07-27T15:41:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2OTMxMg=="}], "type": "inlineReview"}, {"oid": "53e34fabd51457c03c2544da97e4d4e388a1b90d", "url": "https://github.com/apache/iceberg/commit/53e34fabd51457c03c2544da97e4d4e388a1b90d", "message": "ISSUE-1220: add option to disable manifest reading during estimateStatistics call\n\nhttps://github.com/apache/iceberg/issues/1220\nthis commits add support for table property \"read.spark.use-approximate-statistics\"\nwhich returns table statistics based on table rows and approximate row size and avoid reading manifest and data files\nto get table size and row count.\nthis feature is useful for spark2 as spark2 does not perform predicate pushdown before calling\nestimateStatistics which ends up scanning entire table.\nthis feature may be useful for spark3 if table manifest list is large even after predicate pushdown.", "committedDate": "2020-07-27T15:43:44Z", "type": "forcePushed"}, {"oid": "b6e7df071924edc63f3add28283f77bc1c77255d", "url": "https://github.com/apache/iceberg/commit/b6e7df071924edc63f3add28283f77bc1c77255d", "message": "ISSUE-1220: add option to disable manifest reading during estimateStatistics call\n\nhttps://github.com/apache/iceberg/issues/1220\nthis commits add support for table property \"read.spark.use-approximate-statistics\"\nwhich returns table statistics based on table rows and approximate row size and avoid reading manifest and data files\nto get table size and row count.\nthis feature is useful for spark2 as spark2 does not perform predicate pushdown before calling\nestimateStatistics which ends up scanning entire table.\nthis feature may be useful for spark3 if table manifest list is large even after predicate pushdown.", "committedDate": "2020-07-27T15:45:16Z", "type": "commit"}, {"oid": "b6e7df071924edc63f3add28283f77bc1c77255d", "url": "https://github.com/apache/iceberg/commit/b6e7df071924edc63f3add28283f77bc1c77255d", "message": "ISSUE-1220: add option to disable manifest reading during estimateStatistics call\n\nhttps://github.com/apache/iceberg/issues/1220\nthis commits add support for table property \"read.spark.use-approximate-statistics\"\nwhich returns table statistics based on table rows and approximate row size and avoid reading manifest and data files\nto get table size and row count.\nthis feature is useful for spark2 as spark2 does not perform predicate pushdown before calling\nestimateStatistics which ends up scanning entire table.\nthis feature may be useful for spark3 if table manifest list is large even after predicate pushdown.", "committedDate": "2020-07-27T15:45:16Z", "type": "forcePushed"}, {"oid": "756180a2e92607cd2aa3d5ebb709d057936c9ed9", "url": "https://github.com/apache/iceberg/commit/756180a2e92607cd2aa3d5ebb709d057936c9ed9", "message": "Update SparkSchemaUtil.java", "committedDate": "2020-07-27T16:53:21Z", "type": "commit"}, {"oid": "b054e9e21cbd93f29c49a8bc74ddd93b4e4cdc77", "url": "https://github.com/apache/iceberg/commit/b054e9e21cbd93f29c49a8bc74ddd93b4e4cdc77", "message": "Update TestSparkSchemaUtil.java", "committedDate": "2020-07-27T16:54:09Z", "type": "commit"}, {"oid": "fe97fd722f923bb82452590efa6088cf8fd7b0d7", "url": "https://github.com/apache/iceberg/commit/fe97fd722f923bb82452590efa6088cf8fd7b0d7", "message": "Update SparkBatchScan.java", "committedDate": "2020-07-27T17:08:15Z", "type": "commit"}]}