{"pr_number": 1124, "pr_title": "Add Spark 3 data source classes", "pr_createdAt": "2020-06-17T23:52:56Z", "pr_url": "https://github.com/apache/iceberg/pull/1124", "timeline": [{"oid": "b9c2d2840df447c436db2e8c891c4e74107e7bfd", "url": "https://github.com/apache/iceberg/commit/b9c2d2840df447c436db2e8c891c4e74107e7bfd", "message": "Add Spark 3 source implementation.", "committedDate": "2020-06-20T01:07:40Z", "type": "forcePushed"}, {"oid": "6e33fa125caa1e9019b828d7986e7e2cef1e9ca2", "url": "https://github.com/apache/iceberg/commit/6e33fa125caa1e9019b828d7986e7e2cef1e9ca2", "message": "Add Spark 3 source implementation.", "committedDate": "2020-06-20T01:08:05Z", "type": "forcePushed"}, {"oid": "0aa8d7b6a795454d07f9bb9a6fd848df71713f51", "url": "https://github.com/apache/iceberg/commit/0aa8d7b6a795454d07f9bb9a6fd848df71713f51", "message": "Add Spark 3 source implementation.", "committedDate": "2020-06-20T01:09:23Z", "type": "forcePushed"}, {"oid": "0e037dab10b6ed6d474cad4413522c054daae3de", "url": "https://github.com/apache/iceberg/commit/0e037dab10b6ed6d474cad4413522c054daae3de", "message": "Spark 3: exclude Arrow that breaks the build.", "committedDate": "2020-06-23T19:49:18Z", "type": "commit"}, {"oid": "2979ed983dce898c714f40854d28cc638d262058", "url": "https://github.com/apache/iceberg/commit/2979ed983dce898c714f40854d28cc638d262058", "message": "Update and add tests.", "committedDate": "2020-06-23T20:01:51Z", "type": "forcePushed"}, {"oid": "fef99b6a97097e9cb87f9ffd035eef76c9c579e3", "url": "https://github.com/apache/iceberg/commit/fef99b6a97097e9cb87f9ffd035eef76c9c579e3", "message": "Update and add tests.", "committedDate": "2020-06-23T21:31:58Z", "type": "forcePushed"}, {"oid": "9df6eee65fd8198d131286ff48836b0f0d4385a5", "url": "https://github.com/apache/iceberg/commit/9df6eee65fd8198d131286ff48836b0f0d4385a5", "message": "Add Spark 3 source implementation.", "committedDate": "2020-06-23T23:51:39Z", "type": "commit"}, {"oid": "d03b0b2136e5470a1a7549737136ffc7debd6c7d", "url": "https://github.com/apache/iceberg/commit/d03b0b2136e5470a1a7549737136ffc7debd6c7d", "message": "Update and add tests.", "committedDate": "2020-06-23T23:51:39Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDU5NDYyNw==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r444594627", "bodyText": "nit: seems like it should be locationSet", "author": "aokolnychyi", "createdAt": "2020-06-24T01:31:02Z", "path": "core/src/main/java/org/apache/iceberg/hadoop/Util.java", "diffHunk": "@@ -63,4 +66,16 @@ public static FileSystem getFs(Path path, Configuration conf) {\n \n     return locationSets.toArray(new String[0]);\n   }\n+\n+  public static String[] blockLocations(FileIO io, CombinedScanTask task) {\n+    Set<String> locationSets = Sets.newHashSet();", "originalCommit": "d03b0b2136e5470a1a7549737136ffc7debd6c7d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDU5NTI5OA==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r444595298", "bodyText": "Okay, this seems to take just the type of NO_LOCATION_PREFERENCE. Internally, it will keep all elements.", "author": "aokolnychyi", "createdAt": "2020-06-24T01:33:56Z", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopInputFile.java", "diffHunk": "@@ -169,6 +174,24 @@ public FileStatus getStat() {\n     return lazyStat();\n   }\n \n+  public Path getPath() {\n+    return path;\n+  }\n+\n+  public String[] getBlockLocations(long start, long end) {\n+    List<String> hosts = Lists.newArrayList();\n+    try {\n+      for (BlockLocation location : fs.getFileBlockLocations(path, start, end)) {\n+        Collections.addAll(hosts, location.getHosts());\n+      }\n+\n+      return hosts.toArray(NO_LOCATION_PREFERENCE);", "originalCommit": "d03b0b2136e5470a1a7549737136ffc7debd6c7d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTAyMzUxNg==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r445023516", "bodyText": "Yeah, it is odd how this API works, but when you pass in an array with fewer elements than you need, it will allocate a new one for you that is the right size. So we pass in a 0-element array. If you just call toArray, Java won't get the type right. The streams API allows passing in an array constructor (String[]::new), but it seems awkward to create a stream when there's already a working, although awkward, API call for it.", "author": "rdblue", "createdAt": "2020-06-24T16:33:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDU5NTI5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDU5OTA2OA==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r444599068", "bodyText": "nit: shall we group constants and put them above the constructor?", "author": "aokolnychyi", "createdAt": "2020-06-24T01:48:24Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -0,0 +1,512 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.UpdateProperties;\n+import org.apache.iceberg.UpdateSchema;\n+import org.apache.iceberg.expressions.BoundPredicate;\n+import org.apache.iceberg.expressions.ExpressionVisitors;\n+import org.apache.iceberg.expressions.UnboundPredicate;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.transforms.PartitionSpecVisitor;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.spark.sql.connector.catalog.TableChange;\n+import org.apache.spark.sql.connector.expressions.Expression;\n+import org.apache.spark.sql.connector.expressions.Expressions;\n+import org.apache.spark.sql.connector.expressions.Literal;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.IntegerType;\n+import org.apache.spark.sql.types.LongType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+public class Spark3Util {\n+\n+  private static final ImmutableSet<String> LOCALITY_WHITELIST_FS = ImmutableSet.of(\"hdfs\");\n+\n+  private Spark3Util() {\n+  }\n+\n+  private static final Joiner DOT = Joiner.on(\".\");", "originalCommit": "d03b0b2136e5470a1a7549737136ffc7debd6c7d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDYwMjk5Mg==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r444602992", "bodyText": "nit: I think this will fit on one line", "author": "aokolnychyi", "createdAt": "2020-06-24T02:03:51Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/SparkCatalog.java", "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.CachingCatalog;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.Transaction;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hive.HiveCatalog;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.iceberg.spark.source.StagedSparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableChange;\n+import org.apache.spark.sql.connector.catalog.TableChange.RemoveProperty;\n+import org.apache.spark.sql.connector.catalog.TableChange.SetProperty;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+/**\n+ * A Spark TableCatalog implementation that wraps Iceberg's {@link Catalog} interface.\n+ */\n+public class SparkCatalog implements StagingTableCatalog {\n+  private String catalogName = null;\n+  private Catalog icebergCatalog = null;\n+\n+  /**\n+   * Build an Iceberg {@link Catalog} to be used by this Spark catalog adapter.\n+   *\n+   * @param name Spark's catalog name\n+   * @param options Spark's catalog options\n+   * @return an Iceberg catalog\n+   */\n+  protected Catalog buildIcebergCatalog(String name, CaseInsensitiveStringMap options) {\n+    // TODO: add name to catalogs\n+    Configuration conf = SparkSession.active().sparkContext().hadoopConfiguration();\n+    String catalogType = options.getOrDefault(\"type\", \"hive\");\n+    switch (catalogType) {\n+      case \"hive\":\n+        int clientPoolSize = options.getInt(\"clients\", 2);\n+        String uri = options.get(\"uri\");\n+        return new HiveCatalog(uri, clientPoolSize, conf);\n+\n+      case \"hadoop\":\n+        String warehouseLocation = options.get(\"warehouse\");\n+        return new HadoopCatalog(conf, warehouseLocation);\n+\n+      default:\n+        throw new UnsupportedOperationException(\"Unknown catalog type: \" + catalogType);\n+    }\n+  }\n+\n+  /**\n+   * Build an Iceberg {@link TableIdentifier} for the given Spark identifier.\n+   *\n+   * @param identifier Spark's identifier\n+   * @return an Iceberg identifier\n+   */\n+  protected TableIdentifier buildIdentifier(Identifier identifier) {\n+    return TableIdentifier.of(Namespace.of(identifier.namespace()), identifier.name());\n+  }\n+\n+  @Override\n+  public Identifier[] listTables(String[] namespace) {\n+    // TODO: handle namespaces\n+    return new Identifier[0];\n+  }\n+\n+  @Override\n+  public SparkTable loadTable(Identifier ident) throws NoSuchTableException {\n+    try {\n+      return new SparkTable(icebergCatalog.loadTable(buildIdentifier(ident)));\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new NoSuchTableException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public SparkTable createTable(Identifier ident, StructType schema,\n+                                Transform[] transforms,\n+                                Map<String, String> properties)\n+      throws TableAlreadyExistsException {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    try {\n+      return new SparkTable(icebergCatalog.createTable(\n+          buildIdentifier(ident),\n+          icebergSchema,\n+          Spark3Util.toPartitionSpec(icebergSchema, transforms),\n+          properties.get(\"location\"),\n+          properties));\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistsException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public StagedTable stageCreate(Identifier ident, StructType schema, Transform[] transforms,\n+                                 Map<String, String> properties) throws TableAlreadyExistsException {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    try {\n+      return new StagedSparkTable(icebergCatalog.newCreateTableTransaction(buildIdentifier(ident), icebergSchema,\n+          Spark3Util.toPartitionSpec(icebergSchema, transforms), properties.get(\"location\"), properties));\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistsException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public StagedTable stageReplace(Identifier ident, StructType schema, Transform[] transforms,\n+                                  Map<String, String> properties) throws NoSuchTableException {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    try {\n+      return new StagedSparkTable(icebergCatalog.newReplaceTableTransaction(buildIdentifier(ident), icebergSchema,\n+          Spark3Util.toPartitionSpec(icebergSchema, transforms), properties.get(\"location\"), properties,\n+          false /* do not create */));\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new NoSuchTableException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public StagedTable stageCreateOrReplace(Identifier ident, StructType schema, Transform[] transforms,\n+                                          Map<String, String> properties) {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    return new StagedSparkTable(icebergCatalog.newReplaceTableTransaction(buildIdentifier(ident), icebergSchema,\n+        Spark3Util.toPartitionSpec(icebergSchema, transforms), properties.get(\"location\"), properties,\n+        true /* create or replace */));\n+  }\n+\n+  @Override\n+  public SparkTable alterTable(Identifier ident, TableChange... changes)\n+      throws NoSuchTableException {", "originalCommit": "d03b0b2136e5470a1a7549737136ffc7debd6c7d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDYwNDQzOA==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r444604438", "bodyText": "Should we use TableCatalog.PROP_LOCATION? What about comment, owner? I recall some bug fixes in Spark but will need to check again as I forgot the context.", "author": "aokolnychyi", "createdAt": "2020-06-24T02:09:26Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/SparkCatalog.java", "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.CachingCatalog;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.Transaction;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hive.HiveCatalog;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.iceberg.spark.source.StagedSparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableChange;\n+import org.apache.spark.sql.connector.catalog.TableChange.RemoveProperty;\n+import org.apache.spark.sql.connector.catalog.TableChange.SetProperty;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+/**\n+ * A Spark TableCatalog implementation that wraps Iceberg's {@link Catalog} interface.\n+ */\n+public class SparkCatalog implements StagingTableCatalog {\n+  private String catalogName = null;\n+  private Catalog icebergCatalog = null;\n+\n+  /**\n+   * Build an Iceberg {@link Catalog} to be used by this Spark catalog adapter.\n+   *\n+   * @param name Spark's catalog name\n+   * @param options Spark's catalog options\n+   * @return an Iceberg catalog\n+   */\n+  protected Catalog buildIcebergCatalog(String name, CaseInsensitiveStringMap options) {\n+    // TODO: add name to catalogs\n+    Configuration conf = SparkSession.active().sparkContext().hadoopConfiguration();\n+    String catalogType = options.getOrDefault(\"type\", \"hive\");\n+    switch (catalogType) {\n+      case \"hive\":\n+        int clientPoolSize = options.getInt(\"clients\", 2);\n+        String uri = options.get(\"uri\");\n+        return new HiveCatalog(uri, clientPoolSize, conf);\n+\n+      case \"hadoop\":\n+        String warehouseLocation = options.get(\"warehouse\");\n+        return new HadoopCatalog(conf, warehouseLocation);\n+\n+      default:\n+        throw new UnsupportedOperationException(\"Unknown catalog type: \" + catalogType);\n+    }\n+  }\n+\n+  /**\n+   * Build an Iceberg {@link TableIdentifier} for the given Spark identifier.\n+   *\n+   * @param identifier Spark's identifier\n+   * @return an Iceberg identifier\n+   */\n+  protected TableIdentifier buildIdentifier(Identifier identifier) {\n+    return TableIdentifier.of(Namespace.of(identifier.namespace()), identifier.name());\n+  }\n+\n+  @Override\n+  public Identifier[] listTables(String[] namespace) {\n+    // TODO: handle namespaces\n+    return new Identifier[0];\n+  }\n+\n+  @Override\n+  public SparkTable loadTable(Identifier ident) throws NoSuchTableException {\n+    try {\n+      return new SparkTable(icebergCatalog.loadTable(buildIdentifier(ident)));\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new NoSuchTableException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public SparkTable createTable(Identifier ident, StructType schema,\n+                                Transform[] transforms,\n+                                Map<String, String> properties)\n+      throws TableAlreadyExistsException {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    try {\n+      return new SparkTable(icebergCatalog.createTable(\n+          buildIdentifier(ident),\n+          icebergSchema,\n+          Spark3Util.toPartitionSpec(icebergSchema, transforms),\n+          properties.get(\"location\"),\n+          properties));\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistsException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public StagedTable stageCreate(Identifier ident, StructType schema, Transform[] transforms,\n+                                 Map<String, String> properties) throws TableAlreadyExistsException {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    try {\n+      return new StagedSparkTable(icebergCatalog.newCreateTableTransaction(buildIdentifier(ident), icebergSchema,\n+          Spark3Util.toPartitionSpec(icebergSchema, transforms), properties.get(\"location\"), properties));\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistsException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public StagedTable stageReplace(Identifier ident, StructType schema, Transform[] transforms,\n+                                  Map<String, String> properties) throws NoSuchTableException {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    try {\n+      return new StagedSparkTable(icebergCatalog.newReplaceTableTransaction(buildIdentifier(ident), icebergSchema,\n+          Spark3Util.toPartitionSpec(icebergSchema, transforms), properties.get(\"location\"), properties,\n+          false /* do not create */));\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new NoSuchTableException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public StagedTable stageCreateOrReplace(Identifier ident, StructType schema, Transform[] transforms,\n+                                          Map<String, String> properties) {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    return new StagedSparkTable(icebergCatalog.newReplaceTableTransaction(buildIdentifier(ident), icebergSchema,\n+        Spark3Util.toPartitionSpec(icebergSchema, transforms), properties.get(\"location\"), properties,\n+        true /* create or replace */));\n+  }\n+\n+  @Override\n+  public SparkTable alterTable(Identifier ident, TableChange... changes)\n+      throws NoSuchTableException {\n+    SetProperty setLocation = null;\n+    SetProperty setSnapshotId = null;\n+    SetProperty pickSnapshotId = null;\n+    List<TableChange> propertyChanges = Lists.newArrayList();\n+    List<TableChange> schemaChanges = Lists.newArrayList();\n+\n+    for (TableChange change : changes) {\n+      if (change instanceof SetProperty) {\n+        SetProperty set = (SetProperty) change;\n+        if (\"location\".equalsIgnoreCase(set.property())) {", "originalCommit": "d03b0b2136e5470a1a7549737136ffc7debd6c7d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTAyMzc1OA==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r445023758", "bodyText": "I'll double-check these.", "author": "rdblue", "createdAt": "2020-06-24T16:33:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDYwNDQzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA1Nzg5OA==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r445057898", "bodyText": "Updated to use PROP_LOCATION. While comment and owner are reserved properties in Spark, but Iceberg tracks these as table properties.\nSide note: we have a class to sync some table properties into other metadata fields in our metastore implementation. But the Iceberg API doesn't explicitly track all of them. I think it makes sense to use table properties here.", "author": "rdblue", "createdAt": "2020-06-24T17:31:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDYwNDQzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc2NzgwOQ==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r446767809", "bodyText": "Makes sense.", "author": "aokolnychyi", "createdAt": "2020-06-29T04:38:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDYwNDQzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDYwNTkyNQ==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r444605925", "bodyText": "Does it mean ALTER TABLE t SET TBLPROPERTIES can be used for rollbacks and cherry-picking? Will it make sense to build dedicated SQL commands for such operations and plugin via session extensions? I know it is much easier to do this via SET TBLPROPERTIES but I am not sure how natural it is.", "author": "aokolnychyi", "createdAt": "2020-06-24T02:14:49Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/SparkCatalog.java", "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.CachingCatalog;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.Transaction;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hive.HiveCatalog;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.iceberg.spark.source.StagedSparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableChange;\n+import org.apache.spark.sql.connector.catalog.TableChange.RemoveProperty;\n+import org.apache.spark.sql.connector.catalog.TableChange.SetProperty;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+/**\n+ * A Spark TableCatalog implementation that wraps Iceberg's {@link Catalog} interface.\n+ */\n+public class SparkCatalog implements StagingTableCatalog {\n+  private String catalogName = null;\n+  private Catalog icebergCatalog = null;\n+\n+  /**\n+   * Build an Iceberg {@link Catalog} to be used by this Spark catalog adapter.\n+   *\n+   * @param name Spark's catalog name\n+   * @param options Spark's catalog options\n+   * @return an Iceberg catalog\n+   */\n+  protected Catalog buildIcebergCatalog(String name, CaseInsensitiveStringMap options) {\n+    // TODO: add name to catalogs\n+    Configuration conf = SparkSession.active().sparkContext().hadoopConfiguration();\n+    String catalogType = options.getOrDefault(\"type\", \"hive\");\n+    switch (catalogType) {\n+      case \"hive\":\n+        int clientPoolSize = options.getInt(\"clients\", 2);\n+        String uri = options.get(\"uri\");\n+        return new HiveCatalog(uri, clientPoolSize, conf);\n+\n+      case \"hadoop\":\n+        String warehouseLocation = options.get(\"warehouse\");\n+        return new HadoopCatalog(conf, warehouseLocation);\n+\n+      default:\n+        throw new UnsupportedOperationException(\"Unknown catalog type: \" + catalogType);\n+    }\n+  }\n+\n+  /**\n+   * Build an Iceberg {@link TableIdentifier} for the given Spark identifier.\n+   *\n+   * @param identifier Spark's identifier\n+   * @return an Iceberg identifier\n+   */\n+  protected TableIdentifier buildIdentifier(Identifier identifier) {\n+    return TableIdentifier.of(Namespace.of(identifier.namespace()), identifier.name());\n+  }\n+\n+  @Override\n+  public Identifier[] listTables(String[] namespace) {\n+    // TODO: handle namespaces\n+    return new Identifier[0];\n+  }\n+\n+  @Override\n+  public SparkTable loadTable(Identifier ident) throws NoSuchTableException {\n+    try {\n+      return new SparkTable(icebergCatalog.loadTable(buildIdentifier(ident)));\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new NoSuchTableException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public SparkTable createTable(Identifier ident, StructType schema,\n+                                Transform[] transforms,\n+                                Map<String, String> properties)\n+      throws TableAlreadyExistsException {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    try {\n+      return new SparkTable(icebergCatalog.createTable(\n+          buildIdentifier(ident),\n+          icebergSchema,\n+          Spark3Util.toPartitionSpec(icebergSchema, transforms),\n+          properties.get(\"location\"),\n+          properties));\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistsException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public StagedTable stageCreate(Identifier ident, StructType schema, Transform[] transforms,\n+                                 Map<String, String> properties) throws TableAlreadyExistsException {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    try {\n+      return new StagedSparkTable(icebergCatalog.newCreateTableTransaction(buildIdentifier(ident), icebergSchema,\n+          Spark3Util.toPartitionSpec(icebergSchema, transforms), properties.get(\"location\"), properties));\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistsException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public StagedTable stageReplace(Identifier ident, StructType schema, Transform[] transforms,\n+                                  Map<String, String> properties) throws NoSuchTableException {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    try {\n+      return new StagedSparkTable(icebergCatalog.newReplaceTableTransaction(buildIdentifier(ident), icebergSchema,\n+          Spark3Util.toPartitionSpec(icebergSchema, transforms), properties.get(\"location\"), properties,\n+          false /* do not create */));\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new NoSuchTableException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public StagedTable stageCreateOrReplace(Identifier ident, StructType schema, Transform[] transforms,\n+                                          Map<String, String> properties) {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    return new StagedSparkTable(icebergCatalog.newReplaceTableTransaction(buildIdentifier(ident), icebergSchema,\n+        Spark3Util.toPartitionSpec(icebergSchema, transforms), properties.get(\"location\"), properties,\n+        true /* create or replace */));\n+  }\n+\n+  @Override\n+  public SparkTable alterTable(Identifier ident, TableChange... changes)\n+      throws NoSuchTableException {\n+    SetProperty setLocation = null;\n+    SetProperty setSnapshotId = null;\n+    SetProperty pickSnapshotId = null;\n+    List<TableChange> propertyChanges = Lists.newArrayList();\n+    List<TableChange> schemaChanges = Lists.newArrayList();\n+\n+    for (TableChange change : changes) {\n+      if (change instanceof SetProperty) {\n+        SetProperty set = (SetProperty) change;\n+        if (\"location\".equalsIgnoreCase(set.property())) {\n+          setLocation = set;\n+        } else if (\"current-snapshot-id\".equalsIgnoreCase(set.property())) {\n+          setSnapshotId = set;\n+        } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(set.property())) {", "originalCommit": "d03b0b2136e5470a1a7549737136ffc7debd6c7d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTAyNDIxMQ==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r445024211", "bodyText": "Yes, this is what we do to expose cherry-picking and setting the current snapshot explicitly through SQL. I can remove it if there are objections to this.", "author": "rdblue", "createdAt": "2020-06-24T16:34:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDYwNTkyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc2OTYwMA==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r446769600", "bodyText": "These are very valuable features. Internally, we offer a SQL command to rollback tables. We don't expose cherry-picking yet but we are really excited about this feature. Do we want to have some SQL commands directly in Iceberg? I'd vote for that. It probably makes sense to agree on the syntax for common operations and use that across all query engines and all teams. For example, we can start with SNAPSHOT, MIGRATE, a command to rollback and a command to cherry-pick changes. These are easy to agree on. Later, we can build a VACUUM command that can optimize the table state. This one will be harder but we can add it later.\nIf that's something that we want to build, I can create a proposal in a couple of days.", "author": "aokolnychyi", "createdAt": "2020-06-29T04:46:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDYwNTkyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzE1ODE1Nw==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r447158157", "bodyText": "I'd love to see standardization here.", "author": "rdblue", "createdAt": "2020-06-29T18:09:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDYwNTkyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDYwNzEwNA==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r444607104", "bodyText": "nit: adn -> and", "author": "aokolnychyi", "createdAt": "2020-06-24T02:19:03Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/SparkCatalog.java", "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.CachingCatalog;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.Transaction;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hive.HiveCatalog;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.iceberg.spark.source.StagedSparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableChange;\n+import org.apache.spark.sql.connector.catalog.TableChange.RemoveProperty;\n+import org.apache.spark.sql.connector.catalog.TableChange.SetProperty;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+/**\n+ * A Spark TableCatalog implementation that wraps Iceberg's {@link Catalog} interface.\n+ */\n+public class SparkCatalog implements StagingTableCatalog {\n+  private String catalogName = null;\n+  private Catalog icebergCatalog = null;\n+\n+  /**\n+   * Build an Iceberg {@link Catalog} to be used by this Spark catalog adapter.\n+   *\n+   * @param name Spark's catalog name\n+   * @param options Spark's catalog options\n+   * @return an Iceberg catalog\n+   */\n+  protected Catalog buildIcebergCatalog(String name, CaseInsensitiveStringMap options) {\n+    // TODO: add name to catalogs\n+    Configuration conf = SparkSession.active().sparkContext().hadoopConfiguration();\n+    String catalogType = options.getOrDefault(\"type\", \"hive\");\n+    switch (catalogType) {\n+      case \"hive\":\n+        int clientPoolSize = options.getInt(\"clients\", 2);\n+        String uri = options.get(\"uri\");\n+        return new HiveCatalog(uri, clientPoolSize, conf);\n+\n+      case \"hadoop\":\n+        String warehouseLocation = options.get(\"warehouse\");\n+        return new HadoopCatalog(conf, warehouseLocation);\n+\n+      default:\n+        throw new UnsupportedOperationException(\"Unknown catalog type: \" + catalogType);\n+    }\n+  }\n+\n+  /**\n+   * Build an Iceberg {@link TableIdentifier} for the given Spark identifier.\n+   *\n+   * @param identifier Spark's identifier\n+   * @return an Iceberg identifier\n+   */\n+  protected TableIdentifier buildIdentifier(Identifier identifier) {\n+    return TableIdentifier.of(Namespace.of(identifier.namespace()), identifier.name());\n+  }\n+\n+  @Override\n+  public Identifier[] listTables(String[] namespace) {\n+    // TODO: handle namespaces\n+    return new Identifier[0];\n+  }\n+\n+  @Override\n+  public SparkTable loadTable(Identifier ident) throws NoSuchTableException {\n+    try {\n+      return new SparkTable(icebergCatalog.loadTable(buildIdentifier(ident)));\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new NoSuchTableException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public SparkTable createTable(Identifier ident, StructType schema,\n+                                Transform[] transforms,\n+                                Map<String, String> properties)\n+      throws TableAlreadyExistsException {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    try {\n+      return new SparkTable(icebergCatalog.createTable(\n+          buildIdentifier(ident),\n+          icebergSchema,\n+          Spark3Util.toPartitionSpec(icebergSchema, transforms),\n+          properties.get(\"location\"),\n+          properties));\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistsException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public StagedTable stageCreate(Identifier ident, StructType schema, Transform[] transforms,\n+                                 Map<String, String> properties) throws TableAlreadyExistsException {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    try {\n+      return new StagedSparkTable(icebergCatalog.newCreateTableTransaction(buildIdentifier(ident), icebergSchema,\n+          Spark3Util.toPartitionSpec(icebergSchema, transforms), properties.get(\"location\"), properties));\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistsException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public StagedTable stageReplace(Identifier ident, StructType schema, Transform[] transforms,\n+                                  Map<String, String> properties) throws NoSuchTableException {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    try {\n+      return new StagedSparkTable(icebergCatalog.newReplaceTableTransaction(buildIdentifier(ident), icebergSchema,\n+          Spark3Util.toPartitionSpec(icebergSchema, transforms), properties.get(\"location\"), properties,\n+          false /* do not create */));\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new NoSuchTableException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public StagedTable stageCreateOrReplace(Identifier ident, StructType schema, Transform[] transforms,\n+                                          Map<String, String> properties) {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    return new StagedSparkTable(icebergCatalog.newReplaceTableTransaction(buildIdentifier(ident), icebergSchema,\n+        Spark3Util.toPartitionSpec(icebergSchema, transforms), properties.get(\"location\"), properties,\n+        true /* create or replace */));\n+  }\n+\n+  @Override\n+  public SparkTable alterTable(Identifier ident, TableChange... changes)\n+      throws NoSuchTableException {\n+    SetProperty setLocation = null;\n+    SetProperty setSnapshotId = null;\n+    SetProperty pickSnapshotId = null;\n+    List<TableChange> propertyChanges = Lists.newArrayList();\n+    List<TableChange> schemaChanges = Lists.newArrayList();\n+\n+    for (TableChange change : changes) {\n+      if (change instanceof SetProperty) {\n+        SetProperty set = (SetProperty) change;\n+        if (\"location\".equalsIgnoreCase(set.property())) {\n+          setLocation = set;\n+        } else if (\"current-snapshot-id\".equalsIgnoreCase(set.property())) {\n+          setSnapshotId = set;\n+        } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(set.property())) {\n+          pickSnapshotId = set;\n+        } else {\n+          propertyChanges.add(set);\n+        }\n+      } else if (change instanceof RemoveProperty) {\n+        propertyChanges.add(change);\n+      } else {\n+        schemaChanges.add(change);\n+      }\n+    }\n+\n+    try {\n+      Table table = icebergCatalog.loadTable(buildIdentifier(ident));\n+      commitChanges(table, setLocation, setSnapshotId, pickSnapshotId, propertyChanges, schemaChanges);\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new NoSuchTableException(ident);\n+    }\n+\n+    return null;\n+  }\n+\n+  @Override\n+  public boolean dropTable(Identifier ident) {\n+    try {\n+      return icebergCatalog.dropTable(buildIdentifier(ident), true);\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      return false;\n+    }\n+  }\n+\n+  @Override\n+  public void renameTable(Identifier from, Identifier to) throws NoSuchTableException, TableAlreadyExistsException {\n+    try {\n+      icebergCatalog.renameTable(buildIdentifier(from), buildIdentifier(to));\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new NoSuchTableException(from);\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistsException(to);\n+    }\n+  }\n+\n+  @Override\n+  public void invalidateTable(Identifier ident) {\n+    try {\n+      icebergCatalog.loadTable(buildIdentifier(ident)).refresh();\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException ignored) {\n+      // ignore if the table doesn't exist, it is not cached\n+    }\n+  }\n+\n+  @Override\n+  public final void initialize(String name, CaseInsensitiveStringMap options) {\n+    boolean cacheEnabled = Boolean.parseBoolean(options.getOrDefault(\"cache-enabled\", \"true\"));\n+    Catalog catalog = buildIcebergCatalog(name, options);\n+\n+    this.catalogName = name;\n+    this.icebergCatalog = cacheEnabled ? CachingCatalog.wrap(catalog) : catalog;\n+  }\n+\n+  @Override\n+  public String name() {\n+    return catalogName;\n+  }\n+\n+  private static void commitChanges(Table table, SetProperty setLocation, SetProperty setSnapshotId,\n+                                    SetProperty pickSnapshotId, List<TableChange> propertyChanges,\n+                                    List<TableChange> schemaChanges) {\n+    // don't allow setting the snapshot adn picking a commit at the same time because order is ambiguous and choosing", "originalCommit": "d03b0b2136e5470a1a7549737136ffc7debd6c7d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDYwODUwMw==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r444608503", "bodyText": "In which cases will we both update props and modify schema?", "author": "aokolnychyi", "createdAt": "2020-06-24T02:24:09Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/SparkCatalog.java", "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.CachingCatalog;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.Transaction;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hive.HiveCatalog;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.iceberg.spark.source.StagedSparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableChange;\n+import org.apache.spark.sql.connector.catalog.TableChange.RemoveProperty;\n+import org.apache.spark.sql.connector.catalog.TableChange.SetProperty;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+/**\n+ * A Spark TableCatalog implementation that wraps Iceberg's {@link Catalog} interface.\n+ */\n+public class SparkCatalog implements StagingTableCatalog {\n+  private String catalogName = null;\n+  private Catalog icebergCatalog = null;\n+\n+  /**\n+   * Build an Iceberg {@link Catalog} to be used by this Spark catalog adapter.\n+   *\n+   * @param name Spark's catalog name\n+   * @param options Spark's catalog options\n+   * @return an Iceberg catalog\n+   */\n+  protected Catalog buildIcebergCatalog(String name, CaseInsensitiveStringMap options) {\n+    // TODO: add name to catalogs\n+    Configuration conf = SparkSession.active().sparkContext().hadoopConfiguration();\n+    String catalogType = options.getOrDefault(\"type\", \"hive\");\n+    switch (catalogType) {\n+      case \"hive\":\n+        int clientPoolSize = options.getInt(\"clients\", 2);\n+        String uri = options.get(\"uri\");\n+        return new HiveCatalog(uri, clientPoolSize, conf);\n+\n+      case \"hadoop\":\n+        String warehouseLocation = options.get(\"warehouse\");\n+        return new HadoopCatalog(conf, warehouseLocation);\n+\n+      default:\n+        throw new UnsupportedOperationException(\"Unknown catalog type: \" + catalogType);\n+    }\n+  }\n+\n+  /**\n+   * Build an Iceberg {@link TableIdentifier} for the given Spark identifier.\n+   *\n+   * @param identifier Spark's identifier\n+   * @return an Iceberg identifier\n+   */\n+  protected TableIdentifier buildIdentifier(Identifier identifier) {\n+    return TableIdentifier.of(Namespace.of(identifier.namespace()), identifier.name());\n+  }\n+\n+  @Override\n+  public Identifier[] listTables(String[] namespace) {\n+    // TODO: handle namespaces\n+    return new Identifier[0];\n+  }\n+\n+  @Override\n+  public SparkTable loadTable(Identifier ident) throws NoSuchTableException {\n+    try {\n+      return new SparkTable(icebergCatalog.loadTable(buildIdentifier(ident)));\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new NoSuchTableException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public SparkTable createTable(Identifier ident, StructType schema,\n+                                Transform[] transforms,\n+                                Map<String, String> properties)\n+      throws TableAlreadyExistsException {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    try {\n+      return new SparkTable(icebergCatalog.createTable(\n+          buildIdentifier(ident),\n+          icebergSchema,\n+          Spark3Util.toPartitionSpec(icebergSchema, transforms),\n+          properties.get(\"location\"),\n+          properties));\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistsException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public StagedTable stageCreate(Identifier ident, StructType schema, Transform[] transforms,\n+                                 Map<String, String> properties) throws TableAlreadyExistsException {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    try {\n+      return new StagedSparkTable(icebergCatalog.newCreateTableTransaction(buildIdentifier(ident), icebergSchema,\n+          Spark3Util.toPartitionSpec(icebergSchema, transforms), properties.get(\"location\"), properties));\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistsException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public StagedTable stageReplace(Identifier ident, StructType schema, Transform[] transforms,\n+                                  Map<String, String> properties) throws NoSuchTableException {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    try {\n+      return new StagedSparkTable(icebergCatalog.newReplaceTableTransaction(buildIdentifier(ident), icebergSchema,\n+          Spark3Util.toPartitionSpec(icebergSchema, transforms), properties.get(\"location\"), properties,\n+          false /* do not create */));\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new NoSuchTableException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public StagedTable stageCreateOrReplace(Identifier ident, StructType schema, Transform[] transforms,\n+                                          Map<String, String> properties) {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    return new StagedSparkTable(icebergCatalog.newReplaceTableTransaction(buildIdentifier(ident), icebergSchema,\n+        Spark3Util.toPartitionSpec(icebergSchema, transforms), properties.get(\"location\"), properties,\n+        true /* create or replace */));\n+  }\n+\n+  @Override\n+  public SparkTable alterTable(Identifier ident, TableChange... changes)\n+      throws NoSuchTableException {\n+    SetProperty setLocation = null;\n+    SetProperty setSnapshotId = null;\n+    SetProperty pickSnapshotId = null;\n+    List<TableChange> propertyChanges = Lists.newArrayList();\n+    List<TableChange> schemaChanges = Lists.newArrayList();\n+\n+    for (TableChange change : changes) {\n+      if (change instanceof SetProperty) {\n+        SetProperty set = (SetProperty) change;\n+        if (\"location\".equalsIgnoreCase(set.property())) {\n+          setLocation = set;\n+        } else if (\"current-snapshot-id\".equalsIgnoreCase(set.property())) {\n+          setSnapshotId = set;\n+        } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(set.property())) {\n+          pickSnapshotId = set;\n+        } else {\n+          propertyChanges.add(set);\n+        }\n+      } else if (change instanceof RemoveProperty) {\n+        propertyChanges.add(change);\n+      } else {\n+        schemaChanges.add(change);\n+      }\n+    }\n+\n+    try {\n+      Table table = icebergCatalog.loadTable(buildIdentifier(ident));\n+      commitChanges(table, setLocation, setSnapshotId, pickSnapshotId, propertyChanges, schemaChanges);\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new NoSuchTableException(ident);\n+    }\n+\n+    return null;\n+  }\n+\n+  @Override\n+  public boolean dropTable(Identifier ident) {\n+    try {\n+      return icebergCatalog.dropTable(buildIdentifier(ident), true);\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      return false;\n+    }\n+  }\n+\n+  @Override\n+  public void renameTable(Identifier from, Identifier to) throws NoSuchTableException, TableAlreadyExistsException {\n+    try {\n+      icebergCatalog.renameTable(buildIdentifier(from), buildIdentifier(to));\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new NoSuchTableException(from);\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistsException(to);\n+    }\n+  }\n+\n+  @Override\n+  public void invalidateTable(Identifier ident) {\n+    try {\n+      icebergCatalog.loadTable(buildIdentifier(ident)).refresh();\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException ignored) {\n+      // ignore if the table doesn't exist, it is not cached\n+    }\n+  }\n+\n+  @Override\n+  public final void initialize(String name, CaseInsensitiveStringMap options) {\n+    boolean cacheEnabled = Boolean.parseBoolean(options.getOrDefault(\"cache-enabled\", \"true\"));\n+    Catalog catalog = buildIcebergCatalog(name, options);\n+\n+    this.catalogName = name;\n+    this.icebergCatalog = cacheEnabled ? CachingCatalog.wrap(catalog) : catalog;\n+  }\n+\n+  @Override\n+  public String name() {\n+    return catalogName;\n+  }\n+\n+  private static void commitChanges(Table table, SetProperty setLocation, SetProperty setSnapshotId,\n+                                    SetProperty pickSnapshotId, List<TableChange> propertyChanges,\n+                                    List<TableChange> schemaChanges) {\n+    // don't allow setting the snapshot adn picking a commit at the same time because order is ambiguous and choosing\n+    // one order leads to different results\n+    Preconditions.checkArgument(setSnapshotId == null || pickSnapshotId == null,\n+        \"Cannot set the current the current snapshot ID and cherry-pick snapshot changes\");\n+\n+    if (setSnapshotId != null) {\n+      long newSnapshotId = Long.parseLong(setSnapshotId.value());\n+      table.manageSnapshots().setCurrentSnapshot(newSnapshotId).commit();\n+    }\n+\n+    // if updating the table snapshot, perform that update first in case it fails\n+    if (pickSnapshotId != null) {\n+      long newSnapshotId = Long.parseLong(pickSnapshotId.value());\n+      table.manageSnapshots().cherrypick(newSnapshotId).commit();\n+    }\n+\n+    Transaction transaction = table.newTransaction();", "originalCommit": "d03b0b2136e5470a1a7549737136ffc7debd6c7d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTAyNTkwMA==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r445025900", "bodyText": "I originally proposed a public API for updating tables, which could batch operations together into a group:\nTable updated = spark.catalog(\"prod\")\n    .updateTable(\"name\")\n    .addColumn(\"z\", \"bigint\")\n    .renameColumn(\"a\", \"x\")\n    .set(\"prop\", \"value\")\nI think it makes sense to assume that this may happen. We could also receive direct calls through the catalog API.", "author": "rdblue", "createdAt": "2020-06-24T16:37:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDYwODUwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDYwODU4Mw==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r444608583", "bodyText": "Shall we explicitly check whether this is ColumnChange and throw an exception if we did not recognize the change type?", "author": "aokolnychyi", "createdAt": "2020-06-24T02:24:23Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/SparkCatalog.java", "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.CachingCatalog;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.Transaction;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hive.HiveCatalog;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.iceberg.spark.source.StagedSparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableChange;\n+import org.apache.spark.sql.connector.catalog.TableChange.RemoveProperty;\n+import org.apache.spark.sql.connector.catalog.TableChange.SetProperty;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+/**\n+ * A Spark TableCatalog implementation that wraps Iceberg's {@link Catalog} interface.\n+ */\n+public class SparkCatalog implements StagingTableCatalog {\n+  private String catalogName = null;\n+  private Catalog icebergCatalog = null;\n+\n+  /**\n+   * Build an Iceberg {@link Catalog} to be used by this Spark catalog adapter.\n+   *\n+   * @param name Spark's catalog name\n+   * @param options Spark's catalog options\n+   * @return an Iceberg catalog\n+   */\n+  protected Catalog buildIcebergCatalog(String name, CaseInsensitiveStringMap options) {\n+    // TODO: add name to catalogs\n+    Configuration conf = SparkSession.active().sparkContext().hadoopConfiguration();\n+    String catalogType = options.getOrDefault(\"type\", \"hive\");\n+    switch (catalogType) {\n+      case \"hive\":\n+        int clientPoolSize = options.getInt(\"clients\", 2);\n+        String uri = options.get(\"uri\");\n+        return new HiveCatalog(uri, clientPoolSize, conf);\n+\n+      case \"hadoop\":\n+        String warehouseLocation = options.get(\"warehouse\");\n+        return new HadoopCatalog(conf, warehouseLocation);\n+\n+      default:\n+        throw new UnsupportedOperationException(\"Unknown catalog type: \" + catalogType);\n+    }\n+  }\n+\n+  /**\n+   * Build an Iceberg {@link TableIdentifier} for the given Spark identifier.\n+   *\n+   * @param identifier Spark's identifier\n+   * @return an Iceberg identifier\n+   */\n+  protected TableIdentifier buildIdentifier(Identifier identifier) {\n+    return TableIdentifier.of(Namespace.of(identifier.namespace()), identifier.name());\n+  }\n+\n+  @Override\n+  public Identifier[] listTables(String[] namespace) {\n+    // TODO: handle namespaces\n+    return new Identifier[0];\n+  }\n+\n+  @Override\n+  public SparkTable loadTable(Identifier ident) throws NoSuchTableException {\n+    try {\n+      return new SparkTable(icebergCatalog.loadTable(buildIdentifier(ident)));\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new NoSuchTableException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public SparkTable createTable(Identifier ident, StructType schema,\n+                                Transform[] transforms,\n+                                Map<String, String> properties)\n+      throws TableAlreadyExistsException {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    try {\n+      return new SparkTable(icebergCatalog.createTable(\n+          buildIdentifier(ident),\n+          icebergSchema,\n+          Spark3Util.toPartitionSpec(icebergSchema, transforms),\n+          properties.get(\"location\"),\n+          properties));\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistsException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public StagedTable stageCreate(Identifier ident, StructType schema, Transform[] transforms,\n+                                 Map<String, String> properties) throws TableAlreadyExistsException {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    try {\n+      return new StagedSparkTable(icebergCatalog.newCreateTableTransaction(buildIdentifier(ident), icebergSchema,\n+          Spark3Util.toPartitionSpec(icebergSchema, transforms), properties.get(\"location\"), properties));\n+    } catch (AlreadyExistsException e) {\n+      throw new TableAlreadyExistsException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public StagedTable stageReplace(Identifier ident, StructType schema, Transform[] transforms,\n+                                  Map<String, String> properties) throws NoSuchTableException {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    try {\n+      return new StagedSparkTable(icebergCatalog.newReplaceTableTransaction(buildIdentifier(ident), icebergSchema,\n+          Spark3Util.toPartitionSpec(icebergSchema, transforms), properties.get(\"location\"), properties,\n+          false /* do not create */));\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new NoSuchTableException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public StagedTable stageCreateOrReplace(Identifier ident, StructType schema, Transform[] transforms,\n+                                          Map<String, String> properties) {\n+    Schema icebergSchema = SparkSchemaUtil.convert(schema);\n+    return new StagedSparkTable(icebergCatalog.newReplaceTableTransaction(buildIdentifier(ident), icebergSchema,\n+        Spark3Util.toPartitionSpec(icebergSchema, transforms), properties.get(\"location\"), properties,\n+        true /* create or replace */));\n+  }\n+\n+  @Override\n+  public SparkTable alterTable(Identifier ident, TableChange... changes)\n+      throws NoSuchTableException {\n+    SetProperty setLocation = null;\n+    SetProperty setSnapshotId = null;\n+    SetProperty pickSnapshotId = null;\n+    List<TableChange> propertyChanges = Lists.newArrayList();\n+    List<TableChange> schemaChanges = Lists.newArrayList();\n+\n+    for (TableChange change : changes) {\n+      if (change instanceof SetProperty) {\n+        SetProperty set = (SetProperty) change;\n+        if (\"location\".equalsIgnoreCase(set.property())) {\n+          setLocation = set;\n+        } else if (\"current-snapshot-id\".equalsIgnoreCase(set.property())) {\n+          setSnapshotId = set;\n+        } else if (\"cherry-pick-snapshot-id\".equalsIgnoreCase(set.property())) {\n+          pickSnapshotId = set;\n+        } else {\n+          propertyChanges.add(set);\n+        }\n+      } else if (change instanceof RemoveProperty) {\n+        propertyChanges.add(change);\n+      } else {\n+        schemaChanges.add(change);", "originalCommit": "d03b0b2136e5470a1a7549737136ffc7debd6c7d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "5d8705c5ede8ca1f5994b4d18f86f5c806ab5e0d", "url": "https://github.com/apache/iceberg/commit/5d8705c5ede8ca1f5994b4d18f86f5c806ab5e0d", "message": "Address review comments.", "committedDate": "2020-06-24T17:40:43Z", "type": "commit"}, {"oid": "a4c440d27fa3851cd65b8c19301d4e8669ad27d3", "url": "https://github.com/apache/iceberg/commit/a4c440d27fa3851cd65b8c19301d4e8669ad27d3", "message": "Update and add tests.", "committedDate": "2020-06-24T17:41:13Z", "type": "commit"}, {"oid": "a4c440d27fa3851cd65b8c19301d4e8669ad27d3", "url": "https://github.com/apache/iceberg/commit/a4c440d27fa3851cd65b8c19301d4e8669ad27d3", "message": "Update and add tests.", "committedDate": "2020-06-24T17:41:13Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc3MTEyMg==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r446771122", "bodyText": "Does this mean we will no longer support check-nullability option in Spark 3?", "author": "aokolnychyi", "createdAt": "2020-06-29T04:52:31Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWrites.java", "diffHunk": "@@ -233,17 +234,14 @@ private void writeData(Iterable<Record> records, Schema schema, String location)\n \n   @Test\n   public void testNullableWithWriteOption() throws IOException {\n+    Assume.assumeTrue(\"Spark 3.0 rejects writing nulls to a required column\", spark.version().startsWith(\"2\"));", "originalCommit": "a4c440d27fa3851cd65b8c19301d4e8669ad27d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc3MjUxOQ==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r446772519", "bodyText": "I see check-nullability in the Spark 3 code, though.", "author": "aokolnychyi", "createdAt": "2020-06-29T04:58:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc3MTEyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzE3MzU4NQ==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r447173585", "bodyText": "We can support it for DataFrame operations by returning a table schema with all optional columns when loading the table. But since that's more work to produce an all-optional schema, I think it should be done in a follow-up.", "author": "rdblue", "createdAt": "2020-06-29T18:36:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc3MTEyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc3MzEzMQ==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r446773131", "bodyText": "nit: seems like we actually throw an exception instead of ignoring.", "author": "aokolnychyi", "createdAt": "2020-06-29T05:01:01Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -0,0 +1,514 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.UpdateProperties;\n+import org.apache.iceberg.UpdateSchema;\n+import org.apache.iceberg.expressions.BoundPredicate;\n+import org.apache.iceberg.expressions.ExpressionVisitors;\n+import org.apache.iceberg.expressions.UnboundPredicate;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.transforms.PartitionSpecVisitor;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.spark.sql.connector.catalog.TableChange;\n+import org.apache.spark.sql.connector.expressions.Expression;\n+import org.apache.spark.sql.connector.expressions.Expressions;\n+import org.apache.spark.sql.connector.expressions.Literal;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.IntegerType;\n+import org.apache.spark.sql.types.LongType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+public class Spark3Util {\n+\n+  private static final ImmutableSet<String> LOCALITY_WHITELIST_FS = ImmutableSet.of(\"hdfs\");\n+  private static final Joiner DOT = Joiner.on(\".\");\n+\n+  private Spark3Util() {\n+  }\n+\n+  /**\n+   * Applies a list of Spark table changes to an {@link UpdateProperties} operation.\n+   * <p>\n+   * All non-property changes in the list are ignored.", "originalCommit": "a4c440d27fa3851cd65b8c19301d4e8669ad27d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzE1ODk1MA==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r447158950", "bodyText": "Oops. I'll fix this.", "author": "rdblue", "createdAt": "2020-06-29T18:10:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc3MzEzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc3MzIxMw==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r446773213", "bodyText": "nit: same here.", "author": "aokolnychyi", "createdAt": "2020-06-29T05:01:21Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -0,0 +1,514 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.UpdateProperties;\n+import org.apache.iceberg.UpdateSchema;\n+import org.apache.iceberg.expressions.BoundPredicate;\n+import org.apache.iceberg.expressions.ExpressionVisitors;\n+import org.apache.iceberg.expressions.UnboundPredicate;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.transforms.PartitionSpecVisitor;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.spark.sql.connector.catalog.TableChange;\n+import org.apache.spark.sql.connector.expressions.Expression;\n+import org.apache.spark.sql.connector.expressions.Expressions;\n+import org.apache.spark.sql.connector.expressions.Literal;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.IntegerType;\n+import org.apache.spark.sql.types.LongType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+public class Spark3Util {\n+\n+  private static final ImmutableSet<String> LOCALITY_WHITELIST_FS = ImmutableSet.of(\"hdfs\");\n+  private static final Joiner DOT = Joiner.on(\".\");\n+\n+  private Spark3Util() {\n+  }\n+\n+  /**\n+   * Applies a list of Spark table changes to an {@link UpdateProperties} operation.\n+   * <p>\n+   * All non-property changes in the list are ignored.\n+   *\n+   * @param pendingUpdate an uncommitted UpdateProperties operation to configure\n+   * @param changes a list of Spark table changes\n+   * @return the UpdateProperties operation configured with the changes\n+   */\n+  public static UpdateProperties applyPropertyChanges(UpdateProperties pendingUpdate, List<TableChange> changes) {\n+    for (TableChange change : changes) {\n+      if (change instanceof TableChange.SetProperty) {\n+        TableChange.SetProperty set = (TableChange.SetProperty) change;\n+        pendingUpdate.set(set.property(), set.value());\n+\n+      } else if (change instanceof TableChange.RemoveProperty) {\n+        TableChange.RemoveProperty remove = (TableChange.RemoveProperty) change;\n+        pendingUpdate.remove(remove.property());\n+\n+      } else {\n+        throw new UnsupportedOperationException(\"Cannot apply unknown table change: \" + change);\n+      }\n+    }\n+\n+    return pendingUpdate;\n+  }\n+\n+  /**\n+   * Applies a list of Spark table changes to an {@link UpdateSchema} operation.\n+   * <p>\n+   * All non-schema changes in the list are ignored.", "originalCommit": "a4c440d27fa3851cd65b8c19301d4e8669ad27d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc3NDYxMg==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r446774612", "bodyText": "This won't work until we expose custom bucketing function in Spark? The data has to be sorted by partition columns?", "author": "aokolnychyi", "createdAt": "2020-06-29T05:07:16Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -0,0 +1,514 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.UpdateProperties;\n+import org.apache.iceberg.UpdateSchema;\n+import org.apache.iceberg.expressions.BoundPredicate;\n+import org.apache.iceberg.expressions.ExpressionVisitors;\n+import org.apache.iceberg.expressions.UnboundPredicate;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.transforms.PartitionSpecVisitor;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.spark.sql.connector.catalog.TableChange;\n+import org.apache.spark.sql.connector.expressions.Expression;\n+import org.apache.spark.sql.connector.expressions.Expressions;\n+import org.apache.spark.sql.connector.expressions.Literal;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.IntegerType;\n+import org.apache.spark.sql.types.LongType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+public class Spark3Util {\n+\n+  private static final ImmutableSet<String> LOCALITY_WHITELIST_FS = ImmutableSet.of(\"hdfs\");\n+  private static final Joiner DOT = Joiner.on(\".\");\n+\n+  private Spark3Util() {\n+  }\n+\n+  /**\n+   * Applies a list of Spark table changes to an {@link UpdateProperties} operation.\n+   * <p>\n+   * All non-property changes in the list are ignored.\n+   *\n+   * @param pendingUpdate an uncommitted UpdateProperties operation to configure\n+   * @param changes a list of Spark table changes\n+   * @return the UpdateProperties operation configured with the changes\n+   */\n+  public static UpdateProperties applyPropertyChanges(UpdateProperties pendingUpdate, List<TableChange> changes) {\n+    for (TableChange change : changes) {\n+      if (change instanceof TableChange.SetProperty) {\n+        TableChange.SetProperty set = (TableChange.SetProperty) change;\n+        pendingUpdate.set(set.property(), set.value());\n+\n+      } else if (change instanceof TableChange.RemoveProperty) {\n+        TableChange.RemoveProperty remove = (TableChange.RemoveProperty) change;\n+        pendingUpdate.remove(remove.property());\n+\n+      } else {\n+        throw new UnsupportedOperationException(\"Cannot apply unknown table change: \" + change);\n+      }\n+    }\n+\n+    return pendingUpdate;\n+  }\n+\n+  /**\n+   * Applies a list of Spark table changes to an {@link UpdateSchema} operation.\n+   * <p>\n+   * All non-schema changes in the list are ignored.\n+   *\n+   * @param pendingUpdate an uncommitted UpdateSchema operation to configure\n+   * @param changes a list of Spark table changes\n+   * @return the UpdateSchema operation configured with the changes\n+   */\n+  public static UpdateSchema applySchemaChanges(UpdateSchema pendingUpdate, List<TableChange> changes) {\n+    for (TableChange change : changes) {\n+      if (change instanceof TableChange.AddColumn) {\n+        apply(pendingUpdate, (TableChange.AddColumn) change);\n+\n+      } else if (change instanceof TableChange.UpdateColumnType) {\n+        TableChange.UpdateColumnType update = (TableChange.UpdateColumnType) change;\n+        Type newType = SparkSchemaUtil.convert(update.newDataType());\n+        Preconditions.checkArgument(newType.isPrimitiveType(),\n+            \"Cannot update '%s', not a primitive type: %s\", DOT.join(update.fieldNames()), update.newDataType());\n+        pendingUpdate.updateColumn(DOT.join(update.fieldNames()), newType.asPrimitiveType());\n+\n+      } else if (change instanceof TableChange.UpdateColumnComment) {\n+        TableChange.UpdateColumnComment update = (TableChange.UpdateColumnComment) change;\n+        pendingUpdate.updateColumnDoc(DOT.join(update.fieldNames()), update.newComment());\n+\n+      } else if (change instanceof TableChange.RenameColumn) {\n+        TableChange.RenameColumn rename = (TableChange.RenameColumn) change;\n+        pendingUpdate.renameColumn(DOT.join(rename.fieldNames()), rename.newName());\n+\n+      } else if (change instanceof TableChange.DeleteColumn) {\n+        TableChange.DeleteColumn delete = (TableChange.DeleteColumn) change;\n+        pendingUpdate.deleteColumn(DOT.join(delete.fieldNames()));\n+\n+      } else if (change instanceof TableChange.UpdateColumnNullability) {\n+        TableChange.UpdateColumnNullability update = (TableChange.UpdateColumnNullability) change;\n+        if (update.nullable()) {\n+          pendingUpdate.makeColumnOptional(DOT.join(update.fieldNames()));\n+        } else {\n+          pendingUpdate.requireColumn(DOT.join(update.fieldNames()));\n+        }\n+\n+      } else if (change instanceof TableChange.UpdateColumnPosition) {\n+        apply(pendingUpdate, (TableChange.UpdateColumnPosition) change);\n+\n+      } else {\n+        throw new UnsupportedOperationException(\"Cannot apply unknown table change: \" + change);\n+      }\n+    }\n+\n+    return pendingUpdate;\n+  }\n+\n+  private static void apply(UpdateSchema pendingUpdate, TableChange.UpdateColumnPosition update) {\n+    Preconditions.checkArgument(update.position() != null, \"Invalid position: null\");\n+\n+    if (update.position() instanceof TableChange.After) {\n+      TableChange.After after = (TableChange.After) update.position();\n+      String referenceField = peerName(update.fieldNames(), after.column());\n+      pendingUpdate.moveAfter(DOT.join(update.fieldNames()), referenceField);\n+\n+    } else if (update.position() instanceof TableChange.First) {\n+      pendingUpdate.moveFirst(DOT.join(update.fieldNames()));\n+\n+    } else {\n+      throw new IllegalArgumentException(\"Unknown position for reorder: \" + update.position());\n+    }\n+  }\n+\n+  private static void apply(UpdateSchema pendingUpdate, TableChange.AddColumn add) {\n+    Type type = SparkSchemaUtil.convert(add.dataType());\n+    pendingUpdate.addColumn(parentName(add.fieldNames()), leafName(add.fieldNames()), type, add.comment());\n+\n+    if (add.position() instanceof TableChange.After) {\n+      TableChange.After after = (TableChange.After) add.position();\n+      String referenceField = peerName(add.fieldNames(), after.column());\n+      pendingUpdate.moveAfter(DOT.join(add.fieldNames()), referenceField);\n+\n+    } else if (add.position() instanceof TableChange.First) {\n+      pendingUpdate.moveFirst(DOT.join(add.fieldNames()));\n+\n+    } else {\n+      Preconditions.checkArgument(add.position() == null,\n+          \"Cannot add '%s' at unknown position: %s\", DOT.join(add.fieldNames()), add.position());\n+    }\n+  }\n+\n+  /**\n+   * Converts a PartitionSpec to Spark transforms.\n+   *\n+   * @param spec a PartitionSpec\n+   * @return an array of Transforms\n+   */\n+  public static Transform[] toTransforms(PartitionSpec spec) {\n+    List<Transform> transforms = PartitionSpecVisitor.visit(spec.schema(), spec,\n+        new PartitionSpecVisitor<Transform>() {\n+          @Override\n+          public Transform identity(String sourceName, int sourceId) {\n+            return Expressions.identity(sourceName);\n+          }\n+\n+          @Override\n+          public Transform bucket(String sourceName, int sourceId, int width) {\n+            return Expressions.bucket(width, sourceName);\n+          }\n+\n+          @Override\n+          public Transform truncate(String sourceName, int sourceId, int width) {\n+            return Expressions.apply(\"truncate\", Expressions.column(sourceName), Expressions.literal(width));\n+          }\n+\n+          @Override\n+          public Transform year(String sourceName, int sourceId) {\n+            return Expressions.years(sourceName);\n+          }\n+\n+          @Override\n+          public Transform month(String sourceName, int sourceId) {\n+            return Expressions.months(sourceName);\n+          }\n+\n+          @Override\n+          public Transform day(String sourceName, int sourceId) {\n+            return Expressions.days(sourceName);\n+          }\n+\n+          @Override\n+          public Transform hour(String sourceName, int sourceId) {\n+            return Expressions.hours(sourceName);\n+          }\n+        });\n+\n+    return transforms.toArray(new Transform[0]);\n+  }\n+\n+  /**\n+   * Converts Spark transforms into a {@link PartitionSpec}.\n+   *\n+   * @param schema the table schema\n+   * @param partitioning Spark Transforms\n+   * @return a PartitionSpec\n+   */\n+  public static PartitionSpec toPartitionSpec(Schema schema, Transform[] partitioning) {\n+    if (partitioning == null || partitioning.length == 0) {\n+      return PartitionSpec.unpartitioned();\n+    }\n+\n+    PartitionSpec.Builder builder = PartitionSpec.builderFor(schema);\n+    for (Transform transform : partitioning) {\n+      Preconditions.checkArgument(transform.references().length == 1,\n+          \"Cannot convert transform with more than one column reference: %s\", transform);\n+      String colName = DOT.join(transform.references()[0].fieldNames());\n+      switch (transform.name()) {\n+        case \"identity\":\n+          builder.identity(colName);\n+          break;\n+        case \"bucket\":\n+          builder.bucket(colName, findWidth(transform));", "originalCommit": "a4c440d27fa3851cd65b8c19301d4e8669ad27d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc4NjcxOQ==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r446786719", "bodyText": "I see below that we can make this work using UDFs.", "author": "aokolnychyi", "createdAt": "2020-06-29T05:51:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc3NDYxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzE1OTI3OQ==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r447159279", "bodyText": "Yes, we have an example where you can use a UDF for this.", "author": "rdblue", "createdAt": "2020-06-29T18:11:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc3NDYxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc3NTU5NA==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r446775594", "bodyText": "This is Iceberg specific alias that we can consume via ApplyTransform?", "author": "aokolnychyi", "createdAt": "2020-06-29T05:11:13Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -0,0 +1,514 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.UpdateProperties;\n+import org.apache.iceberg.UpdateSchema;\n+import org.apache.iceberg.expressions.BoundPredicate;\n+import org.apache.iceberg.expressions.ExpressionVisitors;\n+import org.apache.iceberg.expressions.UnboundPredicate;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.transforms.PartitionSpecVisitor;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.spark.sql.connector.catalog.TableChange;\n+import org.apache.spark.sql.connector.expressions.Expression;\n+import org.apache.spark.sql.connector.expressions.Expressions;\n+import org.apache.spark.sql.connector.expressions.Literal;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.IntegerType;\n+import org.apache.spark.sql.types.LongType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+public class Spark3Util {\n+\n+  private static final ImmutableSet<String> LOCALITY_WHITELIST_FS = ImmutableSet.of(\"hdfs\");\n+  private static final Joiner DOT = Joiner.on(\".\");\n+\n+  private Spark3Util() {\n+  }\n+\n+  /**\n+   * Applies a list of Spark table changes to an {@link UpdateProperties} operation.\n+   * <p>\n+   * All non-property changes in the list are ignored.\n+   *\n+   * @param pendingUpdate an uncommitted UpdateProperties operation to configure\n+   * @param changes a list of Spark table changes\n+   * @return the UpdateProperties operation configured with the changes\n+   */\n+  public static UpdateProperties applyPropertyChanges(UpdateProperties pendingUpdate, List<TableChange> changes) {\n+    for (TableChange change : changes) {\n+      if (change instanceof TableChange.SetProperty) {\n+        TableChange.SetProperty set = (TableChange.SetProperty) change;\n+        pendingUpdate.set(set.property(), set.value());\n+\n+      } else if (change instanceof TableChange.RemoveProperty) {\n+        TableChange.RemoveProperty remove = (TableChange.RemoveProperty) change;\n+        pendingUpdate.remove(remove.property());\n+\n+      } else {\n+        throw new UnsupportedOperationException(\"Cannot apply unknown table change: \" + change);\n+      }\n+    }\n+\n+    return pendingUpdate;\n+  }\n+\n+  /**\n+   * Applies a list of Spark table changes to an {@link UpdateSchema} operation.\n+   * <p>\n+   * All non-schema changes in the list are ignored.\n+   *\n+   * @param pendingUpdate an uncommitted UpdateSchema operation to configure\n+   * @param changes a list of Spark table changes\n+   * @return the UpdateSchema operation configured with the changes\n+   */\n+  public static UpdateSchema applySchemaChanges(UpdateSchema pendingUpdate, List<TableChange> changes) {\n+    for (TableChange change : changes) {\n+      if (change instanceof TableChange.AddColumn) {\n+        apply(pendingUpdate, (TableChange.AddColumn) change);\n+\n+      } else if (change instanceof TableChange.UpdateColumnType) {\n+        TableChange.UpdateColumnType update = (TableChange.UpdateColumnType) change;\n+        Type newType = SparkSchemaUtil.convert(update.newDataType());\n+        Preconditions.checkArgument(newType.isPrimitiveType(),\n+            \"Cannot update '%s', not a primitive type: %s\", DOT.join(update.fieldNames()), update.newDataType());\n+        pendingUpdate.updateColumn(DOT.join(update.fieldNames()), newType.asPrimitiveType());\n+\n+      } else if (change instanceof TableChange.UpdateColumnComment) {\n+        TableChange.UpdateColumnComment update = (TableChange.UpdateColumnComment) change;\n+        pendingUpdate.updateColumnDoc(DOT.join(update.fieldNames()), update.newComment());\n+\n+      } else if (change instanceof TableChange.RenameColumn) {\n+        TableChange.RenameColumn rename = (TableChange.RenameColumn) change;\n+        pendingUpdate.renameColumn(DOT.join(rename.fieldNames()), rename.newName());\n+\n+      } else if (change instanceof TableChange.DeleteColumn) {\n+        TableChange.DeleteColumn delete = (TableChange.DeleteColumn) change;\n+        pendingUpdate.deleteColumn(DOT.join(delete.fieldNames()));\n+\n+      } else if (change instanceof TableChange.UpdateColumnNullability) {\n+        TableChange.UpdateColumnNullability update = (TableChange.UpdateColumnNullability) change;\n+        if (update.nullable()) {\n+          pendingUpdate.makeColumnOptional(DOT.join(update.fieldNames()));\n+        } else {\n+          pendingUpdate.requireColumn(DOT.join(update.fieldNames()));\n+        }\n+\n+      } else if (change instanceof TableChange.UpdateColumnPosition) {\n+        apply(pendingUpdate, (TableChange.UpdateColumnPosition) change);\n+\n+      } else {\n+        throw new UnsupportedOperationException(\"Cannot apply unknown table change: \" + change);\n+      }\n+    }\n+\n+    return pendingUpdate;\n+  }\n+\n+  private static void apply(UpdateSchema pendingUpdate, TableChange.UpdateColumnPosition update) {\n+    Preconditions.checkArgument(update.position() != null, \"Invalid position: null\");\n+\n+    if (update.position() instanceof TableChange.After) {\n+      TableChange.After after = (TableChange.After) update.position();\n+      String referenceField = peerName(update.fieldNames(), after.column());\n+      pendingUpdate.moveAfter(DOT.join(update.fieldNames()), referenceField);\n+\n+    } else if (update.position() instanceof TableChange.First) {\n+      pendingUpdate.moveFirst(DOT.join(update.fieldNames()));\n+\n+    } else {\n+      throw new IllegalArgumentException(\"Unknown position for reorder: \" + update.position());\n+    }\n+  }\n+\n+  private static void apply(UpdateSchema pendingUpdate, TableChange.AddColumn add) {\n+    Type type = SparkSchemaUtil.convert(add.dataType());\n+    pendingUpdate.addColumn(parentName(add.fieldNames()), leafName(add.fieldNames()), type, add.comment());\n+\n+    if (add.position() instanceof TableChange.After) {\n+      TableChange.After after = (TableChange.After) add.position();\n+      String referenceField = peerName(add.fieldNames(), after.column());\n+      pendingUpdate.moveAfter(DOT.join(add.fieldNames()), referenceField);\n+\n+    } else if (add.position() instanceof TableChange.First) {\n+      pendingUpdate.moveFirst(DOT.join(add.fieldNames()));\n+\n+    } else {\n+      Preconditions.checkArgument(add.position() == null,\n+          \"Cannot add '%s' at unknown position: %s\", DOT.join(add.fieldNames()), add.position());\n+    }\n+  }\n+\n+  /**\n+   * Converts a PartitionSpec to Spark transforms.\n+   *\n+   * @param spec a PartitionSpec\n+   * @return an array of Transforms\n+   */\n+  public static Transform[] toTransforms(PartitionSpec spec) {\n+    List<Transform> transforms = PartitionSpecVisitor.visit(spec.schema(), spec,\n+        new PartitionSpecVisitor<Transform>() {\n+          @Override\n+          public Transform identity(String sourceName, int sourceId) {\n+            return Expressions.identity(sourceName);\n+          }\n+\n+          @Override\n+          public Transform bucket(String sourceName, int sourceId, int width) {\n+            return Expressions.bucket(width, sourceName);\n+          }\n+\n+          @Override\n+          public Transform truncate(String sourceName, int sourceId, int width) {\n+            return Expressions.apply(\"truncate\", Expressions.column(sourceName), Expressions.literal(width));\n+          }\n+\n+          @Override\n+          public Transform year(String sourceName, int sourceId) {\n+            return Expressions.years(sourceName);\n+          }\n+\n+          @Override\n+          public Transform month(String sourceName, int sourceId) {\n+            return Expressions.months(sourceName);\n+          }\n+\n+          @Override\n+          public Transform day(String sourceName, int sourceId) {\n+            return Expressions.days(sourceName);\n+          }\n+\n+          @Override\n+          public Transform hour(String sourceName, int sourceId) {\n+            return Expressions.hours(sourceName);\n+          }\n+        });\n+\n+    return transforms.toArray(new Transform[0]);\n+  }\n+\n+  /**\n+   * Converts Spark transforms into a {@link PartitionSpec}.\n+   *\n+   * @param schema the table schema\n+   * @param partitioning Spark Transforms\n+   * @return a PartitionSpec\n+   */\n+  public static PartitionSpec toPartitionSpec(Schema schema, Transform[] partitioning) {\n+    if (partitioning == null || partitioning.length == 0) {\n+      return PartitionSpec.unpartitioned();\n+    }\n+\n+    PartitionSpec.Builder builder = PartitionSpec.builderFor(schema);\n+    for (Transform transform : partitioning) {\n+      Preconditions.checkArgument(transform.references().length == 1,\n+          \"Cannot convert transform with more than one column reference: %s\", transform);\n+      String colName = DOT.join(transform.references()[0].fieldNames());\n+      switch (transform.name()) {\n+        case \"identity\":\n+          builder.identity(colName);\n+          break;\n+        case \"bucket\":\n+          builder.bucket(colName, findWidth(transform));\n+          break;\n+        case \"years\":\n+          builder.year(colName);\n+          break;\n+        case \"months\":\n+          builder.month(colName);\n+          break;\n+        case \"date\":\n+        case \"days\":\n+          builder.day(colName);\n+          break;\n+        case \"date_hour\":", "originalCommit": "a4c440d27fa3851cd65b8c19301d4e8669ad27d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzE1OTk3NA==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r447159974", "bodyText": "This was what we called ours internally before the Spark community decided to go with hours. Using hour doesn't quite describe it.\nMy thinking is that more alternatives are good here, but I can remove it if you'd like.", "author": "rdblue", "createdAt": "2020-06-29T18:12:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc3NTU5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI1MjYzMQ==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r447252631", "bodyText": "No-no, it is just for me to understand.", "author": "aokolnychyi", "createdAt": "2020-06-29T21:03:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc3NTU5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc3NzUxMg==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r446777512", "bodyText": "It seems this one is not yet used. DS V2 doesn't support describe now?", "author": "aokolnychyi", "createdAt": "2020-06-29T05:19:38Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -0,0 +1,514 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.UpdateProperties;\n+import org.apache.iceberg.UpdateSchema;\n+import org.apache.iceberg.expressions.BoundPredicate;\n+import org.apache.iceberg.expressions.ExpressionVisitors;\n+import org.apache.iceberg.expressions.UnboundPredicate;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.transforms.PartitionSpecVisitor;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.spark.sql.connector.catalog.TableChange;\n+import org.apache.spark.sql.connector.expressions.Expression;\n+import org.apache.spark.sql.connector.expressions.Expressions;\n+import org.apache.spark.sql.connector.expressions.Literal;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.IntegerType;\n+import org.apache.spark.sql.types.LongType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+public class Spark3Util {\n+\n+  private static final ImmutableSet<String> LOCALITY_WHITELIST_FS = ImmutableSet.of(\"hdfs\");\n+  private static final Joiner DOT = Joiner.on(\".\");\n+\n+  private Spark3Util() {\n+  }\n+\n+  /**\n+   * Applies a list of Spark table changes to an {@link UpdateProperties} operation.\n+   * <p>\n+   * All non-property changes in the list are ignored.\n+   *\n+   * @param pendingUpdate an uncommitted UpdateProperties operation to configure\n+   * @param changes a list of Spark table changes\n+   * @return the UpdateProperties operation configured with the changes\n+   */\n+  public static UpdateProperties applyPropertyChanges(UpdateProperties pendingUpdate, List<TableChange> changes) {\n+    for (TableChange change : changes) {\n+      if (change instanceof TableChange.SetProperty) {\n+        TableChange.SetProperty set = (TableChange.SetProperty) change;\n+        pendingUpdate.set(set.property(), set.value());\n+\n+      } else if (change instanceof TableChange.RemoveProperty) {\n+        TableChange.RemoveProperty remove = (TableChange.RemoveProperty) change;\n+        pendingUpdate.remove(remove.property());\n+\n+      } else {\n+        throw new UnsupportedOperationException(\"Cannot apply unknown table change: \" + change);\n+      }\n+    }\n+\n+    return pendingUpdate;\n+  }\n+\n+  /**\n+   * Applies a list of Spark table changes to an {@link UpdateSchema} operation.\n+   * <p>\n+   * All non-schema changes in the list are ignored.\n+   *\n+   * @param pendingUpdate an uncommitted UpdateSchema operation to configure\n+   * @param changes a list of Spark table changes\n+   * @return the UpdateSchema operation configured with the changes\n+   */\n+  public static UpdateSchema applySchemaChanges(UpdateSchema pendingUpdate, List<TableChange> changes) {\n+    for (TableChange change : changes) {\n+      if (change instanceof TableChange.AddColumn) {\n+        apply(pendingUpdate, (TableChange.AddColumn) change);\n+\n+      } else if (change instanceof TableChange.UpdateColumnType) {\n+        TableChange.UpdateColumnType update = (TableChange.UpdateColumnType) change;\n+        Type newType = SparkSchemaUtil.convert(update.newDataType());\n+        Preconditions.checkArgument(newType.isPrimitiveType(),\n+            \"Cannot update '%s', not a primitive type: %s\", DOT.join(update.fieldNames()), update.newDataType());\n+        pendingUpdate.updateColumn(DOT.join(update.fieldNames()), newType.asPrimitiveType());\n+\n+      } else if (change instanceof TableChange.UpdateColumnComment) {\n+        TableChange.UpdateColumnComment update = (TableChange.UpdateColumnComment) change;\n+        pendingUpdate.updateColumnDoc(DOT.join(update.fieldNames()), update.newComment());\n+\n+      } else if (change instanceof TableChange.RenameColumn) {\n+        TableChange.RenameColumn rename = (TableChange.RenameColumn) change;\n+        pendingUpdate.renameColumn(DOT.join(rename.fieldNames()), rename.newName());\n+\n+      } else if (change instanceof TableChange.DeleteColumn) {\n+        TableChange.DeleteColumn delete = (TableChange.DeleteColumn) change;\n+        pendingUpdate.deleteColumn(DOT.join(delete.fieldNames()));\n+\n+      } else if (change instanceof TableChange.UpdateColumnNullability) {\n+        TableChange.UpdateColumnNullability update = (TableChange.UpdateColumnNullability) change;\n+        if (update.nullable()) {\n+          pendingUpdate.makeColumnOptional(DOT.join(update.fieldNames()));\n+        } else {\n+          pendingUpdate.requireColumn(DOT.join(update.fieldNames()));\n+        }\n+\n+      } else if (change instanceof TableChange.UpdateColumnPosition) {\n+        apply(pendingUpdate, (TableChange.UpdateColumnPosition) change);\n+\n+      } else {\n+        throw new UnsupportedOperationException(\"Cannot apply unknown table change: \" + change);\n+      }\n+    }\n+\n+    return pendingUpdate;\n+  }\n+\n+  private static void apply(UpdateSchema pendingUpdate, TableChange.UpdateColumnPosition update) {\n+    Preconditions.checkArgument(update.position() != null, \"Invalid position: null\");\n+\n+    if (update.position() instanceof TableChange.After) {\n+      TableChange.After after = (TableChange.After) update.position();\n+      String referenceField = peerName(update.fieldNames(), after.column());\n+      pendingUpdate.moveAfter(DOT.join(update.fieldNames()), referenceField);\n+\n+    } else if (update.position() instanceof TableChange.First) {\n+      pendingUpdate.moveFirst(DOT.join(update.fieldNames()));\n+\n+    } else {\n+      throw new IllegalArgumentException(\"Unknown position for reorder: \" + update.position());\n+    }\n+  }\n+\n+  private static void apply(UpdateSchema pendingUpdate, TableChange.AddColumn add) {\n+    Type type = SparkSchemaUtil.convert(add.dataType());\n+    pendingUpdate.addColumn(parentName(add.fieldNames()), leafName(add.fieldNames()), type, add.comment());\n+\n+    if (add.position() instanceof TableChange.After) {\n+      TableChange.After after = (TableChange.After) add.position();\n+      String referenceField = peerName(add.fieldNames(), after.column());\n+      pendingUpdate.moveAfter(DOT.join(add.fieldNames()), referenceField);\n+\n+    } else if (add.position() instanceof TableChange.First) {\n+      pendingUpdate.moveFirst(DOT.join(add.fieldNames()));\n+\n+    } else {\n+      Preconditions.checkArgument(add.position() == null,\n+          \"Cannot add '%s' at unknown position: %s\", DOT.join(add.fieldNames()), add.position());\n+    }\n+  }\n+\n+  /**\n+   * Converts a PartitionSpec to Spark transforms.\n+   *\n+   * @param spec a PartitionSpec\n+   * @return an array of Transforms\n+   */\n+  public static Transform[] toTransforms(PartitionSpec spec) {\n+    List<Transform> transforms = PartitionSpecVisitor.visit(spec.schema(), spec,\n+        new PartitionSpecVisitor<Transform>() {\n+          @Override\n+          public Transform identity(String sourceName, int sourceId) {\n+            return Expressions.identity(sourceName);\n+          }\n+\n+          @Override\n+          public Transform bucket(String sourceName, int sourceId, int width) {\n+            return Expressions.bucket(width, sourceName);\n+          }\n+\n+          @Override\n+          public Transform truncate(String sourceName, int sourceId, int width) {\n+            return Expressions.apply(\"truncate\", Expressions.column(sourceName), Expressions.literal(width));\n+          }\n+\n+          @Override\n+          public Transform year(String sourceName, int sourceId) {\n+            return Expressions.years(sourceName);\n+          }\n+\n+          @Override\n+          public Transform month(String sourceName, int sourceId) {\n+            return Expressions.months(sourceName);\n+          }\n+\n+          @Override\n+          public Transform day(String sourceName, int sourceId) {\n+            return Expressions.days(sourceName);\n+          }\n+\n+          @Override\n+          public Transform hour(String sourceName, int sourceId) {\n+            return Expressions.hours(sourceName);\n+          }\n+        });\n+\n+    return transforms.toArray(new Transform[0]);\n+  }\n+\n+  /**\n+   * Converts Spark transforms into a {@link PartitionSpec}.\n+   *\n+   * @param schema the table schema\n+   * @param partitioning Spark Transforms\n+   * @return a PartitionSpec\n+   */\n+  public static PartitionSpec toPartitionSpec(Schema schema, Transform[] partitioning) {\n+    if (partitioning == null || partitioning.length == 0) {\n+      return PartitionSpec.unpartitioned();\n+    }\n+\n+    PartitionSpec.Builder builder = PartitionSpec.builderFor(schema);\n+    for (Transform transform : partitioning) {\n+      Preconditions.checkArgument(transform.references().length == 1,\n+          \"Cannot convert transform with more than one column reference: %s\", transform);\n+      String colName = DOT.join(transform.references()[0].fieldNames());\n+      switch (transform.name()) {\n+        case \"identity\":\n+          builder.identity(colName);\n+          break;\n+        case \"bucket\":\n+          builder.bucket(colName, findWidth(transform));\n+          break;\n+        case \"years\":\n+          builder.year(colName);\n+          break;\n+        case \"months\":\n+          builder.month(colName);\n+          break;\n+        case \"date\":\n+        case \"days\":\n+          builder.day(colName);\n+          break;\n+        case \"date_hour\":\n+        case \"hours\":\n+          builder.hour(colName);\n+          break;\n+        case \"truncate\":\n+          builder.truncate(colName, findWidth(transform));\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\"Transform is not supported: \" + transform);\n+      }\n+    }\n+\n+    return builder.build();\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private static int findWidth(Transform transform) {\n+    for (Expression expr : transform.arguments()) {\n+      if (expr instanceof Literal) {\n+        if (((Literal) expr).dataType() instanceof IntegerType) {\n+          Literal<Integer> lit = (Literal<Integer>) expr;\n+          Preconditions.checkArgument(lit.value() > 0,\n+              \"Unsupported width for transform: %s\", transform.describe());\n+          return lit.value();\n+\n+        } else if (((Literal) expr).dataType() instanceof LongType) {\n+          Literal<Long> lit = (Literal<Long>) expr;\n+          Preconditions.checkArgument(lit.value() > 0 && lit.value() < Integer.MAX_VALUE,\n+              \"Unsupported width for transform: %s\", transform.describe());\n+          if (lit.value() > Integer.MAX_VALUE) {\n+            throw new IllegalArgumentException();\n+          }\n+          return lit.value().intValue();\n+        }\n+      }\n+    }\n+\n+    throw new IllegalArgumentException(\"Cannot find width for transform: \" + transform.describe());\n+  }\n+\n+  private static String leafName(String[] fieldNames) {\n+    Preconditions.checkArgument(fieldNames.length > 0, \"Invalid field name: at least one name is required\");\n+    return fieldNames[fieldNames.length - 1];\n+  }\n+\n+  private static String peerName(String[] fieldNames, String fieldName) {\n+    if (fieldNames.length > 1) {\n+      String[] peerNames = Arrays.copyOf(fieldNames, fieldNames.length);\n+      peerNames[fieldNames.length - 1] = fieldName;\n+      return DOT.join(peerNames);\n+    }\n+    return fieldName;\n+  }\n+\n+  private static String parentName(String[] fieldNames) {\n+    if (fieldNames.length > 1) {\n+      return DOT.join(Arrays.copyOfRange(fieldNames, 0, fieldNames.length - 1));\n+    }\n+    return null;\n+  }\n+\n+  public static String describe(org.apache.iceberg.expressions.Expression expr) {\n+    return ExpressionVisitors.visit(expr, DescribeExpressionVisitor.INSTANCE);\n+  }\n+\n+  public static String describe(Schema schema) {", "originalCommit": "a4c440d27fa3851cd65b8c19301d4e8669ad27d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc3ODI4OA==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r446778288", "bodyText": "Will we implement this in a follow-up?", "author": "aokolnychyi", "createdAt": "2020-06-29T05:22:19Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/SparkCatalog.java", "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.CachingCatalog;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.Transaction;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hive.HiveCatalog;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.iceberg.spark.source.StagedSparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableChange;\n+import org.apache.spark.sql.connector.catalog.TableChange.ColumnChange;\n+import org.apache.spark.sql.connector.catalog.TableChange.RemoveProperty;\n+import org.apache.spark.sql.connector.catalog.TableChange.SetProperty;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+/**\n+ * A Spark TableCatalog implementation that wraps Iceberg's {@link Catalog} interface.\n+ */\n+public class SparkCatalog implements StagingTableCatalog {\n+  private String catalogName = null;\n+  private Catalog icebergCatalog = null;\n+\n+  /**\n+   * Build an Iceberg {@link Catalog} to be used by this Spark catalog adapter.\n+   *\n+   * @param name Spark's catalog name\n+   * @param options Spark's catalog options\n+   * @return an Iceberg catalog\n+   */\n+  protected Catalog buildIcebergCatalog(String name, CaseInsensitiveStringMap options) {\n+    // TODO: add name to catalogs\n+    Configuration conf = SparkSession.active().sparkContext().hadoopConfiguration();\n+    String catalogType = options.getOrDefault(\"type\", \"hive\");\n+    switch (catalogType) {\n+      case \"hive\":\n+        int clientPoolSize = options.getInt(\"clients\", 2);\n+        String uri = options.get(\"uri\");\n+        return new HiveCatalog(uri, clientPoolSize, conf);\n+\n+      case \"hadoop\":\n+        String warehouseLocation = options.get(\"warehouse\");\n+        return new HadoopCatalog(conf, warehouseLocation);\n+\n+      default:\n+        throw new UnsupportedOperationException(\"Unknown catalog type: \" + catalogType);\n+    }\n+  }\n+\n+  /**\n+   * Build an Iceberg {@link TableIdentifier} for the given Spark identifier.\n+   *\n+   * @param identifier Spark's identifier\n+   * @return an Iceberg identifier\n+   */\n+  protected TableIdentifier buildIdentifier(Identifier identifier) {\n+    return TableIdentifier.of(Namespace.of(identifier.namespace()), identifier.name());\n+  }\n+\n+  @Override\n+  public Identifier[] listTables(String[] namespace) {\n+    // TODO: handle namespaces", "originalCommit": "a4c440d27fa3851cd65b8c19301d4e8669ad27d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzE2MDEwNA==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r447160104", "bodyText": "Yes. I already have a follow-up prepared.", "author": "rdblue", "createdAt": "2020-06-29T18:12:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc3ODI4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc3ODc3MQ==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r446778771", "bodyText": "nit: will it make sense to put the throws part to the line above? It should fit.", "author": "aokolnychyi", "createdAt": "2020-06-29T05:24:13Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/SparkCatalog.java", "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.CachingCatalog;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.Transaction;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hive.HiveCatalog;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.iceberg.spark.source.StagedSparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableChange;\n+import org.apache.spark.sql.connector.catalog.TableChange.ColumnChange;\n+import org.apache.spark.sql.connector.catalog.TableChange.RemoveProperty;\n+import org.apache.spark.sql.connector.catalog.TableChange.SetProperty;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+/**\n+ * A Spark TableCatalog implementation that wraps Iceberg's {@link Catalog} interface.\n+ */\n+public class SparkCatalog implements StagingTableCatalog {\n+  private String catalogName = null;\n+  private Catalog icebergCatalog = null;\n+\n+  /**\n+   * Build an Iceberg {@link Catalog} to be used by this Spark catalog adapter.\n+   *\n+   * @param name Spark's catalog name\n+   * @param options Spark's catalog options\n+   * @return an Iceberg catalog\n+   */\n+  protected Catalog buildIcebergCatalog(String name, CaseInsensitiveStringMap options) {\n+    // TODO: add name to catalogs\n+    Configuration conf = SparkSession.active().sparkContext().hadoopConfiguration();\n+    String catalogType = options.getOrDefault(\"type\", \"hive\");\n+    switch (catalogType) {\n+      case \"hive\":\n+        int clientPoolSize = options.getInt(\"clients\", 2);\n+        String uri = options.get(\"uri\");\n+        return new HiveCatalog(uri, clientPoolSize, conf);\n+\n+      case \"hadoop\":\n+        String warehouseLocation = options.get(\"warehouse\");\n+        return new HadoopCatalog(conf, warehouseLocation);\n+\n+      default:\n+        throw new UnsupportedOperationException(\"Unknown catalog type: \" + catalogType);\n+    }\n+  }\n+\n+  /**\n+   * Build an Iceberg {@link TableIdentifier} for the given Spark identifier.\n+   *\n+   * @param identifier Spark's identifier\n+   * @return an Iceberg identifier\n+   */\n+  protected TableIdentifier buildIdentifier(Identifier identifier) {\n+    return TableIdentifier.of(Namespace.of(identifier.namespace()), identifier.name());\n+  }\n+\n+  @Override\n+  public Identifier[] listTables(String[] namespace) {\n+    // TODO: handle namespaces\n+    return new Identifier[0];\n+  }\n+\n+  @Override\n+  public SparkTable loadTable(Identifier ident) throws NoSuchTableException {\n+    try {\n+      return new SparkTable(icebergCatalog.loadTable(buildIdentifier(ident)));\n+    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {\n+      throw new NoSuchTableException(ident);\n+    }\n+  }\n+\n+  @Override\n+  public SparkTable createTable(Identifier ident, StructType schema,\n+                                Transform[] transforms,\n+                                Map<String, String> properties)\n+      throws TableAlreadyExistsException {", "originalCommit": "a4c440d27fa3851cd65b8c19301d4e8669ad27d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc4MTc5Mg==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r446781792", "bodyText": "nit: can be on one line", "author": "aokolnychyi", "createdAt": "2020-06-29T05:34:55Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalog;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.catalog.TableProvider;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.sources.DataSourceRegister;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+public class IcebergSource implements DataSourceRegister, TableProvider {\n+  @Override\n+  public String shortName() {\n+    return \"iceberg\";\n+  }\n+\n+  @Override\n+  public StructType inferSchema(CaseInsensitiveStringMap options) {\n+    return null;\n+  }\n+\n+  @Override\n+  public Transform[] inferPartitioning(CaseInsensitiveStringMap options) {\n+    return getTable(null, null, options).partitioning();\n+  }\n+\n+  @Override\n+  public boolean supportsExternalMetadata() {\n+    return true;\n+  }\n+\n+  @Override\n+  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    // TODO: if partitioning is non-null, the table is being created?\n+    // Get Iceberg table from options\n+    Configuration conf = new Configuration(SparkSession.active().sparkContext().hadoopConfiguration());\n+    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n+\n+    // Build Spark table based on Iceberg table, and return it\n+    if (schema != null) {\n+      return new SparkTable(icebergTable, schema);\n+    } else {\n+      return new SparkTable(icebergTable);\n+    }\n+  }\n+\n+  protected Table findTable(Map<String, String> options, Configuration conf) {\n+    Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n+    String path = options.get(\"path\");\n+\n+    if (path.contains(\"/\")) {\n+      HadoopTables tables = new HadoopTables(conf);\n+      return tables.load(path);\n+    } else {\n+      HiveCatalog hiveCatalog = HiveCatalogs.loadCatalog(conf);\n+      TableIdentifier tableIdentifier = TableIdentifier.parse(path);\n+      return hiveCatalog.loadTable(tableIdentifier);\n+    }\n+  }\n+\n+  private Table getTableAndResolveHadoopConfiguration(", "originalCommit": "a4c440d27fa3851cd65b8c19301d4e8669ad27d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc4MzcwNA==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r446783704", "bodyText": "nit: can be on one line", "author": "aokolnychyi", "createdAt": "2020-06-29T05:41:41Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalog;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.catalog.TableProvider;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.sources.DataSourceRegister;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+public class IcebergSource implements DataSourceRegister, TableProvider {\n+  @Override\n+  public String shortName() {\n+    return \"iceberg\";\n+  }\n+\n+  @Override\n+  public StructType inferSchema(CaseInsensitiveStringMap options) {\n+    return null;\n+  }\n+\n+  @Override\n+  public Transform[] inferPartitioning(CaseInsensitiveStringMap options) {\n+    return getTable(null, null, options).partitioning();\n+  }\n+\n+  @Override\n+  public boolean supportsExternalMetadata() {\n+    return true;\n+  }\n+\n+  @Override\n+  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    // TODO: if partitioning is non-null, the table is being created?\n+    // Get Iceberg table from options\n+    Configuration conf = new Configuration(SparkSession.active().sparkContext().hadoopConfiguration());\n+    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n+\n+    // Build Spark table based on Iceberg table, and return it\n+    if (schema != null) {\n+      return new SparkTable(icebergTable, schema);\n+    } else {\n+      return new SparkTable(icebergTable);\n+    }\n+  }\n+\n+  protected Table findTable(Map<String, String> options, Configuration conf) {\n+    Preconditions.checkArgument(options.containsKey(\"path\"), \"Cannot open table: path is not set\");\n+    String path = options.get(\"path\");\n+\n+    if (path.contains(\"/\")) {\n+      HadoopTables tables = new HadoopTables(conf);\n+      return tables.load(path);\n+    } else {\n+      HiveCatalog hiveCatalog = HiveCatalogs.loadCatalog(conf);\n+      TableIdentifier tableIdentifier = TableIdentifier.parse(path);\n+      return hiveCatalog.loadTable(tableIdentifier);\n+    }\n+  }\n+\n+  private Table getTableAndResolveHadoopConfiguration(\n+      Map<String, String> options, Configuration conf) {\n+    // Overwrite configurations from the Spark Context with configurations from the options.\n+    mergeIcebergHadoopConfs(conf, options);\n+\n+    Table table = findTable(options, conf);\n+\n+    // Set confs from table properties\n+    mergeIcebergHadoopConfs(conf, table.properties());\n+\n+    // Re-overwrite values set in options and table properties but were not in the environment.\n+    mergeIcebergHadoopConfs(conf, options);\n+\n+    return table;\n+  }\n+\n+  private static void mergeIcebergHadoopConfs(\n+      Configuration baseConf, Map<String, String> options) {", "originalCommit": "a4c440d27fa3851cd65b8c19301d4e8669ad27d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc4NTc4NA==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r446785784", "bodyText": "nit: do we need this if as SparkTable handles null correctly?", "author": "aokolnychyi", "createdAt": "2020-06-29T05:48:49Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java", "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalog;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.catalog.TableProvider;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.sources.DataSourceRegister;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+public class IcebergSource implements DataSourceRegister, TableProvider {\n+  @Override\n+  public String shortName() {\n+    return \"iceberg\";\n+  }\n+\n+  @Override\n+  public StructType inferSchema(CaseInsensitiveStringMap options) {\n+    return null;\n+  }\n+\n+  @Override\n+  public Transform[] inferPartitioning(CaseInsensitiveStringMap options) {\n+    return getTable(null, null, options).partitioning();\n+  }\n+\n+  @Override\n+  public boolean supportsExternalMetadata() {\n+    return true;\n+  }\n+\n+  @Override\n+  public SparkTable getTable(StructType schema, Transform[] partitioning, Map<String, String> options) {\n+    // TODO: if partitioning is non-null, the table is being created?\n+    // Get Iceberg table from options\n+    Configuration conf = new Configuration(SparkSession.active().sparkContext().hadoopConfiguration());\n+    Table icebergTable = getTableAndResolveHadoopConfiguration(options, conf);\n+\n+    // Build Spark table based on Iceberg table, and return it\n+    if (schema != null) {", "originalCommit": "a4c440d27fa3851cd65b8c19301d4e8669ad27d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI3MzA0OQ==", "url": "https://github.com/apache/iceberg/pull/1124#discussion_r447273049", "bodyText": "I'll remove this.", "author": "rdblue", "createdAt": "2020-06-29T21:46:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njc4NTc4NA=="}], "type": "inlineReview"}, {"oid": "52c6d6c2685496550f8fc0477ec3b3e22b98fccc", "url": "https://github.com/apache/iceberg/commit/52c6d6c2685496550f8fc0477ec3b3e22b98fccc", "message": "Fix review comments.", "committedDate": "2020-06-29T21:46:56Z", "type": "commit"}]}