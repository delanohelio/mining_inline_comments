{"pr_number": 1444, "pr_title": "Spark: Apply row-level delete files when reading", "pr_createdAt": "2020-09-11T16:54:42Z", "pr_url": "https://github.com/apache/iceberg/pull/1444", "timeline": [{"oid": "acea9bd174be0961167a3c5d9a58401b45dfd401", "url": "https://github.com/apache/iceberg/commit/acea9bd174be0961167a3c5d9a58401b45dfd401", "message": "Apply v2 row-level deletes in Spark row-based reads.", "committedDate": "2020-09-11T16:46:25Z", "type": "commit"}, {"oid": "acbecc9d4bd213ffe3b0848d5bd200f9bbfdd363", "url": "https://github.com/apache/iceberg/commit/acbecc9d4bd213ffe3b0848d5bd200f9bbfdd363", "message": "Decrypt delete files in Spark using the decryption manager.", "committedDate": "2020-09-11T16:47:28Z", "type": "commit"}, {"oid": "61e0840fba718dcdf8f105896cba6dc5c0bcd14e", "url": "https://github.com/apache/iceberg/commit/61e0840fba718dcdf8f105896cba6dc5c0bcd14e", "message": "Do not produce a new required schema if it is not needed.", "committedDate": "2020-09-11T16:48:24Z", "type": "commit"}, {"oid": "8d2ff5dfe44f9c23d1beef3fddec3c3d10e71770", "url": "https://github.com/apache/iceberg/commit/8d2ff5dfe44f9c23d1beef3fddec3c3d10e71770", "message": "Do not use vectorized read path when applying delete files.", "committedDate": "2020-09-11T16:59:27Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzE3NjY3OQ==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r487176679", "bodyText": "@shardulm94, @rdsr, this is a separate public class so that we can use it in IcebergInputFormat. It should be fairly easy to apply the deletes when using generics to read.", "author": "rdblue", "createdAt": "2020-09-11T17:07:10Z", "path": "data/src/main/java/org/apache/iceberg/data/GenericDeleteFilter.java", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.iceberg.data;\n+\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+\n+public class GenericDeleteFilter extends DeleteFilter<Record> {", "originalCommit": "8d2ff5dfe44f9c23d1beef3fddec3c3d10e71770", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "e2d055ccc460afad303bc10409da318e9c9243d8", "url": "https://github.com/apache/iceberg/commit/e2d055ccc460afad303bc10409da318e9c9243d8", "message": "Fix license headers.", "committedDate": "2020-09-11T17:15:28Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI0NjI5MA==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r487246290", "bodyText": "Could be stream.anyMatch(TableScanUtil::hasDeletes) , but I think that's just a personal style decision.", "author": "RussellSpitzer", "createdAt": "2020-09-11T19:25:46Z", "path": "core/src/main/java/org/apache/iceberg/util/TableScanUtil.java", "diffHunk": "@@ -31,6 +31,20 @@\n   private TableScanUtil() {\n   }\n \n+  public static boolean hasDeletes(CombinedScanTask task) {\n+    for (FileScanTask fileTask : task.files()) {", "originalCommit": "e2d055ccc460afad303bc10409da318e9c9243d8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI1MTY4Mw==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r487251683", "bodyText": "nit: Cannot read %s delete file: %s\nJust to note that the problem isn't that the file can't be read, but that the file type is not supported", "author": "RussellSpitzer", "createdAt": "2020-09-11T19:38:25Z", "path": "data/src/main/java/org/apache/iceberg/data/DeleteFilter.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Accessor;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.deletes.Deletes;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.apache.iceberg.util.StructProjection;\n+import org.apache.parquet.Preconditions;\n+\n+public abstract class DeleteFilter<T> {\n+  private static final long DEFAULT_SET_FILTER_THRESHOLD = 100_000L;\n+  private static final Schema POS_DELETE_SCHEMA = new Schema(\n+      MetadataColumns.DELETE_FILE_PATH,\n+      MetadataColumns.DELETE_FILE_POS);\n+\n+  private final long setFilterThreshold;\n+  private final DataFile dataFile;\n+  private final List<DeleteFile> posDeletes;\n+  private final List<DeleteFile> eqDeletes;\n+  private final Schema requiredSchema;\n+  private final Accessor<StructLike> posAccessor;\n+\n+  public DeleteFilter(FileScanTask task, Schema tableSchema, Schema requestedSchema) {\n+    this.setFilterThreshold = DEFAULT_SET_FILTER_THRESHOLD;\n+    this.dataFile = task.file();\n+\n+    ImmutableList.Builder<DeleteFile> posDeleteBuilder = ImmutableList.builder();\n+    ImmutableList.Builder<DeleteFile> eqDeleteBuilder = ImmutableList.builder();\n+    for (DeleteFile delete : task.deletes()) {\n+      switch (delete.content()) {\n+        case POSITION_DELETES:\n+          posDeleteBuilder.add(delete);\n+          break;\n+        case EQUALITY_DELETES:\n+          eqDeleteBuilder.add(delete);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\"Unknown delete file content: \" + delete.content());\n+      }\n+    }\n+\n+    this.posDeletes = posDeleteBuilder.build();\n+    this.eqDeletes = eqDeleteBuilder.build();\n+    this.requiredSchema = fileProjection(tableSchema, requestedSchema, posDeletes, eqDeletes);\n+    this.posAccessor = requiredSchema.accessorForField(MetadataColumns.ROW_POSITION.fieldId());\n+  }\n+\n+  public Schema requiredSchema() {\n+    return requiredSchema;\n+  }\n+\n+  Accessor<StructLike> posAccessor() {\n+    return posAccessor;\n+  }\n+\n+  protected abstract StructLike asStructLike(T record);\n+\n+  protected abstract InputFile getInputFile(String location);\n+\n+  protected long pos(T record) {\n+    return (Long) posAccessor.get(asStructLike(record));\n+  }\n+\n+  public CloseableIterable<T> filter(CloseableIterable<T> records) {\n+    return applyEqDeletes(applyPosDeletes(records));\n+  }\n+\n+  private CloseableIterable<T> applyEqDeletes(CloseableIterable<T> records) {\n+    if (eqDeletes.isEmpty()) {\n+      return records;\n+    }\n+\n+    Multimap<Set<Integer>, DeleteFile> filesByDeleteIds = Multimaps.newMultimap(Maps.newHashMap(), Lists::newArrayList);\n+    for (DeleteFile delete : eqDeletes) {\n+      filesByDeleteIds.put(Sets.newHashSet(delete.equalityFieldIds()), delete);\n+    }\n+\n+    CloseableIterable<T> filteredRecords = records;\n+    for (Map.Entry<Set<Integer>, Collection<DeleteFile>> entry : filesByDeleteIds.asMap().entrySet()) {\n+      Set<Integer> ids = entry.getKey();\n+      Iterable<DeleteFile> deletes = entry.getValue();\n+\n+      Schema deleteSchema = TypeUtil.select(requiredSchema, ids);\n+\n+      // a projection to select and reorder fields of the file schema to match the delete rows\n+      StructProjection projectRow = StructProjection.create(requiredSchema, deleteSchema);\n+\n+      Iterable<CloseableIterable<Record>> deleteRecords = Iterables.transform(deletes,\n+          delete -> openDeletes(delete, dataFile, deleteSchema));\n+      StructLikeSet deleteSet = Deletes.toEqualitySet(\n+          // copy the delete records because they will be held in a set\n+          CloseableIterable.transform(CloseableIterable.concat(deleteRecords), Record::copy),\n+          deleteSchema.asStruct());\n+\n+      filteredRecords = Deletes.filter(filteredRecords,\n+          record -> projectRow.wrap(asStructLike(record)), deleteSet);\n+    }\n+\n+    return filteredRecords;\n+  }\n+\n+  private CloseableIterable<T> applyPosDeletes(CloseableIterable<T> records) {\n+    if (posDeletes.isEmpty()) {\n+      return records;\n+    }\n+\n+    List<CloseableIterable<Record>> deletes = Lists.transform(posDeletes,\n+        delete -> openPosDeletes(delete, dataFile));\n+\n+    // if there are fewer deletes than a reasonable number to keep in memory, use a set\n+    if (posDeletes.stream().mapToLong(DeleteFile::recordCount).sum() < setFilterThreshold) {\n+      return Deletes.filter(\n+          records, this::pos,\n+          Deletes.toPositionSet(dataFile.path(), CloseableIterable.concat(deletes)));\n+    }\n+\n+    return Deletes.streamingFilter(records, this::pos, Deletes.deletePositions(dataFile.path(), deletes));\n+  }\n+\n+  private CloseableIterable<Record> openPosDeletes(DeleteFile file, DataFile dataFile) {\n+    return openDeletes(file, dataFile, POS_DELETE_SCHEMA);\n+  }\n+\n+  private CloseableIterable<Record> openDeletes(DeleteFile deleteFile, DataFile dataFile, Schema deleteSchema) {\n+    InputFile input = getInputFile(deleteFile.path().toString());\n+    switch (deleteFile.format()) {\n+      case AVRO:\n+        return Avro.read(input)\n+            .project(deleteSchema)\n+            .reuseContainers()\n+            .createReaderFunc(DataReader::create)\n+            .build();\n+\n+      case PARQUET:\n+        Parquet.ReadBuilder builder = Parquet.read(input)\n+            .project(deleteSchema)\n+            .reuseContainers()\n+            .createReaderFunc(fileSchema -> GenericParquetReaders.buildReader(deleteSchema, fileSchema));\n+\n+        if (deleteFile.content() == FileContent.POSITION_DELETES) {\n+          builder.filter(Expressions.equal(MetadataColumns.DELETE_FILE_PATH.name(), dataFile.path()));\n+        }\n+\n+        return builder.build();\n+\n+      case ORC:\n+      default:\n+        throw new UnsupportedOperationException(String.format(\n+            \"Cannot read %s file: %s\", deleteFile.format().name(), deleteFile.path()));", "originalCommit": "e2d055ccc460afad303bc10409da318e9c9243d8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODg0MDYwNw==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r488840607", "bodyText": "Updated to \"Cannot read deletes: %s is not a supported format: %s\"", "author": "rdblue", "createdAt": "2020-09-15T17:29:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI1MTY4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI4NDA4NQ==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r487284085", "bodyText": "Is there anything important about the order of applying deletes here? Is the guess here that there will be more Pos Deletes than EqDeletes?", "author": "RussellSpitzer", "createdAt": "2020-09-11T20:49:38Z", "path": "data/src/main/java/org/apache/iceberg/data/DeleteFilter.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.iceberg.data;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Accessor;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.deletes.Deletes;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.apache.iceberg.util.StructProjection;\n+import org.apache.parquet.Preconditions;\n+\n+public abstract class DeleteFilter<T> {\n+  private static final long DEFAULT_SET_FILTER_THRESHOLD = 100_000L;\n+  private static final Schema POS_DELETE_SCHEMA = new Schema(\n+      MetadataColumns.DELETE_FILE_PATH,\n+      MetadataColumns.DELETE_FILE_POS);\n+\n+  private final long setFilterThreshold;\n+  private final FileIO io;\n+  private final DataFile dataFile;\n+  private final List<DeleteFile> posDeletes;\n+  private final List<DeleteFile> eqDeletes;\n+  private final Schema requiredSchema;\n+  private final Accessor<StructLike> posAccessor;\n+\n+  public DeleteFilter(FileIO io, FileScanTask task, Schema tableSchema, Schema requestedSchema) {\n+    this.setFilterThreshold = DEFAULT_SET_FILTER_THRESHOLD;\n+    this.io = io;\n+    this.dataFile = task.file();\n+\n+    ImmutableList.Builder<DeleteFile> posDeleteBuilder = ImmutableList.builder();\n+    ImmutableList.Builder<DeleteFile> eqDeleteBuilder = ImmutableList.builder();\n+    for (DeleteFile delete : task.deletes()) {\n+      switch (delete.content()) {\n+        case POSITION_DELETES:\n+          posDeleteBuilder.add(delete);\n+          break;\n+        case EQUALITY_DELETES:\n+          eqDeleteBuilder.add(delete);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\"Unknown delete file content: \" + delete.content());\n+      }\n+    }\n+\n+    this.posDeletes = posDeleteBuilder.build();\n+    this.eqDeletes = eqDeleteBuilder.build();\n+    this.requiredSchema = fileProjection(tableSchema, requestedSchema, posDeletes, eqDeletes);\n+    this.posAccessor = requiredSchema.accessorForField(MetadataColumns.ROW_POSITION.fieldId());\n+  }\n+\n+  public Schema requiredSchema() {\n+    return requiredSchema;\n+  }\n+\n+  Accessor<StructLike> posAccessor() {\n+    return posAccessor;\n+  }\n+\n+  protected abstract StructLike asStructLike(T record);\n+\n+  protected long pos(T record) {\n+    return (Long) posAccessor.get(asStructLike(record));\n+  }\n+\n+  public CloseableIterable<T> filter(CloseableIterable<T> records) {\n+    return applyEqDeletes(applyPosDeletes(records));", "originalCommit": "acea9bd174be0961167a3c5d9a58401b45dfd401", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI5MjEzMQ==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r487292131", "bodyText": "My thought here is just that EqDelete always uses a set check so it's probably going to be cheaper than the possibility that you have to do the streaming check in pos deletes, but maybe I'm thinking about it wrong.", "author": "RussellSpitzer", "createdAt": "2020-09-11T21:09:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI4NDA4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzM0MDk1Mw==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r487340953", "bodyText": "We might want to do minor compaction to transform the equality delete files to position delete files, so I guess position deletes should be more than equality deletes as time goes on.\nThe records in position deletes are ordered, so the sorted merge-base check wouldn't be more expensive than set based check consider the cost of building the hash set. Right?", "author": "chenjunjiedada", "createdAt": "2020-09-12T00:17:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI4NDA4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODE4MzM4MQ==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r488183381", "bodyText": "I think that equality deletes are more expensive to apply because they require a projection and a set lookup (hash and maybe equality check), and there could be multiple equality deletes to apply. So the idea is to do the cheapest operation first and the most expensive operation last to do fewer expensive filter checks.", "author": "rdblue", "createdAt": "2020-09-14T19:54:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI4NDA4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI1NTI5Mw==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r489255293", "bodyText": "do minor compaction to transform the equality delete files to position delete files\n\n@chenjunjiedada  do we really need to transform equality-deletes to positional-deletes when doing minor compaction ?    Let's take the case in a single bucket:\ntxn-0:  insert-file0,  pos-delete-file0,  equality-delete-file0;\ntxn-1:  insert-file1,  pos-delete-file1,   equality-delete-file1;\ntxn-2:  insert-file2,  pos-delete-file2,  equality-delete-file2;\nThe insert-file0's posDeletes is [pos-delete-file0, pos-delete-file1, pos-delete-file2] and eqDeletes is [equality-delete-file0, equality-delete-file1, equality-delete-file2].\nThe insert-file1's posDeletes is [pos-delete-file1, pos-delete-file2] and eqDeletes is [equality-delete-file1, equality-delete-file2]\nThe insert-file2's posDeletes is [pos-delete-file2] and eqDeletes is [equality-delete-file2]\nYou mean we will transform the equality-delete-file2 and equality-delete-file1 to a new pos-delete-file3 when doing minior compaciton for txn2 ?", "author": "openinx", "createdAt": "2020-09-16T08:24:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI4NDA4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTMzNjYyMw==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r489336623", "bodyText": "@openinx , Since the cost of merging equality delete file is bigger than merging position delete, so it could be an option for minor compaction. While I haven't think about how we will do that compaction.  I guess we should consider the sequence number when compacting the equality delete file, for example, convert equality delete files with same sequence number to a new position delete file.", "author": "chenjunjiedada", "createdAt": "2020-09-16T10:34:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI4NDA4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI5MDUyOA==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r487290528", "bodyText": "I'm a little confused by this check, shouldn't we be using tasks() when planning deletes with the non-batch reader?", "author": "RussellSpitzer", "createdAt": "2020-09-11T21:05:52Z", "path": "spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -385,6 +389,9 @@ private static void mergeIcebergHadoopConfs(\n       }\n     }\n \n+    ValidationException.check(tasks.stream().noneMatch(TableScanUtil::hasDeletes),", "originalCommit": "e2d055ccc460afad303bc10409da318e9c9243d8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODE4MjQ2MQ==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r488182461", "bodyText": "Yes, I think we should use tasks()", "author": "rdblue", "createdAt": "2020-09-14T19:52:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI5MDUyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODgzOTg0NA==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r488839844", "bodyText": "I've fixed this. This should also have been in planBatchInputPartitions because the check is intended to ensure that we don't attempt to read using batch when there are deletes to apply. This was here because I initially disabled all reads with deletes to apply before I implemented deletes for row-based reads.", "author": "rdblue", "createdAt": "2020-09-15T17:28:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI5MDUyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODg0NDMxOQ==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r488844319", "bodyText": "Sgtm :) I was just really confused how anything worked at all with this check, I assumed you had some trick up your sleeve  I just didn't get.", "author": "RussellSpitzer", "createdAt": "2020-09-15T17:33:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI5MDUyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODg1NDg0NQ==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r488854845", "bodyText": "You really shouldn't trust me that much.", "author": "rdblue", "createdAt": "2020-09-15T17:48:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI5MDUyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzM0Mjc3OQ==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r487342779", "bodyText": "Can this be a table property? So that user could tune according to the executor memory?", "author": "chenjunjiedada", "createdAt": "2020-09-12T00:28:22Z", "path": "data/src/main/java/org/apache/iceberg/data/DeleteFilter.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Accessor;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.deletes.Deletes;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.apache.iceberg.util.StructProjection;\n+import org.apache.parquet.Preconditions;\n+\n+public abstract class DeleteFilter<T> {\n+  private static final long DEFAULT_SET_FILTER_THRESHOLD = 100_000L;", "originalCommit": "e2d055ccc460afad303bc10409da318e9c9243d8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODA3ODc0NQ==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r488078745", "bodyText": "I'd also add, since this is based on reader's memory constraints, shouldn't this also be a reader (datasource option) property passed down the scan?", "author": "prodeezy", "createdAt": "2020-09-14T16:48:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzM0Mjc3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODE4MjI0Ng==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r488182246", "bodyText": "Yes, eventually. I just want to keep these commits small and more focused. We can add more plumbing for config in parallel.", "author": "rdblue", "createdAt": "2020-09-14T19:52:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzM0Mjc3OQ=="}], "type": "inlineReview"}, {"oid": "eb4b37b87a266a8b35efd1fbc6b262489ef9f892", "url": "https://github.com/apache/iceberg/commit/eb4b37b87a266a8b35efd1fbc6b262489ef9f892", "message": "Fix checkstyle problems in DeleteFilter.", "committedDate": "2020-09-15T00:53:20Z", "type": "commit"}, {"oid": "1e000369ab83c59c857345e660b745cbeff4b7f7", "url": "https://github.com/apache/iceberg/commit/1e000369ab83c59c857345e660b745cbeff4b7f7", "message": "Fix checkstyle problem in RowDataReader.", "committedDate": "2020-09-15T17:22:46Z", "type": "commit"}, {"oid": "6583cbdffe2663a6682a187637c7886f6248f033", "url": "https://github.com/apache/iceberg/commit/6583cbdffe2663a6682a187637c7886f6248f033", "message": "Fix review comments.", "committedDate": "2020-09-15T17:31:03Z", "type": "commit"}, {"oid": "af56542a3ad69bda840ff49d1a00a52664bb2c34", "url": "https://github.com/apache/iceberg/commit/af56542a3ad69bda840ff49d1a00a52664bb2c34", "message": "Fix NPE caused by returning a null Accessor map.", "committedDate": "2020-09-15T19:52:17Z", "type": "commit"}, {"oid": "389f86b44fbac5a685d20bbfc5e72b69975a91ed", "url": "https://github.com/apache/iceberg/commit/389f86b44fbac5a685d20bbfc5e72b69975a91ed", "message": "Fix Spark 2.4 test failures for missing default database.", "committedDate": "2020-09-15T20:36:18Z", "type": "commit"}, {"oid": "1b90e0b81245eaa2991788ed35c7097a4560544e", "url": "https://github.com/apache/iceberg/commit/1b90e0b81245eaa2991788ed35c7097a4560544e", "message": "Handle AlreadyExistsException.", "committedDate": "2020-09-15T22:28:31Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTE0NjM5MA==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r489146390", "bodyText": "Any specific reason why we changed this test to remove \"data\" from comparison?", "author": "shardulm94", "createdAt": "2020-09-16T03:56:16Z", "path": "data/src/test/java/org/apache/iceberg/data/TestGenericReaderDeletes.java", "diffHunk": "@@ -108,8 +109,9 @@ public void testEqualityDeletesWithRequiredEqColumn() throws IOException {\n         .addDeletes(eqDeletes)\n         .commit();\n \n-    StructLikeSet expected = rowSetWithoutIds(29, 89, 122);\n-    StructLikeSet actual = rowSet(table, \"id\"); // data is added by the reader to apply the eq deletes\n+    StructLikeSet expected = selectColumns(rowSetWithoutIds(29, 89, 122), \"id\");\n+    // data is added by the reader to apply the eq deletes, use StructProjection to remove it from comparison\n+    StructLikeSet actual = selectColumns(rowSet(table, \"id\"), \"id\");", "originalCommit": "1b90e0b81245eaa2991788ed35c7097a4560544e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYxNzYwNQ==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r489617605", "bodyText": "Yes. That column is not in the requested projection.\nIn Spark, the column is not in the returned row, so I had to add selectColumns to remove it from the expected rows. After doing that, I realized that this test was validating both id and data, when the only column it should be validating for correctness is id. While we get the same result, the test was more specific than it needed to be. If we were to remove data from the rows produced by the scan, this test would have broken.", "author": "rdblue", "createdAt": "2020-09-16T17:52:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTE0NjM5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTIzMDcwNQ==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r489230705", "bodyText": "Q: I saw many other classes also defined their own  POS_DELETE_SCHEMA, is it possible to move it to a common class , similar to the MetadataColumn ?", "author": "openinx", "createdAt": "2020-09-16T07:44:01Z", "path": "data/src/main/java/org/apache/iceberg/data/DeleteFilter.java", "diffHunk": "@@ -0,0 +1,238 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Accessor;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.deletes.Deletes;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.apache.iceberg.util.StructProjection;\n+import org.apache.parquet.Preconditions;\n+\n+public abstract class DeleteFilter<T> {\n+  private static final long DEFAULT_SET_FILTER_THRESHOLD = 100_000L;\n+  private static final Schema POS_DELETE_SCHEMA = new Schema(", "originalCommit": "1b90e0b81245eaa2991788ed35c7097a4560544e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYxMzc3Mg==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r489613772", "bodyText": "The schema is not fixed because it can contain data rows. This is the projection schema used to read that ignores any row data and it doesn't need to be shared right now. I think this should be the only place where we need this, outside of tests.", "author": "rdblue", "createdAt": "2020-09-16T17:46:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTIzMDcwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTIzMzA1Ng==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r489233056", "bodyText": "nit: Similar to the code block in \n  \n    \n      iceberg/data/src/main/java/org/apache/iceberg/data/GenericReader.java\n    \n    \n         Line 97\n      in\n      0b9d994\n    \n    \n    \n    \n\n        \n          \n           for (DeleteFile delete : task.deletes()) { \n        \n    \n  \n\n,   maybe we could abstract a common method .", "author": "openinx", "createdAt": "2020-09-16T07:47:59Z", "path": "data/src/main/java/org/apache/iceberg/data/DeleteFilter.java", "diffHunk": "@@ -0,0 +1,238 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Accessor;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.deletes.Deletes;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.apache.iceberg.util.StructProjection;\n+import org.apache.parquet.Preconditions;\n+\n+public abstract class DeleteFilter<T> {\n+  private static final long DEFAULT_SET_FILTER_THRESHOLD = 100_000L;\n+  private static final Schema POS_DELETE_SCHEMA = new Schema(\n+      MetadataColumns.DELETE_FILE_PATH,\n+      MetadataColumns.DELETE_FILE_POS);\n+\n+  private final long setFilterThreshold;\n+  private final DataFile dataFile;\n+  private final List<DeleteFile> posDeletes;\n+  private final List<DeleteFile> eqDeletes;\n+  private final Schema requiredSchema;\n+  private final Accessor<StructLike> posAccessor;\n+\n+  public DeleteFilter(FileScanTask task, Schema tableSchema, Schema requestedSchema) {\n+    this.setFilterThreshold = DEFAULT_SET_FILTER_THRESHOLD;\n+    this.dataFile = task.file();\n+\n+    ImmutableList.Builder<DeleteFile> posDeleteBuilder = ImmutableList.builder();", "originalCommit": "1b90e0b81245eaa2991788ed35c7097a4560544e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI2NDQyOA==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r489264428", "bodyText": "OK, we've just moved it from that class.", "author": "openinx", "createdAt": "2020-09-16T08:38:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTIzMzA1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI2OTc5MQ==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r489269791", "bodyText": "Seems there's no need to create another new LinkedHashSet ? we won't modify this missingIds set, right  ?", "author": "openinx", "createdAt": "2020-09-16T08:46:19Z", "path": "data/src/main/java/org/apache/iceberg/data/DeleteFilter.java", "diffHunk": "@@ -0,0 +1,238 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Accessor;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.deletes.Deletes;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.apache.iceberg.util.StructProjection;\n+import org.apache.parquet.Preconditions;\n+\n+public abstract class DeleteFilter<T> {\n+  private static final long DEFAULT_SET_FILTER_THRESHOLD = 100_000L;\n+  private static final Schema POS_DELETE_SCHEMA = new Schema(\n+      MetadataColumns.DELETE_FILE_PATH,\n+      MetadataColumns.DELETE_FILE_POS);\n+\n+  private final long setFilterThreshold;\n+  private final DataFile dataFile;\n+  private final List<DeleteFile> posDeletes;\n+  private final List<DeleteFile> eqDeletes;\n+  private final Schema requiredSchema;\n+  private final Accessor<StructLike> posAccessor;\n+\n+  public DeleteFilter(FileScanTask task, Schema tableSchema, Schema requestedSchema) {\n+    this.setFilterThreshold = DEFAULT_SET_FILTER_THRESHOLD;\n+    this.dataFile = task.file();\n+\n+    ImmutableList.Builder<DeleteFile> posDeleteBuilder = ImmutableList.builder();\n+    ImmutableList.Builder<DeleteFile> eqDeleteBuilder = ImmutableList.builder();\n+    for (DeleteFile delete : task.deletes()) {\n+      switch (delete.content()) {\n+        case POSITION_DELETES:\n+          posDeleteBuilder.add(delete);\n+          break;\n+        case EQUALITY_DELETES:\n+          eqDeleteBuilder.add(delete);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\"Unknown delete file content: \" + delete.content());\n+      }\n+    }\n+\n+    this.posDeletes = posDeleteBuilder.build();\n+    this.eqDeletes = eqDeleteBuilder.build();\n+    this.requiredSchema = fileProjection(tableSchema, requestedSchema, posDeletes, eqDeletes);\n+    this.posAccessor = requiredSchema.accessorForField(MetadataColumns.ROW_POSITION.fieldId());\n+  }\n+\n+  public Schema requiredSchema() {\n+    return requiredSchema;\n+  }\n+\n+  Accessor<StructLike> posAccessor() {\n+    return posAccessor;\n+  }\n+\n+  protected abstract StructLike asStructLike(T record);\n+\n+  protected abstract InputFile getInputFile(String location);\n+\n+  protected long pos(T record) {\n+    return (Long) posAccessor.get(asStructLike(record));\n+  }\n+\n+  public CloseableIterable<T> filter(CloseableIterable<T> records) {\n+    return applyEqDeletes(applyPosDeletes(records));\n+  }\n+\n+  private CloseableIterable<T> applyEqDeletes(CloseableIterable<T> records) {\n+    if (eqDeletes.isEmpty()) {\n+      return records;\n+    }\n+\n+    Multimap<Set<Integer>, DeleteFile> filesByDeleteIds = Multimaps.newMultimap(Maps.newHashMap(), Lists::newArrayList);\n+    for (DeleteFile delete : eqDeletes) {\n+      filesByDeleteIds.put(Sets.newHashSet(delete.equalityFieldIds()), delete);\n+    }\n+\n+    CloseableIterable<T> filteredRecords = records;\n+    for (Map.Entry<Set<Integer>, Collection<DeleteFile>> entry : filesByDeleteIds.asMap().entrySet()) {\n+      Set<Integer> ids = entry.getKey();\n+      Iterable<DeleteFile> deletes = entry.getValue();\n+\n+      Schema deleteSchema = TypeUtil.select(requiredSchema, ids);\n+\n+      // a projection to select and reorder fields of the file schema to match the delete rows\n+      StructProjection projectRow = StructProjection.create(requiredSchema, deleteSchema);\n+\n+      Iterable<CloseableIterable<Record>> deleteRecords = Iterables.transform(deletes,\n+          delete -> openDeletes(delete, deleteSchema));\n+      StructLikeSet deleteSet = Deletes.toEqualitySet(\n+          // copy the delete records because they will be held in a set\n+          CloseableIterable.transform(CloseableIterable.concat(deleteRecords), Record::copy),\n+          deleteSchema.asStruct());\n+\n+      filteredRecords = Deletes.filter(filteredRecords,\n+          record -> projectRow.wrap(asStructLike(record)), deleteSet);\n+    }\n+\n+    return filteredRecords;\n+  }\n+\n+  private CloseableIterable<T> applyPosDeletes(CloseableIterable<T> records) {\n+    if (posDeletes.isEmpty()) {\n+      return records;\n+    }\n+\n+    List<CloseableIterable<Record>> deletes = Lists.transform(posDeletes, this::openPosDeletes);\n+\n+    // if there are fewer deletes than a reasonable number to keep in memory, use a set\n+    if (posDeletes.stream().mapToLong(DeleteFile::recordCount).sum() < setFilterThreshold) {\n+      return Deletes.filter(\n+          records, this::pos,\n+          Deletes.toPositionSet(dataFile.path(), CloseableIterable.concat(deletes)));\n+    }\n+\n+    return Deletes.streamingFilter(records, this::pos, Deletes.deletePositions(dataFile.path(), deletes));\n+  }\n+\n+  private CloseableIterable<Record> openPosDeletes(DeleteFile file) {\n+    return openDeletes(file, POS_DELETE_SCHEMA);\n+  }\n+\n+  private CloseableIterable<Record> openDeletes(DeleteFile deleteFile, Schema deleteSchema) {\n+    InputFile input = getInputFile(deleteFile.path().toString());\n+    switch (deleteFile.format()) {\n+      case AVRO:\n+        return Avro.read(input)\n+            .project(deleteSchema)\n+            .reuseContainers()\n+            .createReaderFunc(DataReader::create)\n+            .build();\n+\n+      case PARQUET:\n+        Parquet.ReadBuilder builder = Parquet.read(input)\n+            .project(deleteSchema)\n+            .reuseContainers()\n+            .createReaderFunc(fileSchema -> GenericParquetReaders.buildReader(deleteSchema, fileSchema));\n+\n+        if (deleteFile.content() == FileContent.POSITION_DELETES) {\n+          builder.filter(Expressions.equal(MetadataColumns.DELETE_FILE_PATH.name(), dataFile.path()));\n+        }\n+\n+        return builder.build();\n+\n+      case ORC:\n+      default:\n+        throw new UnsupportedOperationException(String.format(\n+            \"Cannot read deletes, %s is not a supported format: %s\", deleteFile.format().name(), deleteFile.path()));\n+    }\n+  }\n+\n+  private static Schema fileProjection(Schema tableSchema, Schema requestedSchema,\n+                                       List<DeleteFile> posDeletes, List<DeleteFile> eqDeletes) {\n+    if (posDeletes.isEmpty() && eqDeletes.isEmpty()) {\n+      return requestedSchema;\n+    }\n+\n+    Set<Integer> requiredIds = Sets.newLinkedHashSet();\n+    if (!posDeletes.isEmpty()) {\n+      requiredIds.add(MetadataColumns.ROW_POSITION.fieldId());\n+    }\n+\n+    for (DeleteFile eqDelete : eqDeletes) {\n+      requiredIds.addAll(eqDelete.equalityFieldIds());\n+    }\n+\n+    Set<Integer> missingIds = Sets.newLinkedHashSet(", "originalCommit": "1b90e0b81245eaa2991788ed35c7097a4560544e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYyNzQ4NQ==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r489627485", "bodyText": "This is mostly to avoid diffing the sets twice, once in isEmpty and once to iterate.", "author": "rdblue", "createdAt": "2020-09-16T18:10:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI2OTc5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTg1NDc0Ng==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r489854746", "bodyText": "Okay, that make sense.", "author": "openinx", "createdAt": "2020-09-17T01:27:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI2OTc5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTMwOTUwMw==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r489309503", "bodyText": "Q:  since we've added few columns which may not be included in projection schema  in the readScheme, then  Will the record from iterator may have more columns than user expected ?\nFor example,  table test=(a,b,c),  the user query the data by select b from test where b >10,  while we use a as the equality field id set for equality delete files,  this CloseableIterable<Record> will return records with column (a,b) , while users would actually expected return records with column (b) ?", "author": "openinx", "createdAt": "2020-09-16T09:48:00Z", "path": "data/src/main/java/org/apache/iceberg/data/GenericReader.java", "diffHunk": "@@ -92,67 +70,16 @@\n   }\n \n   public CloseableIterable<Record> open(FileScanTask task) {\n-    List<DeleteFile> posDeletes = Lists.newArrayList();\n-    List<DeleteFile> eqDeletes = Lists.newArrayList();\n-    for (DeleteFile delete : task.deletes()) {\n-      switch (delete.content()) {\n-        case POSITION_DELETES:\n-          posDeletes.add(delete);\n-          break;\n-        case EQUALITY_DELETES:\n-          eqDeletes.add(delete);\n-          break;\n-        default:\n-          throw new UnsupportedOperationException(\"Unknown delete file content: \" + delete.content());\n-      }\n-    }\n+    DeleteFilter<Record> deletes = new GenericDeleteFilter(io, task, tableSchema, projection);\n+    Schema readSchema = deletes.requiredSchema();\n \n-    Schema fileProjection = fileProjection(posDeletes, eqDeletes);\n-\n-    CloseableIterable<Record> records = openFile(task, fileProjection);\n-    records = applyPosDeletes(records, fileProjection, task.file().path(), posDeletes, task.file());\n-    records = applyEqDeletes(records, fileProjection, eqDeletes, task.file());\n-    records = applyResidual(records, fileProjection, task.residual());\n+    CloseableIterable<Record> records = openFile(task, readSchema);", "originalCommit": "1b90e0b81245eaa2991788ed35c7097a4560544e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYyMTk4Ng==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r489621986", "bodyText": "Yes, the records will have more columns than requested. That's why we project the added columns at the end of the record, (b, a) in your example. @shardulm94 pointed this out on the original PR for generics.\nFor Spark, this is okay because Spark ignores the extra columns if the schema associated with a row doesn't have them. You can see the check here: https://github.com/apache/iceberg/pull/1444/files#diff-7600f4d25cfdef7f5da70e12126b55c7R137-R138\nFor generics, we just return the larger row to avoid needing to make a copy right now. I don't think it is worth the cost of a copy to remove the columns, so we would need to add the ability to truncate the columns of a GenericRecord. I think that adding that feature to GenericRecord should be done in a separate PR, if we decide that it should be done.", "author": "rdblue", "createdAt": "2020-09-16T18:00:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTMwOTUwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTMyMTU3OA==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r489321578", "bodyText": "Looks like should be missingIds? Otherwise, the ROW_POSITION may be added repeatedly.", "author": "JingsongLi", "createdAt": "2020-09-16T10:08:06Z", "path": "data/src/main/java/org/apache/iceberg/data/DeleteFilter.java", "diffHunk": "@@ -0,0 +1,238 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Accessor;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.deletes.Deletes;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.apache.iceberg.util.StructProjection;\n+import org.apache.parquet.Preconditions;\n+\n+public abstract class DeleteFilter<T> {\n+  private static final long DEFAULT_SET_FILTER_THRESHOLD = 100_000L;\n+  private static final Schema POS_DELETE_SCHEMA = new Schema(\n+      MetadataColumns.DELETE_FILE_PATH,\n+      MetadataColumns.DELETE_FILE_POS);\n+\n+  private final long setFilterThreshold;\n+  private final DataFile dataFile;\n+  private final List<DeleteFile> posDeletes;\n+  private final List<DeleteFile> eqDeletes;\n+  private final Schema requiredSchema;\n+  private final Accessor<StructLike> posAccessor;\n+\n+  public DeleteFilter(FileScanTask task, Schema tableSchema, Schema requestedSchema) {\n+    this.setFilterThreshold = DEFAULT_SET_FILTER_THRESHOLD;\n+    this.dataFile = task.file();\n+\n+    ImmutableList.Builder<DeleteFile> posDeleteBuilder = ImmutableList.builder();\n+    ImmutableList.Builder<DeleteFile> eqDeleteBuilder = ImmutableList.builder();\n+    for (DeleteFile delete : task.deletes()) {\n+      switch (delete.content()) {\n+        case POSITION_DELETES:\n+          posDeleteBuilder.add(delete);\n+          break;\n+        case EQUALITY_DELETES:\n+          eqDeleteBuilder.add(delete);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\"Unknown delete file content: \" + delete.content());\n+      }\n+    }\n+\n+    this.posDeletes = posDeleteBuilder.build();\n+    this.eqDeletes = eqDeleteBuilder.build();\n+    this.requiredSchema = fileProjection(tableSchema, requestedSchema, posDeletes, eqDeletes);\n+    this.posAccessor = requiredSchema.accessorForField(MetadataColumns.ROW_POSITION.fieldId());\n+  }\n+\n+  public Schema requiredSchema() {\n+    return requiredSchema;\n+  }\n+\n+  Accessor<StructLike> posAccessor() {\n+    return posAccessor;\n+  }\n+\n+  protected abstract StructLike asStructLike(T record);\n+\n+  protected abstract InputFile getInputFile(String location);\n+\n+  protected long pos(T record) {\n+    return (Long) posAccessor.get(asStructLike(record));\n+  }\n+\n+  public CloseableIterable<T> filter(CloseableIterable<T> records) {\n+    return applyEqDeletes(applyPosDeletes(records));\n+  }\n+\n+  private CloseableIterable<T> applyEqDeletes(CloseableIterable<T> records) {\n+    if (eqDeletes.isEmpty()) {\n+      return records;\n+    }\n+\n+    Multimap<Set<Integer>, DeleteFile> filesByDeleteIds = Multimaps.newMultimap(Maps.newHashMap(), Lists::newArrayList);\n+    for (DeleteFile delete : eqDeletes) {\n+      filesByDeleteIds.put(Sets.newHashSet(delete.equalityFieldIds()), delete);\n+    }\n+\n+    CloseableIterable<T> filteredRecords = records;\n+    for (Map.Entry<Set<Integer>, Collection<DeleteFile>> entry : filesByDeleteIds.asMap().entrySet()) {\n+      Set<Integer> ids = entry.getKey();\n+      Iterable<DeleteFile> deletes = entry.getValue();\n+\n+      Schema deleteSchema = TypeUtil.select(requiredSchema, ids);\n+\n+      // a projection to select and reorder fields of the file schema to match the delete rows\n+      StructProjection projectRow = StructProjection.create(requiredSchema, deleteSchema);\n+\n+      Iterable<CloseableIterable<Record>> deleteRecords = Iterables.transform(deletes,\n+          delete -> openDeletes(delete, deleteSchema));\n+      StructLikeSet deleteSet = Deletes.toEqualitySet(\n+          // copy the delete records because they will be held in a set\n+          CloseableIterable.transform(CloseableIterable.concat(deleteRecords), Record::copy),\n+          deleteSchema.asStruct());\n+\n+      filteredRecords = Deletes.filter(filteredRecords,\n+          record -> projectRow.wrap(asStructLike(record)), deleteSet);\n+    }\n+\n+    return filteredRecords;\n+  }\n+\n+  private CloseableIterable<T> applyPosDeletes(CloseableIterable<T> records) {\n+    if (posDeletes.isEmpty()) {\n+      return records;\n+    }\n+\n+    List<CloseableIterable<Record>> deletes = Lists.transform(posDeletes, this::openPosDeletes);\n+\n+    // if there are fewer deletes than a reasonable number to keep in memory, use a set\n+    if (posDeletes.stream().mapToLong(DeleteFile::recordCount).sum() < setFilterThreshold) {\n+      return Deletes.filter(\n+          records, this::pos,\n+          Deletes.toPositionSet(dataFile.path(), CloseableIterable.concat(deletes)));\n+    }\n+\n+    return Deletes.streamingFilter(records, this::pos, Deletes.deletePositions(dataFile.path(), deletes));\n+  }\n+\n+  private CloseableIterable<Record> openPosDeletes(DeleteFile file) {\n+    return openDeletes(file, POS_DELETE_SCHEMA);\n+  }\n+\n+  private CloseableIterable<Record> openDeletes(DeleteFile deleteFile, Schema deleteSchema) {\n+    InputFile input = getInputFile(deleteFile.path().toString());\n+    switch (deleteFile.format()) {\n+      case AVRO:\n+        return Avro.read(input)\n+            .project(deleteSchema)\n+            .reuseContainers()\n+            .createReaderFunc(DataReader::create)\n+            .build();\n+\n+      case PARQUET:\n+        Parquet.ReadBuilder builder = Parquet.read(input)\n+            .project(deleteSchema)\n+            .reuseContainers()\n+            .createReaderFunc(fileSchema -> GenericParquetReaders.buildReader(deleteSchema, fileSchema));\n+\n+        if (deleteFile.content() == FileContent.POSITION_DELETES) {\n+          builder.filter(Expressions.equal(MetadataColumns.DELETE_FILE_PATH.name(), dataFile.path()));\n+        }\n+\n+        return builder.build();\n+\n+      case ORC:\n+      default:\n+        throw new UnsupportedOperationException(String.format(\n+            \"Cannot read deletes, %s is not a supported format: %s\", deleteFile.format().name(), deleteFile.path()));\n+    }\n+  }\n+\n+  private static Schema fileProjection(Schema tableSchema, Schema requestedSchema,\n+                                       List<DeleteFile> posDeletes, List<DeleteFile> eqDeletes) {\n+    if (posDeletes.isEmpty() && eqDeletes.isEmpty()) {\n+      return requestedSchema;\n+    }\n+\n+    Set<Integer> requiredIds = Sets.newLinkedHashSet();\n+    if (!posDeletes.isEmpty()) {\n+      requiredIds.add(MetadataColumns.ROW_POSITION.fieldId());\n+    }\n+\n+    for (DeleteFile eqDelete : eqDeletes) {\n+      requiredIds.addAll(eqDelete.equalityFieldIds());\n+    }\n+\n+    Set<Integer> missingIds = Sets.newLinkedHashSet(\n+        Sets.difference(requiredIds, TypeUtil.getProjectedIds(requestedSchema)));\n+\n+    if (missingIds.isEmpty()) {\n+      return requestedSchema;\n+    }\n+\n+    // TODO: support adding nested columns. this will currently fail when finding nested columns to add\n+    List<Types.NestedField> columns = Lists.newArrayList(requestedSchema.columns());\n+    for (int fieldId : missingIds) {\n+      if (fieldId == MetadataColumns.ROW_POSITION.fieldId()) {\n+        continue; // add _pos at the end\n+      }\n+\n+      Types.NestedField field = tableSchema.asStruct().field(fieldId);\n+      Preconditions.checkArgument(field != null, \"Cannot find required field for ID %s\", fieldId);\n+\n+      columns.add(field);\n+    }\n+\n+    if (requiredIds.contains(MetadataColumns.ROW_POSITION.fieldId())) {", "originalCommit": "1b90e0b81245eaa2991788ed35c7097a4560544e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYxNjE4Ng==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r489616186", "bodyText": "I think you're right. If the caller has also requested the position column then there is no need to add it at the end.", "author": "rdblue", "createdAt": "2020-09-16T17:50:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTMyMTU3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYyNDA5Nw==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r489624097", "bodyText": "Fixed, thanks for catching this!", "author": "rdblue", "createdAt": "2020-09-16T18:04:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTMyMTU3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQxMTQ4OQ==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r489411489", "bodyText": "Q:  do we need to consider to maintain the deleteSet in a LRU cache if several FileScanTask are located in the same task node  in future ?", "author": "openinx", "createdAt": "2020-09-16T12:51:35Z", "path": "data/src/main/java/org/apache/iceberg/data/DeleteFilter.java", "diffHunk": "@@ -0,0 +1,238 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Accessor;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.deletes.Deletes;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.apache.iceberg.util.StructProjection;\n+import org.apache.parquet.Preconditions;\n+\n+public abstract class DeleteFilter<T> {\n+  private static final long DEFAULT_SET_FILTER_THRESHOLD = 100_000L;\n+  private static final Schema POS_DELETE_SCHEMA = new Schema(\n+      MetadataColumns.DELETE_FILE_PATH,\n+      MetadataColumns.DELETE_FILE_POS);\n+\n+  private final long setFilterThreshold;\n+  private final DataFile dataFile;\n+  private final List<DeleteFile> posDeletes;\n+  private final List<DeleteFile> eqDeletes;\n+  private final Schema requiredSchema;\n+  private final Accessor<StructLike> posAccessor;\n+\n+  public DeleteFilter(FileScanTask task, Schema tableSchema, Schema requestedSchema) {\n+    this.setFilterThreshold = DEFAULT_SET_FILTER_THRESHOLD;\n+    this.dataFile = task.file();\n+\n+    ImmutableList.Builder<DeleteFile> posDeleteBuilder = ImmutableList.builder();\n+    ImmutableList.Builder<DeleteFile> eqDeleteBuilder = ImmutableList.builder();\n+    for (DeleteFile delete : task.deletes()) {\n+      switch (delete.content()) {\n+        case POSITION_DELETES:\n+          posDeleteBuilder.add(delete);\n+          break;\n+        case EQUALITY_DELETES:\n+          eqDeleteBuilder.add(delete);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\"Unknown delete file content: \" + delete.content());\n+      }\n+    }\n+\n+    this.posDeletes = posDeleteBuilder.build();\n+    this.eqDeletes = eqDeleteBuilder.build();\n+    this.requiredSchema = fileProjection(tableSchema, requestedSchema, posDeletes, eqDeletes);\n+    this.posAccessor = requiredSchema.accessorForField(MetadataColumns.ROW_POSITION.fieldId());\n+  }\n+\n+  public Schema requiredSchema() {\n+    return requiredSchema;\n+  }\n+\n+  Accessor<StructLike> posAccessor() {\n+    return posAccessor;\n+  }\n+\n+  protected abstract StructLike asStructLike(T record);\n+\n+  protected abstract InputFile getInputFile(String location);\n+\n+  protected long pos(T record) {\n+    return (Long) posAccessor.get(asStructLike(record));\n+  }\n+\n+  public CloseableIterable<T> filter(CloseableIterable<T> records) {\n+    return applyEqDeletes(applyPosDeletes(records));\n+  }\n+\n+  private CloseableIterable<T> applyEqDeletes(CloseableIterable<T> records) {\n+    if (eqDeletes.isEmpty()) {\n+      return records;\n+    }\n+\n+    Multimap<Set<Integer>, DeleteFile> filesByDeleteIds = Multimaps.newMultimap(Maps.newHashMap(), Lists::newArrayList);\n+    for (DeleteFile delete : eqDeletes) {\n+      filesByDeleteIds.put(Sets.newHashSet(delete.equalityFieldIds()), delete);\n+    }\n+\n+    CloseableIterable<T> filteredRecords = records;\n+    for (Map.Entry<Set<Integer>, Collection<DeleteFile>> entry : filesByDeleteIds.asMap().entrySet()) {\n+      Set<Integer> ids = entry.getKey();\n+      Iterable<DeleteFile> deletes = entry.getValue();\n+\n+      Schema deleteSchema = TypeUtil.select(requiredSchema, ids);\n+\n+      // a projection to select and reorder fields of the file schema to match the delete rows\n+      StructProjection projectRow = StructProjection.create(requiredSchema, deleteSchema);\n+\n+      Iterable<CloseableIterable<Record>> deleteRecords = Iterables.transform(deletes,\n+          delete -> openDeletes(delete, deleteSchema));\n+      StructLikeSet deleteSet = Deletes.toEqualitySet(", "originalCommit": "1b90e0b81245eaa2991788ed35c7097a4560544e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYxMjM1Ng==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r489612356", "bodyText": "Reusing the delete set is a good future optimization. We will need to be careful with that, though. I wouldn't want to keep them around any longer than needed because the set could be fairly large. For Spark, we would not want to keep these sets across tasks and may even want to discard sets as they are no longer needed.", "author": "rdblue", "createdAt": "2020-09-16T17:43:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQxMTQ4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQxNDMwNw==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r489414307", "bodyText": "Overwrite this pos method for avoiding an extra asStructLike(..), right  ?", "author": "openinx", "createdAt": "2020-09-16T12:55:53Z", "path": "data/src/main/java/org/apache/iceberg/data/GenericDeleteFilter.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data;\n+\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+\n+public class GenericDeleteFilter extends DeleteFilter<Record> {\n+  private final FileIO io;\n+  private final InternalRecordWrapper asStructLike;\n+\n+  public GenericDeleteFilter(FileIO io, FileScanTask task, Schema tableSchema, Schema requestedSchema) {\n+    super(task, tableSchema, requestedSchema);\n+    this.io = io;\n+    this.asStructLike = new InternalRecordWrapper(requiredSchema().asStruct());\n+  }\n+\n+  @Override\n+  protected long pos(Record record) {", "originalCommit": "1b90e0b81245eaa2991788ed35c7097a4560544e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYwOTA5OQ==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r489609099", "bodyText": "Yes.", "author": "rdblue", "createdAt": "2020-09-16T17:37:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQxNDMwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMTI3OA==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r489421278", "bodyText": "The same question similar to https://github.com/apache/iceberg/pull/1444/files#r489309503", "author": "openinx", "createdAt": "2020-09-16T13:06:30Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataReader.java", "diffHunk": "@@ -81,23 +82,17 @@\n \n   @Override\n   CloseableIterator<InternalRow> open(FileScanTask task) {\n+    SparkDeleteFilter deletes = new SparkDeleteFilter(task, tableSchema, expectedSchema);\n+\n+    // schema or rows returned by readers\n+    Schema requiredSchema = deletes.requiredSchema();\n+    Map<Integer, ?> idToConstant = PartitionUtil.constantsMap(task, RowDataReader::convertConstant);\n     DataFile file = task.file();\n \n     // update the current file for Spark's filename() function\n     InputFileBlockHolder.set(file.path().toString(), task.start(), task.length());\n \n-    // schema or rows returned by readers\n-    PartitionSpec spec = task.spec();\n-    Set<Integer> idColumns = spec.identitySourceIds();\n-    Schema partitionSchema = TypeUtil.select(expectedSchema, idColumns);\n-    boolean projectsIdentityPartitionColumns = !partitionSchema.columns().isEmpty();\n-\n-    if (projectsIdentityPartitionColumns) {\n-      return open(task, expectedSchema, PartitionUtil.constantsMap(task, RowDataReader::convertConstant))\n-          .iterator();\n-    }\n-    // return the base iterator\n-    return open(task, expectedSchema, ImmutableMap.of()).iterator();\n+    return deletes.filter(open(task, requiredSchema, idToConstant)).iterator();", "originalCommit": "1b90e0b81245eaa2991788ed35c7097a4560544e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYyMjU2MA==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r489622560", "bodyText": "See the other thread.", "author": "rdblue", "createdAt": "2020-09-16T18:01:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMTI3OA=="}], "type": "inlineReview"}, {"oid": "86389e404b00fc64e4c7b719ddcf4f7955915cbc", "url": "https://github.com/apache/iceberg/commit/86389e404b00fc64e4c7b719ddcf4f7955915cbc", "message": "Use missingIds in file projection, fix test checkstyle.", "committedDate": "2020-09-16T18:03:20Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY2MjI5MQ==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r490662291", "bodyText": "Q:  I saw the StructProjection saying it does not support list or map, so that means we don't support that there's any list or map in equality fields ?   For now  I think it's OK.", "author": "openinx", "createdAt": "2020-09-18T02:19:44Z", "path": "data/src/main/java/org/apache/iceberg/data/DeleteFilter.java", "diffHunk": "@@ -0,0 +1,238 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Accessor;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.deletes.Deletes;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.apache.iceberg.util.StructProjection;\n+import org.apache.parquet.Preconditions;\n+\n+public abstract class DeleteFilter<T> {\n+  private static final long DEFAULT_SET_FILTER_THRESHOLD = 100_000L;\n+  private static final Schema POS_DELETE_SCHEMA = new Schema(\n+      MetadataColumns.DELETE_FILE_PATH,\n+      MetadataColumns.DELETE_FILE_POS);\n+\n+  private final long setFilterThreshold;\n+  private final DataFile dataFile;\n+  private final List<DeleteFile> posDeletes;\n+  private final List<DeleteFile> eqDeletes;\n+  private final Schema requiredSchema;\n+  private final Accessor<StructLike> posAccessor;\n+\n+  public DeleteFilter(FileScanTask task, Schema tableSchema, Schema requestedSchema) {\n+    this.setFilterThreshold = DEFAULT_SET_FILTER_THRESHOLD;\n+    this.dataFile = task.file();\n+\n+    ImmutableList.Builder<DeleteFile> posDeleteBuilder = ImmutableList.builder();\n+    ImmutableList.Builder<DeleteFile> eqDeleteBuilder = ImmutableList.builder();\n+    for (DeleteFile delete : task.deletes()) {\n+      switch (delete.content()) {\n+        case POSITION_DELETES:\n+          posDeleteBuilder.add(delete);\n+          break;\n+        case EQUALITY_DELETES:\n+          eqDeleteBuilder.add(delete);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\"Unknown delete file content: \" + delete.content());\n+      }\n+    }\n+\n+    this.posDeletes = posDeleteBuilder.build();\n+    this.eqDeletes = eqDeleteBuilder.build();\n+    this.requiredSchema = fileProjection(tableSchema, requestedSchema, posDeletes, eqDeletes);\n+    this.posAccessor = requiredSchema.accessorForField(MetadataColumns.ROW_POSITION.fieldId());\n+  }\n+\n+  public Schema requiredSchema() {\n+    return requiredSchema;\n+  }\n+\n+  Accessor<StructLike> posAccessor() {\n+    return posAccessor;\n+  }\n+\n+  protected abstract StructLike asStructLike(T record);\n+\n+  protected abstract InputFile getInputFile(String location);\n+\n+  protected long pos(T record) {\n+    return (Long) posAccessor.get(asStructLike(record));\n+  }\n+\n+  public CloseableIterable<T> filter(CloseableIterable<T> records) {\n+    return applyEqDeletes(applyPosDeletes(records));\n+  }\n+\n+  private CloseableIterable<T> applyEqDeletes(CloseableIterable<T> records) {\n+    if (eqDeletes.isEmpty()) {\n+      return records;\n+    }\n+\n+    Multimap<Set<Integer>, DeleteFile> filesByDeleteIds = Multimaps.newMultimap(Maps.newHashMap(), Lists::newArrayList);\n+    for (DeleteFile delete : eqDeletes) {\n+      filesByDeleteIds.put(Sets.newHashSet(delete.equalityFieldIds()), delete);\n+    }\n+\n+    CloseableIterable<T> filteredRecords = records;\n+    for (Map.Entry<Set<Integer>, Collection<DeleteFile>> entry : filesByDeleteIds.asMap().entrySet()) {\n+      Set<Integer> ids = entry.getKey();\n+      Iterable<DeleteFile> deletes = entry.getValue();\n+\n+      Schema deleteSchema = TypeUtil.select(requiredSchema, ids);\n+\n+      // a projection to select and reorder fields of the file schema to match the delete rows\n+      StructProjection projectRow = StructProjection.create(requiredSchema, deleteSchema);", "originalCommit": "86389e404b00fc64e4c7b719ddcf4f7955915cbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTA3ODA0Ng==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r491078046", "bodyText": "That's right. We should extend it later, but I don't think that many deletes will be by list or map.", "author": "rdblue", "createdAt": "2020-09-18T17:00:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY2MjI5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY2OTIxMw==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r490669213", "bodyText": "Reconsidered this again,  I saw argument record is actually read from the requiredSchema,  do we need to wrap this record by the same requiredSchema again  ?  Pls see https://github.com/apache/iceberg/pull/1444/files#diff-a8a025276b1d93b0830f2ee6c91118efR76.\nIf the record is actually matching the requiredSchema,  then we  could just return the record in asStructLike method  ?  Also we don't have to overwrite pos method in this classes again ?", "author": "openinx", "createdAt": "2020-09-18T02:47:27Z", "path": "data/src/main/java/org/apache/iceberg/data/GenericDeleteFilter.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data;\n+\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+\n+public class GenericDeleteFilter extends DeleteFilter<Record> {\n+  private final FileIO io;\n+  private final InternalRecordWrapper asStructLike;\n+\n+  public GenericDeleteFilter(FileIO io, FileScanTask task, Schema tableSchema, Schema requestedSchema) {\n+    super(task, tableSchema, requestedSchema);\n+    this.io = io;\n+    this.asStructLike = new InternalRecordWrapper(requiredSchema().asStruct());\n+  }\n+\n+  @Override\n+  protected long pos(Record record) {\n+    return (Long) posAccessor().get(record);\n+  }\n+\n+  @Override\n+  protected StructLike asStructLike(Record record) {\n+    return asStructLike.wrap(record);", "originalCommit": "86389e404b00fc64e4c7b719ddcf4f7955915cbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTA3NzM4Mg==", "url": "https://github.com/apache/iceberg/pull/1444#discussion_r491077382", "bodyText": "The purpose of the wrapper here is to translate from Iceberg's generic to the internal representation for values. For example, generics will pass timestamptz as an OffsetDateTime to users, but internally Iceberg uses microseconds from epoch as a long.", "author": "rdblue", "createdAt": "2020-09-18T16:58:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY2OTIxMw=="}], "type": "inlineReview"}]}