{"pr_number": 1072, "pr_title": "Support nested identity partitioning for GenericOrcReader", "pr_createdAt": "2020-05-27T07:04:23Z", "pr_url": "https://github.com/apache/iceberg/pull/1072", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUwNjkzNQ==", "url": "https://github.com/apache/iceberg/pull/1072#discussion_r431506935", "bodyText": "Is the rename needed? This could cause commit conflicts.", "author": "rdblue", "createdAt": "2020-05-27T23:53:02Z", "path": "data/src/main/java/org/apache/iceberg/data/TableScanIterable.java", "diffHunk": "@@ -80,15 +80,14 @@\n \n   private CloseableIterable<Record> open(FileScanTask task) {\n     InputFile input = ops.io().newInputFile(task.file().path().toString());\n-    Map<Integer, ?> partition = PartitionUtil.constantsMap(task, TableScanIterable::convertConstant);", "originalCommit": "9f0cecd7443e99b2daece86d4d143a98e597ed9e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU2ODQzNg==", "url": "https://github.com/apache/iceberg/pull/1072#discussion_r431568436", "bodyText": "fixed", "author": "rdsr", "createdAt": "2020-05-28T04:00:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUwNjkzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUwNzA1NQ==", "url": "https://github.com/apache/iceberg/pull/1072#discussion_r431507055", "bodyText": "Thanks!", "author": "rdblue", "createdAt": "2020-05-27T23:53:24Z", "path": "data/src/main/java/org/apache/iceberg/data/TableScanIterable.java", "diffHunk": "@@ -80,15 +80,14 @@\n \n   private CloseableIterable<Record> open(FileScanTask task) {\n     InputFile input = ops.io().newInputFile(task.file().path().toString());\n-    Map<Integer, ?> partition = PartitionUtil.constantsMap(task, TableScanIterable::convertConstant);\n+    Map<Integer, ?> idToConstant = PartitionUtil.constantsMap(task, TableScanIterable::convertConstant);\n \n-    // TODO: join to partition data from the manifest file", "originalCommit": "9f0cecd7443e99b2daece86d4d143a98e597ed9e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUwMzM5Nw==", "url": "https://github.com/apache/iceberg/pull/1072#discussion_r431503397", "bodyText": "Can we just check iPrimitive instead? Previously we didn't have access to the Iceberg type, so we had to look at the type attributes. This is similar to what is being done in SparkParquetReaders", "author": "shardulm94", "createdAt": "2020-05-27T23:41:43Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcReader.java", "diffHunk": "@@ -19,367 +19,125 @@\n \n package org.apache.iceberg.data.orc;\n \n-import java.math.BigDecimal;\n-import java.nio.ByteBuffer;\n-import java.nio.charset.StandardCharsets;\n-import java.time.Instant;\n-import java.time.LocalDate;\n-import java.time.LocalDateTime;\n-import java.time.LocalTime;\n-import java.time.OffsetDateTime;\n-import java.time.ZoneOffset;\n-import java.util.Arrays;\n+import java.util.Collections;\n import java.util.List;\n import java.util.Map;\n-import java.util.UUID;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.GenericRecord;\n import org.apache.iceberg.data.Record;\n import org.apache.iceberg.orc.ORCSchemaUtil;\n import org.apache.iceberg.orc.OrcRowReader;\n-import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n-import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.orc.OrcSchemaWithTypeVisitor;\n+import org.apache.iceberg.orc.OrcValueReader;\n+import org.apache.iceberg.orc.OrcValueReaders;\n+import org.apache.iceberg.types.Type;\n import org.apache.iceberg.types.Types;\n import org.apache.orc.TypeDescription;\n-import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n import org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch;\n \n-/**\n- * ORC reader for Generic Record.\n- */\n public class GenericOrcReader implements OrcRowReader<Record> {\n+  private final OrcValueReader<?> reader;\n \n-  private final Schema schema;\n-  private final List<TypeDescription> columns;\n-  private final Converter[] converters;\n-\n-  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n-  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n-\n-  private GenericOrcReader(Schema expectedSchema, TypeDescription readSchema) {\n-    this.schema = expectedSchema;\n-    this.columns = readSchema.getChildren();\n-    this.converters = buildConverters();\n+  public GenericOrcReader(\n+      org.apache.iceberg.Schema expectedSchema, TypeDescription readOrcSchema, Map<Integer, ?> idToConstant) {\n+    this.reader = OrcSchemaWithTypeVisitor.visit(expectedSchema, readOrcSchema, new ReadBuilder(idToConstant));\n   }\n \n-  private Converter[] buildConverters() {\n-    Preconditions.checkState(schema.columns().size() == columns.size(),\n-        \"Expected schema must have same number of columns as projection.\");\n-    Converter[] newConverters = new Converter[columns.size()];\n-    List<Types.NestedField> icebergCols = schema.columns();\n-    for (int c = 0; c < newConverters.length; ++c) {\n-      newConverters[c] = buildConverter(icebergCols.get(c), columns.get(c));\n-    }\n-    return newConverters;\n+  public static OrcRowReader<Record> buildReader(Schema expectedSchema, TypeDescription fileSchema) {\n+    return new GenericOrcReader(expectedSchema, fileSchema, Collections.emptyMap());\n   }\n \n-  public static OrcRowReader<Record> buildReader(Schema expectedSchema, TypeDescription fileSchema) {\n-    return new GenericOrcReader(expectedSchema, fileSchema);\n+  public static OrcRowReader<Record> buildReader(\n+      Schema expectedSchema, TypeDescription fileSchema, Map<Integer, ?> idToConstant) {\n+    return new GenericOrcReader(expectedSchema, fileSchema, idToConstant);\n   }\n \n   @Override\n   public Record read(VectorizedRowBatch batch, int row) {\n-    Record rowRecord = GenericRecord.create(schema);\n-    for (int c = 0; c < batch.cols.length; ++c) {\n-      rowRecord.set(c, converters[c].convert(batch.cols[c], row));\n-    }\n-    return rowRecord;\n-  }\n-\n-  interface Converter<T> {\n-    default T convert(ColumnVector vector, int row) {\n-      int rowIndex = vector.isRepeating ? 0 : row;\n-      if (!vector.noNulls && vector.isNull[rowIndex]) {\n-        return null;\n-      } else {\n-        return convertNonNullValue(vector, rowIndex);\n-      }\n-    }\n-\n-    T convertNonNullValue(ColumnVector vector, int row);\n-  }\n-\n-  private static class BooleanConverter implements Converter<Boolean> {\n-    @Override\n-    public Boolean convertNonNullValue(ColumnVector vector, int row) {\n-      return ((LongColumnVector) vector).vector[row] != 0;\n-    }\n-  }\n-\n-  private static class ByteConverter implements Converter<Byte> {\n-    @Override\n-    public Byte convertNonNullValue(ColumnVector vector, int row) {\n-      return (byte) ((LongColumnVector) vector).vector[row];\n-    }\n-  }\n-\n-  private static class ShortConverter implements Converter<Short> {\n-    @Override\n-    public Short convertNonNullValue(ColumnVector vector, int row) {\n-      return (short) ((LongColumnVector) vector).vector[row];\n-    }\n-  }\n-\n-  private static class IntConverter implements Converter<Integer> {\n-    @Override\n-    public Integer convertNonNullValue(ColumnVector vector, int row) {\n-      return (int) ((LongColumnVector) vector).vector[row];\n-    }\n-  }\n-\n-  private static class TimeConverter implements Converter<LocalTime> {\n-    @Override\n-    public LocalTime convertNonNullValue(ColumnVector vector, int row) {\n-      return LocalTime.ofNanoOfDay(((LongColumnVector) vector).vector[row] * 1_000);\n-    }\n-  }\n-\n-  private static class DateConverter implements Converter<LocalDate> {\n-    @Override\n-    public LocalDate convertNonNullValue(ColumnVector vector, int row) {\n-      return EPOCH_DAY.plusDays((int) ((LongColumnVector) vector).vector[row]);\n-    }\n-  }\n-\n-  private static class LongConverter implements Converter<Long> {\n-    @Override\n-    public Long convertNonNullValue(ColumnVector vector, int row) {\n-      return ((LongColumnVector) vector).vector[row];\n-    }\n-  }\n-\n-  private static class FloatConverter implements Converter<Float> {\n-    @Override\n-    public Float convertNonNullValue(ColumnVector vector, int row) {\n-      return (float) ((DoubleColumnVector) vector).vector[row];\n-    }\n-  }\n-\n-  private static class DoubleConverter implements Converter<Double> {\n-    @Override\n-    public Double convertNonNullValue(ColumnVector vector, int row) {\n-      return ((DoubleColumnVector) vector).vector[row];\n-    }\n-  }\n-\n-  private static class TimestampTzConverter implements Converter<OffsetDateTime> {\n-    @Override\n-    public OffsetDateTime convertNonNullValue(ColumnVector vector, int row) {\n-      TimestampColumnVector tcv = (TimestampColumnVector) vector;\n-      return Instant.ofEpochSecond(Math.floorDiv(tcv.time[row], 1_000), tcv.nanos[row]).atOffset(ZoneOffset.UTC);\n-    }\n-  }\n-\n-  private static class TimestampConverter implements Converter<LocalDateTime> {\n-    @Override\n-    public LocalDateTime convertNonNullValue(ColumnVector vector, int row) {\n-      TimestampColumnVector tcv = (TimestampColumnVector) vector;\n-      return Instant.ofEpochSecond(Math.floorDiv(tcv.time[row], 1_000), tcv.nanos[row]).atOffset(ZoneOffset.UTC)\n-          .toLocalDateTime();\n-    }\n-  }\n-\n-  private static class FixedConverter implements Converter<byte[]> {\n-    @Override\n-    public byte[] convertNonNullValue(ColumnVector vector, int row) {\n-      BytesColumnVector bytesVector = (BytesColumnVector) vector;\n-      return Arrays.copyOfRange(bytesVector.vector[row], bytesVector.start[row],\n-          bytesVector.start[row] + bytesVector.length[row]);\n-    }\n-  }\n-\n-  private static class BinaryConverter implements Converter<ByteBuffer> {\n-    @Override\n-    public ByteBuffer convertNonNullValue(ColumnVector vector, int row) {\n-      BytesColumnVector bytesVector = (BytesColumnVector) vector;\n-      return ByteBuffer.wrap(bytesVector.vector[row], bytesVector.start[row], bytesVector.length[row]);\n-    }\n-  }\n-\n-  private static class UUIDConverter implements Converter<UUID> {\n-    @Override\n-    public UUID convertNonNullValue(ColumnVector vector, int row) {\n-      BytesColumnVector bytesVector = (BytesColumnVector) vector;\n-      ByteBuffer buf = ByteBuffer.wrap(bytesVector.vector[row], bytesVector.start[row], bytesVector.length[row]);\n-      long mostSigBits = buf.getLong();\n-      long leastSigBits = buf.getLong();\n-      return new UUID(mostSigBits, leastSigBits);\n-    }\n-  }\n-\n-  private static class StringConverter implements Converter<String> {\n-    @Override\n-    public String convertNonNullValue(ColumnVector vector, int row) {\n-      BytesColumnVector bytesVector = (BytesColumnVector) vector;\n-      return new String(bytesVector.vector[row], bytesVector.start[row], bytesVector.length[row],\n-          StandardCharsets.UTF_8);\n-    }\n-  }\n-\n-  private static class DecimalConverter implements Converter<BigDecimal> {\n-    @Override\n-    public BigDecimal convertNonNullValue(ColumnVector vector, int row) {\n-      DecimalColumnVector cv = (DecimalColumnVector) vector;\n-      return cv.vector[row].getHiveDecimal().bigDecimalValue().setScale(cv.scale);\n-    }\n-  }\n-\n-  private static class ListConverter implements Converter<List<?>> {\n-    private final Converter childConverter;\n-\n-    ListConverter(Types.NestedField icebergField, TypeDescription schema) {\n-      Preconditions.checkArgument(icebergField.type().isListType());\n-      TypeDescription child = schema.getChildren().get(0);\n-\n-      childConverter = buildConverter(icebergField\n-          .type()\n-          .asListType()\n-          .fields()\n-          .get(0), child);\n-    }\n-\n-    @Override\n-    public List<?> convertNonNullValue(ColumnVector vector, int row) {\n-      ListColumnVector listVector = (ListColumnVector) vector;\n-      int offset = (int) listVector.offsets[row];\n-      int length = (int) listVector.lengths[row];\n-\n-      List<Object> list = Lists.newArrayListWithExpectedSize(length);\n-      for (int c = 0; c < length; ++c) {\n-        list.add(childConverter.convert(listVector.child, offset + c));\n-      }\n-      return list;\n-    }\n-  }\n-\n-  private static class MapConverter implements Converter<Map<?, ?>> {\n-    private final Converter keyConvert;\n-    private final Converter valueConvert;\n-\n-    MapConverter(Types.NestedField icebergField, TypeDescription schema) {\n-      Preconditions.checkArgument(icebergField.type().isMapType());\n-      TypeDescription keyType = schema.getChildren().get(0);\n-      TypeDescription valueType = schema.getChildren().get(1);\n-      List<Types.NestedField> mapFields = icebergField.type().asMapType().fields();\n-\n-      keyConvert = buildConverter(mapFields.get(0), keyType);\n-      valueConvert = buildConverter(mapFields.get(1), valueType);\n-    }\n-\n-    @Override\n-    public Map<?, ?> convertNonNullValue(ColumnVector vector, int row) {\n-      MapColumnVector mapVector = (MapColumnVector) vector;\n-      final int offset = (int) mapVector.offsets[row];\n-      final int length = (int) mapVector.lengths[row];\n-\n-      Map<Object, Object> map = Maps.newHashMapWithExpectedSize(length);\n-      for (int c = 0; c < length; ++c) {\n-        Object key = keyConvert.convert(mapVector.keys, offset + c);\n-        Object value = valueConvert.convert(mapVector.values, offset + c);\n-        map.put(key, value);\n+    return (Record) reader.read(new StructColumnVector(batch.size, batch.cols), row);\n+  }\n+\n+  private static class ReadBuilder extends OrcSchemaWithTypeVisitor<OrcValueReader<?>> {\n+    private final Map<Integer, ?> idToConstant;\n+\n+    private ReadBuilder(Map<Integer, ?> idToConstant) {\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public OrcValueReader<?> record(\n+        Types.StructType expected, TypeDescription record, List<String> names, List<OrcValueReader<?>> fields) {\n+      return GenericOrcReaders.struct(fields, expected, idToConstant);\n+    }\n+\n+    @Override\n+    public OrcValueReader<?> list(Types.ListType iList, TypeDescription array, OrcValueReader<?> elementReader) {\n+      return GenericOrcReaders.array(elementReader);\n+    }\n+\n+    @Override\n+    public OrcValueReader<?> map(\n+        Types.MapType iMap, TypeDescription map, OrcValueReader<?> keyReader, OrcValueReader<?> valueReader) {\n+      return GenericOrcReaders.map(keyReader, valueReader);\n+    }\n+\n+    @Override\n+    public OrcValueReader<?> primitive(Type.PrimitiveType iPrimitive, TypeDescription primitive) {\n+      switch (primitive.getCategory()) {\n+        case BOOLEAN:\n+          return OrcValueReaders.booleans();\n+        case BYTE:\n+          // Iceberg does not have a byte type. Use int\n+        case SHORT:\n+          // Iceberg does not have a short type. Use int\n+        case INT:\n+          return OrcValueReaders.ints();\n+        case LONG:\n+          String longAttributeValue = primitive.getAttributeValue(ORCSchemaUtil.ICEBERG_LONG_TYPE_ATTRIBUTE);\n+          ORCSchemaUtil.LongType longType = longAttributeValue == null ? ORCSchemaUtil.LongType.LONG :\n+              ORCSchemaUtil.LongType.valueOf(longAttributeValue);", "originalCommit": "9f0cecd7443e99b2daece86d4d143a98e597ed9e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUwMzcxMQ==", "url": "https://github.com/apache/iceberg/pull/1072#discussion_r431503711", "bodyText": "Can we check iPrimitive for type information instead?", "author": "shardulm94", "createdAt": "2020-05-27T23:42:39Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcReader.java", "diffHunk": "@@ -19,367 +19,125 @@\n \n package org.apache.iceberg.data.orc;\n \n-import java.math.BigDecimal;\n-import java.nio.ByteBuffer;\n-import java.nio.charset.StandardCharsets;\n-import java.time.Instant;\n-import java.time.LocalDate;\n-import java.time.LocalDateTime;\n-import java.time.LocalTime;\n-import java.time.OffsetDateTime;\n-import java.time.ZoneOffset;\n-import java.util.Arrays;\n+import java.util.Collections;\n import java.util.List;\n import java.util.Map;\n-import java.util.UUID;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.GenericRecord;\n import org.apache.iceberg.data.Record;\n import org.apache.iceberg.orc.ORCSchemaUtil;\n import org.apache.iceberg.orc.OrcRowReader;\n-import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n-import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.orc.OrcSchemaWithTypeVisitor;\n+import org.apache.iceberg.orc.OrcValueReader;\n+import org.apache.iceberg.orc.OrcValueReaders;\n+import org.apache.iceberg.types.Type;\n import org.apache.iceberg.types.Types;\n import org.apache.orc.TypeDescription;\n-import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n import org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch;\n \n-/**\n- * ORC reader for Generic Record.\n- */\n public class GenericOrcReader implements OrcRowReader<Record> {\n+  private final OrcValueReader<?> reader;\n \n-  private final Schema schema;\n-  private final List<TypeDescription> columns;\n-  private final Converter[] converters;\n-\n-  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n-  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n-\n-  private GenericOrcReader(Schema expectedSchema, TypeDescription readSchema) {\n-    this.schema = expectedSchema;\n-    this.columns = readSchema.getChildren();\n-    this.converters = buildConverters();\n+  public GenericOrcReader(\n+      org.apache.iceberg.Schema expectedSchema, TypeDescription readOrcSchema, Map<Integer, ?> idToConstant) {\n+    this.reader = OrcSchemaWithTypeVisitor.visit(expectedSchema, readOrcSchema, new ReadBuilder(idToConstant));\n   }\n \n-  private Converter[] buildConverters() {\n-    Preconditions.checkState(schema.columns().size() == columns.size(),\n-        \"Expected schema must have same number of columns as projection.\");\n-    Converter[] newConverters = new Converter[columns.size()];\n-    List<Types.NestedField> icebergCols = schema.columns();\n-    for (int c = 0; c < newConverters.length; ++c) {\n-      newConverters[c] = buildConverter(icebergCols.get(c), columns.get(c));\n-    }\n-    return newConverters;\n+  public static OrcRowReader<Record> buildReader(Schema expectedSchema, TypeDescription fileSchema) {\n+    return new GenericOrcReader(expectedSchema, fileSchema, Collections.emptyMap());\n   }\n \n-  public static OrcRowReader<Record> buildReader(Schema expectedSchema, TypeDescription fileSchema) {\n-    return new GenericOrcReader(expectedSchema, fileSchema);\n+  public static OrcRowReader<Record> buildReader(\n+      Schema expectedSchema, TypeDescription fileSchema, Map<Integer, ?> idToConstant) {\n+    return new GenericOrcReader(expectedSchema, fileSchema, idToConstant);\n   }\n \n   @Override\n   public Record read(VectorizedRowBatch batch, int row) {\n-    Record rowRecord = GenericRecord.create(schema);\n-    for (int c = 0; c < batch.cols.length; ++c) {\n-      rowRecord.set(c, converters[c].convert(batch.cols[c], row));\n-    }\n-    return rowRecord;\n-  }\n-\n-  interface Converter<T> {\n-    default T convert(ColumnVector vector, int row) {\n-      int rowIndex = vector.isRepeating ? 0 : row;\n-      if (!vector.noNulls && vector.isNull[rowIndex]) {\n-        return null;\n-      } else {\n-        return convertNonNullValue(vector, rowIndex);\n-      }\n-    }\n-\n-    T convertNonNullValue(ColumnVector vector, int row);\n-  }\n-\n-  private static class BooleanConverter implements Converter<Boolean> {\n-    @Override\n-    public Boolean convertNonNullValue(ColumnVector vector, int row) {\n-      return ((LongColumnVector) vector).vector[row] != 0;\n-    }\n-  }\n-\n-  private static class ByteConverter implements Converter<Byte> {\n-    @Override\n-    public Byte convertNonNullValue(ColumnVector vector, int row) {\n-      return (byte) ((LongColumnVector) vector).vector[row];\n-    }\n-  }\n-\n-  private static class ShortConverter implements Converter<Short> {\n-    @Override\n-    public Short convertNonNullValue(ColumnVector vector, int row) {\n-      return (short) ((LongColumnVector) vector).vector[row];\n-    }\n-  }\n-\n-  private static class IntConverter implements Converter<Integer> {\n-    @Override\n-    public Integer convertNonNullValue(ColumnVector vector, int row) {\n-      return (int) ((LongColumnVector) vector).vector[row];\n-    }\n-  }\n-\n-  private static class TimeConverter implements Converter<LocalTime> {\n-    @Override\n-    public LocalTime convertNonNullValue(ColumnVector vector, int row) {\n-      return LocalTime.ofNanoOfDay(((LongColumnVector) vector).vector[row] * 1_000);\n-    }\n-  }\n-\n-  private static class DateConverter implements Converter<LocalDate> {\n-    @Override\n-    public LocalDate convertNonNullValue(ColumnVector vector, int row) {\n-      return EPOCH_DAY.plusDays((int) ((LongColumnVector) vector).vector[row]);\n-    }\n-  }\n-\n-  private static class LongConverter implements Converter<Long> {\n-    @Override\n-    public Long convertNonNullValue(ColumnVector vector, int row) {\n-      return ((LongColumnVector) vector).vector[row];\n-    }\n-  }\n-\n-  private static class FloatConverter implements Converter<Float> {\n-    @Override\n-    public Float convertNonNullValue(ColumnVector vector, int row) {\n-      return (float) ((DoubleColumnVector) vector).vector[row];\n-    }\n-  }\n-\n-  private static class DoubleConverter implements Converter<Double> {\n-    @Override\n-    public Double convertNonNullValue(ColumnVector vector, int row) {\n-      return ((DoubleColumnVector) vector).vector[row];\n-    }\n-  }\n-\n-  private static class TimestampTzConverter implements Converter<OffsetDateTime> {\n-    @Override\n-    public OffsetDateTime convertNonNullValue(ColumnVector vector, int row) {\n-      TimestampColumnVector tcv = (TimestampColumnVector) vector;\n-      return Instant.ofEpochSecond(Math.floorDiv(tcv.time[row], 1_000), tcv.nanos[row]).atOffset(ZoneOffset.UTC);\n-    }\n-  }\n-\n-  private static class TimestampConverter implements Converter<LocalDateTime> {\n-    @Override\n-    public LocalDateTime convertNonNullValue(ColumnVector vector, int row) {\n-      TimestampColumnVector tcv = (TimestampColumnVector) vector;\n-      return Instant.ofEpochSecond(Math.floorDiv(tcv.time[row], 1_000), tcv.nanos[row]).atOffset(ZoneOffset.UTC)\n-          .toLocalDateTime();\n-    }\n-  }\n-\n-  private static class FixedConverter implements Converter<byte[]> {\n-    @Override\n-    public byte[] convertNonNullValue(ColumnVector vector, int row) {\n-      BytesColumnVector bytesVector = (BytesColumnVector) vector;\n-      return Arrays.copyOfRange(bytesVector.vector[row], bytesVector.start[row],\n-          bytesVector.start[row] + bytesVector.length[row]);\n-    }\n-  }\n-\n-  private static class BinaryConverter implements Converter<ByteBuffer> {\n-    @Override\n-    public ByteBuffer convertNonNullValue(ColumnVector vector, int row) {\n-      BytesColumnVector bytesVector = (BytesColumnVector) vector;\n-      return ByteBuffer.wrap(bytesVector.vector[row], bytesVector.start[row], bytesVector.length[row]);\n-    }\n-  }\n-\n-  private static class UUIDConverter implements Converter<UUID> {\n-    @Override\n-    public UUID convertNonNullValue(ColumnVector vector, int row) {\n-      BytesColumnVector bytesVector = (BytesColumnVector) vector;\n-      ByteBuffer buf = ByteBuffer.wrap(bytesVector.vector[row], bytesVector.start[row], bytesVector.length[row]);\n-      long mostSigBits = buf.getLong();\n-      long leastSigBits = buf.getLong();\n-      return new UUID(mostSigBits, leastSigBits);\n-    }\n-  }\n-\n-  private static class StringConverter implements Converter<String> {\n-    @Override\n-    public String convertNonNullValue(ColumnVector vector, int row) {\n-      BytesColumnVector bytesVector = (BytesColumnVector) vector;\n-      return new String(bytesVector.vector[row], bytesVector.start[row], bytesVector.length[row],\n-          StandardCharsets.UTF_8);\n-    }\n-  }\n-\n-  private static class DecimalConverter implements Converter<BigDecimal> {\n-    @Override\n-    public BigDecimal convertNonNullValue(ColumnVector vector, int row) {\n-      DecimalColumnVector cv = (DecimalColumnVector) vector;\n-      return cv.vector[row].getHiveDecimal().bigDecimalValue().setScale(cv.scale);\n-    }\n-  }\n-\n-  private static class ListConverter implements Converter<List<?>> {\n-    private final Converter childConverter;\n-\n-    ListConverter(Types.NestedField icebergField, TypeDescription schema) {\n-      Preconditions.checkArgument(icebergField.type().isListType());\n-      TypeDescription child = schema.getChildren().get(0);\n-\n-      childConverter = buildConverter(icebergField\n-          .type()\n-          .asListType()\n-          .fields()\n-          .get(0), child);\n-    }\n-\n-    @Override\n-    public List<?> convertNonNullValue(ColumnVector vector, int row) {\n-      ListColumnVector listVector = (ListColumnVector) vector;\n-      int offset = (int) listVector.offsets[row];\n-      int length = (int) listVector.lengths[row];\n-\n-      List<Object> list = Lists.newArrayListWithExpectedSize(length);\n-      for (int c = 0; c < length; ++c) {\n-        list.add(childConverter.convert(listVector.child, offset + c));\n-      }\n-      return list;\n-    }\n-  }\n-\n-  private static class MapConverter implements Converter<Map<?, ?>> {\n-    private final Converter keyConvert;\n-    private final Converter valueConvert;\n-\n-    MapConverter(Types.NestedField icebergField, TypeDescription schema) {\n-      Preconditions.checkArgument(icebergField.type().isMapType());\n-      TypeDescription keyType = schema.getChildren().get(0);\n-      TypeDescription valueType = schema.getChildren().get(1);\n-      List<Types.NestedField> mapFields = icebergField.type().asMapType().fields();\n-\n-      keyConvert = buildConverter(mapFields.get(0), keyType);\n-      valueConvert = buildConverter(mapFields.get(1), valueType);\n-    }\n-\n-    @Override\n-    public Map<?, ?> convertNonNullValue(ColumnVector vector, int row) {\n-      MapColumnVector mapVector = (MapColumnVector) vector;\n-      final int offset = (int) mapVector.offsets[row];\n-      final int length = (int) mapVector.lengths[row];\n-\n-      Map<Object, Object> map = Maps.newHashMapWithExpectedSize(length);\n-      for (int c = 0; c < length; ++c) {\n-        Object key = keyConvert.convert(mapVector.keys, offset + c);\n-        Object value = valueConvert.convert(mapVector.values, offset + c);\n-        map.put(key, value);\n+    return (Record) reader.read(new StructColumnVector(batch.size, batch.cols), row);\n+  }\n+\n+  private static class ReadBuilder extends OrcSchemaWithTypeVisitor<OrcValueReader<?>> {\n+    private final Map<Integer, ?> idToConstant;\n+\n+    private ReadBuilder(Map<Integer, ?> idToConstant) {\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public OrcValueReader<?> record(\n+        Types.StructType expected, TypeDescription record, List<String> names, List<OrcValueReader<?>> fields) {\n+      return GenericOrcReaders.struct(fields, expected, idToConstant);\n+    }\n+\n+    @Override\n+    public OrcValueReader<?> list(Types.ListType iList, TypeDescription array, OrcValueReader<?> elementReader) {\n+      return GenericOrcReaders.array(elementReader);\n+    }\n+\n+    @Override\n+    public OrcValueReader<?> map(\n+        Types.MapType iMap, TypeDescription map, OrcValueReader<?> keyReader, OrcValueReader<?> valueReader) {\n+      return GenericOrcReaders.map(keyReader, valueReader);\n+    }\n+\n+    @Override\n+    public OrcValueReader<?> primitive(Type.PrimitiveType iPrimitive, TypeDescription primitive) {\n+      switch (primitive.getCategory()) {\n+        case BOOLEAN:\n+          return OrcValueReaders.booleans();\n+        case BYTE:\n+          // Iceberg does not have a byte type. Use int\n+        case SHORT:\n+          // Iceberg does not have a short type. Use int\n+        case INT:\n+          return OrcValueReaders.ints();\n+        case LONG:\n+          String longAttributeValue = primitive.getAttributeValue(ORCSchemaUtil.ICEBERG_LONG_TYPE_ATTRIBUTE);\n+          ORCSchemaUtil.LongType longType = longAttributeValue == null ? ORCSchemaUtil.LongType.LONG :\n+              ORCSchemaUtil.LongType.valueOf(longAttributeValue);\n+          switch (longType) {\n+            case TIME:\n+              return GenericOrcReaders.times();\n+            case LONG:\n+              return OrcValueReaders.longs();\n+            default:\n+              throw new IllegalStateException(\"Unhandled Long type found in ORC type attribute: \" + longType);\n+          }\n+\n+        case FLOAT:\n+          return OrcValueReaders.floats();\n+        case DOUBLE:\n+          return OrcValueReaders.doubles();\n+        case DATE:\n+          return GenericOrcReaders.dates();\n+        case TIMESTAMP:\n+          return GenericOrcReaders.timestamps();\n+        case TIMESTAMP_INSTANT:\n+          return GenericOrcReaders.timestampTzs();\n+        case DECIMAL:\n+          return GenericOrcReaders.decimals();\n+        case CHAR:\n+        case VARCHAR:\n+        case STRING:\n+          return GenericOrcReaders.strings();\n+        case BINARY:\n+          String binaryAttributeValue = primitive.getAttributeValue(ORCSchemaUtil.ICEBERG_BINARY_TYPE_ATTRIBUTE);\n+          ORCSchemaUtil.BinaryType binaryType = binaryAttributeValue == null ? ORCSchemaUtil.BinaryType.BINARY :\n+              ORCSchemaUtil.BinaryType.valueOf(binaryAttributeValue);", "originalCommit": "9f0cecd7443e99b2daece86d4d143a98e597ed9e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUwMzk5MQ==", "url": "https://github.com/apache/iceberg/pull/1072#discussion_r431503991", "bodyText": "OrcValueReader<LocalTime>", "author": "shardulm94", "createdAt": "2020-05-27T23:43:31Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcReaders.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data.orc;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.orc.OrcValueReader;\n+import org.apache.iceberg.orc.OrcValueReaders;\n+import org.apache.iceberg.types.Types;\n+import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n+\n+\n+public class GenericOrcReaders {\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  private GenericOrcReaders() {\n+  }\n+\n+  public static OrcValueReader<Record> struct(\n+      List<OrcValueReader<?>> readers, Types.StructType struct, Map<Integer, ?> idToConstant) {\n+    return new StructReader(readers, struct, idToConstant);\n+  }\n+\n+  public static OrcValueReader<List<?>> array(OrcValueReader<?> elementReader) {\n+    return new ListReader(elementReader);\n+  }\n+\n+  public static OrcValueReader<Map<?, ?>> map(OrcValueReader<?> keyReader, OrcValueReader<?> valueReader) {\n+    return new MapReader(keyReader, valueReader);\n+  }\n+\n+  public static OrcValueReader<OffsetDateTime> timestampTzs() {\n+    return TimestampTzReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<BigDecimal> decimals() {\n+    return DecimalReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<String> strings() {\n+    return StringReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<UUID> uuid() {\n+    return UUIDReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<ByteBuffer> bytes() {\n+    return BytesReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<?> times() {", "originalCommit": "9f0cecd7443e99b2daece86d4d143a98e597ed9e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU0MDUxMA==", "url": "https://github.com/apache/iceberg/pull/1072#discussion_r431540510", "bodyText": "Use DateTimeUtil.daysFromDays?", "author": "shardulm94", "createdAt": "2020-05-28T02:00:18Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcReaders.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data.orc;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.orc.OrcValueReader;\n+import org.apache.iceberg.orc.OrcValueReaders;\n+import org.apache.iceberg.types.Types;\n+import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n+\n+\n+public class GenericOrcReaders {\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  private GenericOrcReaders() {\n+  }\n+\n+  public static OrcValueReader<Record> struct(\n+      List<OrcValueReader<?>> readers, Types.StructType struct, Map<Integer, ?> idToConstant) {\n+    return new StructReader(readers, struct, idToConstant);\n+  }\n+\n+  public static OrcValueReader<List<?>> array(OrcValueReader<?> elementReader) {\n+    return new ListReader(elementReader);\n+  }\n+\n+  public static OrcValueReader<Map<?, ?>> map(OrcValueReader<?> keyReader, OrcValueReader<?> valueReader) {\n+    return new MapReader(keyReader, valueReader);\n+  }\n+\n+  public static OrcValueReader<OffsetDateTime> timestampTzs() {\n+    return TimestampTzReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<BigDecimal> decimals() {\n+    return DecimalReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<String> strings() {\n+    return StringReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<UUID> uuid() {\n+    return UUIDReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<ByteBuffer> bytes() {\n+    return BytesReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<?> times() {\n+    return TimeReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<LocalDate> dates() {\n+    return DateReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<LocalDateTime> timestamps() {\n+    return TimestampReader.INSTANCE;\n+  }\n+\n+  private static class TimestampTzReader implements OrcValueReader<OffsetDateTime> {\n+    public static final OrcValueReader<OffsetDateTime> INSTANCE = new TimestampTzReader();\n+\n+    private TimestampTzReader() {\n+    }\n+\n+    @Override\n+    public OffsetDateTime nonNullRead(ColumnVector vector, int row) {\n+      TimestampColumnVector tcv = (TimestampColumnVector) vector;\n+      return Instant.ofEpochSecond(Math.floorDiv(tcv.time[row], 1_000), tcv.nanos[row]).atOffset(ZoneOffset.UTC);\n+    }\n+  }\n+\n+  private static class TimeReader implements OrcValueReader<LocalTime> {\n+    public static final OrcValueReader<LocalTime> INSTANCE = new TimeReader();\n+\n+    private TimeReader() {\n+    }\n+\n+    @Override\n+    public LocalTime nonNullRead(ColumnVector vector, int row) {\n+      return LocalTime.ofNanoOfDay(((LongColumnVector) vector).vector[row] * 1_000);\n+    }\n+  }\n+\n+  private static class DateReader implements OrcValueReader<LocalDate> {\n+    public static final OrcValueReader<LocalDate> INSTANCE = new DateReader();\n+\n+    private DateReader() {\n+    }\n+\n+    @Override\n+    public LocalDate nonNullRead(ColumnVector vector, int row) {\n+      return EPOCH_DAY.plusDays((int) ((LongColumnVector) vector).vector[row]);", "originalCommit": "9f0cecd7443e99b2daece86d4d143a98e597ed9e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU0MDg0Ng==", "url": "https://github.com/apache/iceberg/pull/1072#discussion_r431540846", "bodyText": "Use DateTimeUtil.timeFromMicros?", "author": "shardulm94", "createdAt": "2020-05-28T02:01:35Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcReaders.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data.orc;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.orc.OrcValueReader;\n+import org.apache.iceberg.orc.OrcValueReaders;\n+import org.apache.iceberg.types.Types;\n+import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n+\n+\n+public class GenericOrcReaders {\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  private GenericOrcReaders() {\n+  }\n+\n+  public static OrcValueReader<Record> struct(\n+      List<OrcValueReader<?>> readers, Types.StructType struct, Map<Integer, ?> idToConstant) {\n+    return new StructReader(readers, struct, idToConstant);\n+  }\n+\n+  public static OrcValueReader<List<?>> array(OrcValueReader<?> elementReader) {\n+    return new ListReader(elementReader);\n+  }\n+\n+  public static OrcValueReader<Map<?, ?>> map(OrcValueReader<?> keyReader, OrcValueReader<?> valueReader) {\n+    return new MapReader(keyReader, valueReader);\n+  }\n+\n+  public static OrcValueReader<OffsetDateTime> timestampTzs() {\n+    return TimestampTzReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<BigDecimal> decimals() {\n+    return DecimalReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<String> strings() {\n+    return StringReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<UUID> uuid() {\n+    return UUIDReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<ByteBuffer> bytes() {\n+    return BytesReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<?> times() {\n+    return TimeReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<LocalDate> dates() {\n+    return DateReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<LocalDateTime> timestamps() {\n+    return TimestampReader.INSTANCE;\n+  }\n+\n+  private static class TimestampTzReader implements OrcValueReader<OffsetDateTime> {\n+    public static final OrcValueReader<OffsetDateTime> INSTANCE = new TimestampTzReader();\n+\n+    private TimestampTzReader() {\n+    }\n+\n+    @Override\n+    public OffsetDateTime nonNullRead(ColumnVector vector, int row) {\n+      TimestampColumnVector tcv = (TimestampColumnVector) vector;\n+      return Instant.ofEpochSecond(Math.floorDiv(tcv.time[row], 1_000), tcv.nanos[row]).atOffset(ZoneOffset.UTC);\n+    }\n+  }\n+\n+  private static class TimeReader implements OrcValueReader<LocalTime> {\n+    public static final OrcValueReader<LocalTime> INSTANCE = new TimeReader();\n+\n+    private TimeReader() {\n+    }\n+\n+    @Override\n+    public LocalTime nonNullRead(ColumnVector vector, int row) {\n+      return LocalTime.ofNanoOfDay(((LongColumnVector) vector).vector[row] * 1_000);", "originalCommit": "9f0cecd7443e99b2daece86d4d143a98e597ed9e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU3NzgwOA==", "url": "https://github.com/apache/iceberg/pull/1072#discussion_r431577808", "bodyText": "fixed", "author": "rdsr", "createdAt": "2020-05-28T04:42:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU0MDg0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTYwNjU5MQ==", "url": "https://github.com/apache/iceberg/pull/1072#discussion_r431606591", "bodyText": "Nit: Rename to bytebuffers() and ByteBufferReader? Less confusion between OrcValueReader.bytes() and this", "author": "shardulm94", "createdAt": "2020-05-28T06:23:54Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcReaders.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data.orc;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.iceberg.data.DateTimeUtil;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.orc.OrcValueReader;\n+import org.apache.iceberg.orc.OrcValueReaders;\n+import org.apache.iceberg.types.Types;\n+import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n+\n+\n+public class GenericOrcReaders {\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  private GenericOrcReaders() {\n+  }\n+\n+  public static OrcValueReader<Record> struct(\n+      List<OrcValueReader<?>> readers, Types.StructType struct, Map<Integer, ?> idToConstant) {\n+    return new StructReader(readers, struct, idToConstant);\n+  }\n+\n+  public static OrcValueReader<List<?>> array(OrcValueReader<?> elementReader) {\n+    return new ListReader(elementReader);\n+  }\n+\n+  public static OrcValueReader<Map<?, ?>> map(OrcValueReader<?> keyReader, OrcValueReader<?> valueReader) {\n+    return new MapReader(keyReader, valueReader);\n+  }\n+\n+  public static OrcValueReader<OffsetDateTime> timestampTzs() {\n+    return TimestampTzReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<BigDecimal> decimals() {\n+    return DecimalReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<String> strings() {\n+    return StringReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<UUID> uuid() {\n+    return UUIDReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<ByteBuffer> bytes() {", "originalCommit": "d35288ced15f7c8d0878e22c076c094ca328e122", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTkwNTExOA==", "url": "https://github.com/apache/iceberg/pull/1072#discussion_r431905118", "bodyText": "I would prefer to keep it as bytes. bytes[] vs ByteBuffer seems the datatypes which Spark and Generic Reader understand. The high level idea is still returning bytes in some form.", "author": "rdsr", "createdAt": "2020-05-28T14:59:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTYwNjU5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTkzNDg4NA==", "url": "https://github.com/apache/iceberg/pull/1072#discussion_r431934884", "bodyText": "I think the current name is okay, too.", "author": "rdblue", "createdAt": "2020-05-28T15:40:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTYwNjU5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTYwNjc2Mg==", "url": "https://github.com/apache/iceberg/pull/1072#discussion_r431606762", "bodyText": "Nit: Rename to uuids()? More consistent naming with other methods", "author": "shardulm94", "createdAt": "2020-05-28T06:24:23Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcReaders.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data.orc;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.iceberg.data.DateTimeUtil;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.orc.OrcValueReader;\n+import org.apache.iceberg.orc.OrcValueReaders;\n+import org.apache.iceberg.types.Types;\n+import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n+\n+\n+public class GenericOrcReaders {\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  private GenericOrcReaders() {\n+  }\n+\n+  public static OrcValueReader<Record> struct(\n+      List<OrcValueReader<?>> readers, Types.StructType struct, Map<Integer, ?> idToConstant) {\n+    return new StructReader(readers, struct, idToConstant);\n+  }\n+\n+  public static OrcValueReader<List<?>> array(OrcValueReader<?> elementReader) {\n+    return new ListReader(elementReader);\n+  }\n+\n+  public static OrcValueReader<Map<?, ?>> map(OrcValueReader<?> keyReader, OrcValueReader<?> valueReader) {\n+    return new MapReader(keyReader, valueReader);\n+  }\n+\n+  public static OrcValueReader<OffsetDateTime> timestampTzs() {\n+    return TimestampTzReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<BigDecimal> decimals() {\n+    return DecimalReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<String> strings() {\n+    return StringReader.INSTANCE;\n+  }\n+\n+  public static OrcValueReader<UUID> uuid() {", "originalCommit": "d35288ced15f7c8d0878e22c076c094ca328e122", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "865ff695dd377b80e9771e4a93ac2fed8f075808", "url": "https://github.com/apache/iceberg/commit/865ff695dd377b80e9771e4a93ac2fed8f075808", "message": "Support nested identity partitioning for GenericOrcReader", "committedDate": "2020-05-28T15:56:38Z", "type": "commit"}, {"oid": "d7fb4ab7fe958f62f1598b0d635382a5404e2ac4", "url": "https://github.com/apache/iceberg/commit/d7fb4ab7fe958f62f1598b0d635382a5404e2ac4", "message": "Address comments", "committedDate": "2020-05-28T15:56:38Z", "type": "commit"}, {"oid": "f7cabe010931c02c6d44ddc2ad97ac5714235b17", "url": "https://github.com/apache/iceberg/commit/f7cabe010931c02c6d44ddc2ad97ac5714235b17", "message": "Address comments", "committedDate": "2020-05-28T15:56:38Z", "type": "commit"}, {"oid": "6d43c8b900ee9a4810b5c06e803f914446883b85", "url": "https://github.com/apache/iceberg/commit/6d43c8b900ee9a4810b5c06e803f914446883b85", "message": "Fix DateTimeUtil package", "committedDate": "2020-05-28T16:00:33Z", "type": "commit"}, {"oid": "6d43c8b900ee9a4810b5c06e803f914446883b85", "url": "https://github.com/apache/iceberg/commit/6d43c8b900ee9a4810b5c06e803f914446883b85", "message": "Fix DateTimeUtil package", "committedDate": "2020-05-28T16:00:33Z", "type": "forcePushed"}]}