{"pr_number": 1299, "pr_title": "Flink: support to RowData partition.", "pr_createdAt": "2020-08-05T13:13:24Z", "pr_url": "https://github.com/apache/iceberg/pull/1299", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzUwNDAzMg==", "url": "https://github.com/apache/iceberg/pull/1299#discussion_r467504032", "bodyText": "Iceberg requires strings to be CharSequence, not necessarily String. So if you have UTF8 data, you can potentially just wrap it to produce a CharSequence rather than building an immutable JVM string.\nNot a blocker, just something to keep in mind for the future.", "author": "rdblue", "createdAt": "2020-08-08T21:04:05Z", "path": "flink/src/main/java/org/apache/iceberg/flink/RowDataWrapper.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.lang.reflect.Array;\n+import java.nio.ByteBuffer;\n+import java.time.LocalDateTime;\n+import java.util.UUID;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.DecimalType;\n+import org.apache.flink.table.types.logical.LocalZonedTimestampType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TimestampType;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.DateTimeUtil;\n+\n+class RowDataWrapper implements StructLike {\n+\n+  private final LogicalType[] types;\n+  private final PositionalGetter<?>[] getters;\n+  private RowData rowData = null;\n+\n+  RowDataWrapper(RowType rowType, Types.StructType struct) {\n+    int size = rowType.getFieldCount();\n+\n+    types = (LogicalType[]) Array.newInstance(LogicalType.class, size);\n+    getters = (PositionalGetter[]) Array.newInstance(PositionalGetter.class, size);\n+\n+    for (int i = 0; i < size; i++) {\n+      types[i] = rowType.getTypeAt(i);\n+      getters[i] = buildGetter(types[i], struct.fields().get(i).type());\n+    }\n+  }\n+\n+  RowDataWrapper wrap(RowData data) {\n+    this.rowData = data;\n+    return this;\n+  }\n+\n+  @Override\n+  public int size() {\n+    return types.length;\n+  }\n+\n+  @Override\n+  public <T> T get(int pos, Class<T> javaClass) {\n+    if (rowData.isNullAt(pos)) {\n+      return null;\n+    } else if (getters[pos] != null) {\n+      return javaClass.cast(getters[pos].get(rowData, pos));\n+    }\n+\n+    Object value = RowData.createFieldGetter(types[pos], pos).getFieldOrNull(rowData);\n+    return javaClass.cast(value);\n+  }\n+\n+  @Override\n+  public <T> void set(int pos, T value) {\n+    throw new UnsupportedOperationException(\"Could not set a field in the RowDataWrapper because rowData is read-only\");\n+  }\n+\n+  private interface PositionalGetter<T> {\n+    T get(RowData data, int pos);\n+  }\n+\n+  private static PositionalGetter<?> buildGetter(LogicalType logicalType, Type type) {\n+    switch (type.typeId()) {\n+      case STRING:\n+        return (row, pos) -> row.getString(pos).toString();", "originalCommit": "3b099fd92c5fdef8228fabdb42140646e8da8dd2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzUwNDE0Mg==", "url": "https://github.com/apache/iceberg/pull/1299#discussion_r467504142", "bodyText": "This needs to be in microseconds, not milliseconds. We should probably include a comment about it as well.", "author": "rdblue", "createdAt": "2020-08-08T21:05:03Z", "path": "flink/src/main/java/org/apache/iceberg/flink/RowDataWrapper.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.lang.reflect.Array;\n+import java.nio.ByteBuffer;\n+import java.time.LocalDateTime;\n+import java.util.UUID;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.DecimalType;\n+import org.apache.flink.table.types.logical.LocalZonedTimestampType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TimestampType;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.DateTimeUtil;\n+\n+class RowDataWrapper implements StructLike {\n+\n+  private final LogicalType[] types;\n+  private final PositionalGetter<?>[] getters;\n+  private RowData rowData = null;\n+\n+  RowDataWrapper(RowType rowType, Types.StructType struct) {\n+    int size = rowType.getFieldCount();\n+\n+    types = (LogicalType[]) Array.newInstance(LogicalType.class, size);\n+    getters = (PositionalGetter[]) Array.newInstance(PositionalGetter.class, size);\n+\n+    for (int i = 0; i < size; i++) {\n+      types[i] = rowType.getTypeAt(i);\n+      getters[i] = buildGetter(types[i], struct.fields().get(i).type());\n+    }\n+  }\n+\n+  RowDataWrapper wrap(RowData data) {\n+    this.rowData = data;\n+    return this;\n+  }\n+\n+  @Override\n+  public int size() {\n+    return types.length;\n+  }\n+\n+  @Override\n+  public <T> T get(int pos, Class<T> javaClass) {\n+    if (rowData.isNullAt(pos)) {\n+      return null;\n+    } else if (getters[pos] != null) {\n+      return javaClass.cast(getters[pos].get(rowData, pos));\n+    }\n+\n+    Object value = RowData.createFieldGetter(types[pos], pos).getFieldOrNull(rowData);\n+    return javaClass.cast(value);\n+  }\n+\n+  @Override\n+  public <T> void set(int pos, T value) {\n+    throw new UnsupportedOperationException(\"Could not set a field in the RowDataWrapper because rowData is read-only\");\n+  }\n+\n+  private interface PositionalGetter<T> {\n+    T get(RowData data, int pos);\n+  }\n+\n+  private static PositionalGetter<?> buildGetter(LogicalType logicalType, Type type) {\n+    switch (type.typeId()) {\n+      case STRING:\n+        return (row, pos) -> row.getString(pos).toString();\n+\n+      case FIXED:\n+      case BINARY:\n+        return (row, pos) -> ByteBuffer.wrap(row.getBinary(pos));\n+\n+      case UUID:\n+        return (row, pos) -> UUID.nameUUIDFromBytes(row.getBinary(pos));\n+\n+      case DECIMAL:\n+        DecimalType decimalType = (DecimalType) logicalType;\n+        return (row, pos) -> row.getDecimal(pos, decimalType.getPrecision(), decimalType.getScale()).toBigDecimal();\n+\n+      case TIME:\n+        return (row, pos) -> (long) row.getInt(pos);", "originalCommit": "3b099fd92c5fdef8228fabdb42140646e8da8dd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzY3MjA4Nw==", "url": "https://github.com/apache/iceberg/pull/1299#discussion_r467672087", "bodyText": "You are right.  Flink's time type is milliseconds,  here we need microseconds.  Will add unit tests to address this bug.", "author": "openinx", "createdAt": "2020-08-10T03:21:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzUwNDE0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzUwNDIxNg==", "url": "https://github.com/apache/iceberg/pull/1299#discussion_r467504216", "bodyText": "Nano of millisecond is always positive, right? In that case there is no need for floorDiv.", "author": "rdblue", "createdAt": "2020-08-08T21:06:05Z", "path": "flink/src/main/java/org/apache/iceberg/flink/RowDataWrapper.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.lang.reflect.Array;\n+import java.nio.ByteBuffer;\n+import java.time.LocalDateTime;\n+import java.util.UUID;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.DecimalType;\n+import org.apache.flink.table.types.logical.LocalZonedTimestampType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.TimestampType;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.DateTimeUtil;\n+\n+class RowDataWrapper implements StructLike {\n+\n+  private final LogicalType[] types;\n+  private final PositionalGetter<?>[] getters;\n+  private RowData rowData = null;\n+\n+  RowDataWrapper(RowType rowType, Types.StructType struct) {\n+    int size = rowType.getFieldCount();\n+\n+    types = (LogicalType[]) Array.newInstance(LogicalType.class, size);\n+    getters = (PositionalGetter[]) Array.newInstance(PositionalGetter.class, size);\n+\n+    for (int i = 0; i < size; i++) {\n+      types[i] = rowType.getTypeAt(i);\n+      getters[i] = buildGetter(types[i], struct.fields().get(i).type());\n+    }\n+  }\n+\n+  RowDataWrapper wrap(RowData data) {\n+    this.rowData = data;\n+    return this;\n+  }\n+\n+  @Override\n+  public int size() {\n+    return types.length;\n+  }\n+\n+  @Override\n+  public <T> T get(int pos, Class<T> javaClass) {\n+    if (rowData.isNullAt(pos)) {\n+      return null;\n+    } else if (getters[pos] != null) {\n+      return javaClass.cast(getters[pos].get(rowData, pos));\n+    }\n+\n+    Object value = RowData.createFieldGetter(types[pos], pos).getFieldOrNull(rowData);\n+    return javaClass.cast(value);\n+  }\n+\n+  @Override\n+  public <T> void set(int pos, T value) {\n+    throw new UnsupportedOperationException(\"Could not set a field in the RowDataWrapper because rowData is read-only\");\n+  }\n+\n+  private interface PositionalGetter<T> {\n+    T get(RowData data, int pos);\n+  }\n+\n+  private static PositionalGetter<?> buildGetter(LogicalType logicalType, Type type) {\n+    switch (type.typeId()) {\n+      case STRING:\n+        return (row, pos) -> row.getString(pos).toString();\n+\n+      case FIXED:\n+      case BINARY:\n+        return (row, pos) -> ByteBuffer.wrap(row.getBinary(pos));\n+\n+      case UUID:\n+        return (row, pos) -> UUID.nameUUIDFromBytes(row.getBinary(pos));\n+\n+      case DECIMAL:\n+        DecimalType decimalType = (DecimalType) logicalType;\n+        return (row, pos) -> row.getDecimal(pos, decimalType.getPrecision(), decimalType.getScale()).toBigDecimal();\n+\n+      case TIME:\n+        return (row, pos) -> (long) row.getInt(pos);\n+\n+      case TIMESTAMP:\n+        switch (logicalType.getTypeRoot()) {\n+          case TIMESTAMP_WITHOUT_TIME_ZONE:\n+            TimestampType timestampType = (TimestampType) logicalType;\n+            return (row, pos) -> {\n+              LocalDateTime localDateTime = row.getTimestamp(pos, timestampType.getPrecision()).toLocalDateTime();\n+              return DateTimeUtil.microsFromTimestamp(localDateTime);\n+            };\n+\n+          case TIMESTAMP_WITH_LOCAL_TIME_ZONE:\n+            LocalZonedTimestampType lzTs = (LocalZonedTimestampType) logicalType;\n+            return (row, pos) -> {\n+              TimestampData timestampData = row.getTimestamp(pos, lzTs.getPrecision());\n+              return timestampData.getMillisecond() * 1000 + Math.floorDiv(timestampData.getNanoOfMillisecond(), 1000);", "originalCommit": "3b099fd92c5fdef8228fabdb42140646e8da8dd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzY3MjY0Mg==", "url": "https://github.com/apache/iceberg/pull/1299#discussion_r467672642", "bodyText": "It's true.", "author": "openinx", "createdAt": "2020-08-10T03:24:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzUwNDIxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzUwNDYwMw==", "url": "https://github.com/apache/iceberg/pull/1299#discussion_r467504603", "bodyText": "This has the same time bug as the getter. I think it would be better to avoid deriving expected values and just hard-code them. That makes tests easier to read, and more reliable.", "author": "rdblue", "createdAt": "2020-08-08T21:10:44Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestRowDataPartitionKey.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.flink.data.RandomRowData;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.DateTimeUtil;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+public class TestRowDataPartitionKey {\n+  private static final Schema SCHEMA = new Schema(\n+      Types.NestedField.required(0, \"boolType\", Types.BooleanType.get()),\n+      Types.NestedField.required(1, \"id\", Types.IntegerType.get()),\n+      Types.NestedField.required(2, \"longType\", Types.LongType.get()),\n+      Types.NestedField.required(3, \"dateType\", Types.DateType.get()),\n+      Types.NestedField.required(4, \"timeType\", Types.TimeType.get()),\n+      Types.NestedField.required(5, \"stringType\", Types.StringType.get()),\n+      Types.NestedField.required(6, \"timestampWithoutZone\", Types.TimestampType.withoutZone()),\n+      Types.NestedField.required(7, \"timestampWithZone\", Types.TimestampType.withZone()),\n+      Types.NestedField.required(8, \"fixedType\", Types.FixedType.ofLength(5)),\n+      Types.NestedField.required(9, \"uuidType\", Types.UUIDType.get()),\n+      Types.NestedField.required(10, \"binaryType\", Types.BinaryType.get()),\n+      Types.NestedField.required(11, \"decimalType1\", Types.DecimalType.of(18, 3)),\n+      Types.NestedField.required(12, \"decimalType2\", Types.DecimalType.of(10, 5)),\n+      Types.NestedField.required(13, \"decimalType3\", Types.DecimalType.of(38, 19)),\n+      Types.NestedField.required(14, \"floatType\", Types.FloatType.get()),\n+      Types.NestedField.required(15, \"doubleType\", Types.DoubleType.get())\n+  );\n+\n+  private static final List<String> SUPPORTED_PRIMITIVES = SCHEMA.asStruct().fields().stream()\n+      .map(Types.NestedField::name).collect(Collectors.toList());\n+\n+  private static final Schema NESTED_SCHEMA = new Schema(\n+      Types.NestedField.required(1, \"structType\", Types.StructType.of(\n+          Types.NestedField.optional(2, \"innerStringType\", Types.StringType.get()),\n+          Types.NestedField.optional(3, \"innerIntegerType\", Types.IntegerType.get())\n+      ))\n+  );\n+\n+  @Test\n+  public void testNullPartitionValue() {\n+    Schema schema = new Schema(\n+        Types.NestedField.optional(1, \"id\", Types.IntegerType.get()),\n+        Types.NestedField.optional(2, \"data\", Types.StringType.get())\n+    );\n+\n+    PartitionSpec spec = PartitionSpec.builderFor(schema)\n+        .identity(\"data\")\n+        .build();\n+\n+    List<RowData> rows = Lists.newArrayList(\n+        GenericRowData.of(1, StringData.fromString(\"a\")),\n+        GenericRowData.of(2, StringData.fromString(\"b\")),\n+        GenericRowData.of(3, null)\n+    );\n+\n+    RowDataWrapper rowWrapper = new RowDataWrapper(FlinkSchemaUtil.convert(schema), schema.asStruct());\n+\n+    for (RowData row : rows) {\n+      PartitionKey partitionKey = new PartitionKey(spec, schema);\n+      partitionKey.partition(rowWrapper.wrap(row));\n+      Assert.assertEquals(partitionKey.size(), 1);\n+\n+      String expectedStr = row.isNullAt(1) ? null : row.getString(1).toString();\n+      Assert.assertEquals(expectedStr, partitionKey.get(0, String.class));\n+    }\n+  }\n+\n+  @Test\n+  public void testPartitionWithOneNestedField() {\n+    RowDataWrapper rowWrapper = new RowDataWrapper(FlinkSchemaUtil.convert(NESTED_SCHEMA), NESTED_SCHEMA.asStruct());\n+    Iterable<RowData> rows = RandomRowData.generate(NESTED_SCHEMA, 10, 1991);\n+\n+    PartitionSpec spec1 = PartitionSpec.builderFor(NESTED_SCHEMA)\n+        .identity(\"structType.innerStringType\")\n+        .build();\n+    PartitionSpec spec2 = PartitionSpec.builderFor(NESTED_SCHEMA)\n+        .identity(\"structType.innerIntegerType\")\n+        .build();\n+\n+    for (RowData row : rows) {\n+      RowData innerRow = row.getRow(0, 2);\n+\n+      PartitionKey partitionKey1 = new PartitionKey(spec1, NESTED_SCHEMA);\n+      partitionKey1.partition(rowWrapper.wrap(row));\n+      Assert.assertEquals(partitionKey1.size(), 1);\n+\n+      String expectedStr = innerRow.isNullAt(0) ? null : innerRow.getString(0).toString();\n+      Assert.assertEquals(expectedStr, partitionKey1.get(0, String.class));\n+\n+      PartitionKey partitionKey2 = new PartitionKey(spec2, NESTED_SCHEMA);\n+      partitionKey2.partition(rowWrapper.wrap(row));\n+      Assert.assertEquals(partitionKey2.size(), 1);\n+\n+      Integer expectedInt = innerRow.isNullAt(1) ? null : innerRow.getInt(1);\n+      Assert.assertEquals(expectedInt, partitionKey2.get(0, Integer.class));\n+    }\n+  }\n+\n+  @Test\n+  public void testPartitionMultipleNestedField() {\n+    RowDataWrapper rowWrapper = new RowDataWrapper(FlinkSchemaUtil.convert(NESTED_SCHEMA), NESTED_SCHEMA.asStruct());\n+    Iterable<RowData> rows = RandomRowData.generate(NESTED_SCHEMA, 10, 1992);\n+\n+    PartitionSpec spec1 = PartitionSpec.builderFor(NESTED_SCHEMA)\n+        .identity(\"structType.innerIntegerType\")\n+        .identity(\"structType.innerStringType\")\n+        .build();\n+    PartitionSpec spec2 = PartitionSpec.builderFor(NESTED_SCHEMA)\n+        .identity(\"structType.innerStringType\")\n+        .identity(\"structType.innerIntegerType\")\n+        .build();\n+\n+    PartitionKey pk1 = new PartitionKey(spec1, NESTED_SCHEMA);\n+    PartitionKey pk2 = new PartitionKey(spec2, NESTED_SCHEMA);\n+\n+    for (RowData row : rows) {\n+      RowData innerRow = row.getRow(0, 2);\n+\n+      pk1.partition(rowWrapper.wrap(row));\n+      Assert.assertEquals(2, pk1.size());\n+\n+      Integer expectedInt = innerRow.isNullAt(1) ? null : innerRow.getInt(1);\n+      Assert.assertEquals(expectedInt, pk1.get(0, Integer.class));\n+\n+      String expectedStr = innerRow.isNullAt(0) ? null : innerRow.getString(0).toString();\n+      Assert.assertEquals(expectedStr, pk1.get(1, String.class));\n+\n+      pk2.partition(rowWrapper.wrap(row));\n+      Assert.assertEquals(2, pk2.size());\n+\n+      expectedStr = innerRow.isNullAt(0) ? null : innerRow.getString(0).toString();\n+      Assert.assertEquals(expectedStr, pk2.get(0, String.class));\n+\n+      expectedInt = innerRow.isNullAt(1) ? null : innerRow.getInt(1);\n+      Assert.assertEquals(expectedInt, pk2.get(1, Integer.class));\n+    }\n+  }\n+\n+  private static Object transform(Object value, Type type) {", "originalCommit": "3b099fd92c5fdef8228fabdb42140646e8da8dd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzUwNDcwNw==", "url": "https://github.com/apache/iceberg/pull/1299#discussion_r467504707", "bodyText": "An alternative to hard-coding is to validate against a different object model. For example, you could generate the data with Iceberg generics, convert them to RowData, and then validate that the partitions produced from both object models match. That would catch the time bug and would also allow you to avoid needing to generate random RowData.", "author": "rdblue", "createdAt": "2020-08-08T21:12:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzUwNDYwMw=="}], "type": "inlineReview"}, {"oid": "3f833fa54b17222fe257d03f749133950e6d152b", "url": "https://github.com/apache/iceberg/commit/3f833fa54b17222fe257d03f749133950e6d152b", "message": "Flink: support to RowData partition.", "committedDate": "2020-08-10T03:16:17Z", "type": "commit"}, {"oid": "a96a4ac1e0cfee8700a5dee922d6c398ba185180", "url": "https://github.com/apache/iceberg/commit/a96a4ac1e0cfee8700a5dee922d6c398ba185180", "message": "Add bool/long/timestamp type in the unit tests.", "committedDate": "2020-08-10T03:16:17Z", "type": "commit"}, {"oid": "2443b70670f6d591dba8e72b44ea3a0d0c232584", "url": "https://github.com/apache/iceberg/commit/2443b70670f6d591dba8e72b44ea3a0d0c232584", "message": "Addressming the time bug and rewrite the unit tests by comparing record and row.", "committedDate": "2020-08-10T03:52:58Z", "type": "commit"}, {"oid": "2443b70670f6d591dba8e72b44ea3a0d0c232584", "url": "https://github.com/apache/iceberg/commit/2443b70670f6d591dba8e72b44ea3a0d0c232584", "message": "Addressming the time bug and rewrite the unit tests by comparing record and row.", "committedDate": "2020-08-10T03:52:58Z", "type": "forcePushed"}, {"oid": "8500e2b9bbf857e199ecc4dd4f118cfa00f5062e", "url": "https://github.com/apache/iceberg/commit/8500e2b9bbf857e199ecc4dd4f118cfa00f5062e", "message": "Convert Record to RowData.", "committedDate": "2020-08-10T14:48:56Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzk2Mzc3MA==", "url": "https://github.com/apache/iceberg/pull/1299#discussion_r467963770", "bodyText": "Here, we will truncate the localTime to be milliseconds,  so we will erase the microseconds part.  That's to say,  the partition value will be different between Record and RowData because lost microseconds.  Should we disable the TIME type as a partition key when using flink sink connector in case of partition value mismatching ?\nThe following unit tests indicate the above thing : https://github.com/apache/iceberg/pull/1299/files#diff-97304b05e2faea4a749031f514361a70R193", "author": "openinx", "createdAt": "2020-08-10T14:57:32Z", "path": "flink/src/test/java/org/apache/iceberg/flink/RowDataConverter.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.TimeUnit;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+class RowDataConverter {\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  private RowDataConverter() {\n+  }\n+\n+  static RowData convert(Schema iSchema, Record record) {\n+    return convert(iSchema.asStruct(), record);\n+  }\n+\n+  private static RowData convert(Types.StructType struct, Record record) {\n+    GenericRowData rowData = new GenericRowData(struct.fields().size());\n+    List<Types.NestedField> fields = struct.fields();\n+    for (int i = 0; i < fields.size(); i += 1) {\n+      Types.NestedField field = fields.get(i);\n+\n+      Type fieldType = field.type();\n+\n+      switch (fieldType.typeId()) {\n+        case STRUCT:\n+          rowData.setField(i, convert(fieldType.asStructType(), record.get(i)));\n+          break;\n+        case LIST:\n+          rowData.setField(i, convert(fieldType.asListType(), record.get(i)));\n+          break;\n+        case MAP:\n+          rowData.setField(i, convert(fieldType.asMapType(), record.get(i)));\n+          break;\n+        default:\n+          rowData.setField(i, convert(fieldType, record.get(i)));\n+      }\n+    }\n+    return rowData;\n+  }\n+\n+  private static Object convert(Type type, Object object) {\n+    if (object == null) {\n+      return null;\n+    }\n+\n+    switch (type.typeId()) {\n+      case BOOLEAN:\n+      case INTEGER:\n+      case LONG:\n+      case FLOAT:\n+      case DOUBLE:\n+      case FIXED:\n+        return object;\n+      case DATE:\n+        return (int) ChronoUnit.DAYS.between(EPOCH_DAY, (LocalDate) object);\n+      case TIME:\n+        // Iceberg's time is in microseconds, while flink's time is in milliseconds.\n+        LocalTime localTime = (LocalTime) object;\n+        return (int) TimeUnit.NANOSECONDS.toMillis(localTime.toNanoOfDay());", "originalCommit": "8500e2b9bbf857e199ecc4dd4f118cfa00f5062e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIyNjc3NA==", "url": "https://github.com/apache/iceberg/pull/1299#discussion_r468226774", "bodyText": "I don't think it is necessary to disable because any data written by Flink will necessarily be a millisecond-precision value. Partitioning is still correct with respect to the data that was written, because all of the data has millisecond values.", "author": "rdblue", "createdAt": "2020-08-10T22:45:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzk2Mzc3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI5NTg5Nw==", "url": "https://github.com/apache/iceberg/pull/1299#discussion_r468295897", "bodyText": "For the same data with time type,  if flink write them into an iceberg table A, and hive MR or spark read it, in this case,  there should be no problem.  But for the same data set,  both flink and spark write them into difference tables A and B, then there should be difference between table A and B because of lost microseconds.  The differences sounds reasonable because of the different behavior from different compute engines.", "author": "openinx", "createdAt": "2020-08-11T02:50:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzk2Mzc3MA=="}], "type": "inlineReview"}]}